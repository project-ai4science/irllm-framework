id,date,a_abstract,b_id,b_abstract,b_categories,c_id,c_abstract,c_categories,y_true,research_type,a_title,a_categories,b_title,c_title
neg-d2-0,2025-03-23,,2503.18058," Establishing detailed relationships between transnormal systems of different
types and their behaviors under covering maps, this paper presents a
classification of transnormal systems on compact 3-manifolds in the sense of
equivalence. For CPC transnormal systems, we show that the ambient manifolds
must be locally isometric to one of six standard geometries up to equivalence.
We also find some equivalence classes containing no CPC transnormal system,
highlighting a critical distinction between isoparametric foliations and CPC
transnormal systems, which has not been previously addressed in the literature.",['math.DG'],2501.01753," Detecting the transition from laminar to turbulent flow in particulate pipe
systems remains a complex issue in fluid dynamics, often requiring
sophisticated and costly experimental apparatus. This research presents an
innovative streak visualization method designed to offer a simple and robust
approach to identify transitional turbulent patterns in particulate pipe flows
with neutrally buoyant particles. The technique employs a laser arrangement and
a low-cost camera setup to capture particle-generated streaks within the fluid,
enabling real-time observation of flow patterns. Validation of the proposed
method was conducted through comparison with established techniques like
Particle Image Velocimetry (PIV) and pressure drop measurements, confirming its
accuracy and reliability. Experiments demonstrate the streak visualization
method's capacity to differentiate between laminar, transitional, and turbulent
flow regimes by analyzing the standard deviation of streak angles. The method
is especially efficient at low particle concentration, ie precisely where other
more established methods become less effective. Furthermore, this technique
enables us to identify a critical Reynolds number using Kullback-Leibler
divergence built on the statistical distribution of streak angles, which is
consistent with previous studies.
  Because it is effective at low concentrations and robust, this streak
visualization technique opens new perspectives for the characterization of
particulate pipe flows not only in the confines of the laboratory but also in
less controlled industrial multi-phase flows where determining the laminar or
turbulent nature of the flow is a prerequisite for flowmeter calibration.",['physics.flu-dyn'],False,,,,"On the Classification of Isoparametric Hypersurfaces with Constant
  Principal Curvatures in Compact 3-Manifolds","Detecting Turbulent Patterns in Particulate Pipe Flow by Streak Angle
  Visualization"
neg-d2-1,2025-01-14,,2501.08479," Serverless computing offers elasticity unmatched by conventional server-based
cloud infrastructure. Although modern data processing systems embrace
serverless storage, such as Amazon S3, they continue to manage their compute
resources as servers. This is challenging for unpredictable workloads, leaving
clusters often underutilized. Recent research shows the potential of serverless
compute resources, such as cloud functions, for elastic data processing, but
also sees limitations in performance robustness and cost efficiency for long
running workloads. These challenges require holistic approaches across the
system stack. However, to the best of our knowledge, there is no end-to-end
data processing system built entirely on serverless infrastructure. In this
paper, we present Skyrise, our effort towards building the first fully
serverless SQL query processor. Skyrise exploits the elasticity of its
underlying infrastructure, while alleviating the inherent limitations with a
number of adaptive and cost-aware techniques. We show that both Skyrise's
performance and cost are competitive to other cloud data systems for
terabyte-scale queries of the analytical TPC-H benchmark.",['cs.DB'],2502.09337," This thesis is an exposition of the author's contribution on effective
descent morphisms in various categories of generalized categorical structures.
It consists of: Chapter 1, where an elementary description of descent theory
and the content of each remaining chapter is provided, supplemented with
references; Chapter 2, consisting of various descent theoretical definitions
and results employed in the remainder of this work; four chapters, each
corresponding to an article written by the author during the period of his PhD
studies.",['math.CT'],False,,,,"Skyrise: Exploiting Serverless Cloud Infrastructure for Elastic Data
  Processing",Some aspects of descent theory and applications
neg-d2-2,2025-01-29,,2501.17489," Brain-computer interfaces (BCIs) present a promising avenue by translating
neural activity directly into text, eliminating the need for physical actions.
However, existing non-invasive BCI systems have not successfully covered the
entire alphabet, limiting their practicality. In this paper, we propose a novel
non-invasive EEG-based BCI system with Curriculum-based Neural Spelling
Framework, which recognizes all 26 alphabet letters by decoding neural signals
associated with handwriting first, and then apply a Generative AI (GenAI) to
enhance spell-based neural language decoding tasks. Our approach combines the
ease of handwriting with the accessibility of EEG technology, utilizing
advanced neural decoding algorithms and pre-trained large language models
(LLMs) to translate EEG patterns into text with high accuracy. This system show
how GenAI can improve the performance of typical spelling-based neural language
decoding task, and addresses the limitations of previous methods, offering a
scalable and user-friendly solution for individuals with communication
impairments, thereby enhancing inclusive communication options.","['cs.HC', 'cs.AI']",2502.1968," Recent advances in Multi-Modal Large Language Models (M-LLMs) show promising
results in video reasoning. Popular Multi-Modal Large Language Model (M-LLM)
frameworks usually apply naive uniform sampling to reduce the number of video
frames that are fed into an M-LLM, particularly for long context videos.
However, it could lose crucial context in certain periods of a video, so that
the downstream M-LLM may not have sufficient visual information to answer a
question. To attack this pain point, we propose a light-weight M-LLM -based
frame selection method that adaptively select frames that are more relevant to
users' queries. In order to train the proposed frame selector, we introduce two
supervision signals (i) Spatial signal, where single frame importance score by
prompting a M-LLM; (ii) Temporal signal, in which multiple frames selection by
prompting Large Language Model (LLM) using the captions of all frame
candidates. The selected frames are then digested by a frozen downstream video
M-LLM for visual reasoning and question answering. Empirical results show that
the proposed M-LLM video frame selector improves the performances various
downstream video Large Language Model (video-LLM) across medium (ActivityNet,
NExT-QA) and long (EgoSchema, LongVideoBench) context video question answering
benchmarks.","['cs.CV', 'cs.AI']",False,,,,Neural Spelling: A Spell-Based BCI System for Language Neural Decoding,M-LLM Based Video Frame Selection for Efficient Video Understanding
neg-d2-3,2025-01-24,,2501.14901," In a recent study, Dannen et al. surveyed a large parameter space to study
the transition from efficient to inefficient line driving. They found that when
the line force significantly weakens due to ionization, the winds are variable,
with a characteristic frequency comparable to the Lamb cut-off frequency of a
stratified atmosphere, {\omega}c. In this work, we present a set of simulations
and perturbation analyses that elucidate the variability source and
characteristics. We found that the line force adds wave energy and amplifies
perturbations with frequencies near {\omega}c. This selective amplification
results from the coupling between the natural tendency of velocity
perturbations to grow in a stratified atmosphere and the dependence of the line
force on the velocity gradient, per the Castor-Abbott-Klein line-driven wind
theory. We also found that the variability stems from self-excitation that
occurs in the exponential atmosphere due to the non-linearity introduced by the
absolute value of the velocity gradient in the line force prescription. We
conclude that self-consistently calculating ionization is insufficient for
modeling the dynamics in the subsonic atmosphere. Instead future wind models
should relax the Sobolev approximation, or model the radiative transfer to
capture the dynamics and instabilities at the base of the wind.",['astro-ph.GA'],2502.18295," In this note, we provide a proof of the existence and complete classification
of $G$-invariant star products with quantum momentum maps on Poisson manifolds
by means of an equivariant version of the formality theorem.","['math.QA', 'math-ph', 'math.MP', 'math.SG']",False,,,,Oscillatory Line-Driven Winds: The Role of Atmospheric Stratification,Quantization of the Momentum Map via $\frak{g}$-adapted Formalities
neg-d2-4,2025-03-06,,2503.04612," This note is concerned with the distribution of the angles between Oseledets
subspaces for linear cocycles driven by an ergodic transformation. We restrict
ourselves to dimension $2$, and give particular attention to the question of
log-integrability of those angles. In the setting of random i.i.d.\ products of
matrices, we construct examples of probability measures on \(\GL_2(\R)\) with
finite first moment, for which the angle between Oseledets directions of the
associated cocycle is not log-integrable. Building on work for the totally
irreducible case by Benoist and Quint, we show that for probability measures
with finite second moment the angle between Oseledets subspaces is always
log-integrable. Then we pivot to general measurable \(\GL_2(\R)\)-cocycles over
an arbitrary ergodic automorphism of a non-atomic Lebesgue space. We show that
no integrability condition on the distribution of the matrices is sufficient to
guarantee log-integrability of the angle between Oseledets spaces. In fact, in
this context we show that the joint distribution of the Oseledets spaces may be
chosen arbitrarily. We also obtain a similar flexibility result for bounded
cocycles under the proper condition on the distribution of angles.",['math.DS'],2501.0317," Microwave shielding is an important technique that can suppress the losses
that arise from collisions of ultracold polar molecules. It has been
instrumental in achieving molecular Bose-Einstein condensation (BEC) for NaCs
[Bigagli et al., Nature 631, 289 (2024)]. We demonstrate that microwave
shielding is universal, in the sense that the 2-body collision properties of
different molecules are very similar when expressed in suitable reduced units
of length and energy. This applies to rate coefficients for inelastic
scattering and loss, to scattering lengths, and to the properties of 2-molecule
bound states. We also explore the small deviations from universality that arise
at very large Rabi frequencies. In general, the collision properties are
near-universal except when the Rabi frequency exceeds a few percent of the
molecular rotational constant. The universality extends to elliptically
polarized microwaves and to combinations of multiple fields. Our results
indicate that the methods that have been used to achieve BEC for NaCs can be
transferred directly to most other polar molecules.","['cond-mat.quant-gas', 'physics.atom-ph']",False,,,,On the distribution of the angle between Oseledets spaces,Universality in the microwave shielding of ultracold polar molecules
neg-d2-5,2025-01-09,,2501.0571," Recent research shows that emotions can enhance users' cognition and
influence information communication. While research on visual emotion analysis
is extensive, limited work has been done on helping users generate emotionally
rich image content. Existing work on emotional image generation relies on
discrete emotion categories, making it challenging to capture complex and
subtle emotional nuances accurately. Additionally, these methods struggle to
control the specific content of generated images based on text prompts. In this
work, we introduce the new task of continuous emotional image content
generation (C-EICG) and present EmotiCrafter, an emotional image generation
model that generates images based on text prompts and Valence-Arousal values.
Specifically, we propose a novel emotion-embedding mapping network that embeds
Valence-Arousal values into textual features, enabling the capture of specific
emotions in alignment with intended input prompts. Additionally, we introduce a
loss function to enhance emotion expression. The experimental results show that
our method effectively generates images representing specific emotions with the
desired content and outperforms existing techniques.",['cs.CV'],2501.05114," A growing number of directly-imaged companions have been recently
characterised, with robust constraints on carbon-to-oxygen ratios and even
isotopic ratios. Many companions and isolated targets have also shown spectral
variability. In this work we observed the super-Jupiter AB~Pictoris~b across
four consecutive nights using VLT/CRIRES+ as part of the ESO SupJup survey,
exploring how the constraints on chemical composition and temperature profile
change over time using spectral line shape variations between nights. We
performed atmospheric retrievals of the high-resolution observations and found
broadly consistent results across all four nights, but there were differences
for some parameters. We clearly detect H$_2$O, $^{12}$CO and $^{13}$CO in each
night, but abundances varied by $\sim2\sigma$, which was correlated to the deep
atmosphere temperature profiles. We also found differences in the
$^{12}$C$/^{13}$C ratios in each night by up to $\sim3\sigma$, which seemed to
be correlated with the cloud deck pressure. Our combined retrieval
simultaneously analysing all nights together constrained broadly the average of
each night individually, with the C/O$=0.59\pm0.01$, consistent with solar
composition, and $^{12}$C$/^{13}$C~$ = 102\pm8$, slightly higher than the ISM
and Solar System values. We also find a low projected rotational velocity,
suggesting that AB~Pictoris~b is either intrinsically a slow rotator due to its
young age or that the spin axis is observed pole-on with a $\sim90^\circ$
misalignment with its orbit inclination. Future observations will be able to
further explore the variability and orbit of AB~Pictoris~b as well as for other
companions.",['astro-ph.EP'],False,,,,"EmotiCrafter: Text-to-Emotional-Image Generation based on
  Valence-Arousal Model","The ESO SupJup Survey V: Exploring Atmospheric Variability and Orbit of
  the Super-Jupiter AB Pictoris b with CRIRES+"
neg-d2-6,2025-03-12,,2503.09389," We study canonical-equilibrium properties of Random Field $O(n)$ Models
involving classical continuous vector spins of $n$ components with mean-field
interactions and subject to disordered fields acting on individual spins. To
this end, we employ two complementary approaches: the mean-field approximation,
valid for any disorder distribution, and the replica trick, applicable when the
disordered fields are sampled from a Gaussian distribution. On the basis of an
exact analysis, we demonstrate that when replica symmetry holds, both the
approaches yield identical expression for the free energy per spin of the
system. As consequences, we study the case of $n=2$ ($XY$ spins) and that of
$n=3$ (Heisenberg spins) for two representative choices of the disorder
distribution, namely, a Gaussian and a symmetric bimodal distribution. For both
$n=2$ and $n=3$, we demonstrate that while the magnetization exhibits a
continuous phase transition as a function of temperature for the Gaussian case,
the transition could be either continuous or first-order with an emergent
tricriticality when the disorder distribution is bimodal. We also discuss in
the context of our models the issue of self-averaging of extensive variables
near the critical point of a continuous phase transition.","['cond-mat.stat-mech', 'cond-mat.dis-nn']",2501.17687," We study magneto-transport through topological insulator nanowires shaped in
the form of a constriction, as can be obtained by etching techniques. The
magnetic field is coaxial, potentially turning the nanowire into a
magneto-chiral junction. We show in a detailed analytical and numerical study
that two main transport regimes emerge, depending on the central narrow region
being short or long as compared to the magnetic length at the junction entrance
and exit. In both cases the central region hosts Dirac-particle-in-a-box states
due to magnetic confinement, whose conductance properties are strongly
influenced by Landau levels at the ends of the constriction. Notably, in the
low-energy regime only chiral states with a specific handedness can transport
charge across the junction. Based on these properties and general symmetry
considerations we argue that the shaped nanowire should exhibit strong
magneto-chiral non-reciprocal transport beyond linear response. We employ a
numerical tight-binding implementation of an effective 2D model on a
non-homogeneous grid, capable of simulating samples of realistic sizes, and
test its soundness against full simulations for scaled-down 3D topological
insulator wires.",['cond-mat.mes-hall'],False,,,,"Canonical equilibrium of mean-field $O(n)$~models in presence of random
  fields","Topological insulator constrictions -- Dirac particles in a
  magneto-chiral box"
neg-d2-7,2025-02-27,,2502.2045," Superconductivity has recently been demonstrated in La$_3$Ni$_2$O$_7$ up to
91K under moderate pressure in bulk crystals, and up to 48K at ambient pressure
in thin films grown under compressive strain. Key questions remain open
regarding the crystal structure and low-energy electronic states that support
superconductivity in these compounds. Here we take advantage of the natural
polymorphism between bilayer (2222) or alternating monolayer-trilayer (1313)
stacking sequences that arises in bulk La$_3$Ni$_2$O$_7$ crystals to identify
universal features in this family of materials. Employing angle-resolved
photoemission spectroscopy (ARPES) we observe the fingerprint of a spin-density
wave (SDW) instability, strong and coherent enough to modify the electronic
structure. We demonstrate that this feature is a `translated' $\beta$ Fermi
surface associated with a scattering vector $Q_{t\beta}$ which matches the
$Q_{SDW}$ detected by neutron and x-ray scattering experiments. This
observation provides an important link between surface and bulk probes, and
demonstrates a universal connection between magnetism and fermiology in
La$_3$Ni$_2$O$_7$ as well as La$_4$Ni$_3$O$_{10}$. We simulate the spectral
weight distribution observed in our ARPES dichroism experiments and establish
that the low-energy electronic phenomenology is dominated by oxygen-centered
planar orbitals, which -- upon moving along the Fermi surface away from the
Ni-O-Ni bond directions -- evolve from the $d_{3x^2-r^2}$ and $d_{3y^2-r^2}$
symmetry characteristic of 3-spin polarons to the familiar $d_{x^2-y^2}$
Zhang-Rice singlets that support high-temperature superconductivity in
cuprates. Despite the multiorbital nature of the nickelates, our work
establishes an empirical correspondence between the low-energy electronic
structure of cuprates and nickelates, thus suggesting a common origin for their
unconventional superconductivity.",['cond-mat.supr-con'],2503.16491," The rapid adoption of generative AI in software development has impacted the
industry, yet its effects on developers with visual impairments remain largely
unexplored. To address this gap, we used an Activity Theory framework to
examine how developers with visual impairments interact with AI coding
assistants. For this purpose, we conducted a study where developers who are
visually impaired completed a series of programming tasks using a generative AI
coding assistant. We uncovered that, while participants found the AI assistant
beneficial and reported significant advantages, they also highlighted
accessibility challenges. Specifically, the AI coding assistant often
exacerbated existing accessibility barriers and introduced new challenges. For
example, it overwhelmed users with an excessive number of suggestions, leading
developers who are visually impaired to express a desire for ``AI timeouts.''
Additionally, the generative AI coding assistant made it more difficult for
developers to switch contexts between the AI-generated content and their own
code. Despite these challenges, participants were optimistic about the
potential of AI coding assistants to transform the coding experience for
developers with visual impairments. Our findings emphasize the need to apply
activity-centered design principles to generative AI assistants, ensuring they
better align with user behaviors and address specific accessibility needs. This
approach can enable the assistants to provide more intuitive, inclusive, and
effective experiences, while also contributing to the broader goal of enhancing
accessibility in software development.","['cs.HC', 'cs.AI', 'cs.CY']",False,,,,"Universal electronic structure of layered nickelates via oxygen-centered
  planar orbitals","The Impact of Generative AI Coding Assistants on Developers Who Are
  Visually Impaired"
neg-d2-8,2025-02-03,,2502.01185," We present a novel deep learning network for Active Speech Cancellation
(ASC), advancing beyond Active Noise Cancellation (ANC) methods by effectively
canceling both noise and speech signals. The proposed Multi-Band Mamba
architecture segments input audio into distinct frequency bands, enabling
precise anti-signal generation and improved phase alignment across frequencies.
Additionally, we introduce an optimization-driven loss function that provides
near-optimal supervisory signals for anti-signal generation. Experimental
results demonstrate substantial performance gains, achieving up to 7.2dB
improvement in ANC scenarios and 6.2dB in ASC, significantly outperforming
existing methods. Audio samples are available at
https://mishalydev.github.io/DeepASC-Demo","['cs.SD', 'cs.AI', 'cs.LG', 'eess.AS', 'eess.SP']",2502.04659," Blockchains have revolutionized decentralized applications, with
composability enabling atomic, trustless interactions across smart contracts.
However, layer 2 (L2) scalability solutions like rollups introduce
fragmentation and hinder composability. Current cross-chain protocols,
including atomic swaps, bridges, and shared sequencers, lack the necessary
coordination mechanisms or rely on trust assumptions, and are thus not
sufficient to support full cross-rollup composability. This paper presents
$\mathsf{CRATE}$, a secure protocol for cross-rollup composability that ensures
all-or-nothing and serializable execution of cross-rollup transactions (CRTs).
$\mathsf{CRATE}$ supports rollups on distinct layer 1 (L1) chains, achieves
finality in 4 rounds on L1, and only relies on the underlying L1s and the
liveness of L2s. We introduce two formal models for CRTs, define atomicity
within them, and formally prove the security of $\mathsf{CRATE}$. We also
provide an implementation of $\mathsf{CRATE}$ along with a cross-rollup flash
loan application; our experiments demonstrate that $\mathsf{CRATE}$ is
practical in terms of gas usage on L1.",['cs.CR'],False,,,,Deep Active Speech Cancellation with Multi-Band Mamba Network,$\mathsf{CRATE}$: Cross-Rollup Atomic Transaction Execution
neg-d2-9,2025-01-08,,2501.04442," This paper explores an eclectic range of path-planning methodologies
engineered for rolling surfaces. Our focus is on the kinematic intricacies of
rolling contact systems, which are investigated through a motion planning lens.
Beyond summarizing the approaches to single-contact rotational surfaces, we
explore the challenging domain of spin-rolling multi-contact systems. Our work
proposes solutions for the higher-dimensional problem of multiple rotating
objects in contact. Venturing beyond kinematics, these methodologies find
application across a spectrum of domains, including rolling robots,
reconfigurable swarm robotics, micro/nano manipulation, and nonprehensile
manipulations. Through meticulously examining established planning strategies,
we unveil their practical implementations in various real-world scenarios, from
intricate dexterous manipulation tasks to the nimble manoeuvring of rolling
robots and even shape planning of multi-contact swarms of particles. This study
introduces the persistent challenges and unexplored frontiers of robotics,
intricately linked to both path planning and mechanism design. As we illuminate
existing solutions, we also set the stage for future breakthroughs in this
dynamic and rapidly evolving field by highlighting the critical importance of
addressing rolling contact problems.",['cs.RO'],2503.09765," Market fragmentation across multiple Automated Market Makers (AMMs) creates
inefficiencies such as costly arbitrage, unnecessarily high slippage and
delayed incorporation of new information into prices. These inefficiencies
raise trading costs, reduce liquidity provider profits, and degrade overall
market efficiency. To address these issues, we propose a modification of the
Constant Product Market Maker (CPMM) pricing mechanism, called the Global
Market Maker (GMM), which aggregates liquidity information from all AMMs to
mitigate these inefficiencies. Through theoretical and numerical analyses, we
demonstrate that the GMM enhances profits for both AMMs and traders by
eliminating arbitrage opportunities. Additionally, it reduces the profitability
of sandwich attacks and minimizes impermanent losses.","['econ.GN', 'q-fin.EC']",False,,,,"A Survey on Path Planning Problem of Rolling Contacts: Approaches,
  Applications and Future Challenges",Pooling Liquidity Pools in AMMs
neg-d2-10,2025-01-10,,2501.06079," In this work we deal with set-valued functions with values in the power set
of a separated locally convex space where a nontrivial pointed convex cone
induces a partial order relation. A set-valued function is evenly convex if its
epigraph is an evenly convex set, i.e., it is the intersection of an arbitrary
family of open half-spaces. In this paper we characterize evenly convex
set-valued functions as the pointwise supremum of its set-valued e-affine
minorants. Moreover, a suitable conjugation pattern will be developed for these
functions, as well as the counterpart of the biconjugation Fenchel-Moreau
theorem.",['math.OC'],2502.17842," Semantic communication marks a new paradigm shift from bit-wise data
transmission to semantic information delivery for the purpose of bandwidth
reduction. To more effectively carry out specialized downstream tasks at the
receiver end, it is crucial to define the most critical semantic message in the
data based on the task or goal-oriented features. In this work, we propose a
novel goal-oriented communication (GO-COM) framework, namely Goal-Oriented
Semantic Variational Autoencoder (GOS-VAE), by focusing on the extraction of
the semantics vital to the downstream tasks. Specifically, we adopt a Vector
Quantized Variational Autoencoder (VQ-VAE) to compress media data at the
transmitter side. Instead of targeting the pixel-wise image data
reconstruction, we measure the quality-of-service at the receiver end based on
a pre-defined task-incentivized model. Moreover, to capture the relevant
semantic features in the data reconstruction, imitation learning is adopted to
measure the data regeneration quality in terms of goal-oriented semantics. Our
experimental results demonstrate the power of imitation learning in
characterizing goal-oriented semantics and bandwidth efficiency of our proposed
GOS-VAE.","['cs.LG', 'cs.NI']",False,,,,Set-valued evenly convex functions: characterizations and c-conjugacy,"Task-Driven Semantic Quantization and Imitation Learning for
  Goal-Oriented Communications"
neg-d2-11,2025-03-12,,2503.09802," We study the task of list-decodable linear regression using batches. A batch
is called clean if it consists of i.i.d. samples from an unknown linear
regression distribution. For a parameter $\alpha \in (0, 1/2)$, an unknown
$\alpha$-fraction of the batches are clean and no assumptions are made on the
remaining ones. The goal is to output a small list of vectors at least one of
which is close to the true regressor vector in $\ell_2$-norm. [DJKS23] gave an
efficient algorithm, under natural distributional assumptions, with the
following guarantee. Assuming that the batch size $n$ satisfies $n \geq
\tilde{\Omega}(\alpha^{-1})$ and the number of batches is $m = \mathrm{poly}(d,
n, 1/\alpha)$, their algorithm runs in polynomial time and outputs a list of
$O(1/\alpha^2)$ vectors at least one of which is
$\tilde{O}(\alpha^{-1/2}/\sqrt{n})$ close to the target regressor. Here we
design a new polynomial time algorithm with significantly stronger guarantees
under the assumption that the low-degree moments of the covariates distribution
are Sum-of-Squares (SoS) certifiably bounded. Specifically, for any constant
$\delta>0$, as long as the batch size is $n \geq
\Omega_{\delta}(\alpha^{-\delta})$ and the degree-$\Theta(1/\delta)$ moments of
the covariates are SoS certifiably bounded, our algorithm uses $m =
\mathrm{poly}((dn)^{1/\delta}, 1/\alpha)$ batches, runs in polynomial-time, and
outputs an $O(1/\alpha)$-sized list of vectors one of which is
$O(\alpha^{-\delta/2}/\sqrt{n})$ close to the target. That is, our algorithm
achieves substantially smaller minimum batch size and final error, while
achieving the optimal list size. Our approach uses higher-order moment
information by carefully combining the SoS paradigm interleaved with an
iterative method and a novel list pruning procedure. In the process, we give an
SoS proof of the Marcinkiewicz-Zygmund inequality that may be of broader
applicability.","['cs.LG', 'cs.DS', 'math.ST', 'stat.ML', 'stat.TH']",2502.13793," Green hydrogen is likely to play a major role in decarbonising the aviation
industry. It is crucial to understand the effects of microstructure on hydrogen
redistribution, which may be implicated in the embrittlement of candidate fuel
system metals. We have developed a stochastic multiscale finite element
modelling framework that integrates micromechanical and hydrogen transport
models, such that the dominant microstructural effects can be efficiently
accounted for at millimetre length scales. Our results show that microstructure
has a significant effect on hydrogen localisation in elastically anisotropic
materials, which exhibit an interesting interplay between microstructure and
millimetre-scale hydrogen redistribution at various loading rates. Considering
316L stainless steel and nickel, a direct comparison of model predictions
against experimental hydrogen embrittlement data reveals that the reported
sensitivity to loading rate is strongly linked with rate-dependent grain scale
diffusion. These findings highlight the need to incorporate microstructural
characteristics in the design of hydrogen resistant materials.",['cond-mat.mtrl-sci'],False,,,,Batch List-Decodable Linear Regression via Higher Moments,"The link between Microstructural Heterogeneity, Diffusivity, and
  Hydrogen Embrittlement"
neg-d2-12,2025-03-11,,2503.08062," Orthogonal frequency division multiplexing (OFDM), which has been the
dominating waveform for contemporary wireless communications, is also regarded
as a competitive candidate for future integrated sensing and communication
(ISAC) systems. Existing works on OFDM-ISAC usually assume that the maximum
sensing range should be limited by the cyclic prefix (CP) length since
inter-symbol interference (ISI) and inter-carrier interference (ICI) should be
avoided. However, in this paper, we provide rigorous analysis to reveal that
the random data embedded in OFDM-ISAC signal can actually act as a free ``mask""
for ISI, which makes ISI/ICI random and hence greatly attenuated after radar
signal processing. The derived signal-to-interference-plus-noise ratio (SINR)
in the range profile demonstrates that the maximum sensing range of OFDM-ISAC
can greatly exceed the ISI-free distance that is limited by the CP length,
which is validated by simulation results. To further mitigate power degradation
for long-range targets, a novel sliding window sensing method is proposed,
which iteratively detects and cancels short-range targets before shifting the
detection window. The shifted detection window can effectively compensate the
power degradation due to insufficient CP length for long-range targets. Such
results provide valuable guidance for the CP length design in OFDM-ISAC
systems.","['eess.SP', 'cs.ET', 'cs.IT', 'math.IT']",2501.10182," In recent years, Semantic Communication (SemCom), which aims to achieve
efficient and reliable transmission of meaning between agents, has garnered
significant attention from both academia and industry. To ensure the security
of communication systems, encryption techniques are employed to safeguard
confidentiality and integrity. However, traditional cryptography-based
encryption algorithms encounter obstacles when applied to SemCom. Motivated by
this, this paper explores the feasibility of applying homomorphic encryption to
SemCom. Initially, we review the encryption algorithms utilized in mobile
communication systems and analyze the challenges associated with their
application to SemCom. Subsequently, we employ scale-invariant feature
transform to demonstrate that semantic features can be preserved in homomorphic
encrypted ciphertext. Based on this finding, we propose a task-oriented SemCom
scheme secured through homomorphic encryption. We design the privacy preserved
deep joint source-channel coding (JSCC) encoder and decoder, and the frequency
of key updates can be adjusted according to service requirements without
compromising transmission performance. Simulation results validate that, when
compared to plaintext images, the proposed scheme can achieve almost the same
classification accuracy performance when dealing with homomorphic ciphertext
images. Furthermore, we provide potential future research directions for
homomorphic encrypted SemCom.","['cs.CR', 'eess.SP']",False,,,,How Does CP Length Affect the Sensing Range for OFDM-ISAC?,Secure Semantic Communication With Homomorphic Encryption
neg-d2-13,2025-01-09,,2501.05055," Transport of charge carriers in mechanically soft semiconductors is mainly
limited by their interaction with slow intermolecular phonons. Carrier motion
exhibits a crossover from superdiffusive to subdiffusive, producing a distinct
low-frequency peak in the dynamical-mobility profile. These features can be
understood within approaches relying on the timescale separation between
carrier and phonon dynamics, such as the transient localization scenario (TLS).
However, recovering them from fully quantum dynamics has proved elusive. Using
the hierarchical equations of motion (HEOM)-based approach exposed in a
companion paper (arXiv:2501.05054), we study carrier transport in the
one-dimensional Peierls model near the adiabatic limit. We find that the TLS
approximates HEOM dynamics very well at higher temperatures and for stronger
interactions. Then, the transport is predominantly phonon-assisted, and turns
diffusive from the subdiffusive side well before one phonon period. In
contrast, the band current dominates at moderate temperatures and interactions,
relevant for transport in realistic materials. We then conclude that the
super-to-subdiffusive crossover is transient, so that the diffusive motion sets
in from the superdiffusive side after a couple of phonon periods. The
low-frequency dynamical mobility then additionally exhibits a dip at
approximately one phonon frequency, and the zero-frequency peak. Our findings
in this moderate regime show limitations of the TLS, and support the results of
the most advanced quantum-classical simulations. We expect that the qualitative
differences between HEOM and TLS dynamics would diminish for a more realistic
phonon density of states.","['cond-mat.str-el', 'cond-mat.mes-hall', 'physics.chem-ph', 'quant-ph']",2501.17356," Watermarking, the practice of embedding imperceptible information into media
such as images, videos, audio, and text, is essential for intellectual property
protection, content provenance and attribution. The growing complexity of
digital ecosystems necessitates watermarks for different uses to be embedded in
the same media. However, to detect and decode all watermarks, they need to
coexist well with one another. We perform the first study of coexistence of
deep image watermarking methods and, contrary to intuition, we find that
various open-source watermarks can coexist with only minor impacts on image
quality and decoding robustness. The coexistence of watermarks also opens the
avenue for ensembling watermarking methods. We show how ensembling can increase
the overall message capacity and enable new trade-offs between capacity,
accuracy, robustness and image quality, without needing to retrain the base
models.","['cs.CV', 'cs.AI', 'cs.CY']",False,,,,"Charge transport limited by nonlocal electron-phonon interaction. II.
  Numerically exact quantum dynamics in the slow-phonon regime",On the Coexistence and Ensembling of Watermarks
neg-d2-14,2025-01-06,,2501.02844," Text classification is a fundamental task in data mining, pivotal to various
applications such as tabular understanding and recommendation. Although neural
network-based models, such as CNN and BERT, have demonstrated remarkable
performance in text classification, their effectiveness heavily relies on
abundant labeled training data. This dependency makes these models less
effective in dynamic few-shot text classification, where labeled data is
scarce, and new target labels frequently appear based on application needs.
Recently, large language models (LLMs) have shown promise due to their
extensive pretraining and contextual understanding ability. Current approaches
provide LLMs with text inputs, candidate labels, and additional side
information (e.g., descriptions) to classify texts. However, their
effectiveness is hindered by the increased input size and the noise introduced
through side information processing. To address these limitations, we propose a
graph-based online retrieval-augmented generation framework, namely GORAG, for
dynamic few-shot text classification. Rather than treating each input
independently, GORAG constructs and maintains a weighted graph by extracting
side information across all target texts. In this graph, text keywords and
labels are represented as nodes, with edges indicating the correlations between
them. To model these correlations, GORAG employs an edge weighting mechanism to
prioritize the importance and reliability of extracted information and
dynamically retrieves relevant context using a minimum-cost spanning tree
tailored for each text input. Empirical evaluations demonstrate that GORAG
outperforms existing approaches by providing more comprehensive and precise
contextual information.","['cs.CL', 'cs.IR', 'cs.LG']",2502.17498," In the Large Language Model(LLM) reasoning scenario, people often estimate
state value via Monte Carlo sampling. Though Monte Carlo estimation is an
elegant method with less inductive bias, noise and errors are inevitably
introduced due to the limited sampling. To handle the problem, we inject the
structural prior into the value representation and transfer the scalar value
into the expectation of a pre-defined categorical distribution, representing
the noise and errors from a distribution perspective. Specifically, by treating
the result of Monte Carlo sampling as a single sample from the prior
ground-truth Binomial distribution, we quantify the sampling error as the
mismatch between posterior estimated distribution and ground-truth
distribution, which is thus optimized via distribution selection optimization.
We test the performance of value-based process verifiers on Best-of-N task and
Beam search task. Compared with the scalar value representation, we show that
reasonable structural prior injection induced by different objective functions
or optimization methods can improve the performance of value-based process
verifiers for about 1$\sim$2 points at little-to-no cost. We also show that
under different structural prior, the verifiers' performances vary greatly
despite having the same optimal solution, indicating the importance of
reasonable structural prior injection.","['cs.LG', 'cs.AI', 'cs.CL']",False,,,,"Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text
  Classification",Improving Value-based Process Verifier via Structural Prior Injection
neg-d2-15,2025-02-03,,2502.01922," We present a novel prompt design for Large Language Models (LLMs) tailored to
Asynchronous Time Series. Unlike regular time series, which assume values at
evenly spaced time points, asynchronous time series consist of timestamped
events occurring at irregular intervals, each described in natural language.
Our approach effectively utilizes the rich natural language of event
descriptions, allowing LLMs to benefit from their broad world knowledge for
reasoning across different domains and tasks. This allows us to extend the
scope of asynchronous time series analysis beyond forecasting to include tasks
like anomaly detection and data imputation. We further introduce Stochastic
Soft Prompting, a novel prompt-tuning mechanism that significantly improves
model performance, outperforming existing fine-tuning methods such as QLoRA.
Through extensive experiments on real world datasets, we demonstrate that our
approach achieves state-of-the-art performance across different tasks and
datasets.","['cs.LG', 'cs.AI']",2503.04889," We classify gapped and gapless phases of non-Hermitian band structures on
two-dimensional nonorientable parameter spaces. Such spaces arise in a wide
range of physical systems in the presence of non-symmorphic parameter space
symmetries. For gapped phases, we find that nonorientable spaces provide a
natural setting for exploring fundamental structural problems in braid group
theory, such as torsion and conjugacy. Gapless phases, which host exceptional
points (EPs), explicitly violate the fermion doubling theorem, even in two-band
models. We demonstrate that EPs traversing the nonorientable parameter space
exhibit non-Abelian charge inversion. These braided phases and their
transitions leave distinct signatures in the form of bulk Fermi arc
degeneracies, offering a concrete route toward experimental realization and
verification.","['cond-mat.mes-hall', 'math-ph', 'math.MP', 'physics.optics', 'quant-ph']",False,,,,LAST SToP For Modeling Asynchronous Time Series,Exceptional Topology on Nonorientable Manifolds
neg-d2-16,2025-02-25,,2502.17882," Scientific research is inherently global. However, the vast majority of
academic journals are published exclusively in English, creating barriers for
non-native-English-speaking researchers. In this study, we leverage large
language models (LLMs) to translate published scientific articles while
preserving their native JATS XML formatting, thereby developing a practical,
automated approach for implementation by academic journals. Using our approach,
we translate articles across multiple scientific disciplines into 28 languages.
To evaluate translation accuracy, we introduce a novel question-and-answer (QA)
benchmarking method, in which an LLM generates comprehension-based questions
from the original text and then answers them based on the translated text. Our
benchmark results show an average performance of 95.9%, showing that the key
scientific details are accurately conveyed. In a user study, we translate the
scientific papers of 15 researchers into their native languages, finding that
the authors consistently found the translations to accurately capture the
original information in their articles. Interestingly, a third of the authors
found many technical terms ""overtranslated,"" expressing a preference to keep
terminology more familiar in English untranslated. Finally, we demonstrate how
in-context learning techniques can be used to align translations with
domain-specific preferences such as mitigating overtranslation, highlighting
the adaptability and utility of LLM-driven scientific translation. The code and
translated articles are available at https://hankleid.github.io/ProjectMundo.","['cs.AI', 'cs.CL']",2502.12624," Conversational repair is a mechanism used to detect and resolve
miscommunication and misinformation problems when two or more agents interact.
One particular and underexplored form of repair in emergent communication is
the implicit repair mechanism, where the interlocutor purposely conveys the
desired information in such a way as to prevent misinformation from any other
interlocutor. This work explores how redundancy can modify the emergent
communication protocol to continue conveying the necessary information to
complete the underlying task, even with additional external environmental
pressures such as noise. We focus on extending the signaling game, called the
Lewis Game, by adding noise in the communication channel and inputs received by
the agents. Our analysis shows that agents add redundancy to the transmitted
messages as an outcome to prevent the negative impact of noise on the task
success. Additionally, we observe that the emerging communication protocol's
generalization capabilities remain equivalent to architectures employed in
simpler games that are entirely deterministic. Additionally, our method is the
only one suitable for producing robust communication protocols that can handle
cases with and without noise while maintaining increased generalization
performance levels.","['cs.LG', 'cs.MA']",False,,,,"Science Across Languages: Assessing LLM Multilingual Translation of
  Scientific Papers",Implicit Repair with Reinforcement Learning in Emergent Communication
neg-d2-17,2025-01-07,,2501.0387," Slot and intent detection (SID) is a classic natural language understanding
task. Despite this, research has only more recently begun focusing on SID for
dialectal and colloquial varieties. Many approaches for low-resource scenarios
have not yet been applied to dialectal SID data, or compared to each other on
the same datasets. We participate in the VarDial 2025 shared task on slot and
intent detection in Norwegian varieties, and compare multiple set-ups: varying
the training data (English, Norwegian, or dialectal Norwegian), injecting
character-level noise, training on auxiliary tasks, and applying Layer
Swapping, a technique in which layers of models fine-tuned on different
datasets are assembled into a model. We find noise injection to be beneficial
while the effects of auxiliary tasks are mixed. Though some experimentation was
required to successfully assemble a model from layers, it worked surprisingly
well; a combination of models trained on English and small amounts of dialectal
data produced the most robust slot predictions. Our best models achieve 97.6%
intent accuracy and 85.6% slot F1 in the shared task.",['cs.CL'],2503.12059," This work explores the geometrical/algebraic framework of Lie algebroids,
with a specific focus on the decoupling and coupling phenomena within the
bicocycle double cross product realization. The bicocycle double cross product
theory serves as the most general method for (de)coupling an algebroid into the
direct sum of two vector bundles in the presence of mutual
\textit{representations}, along with two twisted cocycle terms. Consequently,
it encompasses unified product, double cross product (matched pairs),
semi-direct product, and cocycle extension frameworks as particular instances.
In addition to algebraic constructions, the research extends to both reversible
and irreversible Lagrangian and Hamiltonian dynamics on (de)coupled Lie
algebroids, as well as Euler-Poincar\'{e}-(Herglotz) and Lie-Poisson-(Herglotz)
dynamics on (de)coupled Lie algebras, providing insights into potential
physical applications.","['math.DG', 'math-ph', 'math.MP']",False,,,,"Add Noise, Tasks, or Layers? MaiNLP at the VarDial 2025 Shared Task on
  Norwegian Dialectal Slot and Intent Detection","On Product Lie Algebroids, and Collective Motion"
neg-d2-18,2025-03-15,,2503.123," By imposing conditions upon the index of a self-centralizing subgroup of a
group, and upon the index of the center of the group, we are able to classify
the Chermak-Delgado lattice of the group. This is our main result. We use this
result to classify the Chermak-Delgado lattices of dicyclic groups and of
metabelian $p$-groups of maximal class.",['math.GR'],2501.02487," We report ACE++, an instruction-based diffusion framework that tackles
various image generation and editing tasks. Inspired by the input format for
the inpainting task proposed by FLUX.1-Fill-dev, we improve the Long-context
Condition Unit (LCU) introduced in ACE and extend this input paradigm to any
editing and generation tasks. To take full advantage of image generative
priors, we develop a two-stage training scheme to minimize the efforts of
finetuning powerful text-to-image diffusion models like FLUX.1-dev. In the
first stage, we pre-train the model using task data with the 0-ref tasks from
the text-to-image model. There are many models in the community based on the
post-training of text-to-image foundational models that meet this training
paradigm of the first stage. For example, FLUX.1-Fill-dev deals primarily with
painting tasks and can be used as an initialization to accelerate the training
process. In the second stage, we finetune the above model to support the
general instructions using all tasks defined in ACE. To promote the widespread
application of ACE++ in different scenarios, we provide a comprehensive set of
models that cover both full finetuning and lightweight finetuning, while
considering general applicability and applicability in vertical scenarios. The
qualitative analysis showcases the superiority of ACE++ in terms of generating
image quality and prompt following ability. Code and models will be available
on the project page: https://ali-vilab. github.io/ACE_plus_page/.",['cs.CV'],False,,,,On the Chermak-Delgado lattice of a finite group,"ACE++: Instruction-Based Image Creation and Editing via Context-Aware
  Content Filling"
neg-d2-19,2025-02-27,,2502.20008," Information retrieval is indispensable for today's Internet applications, yet
traditional semantic matching techniques often fall short in capturing the
fine-grained cross-modal interactions required for complex queries. Although
late-fusion two-tower architectures attempt to bridge this gap by independently
encoding visual and textual data before merging them at a high level, they
frequently overlook the subtle interplay essential for comprehensive
understanding. In this work, we rigorously assess these limitations and
introduce a unified retrieval framework that fuses visual and textual cues from
the ground up, enabling early cross-modal interactions for enhancing context
interpretation. Through a two-stage training process--comprising post-training
adaptation followed by instruction tuning--we adapt MLLMs as retrievers using a
simple one-tower architecture. Our approach outperforms conventional methods
across diverse retrieval scenarios, particularly when processing complex
multi-modal inputs. Notably, the joint fusion encoder yields greater
improvements on tasks that require modality fusion compared to those that do
not, underscoring the transformative potential of early integration strategies
and pointing toward a promising direction for contextually aware and effective
information retrieval.",['cs.CV'],2501.13681," Particle discretizations of partial differential equations are advantageous
for high-dimensional kinetic models in phase space due to their better
scalability than continuum approaches with respect to dimension. Complex
processes collectively referred to as \textit{particle noise} hamper long-time
simulations with particle methods. One approach to address this problem is
particle mesh adaptivity or remapping, known as \textit{particle resampling}.
This paper introduces a resampling method that projects particles to and from a
(finite element) function space. The method is simple; using standard sparse
linear algebra and finite element techniques, it can adapt to almost any set of
new particle locations and preserves all moments up to the order of polynomial
represented exactly by the continuum function space.
  This work is motivated by the Vlasov-Maxwell-Landau model of magnetized
plasmas with up to six dimensions, $3X$ in physical space and $3V$ in velocity
space, and is developed in the context of a $1X$ + $1V$ Vlasov-Poisson model of
Landau damping with logically regular particle and continuum phase space grids.
The evaluation codes are publicly available, along with the data and
reproducibility artifacts, and developed in the PETSc numerical library
(petsc.org).","['physics.plasm-ph', 'physics.comp-ph']",False,,,,"Joint Fusion and Encoding: Advancing Multimodal Retrieval from the
  Ground Up",A projection method for particle resampling
neg-d2-20,2025-01-05,,2501.0252," The rapid integration of Internet of Things (IoT) devices into enterprise
environments presents significant security challenges. Many IoT devices are
released to the market with minimal security measures, often harbouring an
average of 25 vulnerabilities per device. To enhance cybersecurity measures and
aid system administrators in managing IoT patches more effectively, we propose
an innovative framework that predicts the time it will take for a vulnerable
IoT device to receive a fix or patch. We developed a survival analysis model
based on the Accelerated Failure Time (AFT) approach, implemented using the
XGBoost ensemble regression model, to predict when vulnerable IoT devices will
receive fixes or patches. By constructing a comprehensive IoT vulnerabilities
database that combines public and private sources, we provide insights into
affected devices, vulnerability detection dates, published CVEs, patch release
dates, and associated Twitter activity trends. We conducted thorough
experiments evaluating different combinations of features, including
fundamental device and vulnerability data, National Vulnerability Database
(NVD) information such as CVE, CWE, and CVSS scores, transformed textual
descriptions into sentence vectors, and the frequency of Twitter trends related
to CVEs. Our experiments demonstrate that the proposed model accurately
predicts the time to fix for IoT vulnerabilities, with data from VulDB and NVD
proving particularly effective. Incorporating Twitter trend data offered
minimal additional benefit. This framework provides a practical tool for
organisations to anticipate vulnerability resolutions, improve IoT patch
management, and strengthen their cybersecurity posture against potential
threats.",['cs.CR'],2502.1334," Accurate visualization of double star astrometric data is essential for
effective analysis and interpretation. This article presents a Python toolkit
designed for astronomers who need to plot measurements from diverse sources --
historical, Gaia DR3, and the Las Cumbres Observatory (LCO) network -- while
maintaining a 1:1 aspect ratio to avoid visually distorting the data. The
toolkit is composed of three scripts: one that handles polar coordinates (P.A.,
separation), one for Cartesian (X, Y) coordinates, and another with the option
to include predicted theoretical points. This paper describes the purpose,
functionality, and usage of these scripts, including example figures,
installation guides, and licensing information.
  This toolkit has been used by the author and collaborators in published and
submitted research on double star systems, demonstrating its versatility for
both professional and student-driven investigations.","['astro-ph.IM', 'astro-ph.SR']",False,,,,"Predicting IoT Device Vulnerability Fix Times with Survival and Failure
  Time Models","A Python Toolkit for Plotting Double Star Observations with 1:1 Aspect
  Ratio"
neg-d2-21,2025-01-04,,2501.02196," Generative relation extraction (RE) commonly involves first reformulating RE
as a linguistic modeling problem easily tackled with pre-trained language
models (PLM) and then fine-tuning a PLM with supervised cross-entropy loss.
Although having achieved promising performance, existing approaches assume only
one deterministic relation between each pair of entities without considering
real scenarios where multiple relations may be valid, i.e., entity pair
overlap, causing their limited applications. To address this problem, we
introduce a novel contrastive prompt tuning method for RE, CPTuning, which
learns to associate a candidate relation between two in-context entities with a
probability mass above or below a threshold, corresponding to whether the
relation exists. Beyond learning schema, CPTuning also organizes RE as a
verbalized relation generation task and uses Trie-constrained decoding to
ensure a model generates valid relations. It adaptively picks out the generated
candidate relations with a high estimated likelihood in inference, thereby
achieving multi-relation extraction. We conduct extensive experiments on four
widely used datasets to validate our method. Results show that T5-large
fine-tuned with CPTuning significantly outperforms previous methods, regardless
of single or multiple relations extraction.","['cs.CL', 'cs.AI']",2501.14118," We develop a new methodology to select scenarios of DER adoption most
critical for distribution grids. Anticipating risks of future voltage and line
flow violations due to additional PV adopters is central for utility investment
planning but continues to rely on deterministic or ad hoc scenario selection.
We propose a highly efficient search framework based on multi-objective
Bayesian Optimization. We treat underlying grid stress metrics as
computationally expensive black-box functions, approximated via Gaussian
Process surrogates and design an acquisition function based on probability of
scenarios being Pareto-critical across a collection of line- and bus-based
violation objectives. Our approach provides a statistical guarantee and offers
an order of magnitude speed-up relative to a conservative exhaustive search.
Case studies on realistic feeders with 200-400 buses demonstrate the
effectiveness and accuracy of our approach.","['cs.LG', 'stat.AP', 'stat.ML']",False,,,,CPTuning: Contrastive Prompt Tuning for Generative Relation Extraction,"Selecting Critical Scenarios of DER Adoption in Distribution Grids Using
  Bayesian Optimization"
neg-d2-22,2025-03-23,,2503.18013," Large Vision-Language Models (LVLMs) typically follow a two-stage training
paradigm-pretraining and supervised fine-tuning. Recently, preference
optimization, derived from the language domain, has emerged as an effective
post-training reinforcement strategy to enhance capabilities of LVLMs. However,
constructing high-quality human-annotated preference data and developing robust
reward models to mimic these preferences are both costly and challenging.
Motivated by this observation, we propose Vision-R1, a novel vision-guided
R1-like reinforcement learning algorithm for LVLMs that rewards models with
definitive vision feedback. It only leverages curated instruction data,
eliminating the need for specialized reward models and handcrafted preference
datasets. We incorporate a criterion-driven reward function that further
integrates multi-dimensional feedback to evaluate model completions
comprehensively based on the vision task logic. Furthermore, we introduce a
progressive rule refinement strategy that dynamically adjusts the reward
criteria during training, enabling continuous model improvement and mitigating
reward hacking. Extensive experiments on both in-distribution and
out-of-distribution benchmarks demonstrate that fine-tuning the 7B LVLMs with
Vision-R1 achieves consistent performance gains, with even up to 50%
improvement and surpassing the state-of-the-art 10x size model.","['cs.CV', 'cs.AI']",2501.05015," Adversarial attacks are allegedly unnoticeable. Prior studies have designed
attack noticeability measures on graphs, primarily using statistical tests to
compare the topology of original and (possibly) attacked graphs. However, we
observe two critical limitations in the existing measures. First, because the
measures rely on simple rules, attackers can readily enhance their attacks to
bypass them, reducing their attack ""noticeability"" and, yet, maintaining their
attack performance. Second, because the measures naively leverage global
statistics, such as degree distributions, they may entirely overlook attacks
until severe perturbations occur, letting the attacks be almost ""totally
unnoticeable."" To address the limitations, we introduce HideNSeek, a learnable
measure for graph attack noticeability. First, to mitigate the bypass problem,
HideNSeek learns to distinguish the original and (potential) attack edges using
a learnable edge scorer (LEO), which scores each edge on its likelihood of
being an attack. Second, to mitigate the overlooking problem, HideNSeek
conducts imbalance-aware aggregation of all the edge scores to obtain the final
noticeability score. Using six real-world graphs, we empirically demonstrate
that HideNSeek effectively alleviates the observed limitations, and LEO (i.e.,
our learnable edge scorer) outperforms eleven competitors in distinguishing
attack edges under five different attack methods. For an additional
application, we show that LEO boost the performance of robust GNNs by removing
attack-like edges.","['cs.LG', 'cs.AI']",False,,,,"Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models
  via Vision-Guided Reinforcement Learning","On Measuring Unnoticeability of Graph Adversarial Attacks: Observations,
  New Measure, and Applications"
neg-d2-23,2025-01-12,,2501.06813," Subset selection is a fundamental problem in combinatorial optimization,
which has a wide range of applications such as influence maximization and
sparse regression. The goal is to select a subset of limited size from a ground
set in order to maximize a given objective function. However, the evaluation of
the objective function in real-world scenarios is often noisy. Previous
algorithms, including the greedy algorithm and multi-objective evolutionary
algorithms POSS and PONSS, either struggle in noisy environments or consume
excessive computational resources. In this paper, we focus on the noisy subset
selection problem with a cardinality constraint, where the evaluation of a
subset is noisy. We propose a novel approach based on Pareto Optimization with
Robust Evaluation for noisy subset selection (PORE), which maximizes a robust
evaluation function and minimizes the subset size simultaneously. PORE can
efficiently identify well-structured solutions and handle computational
resources, addressing the limitations observed in PONSS. Our experiments,
conducted on real-world datasets for influence maximization and sparse
regression, demonstrate that PORE significantly outperforms previous methods,
including the classical greedy algorithm, POSS, and PONSS. Further validation
through ablation studies confirms the effectiveness of our robust evaluation
function.",['cs.NE'],2501.02019," Modeling the associations between real world entities from their multivariate
cross-sectional profiles can provide cues into the concerted working of these
entities as a system. Several techniques have been proposed for deciphering
these associations including constraint-based Bayesian structure learning (BSL)
algorithms that model them as directed acyclic graphs. Benchmarking these
algorithms have typically focused on assessing the variation in performance
measures such as sensitivity as a function of the dimensionality represented by
the number of nodes in the DAG, and sample size. The present study elucidates
the importance of network topology in benchmarking exercises. More
specifically, it investigates variations in sensitivity across distinct network
topologies while constraining the nodes, edges, and sample-size to be
identical, eliminating these as potential confounders. Sensitivity of three
popular constraint-based BSL algorithms (Peter-Clarke, Grow-Shrink, Incremental
Association Markov Blanket) in learning the network structure from multivariate
cross-sectional profiles sampled from network models with sub-linear, linear,
and super-linear DAG topologies generated using preferential attachment is
investigated. Results across linear and nonlinear models revealed statistically
significant $(\alpha=0.05)$ decrease in sensitivity estimates from sub-linear
to super-linear topology constitutively across the three algorithms. These
results are demonstrated on networks with nodes $(N_{nods}=48,64)$, noise
strengths $(\sigma =3,6)$ and sample size $(N = 2^{10})$. The findings
elucidate the importance of accommodating the network topology in
constraint-based BSL benchmarking exercises.","['cs.LG', 'cs.AI', 'q-bio.MN']",False,,,,Pareto Optimization with Robust Evaluation for Noisy Subset Selection,"Benchmarking Constraint-Based Bayesian Structure Learning Algorithms:
  Role of Network Topology"
neg-d2-24,2025-03-01,,2503.00372," In cooperative multi-agent reinforcement learning (MARL), agents typically
form a single grand coalition based on credit assignment to tackle a composite
task, often resulting in suboptimal performance. This paper proposed a
nucleolus-based credit assignment grounded in cooperative game theory, enabling
the autonomous partitioning of agents into multiple small coalitions that can
effectively identify and complete subtasks within a larger composite task.
Specifically, our designed nucleolus Q-learning could assign fair credits to
each agent, and the nucleolus Q-operator provides theoretical guarantees with
interpretability for both learning convergence and the stability of the formed
small coalitions. Through experiments on Predator-Prey and StarCraft scenarios
across varying difficulty levels, our approach demonstrated the emergence of
multiple effective coalitions during MARL training, leading to faster learning
and superior performance in terms of win rate and cumulative rewards especially
in hard and super-hard environments, compared to four baseline methods. Our
nucleolus-based credit assignment showed the promise for complex composite
tasks requiring effective subteams of agents.","['cs.MA', 'cs.AI']",2503.06971," Europa, Jupiter's second Galilean moon, is believed to host a subsurface
ocean in contact with a rocky mantle, where hydrothermal activity may drive the
synthesis of organic molecules. Of these molecules, abiotic synthesis of
aromatic amino acids is unlikely, and their detection on Europa could be
considered a biosignature. Fluorescence from aromatic amino acids, with
characteristic emissions in the 200-400 nanometer wavelength range, can be
induced by a laser and may be detectable where ocean material has been
relatively recently emplaced on Europa's surface, as indicated by geologically
young terrain and surface features. However, surface bombardment by charged
particles from the Jovian magnetosphere and solar ultraviolet (UV) radiation
degrades organic molecules, limiting their longevity. We model radiolysis and
photolysis of aromatic amino acids embedded in ice, showing dependencies on
hemispheric and latitudinal patterns of charged particle bombardment and ice
phase. We demonstrate that biosignatures contained within freshly deposited ice
in high-latitude regions on the surface of Europa are detectable using
laser-induced UV fluorescence, even from an orbiting spacecraft.",['astro-ph.EP'],False,,,,"Nucleolus Credit Assignment for Effective Coalitions in Multi-agent
  Reinforcement Learning",Fluorescent Biomolecules Detectable in Near-Surface Ice on Europa
neg-d2-25,2025-02-24,,2502.1718," How much energy is required to unbind baryons from the cosmological
structures that originally bind them? This tutorial article explains why trying
to answer this question using just a halo model can be misleading. Instead, it
recommends parsing the universe into ``bound domains,'' which are the
gravitationally bound structures that ultimately become widely separated
islands as the universe evolves. It explains why a bound domain's potential
well was about as deep ~1 Gyr after the Big Bang as it is now, and it outlines
how future research might take advantage of a bound-domain approach to make
progress on some open questions about the baryon distributions in and around
galaxy groups and clusters.","['hep-ph', 'astro-ph.GA']",2501.04041," We implement a stabilized finite element method for steady
Darcy-Brinkman-Forchheimer model within the continuous Galerkin framework. The
nonlinear fluid model is first linearized using a standard \textit{Newton's
method. The sequence of linear problems is then discretized utilizing a stable
\textit{inf-sup} type continuous finite elements based on the
\textit{Taylor-Hood} pair to approximate the primary variables: velocity and
pressure}. Such a pair is known to be optimal for the approximation of the
isotropic Navier-Stokes equation. To overcome the well-known numerical
instability in the convection-dominated problems, the Grad-Div stabilization is
employed with an efficient \textit{augmented Lagrangian-type} penalty method.
We use the penalty term to develop the \textit{block Schur complement}
preconditioner, which is later coupled with a Krylov-space-based iterative
linear solver. In addition, the Kelly error estimator for the adaptive mesh
refinement is employed to achieve better numerical results with less
computational cost. Performance of the proposed algorithm is verified for a
classical benchmark problem. Particularly for the Forchheimer parameter, we
present some interesting flow patterns with the velocity components and their
streamlines along the mid-lines in the computational domain. The role of the
Forchheimer term is highlighted for different porous medium scenarios. This
study can offer an attractive setting for discretizing many multi-physics
problems along with the fluid flow having inertial effects in porous media.","['math.NA', 'cs.NA']",False,,,,Bound Domains,"A stabilized finite element method for steady Darcy-Brinkman-Forchheimer
  flow model with different viscous and inertial resistances in porous media"
neg-d2-26,2025-02-19,,2502.13793," Green hydrogen is likely to play a major role in decarbonising the aviation
industry. It is crucial to understand the effects of microstructure on hydrogen
redistribution, which may be implicated in the embrittlement of candidate fuel
system metals. We have developed a stochastic multiscale finite element
modelling framework that integrates micromechanical and hydrogen transport
models, such that the dominant microstructural effects can be efficiently
accounted for at millimetre length scales. Our results show that microstructure
has a significant effect on hydrogen localisation in elastically anisotropic
materials, which exhibit an interesting interplay between microstructure and
millimetre-scale hydrogen redistribution at various loading rates. Considering
316L stainless steel and nickel, a direct comparison of model predictions
against experimental hydrogen embrittlement data reveals that the reported
sensitivity to loading rate is strongly linked with rate-dependent grain scale
diffusion. These findings highlight the need to incorporate microstructural
characteristics in the design of hydrogen resistant materials.",['cond-mat.mtrl-sci'],2502.09003," Supervised fine-tuning is a standard method for adapting pre-trained large
language models (LLMs) to downstream tasks. Quantization has been recently
studied as a post-training technique for efficient LLM deployment. To obtain
quantized fine-tuned LLMs, conventional pipelines would first fine-tune the
pre-trained models, followed by post-training quantization. This often yields
suboptimal performance as it fails to leverage the synergy between fine-tuning
and quantization. To effectively realize low-bit quantization of weights,
activations, and KV caches in LLMs, we propose an algorithm named Rotated
Straight-Through-Estimator (RoSTE), which combines quantization-aware
supervised fine-tuning (QA-SFT) with an adaptive rotation strategy that
identifies an effective rotation configuration to reduce activation outliers.
We provide theoretical insights on RoSTE by analyzing its prediction error when
applied to an overparameterized least square quantized training problem. Our
findings reveal that the prediction error is directly proportional to the
quantization error of the converged weights, which can be effectively managed
through an optimized rotation configuration. Experiments on Pythia, Qwen and
Llama models of different sizes demonstrate the effectiveness of RoSTE.
Compared to existing post-SFT quantization baselines, our method consistently
achieves superior performances across various tasks and different LLM
architectures.","['cs.LG', 'cs.AI']",False,,,,"The link between Microstructural Heterogeneity, Diffusivity, and
  Hydrogen Embrittlement","RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach
  for Large Language Models"
neg-d2-27,2025-03-07,,2503.05488," Document Key Information Extraction (KIE) is a technology that transforms
valuable information in document images into structured data, and it has become
an essential function in industrial settings. However, current evaluation
metrics of this technology do not accurately reflect the critical attributes of
its industrial applications. In this paper, we present KIEval, a novel
application-centric evaluation metric for Document KIE models. Unlike prior
metrics, KIEval assesses Document KIE models not just on the extraction of
individual information (entity) but also of the structured information
(grouping). Evaluation of structured information provides assessment of
Document KIE models that are more reflective of extracting grouped information
from documents in industrial settings. Designed with industrial application in
mind, we believe that KIEval can become a standard evaluation metric for
developing or applying Document KIE models in practice. The code will be
publicly available.",['cs.CL'],2501.18993," Image Super-Resolution (ISR) has seen significant progress with the
introduction of remarkable generative models. However, challenges such as the
trade-off issues between fidelity and realism, as well as computational
complexity, have also posed limitations on their application. Building upon the
tremendous success of autoregressive models in the language domain, we propose
\textbf{VARSR}, a novel visual autoregressive modeling for ISR framework with
the form of next-scale prediction. To effectively integrate and preserve
semantic information in low-resolution images, we propose using prefix tokens
to incorporate the condition. Scale-aligned Rotary Positional Encodings are
introduced to capture spatial structures and the diffusion refiner is utilized
for modeling quantization residual loss to achieve pixel-level fidelity.
Image-based Classifier-free Guidance is proposed to guide the generation of
more realistic images. Furthermore, we collect large-scale data and design a
training process to obtain robust generative priors. Quantitative and
qualitative results show that VARSR is capable of generating high-fidelity and
high-realism images with more efficiency than diffusion-based methods. Our
codes will be released at https://github.com/qyp2000/VARSR.",['cs.CV'],False,,,,KIEval: Evaluation Metric for Document Key Information Extraction,Visual Autoregressive Modeling for Image Super-Resolution
neg-d2-28,2025-03-18,,2503.14064," The rapid advancement in AI-generated video synthesis has led to a growth
demand for standardized and effective evaluation metrics. Existing metrics lack
a unified framework for systematically categorizing methodologies, limiting a
holistic understanding of the evaluation landscape. Additionally, fragmented
implementations and the absence of standardized interfaces lead to redundant
processing overhead. Furthermore, many prior approaches are constrained by
dataset-specific dependencies, limiting their applicability across diverse
video domains. To address these challenges, we introduce AIGVE-Tool
(AI-Generated Video Evaluation Toolkit), a unified framework that provides a
structured and extensible evaluation pipeline for a comprehensive AI-generated
video evaluation. Organized within a novel five-category taxonomy, AIGVE-Tool
integrates multiple evaluation methodologies while allowing flexible
customization through a modular configuration system. Additionally, we propose
AIGVE-Bench, a large-scale benchmark dataset created with five SOTA video
generation models based on hand-crafted instructions and prompts. This dataset
systematically evaluates various video generation models across nine critical
quality dimensions. Extensive experiments demonstrate the effectiveness of
AIGVE-Tool in providing standardized and reliable evaluation results,
highlighting specific strengths and limitations of current models and
facilitating the advancements of next-generation AI-generated video techniques.",['cs.CV'],2502.17524," Bearings play an integral role in ensuring the reliability and efficiency of
rotating machinery - reducing friction and handling critical loads. Bearing
failures that constitute up to 90% of mechanical faults highlight the
imperative need for reliable condition monitoring and fault detection. This
study proposes a multimodal bearing fault classification approach that relies
on vibration and motor phase current signals within a one-dimensional
convolutional neural network (1D CNN) framework. The method fuses features from
multiple signals to enhance the accuracy of fault detection. Under the baseline
condition (1,500 rpm, 0.7 Nm load torque, and 1,000 N radial force), the model
reaches an accuracy of 96% with addition of L2 regularization. This represents
a notable improvement of 2% compared to the non-regularized model. In addition,
the model demonstrates robust performance across three distinct operating
conditions by employing transfer learning (TL) strategies. Among the tested TL
variants, the approach that preserves parameters up to the first max-pool layer
and then adjusts subsequent layers achieves the highest performance. While this
approach attains excellent accuracy across varied conditions, it requires more
computational time due to its greater number of trainable parameters. To
address resource constraints, less computationally intensive models offer
feasible trade-offs, albeit at a slight accuracy cost. Overall, this multimodal
1D CNN framework with late fusion and TL strategies lays a foundation for more
accurate, adaptable, and efficient bearing fault classification in industrial
environments with variable operating conditions.","['eess.SP', 'cs.AI', 'cs.LG']",False,,,,"AIGVE-Tool: AI-Generated Video Evaluation Toolkit with Multifaceted
  Benchmark","Multimodal Bearing Fault Classification Under Variable Conditions: A 1D
  CNN with Transfer Learning"
neg-d2-29,2025-01-02,,2501.01515," Motivated by deep learning regimes with multiple interacting yet distinct
model components, we introduce learning diagrams, graphical depictions of
training setups that capture parameterized learning as data rather than code. A
learning diagram compiles to a unique loss function on which component models
are trained. The result of training on this loss is a collection of models
whose predictions ``agree"" with one another. We show that a number of popular
learning setups such as few-shot multi-task learning, knowledge distillation,
and multi-modal learning can be depicted as learning diagrams. We further
implement learning diagrams in a library that allows users to build diagrams of
PyTorch and Flux.jl models. By implementing some classic machine learning use
cases, we demonstrate how learning diagrams allow practitioners to build
complicated models as compositions of smaller components, identify
relationships between workflows, and manipulate models during or after
training. Leveraging a category theoretic framework, we introduce a rigorous
semantics for learning diagrams that puts such operations on a firm
mathematical foundation.","['cs.LG', 'cs.AI', 'cs.PL', 'math.CT']",2502.10559," Accurate morphometric assessment of cartilage-such as thickness/volume-via
MRI is essential for monitoring knee osteoarthritis. Segmenting cartilage
remains challenging and dependent on extensive expert-annotated datasets, which
are heavily subjected to inter-reader variability. Recent advancements in
Visual Foundational Models (VFM), especially memory-based approaches, offer
opportunities for improving generalizability and robustness. This study
introduces a deep learning (DL) method for cartilage and meniscus segmentation
from 3D MRIs using interactive, memory-based VFMs. To improve spatial awareness
and convergence, we incorporated a Hybrid Shuffling Strategy (HSS) during
training and applied a segmentation mask propagation technique to enhance
annotation efficiency. We trained four AI models-a CNN-based 3D-VNet, two
automatic transformer-based models (SaMRI2D and SaMRI3D), and a
transformer-based promptable memory-based VFM (SAMRI-2)-on 3D knee MRIs from
270 patients using public and internal datasets and evaluated on 57 external
cases, including multi-radiologist annotations and different data acquisitions.
Model performance was assessed against reference standards using Dice Score
(DSC) and Intersection over Union (IoU), with additional morphometric
evaluations to further quantify segmentation accuracy. SAMRI-2 model, trained
with HSS, outperformed all other models, achieving an average DSC improvement
of 5 points, with a peak improvement of 12 points for tibial cartilage. It also
demonstrated the lowest cartilage thickness errors, reducing discrepancies by
up to threefold. Notably, SAMRI-2 maintained high performance with as few as
three user clicks per volume, reducing annotation effort while ensuring
anatomical precision. This memory-based VFM with spatial awareness offers a
novel approach for reliable AI-assisted knee MRI segmentation, advancing DL in
musculoskeletal imaging.","['eess.IV', 'cs.AI', 'cs.CV']",False,,,,"DiagrammaticLearning: A Graphical Language for Compositional Training
  Regimes","SAMRI-2: A Memory-based Model for Cartilage and Meniscus Segmentation in
  3D MRIs of the Knee Joint"
neg-d2-30,2025-02-17,,2502.11482," Continual learning (CL) is essential for Large Language Models (LLMs) to
adapt to evolving real-world demands, yet they are susceptible to catastrophic
forgetting (CF). While traditional CF solutions rely on expensive data
rehearsal, recent rehearsal-free methods employ model-based and
regularization-based strategies to address this issue. However, these
approaches often neglect the model's plasticity, which is crucial to achieving
optimal performance on newly learned tasks. Consequently, a key challenge in CL
is striking a balance between preserving plasticity and mitigating CF. To
tackle this challenge, we propose the $\textbf{D}$ecomposed
$\textbf{A}$ttention-based $\textbf{T}$ask $\textbf{A}$daptation (DATA), which
explicitly decouples and learns both task-specific and task-shared knowledge
using high-rank and low-rank task adapters (e.g., LoRAs). For new tasks, DATA
dynamically adjusts the weights of adapters of different ranks based on their
relevance and distinction from previous tasks, allowing the model to acquire
new task-specific skills while effectively retaining previously learned
knowledge. Specifically, we implement a decomposed component weighting strategy
comprising learnable components that collectively generate attention-based
weights, allowing the model to integrate and utilize diverse knowledge from
each DATA. Extensive experiments on three widely used benchmarks demonstrate
that our proposed method achieves state-of-the-art performance. Notably, our
approach significantly enhances model plasticity and mitigates CF by extending
learnable components and employing stochastic restoration during training
iterations.","['cs.LG', 'cs.AI', 'cs.CL']",2502.13005," Characterizing bipolaron binding, and understanding how it depends on
electron-phonon interaction, is crucial to unraveling the nature of emergent
many-body states in strongly interacting electron-phonon systems. So far, most
studies of bipolarons have been limited to the Holstein model, in which the
coupling constant is momentum-independent. The paradigmatic example of
momentum-dependent electron-phonon interaction comes from the system in which
phonon distortions modify electron hopping, the SSH model. Already individual
polarons in the SSH model are richer than the Holstein model counterparts, and
feature a phase transition into the finite momentum ground state with
increasing electron-phonon interaction. In this paper, we use a variational
approach to study bipolarons in the one-dimensional SSH model and discuss their
ground state, dispersion, and excitation spectra. We explore the full parameter
range of the system, including the adiabatic regime of slow phonons, which was
inaccessible to previous theoretical studies. In agreement with earlier
studies, we find that in the anti-adiabatic strongly interacting regime,
bipolarons have low effective mass. By contrast, in the adiabatic case, we find
that increasing electron-phonon interactions results in an exponential increase
of the bipolaron mass. We establish the existence of multiple branches of bound
excited states of SSH bipolaron and discuss the signatures of these bound
states in dynamics. We show that in the anti-adiabatic regime, response
functions obey a parity selection rule, that imposes symmetry constraints on
the excitation spectra and provides a clear signature of SSH bipolarons.",['cond-mat.str-el'],False,,,,"DATA: Decomposed Attention-based Task Adaptation for Rehearsal-Free
  Continual Learning",Bipolaron dynamics in the one-dimensional SSH model
neg-d2-31,2025-02-02,,2502.00868," We formulate and answer Gorenstein projective, flat, and injective analogues
of a classical projectivity question for group rings under some mild additional
assumptions. Although the original question, that was proposed by Jang-Hyun Jo
in 2007, was for integral group rings, in this article, we deal with more
general commutative base rings. We make use of the vast developments that have
happened in the field of Gorenstein homological algebra over group rings in
recent years, and we also improve and generalize several existing results from
this area along the way.",['math.RA'],2501.04483," The Ethereum blockchain has a \emph{gas system} that associates operations
with a cost in gas units. Two central concepts of this system are the \emph{gas
limit} assigned by the issuer of a transaction and the \emph{gas used} by a
transaction. The former is a budget that must not be exhausted before the
completion of the transaction execution; otherwise, the execution fails.
Therefore, it seems rather essential to determine the \emph{minimum gas limit}
that ensures the execution of a transaction will not abort due to the lack of
gas. Despite its practical relevance, this concept has not been properly
addressed. In the literature, gas used and minimum gas limit are conflated.
This paper proposes a precise notion of minimum gas limit and how it can differ
from gas used by a transaction; this is also demonstrated with a quantitative
study on real transactions of the Ethereum blockchain. Another significant
contribution is the proposition of a fairly precise estimator for each of the
two metrics. Again, the confusion between these concepts has led to the
creation of estimators only for the gas used by a transaction. We demonstrate
that the minimum gas limit for the state of the Ethereum blockchain (after the
block) $t$ can serve as a near-perfect estimation for the execution of the
transaction at block $t + \Delta$, where $\Delta \leq 11$; the same holds for
estimating gas used. These precise estimators can be very valuable in helping
the users predict the gas budget of transactions and developers in optimising
their smart contracts; over and underestimating gas used and minimum gas limit
can lead to a number of practical issues. Overall, this paper serves as an
important reference for blockchain developers and users as to how the gas
system really works.","['cs.SE', 'cs.CE', 'cs.DC', 'cs.ET', 'cs.NI']",False,,,,Gorenstein analogues of a projectivity criterion over group algebras,"Demystification and Near-perfect Estimation of Minimum Gas Limit and Gas
  Used for Ethereum Smart Contracts"
neg-d2-32,2025-02-19,,2502.13728," Dataset Distillation (DD) is a powerful technique for reducing large datasets
into compact, representative synthetic datasets, accelerating Machine Learning
training. However, traditional DD methods operate in a centralized manner,
which poses significant privacy threats and reduces its applicability. To
mitigate these risks, we propose a Secure Federated Data Distillation (SFDD)
framework to decentralize the distillation process while preserving privacy.
Unlike existing Federated Distillation techniques that focus on training global
models with distilled knowledge, our approach aims to produce a distilled
dataset without exposing local contributions. We leverage the
gradient-matching-based distillation method, adapting it for a distributed
setting where clients contribute to the distillation process without sharing
raw data. The central aggregator iteratively refines a synthetic dataset by
integrating client-side updates while ensuring data confidentiality. To make
our approach resilient to inference attacks perpetrated by the server that
could exploit gradient updates to reconstruct private data, we create an
optimized Local Differential Privacy approach, called LDPO-RLD. Furthermore, we
assess the framework's resilience against malicious clients executing backdoor
attacks (such as Doorping) and demonstrate robustness under the assumption of a
sufficient number of participating clients. Our experimental results
demonstrate the effectiveness of SFDD and that the proposed defense concretely
mitigates the identified vulnerabilities, with minimal impact on the
performance of the distilled dataset. By addressing the interplay between
privacy and federation in dataset distillation, this work advances the field of
privacy-preserving Machine Learning making our SFDD framework a viable solution
for sensitive data-sharing applications.","['cs.CR', 'cs.AI']",2502.07744," A realistic description of active particles should include interactions with
the medium, commonly a momentum-conserving simple fluid, in which they are
suspended. In this work, we consider a multi-species suspension of
self-diffusiophoretic Janus colloids interacting via chemical and hydrodynamic
fields. Through a systematic coarse-graining of the microscopic dynamics, we
calculate the multi-component contribution to the hydrodynamic stress tensor of
the incompressible Stokesian fluid in which the particles are immersed. For a
single species, we find that the strength of the stress produced by the
gradients of the number density field is determined by the particles'
self-propulsion and chemotactic alignment, and can be tuned to be either
contractile or extensile. For a multi-species system, we unveil how different
forms of activity modify the stress tensor, and how non-reciprocity in
hydrodynamic interactions emerges in an active binary mixture.","['cond-mat.stat-mech', 'cond-mat.soft']",False,,,,Secure Federated Data Distillation,"Hydrodynamic stresses in a multi-species suspension of active Janus
  colloids"
neg-d2-33,2025-01-31,,2501.19176," Full-Field Digital Mammography (FFDM) is the primary imaging modality for
routine breast cancer screening; however, its effectiveness is limited in
patients with dense breast tissue or fibrocystic conditions. Contrast-Enhanced
Spectral Mammography (CESM), a second-level imaging technique, offers enhanced
accuracy in tumor detection. Nonetheless, its application is restricted due to
higher radiation exposure, the use of contrast agents, and limited
accessibility. As a result, CESM is typically reserved for select cases,
leaving many patients to rely solely on FFDM despite the superior diagnostic
performance of CESM. While biopsy remains the gold standard for definitive
diagnosis, it is an invasive procedure that can cause discomfort for patients.
We introduce a multimodal, multi-view deep learning approach for virtual
biopsy, integrating FFDM and CESM modalities in craniocaudal and mediolateral
oblique views to classify lesions as malignant or benign. To address the
challenge of missing CESM data, we leverage generative artificial intelligence
to impute CESM images from FFDM scans. Experimental results demonstrate that
incorporating the CESM modality is crucial to enhance the performance of
virtual biopsy. When real CESM data is missing, synthetic CESM images proved
effective, outperforming the use of FFDM alone, particularly in multimodal
configurations that combine FFDM and CESM modalities. The proposed approach has
the potential to improve diagnostic workflows, providing clinicians with
augmented intelligence tools to improve diagnostic accuracy and patient care.
Additionally, as a contribution to the research community, we publicly release
the dataset used in our experiments, facilitating further advancements in this
field.","['eess.IV', 'cs.AI', 'cs.CV']",2501.02487," We report ACE++, an instruction-based diffusion framework that tackles
various image generation and editing tasks. Inspired by the input format for
the inpainting task proposed by FLUX.1-Fill-dev, we improve the Long-context
Condition Unit (LCU) introduced in ACE and extend this input paradigm to any
editing and generation tasks. To take full advantage of image generative
priors, we develop a two-stage training scheme to minimize the efforts of
finetuning powerful text-to-image diffusion models like FLUX.1-dev. In the
first stage, we pre-train the model using task data with the 0-ref tasks from
the text-to-image model. There are many models in the community based on the
post-training of text-to-image foundational models that meet this training
paradigm of the first stage. For example, FLUX.1-Fill-dev deals primarily with
painting tasks and can be used as an initialization to accelerate the training
process. In the second stage, we finetune the above model to support the
general instructions using all tasks defined in ACE. To promote the widespread
application of ACE++ in different scenarios, we provide a comprehensive set of
models that cover both full finetuning and lightweight finetuning, while
considering general applicability and applicability in vertical scenarios. The
qualitative analysis showcases the superiority of ACE++ in terms of generating
image quality and prompt following ability. Code and models will be available
on the project page: https://ali-vilab. github.io/ACE_plus_page/.",['cs.CV'],False,,,,"Augmented Intelligence for Multimodal Virtual Biopsy in Breast Cancer
  Using Generative Artificial Intelligence","ACE++: Instruction-Based Image Creation and Editing via Context-Aware
  Content Filling"
neg-d2-34,2025-02-07,,2502.0497," Deep learning survival models often outperform classical methods in
time-to-event predictions, particularly in personalized medicine, but their
""black box"" nature hinders broader adoption. We propose a framework for
gradient-based explanation methods tailored to survival neural networks,
extending their use beyond regression and classification. We analyze the
implications of their theoretical assumptions for time-dependent explanations
in the survival setting and propose effective visualizations incorporating the
temporal dimension. Experiments on synthetic data show that gradient-based
methods capture the magnitude and direction of local and global feature
effects, including time dependencies. We introduce GradSHAP(t), a
gradient-based counterpart to SurvSHAP(t), which outperforms SurvSHAP(t) and
SurvLIME in a computational speed vs. accuracy trade-off. Finally, we apply
these methods to medical data with multi-modal inputs, revealing relevant
tabular features and visual patterns, as well as their temporal dynamics.","['stat.ML', 'cs.LG']",2502.12624," Conversational repair is a mechanism used to detect and resolve
miscommunication and misinformation problems when two or more agents interact.
One particular and underexplored form of repair in emergent communication is
the implicit repair mechanism, where the interlocutor purposely conveys the
desired information in such a way as to prevent misinformation from any other
interlocutor. This work explores how redundancy can modify the emergent
communication protocol to continue conveying the necessary information to
complete the underlying task, even with additional external environmental
pressures such as noise. We focus on extending the signaling game, called the
Lewis Game, by adding noise in the communication channel and inputs received by
the agents. Our analysis shows that agents add redundancy to the transmitted
messages as an outcome to prevent the negative impact of noise on the task
success. Additionally, we observe that the emerging communication protocol's
generalization capabilities remain equivalent to architectures employed in
simpler games that are entirely deterministic. Additionally, our method is the
only one suitable for producing robust communication protocols that can handle
cases with and without noise while maintaining increased generalization
performance levels.","['cs.LG', 'cs.MA']",False,,,,Gradient-based Explanations for Deep Learning Survival Models,Implicit Repair with Reinforcement Learning in Emergent Communication
neg-d2-35,2025-01-20,,2501.11362," In this paper we give the exact order of the discrepancy of the digital van
der Corput--Kronecker sequences that are based on recent counterexamples of the
$X$-adic Littlewood conjecture in positive characteristics. Our result supports
once again the well-established conjecture in the theory of uniform
distribution which states that $D^*_N\leq c \frac{\log^s N}{N},\,c>0$ is the
best possible upper bound for the star discrepancy $D^*_N$ of a sequence in
$[0,1)^s$ or in other words for every sequence in $[0,1)^s$
$\limsup_{N\to\infty}ND^*_N/\log^s N>0$.",['math.NT'],2501.15299," We present a bioassay platform that leverages the lasing threshold
distribution in a microlaser ensemble (ME), consisting of hundreds of
individual microlasers, to measure analyte concentrations in solution. An ME is
formed by placing dye-doped microbeads in a micro Fabry-Perot cavity.
Microbeads are surface modified with biorecognition molecules to capture
analytes, while the quenchers resulting from the presence of the analytes on
the microbeads' surfaces increase the lasing thresholds of microlasers. Since
the number of analytes varies from one microbead (or microlaser) to another due
to the randomness in binding processes, a distribution of the analytes (and
hence the quenchers) in the ME is created, which in turn leads to a lasing
threshold distribution in the ME. Experimentally, multiple pumping energy
densities are used to probe the lasing threshold distribution. A theoretical
model is developed to map the lasing threshold distribution to analyte
distribution in the ME, and then to recover the analyte concentration in
solution. Using streptavidin and interleukin-6 as a model system, our platform
achieves a detection limit of 0.1 pg/mL and a dynamic range exceeding five
orders of magnitude, showing that the ME quenching method can provide a high
sensitivity with a superior dynamic range.","['physics.optics', 'physics.app-ph']",False,,,,"On the exact order of the discrepancy of low discrepancy digital van der
  Corput--Kronecker sequences","Sensitive bioassay with an ultra-large dynamic range via microlaser
  ensemble quenching"
neg-d2-36,2025-02-06,,2502.04228," We investigate the interrelations between the metric properties, order
properties and combinatorial properties of the set of balls in totally bounded
ultrametric space. In particular, the Gurvich-Vyalyi representation of finite,
ultrametric spaces by monotone rooted trees is generalized to the case of
totally bounded ultrametric spaces. It is shown that such spaces have isometric
completions if and only if their labeled representing trees are isomorphic. We
characterize up to isomorphism the representing trees of these spaces and, up
to order isomorphism, the posets of open balls in such spaces.",['math.GN'],2501.14118," We develop a new methodology to select scenarios of DER adoption most
critical for distribution grids. Anticipating risks of future voltage and line
flow violations due to additional PV adopters is central for utility investment
planning but continues to rely on deterministic or ad hoc scenario selection.
We propose a highly efficient search framework based on multi-objective
Bayesian Optimization. We treat underlying grid stress metrics as
computationally expensive black-box functions, approximated via Gaussian
Process surrogates and design an acquisition function based on probability of
scenarios being Pareto-critical across a collection of line- and bus-based
violation objectives. Our approach provides a statistical guarantee and offers
an order of magnitude speed-up relative to a conservative exhaustive search.
Case studies on realistic feeders with 200-400 buses demonstrate the
effectiveness and accuracy of our approach.","['cs.LG', 'stat.AP', 'stat.ML']",False,,,,Totally bounded ultrametric spaces and locally finite trees,"Selecting Critical Scenarios of DER Adoption in Distribution Grids Using
  Bayesian Optimization"
neg-d2-37,2025-03-23,,2503.18276," OpenStreetMap (OSM) has gained popularity recently in autonomous navigation
due to its public accessibility, lower maintenance costs, and broader
geographical coverage. However, existing methods often struggle with noisy OSM
data and incomplete sensor observations, leading to inaccuracies in trajectory
planning. These challenges are particularly evident in complex driving
scenarios, such as at intersections or facing occlusions. To address these
challenges, we propose a robust and explainable two-stage framework to learn an
Orientation Field (OrField) for robot navigation by integrating LiDAR scans and
OSM routes. In the first stage, we introduce the novel representation, OrField,
which can provide orientations for each grid on the map, reasoning jointly from
noisy LiDAR scans and OSM routes. To generate a robust OrField, we train a deep
neural network by encoding a versatile initial OrField and output an optimized
OrField. Based on OrField, we propose two trajectory planners for OSM-guided
robot navigation, called Field-RRT* and Field-Bezier, respectively, in the
second stage by improving the Rapidly Exploring Random Tree (RRT) algorithm and
Bezier curve to estimate the trajectories. Thanks to the robustness of OrField
which captures both global and local information, Field-RRT* and Field-Bezier
can generate accurate and reliable trajectories even in challenging conditions.
We validate our approach through experiments on the SemanticKITTI dataset and
our own campus dataset. The results demonstrate the effectiveness of our
method, achieving superior performance in complex and noisy conditions. Our
code for network training and real-world deployment is available at
https://github.com/IMRL/OriField.",['cs.RO'],2503.08062," Orthogonal frequency division multiplexing (OFDM), which has been the
dominating waveform for contemporary wireless communications, is also regarded
as a competitive candidate for future integrated sensing and communication
(ISAC) systems. Existing works on OFDM-ISAC usually assume that the maximum
sensing range should be limited by the cyclic prefix (CP) length since
inter-symbol interference (ISI) and inter-carrier interference (ICI) should be
avoided. However, in this paper, we provide rigorous analysis to reveal that
the random data embedded in OFDM-ISAC signal can actually act as a free ``mask""
for ISI, which makes ISI/ICI random and hence greatly attenuated after radar
signal processing. The derived signal-to-interference-plus-noise ratio (SINR)
in the range profile demonstrates that the maximum sensing range of OFDM-ISAC
can greatly exceed the ISI-free distance that is limited by the CP length,
which is validated by simulation results. To further mitigate power degradation
for long-range targets, a novel sliding window sensing method is proposed,
which iteratively detects and cancels short-range targets before shifting the
detection window. The shifted detection window can effectively compensate the
power degradation due to insufficient CP length for long-range targets. Such
results provide valuable guidance for the CP length design in OFDM-ISAC
systems.","['eess.SP', 'cs.ET', 'cs.IT', 'math.IT']",False,,,,Learning Orientation Field for OSM-Guided Autonomous Navigation,How Does CP Length Affect the Sensing Range for OFDM-ISAC?
neg-d2-38,2025-02-26,,2502.1968," Recent advances in Multi-Modal Large Language Models (M-LLMs) show promising
results in video reasoning. Popular Multi-Modal Large Language Model (M-LLM)
frameworks usually apply naive uniform sampling to reduce the number of video
frames that are fed into an M-LLM, particularly for long context videos.
However, it could lose crucial context in certain periods of a video, so that
the downstream M-LLM may not have sufficient visual information to answer a
question. To attack this pain point, we propose a light-weight M-LLM -based
frame selection method that adaptively select frames that are more relevant to
users' queries. In order to train the proposed frame selector, we introduce two
supervision signals (i) Spatial signal, where single frame importance score by
prompting a M-LLM; (ii) Temporal signal, in which multiple frames selection by
prompting Large Language Model (LLM) using the captions of all frame
candidates. The selected frames are then digested by a frozen downstream video
M-LLM for visual reasoning and question answering. Empirical results show that
the proposed M-LLM video frame selector improves the performances various
downstream video Large Language Model (video-LLM) across medium (ActivityNet,
NExT-QA) and long (EgoSchema, LongVideoBench) context video question answering
benchmarks.","['cs.CV', 'cs.AI']",2501.16412," Neutrino masses may have evolved dynamically throughout the history of the
Universe, potentially leading to a mass spectrum distinct from the normal or
inverted ordering observed today. While cosmological measurements constrain the
total energy density of neutrinos, they are not directly sensitive to a
dynamically changing mass ordering unless future surveys achieve exceptional
precision in detecting the distinct imprints of each mass eigenstate on
large-scale structures. In this work, we investigate the impact of a dynamic
neutrino mass spectrum on the diffuse supernova neutrino background (DSNB),
which is composed of neutrinos from all supernova explosions throughout cosmic
history and is on the verge of experimental detection. Since neutrino
oscillations are highly sensitive to the mass spectrum, we show that the
electron neutrino survival probability carries distinct signatures of the
evolving neutrino mass spectrum. Our results indicate that the resulting
modifications to the DSNB spectrum would exhibit unique energy-dependent
features. These features are distinguishable from the effects of significant
astrophysical uncertainties, providing a potential avenue for probing the
dynamic nature of neutrino masses.",['hep-ph'],False,,,,M-LLM Based Video Frame Selection for Efficient Video Understanding,"Dynamic Neutrino Mass Ordering and Its Imprint on the Diffuse Supernova
  Neutrino Background"
neg-d2-39,2025-01-21,,2501.12504," The unit group of the ring of integers of a number field, modulo torsion, is
a lattice via the logarithmic Minkowski embedding. We examine the shape of this
lattice, which we call the unit shape, within the family of prime degree $p$
number fields whose Galois closure has dihedral Galois group $D_p$ and a unique
real embedding. In the case $p = 5$, we prove that the unit shapes lie on a
single hypercycle on the modular surface (in this case, the modular surface is
the space of shapes of rank $2$ lattices). For general $p$, we show that the
unit shapes are contained in a finite union of translates of periodic torus
orbits in the space of shapes.",['math.NT'],2502.01694," A key paradigm to improve the reasoning capabilities of large language models
(LLMs) is to allocate more inference-time compute to search against a verifier
or reward model. This process can then be utilized to refine the pretrained
model or distill its reasoning patterns into more efficient models. In this
paper, we study inference-time compute by viewing chain-of-thought (CoT)
generation as a metastable Markov process: easy reasoning steps (e.g.,
algebraic manipulations) form densely connected clusters, while hard reasoning
steps (e.g., applying a relevant theorem) create sparse, low-probability edges
between clusters, leading to phase transitions at longer timescales. Under this
framework, we prove that implementing a search protocol that rewards sparse
edges improves CoT by decreasing the expected number of steps to reach
different clusters. In contrast, we establish a limit on reasoning capability
when the model is restricted to local information of the pretrained graph. We
also show that the information gained by search can be utilized to obtain a
better reasoning model: (1) the pretrained model can be directly finetuned to
favor sparse edges via policy gradient methods, and moreover (2) a compressed
metastable representation of the reasoning dynamics can be distilled into a
smaller, more efficient model.","['cs.AI', 'cs.LG', 'stat.ML']",False,,,,Shapes of unit lattices in $D_p$-number fields,"Metastable Dynamics of Chain-of-Thought Reasoning: Provable Benefits of
  Search, RL and Distillation"
neg-d2-40,2025-01-07,,2501.03763," Human fingers achieve exceptional dexterity and adaptability by combining
structures with varying stiffness levels, from soft tissues (low) to tendons
and cartilage (medium) to bones (high). This paper explores developing a
robotic finger with similar multi-stiffness characteristics. Specifically, we
propose using a lattice configuration, parameterized by voxel size and unit
cell geometry, to optimize and achieve fine-tuned stiffness properties with
high granularity. A significant advantage of this approach is the feasibility
of 3D printing the designs in a single process, eliminating the need for manual
assembly of elements with differing stiffness. Based on this method, we present
a novel, human-like finger, and a soft gripper. We integrate the latter with a
rigid manipulator and demonstrate the effectiveness in pick and place tasks.",['cs.RO'],2502.2122," Explainable AI (XAI) is concerned with how to make AI models more
understandable to people. To date these explanations have predominantly been
technocentric - mechanistic or productivity oriented. This paper introduces the
Explainable AI for the Arts (XAIxArts) manifesto to provoke new ways of
thinking about explainability and AI beyond technocentric discourses.
Manifestos offer a means to communicate ideas, amplify unheard voices, and
foster reflection on practice. To supports the co-creation and revision of the
XAIxArts manifesto we combine a World Caf\'e style discussion format with a
living manifesto to question four core themes: 1) Empowerment, Inclusion, and
Fairness; 2) Valuing Artistic Practice; 3) Hacking and Glitches; and 4)
Openness. Through our interactive living manifesto experience we invite
participants to actively engage in shaping this XIAxArts vision within the CHI
community and beyond.","['cs.HC', 'cs.AI']",False,,,,3D Printable Gradient Lattice Design for Multi-Stiffness Robotic Fingers,XAIxArts Manifesto: Explainable AI for the Arts
neg-d2-41,2025-01-03,,2501.01847," We derive the full system of canonical differential equations for all planar
two-loop massless six-particle master integrals, and determine analytically the
boundary conditions. This fully specifies the solutions, which may be written
as Chen iterated integrals. We argue that this is sufficient information for
evaluating any scattering amplitude in four dimensions up to the finite part.
We support this claim by reducing, for the most complicated integral
topologies, integrals with typical Yang-Mills numerators. We use the analytic
solutions to the differential equations, together with dihedral symmetry, to
provide the full solution space relevant for two-loop six-particle
computations. This includes the relevant function alphabet, as well as the
independent set of iterated integrals up to weight four. We also provide the
answer for all master integrals in terms of iterated integrals that can be
readily evaluated numerically. As a proof of concept, we provide a numerical
implementation that evaluates the integrals in part of the Euclidean region,
and validate this against numerical evaluation of the Feynman integrals. Our
result removes the bottleneck of Feynman integral evaluation, paving the way to
future analytic evaluations of six-particle scattering amplitudes.","['hep-ph', 'hep-th']",2502.04164," Distributed optimization has become the default training paradigm in modern
machine learning due to the growing scale of models and datasets. To mitigate
communication overhead, local updates are often applied before global
aggregation, resulting in a nested optimization approach with inner and outer
steps. However, heavy-tailed stochastic gradient noise remains a significant
challenge, particularly in attention-based models, hindering effective
training. In this work, we propose TailOPT, an efficient framework designed to
address heavy-tailed noise by leveraging adaptive optimization or clipping
techniques. We establish convergence guarantees for the TailOPT framework under
heavy-tailed noise with potentially unbounded gradient variance and local
updates. Among its variants, we highlight a memory and communication efficient
instantiation which we call $Bi^2Clip$, which performs coordinate-wise clipping
at both the inner and outer optimizers, achieving adaptive-like performance
(e.g., Adam) without the cost of maintaining or transmitting additional
gradient statistics. Empirically, TailOPT, including $Bi^2Clip$, demonstrates
superior performance on several language tasks and models, outperforming
state-of-the-art methods.",['cs.LG'],False,,,,"Complete function space for planar two-loop six-particle scattering
  amplitudes",Efficient Distributed Optimization under Heavy-Tailed Noise
neg-d2-42,2025-01-24,,2501.14873," Local-type primordial non-Gaussianity (PNG), predicted by many non-minimal
models of inflation, creates a scale-dependent contribution to the power
spectrum of large-scale structure (LSS) tracers. Its amplitude is characterized
by the product $b_\phi f_{\rm NL}^{\rm loc}$, where $b_\phi$ is an
astrophysical parameter dependent on the properties of the tracer. However,
$b_\phi$ exhibits significant secondary dependence on halo concentration and
other astrophysical properties, which may bias and weaken the constraints on
$f_{\rm NL}^{\rm loc}$. In this work, we demonstrate that incorporating
knowledge of the relation between Lagrangian bias parameters and $b_\phi$ can
significantly enhance PNG constraints. We employ the Hybrid Effective Field
Theory (HEFT) approach at the field-level and a linear regression model to seek
a connection between the bias parameters and $b_{\phi}$ for halo and galaxy
samples, constructed using the \textsc{AbacusSummit} simulation suite and
mimicking the luminous red galaxies (LRGs) and quasi-stellar objects (QSOs) of
the Dark Energy Spectroscopic Instrument (DESI) survey. For the fixed-mass halo
samples, our full bias model reduces the uncertainty by more than 70\%, with
most of that improvement coming from $b_\nabla$, which we find to be an
excellent proxy for concentration. For the galaxy samples, our model reduces
the uncertainty on $b_\phi$ by 80\% for all tracers. By adopting
Lagrangian-bias informed priors on the parameter $b_\phi$, future analyses can
thus constrain $f_{\rm NL}^{\rm loc}$ with less bias and smaller errors.","['astro-ph.CO', 'astro-ph.GA']",2502.06209," This paper introduces a cost-efficient active learning (AL) framework for
classification, featuring a novel query design called candidate set query.
Unlike traditional AL queries requiring the oracle to examine all possible
classes, our method narrows down the set of candidate classes likely to include
the ground-truth class, significantly reducing the search space and labeling
cost. Moreover, we leverage conformal prediction to dynamically generate small
yet reliable candidate sets, adapting to model enhancement over successive AL
rounds. To this end, we introduce an acquisition function designed to
prioritize data points that offer high information gain at lower cost.
Empirical evaluations on CIFAR-10, CIFAR-100, and ImageNet64x64 demonstrate the
effectiveness and scalability of our framework. Notably, it reduces labeling
cost by 42% on ImageNet64x64.","['cs.LG', 'cs.CV']",False,,,,"Refining local-type primordial non-Gaussianity: Sharpened $b_\phi$
  constraints through bias expansion",Enhancing Cost Efficiency in Active Learning with Candidate Set Query
neg-d2-43,2025-03-21,,2503.16955," We introduce a new open-source Python x-ray tracing code for modelling Bragg
diffracting mosaic crystal spectrometers: High Energy Applications Ray Tracer
(HEART). HEART's high modularity enables customizable workflows as well as
efficient development of novel features. Utilizing Numba's just-in-time (JIT)
compiler and the message-passing interface (MPI) allows running HEART in
parallel leading to excellent performance. HEART is intended to be used for
modelling x-ray spectra as they would be seen in experiments that measure x-ray
spectroscopy with a mosaic crystal spectrometer. This enables the user to, for
example, make predictions about what will be seen on a detector in experiment,
perform optimizations on the design of the spectrometer setup, or to study the
effect of the spectrometer on measured spectra. However, the code certainly has
further uses beyond these example use cases. Here, we discuss the physical
model used in the code, and explore a number of different mosaic distribution
functions, intrinsic rocking curves, and sampling approaches which are
available to the user. Finally, we demonstrate its strong predictive capability
in comparison to spectroscopic data collected at the European XFEL in Germany.",['physics.plasm-ph'],2502.10682," Effective deepfake detection tools are becoming increasingly essential over
the last few years due to the growing usage of deepfakes in unethical
practices. There exists a diverse range of deepfake generation techniques,
which makes it challenging to develop an accurate universal detection
mechanism. The 2025 Signal Processing Cup (DFWild-Cup competition) provided a
diverse dataset of deepfake images, which are generated from multiple deepfake
image generators, for training machine learning model(s) to emphasize the
generalization of deepfake detection. To this end, we proposed an
ensemble-based approach that employs three different neural network
architectures: a ResNet-34-based architecture, a data-efficient image
transformer (DeiT), and an XceptionNet with Wavelet Transform to capture both
local and global features of deepfakes. We visualize the specific regions that
these models focus for classification using Grad-CAM, and empirically
demonstrate the effectiveness of these models in grouping real and fake images
into cohesive clusters using t-SNE plots. Individually, the ResNet-34
architecture has achieved 88.9% accuracy, whereas the Xception network and the
DeiT architecture have achieved 87.76% and 89.32% accuracy, respectively. With
these networks, our weighted ensemble model achieves an excellent accuracy of
93.23% on the validation dataset of the SP Cup 2025 competition. Finally, the
confusion matrix and an Area Under the ROC curve of 97.44% further confirm the
stability of our proposed method.","['cs.CV', 'cs.LG', 'eess.IV']",False,,,,HEART: A New X-Ray Tracing Code for Mosaic Crystal Spectrometers,"Hybrid Deepfake Image Detection: A Comprehensive Dataset-Driven Approach
  Integrating Convolutional and Attention Mechanisms with Frequency Domain
  Features"
neg-d2-44,2025-03-15,,2503.12146," Let $\mathcal{D}_{n} \subset \mathbb{N}$ denote the set of the $\tau(n)$
divisors of $n$. We study the function $$ D_{n}(X,Y):=|\{d \in
\mathcal{D}_{n}:\ X \le d \le X+Y\}| $$ for $Y \le X$.",['math.NT'],2503.16955," We introduce a new open-source Python x-ray tracing code for modelling Bragg
diffracting mosaic crystal spectrometers: High Energy Applications Ray Tracer
(HEART). HEART's high modularity enables customizable workflows as well as
efficient development of novel features. Utilizing Numba's just-in-time (JIT)
compiler and the message-passing interface (MPI) allows running HEART in
parallel leading to excellent performance. HEART is intended to be used for
modelling x-ray spectra as they would be seen in experiments that measure x-ray
spectroscopy with a mosaic crystal spectrometer. This enables the user to, for
example, make predictions about what will be seen on a detector in experiment,
perform optimizations on the design of the spectrometer setup, or to study the
effect of the spectrometer on measured spectra. However, the code certainly has
further uses beyond these example use cases. Here, we discuss the physical
model used in the code, and explore a number of different mosaic distribution
functions, intrinsic rocking curves, and sampling approaches which are
available to the user. Finally, we demonstrate its strong predictive capability
in comparison to spectroscopic data collected at the European XFEL in Germany.",['physics.plasm-ph'],False,,,,Divisors of an Integer in a Short Interval,HEART: A New X-Ray Tracing Code for Mosaic Crystal Spectrometers
neg-d2-45,2025-01-07,,2501.04737," The proposed Habitable Worlds Observatory is intended to observe the
atmospheres of nearby terrestrial exoplanets with a resolution greater than
that of any previous instrument. While these observations present a substantial
opportunity for astrobiology, they also incur the risk of false positives and
false negatives. Here, we explore the use of systems science (in the form of
network theory and thermochemical kinetics) to mitigate these risks, and
briefly describe the technical specifications HWO would require in order to use
these methodologies.","['astro-ph.IM', 'astro-ph.EP']",2501.15719," Following a method introduced by Thomas-Vasquez and developed by Grundman, we
prove that many Hilbert modular threefolds of arithmetic genus $0$ and $1$ are
of general type, and that some are of nonnegative Kodaira dimension. The new
ingredient is a detailed study of the geometry and combinatorics of totally
positive integral elements $x$ of a fractional ideal $I$ in a totally real
number field $K$ with the property that $\mathop{\mathrm{tr}} xy <
\mathop{\mathrm{min}} I \mathop{\mathrm{tr}} y$ for some $y \gg 0 \in K$.","['math.NT', 'math.AG']",False,,,,"Network and Kinetics-based Biosignatures: Implications for the Putative
  Habitable World Observatory Design",The Kodaira dimension of Hilbert modular threefolds
neg-d2-46,2025-03-20,,2503.15988," Spectra of vibrational overtone and combination bands from vibrational ground
state of HCNH+ were measured using an action spectroscopy technique with active
background suppression in a cryogenic 22 pole radio frequency ion trap
apparatus. Spectroscopic constants for the upper vibrational levels of the
transitions were determined with vibrational band origins being 6846.77981(90)
$\text{cm}^{-1}$ ($2\nu_1$ , NH stretch), 6640.47624(43) $\text{cm}^{-1}$
($\nu_1 + \nu_2$), 6282.03578(63) $\text{cm}^{-1}$ ($2\nu_2$, CH stretch), and
6588.4894(20) $\text{cm}^{-1}$ ($\nu_2 + \nu_3 + 2\nu_5^0$). State of the art
ab initio VCI calculations up to 10000 $\text{cm}^{-1}$ complement the
experimental data.","['astro-ph.GA', 'physics.atom-ph', 'physics.plasm-ph']",2502.00563," Recent advancements in deep neural networks have significantly enhanced the
performance of semantic segmentation. However, class imbalance and instance
imbalance remain persistent challenges, where smaller instances and thin
boundaries are often overshadowed by larger structures. To address the
multiscale nature of segmented objects, various models have incorporated
mechanisms such as spatial attention and feature pyramid networks. Despite
these advancements, most loss functions are still primarily pixel-wise, while
regional and boundary-focused loss functions often incur high computational
costs or are restricted to small-scale regions. To address this limitation, we
propose complex wavelet mutual information (CWMI) loss, a novel loss function
that leverages mutual information from subband images decomposed by a complex
steerable pyramid. The complex steerable pyramid captures features across
multiple orientations and preserves structural similarity across scales.
Meanwhile, mutual information is well-suited for capturing high-dimensional
directional features and exhibits greater noise robustness. Extensive
experiments on diverse segmentation datasets demonstrate that CWMI loss
achieves significant improvements in both pixel-wise accuracy and topological
metrics compared to state-of-the-art methods, while introducing minimal
computational overhead. The code is available at
https://anonymous.4open.science/r/CWMI-83B7/","['cs.CV', 'eess.IV']",False,,,,Rovibrational Overtone and Combination Bands of the HCNH+ Ion,"Complex Wavelet Mutual Information Loss: A Multi-Scale Loss Function for
  Semantic Segmentation"
neg-d2-47,2025-02-12,,2502.08398," Extreme Ultraviolet (EUV) driven atmospheric escape is a key process in the
atmospheric evolution of close-in exoplanets. In many evolutionary models, the
energy-limited mass-loss rate with a constant efficiency (typically $\sim10\%$)
is assumed for calculating the mass-loss rate. However, hydrodynamic
simulations have demonstrated that this efficiency depends on various stellar
and planetary parameters. Comprehending the underlying physics of the
efficiency is essential for understanding planetary atmospheric evolution and
recent observations of the upper atmosphere of close-in exoplanets. We
introduce relevant temperatures and timescales derived from physical principles
to elucidate the mass-loss process. Our analytical mass-loss model is based on
phenomenology and consistent across a range of planetary parameters. We compare
our mass-loss efficiency and the radiation hydrodynamic simulations. The model
can predict efficiency in both energy-limited and recombination-limited
regimes. We further apply our model to exoplanets observed with hydrogen
absorption (Ly$\alpha$ and H$\alpha$). Our findings suggest that Ly$\alpha$
absorption is detectable in planets subjected to intermediate EUV flux; under
these conditions, the escaping outflow is insufficient in low-EUV environments,
while the photoionization timescale remains short in high-EUV ranges.
Conversely, H$\alpha$ absorption is detectable under high EUV flux conditions,
facilitated by the intense Ly$\alpha$ flux exciting hydrogen atoms. According
to our model, the non-detection of neutral hydrogen can be explained by a low
mass-loss rate and is not necessarily due to stellar wind confinement or the
absence of a hydrogen-dominated atmosphere in many cases. This model assists in
identifying future observational targets and explicates the unusual absorption
detection/non-detection patterns observed in recent studies.",['astro-ph.EP'],2503.09695," Recent work has shown that the triangular lattice spin-$1/2$ $J_1$-$J_2$
Heisenberg and XXZ antiferromagnets may exhibit coplanar or supersolid orders
proximate to a gapless Dirac spin liquid phase. We explore a distinct
$SU(2N)\!\!\times\!\!SU(M)$ fermionic parton approach, complemented by
variational Monte Carlo calculations for the spin-$1/2$ model, to study the
phase diagram of these models. We also calculate their dynamical spin response
including parton interactions within a random phase approximation, and discuss
implications for neutron scattering on triangular lattice cobaltates
Ba$_3$CoSb$_2$O$_9$, Na$_2$BaCo(PO$_4$)$_2$, K$_2$Co(SeO$_3$)$_2$,
Rb$_2$Co(SeO$_3$)$_2$, and Yb-based magnet KYbSe$_2$.",['cond-mat.str-el'],False,,,,"Physically motivated analytic model of energy efficiency for EUV-driven
  atmospheric escape of close-in exoplanets","Modified large-$N$ approach to gapless spin liquids, magnetic orders,
  and dynamics: Application to triangular lattice antiferromagnets"
neg-d2-48,2025-01-15,,2501.08834," Billions of dollars are transacted through smart contracts, making
vulnerabilities a major financial risk. One focus in the security arms race is
on profitable vulnerabilities that attackers can exploit. Fuzzing is a key
method for identifying these vulnerabilities. However, current solutions face
two main limitations: a lack of profit-centric techniques for expediting
detection, and insufficient automation in maximizing the profitability of
discovered vulnerabilities, leaving the analysis to human experts. To address
these gaps, we have developed VERITE, a profit-centric smart contract fuzzing
framework that not only effectively detects those profitable vulnerabilities
but also maximizes the exploited profits.
  VERITE has three key features: 1) DeFi action-based mutators for boosting the
exploration of transactions with different fund flows; 2) potentially
profitable candidates identification criteria, which checks whether the input
has caused abnormal fund flow properties during testing; 3) a gradient
descent-based profit maximization strategy for these identified candidates.
  VERITE is fully developed from scratch and evaluated on a dataset consisting
of 61 exploited real-world DeFi projects with an average of over 1.1 million
dollars loss. The results show that VERITE can automatically extract more than
18 million dollars in total and is significantly better than state-of-the-art
fuzzer ITYFUZZ in both detection (29/10) and exploitation (134 times more
profits gained on average). Remarkably, in 12 targets, it gains more profits
than real-world attacking exploits (1.01 to 11.45 times more). VERITE is also
applied by auditors in contract auditing, where 6 (5 high severity) zero-day
vulnerabilities are found with over $2,500 bounty rewards.","['cs.CR', 'cs.SE']",2503.15798," Mixture-of-Experts (MoE) activates only a subset of experts during inference,
allowing the model to maintain low inference FLOPs and latency even as the
parameter count scales up. However, since MoE dynamically selects the experts,
all the experts need to be loaded into VRAM. Their large parameter size still
limits deployment, and offloading, which load experts into VRAM only when
needed, significantly increase inference latency. To address this, we propose
Mixture of Lookup Experts (MoLE), a new MoE architecture that is efficient in
both communication and VRAM usage. In MoLE, the experts are Feed-Forward
Networks (FFNs) during training, taking the output of the embedding layer as
input. Before inference, these experts can be re-parameterized as lookup tables
(LUTs) that retrieves expert outputs based on input ids, and offloaded to
storage devices. Therefore, we do not need to perform expert computations
during inference. Instead, we directly retrieve the expert's computation
results based on input ids and load them into VRAM, and thus the resulting
communication overhead is negligible. Experiments show that, with the same
FLOPs and VRAM usage, MoLE achieves inference speeds comparable to dense models
and significantly faster than MoE with experts offloading, while maintaining
performance on par with MoE.","['cs.LG', 'cs.CL']",False,,,,Smart Contract Fuzzing Towards Profitable Vulnerabilities,Mixture of Lookup Experts
neg-d2-49,2025-02-07,,2502.0534," We determine forest lease value and optimal harvesting strategies under model
parameter uncertainty within stochastic bio-economic models that account for
catastrophe risk. Catastrophic events are modeled as a Poisson point process,
with a two-factor stochastic convenience yield model capturing the lumber spot
price dynamics. Using lumber futures and US wildfire data, we estimate model
parameters through a Kalman filter and maximum likelihood estimation and define
the model parameter uncertainty set as the 95% confidence region. We
numerically determine the forest lease value under catastrophe risk and
parameter uncertainty using reflected backward stochastic differential
equations (RBSDEs) and establish conservative and optimistic bounds for lease
values and optimal stopping boundaries for harvesting, facilitating Monte Carlo
simulations. Numerical experiments further explore how parameter uncertainty,
catastrophe intensity, and carbon sequestration impact the lease valuation and
harvesting decision. In particular, we explore the costs arising from this form
of uncertainty in the form of a reduction of the lease value. These are
implicit costs that can be attributed to climate risk and will be emphasized
through the importance of forestry resources in the energy transition process.
We conclude that in the presence of parameter uncertainty, it is better to lean
toward a conservative strategy reflecting, to some extent, the worst case than
being overly optimistic. Our results also highlight the critical role of
convenience yield in determining optimal harvesting strategies.","['q-fin.MF', 'econ.GN', 'q-fin.EC']",2502.20594," As the only compact elliptical close enough to resolve into individual stars,
the satellite dwarf galaxy M32 provides a unique opportunity for exploring the
origins of such rare galaxies. In this work, we combined archival and novel
Keck/DEIMOS spectroscopy from a southern extension of the Spectroscopic and
Photometric Landscape of Andromeda's Stellar Halo (SPLASH) survey with optical
HST imaging from the Panchromatic Hubble Andromeda Southern Treasury (PHAST)
survey. The resulting sample of 2525 giant stars is unprecedented both in size
and spatial coverage (0.9-15.5 arcmin, or out to $\sim$23$r_{\rm eff}$ and
$\sim$30$r_{\rm eff}$ along M32's major and minor axes) for probing the
resolved stellar outskirts of M32. Given the structurally complex region near
M32 on the sky, we modeled M32's line-of-sight kinematics simultaneously
alongside M31's rotating stellar disk and potential outliers corresponding to
M31's kinematically hot stellar halo and/or tidal substructure. Inside the
radius corresponding to the observed twisting of isophotal contours in M32's
surface brightness profile ($R_{\rm iso} \sim$ 5$r_{\rm eff}$ $\sim$ 150'' or
0.56 kpc), M32 exhibits a line-of-sight velocity distribution characteristic of
ordered rotation, transitioning to a distribution with heavier outliers beyond
this radius. Within $R_{\rm iso}$, the rotational direction is aligned with
M32's major-axis rotation, but shifts to become roughly aligned with M32's
minor axis beyond $R_{\rm iso}$. We interpret these kinematical signatures in
the stellar outskirts of M32 as evidence of tidal distortion from interactions
with M31 and discuss their implications for M32 formation pathways.",['astro-ph.GA'],False,,,,"Robust valuation and optimal harvesting of forestry resources in the
  presence of catastrophe risk and parameter uncertainty","Kinematical Modeling of the Resolved Stellar Outskirts of M32:
  Constraints on Tidal Stripping Scenarios"
neg-d2-50,2025-02-18,,2502.12882," We present efficient classical algorithms to approximate expectation values
and probability amplitudes in linear optical circuits. Specifically, our
classical algorithm efficiently approximates the expectation values of
observables in linear optical circuits for arbitrary product input states
within an additive error under a mild condition. This result suggests that
certain applications of linear optical circuits relying on expectation value
estimation, such as photonic variational algorithms, may face challenges in
achieving quantum advantage. In addition, the (marginal) output probabilities
of boson sampling with arbitrary product input states can be efficiently
approximated using our algorithm, implying that boson sampling can be
efficiently simulated if its output probability distribution is polynomially
sparse. Moreover, our method generalizes Gurvits's algorithm, originally
designed to approximate the permanent, to also approximate the hafnian of
complex symmetric matrices with an additive error. The algorithm also solves a
molecular vibronic spectra problem for arbitrary product input states as
precisely as boson samplers. Finally, our method extends to near-Clifford
circuits, enabling the classical approximation of their expectation values of
any observables and (marginal) output probabilities.",['quant-ph'],2503.16855," Hand gesture-based Sign Language Recognition (SLR) serves as a crucial
communication bridge between deaf and non-deaf individuals. Existing SLR
systems perform well for their cultural SL but may struggle with multi-cultural
sign languages (McSL). To address these challenges, this paper proposes a Stack
Spatial-Temporal Transformer Network that leverages multi-head attention
mechanisms to capture both spatial and temporal dependencies with hierarchical
features using the Stack Transfer concept. In the proceed, firstly, we applied
a fully connected layer to make a embedding vector which has high expressive
power from the original dataset, then fed them a stack newly proposed
transformer to achieve hierarchical features with short-range and long-range
dependency. The network architecture is composed of several stages that process
spatial and temporal relationships sequentially, ensuring effective feature
extraction. After making the fully connected layer, the embedding vector is
processed by the Spatial Multi-Head Attention Transformer, which captures
spatial dependencies between joints. In the next stage, the Temporal Multi-Head
Attention Transformer captures long-range temporal dependencies, and again, the
features are concatenated with the output using another skip connection. The
processed features are then passed to the Feed-Forward Network (FFN), which
refines the feature representations further. After the FFN, additional skip
connections are applied to combine the output with earlier layers, followed by
a final normalization layer to produce the final output feature tensor. This
process is repeated for 10 transformer blocks. The extensive experiment shows
that the JSL, KSL and ASL datasets achieved good performance accuracy. Our
approach demonstrates improved performance in McSL, and it will be consider as
a novel work in this domain.",['cs.CV'],False,,,,Efficient classical algorithms for linear optical circuits,"Stack Transformer Based Spatial-Temporal Attention Model for Dynamic
  Multi-Culture Sign Language Recognition"
neg-d2-51,2025-02-11,,2502.07547," In machine learning practice, early stopping has been widely used to
regularize models and can save computational costs by halting the training
process when the model's performance on a validation set stops improving.
However, conventional early stopping applies the same stopping criterion to all
instances without considering their individual learning statuses, which leads
to redundant computations on instances that are already well-learned. To
further improve the efficiency, we propose an Instance-dependent Early Stopping
(IES) method that adapts the early stopping mechanism from the entire training
set to the instance level, based on the core principle that once the model has
mastered an instance, the training on it should stop. IES considers an instance
as mastered if the second-order differences of its loss value remain within a
small range around zero. This offers a more consistent measure of an instance's
learning status compared with directly using the loss value, and thus allows
for a unified threshold to determine when an instance can be excluded from
further backpropagation. We show that excluding mastered instances from
backpropagation can increase the gradient norms, thereby accelerating the
decrease of the training loss and speeding up the training process. Extensive
experiments on benchmarks demonstrate that IES method can reduce
backpropagation instances by 10%-50% while maintaining or even slightly
improving the test accuracy and transfer learning performance of a model.",['cs.LG'],2502.03616," Noncooperative multi-agent systems often face coordination challenges due to
conflicting preferences among agents. In particular, agents acting in their own
self-interest can settle on different equilibria, leading to suboptimal
outcomes or even safety concerns. We propose an algorithm named trading auction
for consensus (TACo), a decentralized approach that enables noncooperative
agents to reach consensus without communicating directly or disclosing private
valuations. TACo facilitates coordination through a structured trading-based
auction, where agents iteratively select choices of interest and provably reach
an agreement within an a priori bounded number of steps. A series of numerical
experiments validate that the termination guarantees of TACo hold in practice,
and show that TACo achieves a median performance that minimizes the total cost
across all agents, while allocating resources significantly more fairly than
baseline approaches.","['cs.GT', 'cs.MA']",False,,,,Instance-dependent Early Stopping,Noncooperative Equilibrium Selection via a Trading-based Auction
neg-d2-52,2025-01-27,,2501.15786," The authors present their research on chocolate games with a pass-move.
Chocolate games are generalizations of Nim. In this work, we modify the
standard rules of the game to allow a one-time pass; that is, a pass-move may
be used at most once in the game, but not from a terminal position. Once the
pass has been used by either player, it is no longer available. In the case of
the classical three-pile nim with $x,y,z$ for the number of stones of each
pile, the previous player wins when the ``exclusive or'' of $x,y,z$ is $0$, and
its Grundy number is calculated as ``exclusive or'' of $x,y,z$. However, no
mathematical formula is known for the previous player's winning position when a
pass-move is allowed. In this study, we show a theorem to determine the Grundy
number of a position with a pass where the position can be considered as a
disjunctive sum of two positions which have a special property. By using the
theorem, we show closed formulas for positions which have Grundy numbers $0,
1,$ and $2$ for some chocolate games with a pass.",['math.CO'],2502.06974," We characterise when a rank $n$ generalised Baumslag-Solitar group is CAT(0)
and when it is biautomatic.",['math.GR'],False,,,,Chocolate Games with a Pass,Higher-rank GBS groups: non-positive curvature and biautomaticity
neg-d2-53,2025-02-16,,2502.11159," Recently, the BESIII Collaboration has observed three-body decays $D_s^+\to
\eta \omega\pi^+$, $D^+\to K^0_S\pi^+\omega$ and $D^0\to K^-\pi^+\omega$. In
this work, we investigate the contributions of the subprocesses $\rho^+\to
\omega\pi^+$ in these Cabibbo-favored decays $D \to h\omega\pi$, with $\rho^+=
\{\rho(770)^+, \rho(1450)^+, \rho(770)^+\&\rho(1450)^+\}$ and $h=\{ \eta,
K^0_S, K^-\}$, by introducing these subprocesses into the decay amplitudes of
the relevant decay processes via the vector form factor $F_{\omega\pi}$
measured in the related $\tau$ and $e^+e^-$ processes; we provide the first
theoretical predictions for the branching fractions of the quasi-two-body
decays $D_s^+\to\eta[\rho^+\to]\omega\pi^+$, $D^+\to
K^0_S[\rho^+\to]\omega\pi^+$ and $D^0\to K^-[\rho^+\to]\omega\pi^+$. Our
findings reveal that the contributions from the subprocess
$\rho(770)^+\to\omega\pi^+$ are significant in these observed three-body decays
$D_s^+\to\eta \omega\pi^+$, $D^+\to K^0_S \omega\pi^+$ and $D^0\to K^-
\omega\pi^+$, notwithstanding the contributions originating from the
Breit-Wigner tail effect of $\rho(770)^+$. The numerical results of this study
suggest that these Cabibbo-favored three-body decays are dominated by the
contributions come from the $P$-wave intermediate states $\rho(770)^+$,
$\rho(1450)^+$ and their interference effects.",['hep-ph'],2503.12674," The entanglement spectra for a subsystem in a spin chain fine-tuned to a
quantum-critical point contains signatures of the underlying quantum field
theory that governs its low-energy properties. For an open chain with given
boundary conditions described by a 2D conformal field theory~(CFT), the
entanglement spectrum of the left/right half of the system coincides with a
boundary CFT spectrum, where one of the boundary conditions arise due to the
`entanglement cut'. The latter has been argued to be conformal and has been
numerically found to be the `free' boundary condition for Ising, Potts and free
boson theories. For these models, the `free' boundary condition for the lattice
degree of freedom has a counterpart in the continuum theory. However, this is
not true in general. Here, this question is analyzed for the unitary minimal
models of 2D CFTs using the density matrix renormalization group technique. The
entanglement spectra are computed for blocks of spins in open chains of A-type
restricted solid-on-solid models with identical boundary conditions at the
ends. The imposed boundary conditions are realized exactly for these lattice
models due to their integrable nature. The obtained entanglement spectra are in
good agreement with certain boundary CFT spectra. The boundary condition for
the entanglement cut is found to be conformal and to coincide with the one with
the highest boundary entropy. This identification enables determination of the
exponents governing the unusual corrections to the entanglement entropy from
the CFT partition functions. These are compared with numerical results.","['quant-ph', 'hep-th', 'math-ph', 'math.MP']",False,,,,"Contributions of $\rho(770,1450)\to \omega\pi$ for the Cabibbo-favored
  $D \to h\omega\pi$ decays","Boundary Conditions for the Entanglement Cut in 2D Conformal Field
  Theories"
neg-d2-54,2025-02-20,,2502.14312," The aim of this paper is to extend Washburn's capillary rise equation by
incorporating a slip condition at the pipe wall. The governing equation is
derived using fundamental principles from continuum mechanics. A new scaling is
introduced, allowing for a systematic analysis of different flow regimes. We
prove the global-in-time existence and uniqueness of a bounded positive
solution to Washburn's equation that includes the slip parameter, as well as
the continuous dependence of the solution in the maximum norm on the initial
data. Thus, the initial-value problem for Washburn's equation is shown to be
well-posed in the sense of Hadamard. Additionally, we show that the unique
equilibrium solution may be reached either monotonically or in an oscillatory
fashion, similarly to the no-slip case. Finally, we determine the basin of
attraction for the system, ensuring that the equilibrium state will be reached
from the initial data we impose. These results hold for any positive value of
the nondimensional slip parameter in the model, and for all values of the ratio
$h_0/h_e$ in the range $[0,3/2]$, where $h_0$ is the initial height of the
fluid column and $h_e$ is its equilibrium height.","['math.AP', 'math-ph', 'math.MP']",2502.1598," Text-to-SQL models, which parse natural language (NL) questions to executable
SQL queries, are increasingly adopted in real-world applications. However,
deploying such models in the real world often requires adapting them to the
highly specialized database schemas used in specific applications. We find that
existing text-to-SQL models experience significant performance drops when
applied to new schemas, primarily due to the lack of domain-specific data for
fine-tuning. This data scarcity also limits the ability to effectively evaluate
model performance in new domains. Continuously obtaining high-quality
text-to-SQL data for evolving schemas is prohibitively expensive in real-world
scenarios. To bridge this gap, we propose SQLsynth, a human-in-the-loop
text-to-SQL data annotation system. SQLsynth streamlines the creation of
high-quality text-to-SQL datasets through human-LLM collaboration in a
structured workflow. A within-subjects user study comparing SQLsynth with
manual annotation and ChatGPT shows that SQLsynth significantly accelerates
text-to-SQL data annotation, reduces cognitive load, and produces datasets that
are more accurate, natural, and diverse. Our code is available at
https://github.com/adobe/nl_sql_analyzer.","['cs.HC', 'cs.AI', 'cs.DB']",False,,,,"Modelling Capillary Rise with a Slip Boundary Condition: Well-posedness
  and Long-time Dynamics of Solutions to Washburn's Equation","Text-to-SQL Domain Adaptation via Human-LLM Collaborative Data
  Annotation"
neg-d2-55,2025-01-31,,2501.19238," Attosecond spectroscopy of materials has provided invaluable insight into
light-driven coherent electron dynamics. However, attosecond spectroscopies
have so far been focused on weakly-correlated materials. As a result, the
behavior of strongly-correlated systems is largely unknown at sub- to
few-femtosecond timescales, even though it is typically the realm at which
electron-electron interactions operate. Here we conduct attosecond-resolved
experiments on the correlated insulator nickel oxide, and compare its response
to a common band insulator, revealing fundamentally different behaviors. The
results, together with state-of-the art time-dependent $\textit{ab initio}$
calculations, show that the correlated system response is governed by a
laser-driven quench of electron correlations. The evolution of the on-site
electronic interaction is measured here at its natural timescale, marking the
first direct measurement of Hubbard $U$ renormalization in NiO. It is found to
take place within a few femtoseconds, after which structural changes slowly
start to take place. The resulting picture sheds light on the entire
light-induced response of a strongly-correlated system, from attosecond to
long-lived effects.","['cond-mat.str-el', 'cond-mat.mtrl-sci']",2501.19259," The integration of human-intuitive interactions into autonomous systems has
been limited. Traditional Natural Language Processing (NLP) systems struggle
with context and intent understanding, severely restricting human-robot
interaction. Recent advancements in Large Language Models (LLMs) have
transformed this dynamic, allowing for intuitive and high-level communication
through speech and text, and bridging the gap between human commands and
robotic actions. Additionally, autonomous navigation has emerged as a central
focus in robotics research, with artificial intelligence (AI) increasingly
being leveraged to enhance these systems. However, existing AI-based navigation
algorithms face significant challenges in latency-critical tasks where rapid
decision-making is critical. Traditional frame-based vision systems, while
effective for high-level decision-making, suffer from high energy consumption
and latency, limiting their applicability in real-time scenarios. Neuromorphic
vision systems, combining event-based cameras and spiking neural networks
(SNNs), offer a promising alternative by enabling energy-efficient, low-latency
navigation. Despite their potential, real-world implementations of these
systems, particularly on physical platforms such as drones, remain scarce. In
this work, we present Neuro-LIFT, a real-time neuromorphic navigation framework
implemented on a Parrot Bebop2 quadrotor. Leveraging an LLM for natural
language processing, Neuro-LIFT translates human speech into high-level
planning commands which are then autonomously executed using event-based
neuromorphic vision and physics-driven planning. Our framework demonstrates its
capabilities in navigating in a dynamic environment, avoiding obstacles, and
adapting to human instructions in real-time.","['cs.RO', 'cs.CV', 'cs.LG', 'cs.NE', 'cs.SY', 'eess.SY']",False,,,,"Correlations drive the attosecond response of strongly-correlated
  insulators","Neuro-LIFT: A Neuromorphic, LLM-based Interactive Framework for
  Autonomous Drone FlighT at the Edge"
neg-d2-56,2025-01-21,,2502.03476," It is common belief that the equilibrium contact angle, corresponding to the
minimum system energy state, lies between advancing and receding contact
angles. Here, we derive advancing and receding contact angles considering the
micro contacting processes on ideal rough 2D surfaces. Equilibrium contact
angles obtained via energy minimization can be smaller than the receding
contact angle and reach 0 degrees, at which hysteresis diminishes on the
super-hydrophilic surface. Gibbs free energy analyses, numerical simulations
and physical experiments all confirm these new findings.",['cond-mat.soft'],2502.03531," We study (i) Janus deformations and (ii) non-local double trace deformations
of a pair of CFTs, as two different ways to construct CFT duals of traversable
AdS wormholes. First, we construct a simple model of traversable wormholes by
gluing two Poincar\'e AdS geometries and BTZ black holes and compute
holographic two point functions and (pseudo) entanglement entropy. We point out
that a Janus gravity solution describes a traversable wormhole when the
deformation parameter takes imaginary values. On the other hand, we show that
double trace deformations between two decoupled CFTs can reproduce two point
functions of traversable AdS wormholes. By considering the case where the
double trace deformation is given by a non-local $T\overline{T}$ deformation,
we analyze the dual gravity which implies emergence of wormholes. We present
toy model of these deformed CFTs by using free scalars and obtain qualitative
behaviors expected for them. We argue that the crucial difference between the
two constructions is that a global time slice of wormhole is described by a
pure state for Janus deformations, while it is a mixed state for the double
trace deformations.","['hep-th', 'gr-qc', 'quant-ph']",False,,,,Achieve equilibrium outside the contact angle hysteresis,Traversable AdS Wormhole via Non-local Double Trace or Janus Deformation
neg-d2-57,2025-03-18,,2503.14665," This paper introduces a novel approach to uncertainty quantification for
radiance fields by leveraging higher-order moments of the rendering equation.
Uncertainty quantification is crucial for downstream tasks including view
planning and scene understanding, where safety and robustness are paramount.
However, the high dimensionality and complexity of radiance fields pose
significant challenges for uncertainty quantification, limiting the use of
these uncertainty quantification methods in high-speed decision-making. We
demonstrate that the probabilistic nature of the rendering process enables
efficient and differentiable computation of higher-order moments for radiance
field outputs, including color, depth, and semantic predictions. Our method
outperforms existing radiance field uncertainty estimation techniques while
offering a more direct, computationally efficient, and differentiable
formulation without the need for post-processing. Beyond uncertainty
quantification, we also illustrate the utility of our approach in downstream
applications such as next-best-view (NBV) selection and active ray sampling for
neural radiance field training. Extensive experiments on synthetic and
real-world scenes confirm the efficacy of our approach, which achieves
state-of-the-art performance while maintaining simplicity.","['cs.CV', 'cs.RO']",2501.10346," We prove that Hopf manifolds admit holomorphic $(G,X)$-structures, extending
to any dimension a result of McKay and Pokrovskiy. For this, we revisit
Guysinsky-Katok's group of invertible sub-resonant polynomials, and
Bertheloot's approach of Poincar\'e-Dulac normal form theory.",['math.CV'],False,,,,"These Magic Moments: Differentiable Uncertainty Quantification of
  Radiance Field Models",Normal forms and geometric structures on Hopf manifolds
neg-d2-58,2025-03-05,,2503.03974," Voter registration systems are a critical - and surprisingly understudied -
element of most high-stakes elections. Despite a history of targeting by
adversaries, relatively little academic work has been done to increase
visibility into how voter registration systems keep voters' data secure,
accurate, and up to date. Enhancing transparency and verifiability could help
election officials and the public detect and mitigate risks to this essential
component of electoral processes worldwide.
  This work introduces cryptographic verifiability for voter registration
systems. Based on consultation with diverse expert stakeholders that support
elections systems, we precisely define the requirements for cryptographic
verifiability in voter registration and systematize the practical challenges
that must be overcome for near-term deployment.
  We then introduce VRLog, the first system to bring strong verifiability to
voter registration. VRLog enables election officials to provide a transparent
log that (1) allows voters to verify that their registration data has not been
tampered with and (2) allows the public to monitor update patterns and database
consistency. We also introduce VRLog$^x$, an enhancement to VRLog that offers
cryptographic privacy to voter deduplication between jurisdictions - a common
maintenance task currently performed in plaintext or using trusted third
parties. Our designs rely on standard, efficient cryptographic primitives, and
are backward compatible with existing voter registration systems. Finally, we
provide an open-source implementation of VRLog and benchmarks to demonstrate
that the system is practical - capable of running on low-cost commodity
hardware and scaling to support databases the size of the largest U.S. state
voter registration systems.",['cs.CR'],2501.05241," Late gadolinium enhancement MRI (LGE MRI) is the gold standard for the
detection of myocardial scars for post myocardial infarction (MI). LGE MRI
requires the injection of a contrast agent, which carries potential side
effects and increases scanning time and patient discomfort. To address these
issues, we propose a novel framework that combines cardiac motion observed in
cine MRI with image texture information to segment the myocardium and scar
tissue in the left ventricle. Cardiac motion tracking can be formulated as a
full cardiac image cycle registration problem, which can be solved via deep
neural networks. Experimental results prove that the proposed method can
achieve scar segmentation based on non-contrasted cine images with comparable
accuracy to LGE MRI. This demonstrates its potential as an alternative to
contrast-enhanced techniques for scar detection.","['eess.IV', 'cs.CV']",False,,,,Cryptographic Verifiability for Voter Registration Systems,"Contrast-Free Myocardial Scar Segmentation in Cine MRI using Motion and
  Texture Fusion"
neg-d2-59,2025-02-10,,2502.07036," Generative AI (Gen AI) with large language models (LLMs) are being widely
adopted across the industry, academia and government. Cybersecurity is one of
the key sectors where LLMs can be and/or are already being used. There are a
number of problems that inhibit the adoption of trustworthy Gen AI and LLMs in
cybersecurity and such other critical areas. One of the key challenge to the
trustworthiness and reliability of LLMs is: how consistent an LLM is in its
responses? In this paper, we have analyzed and developed a formal definition of
consistency of responses of LLMs. We have formally defined what is consistency
of responses and then develop a framework for consistency evaluation. The paper
proposes two approaches to validate consistency: self-validation, and
validation across multiple LLMs. We have carried out extensive experiments for
several LLMs such as GPT4oMini, GPT3.5, Gemini, Cohere, and Llama3, on a
security benchmark consisting of several cybersecurity questions: informational
and situational. Our experiments corroborate the fact that even though these
LLMs are being considered and/or already being used for several cybersecurity
tasks today, they are often inconsistent in their responses, and thus are
untrustworthy and unreliable for cybersecurity.","['cs.CR', 'cs.AI', 'cs.LG']",2501.08234," This paper addresses a critical challenge in the high-speed passenger railway
industry: designing effective dynamic pricing strategies in the context of
competing and cooperating operators. To address this, a multi-agent
reinforcement learning (MARL) framework based on a non-zero-sum Markov game is
proposed, incorporating random utility models to capture passenger decision
making. Unlike prior studies in areas such as energy, airlines, and mobile
networks, dynamic pricing for railway systems using deep reinforcement learning
has received limited attention. A key contribution of this paper is a
parametrisable and versatile reinforcement learning simulator designed to model
a variety of railway network configurations and demand patterns while enabling
realistic, microscopic modelling of user behaviour, called RailPricing-RL. This
environment supports the proposed MARL framework, which models heterogeneous
agents competing to maximise individual profits while fostering cooperative
behaviour to synchronise connecting services. Experimental results validate the
framework, demonstrating how user preferences affect MARL performance and how
pricing policies influence passenger choices, utility, and overall system
dynamics. This study provides a foundation for advancing dynamic pricing
strategies in railway systems, aligning profitability with system-wide
efficiency, and supporting future research on optimising pricing policies.","['cs.LG', 'cs.AI', 'cs.MA']",False,,,,Automated Consistency Analysis of LLMs,"Dynamic Pricing in High-Speed Railways Using Multi-Agent Reinforcement
  Learning"
neg-d2-60,2025-02-12,,2502.08397," Clustering is a fundamental technique in data analysis and machine learning,
used to group similar data points together. Among various clustering methods,
the Minimum Sum-of-Squares Clustering (MSSC) is one of the most widely used.
MSSC aims to minimize the total squared Euclidean distance between data points
and their corresponding cluster centroids. Due to the unsupervised nature of
clustering, achieving global optimality is crucial, yet computationally
challenging. The complexity of finding the global solution increases
exponentially with the number of data points, making exact methods impractical
for large-scale datasets. Even obtaining strong lower bounds on the optimal
MSSC objective value is computationally prohibitive, making it difficult to
assess the quality of heuristic solutions. We address this challenge by
introducing a novel method to validate heuristic MSSC solutions through
optimality gaps. Our approach employs a divide-and-conquer strategy,
decomposing the problem into smaller instances that can be handled by an exact
solver. The decomposition is guided by an auxiliary optimization problem, the
""anticlustering problem"", for which we design an efficient heuristic.
Computational experiments demonstrate the effectiveness of the method for
large-scale instances, achieving optimality gaps below 3% in most cases while
maintaining reasonable computational times. These results highlight the
practicality of our approach in assessing feasible clustering solutions for
large datasets, bridging a critical gap in MSSC evaluation.","['math.OC', 'cs.LG']",2502.03152," In this work, we have carried out lattice simulations of $(2+1)$-flavor QCD
using highly improved staggered quarks at the physical pion mass on $32^3
\times 8$ and $48^3 \times 12$ lattices, with magnetic field strengths ranging
up to 0.8 GeV$^2$ and nonzero baryon chemical potentials employing the Taylor
expansion framework. We present lattice QCD continuum estimate results, along
with the magnetized hadron resonance and ideal gas comparisons, for the
leading-order Taylor expansion coefficients for bulk thermodynamic quantities
such as pressure, number density, energy density, and entropy density, focusing
on the significant impact of strong magnetic fields.","['hep-lat', 'hep-ph', 'hep-th']",False,,,,Strong bounds for large-scale Minimum Sum-of-Squares Clustering,"QCD Equation of State with Strong Magnetic Fields and Nonzero Baryon
  Density"
neg-d2-61,2025-01-10,,2501.05712," We introduce HRMCR (HAE-RAE Multi-Step Commonsense Reasoning), a benchmark
designed to evaluate large language models' ability to perform multi-step
reasoning in culturally specific contexts, focusing on Korean. The questions
are automatically generated via templates and algorithms, requiring LLMs to
integrate Korean cultural knowledge into sequential reasoning steps. Consistent
with prior observations on emergent abilities, our experiments reveal that
models trained on fewer than \(2 \cdot 10^{25}\) training FLOPs struggle to
solve any questions, showing near-zero performance. Beyond this threshold,
performance improves sharply. State-of-the-art models (e.g., O1) still score
under 50\%, underscoring the difficulty of our tasks. Notably, stepwise
analysis suggests the observed emergent behavior may stem from compounding
errors across multiple steps rather than reflecting a genuinely new capability.
We publicly release the benchmark and commit to regularly updating the dataset
to prevent contamination.",['cs.CL'],2501.10778," Large Neighbourhood Search (LNS) is a powerful heuristic framework for
solving Mixed-Integer Programming (MIP) problems. However, designing effective
variable selection strategies in LNS remains challenging, especially for
diverse sets of problems. In this paper, we propose an approach that integrates
Machine Learning (ML) within the destroy operator of LNS for MIPs with a focus
on minimal offline training. We implement a modular LNS matheuristic as a test
bench to compare different LNS heuristics, including our ML-enhanced LNS.
Experimental results on the MIPLIB 2017 dataset demonstrate that the
matheuristic can significantly improve the performance of state-of-the-art
solvers like Gurobi and SCIP. We conduct analyses on noisy oracles to explore
the impact of prediction accuracy on solution quality. Additionally, we develop
techniques to enhance the ML model through loss adjustments and sampling
routines. Our findings suggest that while random LNS remains competitive, our
Supervised LNS (SLNS) outperforms other baselines and helps set the foundation
for future research on ML for LNS methods that are both efficient and general.",['math.OC'],False,,,,Multi-Step Reasoning in Korean and the Emergent Mirage,Supervised Large Neighbourhood Search for MIPs
neg-d2-62,2025-03-07,,2503.05469," We identify the size of the largest connected component in a subcritical
inhomogeneous random graph with a kernel of preferential attachment type. The
component is polynomial in the graph size with an explicitly given exponent,
which is strictly larger than the exponent for the largest degree in the graph.
This is in stark contrast to the behaviour of inhomogeneous random graphs with
a kernel of rank one. Our proof uses local approximation by branching random
walks going well beyond the weak local limit and novel results on subcritical
killed branching random walks.","['math.PR', 'math.CO']",2502.0194," We present a cost-effective new approach for generating denser depth maps for
Autonomous Driving (AD) and Autonomous Vehicles (AVs) by integrating the images
obtained from deep neural network (DNN) 4D radar detectors with conventional
camera RGB images. Our approach introduces a novel pixel positional encoding
algorithm inspired by Bartlett's spatial spectrum estimation technique. This
algorithm transforms both radar depth maps and RGB images into a unified pixel
image subspace called the Spatial Spectrum, facilitating effective learning
based on their similarities and differences. Our method effectively leverages
high-resolution camera images to train radar depth map generative models,
addressing the limitations of conventional radar detectors in complex vehicular
environments, thus sharpening the radar output. We develop spectrum estimation
algorithms tailored for radar depth maps and RGB images, a comprehensive
training framework for data-driven generative models, and a camera-radar
deployment scheme for AV operation. Our results demonstrate that our approach
also outperforms the state-of-the-art (SOTA) by 27.95% in terms of
Unidirectional Chamfer Distance (UCD).","['cs.CV', 'eess.IV']",False,,,,"The largest subcritical component in inhomogeneous random graphs of
  preferential attachment type","Toward a Low-Cost Perception System in Autonomous Vehicles: A Spectrum
  Learning Approach"
neg-d2-63,2025-01-09,,2501.05114," A growing number of directly-imaged companions have been recently
characterised, with robust constraints on carbon-to-oxygen ratios and even
isotopic ratios. Many companions and isolated targets have also shown spectral
variability. In this work we observed the super-Jupiter AB~Pictoris~b across
four consecutive nights using VLT/CRIRES+ as part of the ESO SupJup survey,
exploring how the constraints on chemical composition and temperature profile
change over time using spectral line shape variations between nights. We
performed atmospheric retrievals of the high-resolution observations and found
broadly consistent results across all four nights, but there were differences
for some parameters. We clearly detect H$_2$O, $^{12}$CO and $^{13}$CO in each
night, but abundances varied by $\sim2\sigma$, which was correlated to the deep
atmosphere temperature profiles. We also found differences in the
$^{12}$C$/^{13}$C ratios in each night by up to $\sim3\sigma$, which seemed to
be correlated with the cloud deck pressure. Our combined retrieval
simultaneously analysing all nights together constrained broadly the average of
each night individually, with the C/O$=0.59\pm0.01$, consistent with solar
composition, and $^{12}$C$/^{13}$C~$ = 102\pm8$, slightly higher than the ISM
and Solar System values. We also find a low projected rotational velocity,
suggesting that AB~Pictoris~b is either intrinsically a slow rotator due to its
young age or that the spin axis is observed pole-on with a $\sim90^\circ$
misalignment with its orbit inclination. Future observations will be able to
further explore the variability and orbit of AB~Pictoris~b as well as for other
companions.",['astro-ph.EP'],2503.08257," A dexterous hand capable of grasping any object is essential for the
development of general-purpose embodied intelligent robots. However, due to the
high degree of freedom in dexterous hands and the vast diversity of objects,
generating high-quality, usable grasping poses in a robust manner is a
significant challenge. In this paper, we introduce DexGrasp Anything, a method
that effectively integrates physical constraints into both the training and
sampling phases of a diffusion-based generative model, achieving
state-of-the-art performance across nearly all open datasets. Additionally, we
present a new dexterous grasping dataset containing over 3.4 million diverse
grasping poses for more than 15k different objects, demonstrating its potential
to advance universal dexterous grasping. The code of our method and our dataset
will be publicly released soon.","['cs.CV', 'cs.AI', 'cs.RO']",False,,,,"The ESO SupJup Survey V: Exploring Atmospheric Variability and Orbit of
  the Super-Jupiter AB Pictoris b with CRIRES+","DexGrasp Anything: Towards Universal Robotic Dexterous Grasping with
  Physics Awareness"
neg-d2-64,2025-03-17,,2503.12995," Using Hahn series, one can attach to any linear Mahler equation a basis of
solutions at 0 reminiscent of the solutions of linear differential equations at
a regular singularity. We show that such a basis of solutions can be produced
by using a variant of Frobenius method.",['cs.SC'],2503.07383," Diverse usage patterns induce complex and variable aging behaviors in
lithium-ion batteries, complicating accurate health diagnosis and prognosis.
Separate diagnostic cycles are often used to untangle the battery's current
state of health from prior complex aging patterns. However, these same
diagnostic cycles alter the battery's degradation trajectory, are
time-intensive, and cannot be practically performed in onboard applications. In
this work, we leverage portions of operational measurements in combination with
an interpretable machine learning model to enable rapid, onboard battery health
diagnostics and prognostics without offline diagnostic testing and the
requirement of historical data. We integrate mechanistic constraints within an
encoder-decoder architecture to extract electrode states in a physically
interpretable latent space and enable improved reconstruction of the
degradation path. The health diagnosis model framework can be flexibly applied
across diverse application interests with slight fine-tuning. We demonstrate
the versatility of this model framework by applying it to three battery-cycling
datasets consisting of 422 cells under different operating conditions,
highlighting the utility of an interpretable diagnostic-free, onboard battery
diagnosis and prognosis model.","['eess.SY', 'cs.LG', 'cs.SY']",False,,,,Frobenius method for Mahler equations,Diagnostic-free onboard battery health assessment
neg-d2-65,2025-02-05,,2502.04364," Recent advancements in diffusion models have driven the growth of text-guided
image editing tools, enabling precise and iterative modifications of
synthesized content. However, as these tools become increasingly accessible,
they also introduce significant risks of misuse, emphasizing the critical need
for robust attribution methods to ensure content authenticity and traceability.
Despite the creative potential of such tools, they pose significant challenges
for attribution, particularly in adversarial settings where edits can be
layered to obscure an image's origins. We propose LambdaTracer, a novel
latent-space attribution method that robustly identifies and differentiates
authentic outputs from manipulated ones without requiring any modifications to
generative or editing pipelines. By adaptively calibrating reconstruction
losses, LambdaTracer remains effective across diverse iterative editing
processes, whether automated through text-guided editing tools such as
InstructPix2Pix and ControlNet or performed manually with editing software such
as Adobe Photoshop. Extensive experiments reveal that our method consistently
outperforms baseline approaches in distinguishing maliciously edited images,
providing a practical solution to safeguard ownership, creativity, and
credibility in the open, fast-evolving AI ecosystems.","['cs.CV', 'cs.AI', 'cs.HC', 'cs.LG']",2502.11343," In this paper, a new class of structured polynomials, which we dub the {\it
separable plus lower degree {\rm (SPLD in short)} polynomials}, is introduced.
The formal definition of an SPLD polynomial, which extends the concept of the
SPQ polynomial (Ahmadi et al. in Math Oper Res 48:1316--1343, 2023), is
defined. A type of bounded degree SOS hierarchy (BSOS-SPLD) is proposed to
efficiently solve the optimization problems with SPLD polynomials, and several
numerical examples are performed much better than the bounded degree SOS
hierarchy (Lasserre et al. in EURO J Comput Optim 5:87--117, 2017). An exact
SOS relaxation for a class of convex SPLD polynomial optimization problems is
proposed. Finally, an application of SPLD polynomials to polynomial regression
problems in statistics is presented.",['math.OC'],False,,,,Lost in Edits? A $\lambda$-Compass for AIGC Provenance,SPLD polynomial optimization and bounded degree SOS hierarchies
neg-d2-66,2025-01-06,,2501.03145," This research focuses on developing a method for restoring the topology of
digital images of paper documents captured by a camera, using algorithms for
detection, segmentation, geometry restoration, and dewarping. Our methodology
employs deep learning (DL) for document outline detection, followed by computer
vision (CV) to create a topological 2D grid using cubic polynomial
interpolation and correct nonlinear distortions by remapping the image. Using
classical CV methods makes the document topology restoration process more
efficient and faster, as it requires significantly fewer computational
resources and memory. We developed a new pipeline for automatic document
dewarping and reconstruction, along with a framework and annotated dataset to
demonstrate its efficiency. Our experiments confirm the promise of our
methodology and its superiority over existing benchmarks (including mobile apps
and popular DL solutions, such as RectiNet, DocGeoNet, and DocTr++) both
visually and in terms of document readability via Optical Character Recognition
(OCR) and geometry restoration metrics. This paves the way for creating
high-quality digital copies of paper documents and enhancing the efficiency of
OCR systems. Project page: https://github.com/HorizonParadox/DRCCBI","['cs.CV', 'cs.AI', 'cs.LG']",2503.0055," \abstract{Urban scaling theories posit that larger cities exhibit
disproportionately higher levels of socioeconomic activity and human
interactions. Yet, evidence from developing contexts (especially those marked
by stark socioeconomic disparities) remains limited. To address this gap, we
analyse a month-long dataset of 3.1~billion voice-call records from Brazil's
100 most populous cities, providing a continental-scale test of urban scaling
laws. We measure interactions using two complementary proxies: the number of
phone-based contacts (voice-call degrees) and the number of trips inferred from
consecutive calls in distinct locations. Our findings reveal clear superlinear
relationships in both metrics, indicating that larger urban centres exhibit
intensified remote communication and physical mobility. We further observe that
gross domestic product (GDP) also scales superlinearly with population,
consistent with broader claims that economic output grows faster than city
size. Conversely, the number of antennas required per user scales sublinearly,
suggesting economies of scale in telecommunications infrastructure. Although
the dataset covers a single provider, its widespread coverage in major cities
supports the robustness of the results. We nonetheless discuss potential
biases, including city-specific marketing campaigns and predominantly prepaid
users, as well as the open question of whether higher interaction drives wealth
or vice versa. Overall, this study enriches our understanding of urban scaling,
emphasising how communication and mobility jointly shape the socioeconomic
landscapes of rapidly growing cities.",['physics.soc-ph'],False,,,,Geometry Restoration and Dewarping of Camera-Captured Document Images,"Validating Urban Scaling Laws through Mobile Phone Data: A
  Continental-Scale Analysis of Brazil's Largest Cities"
neg-d2-67,2025-01-13,,2501.0726," 3D semantic scene completion is critical for multiple downstream tasks in
autonomous systems. It estimates missing geometric and semantic information in
the acquired scene data. Due to the challenging real-world conditions, this
task usually demands complex models that process multi-modal data to achieve
acceptable performance. We propose a unique neural model, leveraging advances
from the state space and diffusion generative modeling to achieve remarkable 3D
semantic scene completion performance with monocular image input. Our technique
processes the data in the conditioned latent space of a variational autoencoder
where diffusion modeling is carried out with an innovative state space
technique. A key component of our neural network is the proposed Skimba (Skip
Mamba) denoiser, which is adept at efficiently processing long-sequence data.
The Skimba diffusion model is integral to our 3D scene completion network,
incorporating a triple Mamba structure, dimensional decomposition residuals and
varying dilations along three directions. We also adopt a variant of this
network for the subsequent semantic segmentation stage of our method. Extensive
evaluation on the standard SemanticKITTI and SSCBench-KITTI360 datasets show
that our approach not only outperforms other monocular techniques by a large
margin, it also achieves competitive performance against stereo methods. The
code is available at https://github.com/xrkong/skimba","['cs.CV', 'cs.AI']",2503.04003," Mobile platforms now power not only smartphones but also in-vehicle systems
like Android Auto and CarPlay. Despite an ecosystem of over 3.5 million Android
apps and more than 200 million Android Auto-compatible vehicles, only a few
hundred apps have been adapted for automotive use. To better understand this
gap, we studied 147 reported issues related to Android Auto and identified
their root causes. We found that more than 70% of issues result from UI
incompatibilities, 24% from media playback errors, and around 5% from failures
in voice command handling, showing a lack of effective tools for developers. We
introduce CarCompat, a static analysis framework that detects compatibility
problems in Android Auto apps. CarCompat constructs a Car-Control Flow Graph
(CCFG) to capture interactions among app components, lifecycle methods, and
platform-specific callbacks. It applies specialized checkers to detect UI
violations, media playback errors, and issues with voice command handling. We
evaluated CarCompat on a dataset of 54 Android Auto apps and detected 25 new
issues, 4 of which were confirmed by developers, and 2 developers have already
released their fixes. The results show that CarCompat helps developers identify
and fix compatibility issues, improving the in-vehicle experience.","['cs.SE', 'cs.PL']",False,,,,Skip Mamba Diffusion for Monocular 3D Semantic Scene Completion,Understanding and Detecting Compatibility Issues in Android Auto Apps
neg-d2-68,2025-01-07,,2501.03756," Interior models of gas giants in the Solar System traditionally assume a
fully convective molecular hydrogen envelope. However, recent observations from
the Juno mission suggest a possible depletion of alkali metals in Jupiter's
molecular hydrogen envelope, indicating that a stable radiative layer could
exist at the kilobar level. Recent studies propose that deep stable layers help
reconcile various Jupiter observations, including its atmospheric water and CO
abundances and the depth of its zonal winds. However, opacity tables used to
infer stable layers are often outdated and incomplete, leaving the precise
molecular hydrogen envelope composition required for a deep radiative zone
uncertain. In this paper, we determine atmospheric compositions that can lead
to the formation of a radiative zone at the kilobar level in Jupiter and Saturn
today. We computed radiative opacity tables covering pressures up to $10^5$
bar, including the most abundant molecules present in the gas giants of the
Solar System, as well as contributions from free electrons, metal hydrides,
oxides, and atomic species, using the most up-to-date line lists published in
the literature. These tables were used to calculate Rosseland-mean opacities
for the molecular hydrogen envelopes of Jupiter and Saturn, which were then
compared to the critical mean opacity required to maintain convection. We find
that the presence of a radiative zone is controlled by the existence of K, Na,
and NaH in the atmosphere of Jupiter and Saturn. For Jupiter, the elemental
abundance of K and Na must be less than $\sim 10^{-3}$ times solar to form a
radiative zone. In contrast, for Saturn, the required abundance for K and Na is
below $\sim 10^{-4}$ times solar.",['astro-ph.EP'],2503.14187," In this paper, we consider the inviscid limit problem to the higher
dimensional incompressible Navier-Stokes equations in the whole space. It was
proved in \cite[J. Funct. Anal., 276 (2019)]{GZ} that given initial data
$u_0\in B^{s}_{p,r}$ with $1\leq r<\infty$, the solutions of the Navier-Stokes
equations converge strongly in $B^{s}_{p,r}$ to the Euler equations as the
viscosity parameter tends to zero. In the case when $r=\infty$, we prove the
failure of the $B^{s}_{p,\infty}$-convergence of the Navier-Stokes equations
toward the Euler equations in the inviscid limit.",['math.AP'],False,,,,"Conditions for radiative zones in the molecular hydrogen envelope of
  Jupiter and Saturn: The role of alkali metals","Non-convergence of the Navier-Stokes equations toward the Euler
  equations in weak Besov spaces"
neg-d2-69,2025-02-10,,2502.0709," Accurate prediction with multimodal data-encompassing tabular, textual, and
visual inputs or outputs-is fundamental to advancing analytics in diverse
application domains. Traditional approaches often struggle to integrate
heterogeneous data types while maintaining high predictive accuracy. We
introduce Generative Distribution Prediction (GDP), a novel framework that
leverages multimodal synthetic data generation-such as conditional diffusion
models-to enhance predictive performance across structured and unstructured
modalities. GDP is model-agnostic, compatible with any high-fidelity generative
model, and supports transfer learning for domain adaptation. We establish a
rigorous theoretical foundation for GDP, providing statistical guarantees on
its predictive accuracy when using diffusion models as the generative backbone.
By estimating the data-generating distribution and adapting to various loss
functions for risk minimization, GDP enables accurate point predictions across
multimodal settings. We empirically validate GDP on four supervised learning
tasks-tabular data prediction, question answering, image captioning, and
adaptive quantile regression-demonstrating its versatility and effectiveness
across diverse domains.","['stat.ML', 'cs.AI', 'cs.LG']",2503.0603," Computed tomography (CT) is extensively used for accurate visualization and
segmentation of organs and lesions. While deep learning models such as
convolutional neural networks (CNNs) and vision transformers (ViTs) have
significantly improved CT image analysis, their performance often declines when
applied to diverse, real-world clinical data. Although foundation models offer
a broader and more adaptable solution, their potential is limited due to the
challenge of obtaining large-scale, voxel-level annotations for medical images.
In response to these challenges, prompting-based models using visual or text
prompts have emerged. Visual-prompting methods, such as the Segment Anything
Model (SAM), still require significant manual input and can introduce ambiguity
when applied to clinical scenarios. Instead, foundation models that use text
prompts offer a more versatile and clinically relevant approach. Notably,
current text-prompt models, such as the CLIP-Driven Universal Model, are
limited to text prompts already encountered during training and struggle to
process the complex and diverse scenarios of real-world clinical applications.
Instead of fine-tuning models trained from natural imaging, we propose
OpenVocabCT, a vision-language model pretrained on large-scale 3D CT images for
universal text-driven segmentation. Using the large-scale CT-RATE dataset, we
decompose the diagnostic reports into fine-grained, organ-level descriptions
using large language models for multi-granular contrastive learning. We
evaluate our OpenVocabCT on downstream segmentation tasks across nine public
datasets for organ and tumor segmentation, demonstrating the superior
performance of our model compared to existing methods. All code, datasets, and
models will be publicly released at https://github.com/ricklisz/OpenVocabCT.","['cs.CV', 'cs.AI']",False,,,,"Generative Distribution Prediction: A Unified Approach to Multimodal
  Learning",Towards Universal Text-driven CT Image Segmentation
neg-d2-70,2025-01-26,,2501.15467," The recently suggested bipartite analysis extends the Kauffman planar
decomposition to arbitrary $N$, i.e. extends it from the Jones polynomial to
the HOMFLY polynomial. This provides a generic and straightforward
non-perturbative calculus in an arbitrary Chern--Simons theory. Technically,
this approach is restricted to knots and links which possess bipartite
realizations, i.e. can be entirely glued from antiparallel lock (two-vertex)
tangles rather than single-vertex $R$-matrices. However, we demonstrate that
the resulting positive decomposition (PD), i.e. the representation of the
fundamental HOMFLY polynomials as positive integer polynomials of the three
parameters $\phi$, $\bar\phi$ and $D$, exists for arbitrary knots, not only
bipartite ones. This poses new questions about the true significance of
bipartite expansion, which appears to make sense far beyond its original scope,
and its generalizations to higher representations. We have provided two
explanations for the existence of the PD for non-bipartite knots. An
interesting option is to resolve a particular bipartite vertex in a
not-fully-bipartite diagram and reduce the HOMFLY polynomial to a linear
combination of those for smaller diagrams. If the resulting diagrams correspond
to bipartite links, this option provides a PD even to an initially
non-bipartite knot. Another possibility for a non-bipartite knot is to have a
bipartite clone with the same HOMFLY polynomial providing this PD. We also
suggest a promising criterium for the existence of a bipartite realization
behind a given PD, which is based on the study of the precursor Jones
polynomials.","['hep-th', 'math-ph', 'math.GT', 'math.MP']",2501.18689," The first direct detection of gravitational waves in 2015 marked the
beginning of a new era for the study of compact objects. Upcoming detectors,
such as the Einstein Telescope, are expected to add thousands of binary
coalescences to the list. However, from a theoretical perspective, our
understanding of compact objects is hindered by many uncertainties, and a
comprehensive study of the nature of stellar remnants from core-collapse
supernovae is still lacking. In this work, we investigate the properties of
stellar remnants using a homogeneous grid of rotating and non-rotating massive
stars at various metallicities from Limongi and Chieffi 2018. We simulate the
supernova explosion of the evolved progenitors using the HYdrodynamic Ppm
Explosion with Radiation diffusION (HYPERION) code (Limongi and Chieffi 2020),
assuming a thermal bomb model calibrated to match the main properties of
SN1987A. We find that the heaviest black hole that can form depends on the
initial stellar rotation, metallicity, and the assumed criterion for the onset
of pulsational pair-instability supernovae. Non-rotating progenitors at
$\big[\rm Fe/H \big]=-3$ can form black holes up to $\sim 87 M_\odot$, falling
within the theorized pair-instability mass gap. Conversely, enhanced wind mass
loss prevents the formation of BHs more massive than $\sim 41.6 M_\odot$ from
rotating progenitors. We use our results to study the black hole mass
distribution from a population of $10^6$ isolated massive stars following a
Kroupa initial mass function. Finally, we provide fitting formulas to compute
the mass of compact remnants as a function of stellar progenitor properties.
Our up-to-date prescriptions can be easily implemented in rapid population
synthesis codes.","['astro-ph.HE', 'astro-ph.SR']",False,,,,Bipartite expansion beyond biparticity,The initial mass-remnant mass relation for core collapse supernovae
neg-d2-71,2025-01-06,,2501.02824," Anesthetics are crucial in surgical procedures and therapeutic interventions,
but they come with side effects and varying levels of effectiveness, calling
for novel anesthetic agents that offer more precise and controllable effects.
Targeting Gamma-aminobutyric acid (GABA) receptors, the primary inhibitory
receptors in the central nervous system, could enhance their inhibitory action,
potentially reducing side effects while improving the potency of anesthetics.
In this study, we introduce a proteomic learning of GABA receptor-mediated
anesthesia based on 24 GABA receptor subtypes by considering over 4000 proteins
in protein-protein interaction (PPI) networks and over 1.5 millions known
binding compounds. We develop a corresponding drug-target interaction network
to identify potential lead compounds for novel anesthetic design. To ensure
robust proteomic learning predictions, we curated a dataset comprising 136
targets from a pool of 980 targets within the PPI networks. We employed three
machine learning algorithms, integrating advanced natural language processing
(NLP) models such as pretrained transformer and autoencoder embeddings. Through
a comprehensive screening process, we evaluated the side effects and
repurposing potential of over 180,000 drug candidates targeting the GABRA5
receptor. Additionally, we assessed the ADMET (absorption, distribution,
metabolism, excretion, and toxicity) properties of these candidates to identify
those with near-optimal characteristics. This approach also involved optimizing
the structures of existing anesthetics. Our work presents an innovative
strategy for the development of new anesthetic drugs, optimization of
anesthetic use, and deeper understanding of potential anesthesia-related side
effects.","['q-bio.BM', 'cs.LG']",2501.14716," The spectral theory on the $S$-spectrum originated to give quaternionic
quantum mechanics a precise mathematical foundation and as a spectral theory
for linear operators in vector analysis.
  This theory has proven to be significantly more general than initially
anticipated, naturally extending to fully Clifford operators and revealing
unexpected connections with the spectral theory based on the monogenic
spectrum, developed over forty years ago by A. McIntosh and collaborators.
  In recent years, we have combined slice hyperholomorphic functions with the
Fueter-Sce mapping theorem, also called Fueter-Sce extension theorem, to
broaden the class of functions and operators to which the theory can be
applied. This generalization has led to the definition of what we call the {\em
fine structures on the $S$-spectrum}, consisting of classes of functions that
admit an integral representation and their associated functional calculi.
  In this paper, we focus on the fine structures within the Clifford algebra
setting, particularly addressing polyharmonic functions, polyanalytic
functions, holomorphic Cliffordian functions and their associated functional
calculi defined via integral representation formulas.
  Moreover, we demonstrate that the monogenic functional calculus, defined via
the monogenic Cauchy formula, and the $F$-functional calculus of the fine
structures, defined via the Fueter-Sce mapping theorem in integral form, yield
the same operator.",['math.FA'],False,,,,"Proteomic Learning of Gamma-Aminobutyric Acid (GABA) Receptor-Mediated
  Anesthesia","Functions and operators of the polyharmonic and polyanalytic Clifford
  fine structures on the $S$-spectrum"
neg-d2-72,2025-02-10,,2502.06179," Driver decision quality in take-overs is critical for effective
human-Autonomous Driving System (ADS) collaboration. However, current research
lacks detailed analysis of its variations. This paper introduces two
metrics--Actual Achieved Gain (AAG) and Optimal Perceived Gain (OPG)--to assess
decision quality, with OPG representing optimal decisions and AAG reflecting
actual outcomes. Both are calculated as weighted averages of perceived gains
and losses, influenced by ADS accuracy. Study 1 (N=315) used a 21-point
Thurstone scale to measure perceived gains and losses-key components of AAG and
OPG-across typical tasks: route selection, overtaking, and collision avoidance.
Studies 2 (N=54) and 3 (N=54) modeled decision quality under varying ADS
accuracy and decision time. Results show with sufficient time (>3.5s), AAG
converges towards OPG, indicating rational decision-making, while limited time
leads to intuitive and deterministic choices. Study 3 also linked AAG-OPG
deviations to irrational behaviors. An intervention study (N=8) and a pilot
(N=4) employing voice alarms and multi-modal alarms based on these deviations
demonstrated AAG's potential to improve decision quality.",['cs.HC'],2503.11461," Navigating unknown three-dimensional (3D) rugged environments is challenging
for multi-robot systems. Traditional discrete systems struggle with rough
terrain due to limited individual mobility, while modular systems--where rigid,
controllable constraints link robot units--improve traversal but suffer from
high control complexity and reduced flexibility. To address these limitations,
we propose the Multi-Robot System with Controllable Weak Constraints (MRS-CWC),
where robot units are connected by constraints with dynamically adjustable
stiffness. This adaptive mechanism softens or stiffens in real-time during
environmental interactions, ensuring a balance between flexibility and
mobility. We formulate the system's dynamics and control model and evaluate
MRS-CWC against six baseline methods and an ablation variant in a benchmark
dataset with 100 different simulation terrains. Results show that MRS-CWC
achieves the highest navigation completion rate and ranks second in success
rate, efficiency, and energy cost in the highly rugged terrain group,
outperforming all baseline methods without relying on environmental modeling,
path planning, or complex control. Even where MRS-CWC ranks second, its
performance is only slightly behind a more complex ablation variant with
environmental modeling and path planning. Finally, we develop a physical
prototype and validate its feasibility in a constructed rugged environment. For
videos, simulation benchmarks, and code, please visit
https://wyd0817.github.io/project-mrs-cwc/.","['cs.RO', 'cs.MA']",False,,,,"Actual Achieved Gain and Optimal Perceived Gain: Modeling Human
  Take-over Decisions Towards Automated Vehicles' Suggestions","MRS-CWC: A Weakly Constrained Multi-Robot System with Controllable
  Constraint Stiffness for Mobility and Navigation in Unknown 3D Rough
  Environments"
neg-d2-73,2025-01-31,,2501.19259," The integration of human-intuitive interactions into autonomous systems has
been limited. Traditional Natural Language Processing (NLP) systems struggle
with context and intent understanding, severely restricting human-robot
interaction. Recent advancements in Large Language Models (LLMs) have
transformed this dynamic, allowing for intuitive and high-level communication
through speech and text, and bridging the gap between human commands and
robotic actions. Additionally, autonomous navigation has emerged as a central
focus in robotics research, with artificial intelligence (AI) increasingly
being leveraged to enhance these systems. However, existing AI-based navigation
algorithms face significant challenges in latency-critical tasks where rapid
decision-making is critical. Traditional frame-based vision systems, while
effective for high-level decision-making, suffer from high energy consumption
and latency, limiting their applicability in real-time scenarios. Neuromorphic
vision systems, combining event-based cameras and spiking neural networks
(SNNs), offer a promising alternative by enabling energy-efficient, low-latency
navigation. Despite their potential, real-world implementations of these
systems, particularly on physical platforms such as drones, remain scarce. In
this work, we present Neuro-LIFT, a real-time neuromorphic navigation framework
implemented on a Parrot Bebop2 quadrotor. Leveraging an LLM for natural
language processing, Neuro-LIFT translates human speech into high-level
planning commands which are then autonomously executed using event-based
neuromorphic vision and physics-driven planning. Our framework demonstrates its
capabilities in navigating in a dynamic environment, avoiding obstacles, and
adapting to human instructions in real-time.","['cs.RO', 'cs.CV', 'cs.LG', 'cs.NE', 'cs.SY', 'eess.SY']",2503.03925," In recent years, attempts have been made to extend ISS small-gain theorems
from finite networks to countably infinite, locally finite networks. Under
specific assumptions about the interconnection gains and the ISS formulation,
corresponding infinite-dimensional small-gain results have been proven.
However, concerning these assumptions, the results are still too narrow to be
considered a full extension of the state-of-the-art for finite networks. We
take a step to closing this gap by a thorough investigation of various monotone
operators associated with an infinite network and a specific ISS formulation.
Our results shed more light on the theory of finite networks, yield complete
characterizations of the small-gain condition for specific ISS formulations,
and show which obstacles still have to be overcome to obtain a complete theory
for the most general case.","['math.OC', 'math.DS']",False,,,,"Neuro-LIFT: A Neuromorphic, LLM-based Interactive Framework for
  Autonomous Drone FlighT at the Edge",The Small-Gain Condition for Infinite Networks
neg-d2-74,2025-01-26,,2501.15704," Recent measurements of the Hubble constant using type Ia supernovae
explicitly correct for their estimated peculiar velocities using the 2M++
reconstruction of the local density field. The amount of uncertainty from this
reconstruction procedure has thus far been unquantified. To rectify this, we
use mock universe realisations of the 2M++ catalogue and generate predicted
peculiar velocities using the same method as the predictions that are used to
correct for the Pantheon+ catalogue. We find that the method yields
uncertainties of 0.3 km/s/Mpc and hence subdominant to the total uncertainty in
H0.",['astro-ph.CO'],2502.14344," Binary Spiking Neural Networks (BSNNs) inherit the eventdriven paradigm of
SNNs, while also adopting the reduced storage burden of binarization
techniques. These distinct advantages grant BSNNs lightweight and
energy-efficient characteristics, rendering them ideal for deployment on
resource-constrained edge devices. However, due to the binary synaptic weights
and non-differentiable spike function, effectively training BSNNs remains an
open question. In this paper, we conduct an in-depth analysis of the challenge
for BSNN learning, namely the frequent weight sign flipping problem. To
mitigate this issue, we propose an Adaptive Gradient Modulation Mechanism
(AGMM), which is designed to reduce the frequency of weight sign flipping by
adaptively adjusting the gradients during the learning process. The proposed
AGMM can enable BSNNs to achieve faster convergence speed and higher accuracy,
effectively narrowing the gap between BSNNs and their full-precision
equivalents. We validate AGMM on both static and neuromorphic datasets, and
results indicate that it achieves state-of-the-art results among BSNNs. This
work substantially reduces storage demands and enhances SNNs' inherent energy
efficiency, making them highly feasible for resource-constrained environments.",['cs.CV'],False,,,,Uncertainties in the Hubble Constant from Peculiar Velocities,"Towards Accurate Binary Spiking Neural Networks: Learning with Adaptive
  Gradient Modulation Mechanism"
neg-d2-75,2025-02-17,,2502.11972," Waveguides potentially offer an effective medium for interconnecting quantum
processors within a modular framework, facilitating the coherent quantum state
transfer between the qubits across separate chips. In this work, we analyze a
quantum communication scenario where two qubits are connected to a shared
waveguide, whose resonance frequency may match or not match that of the qubits.
Both configurations are simulated from the perspective of quantum
electrodynamics (QED) to assess the system behavior and key factors that
influence reliable inter-chip communication. The primary performance metrics
analyzed are quantum state transfer fidelity and latency, considering the
impact of key system parameters such as the qubit-waveguide detuning, coupling
strength, waveguide decay rate, and qubit decay rate. We present the system
design requirements that yield enhanced state transmission fidelity rates and
lowered latency, and discuss the scalability of waveguide-mediated
interconnects considering various configurations of the system.",['quant-ph'],2501.0883," In this work, we offer a historical stroll through the vast topic of binary
quadratic forms. We begin with a quick review of their history and then an
overview of contemporary algebraic developments on the subject.","['math.HO', 'math.NT']",False,,,,"Waveguide QED Analysis of Quantum-Coherent Links for Modular Quantum
  Computing",Binary quadratic forms: modern developments
neg-d2-76,2025-02-27,,2502.20382," We present a low-cost data generation pipeline that integrates physics-based
simulation, human demonstrations, and model-based planning to efficiently
generate large-scale, high-quality datasets for contact-rich robotic
manipulation tasks. Starting with a small number of embodiment-flexible human
demonstrations collected in a virtual reality simulation environment, the
pipeline refines these demonstrations using optimization-based kinematic
retargeting and trajectory optimization to adapt them across various robot
embodiments and physical parameters. This process yields a diverse, physically
consistent dataset that enables cross-embodiment data transfer, and offers the
potential to reuse legacy datasets collected under different hardware
configurations or physical parameters. We validate the pipeline's effectiveness
by training diffusion policies from the generated datasets for challenging
contact-rich manipulation tasks across multiple robot embodiments, including a
floating Allegro hand and bimanual robot arms. The trained policies are
deployed zero-shot on hardware for bimanual iiwa arms, achieving high success
rates with minimal human input. Project website:
https://lujieyang.github.io/physicsgen/.","['cs.RO', 'cs.AI', 'cs.LG', 'cs.SY', 'eess.SY']",2501.05023," The ESA Euclid mission will survey more than 14,000 deg$^2$ of the sky in
visible and near-infrared wavelengths, mapping the extra-galactic sky to
constrain our cosmological model of the Universe. Although the survey focusses
on regions further than 15 deg from the ecliptic, it should allow for the
detection of more than about $10^5$ Solar System objects (SSOs). After
simulating the expected signal from SSOs in Euclid images acquired with the
visible camera (VIS), we describe an automated pipeline developed to detect
moving objects with an apparent velocity in the range of 0.1-10 arcsec/h,
typically corresponding to sources in the outer Solar System (from Centaurs to
Kuiper-belt objects). In particular, the proposed detection scheme is based on
Sourcextractor software and on applying a new algorithm capable of associating
moving objects amongst different catalogues. After applying a suite of filters
to improve the detection quality, we study the expected purity and completeness
of the SSO detections. We also show how a Kohonen self-organising neural
network can be successfully trained (in an unsupervised fashion) to classify
stars, galaxies, and SSOs. By implementing an early-stopping method in the
training scheme, we show that the network can be used in a predictive way,
allowing one to assign the probability of each detected object being a member
of each considered class.",['astro-ph.IM'],False,,,,"Physics-Driven Data Generation for Contact-Rich Manipulation via
  Trajectory Optimization","Euclid: Detecting Solar System objects in Euclid images and classifying
  them using Kohonen self-organising maps"
neg-d2-77,2025-03-03,,2503.13469," Cardiovascular diseases (CVDs) are disorders impacting the heart and
circulatory system. These disorders are the foremost and continuously
escalating cause of mortality worldwide. One of the main tasks when working
with CVDs is analyzing and identifying pathologies on a 12-lead
electrocardiogram (ECG) with a standard 10-second duration. Using machine
learning (ML) in automatic ECG analysis increases CVD diagnostics'
availability, speed, and accuracy. However, the most significant difficulty in
developing ML models is obtaining a sufficient training dataset. Due to the
limitations of medical data usage, such as expensiveness, errors, the ambiguity
of labels, imbalance of classes, and privacy issues, utilizing synthetic
samples depending on specific pathologies bypasses these restrictions and
improves algorithm quality. Existing solutions for the conditional generation
of ECG signals are mainly built on Generative Adversarial Networks (GANs), and
only a few papers consider the architectures based on Variational Autoencoders
(VAEs), showing comparable results in recent works. This paper proposes the
publicly available conditional Nouveau VAE model for ECG signal generation
(cNVAE-ECG), which produces high-resolution ECGs with multiple pathologies. We
provide an extensive comparison of the proposed model on various practical
downstream tasks, including transfer learning scenarios showing an area under
the receiver operating characteristic (AUROC) increase up to 2% surpassing
GAN-like competitors.","['eess.SP', 'cs.CV', 'cs.LG']",2502.20332," Many recent studies have found evidence for emergent reasoning capabilities
in large language models, but debate persists concerning the robustness of
these capabilities, and the extent to which they depend on structured reasoning
mechanisms. To shed light on these issues, we perform a comprehensive study of
the internal mechanisms that support abstract rule induction in an open-source
language model (Llama3-70B). We identify an emergent symbolic architecture that
implements abstract reasoning via a series of three computations. In early
layers, symbol abstraction heads convert input tokens to abstract variables
based on the relations between those tokens. In intermediate layers, symbolic
induction heads perform sequence induction over these abstract variables.
Finally, in later layers, retrieval heads predict the next token by retrieving
the value associated with the predicted abstract variable. These results point
toward a resolution of the longstanding debate between symbolic and neural
network approaches, suggesting that emergent reasoning in neural networks
depends on the emergence of symbolic mechanisms.","['cs.CL', 'cs.AI']",False,,,,"Conditional Electrocardiogram Generation Using Hierarchical Variational
  Autoencoders","Emergent Symbolic Mechanisms Support Abstract Reasoning in Large
  Language Models"
neg-d2-78,2025-01-21,,2501.12202," We present Hunyuan3D 2.0, an advanced large-scale 3D synthesis system for
generating high-resolution textured 3D assets. This system includes two
foundation components: a large-scale shape generation model -- Hunyuan3D-DiT,
and a large-scale texture synthesis model -- Hunyuan3D-Paint. The shape
generative model, built on a scalable flow-based diffusion transformer, aims to
create geometry that properly aligns with a given condition image, laying a
solid foundation for downstream applications. The texture synthesis model,
benefiting from strong geometric and diffusion priors, produces high-resolution
and vibrant texture maps for either generated or hand-crafted meshes.
Furthermore, we build Hunyuan3D-Studio -- a versatile, user-friendly production
platform that simplifies the re-creation process of 3D assets. It allows both
professional and amateur users to manipulate or even animate their meshes
efficiently. We systematically evaluate our models, showing that Hunyuan3D 2.0
outperforms previous state-of-the-art models, including the open-source models
and closed-source models in geometry details, condition alignment, texture
quality, and etc. Hunyuan3D 2.0 is publicly released in order to fill the gaps
in the open-source 3D community for large-scale foundation generative models.
The code and pre-trained weights of our models are available at:
https://github.com/Tencent/Hunyuan3D-2",['cs.CV'],2503.02292," Selecting the right monitoring level in Remote Patient Monitoring (RPM)
systems for e-healthcare is crucial for balancing patient outcomes, various
resources, and patient's quality of life. A prior work has used one-dimensional
health representations, but patient health is inherently multidimensional and
typically consists of many measurable physiological factors. In this paper, we
introduce a multidimensional health state model within the RPM framework and
use dynamic programming to study optimal monitoring strategies. Our analysis
reveals that the optimal control is characterized by switching curves (for
two-dimensional health states) or switching hyper-surfaces (in general):
patients switch to intensive monitoring when health measurements cross a
specific multidimensional surface. We further study how the optimal switching
curve varies for different medical conditions and model parameters. This
finding of the optimal control structure provides actionable insights for
clinicians and aids in resource planning. The tunable modeling framework
enhances the applicability and effectiveness of RPM services across various
medical conditions.","['eess.SY', 'cs.SY']",False,,,,"Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D
  Assets Generation","Optimal Control for Remote Patient Monitoring with Multidimensional
  Health States"
neg-d2-79,2025-03-10,,2503.07841," We present the first measurements with a new collinear laser spectroscopy
setup at the Argonne Tandem Linac Accelerator System utilizing its unique
capability to deliver neutron-rich refractory metal isotopes produced by the
spontaneous fission of 252Cf. We measured isotope shifts from optical spectra
for nine radioactive ruthenium isotopes 106-114Ru, reaching deep into the
mid-shell region. The extracted charge radii are in excellent agreement with
predictions from the Brussels-Skyrme-on-a-Grid models that account for the
triaxial deformation of nuclear ground states in this region. We show that
triaxial deformation impacts charge radii in models that feature shell effects,
in contrast to what could be concluded from a liquid drop analysis. This
indicates that this exotic type of deformation should not be neglected in
regions where it is known to occur, even if its presence cannot be
unambiguously inferred through laser spectroscopy.","['nucl-ex', 'nucl-th']",2502.10561," Landmarks are critical in navigation, supporting self-orientation and mental
model development. Similar to sighted people, people with low vision (PLV)
frequently look for landmarks via visual cues but face difficulties identifying
some important landmarks due to vision loss. We first conducted a formative
study with six PLV to characterize their challenges and strategies in landmark
selection, identifying their unique landmark categories (e.g., area
silhouettes, accessibility-related objects) and preferred landmark
augmentations. We then designed VisiMark, an AR interface that supports
landmark perception for PLV by providing both overviews of space structures and
in-situ landmark augmentations. We evaluated VisiMark with 16 PLV and found
that VisiMark enabled PLV to perceive landmarks they preferred but could not
easily perceive before, and changed PLV's landmark selection from only
visually-salient objects to cognitive landmarks that are more important and
meaningful. We further derive design considerations for AR-based landmark
augmentation systems for PLV.",['cs.HC'],False,,,,"Fingerprints of triaxiality in the charge radii of neutron-rich
  Ruthenium","VisiMark: Characterizing and Augmenting Landmarks for People with Low
  Vision in Augmented Reality to Support Indoor Navigation"
neg-d2-80,2025-03-14,,2503.11293," A novel mechanism for the production of a cosmic network of fundamental
superstrings based on a time-varying string tension has been recently proposed
in the context of a kinating background driven by the volume modulus of string
compactifications. In this paper, we generalise the analysis of this growth
mechanism by using dynamical system techniques. We first study the cosmological
growth of strings in a spatially-flat Universe filled with a perfect fluid and
a field-dependent tension, finding the fixed points of the phase space of this
system. We then apply this analysis to fundamental strings and EFT strings
obtained from wrapping $p$-branes on $(p-1)$-cycles. We find a cosmological
growth for fundamental strings even without kination, as in scaling fixed
points, and for EFT strings arising from D3- and NS5-branes wrapped around
fibration cycles.","['hep-th', 'astro-ph.CO', 'gr-qc', 'hep-ph']",2502.00858," Effective integration of AI agents into daily life requires them to
understand and adapt to individual human preferences, particularly in
collaborative roles. Although recent studies on embodied intelligence have
advanced significantly, they typically adopt generalized approaches that
overlook personal preferences in planning. We address this limitation by
developing agents that not only learn preferences from few demonstrations but
also learn to adapt their planning strategies based on these preferences. Our
research leverages the observation that preferences, though implicitly
expressed through minimal demonstrations, can generalize across diverse
planning scenarios. To systematically evaluate this hypothesis, we introduce
Preference-based Planning (PbP) benchmark, an embodied benchmark featuring
hundreds of diverse preferences spanning from atomic actions to complex
sequences. Our evaluation of SOTA methods reveals that while symbol-based
approaches show promise in scalability, significant challenges remain in
learning to generate and execute plans that satisfy personalized preferences.
We further demonstrate that incorporating learned preferences as intermediate
representations in planning significantly improves the agent's ability to
construct personalized plans. These findings establish preferences as a
valuable abstraction layer for adaptive planning, opening new directions for
research in preference-guided plan generation and execution.","['cs.AI', 'cs.HC']",False,,,,Growth of Cosmic Strings beyond Kination,Learning to Plan with Personalized Preferences
neg-d2-81,2025-02-02,,2502.01005," Cryogenic solid neon has recently emerged as a pristine solid host for single
electron qubits. At ~10 mK temperatures, electron-on-solid-neon (eNe) charge
qubits have exhibited exceptionally long coherence times and high operation
fidelities. To advance this platform towards a scalable quantum information
architecture, systematic characterization of its noise feature is imperative.
Here, we show the remarkable resilience of solid neon against charge and
thermal noises when eNe qubits are operated away from the charge-insensitive
sweet-spot and at elevated temperatures. Without optimizing neon growth, the
measured charge (voltage) noise on solid neon is already orders of magnitude
lower than that in most stringently grown semiconductors, rivaling the best
records to date. Up to 400 mK, the eNe charge qubits operated at ~5 GHz can
maintain their echo coherence times over 1 microsecond. These observations
highlight solid neon as an ideal host for quantum information processing at
higher temperatures and larger scales.","['quant-ph', 'cond-mat.mes-hall']",2503.1527," Recently, smart contracts have played a vital role in automatic financial and
business transactions. To help end users without programming background to
better understand the logic of smart contracts, previous studies have proposed
models for automatically translating smart contract source code into their
corresponding code summaries. However, in practice, only 13% of smart contracts
deployed on the Ethereum blockchain are associated with source code. The
practical usage of these existing tools is significantly restricted.
Considering that bytecode is always necessary when deploying smart contracts,
in this paper, we first introduce the task of automatically generating smart
contract code summaries from bytecode. We propose a novel approach, named
SmartBT (Smart contract Bytecode Translator) for automatically translating
smart contract bytecode into fine-grained natural language description
directly. Two key challenges are posed for this task: structural code logic
hidden in bytecode and the huge semantic gap between bytecode and natural
language descriptions. To address the first challenge, we transform bytecode
into CFG (Control-Flow Graph) to learn code structural and logic details.
Regarding the second challenge, we introduce an information retrieval component
to fetch similar comments for filling the semantic gap. Then the structural
input and semantic input are used to build an attentional sequence-to-sequence
neural network model. The copy mechanism is employed to copy rare words
directly from similar comments and the coverage mechanism is employed to
eliminate repetitive outputs. The automatic evaluation results show that
SmartBT outperforms a set of baselines by a large margin, and the human
evaluation results show the effectiveness and potential of SmartBT in producing
meaningful and accurate comments for smart contract code from bytecode
directly.",['cs.SE'],False,,,,Noise-resilient solid host for electron qubits above 100 mK,Automating Comment Generation for Smart Contract from Bytecode
neg-d2-82,2025-03-19,,2503.14946," Rising CO$_2$ emissions remain a critical global challenge, particularly in
middle-income countries where economic growth drives environmental degradation.
This study examines the long-run and short-run relationships between CO$_2$
emissions, energy use, GDP per capita, and population across 106 middle-income
countries from 1980 to 2023. Using a Panel Vector Error Correction Model
(VECM), we assess the impact of the Paris Agreement (2015) on emissions while
conducting cointegration tests to confirm long-run equilibrium relationships.
The findings reveal a strong long-run relationship among the variables, with
energy use as the dominant driver of emissions, while GDP per capita has a
moderate impact. However, the Paris Agreement has not significantly altered
emissions trends in middle-income economies. Granger causality tests indicate
that energy use strongly causes emissions, but GDP per capita and population do
not exhibit significant short-run causal effects. Variance decomposition
confirms that energy shocks have the most persistent effects, and impulse
response functions (IRFs) show emissions trajectories are primarily shaped by
economic activity rather than climate agreements. Robustness checks, including
autocorrelation tests, polynomial root stability, and Yamagata-Pesaran slope
homogeneity tests, validate model consistency. These results suggest that while
global agreements set emissions reduction goals, their effectiveness remains
limited without stronger national climate policies, sectoral energy reforms,
and financial incentives for clean energy adoption to ensure sustainable
economic growth.",['econ.EM'],2502.0853," We introduce \emph{residually dominated groups} in pure henselian valued
fields of equicharacteristic zero as an analogue of stably dominated groups
introduced by Hrushovski and Rideau-Kikuchi. We show that when $G$ is a
residually dominated group, there is a finite-to-one group homomorphism from
its connected component into a connected stably dominated group, and we study
the functoriality and universality properties of this map. Moreover, we prove
that there is a group homomorphism into a definable group in the residue field
that witnesses the residual domination. In our proofs, we use the results of
Montenegro, Onshuus, and Simon on groups definable in $\mathrm{NTP}_2$-theories
that extend the theory of fields. Along the way, we also provide an algebraic
characterization of residually dominated types, generalizing the work by Ealy,
Haskell and Simon for stably dominated types in algebraically closed valued
fields and study the properties of residually dominated types.",['math.LO'],False,,,,"Has the Paris Agreement Shaped Emission Trends? A Panel VECM Analysis of
  Energy, Growth, and CO$_2$ in 106 Middle-Income Countries","Residually Dominated Groups in Henselian Valued Fields of
  Equicharacteristic Zero"
neg-d2-83,2025-02-03,,2502.01983," We introduce a diagrammatic perspective for Shannon entropy created by the
first author and Mikhail Khovanov and connect it to information theory and
mutual information. We also give two complete proofs that the $5$-term
dilogarithm deforms to the $4$-term infinitesimal dilogarithm.","['math-ph', 'cs.IT', 'math.IT', 'math.MP']",2502.11843," Large Language Models (LLMs) are widely used as conversational agents,
exploiting their capabilities in various sectors such as education, law,
medicine, and more. However, LLMs are often subjected to context-shifting
behaviour, resulting in a lack of consistent and interpretable
personality-aligned interactions. Adherence to psychological traits lacks
comprehensive analysis, especially in the case of dyadic (pairwise)
conversations. We examine this challenge from two viewpoints, initially using
two conversation agents to generate a discourse on a certain topic with an
assigned personality from the OCEAN framework (Openness, Conscientiousness,
Extraversion, Agreeableness, and Neuroticism) as High/Low for each trait. This
is followed by using multiple judge agents to infer the original traits
assigned to explore prediction consistency, inter-model agreement, and
alignment with the assigned personality. Our findings indicate that while LLMs
can be guided toward personality-driven dialogue, their ability to maintain
personality traits varies significantly depending on the combination of models
and discourse settings. These inconsistencies emphasise the challenges in
achieving stable and interpretable personality-aligned interactions in LLMs.","['cs.CL', 'cs.AI', 'cs.SI']",False,,,,Diagrammatics of information,Can LLM Agents Maintain a Persona in Discourse?
neg-d2-84,2025-01-01,,2501.00873," Capitalizing on the complementary advantages of generative and discriminative
models has always been a compelling vision in machine learning, backed by a
growing body of research. This work discloses the hidden semantic structure
within score-based generative models, unveiling their potential as effective
discriminative priors. Inspired by our theoretical findings, we propose DUSA to
exploit the structured semantic priors underlying diffusion score to facilitate
the test-time adaptation of image classifiers or dense predictors. Notably,
DUSA extracts knowledge from a single timestep of denoising diffusion, lifting
the curse of Monte Carlo-based likelihood estimation over timesteps. We
demonstrate the efficacy of our DUSA in adapting a wide variety of competitive
pre-trained discriminative models on diverse test-time scenarios. Additionally,
a thorough ablation study is conducted to dissect the pivotal elements in DUSA.
Code is publicly available at https://github.com/BIT-DA/DUSA.","['cs.CV', 'cs.LG']",2501.03038," Automatic Music Transcription (AMT), aiming to get musical notes from raw
audio, typically uses frame-level systems with piano-roll outputs or language
model (LM)-based systems with note-level predictions. However, frame-level
systems require manual thresholding, while the LM-based systems struggle with
long sequences. In this paper, we propose a hybrid method combining pre-trained
roll-based encoders with an LM decoder to leverage the strengths of both
methods. Besides, our approach employs a hierarchical prediction strategy,
first predicting onset and pitch, then velocity, and finally offset. The
hierarchical prediction strategy reduces computational costs by breaking down
long sequences into different hierarchies. Evaluated on two benchmark
roll-based encoders, our method outperforms traditional piano-roll outputs 0.01
and 0.022 in onset-offset-velocity F1 score, demonstrating its potential as a
performance-enhancing plug-in for arbitrary roll-based music transcription
encoder.","['cs.SD', 'cs.AI', 'cs.LG', 'eess.AS']",False,,,,"Exploring Structured Semantic Priors Underlying Diffusion Score for
  Test-time Adaptation","Piano Transcription by Hierarchical Language Modeling with Pretrained
  Roll-based Encoders"
neg-d2-85,2025-02-19,,2502.13911," In recent years, ultra-high dose rate (FLASH) radiotherapy has become a novel
cancer treatment technique because of its similar tumor-killing efficacy as
conventional particle therapy while significantly protecting normal tissues.
However, due to the limitation of particle number, achieving FLASH condition in
a compact heavy-ion synchrotron requires a short extraction time of tens of
milliseconds, which is challenging for the conventional RF-KO method. To tackle
this challenge, we introduce autoresonance into the third-order resonant
extraction for the first time, offering an alternative to the conventional
approach of merely increasing the excitation strength. By leveraging a strong
detuning effect, a frequency sweeping excitation with small amplitude can drive
the entire beam into the autoresonant state, thus enabling rapid beam
extraction within a single sweeping period. Compared with the conventional
method, this innovative method requires only the addition of an octupole
magnet. At the same time, it shows that the conventional RF-KO method has a
high autoresonance threshold, so that only a small number of particles that
meet the threshold can be excited to large amplitude and be extracted in each
sweeping period. In this paper, the autoresonance threshold of a particle in
the presence of sextupole and octupole magnetic fields is analyzed, and the
single particle simulation shows good agreement with the theoretical formula.
Furthermore, the autoresonance based rapid extraction process is simulated and
studied, revealing the possibility of millisecond scale beam extraction.","['physics.acc-ph', 'physics.med-ph']",2501.06813," Subset selection is a fundamental problem in combinatorial optimization,
which has a wide range of applications such as influence maximization and
sparse regression. The goal is to select a subset of limited size from a ground
set in order to maximize a given objective function. However, the evaluation of
the objective function in real-world scenarios is often noisy. Previous
algorithms, including the greedy algorithm and multi-objective evolutionary
algorithms POSS and PONSS, either struggle in noisy environments or consume
excessive computational resources. In this paper, we focus on the noisy subset
selection problem with a cardinality constraint, where the evaluation of a
subset is noisy. We propose a novel approach based on Pareto Optimization with
Robust Evaluation for noisy subset selection (PORE), which maximizes a robust
evaluation function and minimizes the subset size simultaneously. PORE can
efficiently identify well-structured solutions and handle computational
resources, addressing the limitations observed in PONSS. Our experiments,
conducted on real-world datasets for influence maximization and sparse
regression, demonstrate that PORE significantly outperforms previous methods,
including the classical greedy algorithm, POSS, and PONSS. Further validation
through ablation studies confirms the effectiveness of our robust evaluation
function.",['cs.NE'],False,,,,Application of autoresonance in rapid beam extraction of synchrotrons,Pareto Optimization with Robust Evaluation for Noisy Subset Selection
neg-d2-86,2025-03-07,,2503.05214," The growing rate of chronic wound occurrence, especially in patients with
diabetes, has become a concerning trend in recent years. Chronic wounds are
difficult and costly to treat, and have become a serious burden on health care
systems worldwide. Chronic wounds can have devastating consequences for the
patient, with infection often leading to reduced quality of life and increased
mortality risk. Innovative deep learning methods for the detection and
monitoring of such wounds have the potential to reduce the impact to both
patient and clinician. We present a novel multimodal segmentation method which
allows for the introduction of patient metadata into the training workflow
whereby the patient data are expressed as Gaussian random fields. Our results
indicate that the proposed method improved performance when utilising multiple
models, each trained on different metadata categories. Using the Diabetic Foot
Ulcer Challenge 2022 test set, when compared to the baseline results
(intersection over union = 0.4670, Dice similarity coefficient = 0.5908) we
demonstrate improvements of +0.0220 and +0.0229 for intersection over union and
Dice similarity coefficient respectively. This paper presents the first study
to focus on integrating patient data into a chronic wound segmentation
workflow. Our results show significant performance gains when training
individual models using specific metadata categories, followed by average
merging of prediction masks using distance transforms. All source code for this
study is available at:
https://github.com/mmu-dermatology-research/multimodal-grf","['eess.IV', 'cs.CV']",2502.14673," Deploying ASR models at an industrial scale poses significant challenges in
hardware resource management, especially for long-form transcription tasks
where audio may last for hours. Large Conformer models, despite their
capabilities, are limited to processing only 15 minutes of audio on an 80GB
GPU. Furthermore, variable input lengths worsen inefficiencies, as standard
batching leads to excessive padding, increasing resource consumption and
execution time. To address this, we introduce ChunkFormer, an efficient ASR
model that uses chunk-wise processing with relative right context, enabling
long audio transcriptions on low-memory GPUs. ChunkFormer handles up to 16
hours of audio on an 80GB GPU, 1.5x longer than the current state-of-the-art
FastConformer, while also boosting long-form transcription performance with up
to 7.7% absolute reduction on word error rate and maintaining accuracy on
shorter tasks compared to Conformer. By eliminating the need for padding in
standard batching, ChunkFormer's masked batching technique reduces execution
time and memory usage by more than 3x in batch processing, substantially
reducing costs for a wide range of ASR systems, particularly regarding GPU
resources for models serving in real-world applications.","['cs.SD', 'eess.AS']",False,,,,"Gaussian Random Fields as an Abstract Representation of Patient Metadata
  for Multimodal Medical Image Segmentation","ChunkFormer: Masked Chunking Conformer For Long-Form Speech
  Transcription"
neg-d2-87,2025-02-21,,2502.15633," 3D Gaussian Splatting (3DGS) has become a popular solution in SLAM, as it can
produce high-fidelity novel views. However, previous GS-based methods primarily
target indoor scenes and rely on RGB-D sensors or pre-trained depth estimation
models, hence underperforming in outdoor scenarios. To address this issue, we
propose a RGB-only gaussian splatting SLAM method for unbounded outdoor
scenes--OpenGS-SLAM. Technically, we first employ a pointmap regression network
to generate consistent pointmaps between frames for pose estimation. Compared
to commonly used depth maps, pointmaps include spatial relationships and scene
geometry across multiple views, enabling robust camera pose estimation. Then,
we propose integrating the estimated camera poses with 3DGS rendering as an
end-to-end differentiable pipeline. Our method achieves simultaneous
optimization of camera poses and 3DGS scene parameters, significantly enhancing
system tracking accuracy. Specifically, we also design an adaptive scale mapper
for the pointmap regression network, which provides more accurate pointmap
mapping to the 3DGS map representation. Our experiments on the Waymo dataset
demonstrate that OpenGS-SLAM reduces tracking error to 9.8\% of previous 3DGS
methods, and achieves state-of-the-art results in novel view synthesis. Project
Page: https://3dagentworld.github.io/opengs-slam/",['cs.CV'],2501.03578," We theoretically propose a circuit of the four-body coupler for
superconducting qubits based on Josephson parametric oscillators (JPOs). Our
coupler for the four-body interaction has a superconducting loop, similar to a
capacitively shunted flux qubit, where an external magnetic flux set to half a
flux quantum is threaded. This coupler circuit is a specific setup of the
circuit called superconducting nonlinear asymmetric inductive elements (SNAIL)
and also is a generalization of the previously proposed one for the four-body
interaction of JPOs. We clarify roles of circuit parameters in the four-body
interaction and, in particular, show that the four-body coupling constant in
our circuit can be significantly increased by tuning capacitance of the coupler
or the area ratio of the Josephson junctions of the coupler.",['quant-ph'],False,,,,RGB-Only Gaussian Splatting SLAM for Unbounded Outdoor Scenes,"Four-body coupler for superconducting qubits based on Josephson
  parametric oscillators"
neg-d2-88,2025-01-07,,2501.03864," Consider the nonlinear stochastic heat equation $$
  \frac{\partial u (t,x)}{\partial t}=\frac{\partial^2 u (t,x)}{\partial x^2}+
\sigma(u (t,x))\dot{W}(t,x),\quad t> 0,\,
  x\in \mathbb{R}, $$ where $\dot W$ is a Gaussian noise which is white in time
and has the covariance of a fractional Brownian motion with Hurst parameter
$H\in(\frac 14,\frac 12)$ in the space variable. When $\sigma(0)=0$, the
well-posedness of the solution and its H\""older continuity have been proved by
Hu et al. \cite{HHLNT2017}. In this paper, we study the asymptotic properties
of the temporal gradient $u(t+\varepsilon, x)-u(t, x)$ at any fixed $t \ge 0$
and $x\in \mathbb R$, as $\varepsilon\downarrow 0$. As applications, we deduce
Khintchine's law of iterated logarithm, Chung's law of iterated logarithm, and
a result on the $q$-variations of the temporal process $\{u(t, x)\}_{t \ge 0}$,
where $x\in \mathbb R$ is fixed.",['math.PR'],2501.0571," Recent research shows that emotions can enhance users' cognition and
influence information communication. While research on visual emotion analysis
is extensive, limited work has been done on helping users generate emotionally
rich image content. Existing work on emotional image generation relies on
discrete emotion categories, making it challenging to capture complex and
subtle emotional nuances accurately. Additionally, these methods struggle to
control the specific content of generated images based on text prompts. In this
work, we introduce the new task of continuous emotional image content
generation (C-EICG) and present EmotiCrafter, an emotional image generation
model that generates images based on text prompts and Valence-Arousal values.
Specifically, we propose a novel emotion-embedding mapping network that embeds
Valence-Arousal values into textual features, enabling the capture of specific
emotions in alignment with intended input prompts. Additionally, we introduce a
loss function to enhance emotion expression. The experimental results show that
our method effectively generates images representing specific emotions with the
desired content and outperforms existing techniques.",['cs.CV'],False,,,,"Temporal regularity for the stochastic heat equation with rough
  dependence in space","EmotiCrafter: Text-to-Emotional-Image Generation based on
  Valence-Arousal Model"
neg-d2-89,2025-02-10,,2502.06678," In this paper, we study a variant of best-arm identification involving
elements of risk sensitivity and communication constraints. Specifically, the
goal of the learner is to identify the arm with the highest quantile reward,
while the communication from an agent (who observes rewards) and the learner
(who chooses actions) is restricted to only one bit of feedback per arm pull.
We propose an algorithm that utilizes noisy binary search as a subroutine,
allowing the learner to estimate quantile rewards through 1-bit feedback. We
derive an instance-dependent upper bound on the sample complexity of our
algorithm and provide an algorithm-independent lower bound for specific
instances, with the two matching to within logarithmic factors under mild
conditions, or even to within constant factors in certain low error probability
scaling regimes. The lower bound is applicable even in the absence of
communication constraints, and thus we conclude that restricting to 1-bit
feedback has a minimal impact on the scaling of the sample complexity.","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT']",2503.04028," Stress-stress correlations in crystalline solids with long-range order can be
straightforwardly derived using elasticity theory. In contrast, the `emergent
elasticity' of amorphous solids, rigid materials characterized by an underlying
disordered structure, defies direct explanation within traditional theoretical
frameworks. To address this challenge, tensor gauge theories have been recently
proposed as a promising approach to describe the emergent elasticity of
disordered solids and predict their stress-stress correlations. In this work,
we revisit this problem in two-dimensional amorphous and crystalline solids by
employing a canonical elasticity theory approach, supported by experimental and
simulation data. We demonstrate that, with respect to static stress-stress
correlations, the response of a 2D disordered solid is indistinguishable from
that of a 2D isotropic crystalline solid and it is well predicted by vanilla
elasticity theory. Moreover, we show that the presence of pinch-point
singularities in the stress response is not an exclusive feature of amorphous
solids. Our results confirm previous observations about the universal character
of static stress-stress correlations in crystalline and amorphous packings.","['cond-mat.soft', 'cond-mat.mtrl-sci', 'cond-mat.stat-mech']",False,,,,Quantile Multi-Armed Bandits with 1-bit Feedback,"Stress-stress correlations in two-dimensional amorphous and crystalline
  solids"
neg-d2-90,2025-01-16,,2501.0969," We revisit the theory of operator-valued free convolution powers given by a
completely positive map $\eta$. We first give a general result, with a new
analytic proof, that the $\eta$-convolution power of the law of $X$ is realized
by $V^*XV$ for any operator $V$ satisfying certain conditions, which unifies
Nica and Speicher's construction in the scalar-valued setting and
Shlyakhtenko's construction in the operator-valued setting. Second, we provide
an analog, for the setting of $\eta$-valued convolution powers, of the analytic
subordination for conditional expectations that holds for additive free
convolution. Finally, we describe a Hilbert-space manipulation that explains
the equivalence between the $n$-fold additive free convolution and the
convolution power with respect to $\eta = n \operatorname{id}$.",['math.OA'],2502.12624," Conversational repair is a mechanism used to detect and resolve
miscommunication and misinformation problems when two or more agents interact.
One particular and underexplored form of repair in emergent communication is
the implicit repair mechanism, where the interlocutor purposely conveys the
desired information in such a way as to prevent misinformation from any other
interlocutor. This work explores how redundancy can modify the emergent
communication protocol to continue conveying the necessary information to
complete the underlying task, even with additional external environmental
pressures such as noise. We focus on extending the signaling game, called the
Lewis Game, by adding noise in the communication channel and inputs received by
the agents. Our analysis shows that agents add redundancy to the transmitted
messages as an outcome to prevent the negative impact of noise on the task
success. Additionally, we observe that the emerging communication protocol's
generalization capabilities remain equivalent to architectures employed in
simpler games that are entirely deterministic. Additionally, our method is the
only one suitable for producing robust communication protocols that can handle
cases with and without noise while maintaining increased generalization
performance levels.","['cs.LG', 'cs.MA']",False,,,,"Operator models and analytic subordination for operator-valued free
  convolution powers",Implicit Repair with Reinforcement Learning in Emergent Communication
neg-d2-91,2025-01-15,,2501.08689," Labeled transition systems can be a great way to visualize the complex
behavior of parallel and communicating systems. However, if, during a
particular timeframe, no synchronization or communication between processes
occurs, then multiple parallel sequences of actions are able to interleave
arbitrarily, and the resulting graph quickly becomes too complex for the human
eye to understand easily. With that in mind, we propose an exact formalization
of these arbitrary interleavings, and an algorithm to find all said
interleavings in deterministic LTSs, to reduce the visual complexity of labeled
transition systems.",['cs.FL'],2502.14381," dtaianomaly is an open-source Python library for time series anomaly
detection, designed to bridge the gap between academic research and real-world
applications. Our goal is to (1) accelerate the development of novel
state-of-the-art anomaly detection techniques through simple extensibility; (2)
offer functionality for large-scale experimental validation; and thereby (3)
bring cutting-edge research to business and industry through a standardized
API, similar to scikit-learn to lower the entry barrier for both new and
experienced users. Besides these key features, dtaianomaly offers (1) a broad
range of built-in anomaly detectors, (2) support for time series preprocessing,
(3) tools for visual analysis, (4) confidence prediction of anomaly scores, (5)
runtime and memory profiling, (6) comprehensive documentation, and (7)
cross-platform unit testing.
  The source code of dtaianomaly, documentation, code examples and installation
guides are publicly available at https://github.com/ML-KULeuven/dtaianomaly.","['cs.LG', 'cs.DB']",False,,,,Mining Diamonds in labeled Transition Systems,dtaianomaly: A Python library for time series anomaly detection
neg-d2-92,2025-02-09,,2502.06135," We analyze the phase structure of 2d lattice CP(1) model with $\theta$ term
by using the bond-weighted tensor renormalization group method. We propose a
new tensor network representation for the model using the quadrature scheme and
confirm that its accuracy is better than that of the conventional
character-like expansion. As a probe to study the phase structure, we adopt the
central charge and the scaling dimensions. The numerical results indicate an
existence of critical point at $\theta=\pi$, which is consistent with the
Haldane's conjecture.",['hep-lat'],2501.03361," We report the results from a study of two massive ($M_{500c} > 6.0 \times
10^{14} M_{\odot}$) strong lensing clusters selected from the South Pole
Telescope cluster survey for their high Einstein radius ($R_E > 40''$),
SPT-CLJ2325$-$4111 and SPT-CLJ0049$-$2440. Ground-based and shallow HST imaging
indicated extensive strong lensing evidence in these fields, with giant arcs
spanning 18\arcsec\ and 31\arcsec, respectively, motivating further space-based
imaging followup. Here, we present multiband HST imaging and ground-based
Magellan spectroscopy of the fields, from which we compile detailed strong
lensing models. The lens models of SPT-CL\,J2325$-$4111 and
SPT-CL\,J0049$-$2440 were optimized using 9, and 8 secure multiple-imaged
systems with a final image-plane rms of 0\farcs63 and 0\farcs73, respectively.
From the lensing analysis, we measure the projected mass density within 500~kpc
of $M(<500 ~{\rm kpc}) = 7.30\pm0.07 \times 10^{14}$$M_{\odot}$, and $M(<500
~{\rm kpc})=7.12^{+0.16}_{-0.19}\times 10^{14}$ $M_{\odot}$ for these two
clusters, and a sub-halos mass ratio of $0.12\pm{0.01}$ and
$0.21^{+0.07}_{-0.05}$, respectively. Both clusters produce a large area with
high magnification ($\mu\geq 3$) for a source at $z=9$, $A^{lens}_{| \mu | \geq
3 }=4.93^{+0.03}_{-0.04} arcmin^2$, and $A^{lens}_{| \mu | \geq 3
}=3.64^{+0.14}_{-0.10} arcmin^2$ respectively, placing them in the top tier of
strong lensing clusters. We conclude that these clusters are spectacular
sightlines for further observations that will reduce the systematic
uncertainties due to cosmic variance. This paper provides the community with
two additional well-calibrated cosmic telescopes, as strong as the Frontier
Fields, suitable for studies of the highly magnified background Universe.","['astro-ph.GA', 'astro-ph.CO']",False,,,,"Phase structure analysis of CP(1) model with $\theta$ term by tensor
  renormalization group","Strong Lensing analysis of SPT-CLJ2325$-$4111 and SPT-CLJ0049$-$2440,
  two Powerful Cosmic Telescopes ($R_E > 40''$) from the SPT Clusters Sample"
neg-d2-93,2025-01-06,,2501.03361," We report the results from a study of two massive ($M_{500c} > 6.0 \times
10^{14} M_{\odot}$) strong lensing clusters selected from the South Pole
Telescope cluster survey for their high Einstein radius ($R_E > 40''$),
SPT-CLJ2325$-$4111 and SPT-CLJ0049$-$2440. Ground-based and shallow HST imaging
indicated extensive strong lensing evidence in these fields, with giant arcs
spanning 18\arcsec\ and 31\arcsec, respectively, motivating further space-based
imaging followup. Here, we present multiband HST imaging and ground-based
Magellan spectroscopy of the fields, from which we compile detailed strong
lensing models. The lens models of SPT-CL\,J2325$-$4111 and
SPT-CL\,J0049$-$2440 were optimized using 9, and 8 secure multiple-imaged
systems with a final image-plane rms of 0\farcs63 and 0\farcs73, respectively.
From the lensing analysis, we measure the projected mass density within 500~kpc
of $M(<500 ~{\rm kpc}) = 7.30\pm0.07 \times 10^{14}$$M_{\odot}$, and $M(<500
~{\rm kpc})=7.12^{+0.16}_{-0.19}\times 10^{14}$ $M_{\odot}$ for these two
clusters, and a sub-halos mass ratio of $0.12\pm{0.01}$ and
$0.21^{+0.07}_{-0.05}$, respectively. Both clusters produce a large area with
high magnification ($\mu\geq 3$) for a source at $z=9$, $A^{lens}_{| \mu | \geq
3 }=4.93^{+0.03}_{-0.04} arcmin^2$, and $A^{lens}_{| \mu | \geq 3
}=3.64^{+0.14}_{-0.10} arcmin^2$ respectively, placing them in the top tier of
strong lensing clusters. We conclude that these clusters are spectacular
sightlines for further observations that will reduce the systematic
uncertainties due to cosmic variance. This paper provides the community with
two additional well-calibrated cosmic telescopes, as strong as the Frontier
Fields, suitable for studies of the highly magnified background Universe.","['astro-ph.GA', 'astro-ph.CO']",2502.12408," Recent advances in speech foundation models are largely driven by scaling
both model size and data, enabling them to perform a wide range of tasks,
including speech recognition. Traditionally, ASR models are evaluated using
metrics like Word Error Rate (WER) and Character Error Rate (CER), which depend
on ground truth labels. As a result of limited labeled data from diverse
domains and testing conditions, the true generalization capabilities of these
models beyond standard benchmarks remain unclear. Moreover, labeling data is
both costly and time-consuming. To address this, we propose a novel label-free
approach for approximating ASR performance metrics, eliminating the need for
ground truth labels. Our method utilizes multimodal embeddings in a unified
space for speech and transcription representations, combined with a
high-quality proxy model to compute proxy metrics. These features are used to
train a regression model to predict key ASR metrics like Word Error Rate (WER)
and Character Error Rate (CER). We experiment with over 40 models across 14
datasets representing both standard and in-the-wild testing conditions. Our
results show that we approximate the metrics within a single-digit absolute
difference across all experimental configurations, outperforming the most
recent baseline by more than 50\%.",['cs.CL'],False,,,,"Strong Lensing analysis of SPT-CLJ2325$-$4111 and SPT-CLJ0049$-$2440,
  two Powerful Cosmic Telescopes ($R_E > 40''$) from the SPT Clusters Sample",On the Robust Approximation of ASR Metrics
neg-d2-94,2025-01-20,,2501.11761," We reveal a nonlinear magnetic dynamo in a Taylor-Couette flow at small
magnetic Prandtl numbers $Pm\leq 1$, which has been previously believed to
exist only at higher $Pm\gtrsim 10$ in this flow. Both the amplitude of initial
perturbations and $Pm$ play a critical role in its onset and evolution. It is
shown that this dynamo exists in two main states -- a weak state dominated by
large-scale modes and a strong, more turbulent state with higher amplitude
dominated by small-scale modes. These findings can be important for dynamo
processes in many astrophysical settings with small $Pm$.","['physics.flu-dyn', 'astro-ph.EP', 'astro-ph.SR', 'physics.plasm-ph']",2501.08106," The method of H- photoionization is interesting for laser assisted charge
exchange injection. In this paper, the model and computation of photoionization
of negative hydrogen ion by using strong lasers is considered. The development
of this work is motivated by using pure lasers for photodetachment of electron
from negative hydrogen ion when it is not convenient or not possible to use
stripping magnet. Herein we develop a method of calculation of high efficiency
photoionization using time dependent wave equation with application of powerful
lasers. We compare this precise method of calculation with simplified method of
calculation through linear model of cross section interaction. Another
mechanism of photodetachment through excitation of the Feshbach resonance is
also considered.","['physics.atom-ph', 'physics.acc-ph']",False,,,,Strong and weak dynamo regimes in Taylor-Couette flows,Photodetachment of negative hydrogen ion beam
neg-d2-95,2025-02-21,,2502.15299," Double-peaked narrow emission lines (DPNELs) might be evidence for the
existence of kpc-scale dual AGNs. There are so far large samples of objects
with DPNELs in narrow emission line galaxies. Here, a systematic search is made
to build a sample of type 1 AGNs with double-peaked [O~{\sc~iii}] from Data
Release 16 of the Sloan Digital Sky Survey (SDSS). Through visually inspecting
and fitting [O~{\sc~iii}], fitting broad H$\alpha$ emission lines, performing
F-test for [O~{\sc~iii}] profiles, and checking broad H$\beta$ and
[O~{\sc~iii}] emission lines, we select 62 type 1 AGNs with reliable
double-peaked [O~{\sc~iii}] from 11557 QSOs with z < 0.3. After visually
checking the 62 SDSS multi-color images, we find only seven objects with signs
of merging. Four possible models for the double-peaked [O~{\sc~iii}] observed
in our sample are discussed: the superposition model, AGN outflow model, dual
AGN model, and rotating disk model. However, the current results can not
provide any one explanation conclusively, and additional observational data are
needed to provide the details of narrow line regions. But at least 22 objects
with different velocity offsets between double-peaked [O~{\sc~iii}] and narrow
H$\alpha$ emission lines could be excluded as dual AGN candidates. The relative
velocity offsets of the [O~{\sc~iii}] blue-shifted/red-shifted components are
negative to their line flux ratios, which is consistent with dual AGN model.
This work provides a new sample of 62 type 1 AGNs with double-peaked
[O~{\sc~iii}] for further study.",['astro-ph.GA'],2501.0854," The task of building semantics for structured data such as CSV, JSON, and XML
files is highly relevant in the knowledge representation field. Even though we
have a vast of structured data on the internet, mapping them to domain
ontologies to build semantics for them is still very challenging as it requires
the construction model to understand and learn graph-structured knowledge.
Otherwise, the task will require human beings' effort and cost. In this paper,
we proposed a novel automatic semantic modeling framework: Knowledge Prompt
Chaining. It can serialize the graph-structured knowledge and inject it into
the LLMs properly in a Prompt Chaining architecture. Through this knowledge
injection and prompting chaining, the model in our framework can learn the
structure information and latent space of the graph and generate the semantic
labels and semantic graphs following the chains' insturction naturally. Based
on experimental results, our method achieves better performance than existing
leading techniques, despite using reduced structured input data.","['cs.CL', 'cs.AI', 'cs.DB']",False,,,,"On type 1 active galactic nuclei with double-peaked [O~{\sc iii}]. I.
  data sample and basic results",Knowledge prompt chaining for semantic modeling
neg-d2-96,2025-01-09,,2501.05241," Late gadolinium enhancement MRI (LGE MRI) is the gold standard for the
detection of myocardial scars for post myocardial infarction (MI). LGE MRI
requires the injection of a contrast agent, which carries potential side
effects and increases scanning time and patient discomfort. To address these
issues, we propose a novel framework that combines cardiac motion observed in
cine MRI with image texture information to segment the myocardium and scar
tissue in the left ventricle. Cardiac motion tracking can be formulated as a
full cardiac image cycle registration problem, which can be solved via deep
neural networks. Experimental results prove that the proposed method can
achieve scar segmentation based on non-contrasted cine images with comparable
accuracy to LGE MRI. This demonstrates its potential as an alternative to
contrast-enhanced techniques for scar detection.","['eess.IV', 'cs.CV']",2501.03361," We report the results from a study of two massive ($M_{500c} > 6.0 \times
10^{14} M_{\odot}$) strong lensing clusters selected from the South Pole
Telescope cluster survey for their high Einstein radius ($R_E > 40''$),
SPT-CLJ2325$-$4111 and SPT-CLJ0049$-$2440. Ground-based and shallow HST imaging
indicated extensive strong lensing evidence in these fields, with giant arcs
spanning 18\arcsec\ and 31\arcsec, respectively, motivating further space-based
imaging followup. Here, we present multiband HST imaging and ground-based
Magellan spectroscopy of the fields, from which we compile detailed strong
lensing models. The lens models of SPT-CL\,J2325$-$4111 and
SPT-CL\,J0049$-$2440 were optimized using 9, and 8 secure multiple-imaged
systems with a final image-plane rms of 0\farcs63 and 0\farcs73, respectively.
From the lensing analysis, we measure the projected mass density within 500~kpc
of $M(<500 ~{\rm kpc}) = 7.30\pm0.07 \times 10^{14}$$M_{\odot}$, and $M(<500
~{\rm kpc})=7.12^{+0.16}_{-0.19}\times 10^{14}$ $M_{\odot}$ for these two
clusters, and a sub-halos mass ratio of $0.12\pm{0.01}$ and
$0.21^{+0.07}_{-0.05}$, respectively. Both clusters produce a large area with
high magnification ($\mu\geq 3$) for a source at $z=9$, $A^{lens}_{| \mu | \geq
3 }=4.93^{+0.03}_{-0.04} arcmin^2$, and $A^{lens}_{| \mu | \geq 3
}=3.64^{+0.14}_{-0.10} arcmin^2$ respectively, placing them in the top tier of
strong lensing clusters. We conclude that these clusters are spectacular
sightlines for further observations that will reduce the systematic
uncertainties due to cosmic variance. This paper provides the community with
two additional well-calibrated cosmic telescopes, as strong as the Frontier
Fields, suitable for studies of the highly magnified background Universe.","['astro-ph.GA', 'astro-ph.CO']",False,,,,"Contrast-Free Myocardial Scar Segmentation in Cine MRI using Motion and
  Texture Fusion","Strong Lensing analysis of SPT-CLJ2325$-$4111 and SPT-CLJ0049$-$2440,
  two Powerful Cosmic Telescopes ($R_E > 40''$) from the SPT Clusters Sample"
neg-d2-97,2025-02-07,,2502.04879," As platforms increasingly rely on learning algorithms, collectives may form
and seek ways to influence these platforms to align with their own interests.
This can be achieved by coordinated submission of altered data. To evaluate the
potential impact of such behavior, it is essential to understand the
computations that collectives must perform to impact platforms in this way. In
particular, collectives need to make a priori assessments of the effect of the
collective before taking action, as they may face potential risks when
modifying their data. Moreover they need to develop implementable coordination
algorithms based on quantities that can be inferred from observed data. We
develop a framework that provides a theoretical and algorithmic treatment of
these issues and present experimental results in a product evaluation domain.","['stat.ML', 'cs.LG']",2501.11126," Multi-antenna coded caching (CC) with multicast beamforming typically relies
on a complex successive interference cancellation (SIC) structure to decode a
superposition of multiple streams received by each user. Signal-level CC
schemes require the regeneration and cancellation of interfering signals at the
physical layer of each receiver, which complicates practical implementations.
To address this, we propose a bit-level multicast scheduling scheme enabling
linear, SIC-free decoding of parallel streams by repeatedly transmitting data
terms with linearly independent coefficients. Two reference strategies and a
novel sparse strategy are considered for constructing the coefficient matrix.
The reference cases include the random strategy, which lacks control over
matrix construction, and the equal-distant strategy, which balances users'
interference and data terms equally. In contrast, the sparse strategy minimizes
the number of multicast streams transmitted in parallel during each interval.
This approach simplifies both the decoding process and the beamforming design
by decoupling the desired data terms for each user and reducing the number of
SINR constraints, respectively. To further enhance the symmetric rate, a
successive projection algorithm is applied to exploit channel properties and
optimize user ordering. With the coefficient matrix and optimized user ordering
in place, multicast beamformers are devised to aggregate desired data from
relevant multicast streams. Numerical simulations validate the effectiveness of
the sparse strategy and user scheduling, demonstrating significant gains in
symmetric rate.","['cs.IT', 'math.IT']",False,,,,Statistical Collusion by Collectives on Learning Platforms,SIC-free Multicast Scheduling for Multi-antenna Coded Caching
neg-d2-98,2025-02-20,,2502.14368," The dynamics of soliton molecules in ultrafast fiber ring laser cavity is
strongly influenced by noise. We show how a parsimonious Langevin model can be
constructed from experimental data, resulting in a mathematical description
that encompasses both the deterministic and stochastic properties of the
evolution of the soliton molecules. In particular, we were able to probe the
response dynamics of the soliton molecule to an external kick in a sub-critical
approach, namely without the need to actually disturb the systems under
investigation. Moreover, the noise experienced by the dissipative solitonic
system, including its distribution and correlation, can now be also analyzed in
details. Our strategy can be applied to any systems where the individual motion
of its constitutive particles can be traced; the case of optical
solitonic-system laser presented here serving as a proof-of-principle
demonstration.","['physics.optics', 'math-ph', 'math.MP']",2501.13616," Altermagnetism has attracted considerable attention for its remarkable
combination of spin-polarized band structures and zero net magnetization,
making it a promising candidate for spintronics applications. We demonstrate
that this magnetic phase represents a case of ``unconventional magnetism,""
first proposed nearly two decades ago by one of the present authors as part of
a broader framework for understanding Landau-Pomeranchuk instabilities in the
spin channel, driven by many-body interactions. By systematically analyzing the
altermagnetism in RuO$_2$ with first-principles calculations, we reconcile
conflicting experimental and theoretical reports by attributing it to RuO$_2$'s
proximity to a quantum phase transition. We emphasize the critical role of
tuning parameters, such as the Hubbard $U$, hole doping, and epitaxial strain,
in modulating quasiparticle interactions near the Fermi surface. This work
provides fresh insights into the origin and tunability of altermagnetism in
RuO$_2$, highlighting its potential as a platform for investigating quantum
phase transitions and the broader realm of unconventional magnetism.",['cond-mat.mtrl-sci'],False,,,,"Langevin model for soliton molecules in ultrafast fiber ring laser
  cavity: investigating experimentally the interplay between noise and inertia","Fragile Unconventional Magnetism in RuO$_2$ by Proximity to
  Landau-Pomeranchuk Instability"
neg-d2-99,2025-01-29,,2501.17484," We present a method for solving a large-scale stochastic capacity expansion
problem which explicitly considers reliability constraints, in particular
constraints on expected energy not served. Our method tackles this problem by a
Lagrange relaxation of the expected energy not served constraints. We solve the
relaxed formulation in an iterative manner, using a subgradient-based method.
Each iteration requires the solution of a stochastic capacity expansion
problem, for which we implement a subgradient decomposition scheme in a
high-performance computing infrastructure. We apply the proposed methodology on
the Economic Viability Assessment model that is used by ENTSO-E in the annual
European Resource Adequacy Assessment, extended to include explicit reliability
constraints. The approach is able to solve this model achieving a 1.3%
optimality gap. We compare our approach against accounting for reliability
through penalizing load shedding at VOLL, and find that the former results in
1.6% savings in total cost. We are also able to quantify the cost savings from
allowing some load curtailment in the capacity planning process, which ranges
from 1.6 to 6% in the cases analyzed.","['eess.SY', 'cs.SY']",2502.10682," Effective deepfake detection tools are becoming increasingly essential over
the last few years due to the growing usage of deepfakes in unethical
practices. There exists a diverse range of deepfake generation techniques,
which makes it challenging to develop an accurate universal detection
mechanism. The 2025 Signal Processing Cup (DFWild-Cup competition) provided a
diverse dataset of deepfake images, which are generated from multiple deepfake
image generators, for training machine learning model(s) to emphasize the
generalization of deepfake detection. To this end, we proposed an
ensemble-based approach that employs three different neural network
architectures: a ResNet-34-based architecture, a data-efficient image
transformer (DeiT), and an XceptionNet with Wavelet Transform to capture both
local and global features of deepfakes. We visualize the specific regions that
these models focus for classification using Grad-CAM, and empirically
demonstrate the effectiveness of these models in grouping real and fake images
into cohesive clusters using t-SNE plots. Individually, the ResNet-34
architecture has achieved 88.9% accuracy, whereas the Xception network and the
DeiT architecture have achieved 87.76% and 89.32% accuracy, respectively. With
these networks, our weighted ensemble model achieves an excellent accuracy of
93.23% on the validation dataset of the SP Cup 2025 competition. Finally, the
confusion matrix and an Area Under the ROC curve of 97.44% further confirm the
stability of our proposed method.","['cs.CV', 'cs.LG', 'eess.IV']",False,,,,"Capacity Expansion Planning under Uncertainty subject to Expected Energy
  Not Served Constraints","Hybrid Deepfake Image Detection: A Comprehensive Dataset-Driven Approach
  Integrating Convolutional and Attention Mechanisms with Frequency Domain
  Features"
neg-d2-100,2025-02-18,,2502.12568," Like humans, Large Language Models (LLMs) struggle to generate high-quality
long-form text that adheres to strict requirements in a single pass. This
challenge is unsurprising, as successful human writing, according to the
Cognitive Writing Theory, is a complex cognitive process involving iterative
planning, translating, reviewing, and monitoring. Motivated by these cognitive
principles, we aim to equip LLMs with human-like cognitive writing capabilities
through CogWriter, a novel training-free framework that transforms LLM
constrained long-form text generation into a systematic cognitive writing
paradigm. Our framework consists of two key modules: (1) a Planning Agent that
performs hierarchical planning to decompose the task, and (2) multiple
Generation Agents that execute these plans in parallel. The system maintains
quality via continuous monitoring and reviewing mechanisms, which evaluate
outputs against specified requirements and trigger necessary revisions.
CogWriter demonstrates exceptional performance on LongGenBench, a benchmark for
complex constrained long-form text generation. Even when using Qwen-2.5-14B as
its backbone, CogWriter surpasses GPT-4o by 22% in complex instruction
completion accuracy while reliably generating texts exceeding 10,000 words. We
hope this cognitive science-inspired approach provides a paradigm for LLM
writing advancements:
\href{https://github.com/KaiyangWan/CogWriter}{CogWriter}.","['cs.CL', 'cs.AI']",2502.15642," Neural Ordinary Differential Equations (Neural ODEs) represent
continuous-time dynamics with neural networks, offering advancements for
modeling and control tasks. However, training Neural ODEs requires solving
differential equations at each epoch, leading to high computational costs. This
work investigates simultaneous optimization methods as a faster training
alternative. In particular, we employ a collocation-based, fully discretized
formulation and use IPOPT--a solver for large-scale nonlinear optimization--to
simultaneously optimize collocation coefficients and neural network parameters.
Using the Van der Pol Oscillator as a case study, we demonstrate faster
convergence compared to traditional training methods. Furthermore, we introduce
a decomposition framework utilizing Alternating Direction Method of Multipliers
(ADMM) to effectively coordinate sub-models among data batches. Our results
show significant potential for (collocation-based) simultaneous Neural ODE
training pipelines.","['cs.LG', 'math.OC']",False,,,,"A Cognitive Writing Perspective for Constrained Long-Form Text
  Generation",Training Neural ODEs Using Fully Discretized Simultaneous Optimization
neg-d2-101,2025-03-15,,2503.12233," A robust full-space physical layer security (PLS) transmission scheme is
proposed in this paper considering the full-space wiretapping challenge of
wireless networks supported by simultaneous transmitting and reflecting
reconfigurable intelligent surface (STAR-RIS). Different from the existing
schemes, the proposed PLS scheme takes account of the uncertainty on the
eavesdropper's position within the 360$^\circ$ service area offered by the
STAR-RIS. Specifically, the large system analytical method is utilized to
derive the asymptotic expression of the average security rate achieved by the
security user, considering that the base station (BS) only has the statistical
information of the eavesdropper's channel state information (CSI) and the
uncertainty of its location. To evaluate the effectiveness of the proposed PLS
scheme, we first formulate an optimization problem aimed at maximizing the
weighted sum rate of the security user and the public user. This optimization
is conducted under the power allocation constraint, and some practical
limitations for STAR-RIS implementation, through jointly designing the active
and passive beamforming variables. A novel iterative algorithm based on the
minimum mean-square error (MMSE) and cross-entropy optimization (CEO) methods
is proposed to effectively address the established non-convex optimization
problem with discrete variables. Simulation results indicate that the proposed
robust PLS scheme can effectively mitigate the information leakage across the
entire coverage area of the STAR-RIS-assisted system, leading to superior
performance gain when compared to benchmark schemes encompassing traditional
RIS-aided scheme.","['cs.IT', 'eess.SP', 'math.IT']",2502.09674," Large Language Models' safety-aligned behaviors, such as refusing harmful
queries, can be represented by linear directions in activation space. Previous
research modeled safety behavior with a single direction, limiting mechanistic
understanding to an isolated safety feature. In this work, we discover that
safety-aligned behavior is jointly controlled by multi-dimensional directions.
Namely, we study the vector space of representation shifts during safety
fine-tuning on Llama 3 8B for refusing jailbreaks. By studying orthogonal
directions in the space, we first find that a dominant direction governs the
model's refusal behavior, while multiple smaller directions represent distinct
and interpretable features like hypothetical narrative and role-playing. We
then measure how different directions promote or suppress the dominant
direction, showing the important role of secondary directions in shaping the
model's refusal representation. Finally, we demonstrate that removing certain
trigger tokens in harmful queries can mitigate these directions to bypass the
learned safety capability, providing new insights on understanding safety
alignment vulnerability from a multi-dimensional perspective. Code and
artifacts are available at https://github.com/BMPixel/safety-residual-space.","['cs.CL', 'cs.AI']",False,,,,"Robust Full-Space Physical Layer Security for STAR-RIS-Aided Wireless
  Networks: Eavesdropper with Uncertain Location and Channel","The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Safety
  Analysis"
neg-d2-102,2025-01-13,,2501.07687," PLAnetary Transits and Oscillations of stars (PLATO) is an ESA M-class
mission to be launched by the end of 2026 to discover and characterize
transiting planets around bright and nearby stars, and in particular habitable
rocky planets hosted by solar-like stars. Over the mission lifetime, an average
of 8% of the science data rate will be allocated to Guest Observer programs
(GOs) selected by ESA through public calls, hence it is essential for the
community to know in advance where the observing fields will be located. In a
previous paper, we identified two preliminary long-pointing fields (LOPN1 and
LOPS1) for PLATO, respectively in the northern and southern hemisphere. Here we
present LOPS2, a slightly adjusted version of the southern field that has
recently been selected by the PLATO Science Working Team as the first field to
be observed by PLATO for at least two continuous years, following the
scientific requirements. In this paper, we describe the astrophysical content
of LOPS2 in detail, including known planetary systems, bright/variable/binary
stars, clusters and synergies with other current and future facilities.","['astro-ph.EP', 'astro-ph.IM', 'astro-ph.SR']",2503.14187," In this paper, we consider the inviscid limit problem to the higher
dimensional incompressible Navier-Stokes equations in the whole space. It was
proved in \cite[J. Funct. Anal., 276 (2019)]{GZ} that given initial data
$u_0\in B^{s}_{p,r}$ with $1\leq r<\infty$, the solutions of the Navier-Stokes
equations converge strongly in $B^{s}_{p,r}$ to the Euler equations as the
viscosity parameter tends to zero. In the case when $r=\infty$, we prove the
failure of the $B^{s}_{p,\infty}$-convergence of the Navier-Stokes equations
toward the Euler equations in the inviscid limit.",['math.AP'],False,,,,"The PLATO field selection process. II. Characterization of LOPS2, the
  first long-pointing field","Non-convergence of the Navier-Stokes equations toward the Euler
  equations in weak Besov spaces"
neg-d2-103,2025-01-02,,2501.01626," This work presents a detailed description of the thermochemical
non-equilibrium dissociation of diatomic molecules, and applies this theory to
the case of $\rm H_2$ dissociation. The master equations are used to derive
corresponding aggregate rate constant expressions that hold for any degree of
thermochemical non-equilibrium. These general expressions are analyzed in three
key limits/ regimes: the thermal equilibrium limit, the quasi-steady-state
(QSS) regime, and the pre-QSS regime. Under several simplifying assumptions, an
analytical source term expression that holds in all of these regimes, and is
only a function of the translational temperature, $T_{\rm t}$, and the fraction
of dissociation, $\phi_{\rm A}$, is proposed. This expression has two input
parameters: the QSS dissociation rate constant in the absence of recombination,
$k_{\rm d,nr}(T_{\rm t})$, and a pre-QSS correction factor, $\eta(T_{\rm t})$.
The value of $\eta(T_{\rm t})$ is evaluated by comparing the predictions of the
proposed expression against existing master equation simulations of a 0-D
isothermal and isochoric reactor for the case of $\rm H_2$ dissociation with
the third-bodies $\rm H_2$, $\rm H$, and $\rm He$. Despite its simple
functional form, the proposed expression is able to reproduce the master
equation results for the majority of the tested conditions. The best fit of
$k_{\rm d,nr}(T_{\rm t})$ is then evaluated by conducting a detailed literature
review. Data from a wide range of experimental and computational studies are
considered for the third-bodies $\rm H_2$, $\rm H$, and inert gases, and fits
that are valid from 200 to 20,000 K are proposed. From this review, the
uncertainty of the proposed fits are estimated to be less than a factor of two.",['physics.chem-ph'],2503.14237," Popular video training methods mainly operate on a fixed number of tokens
sampled from a predetermined spatiotemporal grid, resulting in sub-optimal
accuracy-computation trade-offs due to inherent video redundancy. They also
lack adaptability to varying computational budgets for downstream tasks,
hindering applications of the most competitive model in real-world scenes. We
thus propose a new test setting, Token Optimization, for maximized input
information across budgets, which optimizes the size-limited set of input
tokens through token selection from more suitably sampled videos. To this end,
we propose a novel augmentation tool termed Flux. By making the sampling grid
flexible and leveraging token selection, it is easily adopted in most popular
video training frameworks, boosting model robustness with nearly no additional
cost. We integrate Flux in large-scale video pre-training, and the resulting
FluxViT establishes new state-of-the-art results across extensive tasks at
standard costs. Notably, with 1/4 tokens only, it can still match the
performance of previous state-of-the-art models with Token Optimization,
yielding nearly 90\% savings. All models and data are available at
https://github.com/OpenGVLab/FluxViT.",['cs.CV'],False,,,,"Treatment of Thermal Non-Equilibrium Dissociation Rates: Application to
  $\rm H_2$",Make Your Training Flexible: Towards Deployment-Efficient Video Models
neg-d2-104,2025-01-27,,2501.16185," We study the equation of state of three-dimensional compact U(1) gauge theory
on the lattice by means of numerical simulations, and discuss the implications
of our results for the spectrum of the theory, in connection with previous
results from the literature. We also compare our findings to the case of
non-Abelian gauge theories and comment on the continuum limit.","['hep-lat', 'hep-th']",2502.0853," We introduce \emph{residually dominated groups} in pure henselian valued
fields of equicharacteristic zero as an analogue of stably dominated groups
introduced by Hrushovski and Rideau-Kikuchi. We show that when $G$ is a
residually dominated group, there is a finite-to-one group homomorphism from
its connected component into a connected stably dominated group, and we study
the functoriality and universality properties of this map. Moreover, we prove
that there is a group homomorphism into a definable group in the residue field
that witnesses the residual domination. In our proofs, we use the results of
Montenegro, Onshuus, and Simon on groups definable in $\mathrm{NTP}_2$-theories
that extend the theory of fields. Along the way, we also provide an algebraic
characterization of residually dominated types, generalizing the work by Ealy,
Haskell and Simon for stably dominated types in algebraically closed valued
fields and study the properties of residually dominated types.",['math.LO'],False,,,,"On the equation of state of U(1) lattice gauge theory in three
  dimensions","Residually Dominated Groups in Henselian Valued Fields of
  Equicharacteristic Zero"
neg-d2-105,2025-01-22,,2501.13142," Since the turn of the millennium, computational modelling of biological
systems has evolved remarkably and sees matured use spanning basic and clinical
research. While the topic of the peri-millennial debate about the virtues and
limitations of 'reductionism and integrationism' seems less controversial
today, a new apparent dichotomy dominates discussions: mechanistic vs.
data-driven modelling. In light of this distinction, we provide an overview of
recent achievements and new challenges with a focus on the cardiovascular
system. Attention has shifted from generating a universal model of the human to
either models of individual humans (digital twins) or entire cohorts of models
representative of clinical populations to enable in silico clinical trials.
Disease-specific parameterisation, inter-individual and intra-individual
variability, uncertainty quantification as well as interoperable, standardised,
and quality-controlled data are important issues today, which call for open
tools, data and metadata standards, as well as strong community interactions.
The quantitative, biophysical, and highly controlled approach provided by in
silico methods has become an integral part of physiological and medical
research. In silico methods have the potential to accelerate future progress
also in the fields of integrated multi-physics modelling, multi-scale models,
virtual cohort studies, and machine learning beyond what is feasible today. In
fact, mechanistic and data-driven modelling can complement each other
synergistically and fuel tomorrow's artificial intelligence applications to
further our understanding of physiology and disease mechanisms, to generate new
hypotheses and assess their plausibility, and thus to contribute to the
evolution of preventive, diagnostic, and therapeutic approaches.",['q-bio.QM'],2501.17618," We develop an algorithm for bosonic path integral molecular dynamics (PIMD)
simulations with periodic boundary conditions (PBC) that scales quadratically
with the number of particles. Path integral methods are a powerful tool to
simulate bosonic condensed phases, which exhibit fundamental physical phenomena
such as Bose--Einstein condensation and superfluidity. Recently, we developed a
quadratic scaling algorithm for bosonic PIMD, but employed an ad hoc treatment
of PBC. Here we rigorously enforce PBC in bosonic PIMD. It requires summing
over the spring energies of all periodic images in the partition function, and
a naive implementation scales exponentially with the system size. We present an
algorithm for bosonic PIMD simulations of periodic systems that scales only
quadratically. We benchmark our implementation on the free Bose gas and a model
system of cold atoms in optical lattices. We also study an approximate
treatment of PBC based on the minimum-image convention, and derive a numerical
criterion to determine when it is valid.","['physics.chem-ph', 'cond-mat.quant-gas', 'cond-mat.stat-mech']",False,,,,"Computational modelling of biological systems now and then: revisiting
  tools and visions from the beginning of the century","Periodic Boundary Conditions for Bosonic Path Integral Molecular
  Dynamics"
neg-d2-106,2025-02-10,,2502.07095," Invasive species are a growing threat to marine ecosystems, and the recent
proliferation of the Atlantic blue crab (Callinectes sapidus) in the Po Delta
(Italy) has had significant ecological and economic impacts, particularly on
clam farming. This study explores the influence of C. sapidus on clam
production in the Po Delta, combining biological and ecological data with
socio-economic analysis. Field data collected between August and December 2023
from the Canarin and Scardovari Lagoons revealed seasonal fluctuations in crab
abundance, with a peak in captures during the warmer months. The predatory
behaviour of C. sapidus has led to a sharp decline in clam production, reaching
near-zero levels in early 2024. Statistical analysis confirmed a strong
correlation between the increase of the invasive crab population and the
decrease in clam yields. This study also explores potential management
strategies, including the economic valorisation of C. sapidus as a commercial
resource, turning an ecological challenge into an opportunity. These findings
highlight the urgent need for targeted management interventions to mitigate the
impact of this invasive species on local fisheries and ecosystems.",['q-bio.PE'],2501.10636," Autonomous agricultural vehicles (AAVs), including field robots and
autonomous tractors, are becoming essential in modern farming by improving
efficiency and reducing labor costs. A critical task in AAV operations is
headland turning between crop rows. This task is challenging in orchards with
limited headland space, irregular boundaries, operational constraints, and
static obstacles. While traditional trajectory planning methods work well in
arable farming, they often fail in cluttered orchard environments. This letter
presents a novel trajectory planner that enhances the safety and efficiency of
AAV headland maneuvers, leveraging advancements in autonomous driving. Our
approach includes an efficient front-end algorithm and a high-performance
back-end optimization. Applied to vehicles with various implements, it
outperforms state-of-the-art methods in both standard and challenging orchard
fields. This work bridges agricultural and autonomous driving technologies,
facilitating a broader adoption of AAVs in complex orchards.",['cs.RO'],False,,,,"The devasting economic impact of Callinectes sapidus on the clam fishing
  in the Po Delta (Italy): Striking evidence from novel field data","Efficient and Safe Trajectory Planning for Autonomous Agricultural
  Vehicle Headland Turning in Cluttered Orchard Environments"
neg-d2-107,2025-02-24,,2502.17275," The Doyle-Fuller-Newman model is arguably the most ubiquitous electrochemical
model in lithium-ion battery research. Since it is a highly nonlinear model,
its input-output relations are still poorly understood. Researchers therefore
often employ sensitivity analyses to elucidate relative parametric importance
for certain use cases. However, some methods are ill-suited for the complexity
of the model and appropriate methods often face the downside of only being
applicable to scalar quantities of interest. We implement a novel framework for
global sensitivity analysis of time-dependent model outputs and apply it to a
drive cycle simulation. We conduct a full and a subgroup sensitivity analysis
to resolve lowly sensitive parameters and explore the model error when
unimportant parameters are set to arbitrary values. Our findings suggest that
the method identifies insensitive parameters whose variations cause only small
deviations in the voltage response of the model. By providing the methodology,
we hope research questions related to parametric sensitivity for time-dependent
quantities of interest, such as voltage responses, can be addressed more easily
and adequately in simulative battery research and beyond.","['cond-mat.mtrl-sci', 'stat.CO']",2503.03424," Given a tracial von Neumann algebra $(M,\tau)$, we prove that a state
preserving $M$-bimodular ucp map between two stationary W$^*$-extensions of
$(M,\tau)$ preserves the Furstenberg entropy if and only if it induces an
isomorphism between the Radon-Nikodym factors. With a similar proof, we extend
this result to quasi-factor maps between stationary spaces of locally compact
groups and prove an entropy separation between unique stationary and amenable
spaces. As applications, we use these results to establish rigidity phenomena
for unique stationary Poisson boundaries.","['math.OA', 'math.DS', 'math.GR']",False,,,,"Time-dependent global sensitivity analysis of the Doyle-Fuller-Newman
  model",Rigidity of Furstenberg entropy under ucp maps
neg-d2-108,2025-02-12,,2502.08434," We analyze some aspects of the cubic action for gravity recently proposed by
Cheung and Remmen, which is a particular instance of a first order (Palatini)
action. In this approach both the spacetime metric and the connection are
treated as independent fields. We discuss its BRST invariance and compute
explicitly the one-loop contribution of quantum fluctuations around flat space,
checking that the corresponding Slavnov-Taylor identities are fulfilled.
Finally, our results on a first order action are compared with the existing
ones corresponding to a second order action.","['hep-th', 'gr-qc']",2502.0755," Near the center of our Milky Way is a bar-like structure and the so-called
Expanding 3-kpc arms. We currently have limited knowledge of this important
region, since we are about 8.2 kpc from the center and cannot directly observe
it at optical wavelengths, owing to strong extinction from interstellar dust.
Here we present extremely precise VLBI measurements of water maser sources from
the BeSSeL Survey, where extinction is not a problem, which accurately
determine the 3-dimensional locations and motions of three massive young stars.
Combined with previous measurements, these stars delineate a trail of orbits
outlining the Milky Way's Galactic Bar. We present the first measurements
capturing the dynamics of quasi-elliptical (X1) orbits around the Galactic Bar.
Our findings provide evidence substantiating the existence of such orbits
populated by massive young stars. Our measurements of the position and velocity
of a number of massive young stars, previously identified with the Expanding
3-kpc arms, show that they are more likely located in the X1 orbits about the
Galactic Bar. Also, some stars previously assigned to the Norma spiral arm
appear to be in these orbits, which suggests that this spiral arm does not
extend past the end of the bar.",['astro-ph.GA'],False,,,,One loop analysis of the cubic action for gravity,"The Expanding 3-Kiloparsec Arms are neither Expanding nor Spiral Arms,
  but X1 Orbits driven by the Galactic Bar"
neg-d2-109,2025-03-07,,2503.05497," The study of $t$+$t$ cluster states in $^{6}$He provides valuable insights
into exotic nuclear structures and the behavior of fermionic cluster systems.
This study shows rich cluster resonant state structures above the threshold,
identified by experimental reconstruction and theoretical calculations. The
excitation energy spectrum above the $t$+$t$ threshold in $^{6}$He is measured
via the fragmentation excitation process during the breakup reaction of
$^{9}$Li on a $^{208}$Pb target at an incident energy of 32.7 MeV/nucleon. The
resonant states are reconstructed from the final state coincident particles
$t$+$t$ using the invariant mass method, while the non-resonant background is
estimated using the event mixing method. The two new states of energy level
peaks at $17.016\pm0.002$ and $19.4\pm0.6$ MeV are observed in addition to the
previously observed energy level peaks at $13.9\pm0.3$ and $15.0\pm0.3$ MeV.
Microscopic cluster model calculations exploring the $t+t$ resonance states in
$^6\mathrm{He}$ yield theoretical energy spectra which are then compared with
the current experimental results. The calculated reduced width amplitudes (RWA)
of the $t+t$ channels further confirm the clustering structure of the
identified $t+t$ resonance states.","['nucl-ex', 'nucl-th']",2503.05214," The growing rate of chronic wound occurrence, especially in patients with
diabetes, has become a concerning trend in recent years. Chronic wounds are
difficult and costly to treat, and have become a serious burden on health care
systems worldwide. Chronic wounds can have devastating consequences for the
patient, with infection often leading to reduced quality of life and increased
mortality risk. Innovative deep learning methods for the detection and
monitoring of such wounds have the potential to reduce the impact to both
patient and clinician. We present a novel multimodal segmentation method which
allows for the introduction of patient metadata into the training workflow
whereby the patient data are expressed as Gaussian random fields. Our results
indicate that the proposed method improved performance when utilising multiple
models, each trained on different metadata categories. Using the Diabetic Foot
Ulcer Challenge 2022 test set, when compared to the baseline results
(intersection over union = 0.4670, Dice similarity coefficient = 0.5908) we
demonstrate improvements of +0.0220 and +0.0229 for intersection over union and
Dice similarity coefficient respectively. This paper presents the first study
to focus on integrating patient data into a chronic wound segmentation
workflow. Our results show significant performance gains when training
individual models using specific metadata categories, followed by average
merging of prediction masks using distance transforms. All source code for this
study is available at:
https://github.com/mmu-dermatology-research/multimodal-grf","['eess.IV', 'cs.CV']",False,,,,$t$+$t$ cluster states in $^{6}$He,"Gaussian Random Fields as an Abstract Representation of Patient Metadata
  for Multimodal Medical Image Segmentation"
neg-d2-110,2025-02-06,,2502.03879," We discuss the thermal CP phase transition in QCD at $\theta=\pi$ under a
weak magnetic field background, where the electromagnetic scale anomaly gets
significant. To explicitize, we work on a two-flavor Nambu-Jona-Lasinio model
at $\theta=\pi$ in the mean field approximation, including the
electromagnetic-scale anomaly term. We find that the thermal CP phase
transition becomes first order and the strength of the first order gets more
prominent as the magnetic field increases. The associated potential barrier is
thermally created by the electromagnetic scale anomaly and gives rise to
criticality due to the induced potential of a non-perturbative form $\sim
\frac{|eB|^3}{f_\pi} \frac{|P|}{P^2 + m_0^2}$, where $eB$ denotes the magnetic
field strength; $P$ the CP order parameter, and $m_0$ the isospin-symmetric
current-quark mass.","['hep-ph', 'hep-lat', 'hep-th', 'nucl-th']",2502.09507," The remarkable generalization performance of contrastive vision-language
models like CLIP is often attributed to the diversity of their training
distributions. However, key questions remain unanswered: Can CLIP generalize to
an entirely unseen domain when trained on a diverse mixture of domains (domain
generalization)? Can it generalize to unseen classes within partially seen
domains (compositional generalization)? What factors affect such
generalization? To answer these questions, we trained CLIP models on
systematically constructed training distributions with controlled domain
diversity and object class exposure. Our experiments show that domain diversity
is essential for both domain and compositional generalization, yet
compositional generalization can be surprisingly weaker than domain
generalization when the training distribution contains a suboptimal subset of
the test domain. Through data-centric and mechanistic analyses, we find that
successful generalization requires learning of shared representations already
in intermediate layers and shared circuitry.","['cs.LG', 'cs.CV']",False,,,,"First-order CP phase transition in two-flavor QCD at $\theta = \pi$
  under electromagnetic scale anomaly via a Nambu-Jona-Lasinio description",When and How Does CLIP Enable Domain and Compositional Generalization?
neg-d2-111,2025-03-19,,2503.1527," Recently, smart contracts have played a vital role in automatic financial and
business transactions. To help end users without programming background to
better understand the logic of smart contracts, previous studies have proposed
models for automatically translating smart contract source code into their
corresponding code summaries. However, in practice, only 13% of smart contracts
deployed on the Ethereum blockchain are associated with source code. The
practical usage of these existing tools is significantly restricted.
Considering that bytecode is always necessary when deploying smart contracts,
in this paper, we first introduce the task of automatically generating smart
contract code summaries from bytecode. We propose a novel approach, named
SmartBT (Smart contract Bytecode Translator) for automatically translating
smart contract bytecode into fine-grained natural language description
directly. Two key challenges are posed for this task: structural code logic
hidden in bytecode and the huge semantic gap between bytecode and natural
language descriptions. To address the first challenge, we transform bytecode
into CFG (Control-Flow Graph) to learn code structural and logic details.
Regarding the second challenge, we introduce an information retrieval component
to fetch similar comments for filling the semantic gap. Then the structural
input and semantic input are used to build an attentional sequence-to-sequence
neural network model. The copy mechanism is employed to copy rare words
directly from similar comments and the coverage mechanism is employed to
eliminate repetitive outputs. The automatic evaluation results show that
SmartBT outperforms a set of baselines by a large margin, and the human
evaluation results show the effectiveness and potential of SmartBT in producing
meaningful and accurate comments for smart contract code from bytecode
directly.",['cs.SE'],2501.1542," Classifier-Free Guidance (CFG) has been a default technique in various visual
generative models, yet it requires inference from both conditional and
unconditional models during sampling. We propose to build visual models that
are free from guided sampling. The resulting algorithm, Guidance-Free Training
(GFT), matches the performance of CFG while reducing sampling to a single
model, halving the computational cost. Unlike previous distillation-based
approaches that rely on pretrained CFG networks, GFT enables training directly
from scratch. GFT is simple to implement. It retains the same maximum
likelihood objective as CFG and differs mainly in the parameterization of
conditional models. Implementing GFT requires only minimal modifications to
existing codebases, as most design choices and hyperparameters are directly
inherited from CFG. Our extensive experiments across five distinct visual
models demonstrate the effectiveness and versatility of GFT. Across domains of
diffusion, autoregressive, and masked-prediction modeling, GFT consistently
achieves comparable or even lower FID scores, with similar diversity-fidelity
trade-offs compared with CFG baselines, all while being guidance-free. Code
will be available at https://github.com/thu-ml/GFT.","['cs.CV', 'cs.AI', 'cs.LG']",False,,,,Automating Comment Generation for Smart Contract from Bytecode,Visual Generation Without Guidance
neg-d2-112,2025-02-02,,2502.01694," A key paradigm to improve the reasoning capabilities of large language models
(LLMs) is to allocate more inference-time compute to search against a verifier
or reward model. This process can then be utilized to refine the pretrained
model or distill its reasoning patterns into more efficient models. In this
paper, we study inference-time compute by viewing chain-of-thought (CoT)
generation as a metastable Markov process: easy reasoning steps (e.g.,
algebraic manipulations) form densely connected clusters, while hard reasoning
steps (e.g., applying a relevant theorem) create sparse, low-probability edges
between clusters, leading to phase transitions at longer timescales. Under this
framework, we prove that implementing a search protocol that rewards sparse
edges improves CoT by decreasing the expected number of steps to reach
different clusters. In contrast, we establish a limit on reasoning capability
when the model is restricted to local information of the pretrained graph. We
also show that the information gained by search can be utilized to obtain a
better reasoning model: (1) the pretrained model can be directly finetuned to
favor sparse edges via policy gradient methods, and moreover (2) a compressed
metastable representation of the reasoning dynamics can be distilled into a
smaller, more efficient model.","['cs.AI', 'cs.LG', 'stat.ML']",2501.10074," Spatial reasoning is an essential problem in embodied AI research. Efforts to
enhance spatial reasoning abilities through supplementary spatial data and
fine-tuning have proven limited and ineffective when addressing complex
embodied tasks, largely due to their dependence on language-based outputs.
While some approaches have introduced a point-based action space to mitigate
this issue, they fall short in managing more intricate tasks within complex
environments. This deficiency arises from their failure to fully exploit the
inherent thinking and reasoning capabilities that are fundamental strengths of
Vision-Language Models (VLMs). To address these limitations, we propose a novel
approach named SpatialCoT, specifically designed to bolster the spatial
reasoning capabilities of VLMs. Our approach comprises two stages: spatial
coordinate bi-directional alignment, which aligns vision-language inputs with
spatial coordinates, and chain-of-thought spatial grounding, which harnesses
the reasoning capabilities of language models for advanced spatial reasoning.
We evaluate SpatialCoT on challenging navigation and manipulation tasks, both
in simulation and real-world settings. Experimental results demonstrate that
our method significantly outperforms previous state-of-the-art approaches in
both tasks.","['cs.RO', 'cs.AI', 'cs.CV']",False,,,,"Metastable Dynamics of Chain-of-Thought Reasoning: Provable Benefits of
  Search, RL and Distillation","SpatialCoT: Advancing Spatial Reasoning through Coordinate Alignment and
  Chain-of-Thought for Embodied Task Planning"
neg-d2-113,2025-03-10,,2503.07383," Diverse usage patterns induce complex and variable aging behaviors in
lithium-ion batteries, complicating accurate health diagnosis and prognosis.
Separate diagnostic cycles are often used to untangle the battery's current
state of health from prior complex aging patterns. However, these same
diagnostic cycles alter the battery's degradation trajectory, are
time-intensive, and cannot be practically performed in onboard applications. In
this work, we leverage portions of operational measurements in combination with
an interpretable machine learning model to enable rapid, onboard battery health
diagnostics and prognostics without offline diagnostic testing and the
requirement of historical data. We integrate mechanistic constraints within an
encoder-decoder architecture to extract electrode states in a physically
interpretable latent space and enable improved reconstruction of the
degradation path. The health diagnosis model framework can be flexibly applied
across diverse application interests with slight fine-tuning. We demonstrate
the versatility of this model framework by applying it to three battery-cycling
datasets consisting of 422 cells under different operating conditions,
highlighting the utility of an interpretable diagnostic-free, onboard battery
diagnosis and prognosis model.","['eess.SY', 'cs.LG', 'cs.SY']",2501.13068," The interconnection between the human lungs and other organs, such as the
liver and kidneys, is crucial for understanding the underlying risks and
effects of lung diseases and improving patient care. However, most research
chest CT imaging is focused solely on the lungs due to considerations of cost
and radiation dose. This restricted field of view (FOV) in the acquired images
poses challenges to comprehensive analysis and hinders the ability to gain
insights into the impact of lung diseases on other organs. To address this, we
propose SCOPE (Spatial Coverage Optimization with Prior Encoding), a novel
approach to capture the inter-organ relationships from CT images and extend the
FOV of chest CT images. Our approach first trains a variational autoencoder
(VAE) to encode 2D axial CT slices individually, then stacks the latent
representations of the VAE to form a 3D context for training a latent diffusion
model. Once trained, our approach extends the FOV of CT images in the
z-direction by generating new axial slices in a zero-shot manner. We evaluated
our approach on the National Lung Screening Trial (NLST) dataset, and results
suggest that it effectively extends the FOV to include the liver and kidneys,
which are not completely covered in the original NLST data acquisition.
Quantitative results on a held-out whole-body dataset demonstrate that the
generated slices exhibit high fidelity with acquired data, achieving an SSIM of
0.81.","['cs.CV', 'eess.IV']",False,,,,Diagnostic-free onboard battery health assessment,"Beyond the Lungs: Extending the Field of View in Chest CT with Latent
  Diffusion Models"
neg-d2-114,2025-02-03,,2502.01187," Memorization in Large Language Models (LLMs) poses privacy and security
risks, as models may unintentionally reproduce sensitive or copyrighted data.
Existing analyses focus on average-case scenarios, often neglecting the highly
skewed distribution of memorization. This paper examines memorization in LLM
supervised fine-tuning (SFT), exploring its relationships with training
duration, dataset size, and inter-sample similarity. By analyzing memorization
probabilities over sequence lengths, we link this skewness to the token
generation process, offering insights for estimating memorization and comparing
it to established metrics. Through theoretical analysis and empirical
evaluation, we provide a comprehensive understanding of memorization behaviors
and propose strategies to detect and mitigate risks, contributing to more
privacy-preserving LLMs.","['cs.AI', 'cs.CL', 'cs.LG']",2501.06374," This paper introduces AFRIDOC-MT, a document-level multi-parallel translation
dataset covering English and five African languages: Amharic, Hausa, Swahili,
Yor\`ub\'a, and Zulu. The dataset comprises 334 health and 271 information
technology news documents, all human-translated from English to these
languages. We conduct document-level translation benchmark experiments by
evaluating neural machine translation (NMT) models and large language models
(LLMs) for translations between English and these languages, at both the
sentence and pseudo-document levels. These outputs are realigned to form
complete documents for evaluation. Our results indicate that NLLB-200 achieved
the best average performance among the standard NMT models, while GPT-4o
outperformed general-purpose LLMs. Fine-tuning selected models led to
substantial performance gains, but models trained on sentences struggled to
generalize effectively to longer documents. Furthermore, our analysis reveals
that some LLMs exhibit issues such as under-generation, repetition of words or
phrases, and off-target translations, especially for African languages.",['cs.CL'],False,,,,"Skewed Memorization in Large Language Models: Quantification and
  Decomposition",AFRIDOC-MT: Document-level MT Corpus for African Languages
neg-d2-115,2025-01-17,,2501.10636," Autonomous agricultural vehicles (AAVs), including field robots and
autonomous tractors, are becoming essential in modern farming by improving
efficiency and reducing labor costs. A critical task in AAV operations is
headland turning between crop rows. This task is challenging in orchards with
limited headland space, irregular boundaries, operational constraints, and
static obstacles. While traditional trajectory planning methods work well in
arable farming, they often fail in cluttered orchard environments. This letter
presents a novel trajectory planner that enhances the safety and efficiency of
AAV headland maneuvers, leveraging advancements in autonomous driving. Our
approach includes an efficient front-end algorithm and a high-performance
back-end optimization. Applied to vehicles with various implements, it
outperforms state-of-the-art methods in both standard and challenging orchard
fields. This work bridges agricultural and autonomous driving technologies,
facilitating a broader adoption of AAVs in complex orchards.",['cs.RO'],2501.0883," In this work, we offer a historical stroll through the vast topic of binary
quadratic forms. We begin with a quick review of their history and then an
overview of contemporary algebraic developments on the subject.","['math.HO', 'math.NT']",False,,,,"Efficient and Safe Trajectory Planning for Autonomous Agricultural
  Vehicle Headland Turning in Cluttered Orchard Environments",Binary quadratic forms: modern developments
neg-d2-116,2025-03-02,,2503.01924," Adversarial robustness is a critical challenge in deploying deep neural
networks for real-world applications. While adversarial training is a widely
recognized defense strategy, most existing studies focus on balanced datasets,
overlooking the prevalence of long-tailed distributions in real-world data,
which significantly complicates robustness. This paper provides a comprehensive
analysis of adversarial training under long-tailed distributions and identifies
limitations in the current state-of-the-art method, AT-BSL, in achieving robust
performance under such conditions. To address these challenges, we propose a
novel training framework, TAET, which integrates an initial stabilization phase
followed by a stratified equalization adversarial training phase. Additionally,
prior work on long-tailed robustness has largely ignored the crucial evaluation
metric of balanced accuracy. To bridge this gap, we introduce the concept of
balanced robustness, a comprehensive metric tailored for assessing robustness
under long-tailed distributions. Extensive experiments demonstrate that our
method surpasses existing advanced defenses, achieving significant improvements
in both memory and computational efficiency. This work represents a substantial
advancement in addressing robustness challenges in real-world applications. Our
code is available at:
https://github.com/BuhuiOK/TAET-Two-Stage-Adversarial-Equalization-Training-on-Long-Tailed-Distributions.","['cs.LG', 'cs.AI', 'stat.ML']",2503.11567," A modification of the optical model for rough surfaces, implemented in Geant4
as a part of the unified model, is suggested. The modified model takes into
account the variation of the interaction probability of the photon with the
microfacet based on the relative orientation of the photon and the sampled
microfacet's normal. The implementation is using a rejection algorithm and
assumes the interaction probability to be proportional to the projection of the
microfacet area on the plane perpendicular to the photon direction. A
comparison of the results obtained with the original and the modified models,
as well as obtained in direct Monte Carlo simulations are presented for several
test surfaces constructed using a pattern of elementary geometrical shapes.",['physics.ins-det'],False,,,,"TAET: Two-Stage Adversarial Equalization Training on Long-Tailed
  Distributions","Microfacet projected area-based correction for unified model of Geant4
  for rough surfaces"
neg-d2-117,2025-03-07,,2503.05503," We construct steady non-spherical bubbles and drops, which are traveling wave
solutions to the axisymmetric two-phase Euler equations with surface tension,
whose inner phase is a bounded connected domain. The solutions have a uniform
vorticity distribution in this inner phase and they have a vortex sheet on its
surface.
  Our construction relies on a perturbative approach around an explicit
spherical solution, given by Hill's vortex enclosed by a spherical vortex
sheet. The construction is sensitive to the Weber numbers describing the flow.
At critical Weber numbers, we perform a bifurcation analysis utilizing the
Crandall-Rabinowitz theorem in Sobolev spaces on the 2-sphere. Away from these
critical numbers, our construction relies on the implicit function theorem.
  Our results imply that the model containing surface tension is richer than
the ordinary one-phase Euler equations, in the sense that for the latter,
Hill's spherical vortex is unique (modulo translations) among all axisymmetric
simply connected uniform vortices of a given circulation.","['math.AP', 'physics.flu-dyn']",2502.0194," We present a cost-effective new approach for generating denser depth maps for
Autonomous Driving (AD) and Autonomous Vehicles (AVs) by integrating the images
obtained from deep neural network (DNN) 4D radar detectors with conventional
camera RGB images. Our approach introduces a novel pixel positional encoding
algorithm inspired by Bartlett's spatial spectrum estimation technique. This
algorithm transforms both radar depth maps and RGB images into a unified pixel
image subspace called the Spatial Spectrum, facilitating effective learning
based on their similarities and differences. Our method effectively leverages
high-resolution camera images to train radar depth map generative models,
addressing the limitations of conventional radar detectors in complex vehicular
environments, thus sharpening the radar output. We develop spectrum estimation
algorithms tailored for radar depth maps and RGB images, a comprehensive
training framework for data-driven generative models, and a camera-radar
deployment scheme for AV operation. Our results demonstrate that our approach
also outperforms the state-of-the-art (SOTA) by 27.95% in terms of
Unidirectional Chamfer Distance (UCD).","['cs.CV', 'eess.IV']",False,,,,Steady bubbles and drops in inviscid fluids,"Toward a Low-Cost Perception System in Autonomous Vehicles: A Spectrum
  Learning Approach"
neg-d2-118,2025-02-26,,2502.19632," We present extensive proper motion measurements of the Crab Nebula made from
Canada-France-Hawaii Telescope MegaPrime/MegaCam images taken in 2007, 2016,
and 2019. A total of 19974 proper motion vectors with uncertainty
$<10$\,mas\,yr$^{-1}$ located over the majority of the Crab Nebula are used to
map the supernova remnant's two-dimensional expansion properties that reflect
the dynamics of the original explosion, acceleration of ejecta imparted by
spin-down energy from the pulsar, and interaction between the ejecta and
surrounding cicumstellar material (CSM). The average convergence date we derive
is 1105.5 $\pm$ 0.5 CE, which is 15-35 yr earlier compared to most previous
estimates. We find that it varies as a function of position angle around the
nebula, with the earliest date and smallest proper motions measured along the
equator defined by the east and west bays. The lower acceleration of material
along the equatorial plane may be indicative of the supernova's interaction
with a disk-like CSM geometry. Comparing our measurements to previous
analytical solutions of the Crab's expansion and our own numerical simulation
using the moving mesh hydrodynamics code \texttt{Sprout}, we conclude that the
ejecta have relaxed closer to homologous expansion than expected for the
commonly adopted pulsar spindown age of $\tau \sim 700$ yr and a pulsar wind
nebula (PWN) still evolving inside the flat part of the ejecta density profile.
These findings provide further evidence that the PWN has broken out of the
inner flat part of the supernova ejecta density profile and has experienced
``blowout''.","['astro-ph.HE', 'astro-ph.GA', 'astro-ph.SR']",2501.01623," The ISCHEMIA Trial randomly assigned patients with ischemic heart disease to
an invasive treatment strategy centered on revascularization with a control
group assigned non-invasive medical therapy. As is common in such ``strategy
trials,'' many participants assigned to treatment remained untreated while many
assigned to control crossed over into treatment. Intention-to-treat (ITT)
analyses of strategy trials preserve randomization-based comparisons, but ITT
effects are diluted by non-compliance. Conventional per-protocol analyses that
condition on treatment received are likely biased by discarding random
assignment. In trials where compliance choices are made shortly after
assignment, instrumental variables (IV) methods solve both problems --
recovering an undiluted average causal effect of treatment for treated subjects
who comply with trial protocol. In ISCHEMIA, however, some controls were
revascularized as long as five years after random assignment. This paper
extends the IV framework for strategy trials, allowing for such dynamic
non-random compliance behavior. IV estimates of long-run revascularization
effects on quality of life are markedly larger than previously reported ITT and
per-protocol estimates. We also show how to estimate complier characteristics
in a dynamic-treatment setting. These estimates reveal increasing selection
bias in naive time-varying per-protocol estimates of revascularization effects.
Compliers have baseline health similar to that of the study population, while
control-group crossovers are far sicker.",['econ.EM'],False,,,,The Non-Uniform Expansion of the Crab Nebula,"Instrumental Variables with Time-Varying Exposure: New Estimates of
  Revascularization Effects on Quality of Life"
neg-d2-119,2025-01-21,,2501.11989," For the nonlocal quasilinear fractional $p$-Laplace operator $(-\Delta)^s_p$
with $s\in (0,1)$ and $p\in(1,\infty)$, we investigate the nonexistence and
existence of nontrivial nonnegative solutions $u$ in the local fractional
Sobolev space $W_{\rm loc}^{s,p}(\mathbb R^n)$ that satisfies the inequality
$(-\Delta)^s_p u\ge u^q$ weakly in $\mathbb R^n$, where $q\in(0,\infty)$. In
addition, nonexistence of nontrivial nonnegative weak solutions in the global
fractional Sobolev space $W^{s,p}(\mathbb R^n)$ to the fractional $p$-Laplace
equation $(-\Delta)^s_p u= u^q$ are also investigated. The approach taken in
this paper is mainly based on some delicate analysis of the fundamental
solutions to the fractional $p$-Laplace operator $(-\Delta)^s_p$.","['math.AP', 'math.CA']",2503.09476," In this paper,we propose a Multi-Objective Sequential Quadratic Programming
(MOSQP) algorithm for constrained multi-objective optimization problems,basd on
a low-order smooth penalty function as the merit function for line search. The
algorithm constructs single-objective optimization subproblems based on each
objective function, solves quadratic programming (QP) subproblems to obtain
descent directions for expanding the iterative point set within the feasible
region, and filters non-dominated points after expansion. A new QP problem is
then formulated using information from all objective functions to derive
descent directions. The Armijo step size rule is employed for line search,
combined with Powell's correction formula (1978) for B iteration updates. If QP
subproblems is infesible, the negative gradient of the merit function is
adopted as the search direction. The algorithm is proven to converge to an
approximate Pareto front for constrained multi-objective optimization. Finally,
numerical experiments are performed for specific multi-objective optimization
problems.",['math.OC'],False,,,,"Nontrivial nonnegative weak solutions to fractional $p$-Laplace
  inequalities and equations","A Multi-objective Sequential Quadratic Programming Algorithm Based on
  Low-order Smooth Penalty Function"
neg-d2-120,2025-02-12,,2502.08782," The increasing penetration of Distributed Energy Resources (DERs) in the
distribution system has led to the emergence of a new market actor - the
aggregator. The aggregator serves as a facilitator, enabling flexibility asset
owners to get access to different markets. In which, EVs aggregators are
gaining more attention due to their expanding use and potential to provide
services in various types of markets, particularly in the reserve market.
Currently, TSO indirectly utilizes these resources under the management of the
distribution system operators (DSO), which can negatively impact the
distribution grid. Conversely, adjustments from DSOs can impact service
provision to TSO due to the shortage of TSO usage information. These factors
highlight the importance of evaluating the service provision from aggregators
under different TSO-DSO coordination schemes. This paper focuses on the
provision of flexibility from electric vehicles (EVs) aggregators for balancing
service in the TSO-DSO hybrid-managed and compares it with the DSO-managed
coordination schemes. The behavior of aggregators reacting to price
fluctuations and TSO requests under different coordination schemes and
simulation scenarios is thoroughly evaluated. Additionally, their impact on the
grid is analyzed through the DSO's congestion management process and validated
using data from a real part of the Dutch distribution network. Results find
that the hybrid-managed coordination scheme gives more benefit to the
aggregator than the DSO-managed scheme and the EVs aggregator will gain more
profit in winter than summer due to more upward regulation service is needed.","['eess.SY', 'cs.SY']",2501.12915," In this paper, we treat minimal left-invariant unit vector fields on
oscillator group and their relations with the ones that define a harmonic map.
Particularly, if all structure constants of the oscillator group are equal to
each other, then all unit left invariant vector fields that define a harmonic
map into the unit tangent bundle with Sasaki metric are minimal.",['math.DG'],False,,,,"A comparative study of different TSO-DSO coordination in the reserve
  market",Minimal unit vector fields on oscillator groups
neg-d2-121,2025-03-08,,2503.06296," Question Answering (QA) and Visual Question Answering (VQA) are well-studied
problems in the language and vision domain. One challenging scenario involves
multiple sources of information, each of a different modality, where the answer
to the question may exist in one or more sources. This scenario contains richer
information but is highly complex to handle. In this work, we formulate a novel
question-answer generation (QAG) framework in an environment containing
multi-source, multimodal information. The answer may belong to any or all
sources; therefore, selecting the most prominent answer source or an optimal
combination of all sources for a given question is challenging. To address this
issue, we propose a question-guided attention mechanism that learns attention
across multiple sources and decodes this information for robust and unbiased
answer generation. To learn attention within each source, we introduce an
explicit alignment between questions and various information sources, which
facilitates identifying the most pertinent parts of the source information
relative to the question. Scalability in handling diverse questions poses a
challenge. We address this by extending our model to a sparse
mixture-of-experts (sparse-MoE) framework, enabling it to handle thousands of
question types. Experiments on T5 and Flan-T5 using three datasets demonstrate
the model's efficacy, supported by ablation studies.","['cs.CL', 'cs.LG']",2503.09495," The calibration of the CR39 and Makrofol Nuclear Track Detectors of the
MoEDAL experiment at the CERN-LHC was performed by exposing stacks of detector
foils to heavy ion beams with energies ranging from 340 MeV/nucleon to 150
GeV/nucleon. After chemical etching, the base areas and lengths of etch-pit
cones were measured using automatic and manual optical microscopes. The
response of the detectors, as measured by the ratio of the track-etching rate
over the bulk-etching rate, was determined over a range extending from their
threshold at Z/$\beta\sim7$ and $\sim50$ for CR39 and Makrofol, respectively,
up to Z/$\beta\sim92$",['physics.ins-det'],False,,,,"MoEMoE: Question Guided Dense and Scalable Sparse Mixture-of-Expert for
  Multi-source Multi-modal Answering","Calibration of Solid State Nuclear Track Detectors for Rare Event
  Searches"
neg-d2-122,2025-01-23,,2501.14006," Conditional Average Treatment Effect (CATE) estimation, at the heart of
counterfactual reasoning, is a crucial challenge for causal modeling both
theoretically and applicatively, in domains such as healthcare, sociology, or
advertising. Borrowing domain adaptation principles, a popular design maps the
sample representation to a latent space that balances control and treated
populations while enabling the prediction of the potential outcomes. This paper
presents a new CATE estimation approach based on the asymmetrical search for
two latent spaces called Asymmetrical Latent Representation for Individual
Treatment Effect (ALRITE), where the two latent spaces are respectively
intended to optimize the counterfactual prediction accuracy on the control and
the treated samples. Under moderate assumptions, ALRITE admits an upper bound
on the precision of the estimation of heterogeneous effects (PEHE), and the
approach is empirically successfully validated compared to the state-of-the-art","['cs.LG', 'cs.AI']",2503.09028," Integration of Inverter-based Resources (IBRs) such as solar-powered plants
which lack the intrinsic characteristics such as the inertial response of the
traditional synchronous-generator (SG) based sources presents a new challenge
in the form of analyzing the grid stability under their presence. For example,
solar power is available for approximately from 9 AM-5 PM. However, the result
of the rise in power consumption after 6 PM and the reverting back to the
non-renewable source of power generation during that period puts immense stress
on the grid, testing the ramp limitations of the SGs. Failure to meet the
required power demand due to SG ramp limitations leads to failure of the power
grid and other catastrophes. Numerous mitigation techniques exist in order to
address the ramping issues with adding the energy storage elements (ESE) to the
grid being one. ESEs have higher ramping capabilities compared to the
traditional SGs. Also, the ESEs can store the energy and supply it to the grid
when required making them extremely responsive to high ramp situations.
However, the rate of degradation of the ESEs is faster than the SGs. This
raises an important issue of addressing the degradation of the ESEs while
meeting the required power demand objectives and constraints. This work
proposes a battery degradation-aware model predictive energy management
strategy and it is tested via a numerical simulation on multiple physical
systems such as Shipboard Power Systems (SPS). Moreover, the risk arising due
to the fault in the IBR is also studied by means of a numerical simulation.
Overall, the goal of this study is to make the existing power grid more robust,
resilient, and risk-free from component degradation and eventual failures.",['math.OC'],False,,,,"Asymmetrical Latent Representation for Individual Treatment Effect
  Modeling","Degradation-based Energy Management for Microgrids in the Presence of
  Energy Storage Elements"
neg-d2-123,2025-01-09,,2501.05638," We show that it is NP-hard to distinguish graphs of linear mim-width at most
1211 from graphs of sim-width at least 1216. This implies that Mim-Width,
Sim-Width, One-Sided Mim-Width, and their linear counterparts are all
paraNP-complete, i.e., NP-complete to compute even when upper bounded by a
constant.","['cs.CC', 'cs.DM', 'cs.DS', 'math.CO']",2503.06971," Europa, Jupiter's second Galilean moon, is believed to host a subsurface
ocean in contact with a rocky mantle, where hydrothermal activity may drive the
synthesis of organic molecules. Of these molecules, abiotic synthesis of
aromatic amino acids is unlikely, and their detection on Europa could be
considered a biosignature. Fluorescence from aromatic amino acids, with
characteristic emissions in the 200-400 nanometer wavelength range, can be
induced by a laser and may be detectable where ocean material has been
relatively recently emplaced on Europa's surface, as indicated by geologically
young terrain and surface features. However, surface bombardment by charged
particles from the Jovian magnetosphere and solar ultraviolet (UV) radiation
degrades organic molecules, limiting their longevity. We model radiolysis and
photolysis of aromatic amino acids embedded in ice, showing dependencies on
hemispheric and latitudinal patterns of charged particle bombardment and ice
phase. We demonstrate that biosignatures contained within freshly deposited ice
in high-latitude regions on the surface of Europa are detectable using
laser-induced UV fluorescence, even from an orbiting spacecraft.",['astro-ph.EP'],False,,,,Mim-Width is paraNP-complete,Fluorescent Biomolecules Detectable in Near-Surface Ice on Europa
neg-d2-124,2025-03-18,,2503.14714," This paper revises aesthetics theory through the lens of authenticity and
investigates practical applications using a co-design approach. We encourage
designers to include ordinary clients as co-creators in the co-design process,
guiding them in expressing their aesthetics, values, and preferences while
stimulating their creativity. This paper proposes a bespoke design process
framework for authenticity aesthetics that incorporates empathy, defining,
ideating, prototyping, and testing. This framework delineates the roles and
responsibilities of clients and designers at different phases and highlights
evolving material mediums that enable their communication. The paper concludes
by reflecting on consumerist aesthetics, advocating for designers to focus on
the insights of ordinary clients, design for their authentic uniqueness, and
recognize the broad prospects of bespoke design methods.",['cs.HC'],2502.01005," Cryogenic solid neon has recently emerged as a pristine solid host for single
electron qubits. At ~10 mK temperatures, electron-on-solid-neon (eNe) charge
qubits have exhibited exceptionally long coherence times and high operation
fidelities. To advance this platform towards a scalable quantum information
architecture, systematic characterization of its noise feature is imperative.
Here, we show the remarkable resilience of solid neon against charge and
thermal noises when eNe qubits are operated away from the charge-insensitive
sweet-spot and at elevated temperatures. Without optimizing neon growth, the
measured charge (voltage) noise on solid neon is already orders of magnitude
lower than that in most stringently grown semiconductors, rivaling the best
records to date. Up to 400 mK, the eNe charge qubits operated at ~5 GHz can
maintain their echo coherence times over 1 microsecond. These observations
highlight solid neon as an ideal host for quantum information processing at
higher temperatures and larger scales.","['quant-ph', 'cond-mat.mes-hall']",False,,,,"Authenticity as Aesthetics: Enabling the Client to Dominate
  Decision-making in Co-design",Noise-resilient solid host for electron qubits above 100 mK
neg-d2-125,2025-03-15,,2503.12179," We introduce a new methodology for modeling regular spatial data using
hyperuniform point processes. We show that, under some mixing conditions on the
perturbations, perturbed lattices in general dimension are hyperuniform. Due to
their inherent repulsive structure, they serve as an effective baseline model
for data sets in which points exhibit repulsiveness. Specifically, we derive an
explicit formula for the $K$-function of lattices perturbed by a Gaussian
random field, which proves particularly useful in conjunction with the minimal
contrast method. We apply this approach to a data set representing the grain
centers of a polycrystalline metallic material composed of nickel and titanium.",['math.PR'],2502.01185," We present a novel deep learning network for Active Speech Cancellation
(ASC), advancing beyond Active Noise Cancellation (ANC) methods by effectively
canceling both noise and speech signals. The proposed Multi-Band Mamba
architecture segments input audio into distinct frequency bands, enabling
precise anti-signal generation and improved phase alignment across frequencies.
Additionally, we introduce an optimization-driven loss function that provides
near-optimal supervisory signals for anti-signal generation. Experimental
results demonstrate substantial performance gains, achieving up to 7.2dB
improvement in ANC scenarios and 6.2dB in ASC, significantly outperforming
existing methods. Audio samples are available at
https://mishalydev.github.io/DeepASC-Demo","['cs.SD', 'cs.AI', 'cs.LG', 'eess.AS', 'eess.SP']",False,,,,Fitting regular point patterns with a hyperuniform perturbed lattice,Deep Active Speech Cancellation with Multi-Band Mamba Network
neg-d2-126,2025-03-15,,2503.1227," Within-individual variability of health indicators measured over time is
becoming commonly used to inform about disease progression. Simple summary
statistics (e.g. the standard deviation for each individual) are often used but
they are not suited to account for time changes. In addition, when these
summary statistics are used as covariates in a regression model for
time-to-event outcomes, the estimates of the hazard ratios are subject to
regression dilution. To overcome these issues, a joint model is built where the
association between the time-to-event outcome and multivariate longitudinal
markers is specified in terms of the within-individual variability of the
latter. A mixed-effect location-scale model is used to analyse the longitudinal
biomarkers, their within-individual variability and their correlation. The time
to event is modelled using a proportional hazard regression model, with a
flexible specification of the baseline hazard, and the information from the
longitudinal biomarkers is shared as a function of the random effects. The
model can be used to quantify within-individual variability for the
longitudinal markers and their association with the time-to-event outcome. We
show through a simulation study the performance of the model in comparison with
the standard joint model with constant variance. The model is applied on a
dataset of adult women from the UK cystic fibrosis registry, to evaluate the
association between lung function, malnutrition and mortality.",['stat.ME'],2502.18716," From biology and astronomy to quantum optics, there is a critical need for
high frame rate, high quantum efficiency imaging. In practice, most cameras
only satisfy one of these requirements. Here we introduce interlaced fast
kinetics imaging, a technique that allows burst video acquisition at frame
rates up to 3.33 Mfps using a commercial EMCCD camera with single-photon
sensitivity. This approach leverages EMCCD's intrinsic fast row transfer
dynamics by introducing a tilted lens array into the imaging path, creating a
spatially distributed grid of exposed pixels, each aligned to its own column of
the sensor. The remaining unexposed pixels serve as in-situ storage registers,
allowing subsequent frames to be captured after just one row shift operation.
Our interlaced fast kinetics camera maintains 50% contrast for square wave
intensity modulation frequencies up to 1.61 MHz. We provide benchmarks of the
video performance by capturing two dimensional videos of spatially evolving
patterns that repeat every 2$\mu$s, with spatial resolution of 11$\times$15
pixels. Our approach is compatible with commercial EMCCDs and opens a new route
to ultra-fast imaging at single-photon sensitivity with applications from fast
fluorescence imaging to photon correlation measurement.","['physics.atom-ph', 'physics.optics']",False,,,,"A Bayesian location-scale joint model for time-to-event and multivariate
  longitudinal data with association based on within-individual variability",A Mega-FPS low light camera
neg-d2-127,2025-03-03,,2503.01261," Image quantization is a crucial technique in image generation, aimed at
learning a codebook that encodes an image into a discrete token sequence.
Recent advancements have seen researchers exploring learning multi-modal
codebook (i.e., text-aligned codebook) by utilizing image caption semantics,
aiming to enhance codebook performance in cross-modal tasks. However, existing
image-text paired datasets exhibit a notable flaw in that the text descriptions
tend to be overly concise, failing to adequately describe the images and
provide sufficient semantic knowledge, resulting in limited alignment of text
and codebook at a fine-grained level. In this paper, we propose a novel
Text-Augmented Codebook Learning framework, named TA-VQ, which generates longer
text for each image using the visual-language model for improved text-aligned
codebook learning. However, the long text presents two key challenges: how to
encode text and how to align codebook and text. To tackle two challenges, we
propose to split the long text into multiple granularities for encoding, i.e.,
word, phrase, and sentence, so that the long text can be fully encoded without
losing any key semantic knowledge. Following this, a hierarchical encoder and
novel sampling-based alignment strategy are designed to achieve fine-grained
codebook-text alignment. Additionally, our method can be seamlessly integrated
into existing VQ models. Extensive experiments in reconstruction and various
downstream tasks demonstrate its effectiveness compared to previous
state-of-the-art approaches.",['cs.CV'],2502.03761," Unmanned aerial vehicles (UAVs) are widely used for object detection.
However, the existing UAV-based object detection systems are subject to severe
challenges, namely, their limited computation, energy and communication
resources, which limits the achievable detection performance. To overcome these
challenges, a UAV cognitive semantic communication system is proposed by
exploiting a knowledge graph. Moreover, we design a multi-scale codec for
semantic compression to reduce data transmission volume while guaranteeing
detection performance. Considering the complexity and dynamicity of UAV
communication scenarios, a signal-to-noise ratio (SNR) adaptive module with
robust channel adaptation capability is introduced. Furthermore, an object
detection scheme is proposed by exploiting the knowledge graph to overcome
channel noise interference and compression distortion. Simulation results
conducted on the practical aerial image dataset demonstrate that our proposed
semantic communication system outperforms benchmark systems in terms of
detection accuracy, communication robustness, and computation efficiency,
especially in dealing with low bandwidth compression ratios and low SNR
regimes.",['eess.SP'],False,,,,"Towards Improved Text-Aligned Codebook Learning: Multi-Hierarchical
  Codebook-Text Alignment with Long Text","UAV Cognitive Semantic Communications Enabled by Knowledge Graph for
  Robust Object Detection"
neg-d2-128,2025-01-22,,2501.13217," In 1985, Chv\'{a}tal introduced the concept of star cutsets as a means to
investigate the properties of perfect graphs, which inspired many researchers
to study cutsets with some specific structures, for example, star cutsets,
clique cutsets, stable cutsets. In recent years, approximation algorithms have
developed rapidly, the computational complexity associated with determining the
minimum vertex cut possessing a particular structural property have attracted
considerable academic attention.
  In this paper, we demonstrate that determining whether there is a matching
vertex-cutset in $H$ with size at most $k$, is $\mathbf{NP}$-complete, where
$k$ is a given positive integer and $H$ is a connected graph. Furthermore, we
demonstrate that for a connected graph $H$, there exists a $2$-approximation
algorithm in $O(nm^2)$ for us to find a minimum matching vertex-cutset.
Finally, we show that every plane graph $H$ satisfying $H\not\in\{K_2, K_4\}$
contains a matching vertex-cutset with size at most three, and this bound is
tight.","['cs.DS', 'math.CO']",2501.13597," Spectral clustering is a powerful technique for clustering high-dimensional
data, utilizing graph-based representations to detect complex, non-linear
structures and non-convex clusters. The construction of a similarity graph is
essential for ensuring accurate and effective clustering, making graph
structure learning (GSL) central for enhancing spectral clustering performance
in response to the growing demand for scalable solutions. Despite advancements
in GSL, there is a lack of comprehensive surveys specifically addressing its
role within spectral clustering. To bridge this gap, this survey presents a
comprehensive review of spectral clustering methods, emphasizing on the
critical role of GSL. We explore various graph construction techniques,
including pairwise, anchor, and hypergraph-based methods, in both fixed and
adaptive settings. Additionally, we categorize spectral clustering approaches
into single-view and multi-view frameworks, examining their applications within
one-step and two-step clustering processes. We also discuss multi-view
information fusion techniques and their impact on clustering data. By
addressing current challenges and proposing future research directions, this
survey provides valuable insights for advancing spectral clustering
methodologies and highlights the pivotal role of GSL in tackling large-scale
and high-dimensional data clustering tasks.",['cs.LG'],False,,,,Complexity and Algorithm for the Matching vertex-cutset Problem,"A Comprehensive Survey on Spectral Clustering with Graph Structure
  Learning"
neg-d2-129,2025-03-03,,2503.02077," Designing effective reward functions in multi-agent reinforcement learning
(MARL) is a significant challenge, often leading to suboptimal or misaligned
behaviors in complex, coordinated environments. We introduce Multi-agent
Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality (M3HF),
a novel framework that integrates multi-phase human feedback of mixed quality
into the MARL training process. By involving humans with diverse expertise
levels to provide iterative guidance, M3HF leverages both expert and non-expert
feedback to continuously refine agents' policies. During training, we
strategically pause agent learning for human evaluation, parse feedback using
large language models to assign it appropriately and update reward functions
through predefined templates and adaptive weight by using weight decay and
performance-based adjustments. Our approach enables the integration of nuanced
human insights across various levels of quality, enhancing the interpretability
and robustness of multi-agent cooperation. Empirical results in challenging
environments demonstrate that M3HF significantly outperforms state-of-the-art
methods, effectively addressing the complexities of reward design in MARL and
enabling broader human participation in the training process.","['cs.MA', 'cs.AI', 'cs.LG']",2503.17472," Our goal is to estimate the total gas mass in the direction of the Central
Molecular Zone (CMZ), quantify the various uncertainties associated, and
discuss the implications for the estimates of CR energy densities and dust
opacities. The $H_{\rm{I}}$ 21 cm line and the carbon monoxide isotopes
($^{12}\rm{CO}$, $^{13}\rm{CO}$ and $\rm{C}^{18}\rm{O}$) line emission maps are
used to derive the total gas column density. The gas in the CMZ is separated
from the disk contribution in position and velocity thanks to its different
properties in term of velocity dispersion and brightness ratio of CO isotopes.
The variations of the $X_{\rm{CO}}$ factors are modelled relying on both
theoretical trends from simulations and empirical corrections. We use the new
gas column density estimated together with gamma-ray and dust emission
measurements to derive the CR energy density and dust opacities, respectively.
The $X_{\rm{CO}}$ values in the CMZ range from $(0.32 - 1.37) \ \times$
$10^{20}$ cm$^{-2}$ K$^{-1}$ km$^{-1}$ s, with a distribution that is highly
asymmetric and skewed. The median value is $ \rm{\overline{X}_{CO}^{CMZ}} =
0.39 \ \times$ $10^{20}$ cm$^{-2}$ K$^{-1}$ km$^{-1}$ s. The total gas mass in
the CMZ is estimated to be $2.3_{-0.3}^{+0.3}\times10^{7} \; \rm{M_{\odot}}$
with $\sim 10 \%$ contribution from the atomic phase. Without removing the disk
contamination the total mass is about twice higher, and the atomic gas fraction
increases to $\sim30\%$. The cosmic-ray (CR) energy density in the CMZ,
assuming a 1/r profile, is higher by a factor of two compared to the previous
calculations at TeV energies. Using molecular gas tracers which probes only the
densest molecular cores leads to an overestimation of the CR energy density,
while ignoring the foreground/background contribution leads to an
underestimation of the CR energy density in the CMZ.","['astro-ph.GA', 'astro-ph.HE']",False,,,,"M3HF: Multi-agent Reinforcement Learning from Multi-phase Human Feedback
  of Mixed Quality","Cosmic rays, gas and dust in the Central Molecular Zone I -- $X_{CO}$
  factors, cosmic-ray densities and dust opacities"
neg-d2-130,2025-02-15,,2502.10786," Tuberculosis (TB) remains a formidable global health challenge, driven by
complex spatiotemporal transmission dynamics and influenced by factors such as
population mobility and behavioral changes. We propose an Epidemic-Guided Deep
Learning (EGDL) approach that fuses mechanistic epidemiological principles with
advanced deep learning techniques to enhance early warning systems and
intervention strategies for TB outbreaks. Our framework is built upon a
networked Susceptible-Infectious-Recovered (SIR) model augmented with a
saturated incidence rate and graph Laplacian diffusion, capturing both
long-term transmission dynamics and region-specific population mobility
patterns. Compartmental model parameters are rigorously estimated using
Bayesian inference via the Markov Chain Monte Carlo (MCMC) approach.
Theoretical analysis leveraging the comparison principle and Green's formula
establishes global stability properties of the disease-free and endemic
equilibria. Building on these epidemiological insights, we design two
forecasting architectures, EGDL-Parallel and EGDL-Series, that integrate the
mechanistic outputs of the networked SIR model within deep neural networks.
This integration mitigates the overfitting risks commonly encountered in
data-driven methods and filters out noise inherent in surveillance data,
resulting in reliable forecasts of real-world epidemic trends. Experiments
conducted on TB incidence data from 47 prefectures in Japan demonstrate that
our approach delivers robust and accurate predictions across multiple time
horizons (short to medium-term forecasts). Additionally, incorporating
uncertainty quantification through conformal prediction enhances the model's
practical utility for guiding targeted public health interventions.","['cs.LG', 'q-bio.QM', 'stat.ML']",2502.11524," In this paper we deal with generalizations of the Mahler volume product for
log-concave functions. We show that the polarity transform $\mathcal A$ can be
rescaled so that the Mahler product it induces has upper and lower bounds of
the same asymptotics. We discuss a similar result for the $\mathcal J$
transform.
  As an application, we extend the K\""onig-Milman duality of entropy result to
the class of geometric log-concave functions.",['math.FA'],False,,,,"Epidemic-guided deep learning for spatiotemporal forecasting of
  Tuberculosis outbreak",The Scaled Polarity transform and related inequalities
neg-d2-131,2025-02-14,,2502.10561," Landmarks are critical in navigation, supporting self-orientation and mental
model development. Similar to sighted people, people with low vision (PLV)
frequently look for landmarks via visual cues but face difficulties identifying
some important landmarks due to vision loss. We first conducted a formative
study with six PLV to characterize their challenges and strategies in landmark
selection, identifying their unique landmark categories (e.g., area
silhouettes, accessibility-related objects) and preferred landmark
augmentations. We then designed VisiMark, an AR interface that supports
landmark perception for PLV by providing both overviews of space structures and
in-situ landmark augmentations. We evaluated VisiMark with 16 PLV and found
that VisiMark enabled PLV to perceive landmarks they preferred but could not
easily perceive before, and changed PLV's landmark selection from only
visually-salient objects to cognitive landmarks that are more important and
meaningful. We further derive design considerations for AR-based landmark
augmentation systems for PLV.",['cs.HC'],2502.15168," Style embeddings are useful for stylistic analysis and style transfer;
however, only English style embeddings have been made available. We introduce
Multilingual StyleDistance (mStyleDistance), a multilingual style embedding
model trained using synthetic data and contrastive learning. We train the model
on data from nine languages and create a multilingual STEL-or-Content benchmark
(Wegmann et al., 2022) that serves to assess the embeddings' quality. We also
employ our embeddings in an authorship verification task involving different
languages. Our results show that mStyleDistance embeddings outperform existing
models on these multilingual style benchmarks and generalize well to unseen
features and languages. We make our model publicly available at
https://huggingface.co/StyleDistance/mstyledistance .",['cs.CL'],False,,,,"VisiMark: Characterizing and Augmenting Landmarks for People with Low
  Vision in Augmented Reality to Support Indoor Navigation",mStyleDistance: Multilingual Style Embeddings and their Evaluation
neg-d2-132,2025-03-10,,2503.06971," Europa, Jupiter's second Galilean moon, is believed to host a subsurface
ocean in contact with a rocky mantle, where hydrothermal activity may drive the
synthesis of organic molecules. Of these molecules, abiotic synthesis of
aromatic amino acids is unlikely, and their detection on Europa could be
considered a biosignature. Fluorescence from aromatic amino acids, with
characteristic emissions in the 200-400 nanometer wavelength range, can be
induced by a laser and may be detectable where ocean material has been
relatively recently emplaced on Europa's surface, as indicated by geologically
young terrain and surface features. However, surface bombardment by charged
particles from the Jovian magnetosphere and solar ultraviolet (UV) radiation
degrades organic molecules, limiting their longevity. We model radiolysis and
photolysis of aromatic amino acids embedded in ice, showing dependencies on
hemispheric and latitudinal patterns of charged particle bombardment and ice
phase. We demonstrate that biosignatures contained within freshly deposited ice
in high-latitude regions on the surface of Europa are detectable using
laser-induced UV fluorescence, even from an orbiting spacecraft.",['astro-ph.EP'],2502.0995," Duminil-Copin and Manolescu (2022) recently proved the scaling relations for
planar Fortuin-Kasteleyn (FK) percolation. In particular, they showed that the
one-arm exponent and the mixing rate exponent are sufficient to derive the
other near-critical exponents. The scaling limit of critical FK percolation is
conjectured to be a conformally invariant random collection of loops called the
conformal loop ensemble (CLE). In this paper, we define the CLE analog of the
mixing rate exponent. Assuming the convergence of FK percolation to CLE, we
show that the mixing rate exponent for FK percolation agrees with that of CLE.
We prove that the CLE$_\kappa$ mixing rate exponent equals $\frac{3
\kappa}{8}-1$, thereby answering Question 3 of Duminil-Copin and Manolescu
(2022). The derivation of the CLE exponent is based on an exact formula for the
Radon-Nikodym derivative between the marginal laws of the odd-level and
even-level CLE loops, which is obtained from the coupling between Liouville
quantum gravity and CLE.","['math.PR', 'math-ph', 'math.MP']",False,,,,Fluorescent Biomolecules Detectable in Near-Surface Ice on Europa,Mixing rate exponent of planar Fortuin-Kasteleyn percolation
neg-d2-133,2025-01-06,,2501.02945," Foundation models have become popular in forecasting due to their ability to
make accurate predictions, even with minimal fine-tuning on specific datasets.
In this paper, we demonstrate how the newly released regression variant of
TabPFN, a general tabular foundation model, can be applied to time series
forecasting. We propose a straightforward approach, TabPFN-TS, which pairs
TabPFN with simple feature engineering to achieve strong forecasting
performance. Despite its simplicity and with only 11M parameters, TabPFN-TS
outperforms Chronos-Mini, a model of similar size, and matches or even slightly
outperforms Chronos-Large, which has 65-fold more parameters. A key strength of
our method lies in its reliance solely on artificial data during pre-training,
avoiding the need for large training datasets and eliminating the risk of
benchmark contamination.",['cs.LG'],2502.05544," The semiclassical Boltzmann equation is widely used to study transport
effects. It is usually introduced in an intuitive fashion, which could cause
confusion, e.g., over the collision integral with skew scattering. Actually,
the Boltzmann equation is closely linked to the quantum density matrix,
although term-by-term correspondence between the two is yet to be established.
Here we start from the quantum Liouville equation in the interactive picture
and show that the diagonal components of the equation yield the Boltzmann
equation in homogeneous systems in an applied uniform electric field in the
semiclassical limit, while the off-diagonal components give the anomalous
velocity induced by Berry curvature and the side-jump velocity. The
skew-scattering contribution is obtained when we include corrections beyond the
first-Born approximation. The result derived from the denstiy matrix agrees
with the semiclassical one from wave-packet analysis, showing that the
semiclassical Boltzmann equation is more than an equation built from intuition,
and it can be derived with the density matrix. Our work further clarifies the
origin of the equation and eliminates the puzzles surrounding it.",['cond-mat.mes-hall'],False,,,,"The Tabular Foundation Model TabPFN Outperforms Specialized Time Series
  Forecasting Models Based on Simple Features","Quantum kinetic theory of semiclassical Boltzmann equation with side
  jump and skew scattering"
neg-d2-134,2025-01-28,,2501.16731," The gradient type of methods has been a competitive choice in solving large
scale problems arising from various applications such as machine learning.
However, there is still space to accelerate the gradient methods. To this end,
in this paper, we pay attention to the cyclic steepest descent method (CSD),
and prove that the CSD method has a gradient subsequence that is
R-superlinearly convergent for the 2-dimensional strictly convex quadratic
case. Moreover, we propose a new gradient method called triangle steepest
descent method (TSD) which has a parameter $j$ to control the number of cycles.
This method is motivated by utilizing a geometric property of the steepest
descent method (SD) method to get around the zigzag behavior. We show that the
TSD method is at least R-linearly convergent for strictly convex quadratic
problems. The advantage of the TSD method is that it is not sensitive to the
condition number of a strictly convex quadratic problem. For example, it
performs better than other competitive gradient methods when the condition
number reaches 1e20 or 1e100 for some strictly convex quadratic problems.
Extensive numerical results verify the efficiency of the TSD method compared to
other types of gradient methods.",['math.OC'],2501.0963," The Chern-Simons gravitational term during inflation is usually coupled to
the inflaton field. The resulting theory suffers from ghost-field formation in
the tensor sector, which limits the observational effects of P-violation on
cosmological correlators. In this work, we consider the Chern-Simons term
coupled to an isocurvature component in a multi-field model of inflation. Since
the resulting theory does not affect the quadratic action of tensor
perturbations, ghost fields do not appear. This operator provides (P-violating)
interactions between the isocurvature perturbation and the curvature and tensor
perturbations. We show that combining these couplings with interactions between
the curvature and isocurvature components coming from a turning trajectory, the
resulting $\langle sst \rangle_{PV}$ non-Gaussianities can reach $f^{sst,
PV}_{\rm NL}=B_{PV}^{\zeta\zeta h}(k,k,k)/P^2_{\zeta}(k)\sim \mathcal O(1)$
within the parameter space of the theory. Our result motivates the systematic
study of the Chern-Simons gravitational term coupled to isocurvature fields in
multi-field models of inflation with couplings between the curvature and
isocurvature fields or other mechanisms that transfer effects on the
isocurvature field into the curvature field.","['astro-ph.CO', 'hep-th']",False,,,,"On the acceleration of gradient methods: the triangle steepest descent
  method",Chern-Simons gravitational term coupled to an isocurvature field
neg-d2-135,2025-03-11,,2503.08257," A dexterous hand capable of grasping any object is essential for the
development of general-purpose embodied intelligent robots. However, due to the
high degree of freedom in dexterous hands and the vast diversity of objects,
generating high-quality, usable grasping poses in a robust manner is a
significant challenge. In this paper, we introduce DexGrasp Anything, a method
that effectively integrates physical constraints into both the training and
sampling phases of a diffusion-based generative model, achieving
state-of-the-art performance across nearly all open datasets. Additionally, we
present a new dexterous grasping dataset containing over 3.4 million diverse
grasping poses for more than 15k different objects, demonstrating its potential
to advance universal dexterous grasping. The code of our method and our dataset
will be publicly released soon.","['cs.CV', 'cs.AI', 'cs.RO']",2502.19477," Effective field theories (EFTs) parametrize our ignorance of the underlying
UV theory through their Wilson coefficients. However, not all values of these
coefficients are consistent with fundamental physical principles. In this
paper, we explore the consequences of imposing causal propagation on the
comoving curvature perturbation in the EFT of inflation, particularly its
impact on the primordial power spectrum and the effective sound speed
$c_s^\text{eff}$. We investigate scenarios where $c_s^\text{eff}$ undergoes a
transition, remaining consistent with CMB constraints at early times but later
experiencing a drastic change, becoming highly subluminal. Such scenarios allow
the primordial power spectrum to grow at small scales, potentially leading to
the formation of primordial black holes or the generation of scalar-induced
gravitational waves. We find the generic feature that in a causal theory,
luminal sound speeds imply a free theory, effectively constraining the
dynamics. Additionally, we obtain that when considering natural values for the
Wilson coefficients, maintaining the validity of the EFT and the weakly coupled
regime, and enforcing causal propagation of the EFT modes, the power spectrum
cannot increase drastically. This imposes significant constraints on the
parameter space of models aiming to produce such features.","['hep-th', 'astro-ph.CO']",False,,,,"DexGrasp Anything: Towards Universal Robotic Dexterous Grasping with
  Physics Awareness",Causality Bounds on the Primordial Power Spectrum
neg-d2-136,2025-03-23,,2503.18033," Omnimatte aims to decompose a given video into semantically meaningful
layers, including the background and individual objects along with their
associated effects, such as shadows and reflections. Existing methods often
require extensive training or costly self-supervised optimization. In this
paper, we present OmnimatteZero, a training-free approach that leverages
off-the-shelf pre-trained video diffusion models for omnimatte. It can remove
objects from videos, extract individual object layers along with their effects,
and composite those objects onto new videos. We accomplish this by adapting
zero-shot image inpainting techniques for video object removal, a task they
fail to handle effectively out-of-the-box. We then show that self-attention
maps capture information about the object and its footprints and use them to
inpaint the object's effects, leaving a clean background. Additionally, through
simple latent arithmetic, object layers can be isolated and recombined
seamlessly with new video layers to produce new videos. Evaluations show that
OmnimatteZero not only achieves superior performance in terms of background
reconstruction but also sets a new record for the fastest Omnimatte approach,
achieving real-time performance with minimal frame runtime.",['cs.CV'],2501.02262," The growth of a large-scale magnetic field in the Sun and stars is usually
possible when the dynamo number (D) is above a critical value Dc. As the star
ages, its rotation rate and thus D decrease. Hence, the question is how far the
solar dynamo is from the critical dynamo transition. To answer this question,
we have performed a set of simulations using Babcock-Leighton type dynamo
models at different values of dynamo supercriticality and analyzed various
features of magnetic cycle. By comparing the recovery rates of the dynamo from
the Maunder minimum and statistics (numbers and durations) of the grand minima
and maxima with that of observations and we show that the solar dynamo is only
about two times critical and thus not highly supercritical. The observed
correlation between the polar field proxy and the following cycle amplitudes
and Gnevyshev-Ohl rule are also compatible with this conclusion.","['astro-ph.SR', 'physics.plasm-ph', 'physics.space-ph']",False,,,,"OmnimatteZero: Training-free Real-time Omnimatte with Pre-trained Video
  Diffusion Models","Analyses of features of magnetic cycles at different amounts of dynamo
  supercriticality: Solar dynamo is about two times critical"
neg-d2-137,2025-03-05,,2503.03872," The self-gravitating skyrmion is an exact solution of the Einstein
$SU(2)$-Skyrme model describing a topological soliton with baryon number $B=1$,
living in a $4$-dimensional space-time in the presence of a cosmological
constant. Here we show that, using the maximal embedding Ansatz of $SU(2)$ into
$SU(N)$ in the Euler angles parametrization, this solution can be generalized
to include arbitrary values of the flavor number and, consequently, allowing
higher values of the topological charge. Also, we show that higher-order
corrections in the 't Hooft expansion can be considered while still preserving
the analytical nature of the solutions. Finally we will show that from the
gravitational solutions it is possible to construct skyrmions in flat
space-time at a finite volume.","['hep-th', 'gr-qc']",2501.02963," Power prices can be forecasted using data-driven models or fundamental
models. Data-driven models learn from historical patterns, while fundamental
models simulate electricity markets. Traditionally, fundamental models have
been too computationally demanding to allow for intrinsic parameter estimation
or frequent updates, which are essential for short-term forecasting. In this
paper, we propose a novel data-driven fundamental model that combines the
strengths of both approaches. We estimate the parameters of a fully fundamental
merit order model using historical data, similar to how data-driven models
work. This removes the need for fixed technical parameters or expert
assumptions, allowing most parameters to be calibrated directly to
observations. The model is efficient enough for quick parameter estimation and
forecast generation. We apply it to forecast German day-ahead electricity
prices and demonstrate that it outperforms both classical fundamental and
purely data-driven models. The hybrid model effectively captures price
volatility and sequential price clusters, which are becoming increasingly
important with the expansion of renewable energy sources. It also provides
valuable insights, such as fuel switches, marginal power plant contributions,
estimated parameters, dispatched plants, and power generation.","['stat.AP', 'econ.EM']",False,,,,Universal self-gravitating skyrmions,"A data-driven merit order: Learning a fundamental electricity price
  model"
neg-d2-138,2025-03-03,,2503.09614," To sustain innovation and safeguard national security, the U.S. must
strengthen domestic pathways to computing PhDs by engaging talented
undergraduates early - before they are committed to industry - with research
experiences, mentorship, and financial support for graduate studies.",['cs.CY'],2503.18033," Omnimatte aims to decompose a given video into semantically meaningful
layers, including the background and individual objects along with their
associated effects, such as shadows and reflections. Existing methods often
require extensive training or costly self-supervised optimization. In this
paper, we present OmnimatteZero, a training-free approach that leverages
off-the-shelf pre-trained video diffusion models for omnimatte. It can remove
objects from videos, extract individual object layers along with their effects,
and composite those objects onto new videos. We accomplish this by adapting
zero-shot image inpainting techniques for video object removal, a task they
fail to handle effectively out-of-the-box. We then show that self-attention
maps capture information about the object and its footprints and use them to
inpaint the object's effects, leaving a clean background. Additionally, through
simple latent arithmetic, object layers can be isolated and recombined
seamlessly with new video layers to produce new videos. Evaluations show that
OmnimatteZero not only achieves superior performance in terms of background
reconstruction but also sets a new record for the fastest Omnimatte approach,
achieving real-time performance with minimal frame runtime.",['cs.CV'],False,,,,"Reversing the Computing Research Workforce Shortfall: Bolstering
  Domestic Student Pathways to PhDs","OmnimatteZero: Training-free Real-time Omnimatte with Pre-trained Video
  Diffusion Models"
neg-d2-139,2025-02-27,,2502.20563," It is known that on directed graphs, the correlations between neighbours of a
given site vanish and thus simple mean-field-like arguments can be used to
describe exactly the behaviour of Ising-like systems. We analyse heterogeneous
modifications of such models where a fraction of agents is driven by the voter
or the antivoter dynamics. It turns out that voter agents do not affect the
dynamics of the model and it behaves like a pure Ising model. Antivoter agents
have a stronger impact since they act as a kind of noise, which weakens a
ferromagnetic ordering. Only when Ising spins are driven by the heat-bath
dynamics, the behaviour of the model is correctly described by the mean-field
approximation. The Metropolis dynamics generates some additional correlations
that render the mean-field approach approximate. Simulations on annealed
networks agree with the mean-field approximation but for the model with
antivoters and with the Metropolis dynamics only its heterogeneous version
provides such an agreement. Calculation of the Binder cumulant confirms that
critical points in our models with the heat-bath dynamics belong to the Ising
mean-field universality class. For the Metropolis dynamics, the phase
transition is most likely discontinuous, at least for not too many antivoters.",['cond-mat.stat-mech'],2501.16805," Spoofed traffic has been identified as one of the main issues of concern for
network hygiene nowadays, as it facilitates Distributed Denial-of-Service
(DDoS) attacks by hiding their origin and complicating forensic investigations.
Some indicators of poor network hygiene are packets with Bogon or Martian
source addresses representing either misconfigurations or spoofed packets.
Despite the development of Source Address Validation (SAV) techniques and
guidelines such as BCP 38 and BCP 84, Bogons are often overlooked in the
filtering practices of network operators. This study uses traceroute
measurements from the CAIDA Ark dataset, enriched with historical BGP routing
information from RIPE RIS and RouteViews, to investigate the prevalence of
Bogon addresses over seven years (2017-2023). Our analysis reveals widespread
non-compliance with best practices, with Bogon traffic detected across
thousands of ASes. Notably, 82.69%-97.83% of CAIDA Ark vantage points observe
paths containing Bogon IPs, primarily RFC1918 addresses. Additionally, 19.70%
of all analyzed traceroutes include RFC1918 addresses, while smaller
proportions involve RFC6598 (1.50%) and RFC3927 (0.10%) addresses. We identify
more than 13,000 unique ASes transiting Bogon traffic, with only 11.64%
appearing in more than half of the measurements. Cross-referencing with the
Spoofer project and MANRS initiatives shows a concerning gap: 62.67% of ASes
that do not filter packets with Bogon sources are marked as non-spoofable,
suggesting incomplete SAV implementation. Our contributions include an
assessment of network hygiene using the transiting of Bogon packets as a
metric, an analysis of the main types of Bogon addresses found in traceroutes,
and several proposed recommendations to address the observed gaps, enforcing
the need for stronger compliance with best practices to improve global network
security.",['cs.NI'],False,,,,"Mean-field approximation and phase transitions in an Ising-voter model
  on directed regular random graphs","Martians Among Us: Observing Private or Reserved IPs on the Public
  Internet"
neg-d2-140,2025-03-11,,2503.08334," In this paper, we study a diffuse interface model for two-phase immiscible
flows coupled by Navier-Stokes equations and mass-conserving Allen-Cahn
equations. The contact line (the intersection of the fluid-fluid interface with
the solid wall) moves along the wall when one fluid replaces the other, such as
in liquid spreading or oil-water displacement. The system is equipped with the
generalized Navier boundary conditions (GNBC) for the fluid velocity
${\boldsymbol u}$, and dynamic boundary condition or relaxation boundary
condition for the phase field variable $\phi$. We first obtain the
local-in-time existence of unique strong solutions to the 2D and 3D
Navier-Stokes/Allen-Cahn (NSAC) system with generalized Navier boundary
conditions and dynamic boundary condition. For the 2D case in channels, we
further show these solutions can be extended to any large time $T$.
Additionally, we prove the local-in-time strong solutions for systems with
generalized Navier boundary conditions and relaxation boundary condition in 3D
channels. Finally, we establish a global unique strong solution accompany with
some exponential decay estimates when the fluids are near phase separation
states and the contact angle closes to 90 degrees or the fluid-fluid interface
tension constant is small.",['math.AP'],2501.17464," We propose a new methodology to simulate the discounted penalty applied to a
wind-farm operator by violating ramp-rate limitation policies. It is assumed
that the operator manages a wind turbine plugged into a battery, which either
provides or stores energy on demand to avoid ramp-up and ramp-down events. The
battery stages, namely charging, discharging, or neutral, are modeled as a
semi-Markov process. During each charging/discharging period, the energy
stored/supplied is assumed to follow a modified Brownian bridge that depends on
three parameters. We prove the validity of our methodology by testing the model
on 10 years of real wind-power data and comparing real versus simulated
results.",['stat.AP'],False,,,,Navier-Stokes/Allen-Cahn system with moving contact line,"Modelling a storage system of a wind farm with a ramp-rate limitation: a
  semi-Markov modulated Brownian bridge approach"
neg-d2-141,2025-01-13,,2501.0719," Topological quantum materials that show strongly correlated electrons as well
as topological order, for which spin-orbit coupling is a key ingredient,
exhibit novel states of matter. One such example is the family of pyrochlore
iridates, featuring strong spin-orbital coupling, strong electron interactions
as well as geometric frustration, making them an ideal platform to study novel
topological phases. High-quality epitaxial pyrochlore iridate films, although
challenging to produce, provide a pathway to explore unconventional behaviours
and unravel the intrinsic properties of these largely unexplored materials.
Additionally, designing interfaces with specific properties is crucial to
create multilayered devices that can achieve significant technological
breakthroughs using topological states of these materials. This article reviews
experimental work on epitaxial pyrochlore iridate thin films, discussing
evidence of topological phases found in them. Future research directions are
outlined, which include exploring the rich tunability offered by chemical
doping, especially when combined with the design of epitaxial heterostructures.","['cond-mat.str-el', 'cond-mat.mtrl-sci']",2502.00133," Deep learning methods have demonstrated strong performance in objection
tasks; however, their ability to learn domain-specific applications with
limited training data remains a significant challenge. Transfer learning
techniques address this issue by leveraging knowledge from pre-training on
related datasets, enabling faster and more efficient learning for new tasks.
Finding the right dataset for pre-training can play a critical role in
determining the success of transfer learning and overall model performance. In
this paper, we investigate the impact of pre-training a YOLOv8n model on seven
distinct datasets, evaluating their effectiveness when transferred to the task
of polyp detection. We compare whether large, general-purpose datasets with
diverse objects outperform niche datasets with characteristics similar to
polyps. In addition, we assess the influence of the size of the dataset on the
efficacy of transfer learning. Experiments on the polyp datasets show that
models pre-trained on relevant datasets consistently outperform those trained
from scratch, highlighting the benefit of pre-training on datasets with shared
domain-specific features.","['cs.CV', 'cs.AI']",False,,,,Epitaxial thin films of pyrochlore iridates: a forward looking approach,"Exploring Transfer Learning for Deep Learning Polyp Detection in
  Colonoscopy Images Using YOLOv8"
neg-d2-142,2025-03-17,,2503.12966," Score-based generative models achieve state-of-the-art sampling performance
by denoising a distribution perturbed by Gaussian noise. In this paper, we
focus on a single deterministic denoising step, and compare the optimal
denoiser for the quadratic loss, we name ''full-denoising'', to the alternative
''half-denoising'' introduced by Hyv{\""a}rinen (2024). We show that looking at
the performances in term of distance between distribution tells a more nuanced
story, with different assumptions on the data leading to very different
conclusions. We prove that half-denoising is better than full-denoising for
regular enough densities, while full-denoising is better for singular densities
such as mixtures of Dirac measures or densities supported on a low-dimensional
subspace. In the latter case, we prove that full-denoising can alleviate the
curse of dimensionality under a linear manifold hypothesis.","['cs.LG', 'stat.ML']",2501.06062," In many practical natural language applications, user data are highly
sensitive, requiring anonymous uploads of text data from mobile devices to the
cloud without user identifiers. However, the absence of user identifiers
restricts the ability of cloud-based language models to provide personalized
services, which are essential for catering to diverse user needs. The trivial
method of replacing an explicit user identifier with a static user embedding as
model input still compromises data anonymization. In this work, we propose to
let each mobile device maintain a user-specific distribution to dynamically
generate user embeddings, thereby breaking the one-to-one mapping between an
embedding and a specific user. We further theoretically demonstrate that to
prevent the cloud from tracking users via uploaded embeddings, the local
distributions of different users should either be derived from a linearly
dependent space to avoid identifiability or be close to each other to prevent
accurate attribution. Evaluation on both public and industrial datasets using
different language models reveals a remarkable improvement in accuracy from
incorporating anonymous user embeddings, while preserving real-time inference
requirement.",['cs.LG'],False,,,,"Optimal Denoising in Score-Based Generative Models: The Role of Data
  Regularity","Personalized Language Model Learning on Text Data Without User
  Identifiers"
neg-d2-143,2025-02-18,,2502.12683," We investigate the nuclear Stark effect induced in hydrogen-like atomic
nuclei under super-intense laser fields. Since laser wavelengths are generally
larger than nuclear dimensions, direct laser-nucleus interaction is unfeasible.
Instead, this effect is induced indirectly through electron oscillations in the
laser field, which produce a periodic electric field that shifts the nuclear
energy levels. Using perturbation theory, we derive an expression for the
energy shift and dynamic polarizability of the nucleus as a function of laser
parameters. Our findings reveal that the Nuclear Stark effect can be controlled
by adjusting the laser frequency and intensity, potentially enabling
applications in nuclear and quantum optical systems.",['nucl-th'],2503.08661," This paper proposes a task-oriented co-design framework that integrates
communication, computing, and control to address the key challenges of
bandwidth limitations, noise interference, and latency in mission-critical
industrial Cyber-Physical Systems (CPS). To improve communication efficiency
and robustness, we design a task-oriented Joint Source-Channel Coding (JSCC)
using Information Bottleneck (IB) to enhance data transmission efficiency by
prioritizing task-specific information. To mitigate the perceived End-to-End
(E2E) delays, we develop a Delay-Aware Trajectory-Guided Control Prediction
(DTCP) strategy that integrates trajectory planning with control prediction,
predicting commands based on E2E delay. Moreover, the DTCP is co-designed with
task-oriented JSCC, focusing on transmitting task-specific information for
timely and reliable autonomous driving. Experimental results in the CARLA
simulator demonstrate that, under an E2E delay of 1 second (20 time slots), the
proposed framework achieves a driving score of 48.12, which is 31.59 points
higher than using Better Portable Graphics (BPG) while reducing bandwidth usage
by 99.19%.","['cs.IT', 'cs.CV', 'eess.IV', 'math.IT']",False,,,,"AC nuclear Stark effect in H-atom via super-intense laser-atom
  interaction","Task-Oriented Co-Design of Communication, Computing, and Control for
  Edge-Enabled Industrial Cyber-Physical Systems"
neg-d2-144,2025-03-06,,2503.04889," We classify gapped and gapless phases of non-Hermitian band structures on
two-dimensional nonorientable parameter spaces. Such spaces arise in a wide
range of physical systems in the presence of non-symmorphic parameter space
symmetries. For gapped phases, we find that nonorientable spaces provide a
natural setting for exploring fundamental structural problems in braid group
theory, such as torsion and conjugacy. Gapless phases, which host exceptional
points (EPs), explicitly violate the fermion doubling theorem, even in two-band
models. We demonstrate that EPs traversing the nonorientable parameter space
exhibit non-Abelian charge inversion. These braided phases and their
transitions leave distinct signatures in the form of bulk Fermi arc
degeneracies, offering a concrete route toward experimental realization and
verification.","['cond-mat.mes-hall', 'math-ph', 'math.MP', 'physics.optics', 'quant-ph']",2503.14077," The radiative open circuit voltage loss in a solar cell occurs because the
absorptance spectrum near the band gap shows gradual increase rather than sharp
step function like transition. This broadening effect has been attributed to
band gap fluctuations and or to Urbach tails. In this report, we use modelling
based on Planck s generalized law to distinguish between these two effects. Our
results demonstrate that Urbach tails have only a minimal effect on the
absorptance edge broadening and clarify that even an ideal direct semiconductor
with no band gap fluctuations shows broadening at the absorptance onset.
Furthermore, state of the art inorganic thin film solar cells often incorporate
a band gap gradient across their thickness, which can further contribute to
absorptance broadening. Using Cu(In,Ga)Se2 (CIGSe) absorbers as a case study,
we perform a comprehensive analysis of voltage losses through absolute
photoluminescence and electroluminescence spectroscopy, combined with
photospectrometry and high-spatial-resolution cathodoluminescence measurements.
We find that the loss analysis based on the combination of radiative,
generation and non-radiative losses is complete. Samples with a graded band gap
profile show more pronounced broadening of the absorptance onset and up to 16
mV higher radiative losses compared to the samples with uniform band gap. There
is indication, that band gap-graded samples also have larger lateral band gap
inhomogeneity.",['cond-mat.mtrl-sci'],False,,,,Exceptional Topology on Nonorientable Manifolds,"The effect of a band gap gradient on the radiative losses in the open
  circuit voltage of solar cells"
neg-d2-145,2025-02-06,,2502.04071," Following in the footsteps of the LHC, the Future Circular Collider (FCC)
plans to be the next multi-generational collider project. In the first stage,
FCC-ee will collide intense beams of electrons and positrons at centre of mass
energies between 88 and 365 GeV, making it an electroweak, flavour, Higgs and
top factory. The unprecedented statistical precision requires FCC-ee
experiments to limit their systematic uncertainties to the very minimum.
  The precise reconstruction of the interaction vertices is central to most
measurements at FCC-ee, such as rare flavour physics processes and the
measurement of Higgs and Z decays to bottom and charm quarks and taus. This
contribution will discuss the requirements of FCC-ee vertex detectors, from the
necessary impact parameter resolution via the challenging collision environment
at the Z pole to the tight requirement on the material budget, which should be
kept below 0.3% of a radiation length per detection layer. Next, the proposed
vertex detector designs for FCC-ee are shortly presented, and an outlook is
given on novel detector designs and features.
  The requirements for the vertexing performance translate into requirements
for the sensors used for the vertex detector. As discussed in this
contribution, they need to feature a spatial resolution of about 3 $\mu$m and
provide timing information of O($\mu$s-ns) while keeping power consumption
minimal to allow for air-cooling of the detector - minimising the detector
material budget.
  The only type of sensor capable of aiming to fulfil such requirements are
CMOS Monolithic Active Pixel Sensors (MAPS), which combine signal generation,
amplification and readout into a single silicon die. Therefore, the rest of
this contribution will present an overview of existing and planned MAPS
technologies and prototypes aiming to fulfil the stringent FCC-ee vertex
detector requirements.","['hep-ex', 'physics.ins-det']",2501.11955," Mean field games (MFGs) offer a versatile framework for modeling large-scale
interactive systems across multiple domains. This paper builds upon a previous
work, by developing a state-of-the-art unified approach to decode or design the
unknown stationary state of MFGs, in addition to the underlying parameter
functions governing their behavior. This result is novel, even in the general
realm of inverse problems for nonlinear PDEs. By enabling agents to distill
crucial insights from observed data and unveil intricate hidden structures and
unknown states within MFG systems, our approach surmounts a significant
obstacle, enhancing the applicability of MFGs in real-world scenarios. This
advancement not only enriches our understanding of MFG dynamics but also
broadens the scope for their practical deployment in various contexts.","['math.AP', 'math.OC']",False,,,,The vertexing challenge at FCC-ee,"Simultaneously decoding the unknown stationary state and function
  parameters for mean field games"
neg-d2-146,2025-03-16,,2503.12378," Structural vector autoregressive (SVAR) models are widely used to analyze the
simultaneous relationships between multiple time-dependent data. Various
statistical inference methods have been studied to overcome the identification
problems of SVAR models. However, most of these methods impose strong
assumptions for innovation processes such as the uncorrelation of components.
In this study, we relax the assumptions for innovation processes and propose an
identification method for SVAR models under the zero-restrictions on the
coefficient matrices, which correspond to sufficient conditions for LU
decomposition of the coefficient matrices of the reduced form of the SVAR
models. Moreover, we establish asymptotically normal estimators for the
coefficient matrices and impulse responses, which enable us to construct test
statistics for the simultaneous relationships of time-dependent data. The
finite-sample performance of the proposed method is elucidated by numerical
simulations. We also present an example of an empirical study that analyzes the
impact of policy rates on unemployment and prices.","['econ.EM', 'stat.AP']",2502.01219," Dynamical systems can be coupled in a manner that is designed to drive the
resulting dynamics onto a specified lower dimensional submanifold in the phase
space of the combined system. On the submanifold, the variables of the two
systems have a well-defined unique functional relationship. This process can
thus be viewed as a control technique that ensures generalized synchronization.
Depending on the nature of the dynamical systems and the specified submanifold,
different coupling functions can be derived in order to achieve a desired
control objective. We discuss the circuit implementations of this strategy in
representative examples of coupled chaotic dynamical systems, namely Lorenz
oscillators",['nlin.CD'],False,,,,"Identification and estimation of structural vector autoregressive models
  via LU decomposition",Control Strategy for Generalized Synchrony in Coupled Dynamical Systems
neg-d2-147,2025-01-10,,2501.06374," This paper introduces AFRIDOC-MT, a document-level multi-parallel translation
dataset covering English and five African languages: Amharic, Hausa, Swahili,
Yor\`ub\'a, and Zulu. The dataset comprises 334 health and 271 information
technology news documents, all human-translated from English to these
languages. We conduct document-level translation benchmark experiments by
evaluating neural machine translation (NMT) models and large language models
(LLMs) for translations between English and these languages, at both the
sentence and pseudo-document levels. These outputs are realigned to form
complete documents for evaluation. Our results indicate that NLLB-200 achieved
the best average performance among the standard NMT models, while GPT-4o
outperformed general-purpose LLMs. Fine-tuning selected models led to
substantial performance gains, but models trained on sentences struggled to
generalize effectively to longer documents. Furthermore, our analysis reveals
that some LLMs exhibit issues such as under-generation, repetition of words or
phrases, and off-target translations, especially for African languages.",['cs.CL'],2503.06396," The problem of finding a minimum vertex cover (MVC) in a graph is a
well-known NP-hard problem with significant practical applications in
optimization and scheduling. Its complexity, combined with the increasing scale
of problems, underscores the need for efficient and effective algorithms.
However, existing heuristic algorithms for MVC often rely on simplistic
initialization strategies and overlook the impact of edge attributes and
neighborhood information on vertex selection. In this paper, we introduce
GCNIVC, a novel heuristic search algorithm designed to address the limitations
of existing methods for solving MVC problems in large-scale graphs. Our
approach features two main innovations. First, it utilizes a Graph
Convolutional Network (GCN) to capture the global structure of graphs, which
enables the generation of high-quality initial solutions that enhance the
efficiency of the subsequent search process. Second, GCNIVC introduces a new
heuristic that employs three containers and the concept of double-covered edges
(dc-edges), improving search efficiency and providing greater flexibility for
adding and removing operations based on edge attributes. Through extensive
experiments on benchmark datasets, we demonstrate that GCNIVC outperforms
state-of-the-art MVC algorithms in terms of both accuracy and efficiency. Our
results highlight the effectiveness of GCNIVC's GCN-assisted initialization and
its edge-informed search strategy. This study not only advances the
understanding of MVC problem-solving but also contributes a new tool for
addressing large-scale graph optimization challenges.",['cs.AI'],False,,,,AFRIDOC-MT: Document-level MT Corpus for African Languages,"Optimizing Minimum Vertex Cover Solving via a GCN-assisted Heuristic
  Algorithm"
neg-d2-148,2025-01-14,,2501.08019," As urbanization accelerates, open spaces are increasingly recognized for
their role in enhancing sustainability and well-being, yet they remain
underexplored compared to built spaces. This study introduces an AI-driven
framework that integrates machine learning models (MLMs) and explainable AI
techniques to optimize Sky View Factor (SVF) and visibility, key spatial
metrics influencing thermal comfort and perceived safety in urban spaces.
Unlike global optimization methods, which are computationally intensive and
impractical for localized adjustments, this framework supports incremental
design improvements with lower computational costs and greater flexibility. The
framework employs SHapley Adaptive Explanations (SHAP) to analyze feature
importance and Counterfactual Explanations (CFXs) to propose minimal design
changes. Simulations tested five MLMs, identifying XGBoost as the most
accurate, with building width, park area, and heights of surrounding buildings
as critical for SVF, and distances from southern buildings as key for
visibility. Compared to Genetic Algorithms, which required approximately 15/30
minutes across 3/4 generations to converge, the tested CFX approach achieved
optimized results in 1 minute with a 5% RMSE error, demonstrating significantly
faster performance and suitability for scalable retrofitting strategies. This
interpretable and computationally efficient framework advances urban
performance optimization, providing data-driven insights and practical
retrofitting solutions for enhancing usability and environmental quality across
diverse urban contexts.","['cs.LG', 'cs.AI', 'cs.CY']",2503.16639," Realistic crowd simulations are essential for immersive virtual environments,
relying on both individual behaviors (microscopic dynamics) and overall crowd
patterns (macroscopic characteristics). While recent data-driven methods like
deep reinforcement learning improve microscopic realism, they often overlook
critical macroscopic features such as crowd density and flow, which are
governed by spatio-temporal spawn dynamics, namely, when and where agents enter
a scene. Traditional methods, like random spawn rates, stochastic processes, or
fixed schedules, are not guaranteed to capture the underlying complexity or
lack diversity and realism. To address this issue, we propose a novel approach
called nTPP-GMM that models spatio-temporal spawn dynamics using Neural
Temporal Point Processes (nTPPs) that are coupled with a spawn-conditional
Gaussian Mixture Model (GMM) for agent spawn and goal positions. We evaluate
our approach by orchestrating crowd simulations of three diverse real-world
datasets with nTPP-GMM. Our experiments demonstrate the orchestration with
nTPP-GMM leads to realistic simulations that reflect real-world crowd scenarios
and allow crowd analysis.",['cs.LG'],False,,,,"An AI-driven framework for rapid and localized optimizations of urban
  open spaces","Whenever, Wherever: Towards Orchestrating Crowd Simulations with
  Spatio-Temporal Spawn Dynamics"
neg-d2-149,2025-03-05,,2503.03434," Speculative decoding accelerates inference in large language models (LLMs) by
generating draft tokens for target model verification. Current approaches for
obtaining draft tokens rely on lightweight draft models or additional model
structures to generate draft tokens and retrieve context from databases. Due to
the draft model's small size and limited training data, model-based speculative
decoding frequently becomes less effective in out-of-domain scenarios.
Additionally, the time cost of the drafting phase results in a low upper limit
on acceptance length during the verification step, limiting overall efficiency.
This paper proposes RASD (Retrieval-Augmented Speculative Decoding), which
adopts retrieval methods to enhance model-based speculative decoding. We
introduce tree pruning and tree fusion to achieve this. Specifically, we
develop a pruning method based on the draft model's probability distribution to
construct the optimal retrieval tree. Second, we employ the longest prefix
matching algorithm to merge the tree generated by the draft model with the
retrieval tree, resulting in a unified tree for verification. Experimental
results demonstrate that RASD achieves state-of-the-art inference acceleration
across tasks such as DocQA, Summary, Code, and In-Domain QA. Moreover, RASD
exhibits strong scalability, seamlessly integrating with various speculative
decoding approaches, including both generation-based and retrieval-based
methods.","['cs.CL', 'cs.AI']",2501.17868," Due to its ability to precisely control wireless beams, holographic
multiple-input multiple-output (HMIMO) is expected to be a promising solution
to achieve high-accuracy localization. However, as the scale of HMIMO increases
to improve beam control capability, the corresponding near-field (NF) region
expands, indicating that users may exist in both NF and far-field (FF) regions
with different electromagnetic transmission characteristics. As a result,
existing methods for pure NF or FF localization are no longer applicable. We
consider a hybrid NF and FF localization scenario in this paper, where a base
station (BS) locates multiple users in both NF and FF regions with the aid of a
reconfigurable intelligent surface (RIS), which is a low-cost implementation of
HMIMO. In such a scenario, it is difficult to locate the users and optimize the
RIS phase shifts because whether the location of the user is in the NF or FF
region is unknown, and the channels of different users are coupled. To tackle
this challenge, we propose a RIS-enabled localization method that searches the
users in both NF and FF regions and tackles the coupling issue by jointly
estimating all user locations. We derive the localization error bound by
considering the channel coupling and propose an RIS phase shift optimization
algorithm that minimizes the derived bound. Simulations show the effectiveness
of the proposed method and demonstrate the performance gain compared to pure NF
and FF techniques.",['eess.SP'],False,,,,RASD: Retrieval-Augmented Speculative Decoding,Hybrid Near-field and Far-field Localization with Holographic MIMO
neg-d2-150,2025-02-20,,2502.14381," dtaianomaly is an open-source Python library for time series anomaly
detection, designed to bridge the gap between academic research and real-world
applications. Our goal is to (1) accelerate the development of novel
state-of-the-art anomaly detection techniques through simple extensibility; (2)
offer functionality for large-scale experimental validation; and thereby (3)
bring cutting-edge research to business and industry through a standardized
API, similar to scikit-learn to lower the entry barrier for both new and
experienced users. Besides these key features, dtaianomaly offers (1) a broad
range of built-in anomaly detectors, (2) support for time series preprocessing,
(3) tools for visual analysis, (4) confidence prediction of anomaly scores, (5)
runtime and memory profiling, (6) comprehensive documentation, and (7)
cross-platform unit testing.
  The source code of dtaianomaly, documentation, code examples and installation
guides are publicly available at https://github.com/ML-KULeuven/dtaianomaly.","['cs.LG', 'cs.DB']",2502.20563," It is known that on directed graphs, the correlations between neighbours of a
given site vanish and thus simple mean-field-like arguments can be used to
describe exactly the behaviour of Ising-like systems. We analyse heterogeneous
modifications of such models where a fraction of agents is driven by the voter
or the antivoter dynamics. It turns out that voter agents do not affect the
dynamics of the model and it behaves like a pure Ising model. Antivoter agents
have a stronger impact since they act as a kind of noise, which weakens a
ferromagnetic ordering. Only when Ising spins are driven by the heat-bath
dynamics, the behaviour of the model is correctly described by the mean-field
approximation. The Metropolis dynamics generates some additional correlations
that render the mean-field approach approximate. Simulations on annealed
networks agree with the mean-field approximation but for the model with
antivoters and with the Metropolis dynamics only its heterogeneous version
provides such an agreement. Calculation of the Binder cumulant confirms that
critical points in our models with the heat-bath dynamics belong to the Ising
mean-field universality class. For the Metropolis dynamics, the phase
transition is most likely discontinuous, at least for not too many antivoters.",['cond-mat.stat-mech'],False,,,,dtaianomaly: A Python library for time series anomaly detection,"Mean-field approximation and phase transitions in an Ising-voter model
  on directed regular random graphs"
neg-d2-151,2025-01-22,,2501.1332," As Artificial Intelligence (AI) systems become increasingly integrated into
various aspects of daily life, concerns about privacy and ethical
accountability are gaining prominence. This study explores stakeholder
perspectives on privacy in AI systems, focusing on educators, parents, and AI
professionals. Using qualitative analysis of survey responses from 227
participants, the research identifies key privacy risks, including data
breaches, ethical misuse, and excessive data collection, alongside perceived
benefits such as personalized services, enhanced efficiency, and educational
advancements. Stakeholders emphasized the need for transparency,
privacy-by-design, user empowerment, and ethical oversight to address privacy
concerns effectively. The findings provide actionable insights into balancing
the benefits of AI with robust privacy protections, catering to the diverse
needs of stakeholders. Recommendations include implementing selective data use,
fostering transparency, promoting user autonomy, and integrating ethical
principles into AI development. This study contributes to the ongoing discourse
on ethical AI, offering guidance for designing privacy-centric systems that
align with societal values and build trust among users. By addressing privacy
challenges, this research underscores the importance of developing AI
technologies that are not only innovative but also ethically sound and
responsive to the concerns of all stakeholders.","['cs.CY', 'cs.AI']",2502.08388," In this paper, we study the shadow and observational image of the Kerr-like
Loop Quantum Gravity (LQG) inspired black bounce with the help of the celestial
light source and the thin disk source by employing the backward ray-tracing
method. The results indicate that both the LQG parameter alpha and the rotation
parameter a contribute to a reduction in the shadow size; however, the
influence of a is predominant, while the effect of alpha circular orbit. One
can find that the correlation parameter (a, alpha), along with the observer's
inclination angle, affect the image's asymmetry and the distortion of the inner
shadow. As the inclination increases, the direct and lensed images diverge,
creating a structure resembling a hat. Meanwhile, we also investigate the
redshift distribution of the direct lensed images of the accretion disk under
different parameters and observation angle. The results show that the
distribution of redshift and observed intensity is obviously related to the
behavior of accretion flow. These results may provide a potential approach to
limit black hole parameters, detect quantum gravity effects, and distinguish
the LQG black hole from other black hole models.",['gr-qc'],False,,,,Toward Ethical AI: A Qualitative Analysis of Stakeholder Perspectives,"The shadow and accretion disk images of the rotation loop quantum black
  bounce"
neg-d2-152,2025-01-09,,2501.05023," The ESA Euclid mission will survey more than 14,000 deg$^2$ of the sky in
visible and near-infrared wavelengths, mapping the extra-galactic sky to
constrain our cosmological model of the Universe. Although the survey focusses
on regions further than 15 deg from the ecliptic, it should allow for the
detection of more than about $10^5$ Solar System objects (SSOs). After
simulating the expected signal from SSOs in Euclid images acquired with the
visible camera (VIS), we describe an automated pipeline developed to detect
moving objects with an apparent velocity in the range of 0.1-10 arcsec/h,
typically corresponding to sources in the outer Solar System (from Centaurs to
Kuiper-belt objects). In particular, the proposed detection scheme is based on
Sourcextractor software and on applying a new algorithm capable of associating
moving objects amongst different catalogues. After applying a suite of filters
to improve the detection quality, we study the expected purity and completeness
of the SSO detections. We also show how a Kohonen self-organising neural
network can be successfully trained (in an unsupervised fashion) to classify
stars, galaxies, and SSOs. By implementing an early-stopping method in the
training scheme, we show that the network can be used in a predictive way,
allowing one to assign the probability of each detected object being a member
of each considered class.",['astro-ph.IM'],2503.09113," This paper presents a constraint-guided deep learning framework for
developing physically consistent health indicators in bearing prognostics and
health management. Conventional data-driven methods often lack physical
plausibility, while physics-based models are limited by incomplete system
knowledge. To address this, we integrate domain knowledge into deep learning
using constraints to enforce monotonicity, bound output values between 1 and 0
(representing healthy to failed states), and ensure consistency between signal
energy trends and health indicator estimates. This eliminates the need for
complex loss term balancing. We implement constraint-guided gradient descent
within an autoencoder architecture, creating a constrained autoencoder.
However, the framework is adaptable to other architectures. Using
time-frequency representations of accelerometer signals from the Pronostia
dataset, our constrained model generates smoother, more reliable degradation
profiles compared to conventional methods, aligning with expected physical
behavior. Performance is assessed using three metrics: trendability,
robustness, and consistency. Compared to a conventional baseline, the
constrained model improves all three. Another baseline, incorporating
monotonicity via a soft-ranking loss function, outperforms in trendability but
falls short in robustness and consistency. An ablation study confirms that the
monotonicity constraint enhances trendability, the boundary constraint ensures
consistency, and the energy-health consistency constraint improves robustness.
These findings highlight the effectiveness of constraint-guided deep learning
in producing reliable, physically meaningful health indicators, offering a
promising direction for future prognostic applications.","['cs.LG', 'cs.AI']",False,,,,"Euclid: Detecting Solar System objects in Euclid images and classifying
  them using Kohonen self-organising maps","Constraint-Guided Learning of Data-driven Health Indicator Models: An
  Application on the Pronostia Bearing Dataset"
neg-d2-153,2025-02-13,,2502.097," The late-stage evolution of massive stars is marked by intense instability as
they approach core-collapse. During these phases, giant stellar eruptions lead
to exceptionally high mass-loss rates, forming significant amounts of dust.
However, the survival of these dust grains is challenged by the powerful shock
waves generated when the progenitor explodes as a supernova (SN). We explore
the impact of hydrogen-rich SN explosions from 45, 50, and 60 M$_\odot$
progenitors on dust formed after these eruptions, focusing on interactions with
circumstellar shells occurring from a few years to centuries after the event.
Using 3D hydrodynamical simulations, we track the evolution of dust particles
in a scenario that includes the progenitor's stellar wind, a giant eruption,
and the subsequent SN explosion, following the mass budgets predicted by
stellar evolution models. For a standard SN ejecta mass of 10 M$_\odot$ and
kinetic energy of $10^{51}$ erg, only 25% of the dust mass survives 250 years
post-explosion in a spherical circumstellar medium (CSM), while merely 2%
remains a century after the explosion in a bipolar CSM. If the SN follows the
eruption within a dozen years, 75% of the dust survives for a standard
explosion, dropping to 20% for more massive ejecta (15-20 M$_\odot$) with
kinetic energy of $5 \times 10^{51}$ erg. The geometry of the CSM and the early
transition of the SN remnant into a radiative phase significantly influence
dust survival. As the shock wave weakens and efficiently converts kinetic
energy into thermal radiation (up to half of the injected kinetic energy) the
likelihood of dust survival increases, affecting not only pre-existing dust in
the CSM but also SN-condensed dust and ambient interstellar dust. Contrary to
expectations, a larger fraction of the dust mass can survive if the SN occurs
only a few years after the eruption.","['astro-ph.SR', 'astro-ph.GA']",2502.06135," We analyze the phase structure of 2d lattice CP(1) model with $\theta$ term
by using the bond-weighted tensor renormalization group method. We propose a
new tensor network representation for the model using the quadrature scheme and
confirm that its accuracy is better than that of the conventional
character-like expansion. As a probe to study the phase structure, we adopt the
central charge and the scaling dimensions. The numerical results indicate an
existence of critical point at $\theta=\pi$, which is consistent with the
Haldane's conjecture.",['hep-lat'],False,,,,"The bright, dusty aftermath of giant eruptions & H-rich supernovae. Late
  interaction of supernova shocks & dusty circumstellar shells","Phase structure analysis of CP(1) model with $\theta$ term by tensor
  renormalization group"
neg-d2-154,2025-03-12,,2503.10708," In this work, we introduce Virology-Informed Neural Networks (VINNs), a
powerful tool for capturing the intricate dynamics of viral infection when data
of some compartments of the model are not available. VINNs, an extension of the
widely known Physics-Informed Neural Networks (PINNs), offer an alternative
approach to traditional numerical methods for solving system of differential
equations. We apply this VINN technique on a recently proposed hepatitis B
virus (HBV) infection dynamics model to predict the transmission of the
infection within the liver more accurately. This model consists of four
compartments, namely uninfected and infected hepatocytes, rcDNA-containing
capsids, and free viruses, along with the consideration of capsid recycling.
Leveraging the power of VINNs, we study the impacts of variations in parameter
range, experimental noise, data variability, network architecture, and learning
rate in this work. In order to demonstrate the robustness and effectiveness of
VINNs, we employ this approach on the data collected from nine HBV-infceted
chimpanzees, and it is observed that VINNs can effectively estimate the model
parameters. VINNs reliably capture the dynamics of infection spread and
accurately predict their future progression using real-world data. Furthermore,
VINNs efficiently identify the most influential parameters in HBV dynamics
based solely on experimental data from the capsid component. It is also
expected that this framework can be extended beyond viral dynamics, providing a
powerful tool for uncovering hidden patterns and complex interactions across
various scientific and engineering domains.","['q-bio.QM', 'cs.LG']",2503.08257," A dexterous hand capable of grasping any object is essential for the
development of general-purpose embodied intelligent robots. However, due to the
high degree of freedom in dexterous hands and the vast diversity of objects,
generating high-quality, usable grasping poses in a robust manner is a
significant challenge. In this paper, we introduce DexGrasp Anything, a method
that effectively integrates physical constraints into both the training and
sampling phases of a diffusion-based generative model, achieving
state-of-the-art performance across nearly all open datasets. Additionally, we
present a new dexterous grasping dataset containing over 3.4 million diverse
grasping poses for more than 15k different objects, demonstrating its potential
to advance universal dexterous grasping. The code of our method and our dataset
will be publicly released soon.","['cs.CV', 'cs.AI', 'cs.RO']",False,,,,"Exploration of Hepatitis B Virus Infection Dynamics through
  Virology-Informed Neural Network: A Novel Artificial Intelligence Approach","DexGrasp Anything: Towards Universal Robotic Dexterous Grasping with
  Physics Awareness"
neg-d2-155,2025-02-20,,2502.14989," The Whirlpool Galaxy is a well studied grand design galaxy with two major
spiral arms, and a large satellite NGC 5195. The arms both show long uniform
sections with perturbations ('kinks' or sharp turns) in specific regions.
Comparing the two arms shows a small radial offset between the main kinked
regions. We analysed the morphology and also the velocity field in the disk of
M51 using kinematic maps based on H$\alpha$ and CO line emission. These sample
complementary radial ranges, with the CO map covering the central zone and the
H$\alpha$ map extending to cover the outer zone. We looked for indicators of
density wave resonance, zones where radial flows of gas in the disk plane
reverse their sign. These were present in both velocity maps; their
two-dimensional localization placed them along or closely parallel to the
spiral arms, at a set of well defined galactocentric radii, and notably more
concentrated along the southern, stronger arm. The results can be well
interpreted quantitatively, using a numerical model of the interaction of M51
and NGC5195 in which the satellite has made two relatively recent passes
through the disk plane of M51. During the first pass the pair of dominant
spiral arms was stimulated, and during the second pass the strong kinks in both
arms were formed at about the same time. The second interaction is particularly
well characterised, because the timescale corresponding to the production of
the kinks and the recovery of the original pitch angle is identical for the two
arms.",['astro-ph.GA'],2503.05793," Medical education faces challenges in scalability, accessibility, and
consistency, particularly in clinical skills training for physician-patient
communication. Traditional simulation-based learning, while effective, is
resource-intensive, difficult to schedule, and often highly variable in
feedback quality. Through a collaboration between AI, learning science, and
medical education experts, we co-developed MedSimAI, an AI-powered simulation
platform that enables deliberate practice, self-regulated learning (SRL), and
automated assessment through interactive patient encounters. Leveraging large
language models (LLMs), MedSimAI generates realistic clinical interactions and
provides immediate, structured feedback using established medical evaluation
frameworks such as the Master Interview Rating Scale (MIRS). In a pilot study
with 104 first-year medical students, we examined engagement, conversation
patterns, and user perceptions. Students found MedSimAI beneficial for
repeated, realistic patient-history practice. Conversation analysis revealed
that certain higher-order skills were often overlooked, though students
generally performed systematic histories and empathic listening. By integrating
unlimited practice opportunities, real-time AI assessment, and SRL principles,
MedSimAI addresses key limitations of traditional simulation-based training,
making high-quality clinical education more accessible and scalable.","['cs.CY', 'cs.AI', 'cs.CL']",False,,,,"Morphology and kinematics of the gas in M51: How interaction with
  NGC5195 has moulded the structure of its arms","MedSimAI: Simulation and Formative Feedback Generation to Enhance
  Deliberate Practice in Medical Education"
neg-d2-156,2025-02-14,,2502.10171," We prove area estimates for stable capillary $cmc$ (minimal) hypersurfaces
$\Sigma$ with nonpositive Yamabe invariant that are properly immersed in a
Riemannian $n$-dimensional manifold $M$ with scalar curvature $R^M$ and mean
curvature of the boundary $H^{\partial M}$ bounded from below. We also prove a
local rigidity result in the case $\Sigma$ is embedded and
$\mathcal{J}$-energy-minimizing. In this case, we show that $M$ locally splits
along $\Sigma$ and is isometric to $(-\varepsilon,\varepsilon)\times \Sigma,
dt^2 + e^{-2Ht}g)$, where $g$ is Einstein, or Ricci flat, $H\geq 0$ and
$\partial\Sigma$ is totally geodesic.",['math.DG'],2501.10182," In recent years, Semantic Communication (SemCom), which aims to achieve
efficient and reliable transmission of meaning between agents, has garnered
significant attention from both academia and industry. To ensure the security
of communication systems, encryption techniques are employed to safeguard
confidentiality and integrity. However, traditional cryptography-based
encryption algorithms encounter obstacles when applied to SemCom. Motivated by
this, this paper explores the feasibility of applying homomorphic encryption to
SemCom. Initially, we review the encryption algorithms utilized in mobile
communication systems and analyze the challenges associated with their
application to SemCom. Subsequently, we employ scale-invariant feature
transform to demonstrate that semantic features can be preserved in homomorphic
encrypted ciphertext. Based on this finding, we propose a task-oriented SemCom
scheme secured through homomorphic encryption. We design the privacy preserved
deep joint source-channel coding (JSCC) encoder and decoder, and the frequency
of key updates can be adjusted according to service requirements without
compromising transmission performance. Simulation results validate that, when
compared to plaintext images, the proposed scheme can achieve almost the same
classification accuracy performance when dealing with homomorphic ciphertext
images. Furthermore, we provide potential future research directions for
homomorphic encrypted SemCom.","['cs.CR', 'eess.SP']",False,,,,"Area estimates for capillary cmc hypersurfaces with nonpositive Yamabe
  invariant",Secure Semantic Communication With Homomorphic Encryption
neg-d2-157,2025-02-27,,2502.20332," Many recent studies have found evidence for emergent reasoning capabilities
in large language models, but debate persists concerning the robustness of
these capabilities, and the extent to which they depend on structured reasoning
mechanisms. To shed light on these issues, we perform a comprehensive study of
the internal mechanisms that support abstract rule induction in an open-source
language model (Llama3-70B). We identify an emergent symbolic architecture that
implements abstract reasoning via a series of three computations. In early
layers, symbol abstraction heads convert input tokens to abstract variables
based on the relations between those tokens. In intermediate layers, symbolic
induction heads perform sequence induction over these abstract variables.
Finally, in later layers, retrieval heads predict the next token by retrieving
the value associated with the predicted abstract variable. These results point
toward a resolution of the longstanding debate between symbolic and neural
network approaches, suggesting that emergent reasoning in neural networks
depends on the emergence of symbolic mechanisms.","['cs.CL', 'cs.AI']",2501.08479," Serverless computing offers elasticity unmatched by conventional server-based
cloud infrastructure. Although modern data processing systems embrace
serverless storage, such as Amazon S3, they continue to manage their compute
resources as servers. This is challenging for unpredictable workloads, leaving
clusters often underutilized. Recent research shows the potential of serverless
compute resources, such as cloud functions, for elastic data processing, but
also sees limitations in performance robustness and cost efficiency for long
running workloads. These challenges require holistic approaches across the
system stack. However, to the best of our knowledge, there is no end-to-end
data processing system built entirely on serverless infrastructure. In this
paper, we present Skyrise, our effort towards building the first fully
serverless SQL query processor. Skyrise exploits the elasticity of its
underlying infrastructure, while alleviating the inherent limitations with a
number of adaptive and cost-aware techniques. We show that both Skyrise's
performance and cost are competitive to other cloud data systems for
terabyte-scale queries of the analytical TPC-H benchmark.",['cs.DB'],False,,,,"Emergent Symbolic Mechanisms Support Abstract Reasoning in Large
  Language Models","Skyrise: Exploiting Serverless Cloud Infrastructure for Elastic Data
  Processing"
neg-d2-158,2025-01-27,,2501.16637," In this note, we review the latest qualitative results, referring to the
Li\'enard Equation, in the framework of non-conformable, generalized and
fractional differential operators.",['math.GM'],2502.06011," Off-policy evaluation (OPE) is a critical challenge in robust decision-making
that seeks to assess the performance of a new policy using data collected under
a different policy. However, the existing OPE methodologies suffer from several
limitations arising from statistical uncertainty as well as causal
considerations. In this thesis, we address these limitations by presenting
three different works. Firstly, we consider the problem of high variance in the
importance-sampling-based OPE estimators. We introduce the Marginal Ratio (MR)
estimator, a novel OPE method that reduces variance by focusing on the marginal
distribution of outcomes rather than direct policy shifts, improving robustness
in contextual bandits. Next, we propose Conformal Off-Policy Prediction (COPP),
a principled approach for uncertainty quantification in OPE that provides
finite-sample predictive intervals, ensuring robust decision-making in
risk-sensitive applications. Finally, we address causal unidentifiability in
off-policy decision-making by developing novel bounds for sequential decision
settings, which remain valid under arbitrary unmeasured confounding. We apply
these bounds to assess the reliability of digital twin models, introducing a
falsification framework to identify scenarios where model predictions diverge
from real-world behaviour. Our contributions provide new insights into robust
decision-making under uncertainty and establish principled methods for
evaluating policies in both static and dynamic settings.","['stat.ML', 'cs.LG']",False,,,,On the Li\'enard's type equation: an icon of the Nonlinear Analysis,"Uncertainty Quantification and Causal Considerations for Off-Policy
  Decision Making"
neg-d2-159,2025-01-30,,2501.18848," Symbolic task representation is a powerful tool for encoding human
instructions and domain knowledge. Such instructions guide robots to accomplish
diverse objectives and meet constraints through reinforcement learning (RL).
Most existing methods are based on fixed mappings from environmental states to
symbols. However, in inspection tasks, where equipment conditions must be
evaluated from multiple perspectives to avoid errors of oversight, robots must
fulfill the same symbol from different states. To help robots respond to
flexible symbol mapping, we propose representing symbols and their mapping
specifications separately within an RL policy. This approach imposes on RL
policy to learn combinations of symbolic instructions and mapping
specifications, requiring an efficient learning framework. To cope with this
issue, we introduce an approach for learning flexible policies called Symbolic
Instructions with Adjustable Mapping Specifications (SIAMS). This paper
represents symbolic instructions using linear temporal logic (LTL), a formal
language that can be easily integrated into RL. Our method addresses the
diversified completion patterns of instructions by (1) a specification-aware
state modulation, which embeds differences in mapping specifications in state
features, and (2) a symbol-number-based task curriculum, which gradually
provides tasks according to the learning's progress. Evaluations in 3D
simulations with discrete and continuous action spaces demonstrate that our
method outperforms context-aware multitask RL comparisons.",['cs.RO'],2503.08224," Reconstructing animatable and high-quality 3D head avatars from monocular
videos, especially with realistic relighting, is a valuable task. However, the
limited information from single-view input, combined with the complex head
poses and facial movements, makes this challenging. Previous methods achieve
real-time performance by combining 3D Gaussian Splatting with a parametric head
model, but the resulting head quality suffers from inaccurate face tracking and
limited expressiveness of the deformation model. These methods also fail to
produce realistic effects under novel lighting conditions. To address these
issues, we propose HRAvatar, a 3DGS-based method that reconstructs
high-fidelity, relightable 3D head avatars. HRAvatar reduces tracking errors
through end-to-end optimization and better captures individual facial
deformations using learnable blendshapes and learnable linear blend skinning.
Additionally, it decomposes head appearance into several physical properties
and incorporates physically-based shading to account for environmental
lighting. Extensive experiments demonstrate that HRAvatar not only reconstructs
superior-quality heads but also achieves realistic visual effects under varying
lighting conditions.",['cs.CV'],False,,,,"Reinforcement Learning of Flexible Policies for Symbolic Instructions
  with Adjustable Mapping Specifications",HRAvatar: High-Quality and Relightable Gaussian Head Avatar
neg-d2-160,2025-02-03,,2502.01891," Human label variation (HLV) challenges the standard assumption that a
labelled instance has a single ground truth, instead embracing the natural
variation in human annotation to train and evaluate models. While various
training methods and metrics for HLV have been proposed, it is still unclear
which methods and metrics perform best in what settings. We propose new
evaluation metrics for HLV leveraging fuzzy set theory. Since these new
proposed metrics are differentiable, we then in turn experiment with employing
these metrics as training objectives. We conduct an extensive study over 6 HLV
datasets testing 14 training methods and 6 evaluation metrics. We find that
training on either disaggregated annotations or soft labels performs best
across metrics, outperforming training using the proposed training objectives
with differentiable metrics. We also show that our proposed soft metric is more
interpretable and correlates best with human preference.","['cs.LG', 'cs.CL']",2503.07355," This paper contains a review of the theoretical foundations of Clifford
algebras, spinors and spinor bundles in the so-called co-frame formalism. A
compact index-free notation is introduced, along with a series of identities
useful for computations in supergravity theories.","['math-ph', 'hep-th', 'math.MP']",False,,,,Training and Evaluating with Human Label Variation: An Empirical Study,Tools for Supergravity in the spin coframe formalism
neg-d2-161,2025-01-28,,2501.17217," Motivated by the recent construction of grey galaxy and Dual Dressed Black
Hole solutions in $AdS_5\times S^5$, we present two conjectures relating to the
large $N$ entropy of supersymmetric states in ${\cal N}=4$ Yang-Mills theory.
Our first conjecture asserts the existence of a large number of supersymmetric
states which can be thought of as a non interacting mix of supersymmetric black
holes and supersymmetric `gravitons'. It predicts a microcanonical phase
diagram of supersymmetric states with eleven distinct phases, and makes a sharp
prediction for the supersymmetric entropy (as a function of 5 charges) in each
of these phases. The microcanonical version of the superconformal index
involves a sum over states - with alternating signs - over a line in 5
parameter charge space. Our second conjecture asserts that this sum is
dominated by the point on the line that has the largest supersymmetric entropy.
This conjecture predicts a large $N$ formula for the superconformal index as a
function of indicial charges, and predicts a microcanonical indicial phase
diagram with nine distinct phases. It predicts agreement between the
superconformal index and black hole entropy in one phase (so over one range of
charges), but disagreement in other phases (and so at other values of charges).
We compare our predictions against numerically evaluated superconformal index
at $N\leq10$, and find qualitative agreement.",['hep-th'],2501.14901," In a recent study, Dannen et al. surveyed a large parameter space to study
the transition from efficient to inefficient line driving. They found that when
the line force significantly weakens due to ionization, the winds are variable,
with a characteristic frequency comparable to the Lamb cut-off frequency of a
stratified atmosphere, {\omega}c. In this work, we present a set of simulations
and perturbation analyses that elucidate the variability source and
characteristics. We found that the line force adds wave energy and amplifies
perturbations with frequencies near {\omega}c. This selective amplification
results from the coupling between the natural tendency of velocity
perturbations to grow in a stratified atmosphere and the dependence of the line
force on the velocity gradient, per the Castor-Abbott-Klein line-driven wind
theory. We also found that the variability stems from self-excitation that
occurs in the exponential atmosphere due to the non-linearity introduced by the
absolute value of the velocity gradient in the line force prescription. We
conclude that self-consistently calculating ionization is insufficient for
modeling the dynamics in the subsonic atmosphere. Instead future wind models
should relax the Sobolev approximation, or model the radiative transfer to
capture the dynamics and instabilities at the base of the wind.",['astro-ph.GA'],False,,,,"Supersymmetric Grey Galaxies, Dual Dressed Black Holes and the
  Superconformal Index",Oscillatory Line-Driven Winds: The Role of Atmospheric Stratification
neg-d2-162,2025-02-17,,2502.11843," Large Language Models (LLMs) are widely used as conversational agents,
exploiting their capabilities in various sectors such as education, law,
medicine, and more. However, LLMs are often subjected to context-shifting
behaviour, resulting in a lack of consistent and interpretable
personality-aligned interactions. Adherence to psychological traits lacks
comprehensive analysis, especially in the case of dyadic (pairwise)
conversations. We examine this challenge from two viewpoints, initially using
two conversation agents to generate a discourse on a certain topic with an
assigned personality from the OCEAN framework (Openness, Conscientiousness,
Extraversion, Agreeableness, and Neuroticism) as High/Low for each trait. This
is followed by using multiple judge agents to infer the original traits
assigned to explore prediction consistency, inter-model agreement, and
alignment with the assigned personality. Our findings indicate that while LLMs
can be guided toward personality-driven dialogue, their ability to maintain
personality traits varies significantly depending on the combination of models
and discourse settings. These inconsistencies emphasise the challenges in
achieving stable and interpretable personality-aligned interactions in LLMs.","['cs.CL', 'cs.AI', 'cs.SI']",2503.16639," Realistic crowd simulations are essential for immersive virtual environments,
relying on both individual behaviors (microscopic dynamics) and overall crowd
patterns (macroscopic characteristics). While recent data-driven methods like
deep reinforcement learning improve microscopic realism, they often overlook
critical macroscopic features such as crowd density and flow, which are
governed by spatio-temporal spawn dynamics, namely, when and where agents enter
a scene. Traditional methods, like random spawn rates, stochastic processes, or
fixed schedules, are not guaranteed to capture the underlying complexity or
lack diversity and realism. To address this issue, we propose a novel approach
called nTPP-GMM that models spatio-temporal spawn dynamics using Neural
Temporal Point Processes (nTPPs) that are coupled with a spawn-conditional
Gaussian Mixture Model (GMM) for agent spawn and goal positions. We evaluate
our approach by orchestrating crowd simulations of three diverse real-world
datasets with nTPP-GMM. Our experiments demonstrate the orchestration with
nTPP-GMM leads to realistic simulations that reflect real-world crowd scenarios
and allow crowd analysis.",['cs.LG'],False,,,,Can LLM Agents Maintain a Persona in Discourse?,"Whenever, Wherever: Towards Orchestrating Crowd Simulations with
  Spatio-Temporal Spawn Dynamics"
neg-d2-163,2025-02-17,,2502.12289," Step-by-step reasoning is widely used to enhance the reasoning ability of
large language models (LLMs) in complex problems. Evaluating the quality of
reasoning traces is crucial for understanding and improving LLM reasoning.
However, the evaluation criteria remain highly unstandardized, leading to
fragmented efforts in developing metrics and meta-evaluation benchmarks. To
address this gap, this survey provides a comprehensive overview of step-by-step
reasoning evaluation, proposing a taxonomy of evaluation criteria with four
top-level categories (groundedness, validity, coherence, and utility). We then
categorize metrics based on their implementations, survey which metrics are
used for assessing each criterion, and explore whether evaluator models can
transfer across different criteria. Finally, we identify key directions for
future research.",['cs.CL'],2503.08661," This paper proposes a task-oriented co-design framework that integrates
communication, computing, and control to address the key challenges of
bandwidth limitations, noise interference, and latency in mission-critical
industrial Cyber-Physical Systems (CPS). To improve communication efficiency
and robustness, we design a task-oriented Joint Source-Channel Coding (JSCC)
using Information Bottleneck (IB) to enhance data transmission efficiency by
prioritizing task-specific information. To mitigate the perceived End-to-End
(E2E) delays, we develop a Delay-Aware Trajectory-Guided Control Prediction
(DTCP) strategy that integrates trajectory planning with control prediction,
predicting commands based on E2E delay. Moreover, the DTCP is co-designed with
task-oriented JSCC, focusing on transmitting task-specific information for
timely and reliable autonomous driving. Experimental results in the CARLA
simulator demonstrate that, under an E2E delay of 1 second (20 time slots), the
proposed framework achieves a driving score of 48.12, which is 31.59 points
higher than using Better Portable Graphics (BPG) while reducing bandwidth usage
by 99.19%.","['cs.IT', 'cs.CV', 'eess.IV', 'math.IT']",False,,,,Evaluating Step-by-step Reasoning Traces: A Survey,"Task-Oriented Co-Design of Communication, Computing, and Control for
  Edge-Enabled Industrial Cyber-Physical Systems"
neg-d2-164,2025-03-21,,2503.17547," Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting
neural networks by extracting the concepts represented in their activations.
However, choosing the size of the SAE dictionary (i.e. number of learned
concepts) creates a tension: as dictionary size increases to capture more
relevant concepts, sparsity incentivizes features to be split or absorbed into
more specific features, leaving high-level features missing or warped. We
introduce Matryoshka SAEs, a novel variant that addresses these issues by
simultaneously training multiple nested dictionaries of increasing size,
forcing the smaller dictionaries to independently reconstruct the inputs
without using the larger dictionaries. This organizes features hierarchically -
the smaller dictionaries learn general concepts, while the larger dictionaries
learn more specific concepts, without incentive to absorb the high-level
features. We train Matryoshka SAEs on Gemma-2-2B and TinyStories and find
superior performance on sparse probing and targeted concept erasure tasks, more
disentangled concept representations, and reduced feature absorption. While
there is a minor tradeoff with reconstruction performance, we believe
Matryoshka SAEs are a superior alternative for practical tasks, as they enable
training arbitrarily large SAEs while retaining interpretable features at
different levels of abstraction.","['cs.LG', 'cs.AI']",2501.0194," We report the results of long-term time series photometry on RX J2133.7+5107
(also known as 1RXS J213344.1+510725) obtained at several observatories. Using
data taken during 17 years, we determined the current value of the spin period
of $570.811470$ seconds with the formal accuracy of $0.000006$ seconds and a
spin-up of the white dwarf with a characteristic time of $1.483(1)\times10^5$
years. This is even faster than that reported previously and, if confirmed,
makes this object have one of the fastest spin-up timescales of all known
intermediate polars. We derived an improved value of the superhump period of
the system to be $0^d.280130(1)$. Superhump maxima timings are moving on the
phase curve from season to season, showing non-monotonic changes, without a
change in superhump period.","['astro-ph.SR', 'astro-ph.HE']",False,,,,Learning Multi-Level Features with Matryoshka Sparse Autoencoders,Spin-period variations in the intermediate polar RX J2133.7+5107
neg-d2-165,2025-02-21,,2502.15959," With the rapid development of artificial intelligence (AI), especially in the
medical field, the need for its explainability has grown. In medical image
analysis, a high degree of transparency and model interpretability can help
clinicians better understand and trust the decision-making process of AI
models. In this study, we propose a Knowledge Distillation (KD)-based approach
that aims to enhance the transparency of the AI model in medical image
analysis. The initial step is to use traditional CNN to obtain a teacher model
and then use KD to simplify the CNN architecture, retain most of the features
of the data set, and reduce the number of network layers. It also uses the
feature map of the student model to perform hierarchical analysis to identify
key features and decision-making processes. This leads to intuitive visual
explanations. We selected three public medical data sets (brain tumor, eye
disease, and Alzheimer's disease) to test our method. It shows that even when
the number of layers is reduced, our model provides a remarkable result in the
test set and reduces the time required for the interpretability analysis.",['cs.AI'],2503.04199," RGB-Thermal fusion is a potential solution for various weather and light
conditions in challenging scenarios. However, plenty of studies focus on
designing complex modules to fuse different modalities. With the widespread
application of large language models (LLMs), valuable information can be more
effectively extracted from natural language. Therefore, we aim to leverage the
advantages of large language models to design a structurally simple and highly
adaptable multimodal fusion model architecture. We proposed MultimodAl
Segmentation with TExt PRompts (MASTER) architecture, which integrates LLM into
the fusion of RGB-Thermal multimodal data and allows complex query text to
participate in the fusion process. Our model utilizes a dual-path structure to
extract information from different modalities of images. Additionally, we
employ LLM as the core module for multimodal fusion, enabling the model to
generate learnable codebook tokens from RGB, thermal images, and textual
information. A lightweight image decoder is used to obtain semantic
segmentation results. The proposed MASTER performs exceptionally well in
benchmark tests across various automated driving scenarios, yielding promising
results.","['cs.CV', 'cs.AI']",False,,,,"A Knowledge Distillation-Based Approach to Enhance Transparency of
  Classifier Models",MASTER: Multimodal Segmentation with Text Prompts
neg-d2-166,2025-02-17,,2502.11951," This paper dives into the exciting and rapidly growing field of quantum
computing, explaining its core ideas, current progress, and how it could
revolutionize the way we solve complex problems. It starts by breaking down the
basics, like qubits, quantum circuits, and how principles like superposition
and entanglement make quantum computers fundamentally different-and far more
powerful for certain tasks-than the classical computers we use today. We also
explore how quantum computing deals with complex problems and why it is
uniquely suited for challenges classical systems struggle to handle. A big part
of this paper focuses on Quantum Machine Learning (QML), where the strengths of
quantum computing meet the world of artificial intelligence. By processing
massive datasets and optimizing intricate algorithms, quantum systems offer new
possibilities for machine learning. We highlight different approaches to
combining quantum and classical computing, showing how they can work together
to produce faster and more accurate results. Additionally, we explore the tools
and platforms available-like TensorFlow Quantum, Qiskit and PennyLane-that are
helping researchers and developers bring these theories to life. Of course,
quantum computing has its hurdles. Challenges like scaling up hardware,
correcting errors, and keeping qubits stable are significant roadblocks. Yet,
with rapid advancements in cloud-based platforms and innovative technologies,
the potential of quantum computing feels closer than ever. This paper aims to
offer readers a clear and comprehensive introduction to quantum computing, its
role in machine learning, and the immense possibilities it holds for the future
of technology.","['cs.CE', 'cs.LG', 'quant-ph']",2503.02762," Context: Compared to Class 0 protostars, the higher densities and lower
temperatures of the disk midplanes of Class I young stellar objects (YSOs)
limit the detectability of complex organic molecules (COMs). The elevated
luminosities of eruptive YSOs increase disk temperatures sublimating frozen
molecules and easing their detection.
  Aims: Our aim is to investigate the chemical composition of four FUor-like
Class I YSOs: L1551 IRS 5, Haro 5a IRS, V346 Nor, and OO Ser, and to compare
their abundances of COMs with other YSOs in the literature.
  Methods: We search for COMs line emission in ALMA Band 6 observations. We use
the CASSIS software to determine their column densities (N) and excitation
temperatures (T_ex) assuming local thermodynamical equilibrium.
  Results: We detect 249 transitions from 12 COMs. In L1551 IRS 5 we identified
CH3OH, 13CH3OH, CH318OH, CH2DOH, CH3CHO, CH3OCH3, CH3OCHO, CH3COCH3, C2H5OH,
C2H5CN, 13CH3CN, and CH3C15)N. Haro 5a IRS and OO Ser have emission from CH3OH,
CH3CHO, CH3OCH3, and CH3OCHO. CH3COCH3 is also detected in OO Ser. In V346 Nor
we found CH3OH, CH2DOH, CH3CHO, CH3OCH3, CH3OCHO, and C2H5CN. The emission of
COMs is compact in all targets. The analysis indicates their temperatures are
above 100K. The abundance ratios of COMs derived for these eruptive YSOs, as
well as for other protostars in the literature, span several orders of
magnitude without any clear differentiation between the eruptive and quiescent
YSOs. The column density of the main isotopologue of CH3OH should not be used
as a reference, as most of the lines are optically thick.
  Conclusions: The hot and compact emission of COMs indicates that the four
FUor-like targets are hot corino-like. Spectral studies of such objects can be
useful to investigate the complex organic chemistry at later evolutionary
stages than the usual Class 0 stage.","['astro-ph.GA', 'astro-ph.SR']",False,,,,"Qubit-Based Framework for Quantum Machine Learning: Bridging Classical
  Data and Quantum Algorithms",The hot corino-like chemistry of four FUor-like protostars
neg-d2-167,2025-02-26,,2502.19693," Message passing-based graph neural networks (GNNs) have achieved great
success in many real-world applications. For a sampled mini-batch of target
nodes, the message passing process is divided into two parts: message passing
between nodes within the batch (MP-IB) and message passing from nodes outside
the batch to those within it (MP-OB). However, MP-OB recursively relies on
higher-order out-of-batch neighbors, leading to an exponentially growing
computational cost with respect to the number of layers. Due to the neighbor
explosion, the whole message passing stores most nodes and edges on the GPU
such that many GNNs are infeasible to large-scale graphs. To address this
challenge, we propose an accurate and fast mini-batch approach for large graph
transductive learning, namely topological compensation (TOP), which obtains the
outputs of the whole message passing solely through MP-IB, without the costly
MP-OB. The major pillar of TOP is a novel concept of message invariance, which
defines message-invariant transformations to convert costly MP-OB into fast
MP-IB. This ensures that the modified MP-IB has the same output as the whole
message passing. Experiments demonstrate that TOP is significantly faster than
existing mini-batch methods by order of magnitude on vast graphs (millions of
nodes and billions of edges) with limited accuracy degradation.","['cs.LG', 'cs.AI']",2502.04389," Diagrams play a crucial role in visually conveying complex relationships and
processes within business documentation. Despite recent advances in
Vision-Language Models (VLMs) for various image understanding tasks, accurately
identifying and extracting the structures and relationships depicted in
diagrams continues to pose significant challenges. This study addresses these
challenges by proposing a text-driven approach that bypasses reliance on VLMs'
visual recognition capabilities. Instead, it utilizes the editable source
files--such as xlsx, pptx or docx--where diagram elements (e.g., shapes, lines,
annotations) are preserved as textual metadata. In our proof-of-concept, we
extracted diagram information from xlsx-based system design documents and
transformed the extracted shape data into textual input for Large Language
Models (LLMs). This approach allowed the LLM to analyze relationships and
generate responses to business-oriented questions without the bottleneck of
image-based processing. Experimental comparisons with a VLM-based method
demonstrated that the proposed text-driven framework yielded more accurate
answers for questions requiring detailed comprehension of diagram
structures.The results obtained in this study are not limited to the tested
.xlsx files but can also be extended to diagrams in other documents with source
files, such as Office pptx and docx formats. These findings highlight the
feasibility of circumventing VLM constraints through direct textual extraction
from original source files. By enabling robust diagram understanding through
LLMs, our method offers a promising path toward enhanced workflow efficiency
and information analysis in real-world business scenarios.","['cs.SE', 'cs.AI']",False,,,,Accurate and Scalable Graph Neural Networks via Message Invariance,"Overcoming Vision Language Model Challenges in Diagram Understanding: A
  Proof-of-Concept with XML-Driven Large Language Models Solutions"
neg-d2-168,2025-02-21,,2502.15654," As Large Language Models (LLMs) become increasingly prevalent, their
generated outputs are proliferating across the web, risking a future where
machine-generated content dilutes human-authored text. Since online data is the
primary resource for LLM pre-training, subsequent models could be trained on an
unknown portion of synthetic samples. This will lead to model collapse, a
degenerative process whereby LLMs reinforce their own errors, and ultimately
yield a declining performance. In this study, we investigate the impact of
decoding strategy on model collapse, analysing the characteristics of text at
each model generation, the similarity to human references, and the resulting
model performance. Using the decoding strategies that lead to the most
significant degradation, we evaluate model collapse in more realistic scenarios
where the origin of the data (human or synthetic) is unknown. We train a
machine-generated text detector and propose an importance sampling approach to
alleviate model collapse. Our method is validated on two LLM variants (GPT-2
and SmolLM2) on the open-ended text generation task. We demonstrate that it can
not only prevent model collapse but also improve performance when sufficient
human-authored samples are present.","['cs.CL', 'cs.LG']",2501.06062," In many practical natural language applications, user data are highly
sensitive, requiring anonymous uploads of text data from mobile devices to the
cloud without user identifiers. However, the absence of user identifiers
restricts the ability of cloud-based language models to provide personalized
services, which are essential for catering to diverse user needs. The trivial
method of replacing an explicit user identifier with a static user embedding as
model input still compromises data anonymization. In this work, we propose to
let each mobile device maintain a user-specific distribution to dynamically
generate user embeddings, thereby breaking the one-to-one mapping between an
embedding and a specific user. We further theoretically demonstrate that to
prevent the cloud from tracking users via uploaded embeddings, the local
distributions of different users should either be derived from a linearly
dependent space to avoid identifiability or be close to each other to prevent
accurate attribution. Evaluation on both public and industrial datasets using
different language models reveals a remarkable improvement in accuracy from
incorporating anonymous user embeddings, while preserving real-time inference
requirement.",['cs.LG'],False,,,,Machine-generated text detection prevents language model collapse,"Personalized Language Model Learning on Text Data Without User
  Identifiers"
neg-d2-169,2025-03-11,,2503.16494," In this paper, we examine the wide-ranging impact of artificial intelligence
on society, focusing on its potential to both help and harm global equity,
cognitive abilities, and economic stability. We argue that while artificial
intelligence offers significant opportunities for progress in areas like
healthcare, education, and scientific research, its rapid growth -- mainly
driven by private companies -- may worsen global inequalities, increase
dependence on automated systems for cognitive tasks, and disrupt established
economic paradigms. We emphasize the critical need for strong governance and
ethical guidelines to tackle these issues, urging the academic community to
actively participate in creating policies that ensure the benefits of
artificial intelligence are shared fairly and its risks are managed
effectively.","['physics.soc-ph', 'cs.CY']",2502.1334," Accurate visualization of double star astrometric data is essential for
effective analysis and interpretation. This article presents a Python toolkit
designed for astronomers who need to plot measurements from diverse sources --
historical, Gaia DR3, and the Las Cumbres Observatory (LCO) network -- while
maintaining a 1:1 aspect ratio to avoid visually distorting the data. The
toolkit is composed of three scripts: one that handles polar coordinates (P.A.,
separation), one for Cartesian (X, Y) coordinates, and another with the option
to include predicted theoretical points. This paper describes the purpose,
functionality, and usage of these scripts, including example figures,
installation guides, and licensing information.
  This toolkit has been used by the author and collaborators in published and
submitted research on double star systems, demonstrating its versatility for
both professional and student-driven investigations.","['astro-ph.IM', 'astro-ph.SR']",False,,,,"The impact of artificial intelligence: from cognitive costs to global
  inequality","A Python Toolkit for Plotting Double Star Observations with 1:1 Aspect
  Ratio"
neg-d2-170,2025-02-05,,2502.03123," In this study, Disentanglement in Difference(DiD) is proposed to address the
inherent inconsistency between the statistical independence of latent variables
and the goal of semantic disentanglement in disentanglement representation
learning. Conventional disentanglement methods achieve disentanglement
representation by improving statistical independence among latent variables.
However, the statistical independence of latent variables does not necessarily
imply that they are semantically unrelated, thus, improving statistical
independence does not always enhance disentanglement performance. To address
the above issue, DiD is proposed to directly learn semantic differences rather
than the statistical independence of latent variables. In the DiD, a Difference
Encoder is designed to measure the semantic differences; a contrastive loss
function is established to facilitate inter-dimensional comparison. Both of
them allow the model to directly differentiate and disentangle distinct
semantic factors, thereby resolving the inconsistency between statistical
independence and semantic disentanglement. Experimental results on the dSprites
and 3DShapes datasets demonstrate that the proposed DiD outperforms existing
mainstream methods across various disentanglement metrics.","['cs.LG', 'cs.AI']",2503.03661," We study existence and uniqueness of spherically symmetric solutions of
S_k(D^2v)+beta xi\cdot\nabla v+\alpha v+\abs{v}^{q-1}v=0 in R^n, where
\alpha,\beta are real parameters, n>2,\, q>k\geq 1 and S_k(D^2v) stands for the
k-Hessian operator of v. Our results are based mainly on the analysis of an
associated dynamical system and energy methods. We derive some properties of
the solutions of the above equation for different ranges of the parameters
\alpha and \beta. In particular, we describe with precision its asymptotic
behavior at infinity. Further, according to the position of q with respect to
the first critical exponent \frac{(n+2)k}{n} and the Tso critical exponent
\frac{(n+2)k}{n-2k} we study the existence of three classes of solutions:
crossing, slow decay or fast decay solutions. In particular, if k>1 all the
fast decay solutions have a compact support in R^n. The results also apply to
construct self-similar solutions of type I to a related nonlinear evolution
equation. These are self-similar functions of the form
u(t,x)=t^{-\alpha}v(xt^{-\beta}) with suitable \alpha and \beta.",['math.AP'],False,,,,"Disentanglement in Difference: Directly Learning Semantically
  Disentangled Representations by Maximizing Inter-Factor Differences","A k-Hessian equation with a power nonlinearity source and
  self-similarity"
neg-d2-171,2025-02-14,,2502.10337," We consider a free energy functional defined on probability densities on the
unit sphere $\mathbb{S}^d$, and investigate its global minimizers. The energy
consists of two components: an entropy and a nonlocal interaction energy, which
favour spreading and aggregation behaviour, respectively. We find a threshold
value for the size of the attractive interactions, and establish the global
energy minimizers in each case. The bifurcation at this threshold value is
investigated. We also generalize the results to spaces consisting of an
arbitrary number of spheres (e.g., the flat torus $\mathbb{S}^1 \times
\mathbb{S}^1$).",['math.AP'],2503.04086," Gcd-graphs represent an interesting and historically important class of
integral graphs. Since the pioneering work of Klotz and Sander, numerous
incarnations of these graphs have been explored in the literature. In this
article, we define and establish some foundational properties of gcd-graphs
defined over a general finite commutative ring. In particular, we investigate
the connectivity and diameter of these graphs. Additionally, when the ring is a
finite symmetric $\mathbb{Z}/n$-algebra, we give an explicit description of
their spectrum using the theory of Ramanujan sums that gives a unified
treatment of various results in the literature.","['math.NT', 'math.AC', 'math.CO']",False,,,,"Bifurcation of global energy minimizers for a diffusion-aggregation
  model on sphere",On gcd-graphs over finite rings
neg-d2-172,2025-02-05,,2502.02958," Large Language Models (LLMs) contain large amounts of facts about the world.
These facts can become outdated over time, which has led to the development of
knowledge editing methods (KEs) that can change specific facts in LLMs with
limited side effects. This position paper argues that editing LLMs poses
serious safety risks that have been largely overlooked. First, we note the fact
that KEs are widely available, computationally inexpensive, highly performant,
and stealthy makes them an attractive tool for malicious actors. Second, we
discuss malicious use cases of KEs, showing how KEs can be easily adapted for a
variety of malicious purposes. Third, we highlight vulnerabilities in the AI
ecosystem that allow unrestricted uploading and downloading of updated models
without verification. Fourth, we argue that a lack of social and institutional
awareness exacerbates this risk, and discuss the implications for different
stakeholders. We call on the community to (i) research tamper-resistant models
and countermeasures against malicious model editing, and (ii) actively engage
in securing the AI ecosystem.",['cs.CL'],2503.0993," This paper presents a nonlinear control strategy for an aerial cooperative
payload transportation system consisting of two quadrotor UAVs rigidly
connected to a payload. The system includes human physical interaction
facilitated by an admittance control. The proposed control framework integrates
an adaptive Backstepping controller for the position subsystem and a Fast
Nonsingular Terminal Sliding Mode Control (FNTSMC) for the attitude subsystem
to ensure asymptotic stabilization. The admittance controller interprets the
interaction forces from the human operator, generating reference trajectories
for the position controller to ensure accurate tracking of the operator's
guidance. The system aims to assist humans in payload transportation, providing
both stability and responsiveness. The robustness and effectiveness of the
proposed control scheme in maintaining system stability and performance under
various conditions are presented.","['eess.SY', 'cs.SY']",False,,,,Position: Editing Large Language Models Poses Serious Safety Risks,"Human Physical Interaction based on UAV Cooperative Payload
  Transportation System using Adaptive Backstepping and FNTSMC"
neg-d2-173,2025-02-21,,2502.17498," In the Large Language Model(LLM) reasoning scenario, people often estimate
state value via Monte Carlo sampling. Though Monte Carlo estimation is an
elegant method with less inductive bias, noise and errors are inevitably
introduced due to the limited sampling. To handle the problem, we inject the
structural prior into the value representation and transfer the scalar value
into the expectation of a pre-defined categorical distribution, representing
the noise and errors from a distribution perspective. Specifically, by treating
the result of Monte Carlo sampling as a single sample from the prior
ground-truth Binomial distribution, we quantify the sampling error as the
mismatch between posterior estimated distribution and ground-truth
distribution, which is thus optimized via distribution selection optimization.
We test the performance of value-based process verifiers on Best-of-N task and
Beam search task. Compared with the scalar value representation, we show that
reasonable structural prior injection induced by different objective functions
or optimization methods can improve the performance of value-based process
verifiers for about 1$\sim$2 points at little-to-no cost. We also show that
under different structural prior, the verifiers' performances vary greatly
despite having the same optimal solution, indicating the importance of
reasonable structural prior injection.","['cs.LG', 'cs.AI', 'cs.CL']",2501.09137," We study the gradient descent (GD) dynamics of a depth-2 linear neural
network with a single input and output. We show that GD converges at an
explicit linear rate to a global minimum of the training loss, even with a
large stepsize -- about $2/\textrm{sharpness}$. It still converges for even
larger stepsizes, but may do so very slowly. We also characterize the solution
to which GD converges, which has lower norm and sharpness than the gradient
flow solution. Our analysis reveals a trade off between the speed of
convergence and the magnitude of implicit regularization. This sheds light on
the benefits of training at the ``Edge of Stability'', which induces additional
regularization by delaying convergence and may have implications for training
more complex models.","['cs.LG', 'math.OC', 'stat.ML']",False,,,,Improving Value-based Process Verifier via Structural Prior Injection,"Gradient Descent Converges Linearly to Flatter Minima than Gradient Flow
  in Shallow Linear Networks"
neg-d2-174,2025-03-11,,2503.08401," The mean resolvent operator predicts, in the frequency domain, the mean
linear response to forcing, and, as such, it provides the optimal LTI
approximation of the input-output dynamics of flows in the statistically steady
regime (Leclercq & Sipp 2023). In this paper, we aim at providing numerical
frameworks to extract optimal forcings and responses of the mean resolvent,
also known as mean resolvent modes. For periodic flows, we rewrite the mean
resolvent operator in terms of a harmonic resolvent operator (Wereley & Hall
1990; Padovan & Rowley 2022) to obtain reference mean resolvent modes.
Successively, we propose a projection algorithm approximating those modes
within a subspace of mean-flow resolvent modes. The projected problem is
directly solved in the frequency domain, but we also discuss a time-stepper
version that can bypass the explicit construction of the operator without
recurring to direct-adjoint looping. We evaluate the algorithms on an
incompressible axisymmetric laminar jet periodically forced at the inlet. For a
weakly unsteady case, the mean-flow resolvent correctly approximates the main
receptivity peak of the mean resolvent, but completely fails to capture a
secondary receptivity peak. For a strongly unsteady case, even the main
receptivity peak of the mean resolvent is incorrectly captured by the mean-flow
resolvent. Although the present algorithms are currently restricted to periodic
flows, input projection may be a key ingredient to extend mean resolvent
analysis to more complex statistically steady flows.",['physics.flu-dyn'],2502.11144," In Genome-Wide Association Studies (GWAS), heritability is defined as the
fraction of variance of an outcome explained by a large number of genetic
predictors in a high-dimensional polygenic linear model. This work studies the
asymptotic properties of the most common estimator of heritability from summary
statistics called linkage disequilibrium score (LDSC) regression, together with
a simpler and closely related estimator called GWAS heritability (GWASH). These
estimators are analyzed in their basic versions and under various modifications
used in practice including weighting and standardization. We show that, with
some variations, two conditions which we call weak dependence (WD) and
bounded-kurtosis effects (BKE) are sufficient for consistency of both the basic
LDSC with fixed intercept and GWASH estimators, for both Gaussian and
non-Gaussian predictors. For Gaussian predictors it is shown that these
conditions are also necessary for consistency of GWASH (with truncation) and
simulations suggest that necessity holds too when the predictors are
non-Gaussian. We also show that, with properly truncated weights, weighting
does not change the consistency results, but standardization of the predictors
and outcome, as done in practice, introduces bias in both LDSC and GWASH if the
two essential conditions are violated. Finally, we show that, when population
stratification is present, all the estimators considered are biased, and the
bias is not remedied by using the LDSC regression estimator with free
intercept, as originally suggested by the authors of that estimator.","['math.ST', 'stat.TH']",False,,,,Mean resolvent analysis of periodic flows,"Consistency of heritability estimation from summary statistics in
  high-dimensional linear models"
neg-d2-175,2025-02-27,,2502.20594," As the only compact elliptical close enough to resolve into individual stars,
the satellite dwarf galaxy M32 provides a unique opportunity for exploring the
origins of such rare galaxies. In this work, we combined archival and novel
Keck/DEIMOS spectroscopy from a southern extension of the Spectroscopic and
Photometric Landscape of Andromeda's Stellar Halo (SPLASH) survey with optical
HST imaging from the Panchromatic Hubble Andromeda Southern Treasury (PHAST)
survey. The resulting sample of 2525 giant stars is unprecedented both in size
and spatial coverage (0.9-15.5 arcmin, or out to $\sim$23$r_{\rm eff}$ and
$\sim$30$r_{\rm eff}$ along M32's major and minor axes) for probing the
resolved stellar outskirts of M32. Given the structurally complex region near
M32 on the sky, we modeled M32's line-of-sight kinematics simultaneously
alongside M31's rotating stellar disk and potential outliers corresponding to
M31's kinematically hot stellar halo and/or tidal substructure. Inside the
radius corresponding to the observed twisting of isophotal contours in M32's
surface brightness profile ($R_{\rm iso} \sim$ 5$r_{\rm eff}$ $\sim$ 150'' or
0.56 kpc), M32 exhibits a line-of-sight velocity distribution characteristic of
ordered rotation, transitioning to a distribution with heavier outliers beyond
this radius. Within $R_{\rm iso}$, the rotational direction is aligned with
M32's major-axis rotation, but shifts to become roughly aligned with M32's
minor axis beyond $R_{\rm iso}$. We interpret these kinematical signatures in
the stellar outskirts of M32 as evidence of tidal distortion from interactions
with M31 and discuss their implications for M32 formation pathways.",['astro-ph.GA'],2501.0854," The task of building semantics for structured data such as CSV, JSON, and XML
files is highly relevant in the knowledge representation field. Even though we
have a vast of structured data on the internet, mapping them to domain
ontologies to build semantics for them is still very challenging as it requires
the construction model to understand and learn graph-structured knowledge.
Otherwise, the task will require human beings' effort and cost. In this paper,
we proposed a novel automatic semantic modeling framework: Knowledge Prompt
Chaining. It can serialize the graph-structured knowledge and inject it into
the LLMs properly in a Prompt Chaining architecture. Through this knowledge
injection and prompting chaining, the model in our framework can learn the
structure information and latent space of the graph and generate the semantic
labels and semantic graphs following the chains' insturction naturally. Based
on experimental results, our method achieves better performance than existing
leading techniques, despite using reduced structured input data.","['cs.CL', 'cs.AI', 'cs.DB']",False,,,,"Kinematical Modeling of the Resolved Stellar Outskirts of M32:
  Constraints on Tidal Stripping Scenarios",Knowledge prompt chaining for semantic modeling
neg-d2-176,2025-01-14,,2501.08106," The method of H- photoionization is interesting for laser assisted charge
exchange injection. In this paper, the model and computation of photoionization
of negative hydrogen ion by using strong lasers is considered. The development
of this work is motivated by using pure lasers for photodetachment of electron
from negative hydrogen ion when it is not convenient or not possible to use
stripping magnet. Herein we develop a method of calculation of high efficiency
photoionization using time dependent wave equation with application of powerful
lasers. We compare this precise method of calculation with simplified method of
calculation through linear model of cross section interaction. Another
mechanism of photodetachment through excitation of the Feshbach resonance is
also considered.","['physics.atom-ph', 'physics.acc-ph']",2502.18719," Achieving high subject-independent accuracy in functional near-infrared
spectroscopy (fNIRS)-based brain-computer interfaces (BCIs) remains a
challenge, particularly when minimizing the number of channels. This study
proposes a novel feature extraction scheme and a Pearson correlation-based
channel selection algorithm to enhance classification accuracy while reducing
hardware complexity. Using an open-access fNIRS dataset, our method improved
average accuracy by 28.09% compared to existing approaches, achieving a peak
subject-independent accuracy of 95.98% with only two channels. These results
demonstrate the potential of our optimized feature extraction and channel
selection methods for developing efficient, subject-independent fNIRS-based BCI
systems.","['cs.HC', 'eess.SP']",False,,,,Photodetachment of negative hydrogen ion beam,"Enhancing Subject-Independent Accuracy in fNIRS-based Brain-Computer
  Interfaces with Optimized Channel Selection"
neg-d2-177,2025-02-11,,2502.08085," Traditional methods for visualizing dynamic human expressions, particularly
in medical training, often rely on flat-screen displays or static mannequins,
which have proven inefficient for realistic simulation. In response, we propose
a platform that leverages a 3D interactive facial avatar capable of displaying
non-verbal feedback, including pain signals. This avatar is projected onto a
stereoscopic, view-dependent 3D display, offering a more immersive and
realistic simulated patient experience for pain assessment practice. However,
there is no existing solution that dynamically predicts and projects
interactive 3D facial avatars in real-time. To overcome this, we emphasize the
need for a 3D display projection system that can project the facial avatar
holographically, allowing users to interact with the avatar from any viewpoint.
By incorporating 3D Gaussian Splatting (3DGS) and real-time view-dependent
calibration, we significantly improve the training environment for accurate
pain recognition and assessment.",['cs.GR'],2501.0719," Topological quantum materials that show strongly correlated electrons as well
as topological order, for which spin-orbit coupling is a key ingredient,
exhibit novel states of matter. One such example is the family of pyrochlore
iridates, featuring strong spin-orbital coupling, strong electron interactions
as well as geometric frustration, making them an ideal platform to study novel
topological phases. High-quality epitaxial pyrochlore iridate films, although
challenging to produce, provide a pathway to explore unconventional behaviours
and unravel the intrinsic properties of these largely unexplored materials.
Additionally, designing interfaces with specific properties is crucial to
create multilayered devices that can achieve significant technological
breakthroughs using topological states of these materials. This article reviews
experimental work on epitaxial pyrochlore iridate thin films, discussing
evidence of topological phases found in them. Future research directions are
outlined, which include exploring the rich tunability offered by chemical
doping, especially when combined with the design of epitaxial heterostructures.","['cond-mat.str-el', 'cond-mat.mtrl-sci']",False,,,,Interactive Holographic Visualization for 3D Facial Avatar,Epitaxial thin films of pyrochlore iridates: a forward looking approach
neg-d2-178,2025-02-20,,2502.14509," Does multilingual Neural Machine Translation (NMT) lead to The Curse of the
Multlinguality or provides the Cross-lingual Knowledge Transfer within a
language family? In this study, we explore multiple approaches for extending
the available data-regime in NMT and we prove cross-lingual benefits even in
0-shot translation regime for low-resource languages. With this paper, we
provide state-of-the-art open-source NMT models for translating between
selected Slavic languages. We released our models on the HuggingFace Hub
(https://hf.co/collections/allegro/multislav-6793d6b6419e5963e759a683) under
the CC BY 4.0 license. Slavic language family comprises morphologically rich
Central and Eastern European languages. Although counting hundreds of millions
of native speakers, Slavic Neural Machine Translation is under-studied in our
opinion. Recently, most NMT research focuses either on: high-resource languages
like English, Spanish, and German - in WMT23 General Translation Task 7 out of
8 task directions are from or to English; massively multilingual models
covering multiple language groups; or evaluation techniques.",['cs.CL'],2501.03116," We extend the classical Poincar\'e-Birkhoff-Witt theorem to higher algebra by
establishing a version that applies to spectral Lie algebras. We deduce this
statement from a basic relation between operads in spectra: the commutative
operad is the quotient of the associative operad by a right action of the
spectral Lie operad. This statement, in turn, is a consequence of a fundamental
relation between different $\mathbb{E}_n$-operads, which we articulate and
prove. We deduce a variant of the Poincar\'{e}--Birkhoff--Witt theorem for
relative enveloping algebras of $\mathbb{E}_n$-algebras. Our methods also give
a simple construction and description of the higher enveloping
$\mathbb{E}_n$-algebras of a spectral Lie algebra.","['math.AT', 'math.CT', 'math.RT']",False,,,,"MultiSlav: Using Cross-Lingual Knowledge Transfer to Combat the Curse of
  Multilinguality",Poincar\'{e}-Birkhoff-Witt Theorems in Higher Algebra
neg-d2-179,2025-02-25,,2502.17875," In recent years, the fifth-generation (5G) new radio (NR) signals have
emerged as a promising supplementary resource for urban navigation. However, a
major challenge in utilizing 5G signals lies in their vulnerability to
non-line-of-sight (NLoS) propagation effects, which are especially prevalent in
urban street canyons. This paper applies the direct position estimation (DPE)
method to 5G cellular signals to mitigate the NLoS bias as well as the
multipath effects, thereby enabling precise localization in urbanized
environments. The feasibility of applying the DPE method to NR positioning is
analyzed, followed by a discussion of the tapped delay line (TDL) channel
propagation model provided by the 3rd Generation Partnership Project (3GPP).
The positioning performance is then evaluated through large-scale system-level
simulations. The simulation results demonstrate that 5G DPE achieves
satisfactory positioning accuracy in a 10 dB noisy channel, with an overall
root mean square error (RMSE) constrained within 6 m. In addition, 5G DPE
outperforms the observed time difference of arrival (OTDoA) method by 95.24% in
terms of positioning accuracy in an NLoS-dominated propagation environment.",['eess.SP'],2501.07779," The transition route from laminar to turbulent flow in a magnetohydrodynamic
(MHD) duct with a square cross-section is investigated in the limit of low
magnetic Reynolds number. In the presence of a transverse magnetic field,
Hartmann and Shercliff layers are present on the walls orthogonal and parallel
to the field direction, respectively. We assume reflection symmetries in both
transverse directions, and investigate the competition between transition
mechanisms specific to each boundary layer using direct numerical simulations.
Independently of which wall turbulence eventually occupies, transition relies
exclusively on a tripping of the Shercliff layer by perturbations, while the
Hartmann layer plays a passive role. This is explained, using a dynamical
systems interpretation, by the spatial localization of the edge states in the
Shercliff layer at the expense of the Hartmann layer. The link between these
non-linear coherent structures and the linear optimal modes known from
non-modal stability and energy stability theory is pointed out.",['physics.flu-dyn'],False,,,,"5G Direct Position Estimation for Precise Localization in Dense Urban
  Area",The route to turbulence in magnetohydrodynamic square duct flow
neg-d2-180,2025-02-18,,2502.13101," The integration of Artificial Intelligence (AI) in urban governance presents
significant opportunities to transform decision-making and enhance
accountability. The paper highlights AI's potential to reposition human
discretion and reshape specific types of accountability, elevating the
decision-making capabilities of both frontline bureaucrats and managers while
ensuring ethical standards and public trust are maintained. While AI can
enhance bureaucratic flexibility and efficiency, its integration will also
necessitate new governance frameworks to mitigate risks associated with uneven
capacity distribution, ethical concerns, and public trust. Following the
literature review and theoretical discussion, this study introduces a set of
guiding principles for AI-assisted urban governance, emphasizing equitable AI
deployment, adaptive administrative structures, robust data governance,
transparent human-AI collaboration, and citizen engagement in oversight
mechanisms. By critically evaluating AI's dual role in expanding discretion and
reinforcing accountability, this paper advances a framework for responsible AI
adoption, ensuring that urban governance remains adaptive, transparent, and
aligned with public values.",['cs.CY'],2503.13469," Cardiovascular diseases (CVDs) are disorders impacting the heart and
circulatory system. These disorders are the foremost and continuously
escalating cause of mortality worldwide. One of the main tasks when working
with CVDs is analyzing and identifying pathologies on a 12-lead
electrocardiogram (ECG) with a standard 10-second duration. Using machine
learning (ML) in automatic ECG analysis increases CVD diagnostics'
availability, speed, and accuracy. However, the most significant difficulty in
developing ML models is obtaining a sufficient training dataset. Due to the
limitations of medical data usage, such as expensiveness, errors, the ambiguity
of labels, imbalance of classes, and privacy issues, utilizing synthetic
samples depending on specific pathologies bypasses these restrictions and
improves algorithm quality. Existing solutions for the conditional generation
of ECG signals are mainly built on Generative Adversarial Networks (GANs), and
only a few papers consider the architectures based on Variational Autoencoders
(VAEs), showing comparable results in recent works. This paper proposes the
publicly available conditional Nouveau VAE model for ECG signal generation
(cNVAE-ECG), which produces high-resolution ECGs with multiple pathologies. We
provide an extensive comparison of the proposed model on various practical
downstream tasks, including transfer learning scenarios showing an area under
the receiver operating characteristic (AUROC) increase up to 2% surpassing
GAN-like competitors.","['eess.SP', 'cs.CV', 'cs.LG']",False,,,,"AI and the Transformation of Accountability and Discretion in Urban
  Governance","Conditional Electrocardiogram Generation Using Hierarchical Variational
  Autoencoders"
neg-d2-181,2025-03-08,,2503.06187," In Neural Networks, there are various methods of feature fusion. Different
strategies can significantly affect the effectiveness of feature
representation, consequently influencing the ability of model to extract
representative and discriminative features. In the field of face recognition,
traditional feature fusion methods include feature concatenation and feature
addition. Recently, various attention mechanism-based fusion strategies have
emerged. However, we found that these methods primarily focus on the important
features in the image, referred to as salient features in this paper, while
neglecting another equally important set of features for image recognition
tasks, which we term differential features. This may cause the model to
overlook critical local differences when dealing with complex facial samples.
Therefore, in this paper, we propose an efficient convolution module called
MSConv (Multiplicative and Subtractive Convolution), designed to balance the
learning of model about salient and differential features. Specifically, we
employ multi-scale mixed convolution to capture both local and broader
contextual information from face images, and then utilize Multiplication
Operation (MO) and Subtraction Operation (SO) to extract salient and
differential features, respectively. Experimental results demonstrate that by
integrating both salient and differential features, MSConv outperforms models
that only focus on salient features.","['cs.CV', 'cs.AI']",2501.08827," The key molecules such as triphosphate (ATP), glutathione (GSH), and
homocarnosine (hCs) - central to metabolic processes in the human brain remain
elusive or challenging to detect with upfield 1H-MRSI. Traditional 3D 1H-MRSI
in vivo faces challenges, including a low signal-to-noise ratio and
magnetization transfer effects with water, leading to prolonged measurement
times and reduced resolution. To address these limitations, we propose a
downfield 3D-MRSI method aimed at measuring downfield metabolites with enhanced
spatial resolution, and speed acceptable for clinical practice at 7T. The
CHEmical-shift selective Adiabatic Pulse (CHEAP) technique was integrated into
echo-planar spectroscopic imaging (EPSI) readout sequence for downfield
metabolite and water reference 3D-MRSI. Five healthy subjects and two glioma
patients were scanned to test the feasibility. In this work, CHEAP-EPSI
technique is shown to significantly enhance spatial the resolution to 0.37 ml
while simultaneously reducing the scan time to 10.5 minutes. Its distinct
advantages include low specific absorption rate, effective suppression of water
and lipid signals, and minimal baseline distortions, making it a valuable tool
for research or potentially diagnostic purposes. CHEAP-EPSI improves the
detection sensitivity of downfield metabolites like N-acetyl-aspartate (NAA+)
and DF8.18 (ATP&GSH+), and offers new possibilities for the study of metabolism
in healthy and diseased brain.",['physics.med-ph'],False,,,,MSConv: Multiplicative and Subtractive Convolution for Face Recognition,"CHEmical-shift selective Adiabatic Pulse (CHEAP): Fast and High
  Resolution Downfield 3D 1H-MRSI at 7T"
neg-d2-182,2025-01-07,,2501.04112," In this work, we provide the first example of an infinite family of branch
groups in the class of non-contracting self-similar groups. We show that these
groups are very strongly fractal, not regular branch, and of exponential
growth. Further, we prove that these groups do not have the congruence subgroup
property by explicitly calculating the structure of their rigid kernels. This
class of groups is also the first example of branch groups with non-torsion
rigid kernels. As a consequence of these results, we also determine the
Hausdorff dimension of these groups.",['math.GR'],2503.14669," This paper presents a reinforcement learning-based neuroadaptive control
framework for robotic manipulators operating under deferred constraints. The
proposed approach improves traditional barrier Lyapunov functions by
introducing a smooth constraint enforcement mechanism that offers two key
advantages: (i) it minimizes control effort in unconstrained regions and
progressively increases it near constraints, improving energy efficiency, and
(ii) it enables gradual constraint activation through a prescribed-time
shifting function, allowing safe operation even when initial conditions violate
constraints. To address system uncertainties and improve adaptability, an
actor-critic reinforcement learning framework is employed. The critic network
estimates the value function, while the actor network learns an optimal control
policy in real time, enabling adaptive constraint handling without requiring
explicit system modeling. Lyapunov-based stability analysis guarantees the
boundedness of all closed-loop signals. The effectiveness of the proposed
method is validated through numerical simulations.","['cs.RO', 'cs.SY', 'eess.SY']",False,,,,A Class of Non-Contracting Branch Groups with Non-Torsion Rigid Kernels,"Reinforcement Learning-Based Neuroadaptive Control of Robotic
  Manipulators under Deferred Constraints"
neg-d2-183,2025-01-03,,2501.0194," We report the results of long-term time series photometry on RX J2133.7+5107
(also known as 1RXS J213344.1+510725) obtained at several observatories. Using
data taken during 17 years, we determined the current value of the spin period
of $570.811470$ seconds with the formal accuracy of $0.000006$ seconds and a
spin-up of the white dwarf with a characteristic time of $1.483(1)\times10^5$
years. This is even faster than that reported previously and, if confirmed,
makes this object have one of the fastest spin-up timescales of all known
intermediate polars. We derived an improved value of the superhump period of
the system to be $0^d.280130(1)$. Superhump maxima timings are moving on the
phase curve from season to season, showing non-monotonic changes, without a
change in superhump period.","['astro-ph.SR', 'astro-ph.HE']",2503.12982," Cooperative perception can increase the view field and decrease the occlusion
of an ego vehicle, hence improving the perception performance and safety of
autonomous driving. Despite the success of previous works on cooperative object
detection, they mostly operate on dense Bird's Eye View (BEV) feature maps,
which are computationally demanding and can hardly be extended to long-range
detection problems. More efficient fully sparse frameworks are rarely explored.
In this work, we design a fully sparse framework, SparseAlign, with three key
features: an enhanced sparse 3D backbone, a query-based temporal context
learning module, and a robust detection head specially tailored for sparse
features. Extensive experimental results on both OPV2V and DairV2X datasets
show that our framework, despite its sparsity, outperforms the state of the art
with less communication bandwidth requirements. In addition, experiments on the
OPV2Vt and DairV2Xt datasets for time-aligned cooperative object detection also
show a significant performance gain compared to the baseline works.",['cs.CV'],False,,,,Spin-period variations in the intermediate polar RX J2133.7+5107,SparseAlign: A Fully Sparse Framework for Cooperative Object Detection
neg-d2-184,2025-02-21,,2502.15576," Large language models (LLMs) excel at handling human queries, but they can
occasionally generate flawed or unexpected responses. Understanding their
internal states is crucial for understanding their successes, diagnosing their
failures, and refining their capabilities. Although sparse autoencoders (SAEs)
have shown promise for interpreting LLM internal representations, limited
research has explored how to better explain SAE features, i.e., understanding
the semantic meaning of features learned by SAE. Our theoretical analysis
reveals that existing explanation methods suffer from the frequency bias issue,
where they emphasize linguistic patterns over semantic concepts, while the
latter is more critical to steer LLM behaviors. To address this, we propose
using a fixed vocabulary set for feature interpretations and designing a mutual
information-based objective, aiming to better capture the semantic meaning
behind these features. We further propose two runtime steering strategies that
adjust the learned feature activations based on their corresponding
explanations. Empirical results show that, compared to baselines, our method
provides more discourse-level explanations and effectively steers LLM behaviors
to defend against jailbreak attacks. These findings highlight the value of
explanations for steering LLM behaviors in downstream applications. We will
release our code and data once accepted.",['cs.CL'],2502.2045," Superconductivity has recently been demonstrated in La$_3$Ni$_2$O$_7$ up to
91K under moderate pressure in bulk crystals, and up to 48K at ambient pressure
in thin films grown under compressive strain. Key questions remain open
regarding the crystal structure and low-energy electronic states that support
superconductivity in these compounds. Here we take advantage of the natural
polymorphism between bilayer (2222) or alternating monolayer-trilayer (1313)
stacking sequences that arises in bulk La$_3$Ni$_2$O$_7$ crystals to identify
universal features in this family of materials. Employing angle-resolved
photoemission spectroscopy (ARPES) we observe the fingerprint of a spin-density
wave (SDW) instability, strong and coherent enough to modify the electronic
structure. We demonstrate that this feature is a `translated' $\beta$ Fermi
surface associated with a scattering vector $Q_{t\beta}$ which matches the
$Q_{SDW}$ detected by neutron and x-ray scattering experiments. This
observation provides an important link between surface and bulk probes, and
demonstrates a universal connection between magnetism and fermiology in
La$_3$Ni$_2$O$_7$ as well as La$_4$Ni$_3$O$_{10}$. We simulate the spectral
weight distribution observed in our ARPES dichroism experiments and establish
that the low-energy electronic phenomenology is dominated by oxygen-centered
planar orbitals, which -- upon moving along the Fermi surface away from the
Ni-O-Ni bond directions -- evolve from the $d_{3x^2-r^2}$ and $d_{3y^2-r^2}$
symmetry characteristic of 3-spin polarons to the familiar $d_{x^2-y^2}$
Zhang-Rice singlets that support high-temperature superconductivity in
cuprates. Despite the multiorbital nature of the nickelates, our work
establishes an empirical correspondence between the low-energy electronic
structure of cuprates and nickelates, thus suggesting a common origin for their
unconventional superconductivity.",['cond-mat.supr-con'],False,,,,"Interpreting and Steering LLMs with Mutual Information-based
  Explanations on Sparse Autoencoders","Universal electronic structure of layered nickelates via oxygen-centered
  planar orbitals"
neg-d2-185,2025-02-02,,2502.007," Transformer-based Learned Image Compression (LIC) suffers from a suboptimal
trade-off between decoding latency and rate-distortion (R-D) performance.
Moreover, the critical role of the FeedForward Network (FFN)-based channel
aggregation module has been largely overlooked. Our research reveals that
efficient channel aggregation-rather than complex and time-consuming spatial
operations-is the key to achieving competitive LIC models. Based on this
insight, we initiate the ``S2CFormer'' paradigm, a general architecture that
simplifies spatial operations and enhances channel operations to overcome the
previous trade-off. We present two instances of the S2CFormer: S2C-Conv, and
S2C-Attention. Both models demonstrate state-of-the-art (SOTA) R-D performance
and significantly faster decoding speed. Furthermore, we introduce S2C-Hybrid,
an enhanced variant that maximizes the strengths of different S2CFormer
instances to achieve a better performance-latency trade-off. This model
outperforms all the existing methods on the Kodak, Tecnick, and CLIC
Professional Validation datasets, setting a new benchmark for efficient and
high-performance LIC. The code is at
\href{https://github.com/YunuoChen/S2CFormer}{https://github.com/YunuoChen/S2CFormer}.","['cs.CV', 'eess.IV']",2503.09495," The calibration of the CR39 and Makrofol Nuclear Track Detectors of the
MoEDAL experiment at the CERN-LHC was performed by exposing stacks of detector
foils to heavy ion beams with energies ranging from 340 MeV/nucleon to 150
GeV/nucleon. After chemical etching, the base areas and lengths of etch-pit
cones were measured using automatic and manual optical microscopes. The
response of the detectors, as measured by the ratio of the track-etching rate
over the bulk-etching rate, was determined over a range extending from their
threshold at Z/$\beta\sim7$ and $\sim50$ for CR39 and Makrofol, respectively,
up to Z/$\beta\sim92$",['physics.ins-det'],False,,,,"S2CFormer: Revisiting the RD-Latency Trade-off in Transformer-based
  Learned Image Compression","Calibration of Solid State Nuclear Track Detectors for Rare Event
  Searches"
neg-d2-186,2025-01-28,,2501.17295," Machine Translation (MT) is undergoing a paradigm shift, with systems based
on fine-tuned large language models (LLM) becoming increasingly competitive
with traditional encoder-decoder models trained specifically for translation
tasks. However, LLM-based systems are at a higher risk of generating
hallucinations, which can severely undermine user's trust and safety. Most
prior research on hallucination mitigation focuses on traditional MT models,
with solutions that involve post-hoc mitigation - detecting hallucinated
translations and re-translating them. While effective, this approach introduces
additional complexity in deploying extra tools in production and also increases
latency. To address these limitations, we propose a method that intrinsically
learns to mitigate hallucinations during the model training phase.
Specifically, we introduce a data creation framework to generate hallucination
focused preference datasets. Fine-tuning LLMs on these preference datasets
reduces the hallucination rate by an average of 96% across five language pairs,
while preserving overall translation quality. In a zero-shot setting our
approach reduces hallucinations by 89% on an average across three unseen target
languages.","['cs.CL', 'cs.AI', 'cs.LG']",2502.13967," Image tokenization has enabled major advances in autoregressive image
generation by providing compressed, discrete representations that are more
efficient to process than raw pixels. While traditional approaches use 2D grid
tokenization, recent methods like TiTok have shown that 1D tokenization can
achieve high generation quality by eliminating grid redundancies. However,
these methods typically use a fixed number of tokens and thus cannot adapt to
an image's inherent complexity. We introduce FlexTok, a tokenizer that projects
2D images into variable-length, ordered 1D token sequences. For example, a
256x256 image can be resampled into anywhere from 1 to 256 discrete tokens,
hierarchically and semantically compressing its information. By training a
rectified flow model as the decoder and using nested dropout, FlexTok produces
plausible reconstructions regardless of the chosen token sequence length. We
evaluate our approach in an autoregressive generation setting using a simple
GPT-style Transformer. On ImageNet, this approach achieves an FID<2 across 8 to
128 tokens, outperforming TiTok and matching state-of-the-art methods with far
fewer tokens. We further extend the model to support to text-conditioned image
generation and examine how FlexTok relates to traditional 2D tokenization. A
key finding is that FlexTok enables next-token prediction to describe images in
a coarse-to-fine ""visual vocabulary"", and that the number of tokens to generate
depends on the complexity of the generation task.","['cs.CV', 'cs.LG']",False,,,,"Mitigating Hallucinated Translations in Large Language Models with
  Hallucination-focused Preference Optimization",FlexTok: Resampling Images into 1D Token Sequences of Flexible Length
neg-d2-187,2025-01-20,,2501.17881," Wireless indoor localization has been a pivotal area of research over the
last two decades, becoming a cornerstone for numerous sensing applications.
However, conventional wireless localization methods rely on channel state
information to perform blind modelling and estimation of a limited set of
localization parameters. This oversimplification neglects many sensing scene
details, resulting in suboptimal localization accuracy. To address this
limitation, this paper presents a novel approach to wireless indoor
localization by reformulating it as an inverse problem of wireless ray-tracing,
inferring scene parameters that generates the measured CSI. At the core of our
solution is a fully differentiable ray-tracing simulator that enables
backpropagation to comprehensive parameters of the sensing scene, allowing for
precise localization. To establish a robust localization context, RayLoc
constructs a high-fidelity sensing scene by refining coarse-grained background
model. Furthermore, RayLoc overcomes the challenges of sparse gradient and
local minima by convolving the signal generation process with a Gaussian
kernel. Extensive experiments showcase that RayLoc outperforms traditional
localization baselines and is able to generalize to different sensing
environments.","['eess.SP', 'cs.AI', 'cs.LG', 'cs.NI']",2503.00319," Electrical control of quantum magnetic states is essential in spintronic
science. Initial studies on the ferromagnetic state control were extended to
collinear antiferromagnets and, more recently, noncollinear antiferromagnets.
However, electrical control mechanisms of such exotic magnetic states remain
poorly understood. Here, we report the first experimental and theoretical
example of the current control of helical antiferromagnets, arising from the
competition between collinear antiferromagnetic exchange and interlayer
Dzyaloshinskii-Moriya interaction in new van-der-Waals (vdW) material
Ni1/3NbS2. Due to the intrinsic broken inversion symmetry, an in-plane current
generates spin-orbit torque that, in turn, interacts directly with the helical
antiferromagnetic order. Our theoretical analyses indicate that a weak
ferromagnetic order coexists due to the Dzyaloshinskii-Moriya interaction,
mediating the spin-orbit torque to collectively rotate the helical
antiferromagnetic order. Our Ni1/3NbS2 nanodevice experiments produce
current-dependent resistance change consistent with the theoretical prediction.
This work widens our understanding of the electrical control of helical
antiferromagnets and promotes vdW quantum magnets as interesting material
platforms for electrical control.","['cond-mat.mtrl-sci', 'cond-mat.other', 'physics.app-ph', 'quant-ph']",False,,,,"RayLoc: Wireless Indoor Localization via Fully Differentiable
  Ray-tracing","Current-driven collective control of helical spin texture in van der
  Waals antiferromagnet"
neg-d2-188,2025-01-15,,2501.09137," We study the gradient descent (GD) dynamics of a depth-2 linear neural
network with a single input and output. We show that GD converges at an
explicit linear rate to a global minimum of the training loss, even with a
large stepsize -- about $2/\textrm{sharpness}$. It still converges for even
larger stepsizes, but may do so very slowly. We also characterize the solution
to which GD converges, which has lower norm and sharpness than the gradient
flow solution. Our analysis reveals a trade off between the speed of
convergence and the magnitude of implicit regularization. This sheds light on
the benefits of training at the ``Edge of Stability'', which induces additional
regularization by delaying convergence and may have implications for training
more complex models.","['cs.LG', 'math.OC', 'stat.ML']",2501.15917," This article presents a novel perspective to model and simulate
reconfigurable intelligent surface (RIS)-assisted communication systems.
Traditional methods in antenna design often rely on array method to simulate,
whereas communication system modeling tends to idealize antenna behavior.
Neither approach sufficiently captures the detailed characteristics of
RIS-assisted communication. To address this limitation, we propose a
comprehensive simulation framework that jointly models RIS antenna design and
the communication process. This framework simulates the entire communication
pipeline, encompassing signal generation, modulation, propagation, RIS-based
radiation, signal reception, alignment, demodulation, decision, and processing.
Using a QPSK-modulated signal for validation, we analyze system performance and
investigate the relationship between bit error rate (BER), aperture fill time,
array size, and baseband symbol frequency. The results indicate that larger
array sizes and higher baseband symbol frequencies exacerbate aperture fill
time effects, leading to increased BER. Furthermore, we examine BER variation
with respect to signal-to-noise ratio (SNR) and propose an optimal
matching-based alignment algorithm, which significantly reduces BER compared to
conventional pilot-based alignment methods. This work demonstrates the entire
process of RIS communication, and reveals the source of bit errors, which
provides valuable insights into the design and performance optimization of
RIS-assisted communication systems.",['physics.app-ph'],False,,,,"Gradient Descent Converges Linearly to Flatter Minima than Gradient Flow
  in Shallow Linear Networks","RIS Assisted Wireless Communication: Advanced Modeling, Simulation, and
  Analytical Insights"
neg-d2-189,2025-01-22,,2501.13297," Multi-modal retrieval-augmented Question Answering (MRAQA), integrating text
and images, has gained significant attention in information retrieval (IR) and
natural language processing (NLP). Traditional ranking methods rely on small
encoder-based language models, which are incompatible with modern decoder-based
generative large language models (LLMs) that have advanced various NLP tasks.
To bridge this gap, we propose RAMQA, a unified framework combining
learning-to-rank methods with generative permutation-enhanced ranking
techniques. We first train a pointwise multi-modal ranker using LLaVA as the
backbone. Then, we apply instruction tuning to train a LLaMA model for
re-ranking the top-k documents using an innovative autoregressive multi-task
learning approach. Our generative ranking model generates re-ranked document
IDs and specific answers from document candidates in various permutations.
Experiments on two MRAQA benchmarks, WebQA and MultiModalQA, show significant
improvements over strong baselines, highlighting the effectiveness of our
approach. Code and data are available at: https://github.com/TonyBY/RAMQA","['cs.CL', 'cs.AI', 'cs.IR', 'cs.LG']",2502.051," Virtual reality enables users to experience real-life situations in immersive
environments. Interaction methods significantly shape user experience,
particularly in high fidelity simulations mimicking real world tasks. This
study evaluates two primary VR interaction techniques, hand based and
controller based, through virtual shopping tasks in a simulated supermarket
with 40 participants. Hand-based interaction was preferred for its natural,
immersive qualities and alignment with real-world gestures but faced usability
challenges, including limited haptic feedback and grasping inefficiencies. In
contrast, controller-based interaction offered greater precision and
reliability, making it more suitable for tasks requiring fine motor skills.",['cs.HC'],False,,,,"RAMQA: A Unified Framework for Retrieval-Augmented Multi-Modal Question
  Answering","Hands vs. Controllers: Comparing User Interactions in Virtual Reality
  Shopping Environments"
neg-d2-190,2025-03-19,,2503.15775," A fast method for calculating the reflected and transmitted waves for a given
nonuniform plane wave incident on an arbitrarily oriented and charged planar
interface between two isotropic and possibly lossy media is proposed based on
the decomposition of the complex wave vector and complex wave numbers with
respect to the unit normal vector of the interface. According to the complex
vector analysis, the exact definition of the complex angles of incidence,
reflection and refraction are presented and applied in the complex forms of
Snell's law and Fresnel equations to quickly and correctly calculate the
complex wave vectors and the complex electric fields of the reflected and
refracted waves at a charged interface where the surface charge and current
densities are considered. The calculation procedure and two practical examples
are also given to demonstrate the validity and powerfulness of the proposed
methodology.",['physics.optics'],2501.12504," The unit group of the ring of integers of a number field, modulo torsion, is
a lattice via the logarithmic Minkowski embedding. We examine the shape of this
lattice, which we call the unit shape, within the family of prime degree $p$
number fields whose Galois closure has dihedral Galois group $D_p$ and a unique
real embedding. In the case $p = 5$, we prove that the unit shapes lie on a
single hypercycle on the modular surface (in this case, the modular surface is
the space of shapes of rank $2$ lattices). For general $p$, we show that the
unit shapes are contained in a finite union of translates of periodic torus
orbits in the space of shapes.",['math.NT'],False,,,,"Fast Calculation of Nonuniform Plane Waves at Arbitrarily Oriented and
  Charged Planar Interfaces of Isotropic Lossy Media",Shapes of unit lattices in $D_p$-number fields
neg-d2-191,2025-02-11,,2502.08051," We study the screening mass of the neutral rho-meson in the presence of
strong magnetic fields using the Kroll-Lee-Zumino (KLZ) model. The rho-meson
self-energy is computed at one-loop order within the lowest Landau level (LLL)
approximation, considering the magnetic field as the dominant energy scale. Due
to Lorentz symmetry breaking induced by the external field, we decompose the
self-energy into three independent tensor structures, which give rise to three
distinct modes. Additionally, the four-momentum splits into parallel and
perpendicular components, leading to two types of screening masses: the
parallel screening mass ( $p_0=0$ and $p_\perp \to 0$ ) and the perpendicular
screening mass ( $p_0=0$ and $p_\parallel \to 0$ ). Our results show that the
zero and perpendicular modes exhibit a monotonically increasing behavior with
the magnetic field strength, whereas the parallel mode remains essentially
constant. These findings provide new insights into the behavior of vector
mesons in strongly magnetized media, with implications for QCD under extreme
conditions.","['hep-ph', 'hep-th']",2502.15642," Neural Ordinary Differential Equations (Neural ODEs) represent
continuous-time dynamics with neural networks, offering advancements for
modeling and control tasks. However, training Neural ODEs requires solving
differential equations at each epoch, leading to high computational costs. This
work investigates simultaneous optimization methods as a faster training
alternative. In particular, we employ a collocation-based, fully discretized
formulation and use IPOPT--a solver for large-scale nonlinear optimization--to
simultaneously optimize collocation coefficients and neural network parameters.
Using the Van der Pol Oscillator as a case study, we demonstrate faster
convergence compared to traditional training methods. Furthermore, we introduce
a decomposition framework utilizing Alternating Direction Method of Multipliers
(ADMM) to effectively coordinate sub-models among data batches. Our results
show significant potential for (collocation-based) simultaneous Neural ODE
training pipelines.","['cs.LG', 'math.OC']",False,,,,Screening rho-meson mass in the presence of strong magnetic fields,Training Neural ODEs Using Fully Discretized Simultaneous Optimization
neg-d2-192,2025-02-13,,2502.09522," Synchronizing words in classical automata theory provide a mechanism to reset
any state of a deterministic automaton to a specific target state via a
carefully chosen finite sequence of transition rules. In this work, we extend
the concept of synchronizing words to quantum information theory. Specifically,
we show that with only two quantum channels, it is possible to bring an
arbitrary qutrit state close to a designated target state. Furthermore, we
demonstrate that following this reset, any pure real qutrit state can be
closely approximated using the same two channels. These findings establish a
quantum analogue of synchronizing words, highlighting their potential
applications in constructing minimal sets of universal quantum gates capable of
both resetting and preparing arbitrary states.",['quant-ph'],2501.02019," Modeling the associations between real world entities from their multivariate
cross-sectional profiles can provide cues into the concerted working of these
entities as a system. Several techniques have been proposed for deciphering
these associations including constraint-based Bayesian structure learning (BSL)
algorithms that model them as directed acyclic graphs. Benchmarking these
algorithms have typically focused on assessing the variation in performance
measures such as sensitivity as a function of the dimensionality represented by
the number of nodes in the DAG, and sample size. The present study elucidates
the importance of network topology in benchmarking exercises. More
specifically, it investigates variations in sensitivity across distinct network
topologies while constraining the nodes, edges, and sample-size to be
identical, eliminating these as potential confounders. Sensitivity of three
popular constraint-based BSL algorithms (Peter-Clarke, Grow-Shrink, Incremental
Association Markov Blanket) in learning the network structure from multivariate
cross-sectional profiles sampled from network models with sub-linear, linear,
and super-linear DAG topologies generated using preferential attachment is
investigated. Results across linear and nonlinear models revealed statistically
significant $(\alpha=0.05)$ decrease in sensitivity estimates from sub-linear
to super-linear topology constitutively across the three algorithms. These
results are demonstrated on networks with nodes $(N_{nods}=48,64)$, noise
strengths $(\sigma =3,6)$ and sample size $(N = 2^{10})$. The findings
elucidate the importance of accommodating the network topology in
constraint-based BSL benchmarking exercises.","['cs.LG', 'cs.AI', 'q-bio.MN']",False,,,,Quantum Synchronizing Words: Resetting and Preparing Qutrit States,"Benchmarking Constraint-Based Bayesian Structure Learning Algorithms:
  Role of Network Topology"
neg-d2-193,2025-03-16,,2503.12751," We present R3-Avatar, incorporating a temporal codebook, to overcome the
inability of human avatars to be both animatable and of high-fidelity rendering
quality. Existing video-based reconstruction of 3D human avatars either focuses
solely on rendering, lacking animation support, or learns a pose-appearance
mapping for animating, which degrades under limited training poses or complex
clothing. In this paper, we adopt a ""record-retrieve-reconstruct"" strategy that
ensures high-quality rendering from novel views while mitigating degradation in
novel poses. Specifically, disambiguating timestamps record temporal appearance
variations in a codebook, ensuring high-fidelity novel-view rendering, while
novel poses retrieve corresponding timestamps by matching the most similar
training poses for augmented appearance. Our R3-Avatar outperforms cutting-edge
video-based human avatar reconstruction, particularly in overcoming visual
quality degradation in extreme scenarios with limited training human poses and
complex clothing.",['cs.CV'],2501.06785," Understanding objects in 3D at the part level is essential for humans and
robots to navigate and interact with the environment. Current datasets for
part-level 3D object understanding encompass a limited range of categories. For
instance, the ShapeNet-Part and PartNet datasets only include 16, and 24 object
categories respectively. The 3DCoMPaT dataset, specifically designed for
compositional understanding of parts and materials, contains only 42 object
categories. To foster richer and fine-grained part-level 3D understanding, we
introduce 3DCoMPaT200, a large-scale dataset tailored for compositional
understanding of object parts and materials, with 200 object categories with
$\approx$5 times larger object vocabulary compared to 3DCoMPaT and $\approx$ 4
times larger part categories. Concretely, 3DCoMPaT200 significantly expands
upon 3DCoMPaT, featuring 1,031 fine-grained part categories and 293 distinct
material classes for compositional application to 3D object parts.
Additionally, to address the complexities of compositional 3D modeling, we
propose a novel task of Compositional Part Shape Retrieval using ULIP to
provide a strong 3D foundational model for 3D Compositional Understanding. This
method evaluates the model shape retrieval performance given one, three, or six
parts described in text format. These results show that the model's performance
improves with an increasing number of style compositions, highlighting the
critical role of the compositional dataset. Such results underscore the
dataset's effectiveness in enhancing models' capability to understand complex
3D shapes from a compositional perspective. Code and Data can be found at
http://github.com/3DCoMPaT200/3DCoMPaT200","['cs.CV', 'cs.CL']",False,,,,"R3-Avatar: Record and Retrieve Temporal Codebook for Reconstructing
  Photorealistic Human Avatars","3DCoMPaT200: Language-Grounded Compositional Understanding of Parts and
  Materials of 3D Shapes"
neg-d2-194,2025-03-12,,2503.09765," Market fragmentation across multiple Automated Market Makers (AMMs) creates
inefficiencies such as costly arbitrage, unnecessarily high slippage and
delayed incorporation of new information into prices. These inefficiencies
raise trading costs, reduce liquidity provider profits, and degrade overall
market efficiency. To address these issues, we propose a modification of the
Constant Product Market Maker (CPMM) pricing mechanism, called the Global
Market Maker (GMM), which aggregates liquidity information from all AMMs to
mitigate these inefficiencies. Through theoretical and numerical analyses, we
demonstrate that the GMM enhances profits for both AMMs and traders by
eliminating arbitrage opportunities. Additionally, it reduces the profitability
of sandwich attacks and minimizes impermanent losses.","['econ.GN', 'q-fin.EC']",2502.09016," We study the collective modes of an atomic bright soliton realised in a
quasi-one-dimensional Bose-Einstein condensate, using Bogoliubov-de Gennes
theory. In particular we focus on the breathing mode of the soliton, which is
not a single linearized normal mode but a common component of many modes, and
therefore decays within a $t^{-1/2}$ envelope due to dispersion. If the soliton
is held in the center of a harmonic trap, we show that the breathing amplitude
revives periodically, as atoms shed from the vibrating soliton oscillate in the
trap, and return. After each revival the breathing amplitude again decays, and
this cycle repeats every trap half-period. The amplitude envelope of these
breathing revivals shows a curious asymmetry, however, with a gradual increase
in breathing followed by sudden drop in breathing amplitude that becomes more
and more pronounced in later revivals. We explain this asymmetrical revival
pattern by deriving a close analytical approximation to the Bogoliubov-de
Gennes frequency spectrum, and offer this coherent Bogoliubov-de Gennes
phenomenon as a background against which to compare possible quantum many-body
effects, including decoherence over trap-period time scales.","['cond-mat.quant-gas', 'quant-ph']",False,,,,Pooling Liquidity Pools in AMMs,"Soliton resuscitations: asymmetric revivals of the breathing mode of an
  atomic bright soliton in a harmonic trap"
neg-d2-195,2025-03-24,,2503.18399," We introduce the Turbulent Transport in Tokamaks via Stochastic Trajectories
(T3ST) code, designed to address the problem of turbulent transport using a
statistical approach complementary to gyrokinetics. The code employs
test-particle methods to track the dynamics of charged particles in
axisymmetric magnetic equilibria, accounting for both turbulence and Coulomb
collisions. The turbulence is decoupled from plasma dynamics and represented
through a statistical ensemble of synthetic random fields with specified
spectral properties. This approach enables T3ST to compute transport
coefficients as Lagrangian correlations - orders of magnitude faster than
gyrokinetic codes.",['physics.plasm-ph'],2502.13506," Negation is a fundamental aspect of human communication, yet it remains a
challenge for Language Models (LMs) in Information Retrieval (IR). Despite the
heavy reliance of modern neural IR systems on LMs, little attention has been
given to their handling of negation. In this study, we reproduce and extend the
findings of NevIR, a benchmark study that revealed most IR models perform at or
below the level of random ranking when dealing with negation. We replicate
NevIR's original experiments and evaluate newly developed state-of-the-art IR
models. Our findings show that a recently emerging category - listwise Large
Language Model (LLM) rerankers - outperforms other models but still
underperforms human performance. Additionally, we leverage ExcluIR, a benchmark
dataset designed for exclusionary queries with extensive negation, to assess
the generalizability of negation understanding. Our findings suggest that
fine-tuning on one dataset does not reliably improve performance on the other,
indicating notable differences in their data distributions. Furthermore, we
observe that only cross-encoders and listwise LLM rerankers achieve reasonable
performance across both negation tasks.",['cs.IR'],False,,,,T3ST code: Turbulent Transport in Tokamaks via Stochastic Trajectories,Reproducing NevIR: Negation in Neural Information Retrieval
neg-d2-196,2025-03-12,,2503.09533," Designing strategyproof mechanisms for multi-facility location that optimize
social costs based on agent preferences had been challenging due to the
extensive domain knowledge required and poor worst-case guarantees. Recently,
deep learning models have been proposed as alternatives. However, these models
require some domain knowledge and extensive hyperparameter tuning as well as
lacking interpretability, which is crucial in practice when transparency of the
learned mechanisms is mandatory. In this paper, we introduce a novel approach,
named LLMMech, that addresses these limitations by incorporating large language
models (LLMs) into an evolutionary framework for generating interpretable,
hyperparameter-free, empirically strategyproof, and nearly optimal mechanisms.
Our experimental results, evaluated on various problem settings where the
social cost is arbitrarily weighted across agents and the agent preferences may
not be uniformly distributed, demonstrate that the LLM-generated mechanisms
generally outperform existing handcrafted baselines and deep learning models.
Furthermore, the mechanisms exhibit impressive generalizability to
out-of-distribution agent preferences and to larger instances with more agents.",['cs.LG'],2502.20382," We present a low-cost data generation pipeline that integrates physics-based
simulation, human demonstrations, and model-based planning to efficiently
generate large-scale, high-quality datasets for contact-rich robotic
manipulation tasks. Starting with a small number of embodiment-flexible human
demonstrations collected in a virtual reality simulation environment, the
pipeline refines these demonstrations using optimization-based kinematic
retargeting and trajectory optimization to adapt them across various robot
embodiments and physical parameters. This process yields a diverse, physically
consistent dataset that enables cross-embodiment data transfer, and offers the
potential to reuse legacy datasets collected under different hardware
configurations or physical parameters. We validate the pipeline's effectiveness
by training diffusion policies from the generated datasets for challenging
contact-rich manipulation tasks across multiple robot embodiments, including a
floating Allegro hand and bimanual robot arms. The trained policies are
deployed zero-shot on hardware for bimanual iiwa arms, achieving high success
rates with minimal human input. Project website:
https://lujieyang.github.io/physicsgen/.","['cs.RO', 'cs.AI', 'cs.LG', 'cs.SY', 'eess.SY']",False,,,,Large Language Models for Multi-Facility Location Mechanism Design,"Physics-Driven Data Generation for Contact-Rich Manipulation via
  Trajectory Optimization"
neg-d2-197,2025-02-25,,2502.18295," In this note, we provide a proof of the existence and complete classification
of $G$-invariant star products with quantum momentum maps on Poisson manifolds
by means of an equivariant version of the formality theorem.","['math.QA', 'math-ph', 'math.MP', 'math.SG']",2502.09528," Machine learning algorithms have enabled high quality stereo depth estimation
to run on Augmented and Virtual Reality (AR/VR) devices. However, high energy
consumption across the full image processing stack prevents stereo depth
algorithms from running effectively on battery-limited devices. This paper
introduces SteROI-D, a full stereo depth system paired with a mapping
methodology. SteROI-D exploits Region-of-Interest (ROI) and temporal sparsity
at the system level to save energy. SteROI-D's flexible and heterogeneous
compute fabric supports diverse ROIs. Importantly, we introduce a systematic
mapping methodology to effectively handle dynamic ROIs, thereby maximizing
energy savings. Using these techniques, our 28nm prototype SteROI-D design
achieves up to 4.35x reduction in total system energy compared to a baseline
ASIC.","['cs.CV', 'cs.AR']",False,,,,Quantization of the Momentum Map via $\frak{g}$-adapted Formalities,"SteROI-D: System Design and Mapping for Stereo Depth Inference on
  Regions of Interest"
neg-d2-198,2025-01-07,,2501.03859," In this paper, we present a novel synergistic framework for learning shape
estimation and a shape-aware whole-body control policy for tendon-driven
continuum robots. Our approach leverages the interaction between two Augmented
Neural Ordinary Differential Equations (ANODEs) -- the Shape-NODE and
Control-NODE -- to achieve continuous shape estimation and shape-aware control.
The Shape-NODE integrates prior knowledge from Cosserat rod theory, allowing it
to adapt and account for model mismatches, while the Control-NODE uses this
shape information to optimize a whole-body control policy, trained in a Model
Predictive Control (MPC) fashion. This unified framework effectively overcomes
limitations of existing data-driven methods, such as poor shape awareness and
challenges in capturing complex nonlinear dynamics. Extensive evaluations in
both simulation and real-world environments demonstrate the framework's robust
performance in shape estimation, trajectory tracking, and obstacle avoidance.
The proposed method consistently outperforms state-of-the-art end-to-end,
Neural-ODE, and Recurrent Neural Network (RNN) models, particularly in terms of
tracking accuracy and generalization capabilities.",['cs.RO'],2503.16237," Next-generation telescopes will bring groundbreaking discoveries but they
will also present new technological challenges. The Square Kilometre Array
Observatory (SKAO) will be one of the most demanding scientific
infrastructures, with a projected data output of 700 PB per year to be
distributed to a network of SKA Regional Centres. Current tools are not fully
suited to manage such massive data volumes, therefore, new research is required
to transform science archives from data providers into service providers. In
this paper we examine how a science archive can deliver advanced visualisation
capabilities for the SKA science archive. In particular, we have conducted a
thorough exploration of existing visualisation software for astronomy and other
fields to identify tools capable of addressing Big Data requirements. Using
selected technologies, we have developed a prototype archive that provides
access to interactive visualisations of 3D radio data through web-based
interfaces, adhering to International Virtual Observatory Alliance (IVOA)
recommendations to favour interoperability and Open Science practices. In
addition, we discuss how current IVOA recommendations support these
visualisation capabilities and how they could be expanded. Our prototype
archive includes a service to generate 3D models on the fly as a server
operation, enabling remote visualisations in a flexible manner; for instance, a
set of parameters can be used to customise the models and their visualisation.
We have used SKA precursor and pathfinder data to test its usability and
scalability, concluding that remote visualisation is a viable solution for
handling high-volume data. However, our prototype is constrained by memory
limitations, requiring techniques to reduce memory usage.","['astro-ph.IM', 'astro-ph.GA']",False,,,,"A Synergistic Framework for Learning Shape Estimation and Shape-Aware
  Whole-Body Control Policy for Continuum Robots","3D radio data visualisation in open science platforms for
  next-generation observatories"
neg-d2-199,2025-03-20,,2503.16389," Retinal Optical Coherence Tomography (OCT) segmentation is essential for
diagnosing pathology. Traditional methods focus on either spatial or spectral
domains, overlooking their combined dependencies. We propose a triple-encoder
network that integrates CNNs for spatial features, Fast Fourier Convolution
(FFC) for spectral features, and attention mechanisms to capture global
relationships across both domains. Attention fusion modules integrate
convolution and cross-attention to further enhance features. Our method
achieves an average Dice score improvement from 0.855 to 0.864, outperforming
prior work.","['eess.IV', 'cs.AI', 'cs.CV']",2502.09528," Machine learning algorithms have enabled high quality stereo depth estimation
to run on Augmented and Virtual Reality (AR/VR) devices. However, high energy
consumption across the full image processing stack prevents stereo depth
algorithms from running effectively on battery-limited devices. This paper
introduces SteROI-D, a full stereo depth system paired with a mapping
methodology. SteROI-D exploits Region-of-Interest (ROI) and temporal sparsity
at the system level to save energy. SteROI-D's flexible and heterogeneous
compute fabric supports diverse ROIs. Importantly, we introduce a systematic
mapping methodology to effectively handle dynamic ROIs, thereby maximizing
energy savings. Using these techniques, our 28nm prototype SteROI-D design
achieves up to 4.35x reduction in total system energy compared to a baseline
ASIC.","['cs.CV', 'cs.AR']",False,,,,"Attentional Triple-Encoder Network in Spatiospectral Domains for Medical
  Image Segmentation","SteROI-D: System Design and Mapping for Stereo Depth Inference on
  Regions of Interest"
neg-d2-200,2025-03-04,,2503.02496," This paper addresses the trade-off between internalisation and
externalisation in the management of stochastic trade flows. We consider agents
who must absorb flows and manage risk by deciding whether to warehouse it or
hedge in the market, thereby incurring transaction costs and market impact.
Unlike market makers, these agents cannot skew their quotes to attract
offsetting flows and deter risk-increasing ones, leading to a fundamentally
different problem. Within the Almgren-Chriss framework, we derive
almost-closed-form solutions in the case of quadratic execution costs, while
more general cases require numerical methods. In particular, we discuss the
challenges posed by artificial boundary conditions when using classical
grid-based numerical PDE techniques and propose reinforcement learning methods
as an alternative.",['q-fin.TR'],2503.17201," The commodity and widespread use of online shopping are having an
unprecedented impact on climate, with emission figures from key actors that are
easily comparable to those of a large-scale metropolis. Despite online shopping
being fueled by recommender systems (RecSys) algorithms, the role and potential
of the latter in promoting more sustainable choices is little studied. One of
the main reasons for this could be attributed to the lack of a dataset
containing carbon footprint emissions for the items. While building such a
dataset is a rather challenging task, its presence is pivotal for opening the
doors to novel perspectives, evaluations, and methods for RecSys research. In
this paper, we target this bottleneck and study the environmental role of
RecSys algorithms. First, we mine a dataset that includes carbon footprint
emissions for its items. Then, we benchmark conventional RecSys algorithms in
terms of accuracy and sustainability as two faces of the same coin. We find
that RecSys algorithms optimized for accuracy overlook greenness and that
longer recommendation lists are greener but less accurate. Then, we show that a
simple reranking approach that accounts for the item's carbon footprint can
establish a better trade-off between accuracy and greenness. This reranking
approach is modular, ready to use, and can be applied to any RecSys algorithm
without the need to alter the underlying mechanisms or retrain models. Our
results show that a small sacrifice of accuracy can lead to significant
improvements of recommendation greenness across all algorithms and list
lengths. Arguably, this accuracy-greenness trade-off could even be seen as an
enhancement of user satisfaction, particularly for purpose-driven users who
prioritize the environmental impact of their choices. We anticipate this work
will serve as the starting point for studying RecSys for more sustainable
recommendations.",['cs.IR'],False,,,,"To Hedge or Not to Hedge: Optimal Strategies for Stochastic Trade Flow
  Management","Towards Carbon Footprint-Aware Recommender Systems for Greener Item
  Recommendation"
neg-d2-201,2025-02-12,,2502.08949," Self-supervised graph representation learning has driven significant
advancements in domains such as social network analysis, molecular design, and
electronics design automation (EDA). However, prior works in EDA have mainly
focused on the representation of gate-level digital circuits, failing to
capture analog and mixed-signal circuits. To address this gap, we introduce
DICE: Device-level Integrated Circuits Encoder, the first self-supervised
pretrained graph neural network (GNN) model for any circuit expressed at the
device level. DICE is a message-passing neural network (MPNN) trained through
graph contrastive learning, and its pretraining process is simulation-free,
incorporating two novel data augmentation techniques. Experimental results
demonstrate that DICE achieves substantial performance gains across three
downstream tasks, underscoring its effectiveness for both analog and digital
circuits.",['cs.LG'],2502.10411," Personalised education is one of the domains that can greatly benefit from
the most recent advances in Artificial Intelligence (AI) and Large Language
Models (LLM). However, it is also one of the most challenging applications due
to the cognitive complexity of teaching effectively while personalising the
learning experience to suit independent learners. We hypothesise that one
promising approach to excelling in such demanding use cases is using a
\emph{society of minds}. In this chapter, we present TrueReason, an exemplar
personalised learning system that integrates a multitude of specialised AI
models that can mimic micro skills that are composed together by a LLM to
operationalise planning and reasoning. The architecture of the initial
prototype is presented while describing two micro skills that have been
incorporated in the prototype. The proposed system demonstrates the first step
in building sophisticated AI systems that can take up very complex cognitive
tasks that are demanded by domains such as education.","['cs.CY', 'cs.AI', 'cs.CL', 'cs.IR', 'cs.MA']",False,,,,"Self-Supervised Graph Contrastive Pretraining for Device-level
  Integrated Circuits","TrueReason: An Exemplar Personalised Learning System Integrating
  Reasoning with Foundational Models"
neg-d2-202,2025-02-26,,2502.1972," We study the performance of the linear consensus algorithm on strongly
connected graphs using the linear quadratic (LQ) cost as a performance measure.
  In particular, we derive bounds on the LQ cost by leveraging effective
resistance. Our results extend previous analyses -- which were limited to
reversible cases -- to the nonreversible setting. To facilitate this
generalization, we introduce novel concepts, termed the back-and-forth path and
the pivot node, which serve as effective alternatives to traditional techniques
that require reversibility. Moreover, we apply our approach to geometric graphs
to estimate the LQ cost without the reversibility assumption. The proposed
approach provides a framework that can be adapted to other contexts where
reversibility is typically assumed.","['math.OC', 'cs.MA']",2501.12915," In this paper, we treat minimal left-invariant unit vector fields on
oscillator group and their relations with the ones that define a harmonic map.
Particularly, if all structure constants of the oscillator group are equal to
each other, then all unit left invariant vector fields that define a harmonic
map into the unit tangent bundle with Sasaki metric are minimal.",['math.DG'],False,,,,"Analysis of Linear Consensus Algorithm on Strongly Connected Graph Using
  Effective Resistance",Minimal unit vector fields on oscillator groups
neg-d2-203,2025-01-07,,2501.03705," We investigate correlation time numerically in extremal self-organized
critical models, namely, the Bak-Sneppen evolution and the Robin Hood dynamics.
The (fitness) correlation time is the duration required for the extinction or
mutation of species over the entire spatial region in the critical state. We
apply the methods of finite-size scaling and extreme value theory to understand
the statistics of the correlation time. We find power-law system size scaling
behaviors for the mean, the variance, the mode, and the peak probability of the
correlation time. We obtain data collapse for the correlation time cumulative
probability distribution, and the scaling function follows the generalized
extreme value density close to the Gumbel function.",['cond-mat.stat-mech'],2501.08884," Scenario decision making offers a flexible way of making decision in an
uncertain environment while obtaining probabilistic guarantees on the risk of
failure of the decision. The idea of this approach is to draw samples of the
uncertainty and make a decision based on the samples, called ""scenarios"". The
probabilistic guarantees take the form of a bound on the probability of
sampling a set of scenarios that will lead to a decision whose risk of failure
is above a given maximum tolerance. This bound can be expressed as a function
of the number of sampled scenarios, the maximum tolerated risk, and some
intrinsic property of the problem called the ""compression size"". Several such
bounds have been proposed in the literature under various assumptions on the
problem. We propose new bounds that improve upon the existing ones without
requiring stronger assumptions on the problem.","['math.OC', 'cs.LG']",False,,,,Correlation time in extremal self-organized critical models,Improved Compression Bounds for Scenario Decision Making
neg-d2-204,2025-01-13,,2501.07515," Evolutionary and bioinspired computation are crucial for efficiently
addressing complex optimization problems across diverse application domains. By
mimicking processes observed in nature, like evolution itself, these algorithms
offer innovative solutions beyond the reach of traditional optimization
methods. They excel at finding near-optimal solutions in large, complex search
spaces, making them invaluable in numerous fields. However, both areas are
plagued by challenges at their core, including inadequate benchmarking,
problem-specific overfitting, insufficient theoretical grounding, and
superfluous proposals justified only by their biological metaphor. This
overview recapitulates and analyzes in depth the criticisms concerning the lack
of innovation and rigor in experimental studies within the field. To this end,
we examine the judgmental positions of the existing literature in an informed
attempt to guide the research community toward directions of solid contribution
and advancement in these areas. We summarize guidelines for the design of
evolutionary and bioinspired optimizers, the development of experimental
comparisons, and the derivation of novel proposals that take a step further in
the field. We provide a brief note on automating the process of creating these
algorithms, which may help align metaheuristic optimization research with its
primary objective (solving real-world problems), provided that our identified
pathways are followed. Our conclusions underscore the need for a sustained push
towards innovation and the enforcement of methodological rigor in prospective
studies to fully realize the potential of these advanced computational
techniques.","['cs.NE', 'cs.AI']",2502.03209," Domain walls formed during a phase transition in a simple field theory model
with $\mathbb{Z}_2$ symmetry in a periodic box have been demonstrated to
annihilate as fast as causality allows and their area density scales $\propto
t^{-1}$. We have performed numerical simulations of the dynamics of domain
walls in the Two-Higgs Doublet Model (2HDM) where the potential has
$\mathbb{Z}_2$ symmetry in two spatial dimensions. We observed significant
differences with the standard case. Although the extreme long-time limit is the
same for the $\approx 10^{5}$ sets of random initial configurations analysed,
the percolation process is much slower due to the formation of long-lived
loops. We suggest that this is due to the build up of superconducting currents
on the walls which could lead ultimately to stationary configurations known as
Kinky Vortons. We discuss the relevance of these findings for the production of
Vortons in three spatial dimensions.","['hep-ph', 'astro-ph.CO', 'hep-th']",False,,,,"The Paradox of Success in Evolutionary and Bioinspired Optimization:
  Revisiting Critical Issues, Key Studies, and Methodological Pathways",Percolation of Domain Walls in the Two-Higgs Doublet Model
neg-d2-205,2025-03-10,,2503.0715," We propose a novel approach to the analysis of programmable geometrically
exact shear deformable beam systems made of shape memory polymers. The proposed
method combines the viscoelastic Generalized Maxwell model with the Williams,
Landel and Ferry relaxation principle, enabling the reproduction of the shape
memory effect of structural systems featuring complex geometry and topology.
Very high efficiency is pursued by discretizing the differential problem in
space through the isogeometric collocation (IGA-C) method. The method, in
addition to the desirable attributes of isogeometric analysis (IGA), such as
exactness of the geometric reconstruction of complex shapes and high-order
accuracy, circumvents the need for numerical integration since it discretizes
the problem in the strong form. Other distinguishing features of the proposed
formulation are: i) ${\rm SO}(3)$-consistency for the linearization of the
problem and for the time stepping; ii) minimal (finite) rotation
parametrization, that means only three rotational unknowns are used; iii) no
additional unknowns are needed to account for the rate-dependent material
compared to the purely elastic case. Through different numerical applications
involving challenging initial geometries, we show that the proposed formulation
possesses all the sought attributes in terms of programmability of complex
systems, geometric flexibility, and high order accuracy.","['cs.CE', 'cs.NA', 'math.NA']",2501.05555," Detecting object-level changes between two images across possibly different
views is a core task in many applications that involve visual inspection or
camera surveillance. Existing change-detection approaches suffer from three
major limitations: (1) lack of evaluation on image pairs that contain no
changes, leading to unreported false positive rates; (2) lack of
correspondences (i.e., localizing the regions before and after a change); and
(3) poor zero-shot generalization across different domains. To address these
issues, we introduce a novel method that leverages change correspondences (a)
during training to improve change detection accuracy, and (b) at test time, to
minimize false positives. That is, we harness the supervision labels of where
an object is added or removed to supervise change detectors, improving their
accuracy over previous work by a large margin. Our work is also the first to
predict correspondences between pairs of detected changes using estimated
homography and the Hungarian algorithm. Our model demonstrates superior
performance over existing methods, achieving state-of-the-art results in change
detection and change correspondence accuracy across both in-distribution and
zero-shot benchmarks.","['cs.CV', 'cs.AI']",False,,,,"Simulating programmable morphing of shape memory polymer beam systems
  with complex geometry and topology","Improving Zero-Shot Object-Level Change Detection by Incorporating
  Visual Correspondence"
neg-d2-206,2025-02-04,,2502.02267," Given a metric measure space $(\mathcal{X}, d, \mu)$ satisfying the volume
doubling condition, we consider a semigroup $\{S_t\}$ and the associated heat
operator. We propose general conditions on the heat kernel so that the
solutions of the associated heat equations attain the initial data pointwise.
We demonstrate that these conditions are satisfied by a broad class of
operators, including the Laplace operators perturbed by a gradient, fractional
Laplacian, mixed local-nonlocal operators, Laplacian on Riemannian manifolds,
Dunkl Laplacian and many more. In addition, we consider the Laplace operator in
$\mathbb{R}^n$ with the Hardy potential and establish a characterization for
the pointwise convergence to the initial data. We also prove similar results
for the nonhomogeneous equations and showcase an application for the power-type
nonlinearities.","['math.AP', 'math.FA']",2503.10174," This paper presents a novel sensitivity-based distributed programming (SBDP)
approach for non-convex, large-scale nonlinear programs (NLP). The algorithm
relies on first-order sensitivities to cooperatively solve the central NLP in a
distributed manner with only neighbor-to-neighbor communication and
parallelizable local computations. The scheme is based on primal decomposition
and offers minimal algorithmic complexity. We derive sufficient local
convergence conditions for non-convex problems. Furthermore, we consider the
SBDP method in a distributed optimal control context and derive favorable
convergence properties in this setting. We illustrate these theoretical
findings and the performance of the proposed algorithm with simulations of
various distributed optimization and control problems.",['math.OC'],False,,,,"A unified framework for pointwise convergence to the initial data of
  heat equations in metric measure spaces",Sensitivity-Based Distributed Programming for Non-Convex Optimization
neg-d2-207,2025-01-08,,2501.04949," Zinc oxide (ZnO) has garnered much attention as a promising material for
quantum devices due to its unique characteristics. To utilize the potential of
ZnO for quantum devices, the development of fundamental technological elements
such as high-speed readout and charge sensing capabilities has become
essential. In this study, we address these challenges by demonstrating
radio-frequency (rf) reflectometry and charge sensing in ZnO quantum dots, thus
advancing the potential for qubit applications. A device is fabricated on a
high-quality ZnO heterostructure, featuring gate-defined target and sensor
quantum dots. The sensor dot, integrated into an rf resonator circuit, enables
the detection of single-electron charges in the target dots. Using this setup,
the formation of few-electron double quantum dots is observed by obtaining
their charge stability diagram. Also, a charge stability diagram with a gate
pulse sequence is measured. We discuss the strong electron correlation in ZnO,
which leads to nearly degenerate spin-singlet and -triplet two-electron states
in the (0, 2) charge state, and the perspectives on spin-state readout.",['cond-mat.mes-hall'],2502.17425," To utilize visual information, Multimodal Large Language Model (MLLM) relies
on the perception process of its vision encoder. The completeness and accuracy
of visual perception significantly influence the precision of spatial
reasoning, fine-grained understanding, and other tasks. However, MLLM still
lacks the autonomous capability to control its own visual perception processes,
for example, selectively reviewing specific regions of an image or focusing on
information related to specific object categories. In this work, we propose the
concept of Visual Perception Token, aiming to empower MLLM with a mechanism to
control its visual perception processes. We design two types of Visual
Perception Tokens, termed the Region Selection Token and the Vision Re-Encoding
Token. MLLMs autonomously generate these tokens, just as they generate text,
and use them to trigger additional visual perception actions. The Region
Selection Token explicitly identifies specific regions in an image that require
further perception, while the Vision Re-Encoding Token uses its hidden states
as control signals to guide additional visual perception processes. Extensive
experiments demonstrate the advantages of these tokens in handling spatial
reasoning, improving fine-grained understanding, and other tasks. On average,
the introduction of Visual Perception Tokens improves the performance of a 2B
model by 23.6\%, increasing its score from 0.572 to 0.708, and even outperforms
a 7B parameter model by 13.4\% (from 0.624). Please check out our repo
https://github.com/yu-rp/VisualPerceptionToken","['cs.CV', 'cs.LG']",False,,,,"Charge sensing of few-electron ZnO double quantum dots probed by
  radio-frequency reflectometry",Introducing Visual Perception Token into Multimodal Large Language Model
neg-d2-208,2025-01-17,,2501.10221," We model human activity scheduling behaviour using a deep generative machine
learning approach. Activity schedules, which represent the activities and
associated travel behaviours of individuals, are a core component of many
applied models in the transport, energy and epidemiology domains. Our data
driven approach learns human preferences and scheduling logic without the need
for complex interacting combinations of sub-models and custom-rules, this makes
our approach significantly faster and simpler to operate that existing
approaches. We find activity schedule data combines aspects of both continuous
image data and also discrete text data, requiring novel approaches. We
additionally contribute a novel schedule representation and comprehensive
evaluation framework for generated schedules. Evaluation shows our approach is
able to rapidly generate large, diverse and realistic synthetic samples of
activity schedules.",['cs.LG'],2502.13005," Characterizing bipolaron binding, and understanding how it depends on
electron-phonon interaction, is crucial to unraveling the nature of emergent
many-body states in strongly interacting electron-phonon systems. So far, most
studies of bipolarons have been limited to the Holstein model, in which the
coupling constant is momentum-independent. The paradigmatic example of
momentum-dependent electron-phonon interaction comes from the system in which
phonon distortions modify electron hopping, the SSH model. Already individual
polarons in the SSH model are richer than the Holstein model counterparts, and
feature a phase transition into the finite momentum ground state with
increasing electron-phonon interaction. In this paper, we use a variational
approach to study bipolarons in the one-dimensional SSH model and discuss their
ground state, dispersion, and excitation spectra. We explore the full parameter
range of the system, including the adiabatic regime of slow phonons, which was
inaccessible to previous theoretical studies. In agreement with earlier
studies, we find that in the anti-adiabatic strongly interacting regime,
bipolarons have low effective mass. By contrast, in the adiabatic case, we find
that increasing electron-phonon interactions results in an exponential increase
of the bipolaron mass. We establish the existence of multiple branches of bound
excited states of SSH bipolaron and discuss the signatures of these bound
states in dynamics. We show that in the anti-adiabatic regime, response
functions obey a parity selection rule, that imposes symmetry constraints on
the excitation spectra and provides a clear signature of SSH bipolarons.",['cond-mat.str-el'],False,,,,"Modelling Activity Scheduling Behaviour with Deep Generative Machine
  Learning",Bipolaron dynamics in the one-dimensional SSH model
neg-d2-209,2025-01-08,,2501.10417," In this paper we introduce the generalized inverse of complex square matrix
with respect to other matrix having same size. Some of its representations,
properties and characterizations are obtained. Also some new representation
matrices of W-weighted BT-inverse and W-weighted core-EP inverse are determined
as well as characterizations of generalized inverses A A^\odagger,
A^{odagger,W}, A^\diamond, A^{\diamond,W}.","['math.RA', 'math.FA']",2501.17484," We present a method for solving a large-scale stochastic capacity expansion
problem which explicitly considers reliability constraints, in particular
constraints on expected energy not served. Our method tackles this problem by a
Lagrange relaxation of the expected energy not served constraints. We solve the
relaxed formulation in an iterative manner, using a subgradient-based method.
Each iteration requires the solution of a stochastic capacity expansion
problem, for which we implement a subgradient decomposition scheme in a
high-performance computing infrastructure. We apply the proposed methodology on
the Economic Viability Assessment model that is used by ENTSO-E in the annual
European Resource Adequacy Assessment, extended to include explicit reliability
constraints. The approach is able to solve this model achieving a 1.3%
optimality gap. We compare our approach against accounting for reliability
through penalizing load shedding at VOLL, and find that the former results in
1.6% savings in total cost. We are also able to quantify the cost savings from
allowing some load curtailment in the capacity planning process, which ranges
from 1.6 to 6% in the cases analyzed.","['eess.SY', 'cs.SY']",False,,,,Simultaneous extension of generalized BT-inverses and core-EP inverses,"Capacity Expansion Planning under Uncertainty subject to Expected Energy
  Not Served Constraints"
neg-d2-210,2025-02-13,,2502.09507," The remarkable generalization performance of contrastive vision-language
models like CLIP is often attributed to the diversity of their training
distributions. However, key questions remain unanswered: Can CLIP generalize to
an entirely unseen domain when trained on a diverse mixture of domains (domain
generalization)? Can it generalize to unseen classes within partially seen
domains (compositional generalization)? What factors affect such
generalization? To answer these questions, we trained CLIP models on
systematically constructed training distributions with controlled domain
diversity and object class exposure. Our experiments show that domain diversity
is essential for both domain and compositional generalization, yet
compositional generalization can be surprisingly weaker than domain
generalization when the training distribution contains a suboptimal subset of
the test domain. Through data-centric and mechanistic analyses, we find that
successful generalization requires learning of shared representations already
in intermediate layers and shared circuitry.","['cs.LG', 'cs.CV']",2501.16196," Going beyond short-range interactions, we explore the role of long-range
interactions in the extended XY model for transferring quantum states through
evolution. In particular, employing a spin-1/2 chain with interactions decaying
as a power law, we demonstrate that long-range interactions significantly
enhance the efficiency of a quantum state transfer (QST) protocol, reducing the
minimum time required to achieve fidelity beyond the classical limit. Our study
identifies the long-range regime as providing an optimal balance between
interaction range and transfer efficiency, outperforming the protocol with the
short-range interacting model. Our detailed analysis reveals the impact of
system parameters, such as anisotropy, magnetic field strength, and
coordination number, on QST dynamics. Specifically, we find that intermediate
coordination numbers lead to a faster and more reliable state transfer, while
extreme values diminish performance. Further, we exhibit that the presence of
long-range interactions also improves the achievable fidelity, mitigating its
decline associated with increasing system-size.","['quant-ph', 'cond-mat.quant-gas', 'cond-mat.str-el']",False,,,,When and How Does CLIP Enable Domain and Compositional Generalization?,Expediting quantum state transfer through long-range extended XY model
neg-d2-211,2025-03-17,,2503.13154," We consider a metapopulation made up of $K$ demes, each containing $N$
individuals bearing a heritable quantitative trait. Demes are connected by
migration and undergo independent Moran processes with mutation and selection
based on trait values. Mutation and migration rates are tuned so that each deme
receives a migrant or a mutant in the same slow timescale and is thus
essentially monomorphic at all times for the trait (adaptive dynamics). In the
timescale of mutation/migration, the metapopulation can then be seen as a giant
spatial Moran model with size $K$ that we characterize. As $K\to \infty$ and
physical space becomes continuous, the empirical distribution of the trait
(over the physical and trait spaces) evolves deterministically according to an
integro-differential evolution equation. In this limit, the trait of every
migrant is drawn from this global distribution, so that conditional on its
initial state, traits from finitely many demes evolve independently
(propagation of chaos). Under mean-field dispersal, the value $X_t$ of the
trait at time $t$ and at any given location has a law denoted $\mu_t$ and a
jump kernel with two terms: a mutation-fixation term and a migration-fixation
term involving $\mu_{t-}$ (McKean-Vlasov equation). In the limit where
mutations have small effects and migration is further slowed down accordingly,
we obtain the convergence of $X$, in the new migration timescale, to the
solution of a stochastic differential equation which can be referred to as a
new canonical equation of adaptive dynamics. This equation includes an
advection term representing selection, a diffusive term due to genetic drift,
and a jump term, representing the effect of migration, to a state distributed
according to its own law.","['math.PR', 'q-bio.PE']",2501.04459," Despite widespread adoption of deep learning models to address a variety of
computer vision tasks, planetary science has yet to see extensive utilization
of such tools to address its unique problems. On Titan, the largest moon of
Saturn, tracking seasonal trends and weather patterns of clouds provides
crucial insights into one of the most complex climates in the Solar System, yet
much of the available image data are still analyzed in a conventional way. In
this work, we apply a Mask R-CNN trained via transfer learning to perform
instance segmentation of clouds in Titan images acquired by the Cassini
spacecraft - a previously unexplored approach to a big data problem in
planetary science. We demonstrate that an automated technique can provide
quantitative measures for clouds, such as areas and centroids, that may
otherwise be prohibitively time-intensive to produce by human mapping.
Furthermore, despite Titan specific challenges, our approach yields accuracy
comparable to contemporary cloud identification studies on Earth and other
worlds. We compare the efficiencies of human-driven versus algorithmic
approaches, showing that transfer learning provides speed-ups that may open new
horizons for data investigation for Titan. Moreover, we suggest that such
approaches have broad potential for application to similar problems in
planetary science where they are currently under-utilized. Future planned
missions to the planets and remote sensing initiatives for the Earth promise to
provide a deluge of image data in the coming years that will benefit strongly
from leveraging machine learning approaches to perform the analysis.","['astro-ph.IM', 'astro-ph.EP', 'cs.CV', 'eess.IV']",False,,,,"Evolution of a trait distributed over a large fragmented population:
  Propagation of chaos meets adaptive dynamics",Rapid Automated Mapping of Clouds on Titan With Instance Segmentation
neg-d2-212,2025-03-16,,2503.12698," Precision medicine in the quantitative management of chronic diseases and
oncology would be greatly improved if the Computed Tomography (CT) scan of any
patient could be segmented, parsed and analyzed in a precise and detailed way.
However, there is no such fully annotated CT dataset with all anatomies
delineated for training because of the exceptionally high manual cost, the need
for specialized clinical expertise, and the time required to finish the task.
To this end, we proposed a novel continual learning-driven CT model that can
segment complete anatomies presented using dozens of previously partially
labeled datasets, dynamically expanding its capacity to segment new ones
without compromising previously learned organ knowledge. Existing multi-dataset
approaches are not able to dynamically segment new anatomies without
catastrophic forgetting and would encounter optimization difficulty or
infeasibility when segmenting hundreds of anatomies across the whole range of
body regions. Our single unified CT segmentation model, CL-Net, can highly
accurately segment a clinically comprehensive set of 235 fine-grained
whole-body anatomies. Composed of a universal encoder, multiple optimized and
pruned decoders, CL-Net is developed using 13,952 CT scans from 20 public and
16 private high-quality partially labeled CT datasets of various vendors,
different contrast phases, and pathologies. Extensive evaluation demonstrates
that CL-Net consistently outperforms the upper limit of an ensemble of 36
specialist nnUNets trained per dataset with the complexity of 5% model size and
significantly surpasses the segmentation accuracy of recent leading Segment
Anything-style medical image foundation models by large margins. Our continual
learning-driven CL-Net model would lay a solid foundation to facilitate many
downstream tasks of oncology and chronic diseases using the most widely adopted
CT imaging.","['eess.IV', 'cs.CV']",2502.15009," Conversational query rewriting is crucial for effective conversational
search, yet traditional supervised methods require substantial labeled data,
which is scarce in low-resource settings. This paper introduces Prompt-Guided
In-Context Learning, a novel approach that leverages the in-context learning
capabilities of Large Language Models (LLMs) for few-shot conversational query
rewriting. Our method employs carefully designed prompts, incorporating task
descriptions, input/output format specifications, and a small set of
illustrative examples, to guide pre-trained LLMs to generate
context-independent queries without explicit fine-tuning. Extensive experiments
on benchmark datasets, TREC and Taskmaster-1, demonstrate that our approach
significantly outperforms strong baselines, including supervised models and
contrastive co-training methods, across various evaluation metrics such as
BLEU, ROUGE-L, Success Rate, and MRR. Ablation studies confirm the importance
of in-context examples, and human evaluations further validate the superior
fluency, relevance, and context utilization of our generated rewrites. The
results highlight the potential of prompt-guided in-context learning as an
efficient and effective paradigm for low-resource conversational query
rewriting, reducing the reliance on extensive labeled data and complex training
procedures.",['cs.CL'],False,,,,"A Continual Learning-driven Model for Accurate and Generalizable
  Segmentation of Clinically Comprehensive and Fine-grained Whole-body
  Anatomies in CT","Contextualizing Search Queries In-Context Learning for Conversational
  Rewriting with LLMs"
neg-d2-213,2025-02-17,,2502.12047," In communication theory, attacks like eavesdropping or jamming are typically
assumed to occur at the channel level, while communication parties are expected
to follow established protocols. But what happens if one of the parties turns
malicious? In this work, we investigate a compelling scenario: a
multiple-access channel with two transmitters and one receiver, where one
transmitter deviates from the protocol and acts dishonestly. To address this
challenge, we introduce the Byzantine multiple-access classical-quantum channel
and derive an achievable communication rate for this adversarial setting.","['cs.IT', 'math.IT', 'math.QA']",2502.19152," We investigate the entanglement properties of the Quantum Six-Vertex Model on
a cylinder, focusing on the Shannon-Renyi entropy in the limit of Renyi order
$n = \infty$. This entropy, calculated from the ground state amplitudes of the
equivalent XXZ spin-1/2 chain, allows us to determine the Renyi entanglement
entropy of the corresponding Rokhsar-Kivelson wavefunctions, which describe the
ground states of certain conformal quantum critical points. Our analysis
reveals a novel logarithmic correction to the expected entanglement scaling
when the system size is odd. This anomaly arises from the geometric frustration
of spin configurations imposed by periodic boundary conditions on odd-sized
chains. We demonstrate that the scaling prefactor of this logarithmic term is
directly related to the compactification radius of the low-energy bosonic field
theory description, or equivalently, the Luttinger parameter. Thus, this
correction provides a direct probe of the underlying Conformal Field Theory
(CFT) describing the critical point. Our findings highlight the crucial role of
system size parity in determining the entanglement properties of this model and
offer insights into the interplay between geometry, frustration, and
criticality.","['quant-ph', 'cond-mat.str-el']",False,,,,Quantum Byzantine Multiple Access Channels,Oddities in the Entanglement Scaling of the Quantum Six-Vertex Model
neg-d2-214,2025-03-08,,2503.06314," Understanding how renormalized quasiparticles emerge in strongly correlated
electron materials provides a challenge for both experiment and theory. It has
been predicted that distinctive spin and orbital screening mechanisms drive
this process in multiorbital materials with strong Coulomb and Hund's
interactions. Here, we provide the experimental evidence of both mechanisms
from angle-resolved photoemission spectroscopy on RbFe$_2$As$_2$. We observe
that the emergence of low-energy Fe 3$d_{xy}$ quasiparticles below 90K is tied
to spin screening. A second process changes the spectral weight at high
energies up to room temperature. Supported by theoretical calculations we
attribute it to orbital screening of Fe 3d atomic excitations. These two
cascading screening processes drive the temperature evolution from a bad metal
to a correlated Fermi liquid.",['cond-mat.str-el'],2501.06782," An edge-coloring of a graph $H$ is a function $\mathcal{C}: E(H) \rightarrow
\mathbb{N}$. We say that $H$ is rainbow if all edges of $H$ have different
colors. Given a graph $F$, an edge-colored graph $G$ is $F$-rainbow saturated
if $G$ does not contain a rainbow copy of $F$, but the addition of any nonedge
with any color on it would create a rainbow copy of $F$. The rainbow saturation
number $rsat(n,F)$ is the minimum number of edges in an $F$-rainbow saturated
graph with order $n$. In this paper we proved several results on cycle rainbow
saturation. For $n \geq 5$, we determined the exact value of $rsat(n,C_4)$. For
$ n \geq 15$, we proved that $\frac{3}{2}n-\frac{5}{2} \leq rsat(n,C_{5}) \leq
2n-6$. For $r \geq 6$ and $n \geq r+3$, we showed that $ \frac{6}{5}n \leq
rsat(n,C_r) \leq 2n+O(r^2)$. Moreover, we establish better lower bound on
$C_r$-rainbow saturated graph $G$ while $G$ is rainbow.",['math.CO'],False,,,,"Observation of Two Cascading Screening Processes in an Iron-based
  Superconductor",The Rainbow Saturation Number of Cycles
neg-d2-215,2025-01-13,,2501.07358," We propose a novel deep clustering method that integrates Variational
Autoencoders (VAEs) into the Expectation-Maximization (EM) framework. Our
approach models the probability distribution of each cluster with a VAE and
alternates between updating model parameters by maximizing the Evidence Lower
Bound (ELBO) of the log-likelihood and refining cluster assignments based on
the learned distributions. This enables effective clustering and generation of
new samples from each cluster. Unlike existing VAE-based methods, our approach
eliminates the need for a Gaussian Mixture Model (GMM) prior or additional
regularization techniques. Experiments on MNIST and FashionMNIST demonstrate
superior clustering performance compared to state-of-the-art methods.","['cs.LG', 'stat.ML']",2501.07616," Ingenuity is an autonomous Cyber-Pysical System (CPS) that has successfully
completed more than 70 flights over Mars between 2021 and 2024. Ensuring the
safety of its mission is paramount, as any failure could result in catastrophic
economic damage and significant financial losses. Dataflow Models of
Computation and Communication (DF MoCCs) serve as a formal framework for
specifying and analyzing the timing behavior of such CPSs. In particular, the
Real-time Mode-aware Dataflow (RMDF) model is highly suitable to specify and
analyze real-time and mode-dependent Cyber-Physical Systems (CPSs) like
Ingenuity. This paper showcases the application of RMDF for the specification
and analysis of Ingenuity. We propose a dataflow specification of Ingenuity,
analyze its timing behavior, and provide a feasibility test. Finally, we
proposed a plausible explanation of the timing anomaly that occurred during the
sixth flight of Ingenuity.","['eess.SY', 'cs.SY']",False,,,,Deep Generative Clustering with VAEs and Expectation-Maximization,"The Ingenuity Mars Helicopter Specified and Analyzed with the Real-time
  Mode-aware Dataflow Model"
neg-d2-216,2025-02-01,,2502.00392," Drones have become prevalent robotic platforms with diverse applications,
showing significant potential in Embodied Artificial Intelligence (Embodied
AI). Referring Expression Comprehension (REC) enables drones to locate objects
based on natural language expressions, a crucial capability for Embodied AI.
Despite advances in REC for ground-level scenes, aerial views introduce unique
challenges including varying viewpoints, occlusions and scale variations. To
address this gap, we introduce RefDrone, a REC benchmark for drone scenes.
RefDrone reveals three key challenges in REC: 1) multi-scale and small-scale
target detection; 2) multi-target and no-target samples; 3) complex environment
with rich contextual expressions. To efficiently construct this dataset, we
develop RDAgent (referring drone annotation framework with multi-agent system),
a semi-automated annotation tool for REC tasks. RDAgent ensures high-quality
contextual expressions and reduces annotation cost. Furthermore, we propose
Number GroundingDINO (NGDINO), a novel method designed to handle multi-target
and no-target cases. NGDINO explicitly learns and utilizes the number of
objects referred to in the expression. Comprehensive experiments with
state-of-the-art REC methods demonstrate that NGDINO achieves superior
performance on both the proposed RefDrone and the existing gRefCOCO datasets.
The dataset and code will be publicly at
https://github.com/sunzc-sunny/refdrone.",['cs.CV'],2503.14137," Applying the least action principle to the motion of an ideal gas, we find
Bernoulli's equation where the local velocity is expressed as the gradient of a
velocity potential, while the internal energy depends on the interaction among
the particles of the gas. Then, assuming that the internal energy is
proportional non-locally to the logarithm of the mass density and truncating
the resulting sum of density gradients after the second term, we find an
additional Bohm's quantum potential term in the internal energy. Therefore, the
Bernoulli equation reduces to the Madelung equation, revealing a novel
classical description of quantum fluids that does not require to postulate
quantum mechanics. Finally, non-locality can be removed by introducing a
retarded potential, thus leading to a covariant formulation of the quantum
potential and of the equation of motion of an ideal quantum fluid.",['quant-ph'],False,,,,"RefDrone: A Challenging Benchmark for Referring Expression Comprehension
  in Drone Scenes","A variational formulation of the governing equations of ideal quantum
  fluids"
neg-d2-217,2025-02-05,,2502.03656," Dataset distillation is the concept of condensing large datasets into smaller
but highly representative synthetic samples. While previous research has
primarily focused on image classification, its application to image
Super-Resolution (SR) remains underexplored. This exploratory work studies
multiple dataset distillation techniques applied to SR, including pixel- and
latent-space approaches under different aspects. Our experiments demonstrate
that a 91.12% dataset size reduction can be achieved while maintaining
comparable SR performance to the full dataset. We further analyze
initialization strategies and distillation methods to optimize memory
efficiency and computational costs. Our findings provide new insights into
dataset distillation for SR and set the stage for future advancements.","['cs.CV', 'cs.AI', 'cs.LG']",2502.12683," We investigate the nuclear Stark effect induced in hydrogen-like atomic
nuclei under super-intense laser fields. Since laser wavelengths are generally
larger than nuclear dimensions, direct laser-nucleus interaction is unfeasible.
Instead, this effect is induced indirectly through electron oscillations in the
laser field, which produce a periodic electric field that shifts the nuclear
energy levels. Using perturbation theory, we derive an expression for the
energy shift and dynamic polarizability of the nucleus as a function of laser
parameters. Our findings reveal that the Nuclear Stark effect can be controlled
by adjusting the laser frequency and intensity, potentially enabling
applications in nuclear and quantum optical systems.",['nucl-th'],False,,,,A Study in Dataset Distillation for Image Super-Resolution,"AC nuclear Stark effect in H-atom via super-intense laser-atom
  interaction"
neg-d2-218,2025-03-19,,2503.15246," In this paper, we propose a direct multiobject tracking (MOT) approach for
MIMO-radar signals that operates on raw sensor data via variational message
passing (VMP). Unlike classical track-before-detect (TBD) methods, which often
rely on simplified likelihood models and exclude nuisance parameters (e.g.,
object amplitudes, noise variance), our method adopts a superimposed signal
model and employs a mean-field approximation to jointly estimate both object
existence and object states. By considering correlations within in the radar
signal due to closely spaced objects and jointly estimating nuisance
parameters, the proposed method achieves robust performance for close-by
objects and in low-signal-to-noise ratio (SNR) regimes. Our numerical
evaluation based on MIMO-radar signals demonstrate that our VMP-based
direct-MOT method outperforms a detect-then-track (DTT) pipeline comprising a
super-resolution sparse Bayesian learning (SBL)-based estimation stage followed
by classical MOT using global nearest neighbour data association and a Kalman
filter.",['eess.SP'],2503.03872," The self-gravitating skyrmion is an exact solution of the Einstein
$SU(2)$-Skyrme model describing a topological soliton with baryon number $B=1$,
living in a $4$-dimensional space-time in the presence of a cosmological
constant. Here we show that, using the maximal embedding Ansatz of $SU(2)$ into
$SU(N)$ in the Euler angles parametrization, this solution can be generalized
to include arbitrary values of the flavor number and, consequently, allowing
higher values of the topological charge. Also, we show that higher-order
corrections in the 't Hooft expansion can be considered while still preserving
the analytical nature of the solutions. Finally we will show that from the
gravitational solutions it is possible to construct skyrmions in flat
space-time at a finite volume.","['hep-th', 'gr-qc']",False,,,,"Variational Message Passing-based Multiobject Tracking for MIMO-Radars
  using Raw Sensor Signals",Universal self-gravitating skyrmions
neg-d2-219,2025-03-17,,2503.13806," Accurate segmentation is essential for effective treatment planning and
disease monitoring. Existing medical image segmentation methods predominantly
rely on uni-modal visual inputs, such as images or videos, requiring
labor-intensive manual annotations. Additionally, medical imaging techniques
capture multiple intertwined organs within a single scan, further complicating
segmentation accuracy. To address these challenges, MedSAM, a large-scale
medical segmentation model based on the Segment Anything Model (SAM), was
developed to enhance segmentation accuracy by integrating image features with
user-provided prompts. While MedSAM has demonstrated strong performance across
various medical segmentation tasks, it primarily relies on geometric prompts
(e.g., points and bounding boxes) and lacks support for text-based prompts,
which could help specify subtle or ambiguous anatomical structures. To overcome
these limitations, we propose the Organ-aware Multi-scale Text-guided Medical
Image Segmentation Model (OMT-SAM) for multi-organ segmentation. Our approach
introduces CLIP encoders as a novel image-text prompt encoder, operating with
the geometric prompt encoder to provide informative contextual guidance. We
pair descriptive textual prompts with corresponding images, processing them
through pre-trained CLIP encoders and a cross-attention mechanism to generate
fused image-text embeddings. Additionally, we extract multi-scale visual
features from MedSAM, capturing fine-grained anatomical details at different
levels of granularity. We evaluate OMT-SAM on the FLARE 2021 dataset,
benchmarking its performance against existing segmentation methods. Empirical
results demonstrate that OMT-SAM achieves a mean Dice Similarity Coefficient of
0.937, outperforming MedSAM (0.893) and other segmentation models, highlighting
its superior capability in handling complex medical image segmentation tasks.","['cs.CV', 'cs.AI']",2502.14097," We study the existence and qualitative properties of action ground-states
(that is, bound-states with minimal action) {of the nonlinear Schr\""odinger
equation} over single-knot metric graphs -- which are made of half-lines, loops
and pendants, all connected at a single vertex. First, we prove existence of
action ground-state for generic single-knot graphs, even in the absence of an
associated variational problem. Second, for regular single-knot graphs of
length $\ell$, we perform a complete analysis of positive monotone
bound-states. Furthermore, we characterize all positive bound-states when
$\ell$ is small and prove some symmetry-breaking results for large $\ell$.
Finally, we apply the results to some particular graphs to illustrate the
complex relation between action ground-states and the topological {and metric}
features of the underlying metric graph.
  The proofs are nonvariational, using a careful phase-plane analysis, the
study of sections of period functions, asymptotic estimates and blowup
arguments. We show, in particular, how nonvariational techniques are
complementary to variational ones in order to deeply understand bound-states of
the nonlinear Schr\""odinger equation on metric graphs.","['math.AP', 'math.CA']",False,,,,"Organ-aware Multi-scale Medical Image Segmentation Using Text Prompt
  Engineering","A comprehensive study of bound-states for the nonlinear Schr\""odinger
  equation on single-knot metric graphs"
neg-d2-220,2025-02-06,,2502.04164," Distributed optimization has become the default training paradigm in modern
machine learning due to the growing scale of models and datasets. To mitigate
communication overhead, local updates are often applied before global
aggregation, resulting in a nested optimization approach with inner and outer
steps. However, heavy-tailed stochastic gradient noise remains a significant
challenge, particularly in attention-based models, hindering effective
training. In this work, we propose TailOPT, an efficient framework designed to
address heavy-tailed noise by leveraging adaptive optimization or clipping
techniques. We establish convergence guarantees for the TailOPT framework under
heavy-tailed noise with potentially unbounded gradient variance and local
updates. Among its variants, we highlight a memory and communication efficient
instantiation which we call $Bi^2Clip$, which performs coordinate-wise clipping
at both the inner and outer optimizers, achieving adaptive-like performance
(e.g., Adam) without the cost of maintaining or transmitting additional
gradient statistics. Empirically, TailOPT, including $Bi^2Clip$, demonstrates
superior performance on several language tasks and models, outperforming
state-of-the-art methods.",['cs.LG'],2502.14754," In this paper, we present a simplified proof of Kharitonov's Theorem, an
important result on determining the Hurwitz stability of interval polynomials.
Our new approach to the proof, which is based on the Wronskian of a pair of
polynomials, is not only more elementary in comparison to known methods, but is
able to handle the degree drop case with ease.","['math.OC', 'math.CA', 'math.CV']",False,,,,Efficient Distributed Optimization under Heavy-Tailed Noise,Kharitonov's Theorem with Degree Drop: a Wronskian Approach
neg-d2-221,2025-03-20,,2503.16639," Realistic crowd simulations are essential for immersive virtual environments,
relying on both individual behaviors (microscopic dynamics) and overall crowd
patterns (macroscopic characteristics). While recent data-driven methods like
deep reinforcement learning improve microscopic realism, they often overlook
critical macroscopic features such as crowd density and flow, which are
governed by spatio-temporal spawn dynamics, namely, when and where agents enter
a scene. Traditional methods, like random spawn rates, stochastic processes, or
fixed schedules, are not guaranteed to capture the underlying complexity or
lack diversity and realism. To address this issue, we propose a novel approach
called nTPP-GMM that models spatio-temporal spawn dynamics using Neural
Temporal Point Processes (nTPPs) that are coupled with a spawn-conditional
Gaussian Mixture Model (GMM) for agent spawn and goal positions. We evaluate
our approach by orchestrating crowd simulations of three diverse real-world
datasets with nTPP-GMM. Our experiments demonstrate the orchestration with
nTPP-GMM leads to realistic simulations that reflect real-world crowd scenarios
and allow crowd analysis.",['cs.LG'],2501.12647," The recent discovery of superconductivity in La$_3$Ni$_2$O$_7$ and
La$_4$Ni$_3$O$_{10}$ under high pressure stimulates intensive research
interests. These nickelates crystallize in an orthogonal/monoclinic structure
with tilted NiO$_6$ octahedra at ambient pressure and enter a density-wave-like
phase at low temperatures. The application of pressure suppresses the
octahedral tilting and triggers a transition to tetragonal structure (I4/mmm),
which is believed to be a key prerequisite for the emergence of superconducting
state. Here, by developing a high oxidative environment growth technology, we
report the first tetragonal nickelates La$_4$Ni$_3$O$_{10}$ microcrystals
without octahedral tilting at ambient pressure. In tetragonal
La$_4$Ni$_3$O$_{10}$, transport measurements find that both density-wave and
superconducting transitions are absent up to 160 GPa, indicating a robust
tetragonal metallic ground state. Density functional theory calculations reveal
that the band structure of ambient-pressure tetragonal La$_4$Ni$_3$O$_{10}$
involves more $d_{z2}$ orbital contribution to the Fermi surface, compared to
the monoclinic phase or the high-pressure superconducting tetragonal phase. The
concurrent absence of density-wave state and high-pressure superconductivity in
our ambient-pressure tetragonal crystals of La$_4$Ni$_3$O$_{10}$ suggests an
underlying correlation between these two orders. It suggests that the
tetragonal structure is not necessary, while the density-wave state is crucial
for the superconductivity in nickelates. Our findings impose important
constraints on the mechanism of pressure-induced superconductivity in
nickelates and sheds new light on exploring ambient pressure high-temperature
Ni-based superconductors.","['cond-mat.supr-con', 'cond-mat.mtrl-sci']",False,,,,"Whenever, Wherever: Towards Orchestrating Crowd Simulations with
  Spatio-Temporal Spawn Dynamics","Absence of superconductivity and density-wave transition in
  ambient-pressure tetragonal La$_4$Ni$_3$O$_{10}$"
neg-d2-222,2025-02-18,,2502.12624," Conversational repair is a mechanism used to detect and resolve
miscommunication and misinformation problems when two or more agents interact.
One particular and underexplored form of repair in emergent communication is
the implicit repair mechanism, where the interlocutor purposely conveys the
desired information in such a way as to prevent misinformation from any other
interlocutor. This work explores how redundancy can modify the emergent
communication protocol to continue conveying the necessary information to
complete the underlying task, even with additional external environmental
pressures such as noise. We focus on extending the signaling game, called the
Lewis Game, by adding noise in the communication channel and inputs received by
the agents. Our analysis shows that agents add redundancy to the transmitted
messages as an outcome to prevent the negative impact of noise on the task
success. Additionally, we observe that the emerging communication protocol's
generalization capabilities remain equivalent to architectures employed in
simpler games that are entirely deterministic. Additionally, our method is the
only one suitable for producing robust communication protocols that can handle
cases with and without noise while maintaining increased generalization
performance levels.","['cs.LG', 'cs.MA']",2503.1502," This study presents a shaped reset feedback control strategy to enhance the
performance of precision motion systems. The approach utilizes a phase-lead
compensator as a shaping filter to tune the phase of reset instants, thereby
shaping the nonlinearity in the first-order reset control. {The design achieves
either an increased phase margin while maintaining gain properties or improved
gain without sacrificing phase margin, compared to reset control without the
shaping filter.} Then, frequency-domain design procedures are provided for both
Clegg Integrator (CI)-based and First-Order Reset Element (FORE)-based reset
control systems. Finally, the effectiveness of the proposed strategy is
demonstrated through two experimental case studies on a precision motion stage.
In the first case, the shaped reset control leverages phase-lead benefits to
achieve zero overshoot in the transient response. In the second case, the
shaped reset control strategy enhances the gain advantages of the previous
reset element, resulting in improved steady-state performance, including better
tracking precision and disturbance rejection, while reducing overshoot for an
improved transient response.","['eess.SY', 'cs.SY']",False,,,,Implicit Repair with Reinforcement Learning in Emergent Communication,"Enhancing Reset Control Phase with Lead Shaping Filters: Applications to
  Precision Motion Systems"
neg-d2-223,2025-01-16,,2501.09559," One crucial and basic method for disclosing a secret to every participant in
quantum cryptography is quantum secret sharing. Numerous intricate protocols,
including secure multiparty summation, multiplication, sorting, voting, and
more, can be designed with it. A quantum secret sharing protocol with a $(t,n)$
threshold approach and modulo d, where t and n represent the threshold number
of participants and the total number of participants, respectively was recently
discussed by Song et al. Kao et al. notes that without the information of other
participants, the secret in Song {\em et al.'s}protocol cannot be
reconstructed. We address a protocol that solves this issue in this paper.","['quant-ph', 'cs.CR']",2503.08401," The mean resolvent operator predicts, in the frequency domain, the mean
linear response to forcing, and, as such, it provides the optimal LTI
approximation of the input-output dynamics of flows in the statistically steady
regime (Leclercq & Sipp 2023). In this paper, we aim at providing numerical
frameworks to extract optimal forcings and responses of the mean resolvent,
also known as mean resolvent modes. For periodic flows, we rewrite the mean
resolvent operator in terms of a harmonic resolvent operator (Wereley & Hall
1990; Padovan & Rowley 2022) to obtain reference mean resolvent modes.
Successively, we propose a projection algorithm approximating those modes
within a subspace of mean-flow resolvent modes. The projected problem is
directly solved in the frequency domain, but we also discuss a time-stepper
version that can bypass the explicit construction of the operator without
recurring to direct-adjoint looping. We evaluate the algorithms on an
incompressible axisymmetric laminar jet periodically forced at the inlet. For a
weakly unsteady case, the mean-flow resolvent correctly approximates the main
receptivity peak of the mean resolvent, but completely fails to capture a
secondary receptivity peak. For a strongly unsteady case, even the main
receptivity peak of the mean resolvent is incorrectly captured by the mean-flow
resolvent. Although the present algorithms are currently restricted to periodic
flows, input projection may be a key ingredient to extend mean resolvent
analysis to more complex statistically steady flows.",['physics.flu-dyn'],False,,,,Threshold Quantum Secret Sharing,Mean resolvent analysis of periodic flows
neg-d2-224,2025-02-19,,2502.13606," Understanding the property of neural populations (or voxels) in the human
brain can advance our comprehension of human perceptual and cognitive
processing capabilities and contribute to developing brain-inspired computer
models. Recent encoding models using deep neural networks (DNNs) have
successfully predicted voxel-wise activity. However, interpreting the
properties that explain voxel responses remains challenging because of the
black-box nature of DNNs. As a solution, we propose LLM-assisted Visual Cortex
Captioning (LaVCa), a data-driven approach that uses large language models
(LLMs) to generate natural-language captions for images to which voxels are
selective. By applying LaVCa for image-evoked brain activity, we demonstrate
that LaVCa generates captions that describe voxel selectivity more accurately
than the previously proposed method. Furthermore, the captions generated by
LaVCa quantitatively capture more detailed properties than the existing method
at both the inter-voxel and intra-voxel levels. Furthermore, a more detailed
analysis of the voxel-specific properties generated by LaVCa reveals
fine-grained functional differentiation within regions of interest (ROIs) in
the visual cortex and voxels that simultaneously represent multiple distinct
concepts. These findings offer profound insights into human visual
representations by assigning detailed captions throughout the visual cortex
while highlighting the potential of LLM-based methods in understanding brain
representations. Please check out our webpage at
https://sites.google.com/view/lavca-llm/","['q-bio.NC', 'cs.AI', 'cs.CL', 'cs.CV', 'cs.LG']",2503.01064," Large language models (LLMs) can answer questions and reason about complex
tasks, also from the scientific domain. We assess several multimodal LLMs
(MLLMs) on ScienceQA and find that Gemini models show the highest accuracy with
little context, and the highest textual similarity to human explanations with
richer context. Adapter-tuning of smaller MLLMs did not lead to any reliable
performance. Training from Gemini outputs consistently underperformed training
from the original data.","['cs.CL', 'cs.AI', 'cs.CV']",False,,,,LaVCa: LLM-assisted Visual Cortex Captioning,Scientific Reasoning: Assessment of Multimodal Generative LLMs
neg-d2-225,2025-02-05,,2502.03618," The field of mechanistic interpretability in pre-trained transformer models
has demonstrated substantial evidence supporting the ''linear representation
hypothesis'', which is the idea that high level concepts are encoded as vectors
in the space of activations of a model. Studies also show that model generation
behavior can be steered toward a given concept by adding the concept's vector
to the corresponding activations. We show how to leverage these properties to
build a form of logical implication into models, enabling transparent and
interpretable adjustments that induce a chosen generation behavior in response
to the presence of any given concept. Our method, Logical Implication Model
Steering (LIMS), unlocks new hand engineered reasoning capabilities by
integrating neuro-symbolic logic into pre-trained transformer models.",['cs.LG'],2503.03434," Speculative decoding accelerates inference in large language models (LLMs) by
generating draft tokens for target model verification. Current approaches for
obtaining draft tokens rely on lightweight draft models or additional model
structures to generate draft tokens and retrieve context from databases. Due to
the draft model's small size and limited training data, model-based speculative
decoding frequently becomes less effective in out-of-domain scenarios.
Additionally, the time cost of the drafting phase results in a low upper limit
on acceptance length during the verification step, limiting overall efficiency.
This paper proposes RASD (Retrieval-Augmented Speculative Decoding), which
adopts retrieval methods to enhance model-based speculative decoding. We
introduce tree pruning and tree fusion to achieve this. Specifically, we
develop a pruning method based on the draft model's probability distribution to
construct the optimal retrieval tree. Second, we employ the longest prefix
matching algorithm to merge the tree generated by the draft model with the
retrieval tree, resulting in a unified tree for verification. Experimental
results demonstrate that RASD achieves state-of-the-art inference acceleration
across tasks such as DocQA, Summary, Code, and In-Domain QA. Moreover, RASD
exhibits strong scalability, seamlessly integrating with various speculative
decoding approaches, including both generation-based and retrieval-based
methods.","['cs.CL', 'cs.AI']",False,,,,"The Logical Implication Steering Method for Conditional Interventions on
  Transformer Generation",RASD: Retrieval-Augmented Speculative Decoding
neg-d2-226,2025-02-13,,2502.09905," Abdominal aortic aneurysm (AAA) is a life-threatening condition involving the
permanent dilation of the aorta, often detected incidentally through imaging
for some other condition. The standard clinical approach to managing AAA
follows a one-size-fits-all model based on aneurysm size and growth rate,
leading to underestimation or overestimation of rupture risk in individual
patients. The widely studied stress-based rupture risk estimation using
computational biomechanics requires wall strength information. However,
non-invasive methods for local patient-specific wall strength measurement have
not yet been developed. Recently, we introduced an image-based approach for
patient-specific, in vivo, non-invasive AAA kinematic analysis using
time-resolved 3D computed tomography angiography (4D-CTA) images to measure
wall strain throughout the cardiac cycle. In the present study, we integrated
wall tension computation and strain measurement to develop a novel measure of
local structural integrity of AAA wall - Relative Structural Integrity Index
(RSII), independent of material properties and thickness of the wall and
conditions of blood pressure measurement. Our methods provide a visual map of
AAA wall structural integrity for individual patients using only their medical
images and blood pressure data. We applied our methods to twelve patients.
Additionally, we compared our measure of structural integrity of aneurysmal and
non-aneurysmal aortas. Our results show similar values of the wall structural
integrity measure across the patients, indicating the reliability of our
methods. In line with experimental observations reported in the literature, our
analysis revealed that localized low stiffness areas are primarily found in the
most dilated AAA regions. Our results clearly demonstrate that the AAA wall is
stiffer than the non-aneurysmal aorta.",['cs.CE'],2501.06813," Subset selection is a fundamental problem in combinatorial optimization,
which has a wide range of applications such as influence maximization and
sparse regression. The goal is to select a subset of limited size from a ground
set in order to maximize a given objective function. However, the evaluation of
the objective function in real-world scenarios is often noisy. Previous
algorithms, including the greedy algorithm and multi-objective evolutionary
algorithms POSS and PONSS, either struggle in noisy environments or consume
excessive computational resources. In this paper, we focus on the noisy subset
selection problem with a cardinality constraint, where the evaluation of a
subset is noisy. We propose a novel approach based on Pareto Optimization with
Robust Evaluation for noisy subset selection (PORE), which maximizes a robust
evaluation function and minimizes the subset size simultaneously. PORE can
efficiently identify well-structured solutions and handle computational
resources, addressing the limitations observed in PONSS. Our experiments,
conducted on real-world datasets for influence maximization and sparse
regression, demonstrate that PORE significantly outperforms previous methods,
including the classical greedy algorithm, POSS, and PONSS. Further validation
through ablation studies confirms the effectiveness of our robust evaluation
function.",['cs.NE'],False,,,,"Towards personalised assessment of abdominal aortic aneurysm structural
  integrity",Pareto Optimization with Robust Evaluation for Noisy Subset Selection
neg-d2-227,2025-02-10,,2502.07019," This study investigates the coupled deformation and flow behavior through
thin, hyper-elastic, porous membranes subjected to pressure loading. Using
bulge test experiments, optical deformation measurements, and flow rate
characterization, we analyze the structural and fluid dynamic responses of
membranes with varying material stiffness and porosity patterns. A
two-parameter Gent model captures the hyper-elastic deformation, while local
stretch analyses reveal the evolution of pore sizes across the membrane. We
find that membrane stretch is primarily governed by material stiffness and
applied pressure, independent of porosity. A gradient of increasing pore size
toward the membrane center emerges due to higher local stretch, while the total
open pore area remains approximately constant across radial layers of the
membrane. Flow rate scaling is characterized using a discharge coefficient that
accounts for pore area expansion and pressure losses. While the initial scaling
compares well in most cases, it breaks down for scenarios with significantly
different pore Reynolds numbers, driven by large variations in initial
porosity. To address this, we introduce a Reynolds-dependent correction term
that unifies discharge coefficient predictions across diverse porosity and flow
velocity conditions. These findings enhance the understanding of poro-elastic
systems and provide robust scaling relationships for designing thin, flexible,
porous structures in applications such as bio-inspired aerodynamic systems and
adaptive flow regulation devices.",['physics.flu-dyn'],2501.03137," We investigate the problem of synthesizing distributionally robust control
policies for stochastic systems under safety and reach-avoid specifications.
Using a game-theoretical framework, we consider the setting where the
probability distribution of the disturbance at each time step is selected from
an ambiguity set defined by the Wasserstein distance. The goal is to synthesize
a distributionally robust control policy that ensures the satisfaction
probability exceeds a specified threshold under any distribution within the
ambiguity set. First, for both safety and reach-avoid specifications, we
establish the existence of optimal policies by leveraging the dynamic
programming principles. Then we demonstrate how the associated optimization
problem can be efficiently solved using the dual representation of Wasserstein
distributionally robust optimization. Furthermore, for safety specifications in
particular, we introduce a novel concept of distributionally robust control
barrier certificates and show how these enable the efficient synthesis of
controllers through sum-of-squares programming techniques. Finally, our
experimental results reveal that incorporating distributional robustness during
the synthesis phase significantly improves the satisfaction probability during
online execution, even with limited statistical knowledge of the disturbance
distribution.","['eess.SY', 'cs.SY']",False,,,,Coupled poro-elastic behavior of hyper-elastic membranes,"Distributionally Robust Control Synthesis for Stochastic Systems with
  Safety and Reach-Avoid Specifications"
neg-d2-228,2025-02-10,,2502.07126," We propose to relax traditional axioms in decision theory by incorporating a
measurement, or degree, of satisfaction. For example, if the independence axiom
of expected utility theory is violated, we can measure the size of the
violation. This measure allows us to derive an approximation guarantee for a
utility representation that aligns with the unmodified version of the axiom.
Almost satisfying the axiom implies, then, a utility that is near a utility
representation. We develop specific examples drawn from expected utility theory
under risk and uncertainty.",['econ.TH'],2502.04659," Blockchains have revolutionized decentralized applications, with
composability enabling atomic, trustless interactions across smart contracts.
However, layer 2 (L2) scalability solutions like rollups introduce
fragmentation and hinder composability. Current cross-chain protocols,
including atomic swaps, bridges, and shared sequencers, lack the necessary
coordination mechanisms or rely on trust assumptions, and are thus not
sufficient to support full cross-rollup composability. This paper presents
$\mathsf{CRATE}$, a secure protocol for cross-rollup composability that ensures
all-or-nothing and serializable execution of cross-rollup transactions (CRTs).
$\mathsf{CRATE}$ supports rollups on distinct layer 1 (L1) chains, achieves
finality in 4 rounds on L1, and only relies on the underlying L1s and the
liveness of L2s. We introduce two formal models for CRTs, define atomicity
within them, and formally prove the security of $\mathsf{CRATE}$. We also
provide an implementation of $\mathsf{CRATE}$ along with a cross-rollup flash
loan application; our experiments demonstrate that $\mathsf{CRATE}$ is
practical in terms of gas usage on L1.",['cs.CR'],False,,,,"Decision theory and the ""almost implies near"" phenomenon",$\mathsf{CRATE}$: Cross-Rollup Atomic Transaction Execution
neg-d2-229,2025-01-07,,2501.04162," Let $N$ and $p$ be prime numbers with $p \geq 5$ such that $p || (N + 1)$. In
a previous paper, we showed that there is a cuspform $f$ of weight 2 and level
$\Gamma_0(N^2)$ whose $\ell$-th Fourier coefficient is congruent to $\ell + 1$
modulo a prime above $p$ for all primes $\ell$. In this paper, we prove that
this form $f$ is unique up to Galois conjugacy, and the extension of
$\mathbb{Z}_p$ generated by the coefficients of $f$ is exactly
$\mathbb{Z}_p[\zeta_p + \zeta_p^{-1}]$. We also prove similar results when a
higher power of $p$ divides $N + 1$.",['math.NT'],2502.08434," We analyze some aspects of the cubic action for gravity recently proposed by
Cheung and Remmen, which is a particular instance of a first order (Palatini)
action. In this approach both the spacetime metric and the connection are
treated as independent fields. We discuss its BRST invariance and compute
explicitly the one-loop contribution of quantum fluctuations around flat space,
checking that the corresponding Slavnov-Taylor identities are fulfilled.
Finally, our results on a first order action are compared with the existing
ones corresponding to a second order action.","['hep-th', 'gr-qc']",False,,,,The Eisenstein ideal at prime-square level has constant rank,One loop analysis of the cubic action for gravity
neg-d2-230,2025-03-17,,2503.13568," Autonomous mobile robots are widely used for navigation, transportation, and
inspection tasks indoors and outdoors. In practical situations of limited
satellite signals or poor lighting conditions, navigation depends only on
inertial sensors. In such cases, the navigation solution rapidly drifts due to
inertial measurement errors. In this work, we propose WMINet a wheel-mounted
inertial deep learning approach to estimate the mobile robot's position based
only on its inertial sensors. To that end, we merge two common practical
methods to reduce inertial drift: a wheel-mounted approach and driving the
mobile robot in periodic trajectories. Additionally, we enforce a wheelbase
constraint to further improve positioning performance. To evaluate our proposed
approach we recorded using the Rosbot-XL a wheel-mounted initial dataset
totaling 190 minutes, which is made publicly available. Our approach
demonstrated a 66\% improvement over state-of-the-art approaches. As a
consequence, our approach enables navigation in challenging environments and
bridges the pure inertial gap. This enables seamless robot navigation using
only inertial sensors for short periods.","['cs.RO', 'cs.AI', 'cs.LG']",2502.12181," Explainability remains a significant problem for AI models in medical
imaging, making it challenging for clinicians to trust AI-driven predictions.
We introduce 3D ReX, the first causality-based post-hoc explainability tool for
3D models. 3D ReX uses the theory of actual causality to generate
responsibility maps which highlight the regions most crucial to the model's
decision. We test 3D ReX on a stroke detection model, providing insight into
the spatial distribution of features relevant to stroke.","['eess.IV', 'cs.AI', 'cs.CV', 'cs.LG']",False,,,,"WMINet: A Wheel-Mounted Inertial Learning Approach For Mobile-Robot
  Positioning",3D ReX: Causal Explanations in 3D Neuroimaging Classification
neg-d2-231,2025-01-03,,2501.01854," In this brief note, it is shown that the function p^TW log(p) is convex in p
if W is a diagonally dominant positive definite M-matrix. The techniques used
to prove convexity are well-known in linear algebra and essentially involves
factoring the Hessian in a way that is amenable to martix analysis. Using
similar techniques, two classes of convex homogeneous polynomials is derived -
namely, p^TW p2 and (p^k)^TW p^k - the latter also happen to be SOS-convex.
Lastly, usign the same techniques, it is also shown that the function p^TW ep
is convex over the positive reals only if W is a non-negative diagonal matrix.
Discussions regarding the utility of these functions and examples accompany the
results presented.",['math.OC'],2501.14717," Recent advances in natural language processing have leveraged instruction
tuning to enhance Large Language Models (LLMs) for table-related tasks.
However, previous works train different base models with different training
data, lacking an apples-to-apples comparison across the result table LLMs. To
address this, we fine-tune base models from the Mistral, OLMo, and Phi families
on existing public training datasets. Our replication achieves performance on
par with or surpassing existing table LLMs, establishing new state-of-the-art
performance on Hitab, a table question-answering dataset. More importantly,
through systematic out-of-domain evaluation, we decouple the contributions of
training data and the base model, providing insight into their individual
impacts. In addition, we assess the effects of table-specific instruction
tuning on general-purpose benchmarks, revealing trade-offs between
specialization and generalization.",['cs.CL'],False,,,,On extending the class of convex functions,"Towards Better Understanding Table Instruction Tuning: Decoupling the
  Effects from Data versus Models"
neg-d2-232,2025-02-09,,2502.06103," In 2015, Phulara established a generalization of the famous central set
theorem by an original idea. Roughly speaking, this idea extends a
combinatorial result from one large subset of the given semigroup to countably
many. In this paper, we apply this idea to other combinatorial results to
obtain corresponding generalizations, and do some further investigation.
Moreover, we find that Phulara's generalization can be generalized further that
can deal with uncountably many C-sets.",['math.CO'],2502.01731," During the accretion phase of a core-collapse supernova (SN), dark-photon
(DP) cooling can be largest in the gain layer below the stalled shock wave. In
this way, it could counter-act the usual shock rejuvenation by neutrino energy
deposition and thus prevent the explosion. This peculiar energy-loss profile
derives from the resonant nature of DP production. The largest cooling and thus
strongest constraints obtain for DP masses of 0.1-0.4 MeV, a range
corresponding to the photon plasma mass in the gain region. Electron-capture
SNe, once observationally unambiguously identified, could provide strong bounds
even down to nearly 0.01 MeV. For a coupling strength so small that
neutrino-driven explosions are expected to survive, the DP cooling of the core
is too small to modify the neutrino signal, i.e., our new argument supersedes
the traditional SN1987A cooling bound.","['hep-ph', 'astro-ph.HE']",False,,,,"Several combinatorial results generalized from one large subset of
  semigroups to infinitely many",Dark Photons can Prevent Core-Collapse Supernova Explosions
neg-d2-233,2025-02-16,,2502.11283," In urban areas, the quality of global navigation satellite system (GNSS)
signals deteriorates, leading to reduced positioning accuracy. To address this
issue, 3D-mapping-aided (3DMA) techniques, such as shadow matching and zonotope
shadow matching (ZSM), have been proposed. However, these methods can introduce
a problem known as multi-modal position ambiguity, making it challenging to
select the exact mode in which the receiver is located. Accurately selecting
the correct mode is essential for improving positioning accuracy. A previous
study proposed a method that uses satellite-pseudorange consistency (SPC),
calculated from pseudorange measurements, to select the mode containing the
receiver. This method achieved a mode selection accuracy of approximately 78%.
To further enhance accuracy, the study utilized pseudorange measurements
collected at multiple timesteps from a fixed location and a trained
line-of-sight (LOS) classifier. However, in practice, collecting data at
multiple timesteps from the same location in dynamic environments is
challenging. Moreover, the performance of the trained LOS classifier heavily
depends on the surrounding environment, leading to low reliability. In this
study, we propose a method that estimates and corrects multipath errors based
on the mode distribution obtained from the output of ZSM and extract an
enhanced SPC using the corrected pseudorange measurements. This enables high
mode selection accuracy using only single-timestep pseudorange measurements,
without requiring a trained LOS classifier.",['eess.SP'],2502.00275," Accurate estimation of human hand configuration and the forces they exert is
critical for effective teleoperation and skill transfer in robotic
manipulation. A deeper understanding of human interactions with objects can
further enhance teleoperation performance. To address this need, researchers
have explored methods to capture and translate human manipulation skills and
applied forces to robotic systems. Among these, biosignal-based approaches,
particularly those using forearm ultrasound data, have shown significant
potential for estimating hand movements and finger forces. In this study, we
present a method for simultaneously estimating manipulation skills and applied
hand force using forearm ultrasound data. Data collected from seven
participants were used to train deep learning models for classifying
manipulation skills and estimating grasp force. Our models achieved an average
classification accuracy of 94.87 percent plus or minus 10.16 percent for
manipulation skills and an average root mean square error (RMSE) of 0.51 plus
or minus 0.19 Newtons for force estimation, as evaluated using five-fold
cross-validation. These results highlight the effectiveness of forearm
ultrasound in advancing human-machine interfacing and robotic teleoperation for
complex manipulation tasks. This work enables new and effective possibilities
for human-robot skill transfer and tele-manipulation, bridging the gap between
human dexterity and robotic control.","['cs.RO', 'cs.CV', 'cs.ET', 'cs.HC']",False,,,,"Set-Based Position Ambiguity Reduction Method for Zonotope Shadow
  Matching in Urban Areas Using Estimated Multipath Errors","Simultaneous Estimation of Manipulation Skill and Hand Grasp Force from
  Forearm Ultrasound Images"
neg-d2-234,2025-01-17,,2501.10148," We point out a new effect on the freeze-out process of heavy particles
induced by density perturbations in the early universe, which we call
``acoustically driven freeze-out.'' This beyond-linear effect is caused by the
exponential decoupling of heavy particles from the thermal bath in the presence
of density perturbations, and already at moderately large values $\delta T /
\bar{T} = O (10^{-2})$ it cannot be captured by linear perturbation theory. We
illustrate this effect with leptogenesis taking the decay and inverse decay of
heavy neutrinos into account, and discuss its phenomenological implications. We
found that perturbations always enhance the (spatially averaged) values of the
final lepton asymmetry, and as a result, constraints on the mass of heavy
neutrinos are found to be relaxed in the presence of perturbations.","['hep-ph', 'astro-ph.CO', 'gr-qc']",2502.14812," We introduce the Byzantine Selection Problem, living at the intersection of
game theory and fault-tolerant distributed computing. Here, an event organizer
is presented with a group of $n$ agents, and wants to select $\ell < n$ of them
to form a team. For these purposes, each agent $i$ self-reports a positive
skill value $v_i$, and a team's value is the sum of its members' skill values.
Ideally, the value of the team should be as large as possible, which can be
easily achieved by selecting agents with the highest $\ell$ skill values.
However, an unknown subset of at most $t < n$ agents are byzantine and hence
not to be trusted, rendering their true skill values as $0$. In the spirit of
the distributed computing literature, the identity of the byzantine agents is
not random but instead chosen by an adversary aiming to minimize the value of
the chosen team. Can we still select a team with good guarantees in this
adversarial setting? As it turns out, deterministically, it remains optimal to
select agents with the highest $\ell$ values. Yet, if $t \geq \ell$, the
adversary can choose to make all selected agents byzantine, leading to a team
of value zero. To provide meaningful guarantees, one hence needs to allow for
randomization, in which case the expected value of the selected team needs to
be maximized, assuming again that the adversary plays to minimize it. For this
case, we provide linear-time randomized algorithms that maximize the expected
value of the selected team.","['cs.GT', 'cs.DC', 'cs.DS']",False,,,,Leptogenesis in the presence of density perturbations,Byzantine Game Theory: Sun Tzus Boxes
neg-d2-235,2025-02-03,,2502.01708," In this paper, we study the machine learning elements which we are interested
in together as a machine learning system, consisting of a collection of machine
learning elements and a collection of relations between the elements. The
relations we concern are algebraic operations, binary relations, and binary
relations with composition that can be reasoned categorically. A machine
learning system transformation between two systems is a map between the
systems, which preserves the relations we concern. The system transformations
given by quotient or clustering, representable functor, and Yoneda embedding
are highlighted and discussed by machine learning examples. An adjunction
between machine learning systems, a special machine learning system
transformation loop, provides the optimal way of solving problems. Machine
learning system transformations are linked and compared by their maps at
2-cell, natural transformations. New insights and structures can be obtained
from universal properties and algebraic structures given by monads, which are
generated from adjunctions.","['cs.LG', 'cs.AI', 'cs.DB', 'cs.DM']",2502.16351," Neural radiance field (NeRF) research has made significant progress in
modeling static video content captured in the wild. However, current models and
rendering processes rarely consider scenes captured underwater, which are
useful for studying and filming ocean life. They fail to address visual
artifacts unique to underwater scenes, such as moving fish and suspended
particles. This paper introduces a novel NeRF renderer and optimization scheme
for an implicit MLP-based NeRF model. Our renderer reduces the influence of
floaters and moving objects that interfere with static objects of interest by
estimating a single surface per ray. We use a Gaussian weight function with a
small offset to ensure that the transmittance of the surrounding media remains
constant. Additionally, we enhance our model with a depth-based scaling
function to upscale gradients for near-camera volumes. Overall, our method
outperforms the baseline Nerfacto by approximately 7.5\% and SeaThru-NeRF by
6.2% in terms of PSNR. Subjective evaluation also shows a significant reduction
of artifacts while preserving details of static targets and background compared
to the state of the arts.",['cs.CV'],False,,,,"Aspects of Artificial Intelligence: Transforming Machine Learning
  Systems Naturally","AquaNeRF: Neural Radiance Fields in Underwater Media with Distractor
  Removal"
neg-d2-236,2025-03-11,,2503.0843," We present a fully numerical framework for the optimization of
molecule-specific quantum chemical basis functions within the quantics tensor
train format using a finite-difference scheme. The optimization is driven by
solving the Hartree-Fock equations (HF) with the density-matrix renormalization
group (DMRG) algorithm on Cartesian grids that are iteratively refined. In
contrast to the standard way of tackling the mean-field problem by expressing
the molecular orbitals as linear combinations of atomic orbitals (LCAO) our
method only requires as much basis functions as there are electrons within the
system. Benchmark calculations for atoms and molecules with up to ten electrons
show excellent agreement with LCAO calculations with large basis sets
supporting the validity of the tensor network approach. Our work therefore
offers a promising alternative to well-established HF-solvers and could pave
the way to define highly accurate, fully numerical, molecule-adaptive basis
sets, which, in the future, could lead to benefits for post-HF calculations.","['physics.chem-ph', 'quant-ph']",2503.04612," This note is concerned with the distribution of the angles between Oseledets
subspaces for linear cocycles driven by an ergodic transformation. We restrict
ourselves to dimension $2$, and give particular attention to the question of
log-integrability of those angles. In the setting of random i.i.d.\ products of
matrices, we construct examples of probability measures on \(\GL_2(\R)\) with
finite first moment, for which the angle between Oseledets directions of the
associated cocycle is not log-integrable. Building on work for the totally
irreducible case by Benoist and Quint, we show that for probability measures
with finite second moment the angle between Oseledets subspaces is always
log-integrable. Then we pivot to general measurable \(\GL_2(\R)\)-cocycles over
an arbitrary ergodic automorphism of a non-atomic Lebesgue space. We show that
no integrability condition on the distribution of the matrices is sufficient to
guarantee log-integrability of the angle between Oseledets spaces. In fact, in
this context we show that the joint distribution of the Oseledets spaces may be
chosen arbitrarily. We also obtain a similar flexibility result for bounded
cocycles under the proper condition on the distribution of angles.",['math.DS'],False,,,,Fully numerical Hartree-Fock Calculations with Quantized Tensor Trains,On the distribution of the angle between Oseledets spaces
neg-d2-237,2025-02-03,,2502.01945," Superconducting quantum computers require microwave control lines running
from room temperature to the mixing chamber of a dilution refrigerator. Adding
more lines without preliminary thermal modeling to make predictions risks
overwhelming the cooling power at each thermal stage. In this paper, we
investigate the thermal load of SC-086/50-SCN-CN semi-rigid coaxial cable,
which is commonly used for the control and readout lines of a superconducting
quantum computer, as we increase the number of lines to a quantum processor. We
investigate the makeup of the coaxial cables, verify the materials and
dimensions, and experimentally measure the total thermal conductivity of a
single cable as a function of the temperature from cryogenic to room
temperature values. We also measure the cryogenic DC electrical resistance of
the inner conductor as a function of temperature, allowing for the calculation
of active thermal loads due to Ohmic heating. Fitting this data produces a
numerical thermal conductivity function used to calculate the static heat loads
due to thermal transfer within the wires resulting from a temperature gradient.
The resistivity data is used to calculate active heat loads, and we use these
fits in a cryogenic model of a superconducting quantum processor in a typical
Bluefors XLD1000-SL dilution refrigerator, investigating how the thermal load
increases with processor sizes ranging from 100 to 225 qubits. We conclude that
the theoretical upper limit of the described architecture is approximately 200
qubits. However, including an engineering margin in the cooling power and the
available space for microwave readout circuitry at the mixing chamber, the
practical limit will be approximately 140 qubits.","['quant-ph', 'physics.ins-det']",2501.05712," We introduce HRMCR (HAE-RAE Multi-Step Commonsense Reasoning), a benchmark
designed to evaluate large language models' ability to perform multi-step
reasoning in culturally specific contexts, focusing on Korean. The questions
are automatically generated via templates and algorithms, requiring LLMs to
integrate Korean cultural knowledge into sequential reasoning steps. Consistent
with prior observations on emergent abilities, our experiments reveal that
models trained on fewer than \(2 \cdot 10^{25}\) training FLOPs struggle to
solve any questions, showing near-zero performance. Beyond this threshold,
performance improves sharply. State-of-the-art models (e.g., O1) still score
under 50\%, underscoring the difficulty of our tasks. Notably, stepwise
analysis suggests the observed emergent behavior may stem from compounding
errors across multiple steps rather than reflecting a genuinely new capability.
We publicly release the benchmark and commit to regularly updating the dataset
to prevent contamination.",['cs.CL'],False,,,,Cryogenic Thermal Modeling of Microwave High Density Signaling,Multi-Step Reasoning in Korean and the Emergent Mirage
neg-d2-238,2025-03-11,,2503.09013," Universal adverse weather removal (UAWR) seeks to address various weather
degradations within a unified framework. Recent methods are inspired by prompt
learning using pre-trained vision-language models (e.g., CLIP), leveraging
degradation-aware prompts to facilitate weather-free image restoration,
yielding significant improvements. In this work, we propose CyclicPrompt, an
innovative cyclic prompt approach designed to enhance the effectiveness,
adaptability, and generalizability of UAWR. CyclicPrompt Comprises two key
components: 1) a composite context prompt that integrates weather-related
information and context-aware representations into the network to guide
restoration. This prompt differs from previous methods by marrying learnable
input-conditional vectors with weather-specific knowledge, thereby improving
adaptability across various degradations. 2) The erase-and-paste mechanism,
after the initial guided restoration, substitutes weather-specific knowledge
with constrained restoration priors, inducing high-quality weather-free
concepts into the composite prompt to further fine-tune the restoration
process. Therefore, we can form a cyclic ""Prompt-Restore-Prompt"" pipeline that
adeptly harnesses weather-specific knowledge, textual contexts, and reliable
textures. Extensive experiments on synthetic and real-world datasets validate
the superior performance of CyclicPrompt. The code is available at:
https://github.com/RongxinL/CyclicPrompt.",['cs.CV'],2503.09389," We study canonical-equilibrium properties of Random Field $O(n)$ Models
involving classical continuous vector spins of $n$ components with mean-field
interactions and subject to disordered fields acting on individual spins. To
this end, we employ two complementary approaches: the mean-field approximation,
valid for any disorder distribution, and the replica trick, applicable when the
disordered fields are sampled from a Gaussian distribution. On the basis of an
exact analysis, we demonstrate that when replica symmetry holds, both the
approaches yield identical expression for the free energy per spin of the
system. As consequences, we study the case of $n=2$ ($XY$ spins) and that of
$n=3$ (Heisenberg spins) for two representative choices of the disorder
distribution, namely, a Gaussian and a symmetric bimodal distribution. For both
$n=2$ and $n=3$, we demonstrate that while the magnetization exhibits a
continuous phase transition as a function of temperature for the Gaussian case,
the transition could be either continuous or first-order with an emergent
tricriticality when the disorder distribution is bimodal. We also discuss in
the context of our models the issue of self-averaging of extensive variables
near the critical point of a continuous phase transition.","['cond-mat.stat-mech', 'cond-mat.dis-nn']",False,,,,"Prompt to Restore, Restore to Prompt: Cyclic Prompting for Universal
  Adverse Weather Removal","Canonical equilibrium of mean-field $O(n)$~models in presence of random
  fields"
neg-d2-239,2025-02-21,,2502.15433," A low-energy storage ring with an ultracold electron cooler has been coupled
with a heavy-ion accelerator facilitating high-resolution electron-ion
collision spectroscopy of the heaviest few-electron ions. In the present work
resonant electron-ion recombination of berylliumlike Pb$^{78+}$ ions was
measured in the collision-energy range 9.3-16.5eV and a value of 244.937(30) eV
is derived for the Pb$^{78+}$($2s^2\;^1S_0 - 2s\,2p\;^3P_1$) excitation energy.
This result agrees with the most recent (less accurate) theoretical value of
244.942(52) eV [Malyshev et al., Physical Review A 110, 062824 (2024)], which
has been calculated by applying strong-field QED rigorously up to the second
order. The present investigation suggests that further technical improvements
can potentially increase the experimental accuracy by an order of magnitude.",['physics.atom-ph'],2502.20639," Federated Learning (FL) facilitates collaborative training of a shared global
model without exposing clients' private data. In practical FL systems, clients
(e.g., edge servers, smartphones, and wearables) typically have disparate
system resources. Conventional FL, however, adopts a one-size-fits-all
solution, where a homogeneous large global model is transmitted to and trained
on each client, resulting in an overwhelming workload for less capable clients
and starvation for other clients. To address this issue, we propose FedConv, a
client-friendly FL framework, which minimizes the computation and memory burden
on resource-constrained clients by providing heterogeneous customized
sub-models. FedConv features a novel learning-on-model paradigm that learns the
parameters of the heterogeneous sub-models via convolutional compression.
Unlike traditional compression methods, the compressed models in FedConv can be
directly trained on clients without decompression. To aggregate the
heterogeneous sub-models, we propose transposed convolutional dilation to
convert them back to large models with a unified size while retaining
personalized information from clients. The compression and dilation processes,
transparent to clients, are optimized on the server leveraging a small public
dataset. Extensive experiments on six datasets demonstrate that FedConv
outperforms state-of-the-art FL systems in terms of model accuracy (by more
than 35% on average), computation and communication overhead (with 33% and 25%
reduction, respectively).","['cs.LG', 'cs.AI']",False,,,,"Testing strong-field QED to second-order in the highly correlated atomic
  system berylliumlike Pb78+ by electron-ion recombination spectroscopy","FedConv: A Learning-on-Model Paradigm for Heterogeneous Federated
  Clients"
neg-d2-240,2025-02-14,,2502.1016," The multi-stage method of laser wakefield acceleration (LWFA) presents a
promising approach for developing stable, full-optical, high-energy electron
accelerators. By segmenting the acceleration process into several booster
stages, each powered by independent laser drivers, this technique effectively
mitigates challenges such as electron dephasing, pump depletion, and laser
diffraction. A critical aspect of multi-stage LWFA is the nonlinear interaction
between the injected electron beam and the laser-driven wakefields in the
booster stage. This study investigates the injection and acceleration of
external electron beams within wakefields in the booster stage using
multi-dimensional Particle-In-Cell (PIC) simulations. We provide both
qualitative and quantitative descriptions of the observed physical processes.
Key parameters influencing charge coupling process and the resultant beam
quality have been identified. Furthermore, we have examined how off-axis
injection relative to the driver laser influences the acceleration process and
beam quality. Our findings provide valuable insights for advancing and
optimizing multi-stage plasma-based accelerators.","['physics.plasm-ph', 'physics.acc-ph', 'physics.comp-ph']",2502.08397," Clustering is a fundamental technique in data analysis and machine learning,
used to group similar data points together. Among various clustering methods,
the Minimum Sum-of-Squares Clustering (MSSC) is one of the most widely used.
MSSC aims to minimize the total squared Euclidean distance between data points
and their corresponding cluster centroids. Due to the unsupervised nature of
clustering, achieving global optimality is crucial, yet computationally
challenging. The complexity of finding the global solution increases
exponentially with the number of data points, making exact methods impractical
for large-scale datasets. Even obtaining strong lower bounds on the optimal
MSSC objective value is computationally prohibitive, making it difficult to
assess the quality of heuristic solutions. We address this challenge by
introducing a novel method to validate heuristic MSSC solutions through
optimality gaps. Our approach employs a divide-and-conquer strategy,
decomposing the problem into smaller instances that can be handled by an exact
solver. The decomposition is guided by an auxiliary optimization problem, the
""anticlustering problem"", for which we design an efficient heuristic.
Computational experiments demonstrate the effectiveness of the method for
large-scale instances, achieving optimality gaps below 3% in most cases while
maintaining reasonable computational times. These results highlight the
practicality of our approach in assessing feasible clustering solutions for
large datasets, bridging a critical gap in MSSC evaluation.","['math.OC', 'cs.LG']",False,,,,"Coupling and Acceleration of Externally Injected Electron Beams in
  Laser-Driven Plasma Wakefields",Strong bounds for large-scale Minimum Sum-of-Squares Clustering
neg-d2-241,2025-01-21,,2501.11955," Mean field games (MFGs) offer a versatile framework for modeling large-scale
interactive systems across multiple domains. This paper builds upon a previous
work, by developing a state-of-the-art unified approach to decode or design the
unknown stationary state of MFGs, in addition to the underlying parameter
functions governing their behavior. This result is novel, even in the general
realm of inverse problems for nonlinear PDEs. By enabling agents to distill
crucial insights from observed data and unveil intricate hidden structures and
unknown states within MFG systems, our approach surmounts a significant
obstacle, enhancing the applicability of MFGs in real-world scenarios. This
advancement not only enriches our understanding of MFG dynamics but also
broadens the scope for their practical deployment in various contexts.","['math.AP', 'math.OC']",2503.1204," Recently, Griffin, Ono, and Tsai examined the distribution of the number of
$t$-hooks in partitions of $n$, which was later followed by the work of Craig,
Ono, and Singh on the distribution of the number of $t$-hooks in self-conjugate
partitions of $n$. Motivated by these studies, in this paper, we further
investigate the number of $t$-hooks in some subsets of partitions. More
specifically, we obtain the generating functions for the number of $t$-hooks in
doubled distinct partitions and the number of $t$-shifted hooks in strict
partitions. Based on these generating functions, we prove that the number of
$t$-hooks in doubled distinct partitions and the number of $t$-shifted hooks in
strict partitions are both asymptotically normally distributed.","['math.CO', 'math.NT']",False,,,,"Simultaneously decoding the unknown stationary state and function
  parameters for mean field games",On the distribution of $t$-hooks of doubled distinct partitions
neg-d2-242,2025-02-10,,2502.06509," We study phase transitions in $XY$ models, generalized by inclusion of $n$
higher-order pairwise interactions of equal strength, by Monte Carlo
simulation. It is found that by adding new terms the
Berezinskii-Kosterlitz-Thouless (BKT) transition, observed in the standard $XY$
model, gradually changes to the first-order phase transition. We determine the
critical number of terms for which the first-order transition appears as
$n_c=6$. It is also found that for $n=5$ the transition is pseudo-first-order
but it becomes true first-order if the couplings are allowed to increase. In
general, a more rapid increase of the coupling intensity supports the
first-order transition, however, a too fast increase may result in splitting of
the single transition to multiple transitions. Consequently, the minimal number
of the terms required for the change of the BKT phase transition to first order
in the present model with arbitrary couplings is estimated to be $2 < n_c \leq
5$.",['cond-mat.stat-mech'],2501.05797," Time crystals represent a non-equilibrium state of matter with broken
time-translation symmetry that repeats itself at regular time intervals. Though
initially envisioned as a self-generated and self-sustained periodic motion,
their realization has usually required the utilization of external periodic
inputs or modulations. While at first it looked like, for a time crystal to
exist, the initial proposal had to be abandoned, the recent evidence of
inherent time crystals is bringing back the idea of self-generated time crystal
under the spotlight. In this work, we demonstrate the appearance of a
self-generated space-time crystalline order in hybrid Josephson junctions with
the ferromagnet interface without any external influence. The presence of the
exchange and the Dzyaloshinskii-Moriya interactions in a ferromagnet with
broken structural inversion symmetry modifies the current phase relation and
the critical current due to the coupling between the magnetic moment and
Josephson phase. This breaks the time translation symmetry leading to the
appearance of the time-crystalline order in the spatiotemporal dependence of
superconducting current, which evolves with the double of the modulation
frequency. Due to its unique origin and properties, this inherent time
crystalline order stands out from the commonly known classification of time
crystals into discrete and continuous ones. A self-generated time crystal is
demonstrated in two types of hybrid Josephson junctions: the
superconductor-ferromagnet-superconductor on a topological insulator and the
superconductor-three layer ferromagnet-superconductor. Further, we also show
that a recently developed magnetometry device that visualizes a supercurrent
flow in the Josephson junction at the nanoscale can be used as a platform for
experimental detection of space-time crystalline order in hybrid Josephson
junctions.",['cond-mat.supr-con'],False,,,,"Crossover from BKT to first-order transition induced by higher-order
  terms in 2D XY models",Self-generated time crystal in hybrid Josephson junctions
neg-d2-243,2025-02-24,,2502.17132," This paper explores the advancements and applications of large-scale models
in the medical field, with a particular focus on Medical Large Models (MedLMs).
These models, encompassing Large Language Models (LLMs), Vision Models, 3D
Large Models, and Multimodal Models, are revolutionizing healthcare by
enhancing disease prediction, diagnostic assistance, personalized treatment
planning, and drug discovery. The integration of graph neural networks in
medical knowledge graphs and drug discovery highlights the potential of Large
Graph Models (LGMs) in understanding complex biomedical relationships. The
study also emphasizes the transformative role of Vision-Language Models (VLMs)
and 3D Large Models in medical image analysis, anatomical modeling, and
prosthetic design. Despite the challenges, these technologies are setting new
benchmarks in medical innovation, improving diagnostic accuracy, and paving the
way for personalized healthcare solutions. This paper aims to provide a
comprehensive overview of the current state and future directions of large
models in medicine, underscoring their significance in advancing global health.",['cs.AI'],2503.09425," In arXiv:1303.3724, the authors provide an axiomatic way of constructing new
polynomially bounded o-minimal structures. However, all of the structures
satisfying these axioms must also have smooth cell-decomposition. In this
paper, we generalize their approach by allowing weakly smooth germs into the
construction. In particular, we showed in arXiv:2501.17583 that the o-minimal
structure constructed in [O. Le Gal, J.-P. Rolin. ""An o-minimal structure which
does not admit $C^\infty$ cellular decomposition"" Ann. Inst. Fourier 59 (2009),
pp 543-562] satisfies the assumptions of our theorem.",['math.LO'],False,,,,Applications of Large Models in Medicine,"Quasianalytic algebras with weakly smooth germs generate o-minimal
  structures"
neg-d2-244,2025-02-27,,2503.01888," Integrating the structural inductive biases of Graph Neural Networks (GNNs)
with the global contextual modeling capabilities of Transformers represents a
pivotal challenge in graph representation learning. While GNNs excel at
capturing localized topological patterns through message-passing mechanisms,
their inherent limitations in modeling long-range dependencies and
parallelizability hinder their deployment in large-scale scenarios. Conversely,
Transformers leverage self-attention mechanisms to achieve global receptive
fields but struggle to inherit the intrinsic graph structural priors of GNNs.
This paper proposes a novel knowledge distillation framework that
systematically transfers multiscale structural knowledge from GNN teacher
models to Transformer student models, offering a new perspective on addressing
the critical challenges in cross-architectural distillation. The framework
effectively bridges the architectural gap between GNNs and Transformers through
micro-macro distillation losses and multiscale feature alignment. This work
establishes a new paradigm for inheriting graph structural biases in
Transformer architectures, with broad application prospects.","['cs.LG', 'cs.AI']",2502.13997," Style transfer enables the seamless integration of artistic styles from a
style image into a content image, resulting in visually striking and
aesthetically enriched outputs. Despite numerous advances in this field,
existing methods did not explicitly focus on the signature style, which
represents the distinct and recognizable visual traits of the image such as
geometric and structural patterns, color palettes and brush strokes etc. In
this paper, we introduce SigStyle, a framework that leverages the semantic
priors that embedded in a personalized text-to-image diffusion model to capture
the signature style representation. This style capture process is powered by a
hypernetwork that efficiently fine-tunes the diffusion model for any given
single style image. Style transfer then is conceptualized as the reconstruction
process of content image through learned style tokens from the personalized
diffusion model. Additionally, to ensure the content consistency throughout the
style transfer process, we introduce a time-aware attention swapping technique
that incorporates content information from the original image into the early
denoising steps of target image generation. Beyond enabling high-quality
signature style transfer across a wide range of styles, SigStyle supports
multiple interesting applications, such as local style transfer, texture
transfer, style fusion and style-guided text-to-image generation. Quantitative
and qualitative evaluations demonstrate our approach outperforms existing style
transfer methods for recognizing and transferring the signature styles.",['cs.GR'],False,,,,"Enhancing Transformer with GNN Structural Knowledge via Distillation: A
  Novel Approach",SigStyle: Signature Style Transfer via Personalized Text-to-Image Models
neg-d2-245,2025-02-26,,2502.19075," Consider an analyst who models a strategic situation using an incomplete
information game. The true game may involve correlated, duplicated belief
hierarchies, but the analyst lacks knowledge of the correlation structure and
can only approximate each belief hierarchy. To make predictions in this
setting, the analyst uses belief-invariant Bayes correlated equilibria (BIBCE)
and seeks to determine which one is justifiable. We address this question by
introducing the notion of robustness: a BIBCE is robust if, for every nearby
incomplete information game, there exists a BIBCE close to it. Our main result
provides a sufficient condition for robustness using a generalized potential
function. In a supermodular potential game, a robust BIBCE is a Bayes Nash
equilibrium, whereas this need not hold in other classes of games.",['econ.TH'],2502.16534," Large Language Models (LLMs) are becoming increasingly capable across global
languages. However, the ability to communicate across languages does not
necessarily translate to appropriate cultural representations. A key concern is
US-centric bias, where LLMs reflect US rather than local cultural values. We
propose a novel methodology that compares LLM-generated response distributions
against population-level opinion data from the World Value Survey across four
languages (Danish, Dutch, English, and Portuguese). Using a rigorous linear
mixed-effects regression framework, we compare two families of models: Google's
Gemma models (2B--27B parameters) and successive iterations of OpenAI's
turbo-series. Across the families of models, we find no consistent
relationships between language capabilities and cultural alignment. While the
Gemma models have a positive correlation between language capability and
cultural alignment across languages, the OpenAI models do not. Importantly, we
find that self-consistency is a stronger predictor of multicultural alignment
than multilingual capabilities. Our results demonstrate that achieving
meaningful cultural alignment requires dedicated effort beyond improving
general language capabilities.","['cs.CL', 'cs.AI', 'cs.CY']",False,,,,Incomplete Information Robustness,"Multilingual != Multicultural: Evaluating Gaps Between Multilingual
  Capabilities and Cultural Alignment in LLMs"
neg-d2-246,2025-01-02,,2501.01623," The ISCHEMIA Trial randomly assigned patients with ischemic heart disease to
an invasive treatment strategy centered on revascularization with a control
group assigned non-invasive medical therapy. As is common in such ``strategy
trials,'' many participants assigned to treatment remained untreated while many
assigned to control crossed over into treatment. Intention-to-treat (ITT)
analyses of strategy trials preserve randomization-based comparisons, but ITT
effects are diluted by non-compliance. Conventional per-protocol analyses that
condition on treatment received are likely biased by discarding random
assignment. In trials where compliance choices are made shortly after
assignment, instrumental variables (IV) methods solve both problems --
recovering an undiluted average causal effect of treatment for treated subjects
who comply with trial protocol. In ISCHEMIA, however, some controls were
revascularized as long as five years after random assignment. This paper
extends the IV framework for strategy trials, allowing for such dynamic
non-random compliance behavior. IV estimates of long-run revascularization
effects on quality of life are markedly larger than previously reported ITT and
per-protocol estimates. We also show how to estimate complier characteristics
in a dynamic-treatment setting. These estimates reveal increasing selection
bias in naive time-varying per-protocol estimates of revascularization effects.
Compliers have baseline health similar to that of the study population, while
control-group crossovers are far sicker.",['econ.EM'],2503.10174," This paper presents a novel sensitivity-based distributed programming (SBDP)
approach for non-convex, large-scale nonlinear programs (NLP). The algorithm
relies on first-order sensitivities to cooperatively solve the central NLP in a
distributed manner with only neighbor-to-neighbor communication and
parallelizable local computations. The scheme is based on primal decomposition
and offers minimal algorithmic complexity. We derive sufficient local
convergence conditions for non-convex problems. Furthermore, we consider the
SBDP method in a distributed optimal control context and derive favorable
convergence properties in this setting. We illustrate these theoretical
findings and the performance of the proposed algorithm with simulations of
various distributed optimization and control problems.",['math.OC'],False,,,,"Instrumental Variables with Time-Varying Exposure: New Estimates of
  Revascularization Effects on Quality of Life",Sensitivity-Based Distributed Programming for Non-Convex Optimization
neg-d2-247,2025-01-01,,2501.00986," Star formation quenching in galaxies is a critical process in galaxy
formation. It is widely believed that the quenching process is dominated by the
mass of galaxies and/or their environment. In Paper V, we addressed the
challenge to disentangle the effects of mass and environment by employing the
PAC method, which combines spectroscopic and deep photometric surveys. This
approach enabled us to measure the excess surface density of blue and red
galaxies around massive central galaxies down to $10^{9.0}M_{\odot}$. However,
it is not straightforward to completely separate the two effects. To address
this issue, in this paper, we derive the average quenched fraction of central
(isolated) galaxies, $\bar{f}_{\mathrm{q}}^{\mathrm{cen}}(M_{*})$, by combining
the 3D quenched fraction distribution $f^{\mathrm{sat}}_{\mathrm{q}}(r;
M_{*,\mathrm{cen}}, M_{*,\mathrm{sat}})$, reconstructed from the
$\bar{n}_2w_{\mathrm{p}}(r_{\mathrm{p}})$ measurements, with the stellar
mass-halo mass relation in N-body simulations from Paper IV, and the observed
total quenched fraction, $\bar{f}_{\mathrm{q}}^{\mathrm{all}}(M_{*})$. Using
$f^{\mathrm{sat}}_{\mathrm{q}}(r;M_{*,\mathrm{cen}},M_{*,\mathrm{sat}})$,
$\bar{f}_{\mathrm{q}}^{\mathrm{cen}}(M_{*})$, and the galaxy-halo connection,
we assign a quenched probability to each (sub)halo in the simulation, enabling
a comprehensive study of galaxy quenching. We find that the mass-quenched
fraction increases from 0.3 to 0.87 across the stellar mass range $[10^{9.5},
10^{11.0}]M_{\odot}$, while the environmental quenched fraction decreases from
0.17 to 0.03. The mass effect dominates galaxy quenching across the entire
stellar mass range we studied. Moreover, more massive host halos are more
effective at quenching their satellite galaxies, while satellite stellar mass
has minimal influence on environmental quenching.",['astro-ph.GA'],2502.16394," The growing momentum in lunar exploration programs and urgent need for robust
communication systems capable of operating in dust-laden lunar environments
necessitate comprehensive understanding of channel propagation characteristics
in lunar conditions. In this article, we present a comprehensive analysis of
terahertz (THz) channel propagation characteristics through lunar dust
environments, critical for establishing reliable communication and sensing
infrastructure on the Moon. We develop an extended Mie scattering model
incorporating the unique properties of lunar dust particles (Apollo 11 sample
10084, Apollo 14 sample 14003, and Apollo 17 sample 70051), including their
irregular morphology, dielectric characteristics, and charge-dependent
behavior. Through theoretical analysis and experimental verification, we
examine both power and bit error rate (BER) performance across varying dust
conditions. Our results reveal distinct relationships between particle charge
levels, morphological characteristics, and channel performance with power loss
patterns and BER evolution. Our findings provide essential guidelines for
developing robust lunar communication systems that integrate sensing
capabilities, contributing to the establishment of sustainable lunar
infrastructure.",['physics.app-ph'],False,,,,"Photometric Objects Around Cosmic Webs (PAC). VII. Disentangling Mass
  and Environment Quenching with the Aid of Galaxy-halo Connection in
  Simulations",Propagation Performance of Terahertz Channels in Lunar Dust
neg-d2-248,2025-01-14,,2501.17868," Due to its ability to precisely control wireless beams, holographic
multiple-input multiple-output (HMIMO) is expected to be a promising solution
to achieve high-accuracy localization. However, as the scale of HMIMO increases
to improve beam control capability, the corresponding near-field (NF) region
expands, indicating that users may exist in both NF and far-field (FF) regions
with different electromagnetic transmission characteristics. As a result,
existing methods for pure NF or FF localization are no longer applicable. We
consider a hybrid NF and FF localization scenario in this paper, where a base
station (BS) locates multiple users in both NF and FF regions with the aid of a
reconfigurable intelligent surface (RIS), which is a low-cost implementation of
HMIMO. In such a scenario, it is difficult to locate the users and optimize the
RIS phase shifts because whether the location of the user is in the NF or FF
region is unknown, and the channels of different users are coupled. To tackle
this challenge, we propose a RIS-enabled localization method that searches the
users in both NF and FF regions and tackles the coupling issue by jointly
estimating all user locations. We derive the localization error bound by
considering the channel coupling and propose an RIS phase shift optimization
algorithm that minimizes the derived bound. Simulations show the effectiveness
of the proposed method and demonstrate the performance gain compared to pure NF
and FF techniques.",['eess.SP'],2501.08479," Serverless computing offers elasticity unmatched by conventional server-based
cloud infrastructure. Although modern data processing systems embrace
serverless storage, such as Amazon S3, they continue to manage their compute
resources as servers. This is challenging for unpredictable workloads, leaving
clusters often underutilized. Recent research shows the potential of serverless
compute resources, such as cloud functions, for elastic data processing, but
also sees limitations in performance robustness and cost efficiency for long
running workloads. These challenges require holistic approaches across the
system stack. However, to the best of our knowledge, there is no end-to-end
data processing system built entirely on serverless infrastructure. In this
paper, we present Skyrise, our effort towards building the first fully
serverless SQL query processor. Skyrise exploits the elasticity of its
underlying infrastructure, while alleviating the inherent limitations with a
number of adaptive and cost-aware techniques. We show that both Skyrise's
performance and cost are competitive to other cloud data systems for
terabyte-scale queries of the analytical TPC-H benchmark.",['cs.DB'],False,,,,Hybrid Near-field and Far-field Localization with Holographic MIMO,"Skyrise: Exploiting Serverless Cloud Infrastructure for Elastic Data
  Processing"
neg-d2-249,2025-03-14,,2503.11962," Large Language Models (LLMs) may portray discrimination towards certain
individuals, especially those characterized by multiple attributes (aka
intersectional bias). Discovering intersectional bias in LLMs is challenging,
as it involves complex inputs on multiple attributes (e.g. race and gender). To
address this challenge, we propose HInter, a test technique that
synergistically combines mutation analysis, dependency parsing and metamorphic
oracles to automatically detect intersectional bias in LLMs. HInter generates
test inputs by systematically mutating sentences using multiple mutations,
validates inputs via a dependency invariant and detects biases by checking the
LLM response on the original and mutated sentences. We evaluate HInter using
six LLM architectures and 18 LLM models (GPT3.5, Llama2, BERT, etc) and find
that 14.61% of the inputs generated by HInter expose intersectional bias.
Results also show that our dependency invariant reduces false positives
(incorrect test inputs) by an order of magnitude. Finally, we observed that
16.62% of intersectional bias errors are hidden, meaning that their
corresponding atomic cases do not trigger biases. Overall, this work emphasize
the importance of testing LLMs for intersectional bias.","['cs.CL', 'cs.AI']",2503.06664," High-quality, error-free datasets are a key ingredient in building reliable,
accurate, and unbiased machine learning (ML) models. However, real world
datasets often suffer from errors due to sensor malfunctions, data entry
mistakes, or improper data integration across multiple sources that can
severely degrade model performance. Detecting and correcting these issues
typically require tailor-made solutions and demand extensive domain expertise.
Consequently, automation is challenging, rendering the process labor-intensive
and tedious. In this study, we investigate whether Large Language Models (LLMs)
can help alleviate the burden of manual data cleaning. We set up an experiment
in which an LLM, paired with Python, is tasked with cleaning the training
dataset to improve the performance of a learning algorithm without having the
ability to modify the training pipeline or perform any feature engineering. We
run this experiment on multiple Kaggle datasets that have been intentionally
corrupted with errors. Our results show that LLMs can identify and correct
erroneous entries, such as illogical values or outlier, by leveraging
contextual information from other features within the same row, as well as
feedback from previous iterations. However, they struggle to detect more
complex errors that require understanding data distribution across multiple
rows, such as trends and biases.","['cs.LG', 'cs.AI']",False,,,,HInter: Exposing Hidden Intersectional Bias in Large Language Models,Exploring LLM Agents for Cleaning Tabular Machine Learning Datasets
neg-d2-250,2025-01-22,,2501.12647," The recent discovery of superconductivity in La$_3$Ni$_2$O$_7$ and
La$_4$Ni$_3$O$_{10}$ under high pressure stimulates intensive research
interests. These nickelates crystallize in an orthogonal/monoclinic structure
with tilted NiO$_6$ octahedra at ambient pressure and enter a density-wave-like
phase at low temperatures. The application of pressure suppresses the
octahedral tilting and triggers a transition to tetragonal structure (I4/mmm),
which is believed to be a key prerequisite for the emergence of superconducting
state. Here, by developing a high oxidative environment growth technology, we
report the first tetragonal nickelates La$_4$Ni$_3$O$_{10}$ microcrystals
without octahedral tilting at ambient pressure. In tetragonal
La$_4$Ni$_3$O$_{10}$, transport measurements find that both density-wave and
superconducting transitions are absent up to 160 GPa, indicating a robust
tetragonal metallic ground state. Density functional theory calculations reveal
that the band structure of ambient-pressure tetragonal La$_4$Ni$_3$O$_{10}$
involves more $d_{z2}$ orbital contribution to the Fermi surface, compared to
the monoclinic phase or the high-pressure superconducting tetragonal phase. The
concurrent absence of density-wave state and high-pressure superconductivity in
our ambient-pressure tetragonal crystals of La$_4$Ni$_3$O$_{10}$ suggests an
underlying correlation between these two orders. It suggests that the
tetragonal structure is not necessary, while the density-wave state is crucial
for the superconductivity in nickelates. Our findings impose important
constraints on the mechanism of pressure-induced superconductivity in
nickelates and sheds new light on exploring ambient pressure high-temperature
Ni-based superconductors.","['cond-mat.supr-con', 'cond-mat.mtrl-sci']",2503.15548," The widespread adoption of Retrieval-Augmented Generation (RAG) systems in
real-world applications has heightened concerns about the confidentiality and
integrity of their proprietary knowledge bases. These knowledge bases, which
play a critical role in enhancing the generative capabilities of Large Language
Models (LLMs), are increasingly vulnerable to breaches that could compromise
sensitive information. To address these challenges, this paper proposes an
advanced encryption methodology designed to protect RAG systems from
unauthorized access and data leakage. Our approach encrypts both textual
content and its corresponding embeddings prior to storage, ensuring that all
data remains securely encrypted. This mechanism restricts access to authorized
entities with the appropriate decryption keys, thereby significantly reducing
the risk of unintended data exposure. Furthermore, we demonstrate that our
encryption strategy preserves the performance and functionality of RAG
pipelines, ensuring compatibility across diverse domains and applications. To
validate the robustness of our method, we provide comprehensive security proofs
that highlight its resilience against potential threats and vulnerabilities.
These proofs also reveal limitations in existing approaches, which often lack
robustness, adaptability, or reliance on open-source models. Our findings
suggest that integrating advanced encryption techniques into the design and
deployment of RAG systems can effectively enhance privacy safeguards. This
research contributes to the ongoing discourse on improving security measures
for AI-driven services and advocates for stricter data protection standards
within RAG architectures.","['cs.CR', 'cs.AI']",False,,,,"Absence of superconductivity and density-wave transition in
  ambient-pressure tetragonal La$_4$Ni$_3$O$_{10}$",Privacy-Aware RAG: Secure and Isolated Knowledge Retrieval
neg-d2-251,2025-01-05,,2501.02564," Multi-view clustering (MvC) aims to integrate information from different
views to enhance the capability of the model in capturing the underlying data
structures. The widely used joint training paradigm in MvC is potentially not
fully leverage the multi-view information, since the imbalanced and
under-optimized view-specific features caused by the uniform learning objective
for all views. For instance, particular views with more discriminative
information could dominate the learning process in the joint training paradigm,
leading to other views being under-optimized. To alleviate this issue, we first
analyze the imbalanced phenomenon in the joint-training paradigm of multi-view
clustering from the perspective of gradient descent for each view-specific
feature extractor. Then, we propose a novel balanced multi-view clustering
(BMvC) method, which introduces a view-specific contrastive regularization
(VCR) to modulate the optimization of each view. Concretely, VCR preserves the
sample similarities captured from the joint features and view-specific ones
into the clustering distributions corresponding to view-specific features to
enhance the learning process of view-specific feature extractors. Additionally,
a theoretical analysis is provided to illustrate that VCR adaptively modulates
the magnitudes of gradients for updating the parameters of view-specific
feature extractors to achieve a balanced multi-view learning procedure. In such
a manner, BMvC achieves a better trade-off between the exploitation of
view-specific patterns and the exploration of view-invariance patterns to fully
learn the multi-view information for the clustering task. Finally, a set of
experiments are conducted to verify the superiority of the proposed method
compared with state-of-the-art approaches on eight benchmark MvC datasets.","['cs.CV', 'cs.AI', 'cs.LG']",2503.05805," This paper proposes a diffusion-based auto-bidding framework that leverages
graph representations to model large-scale auction environments. In such
settings, agents must dynamically optimize bidding strategies under constraints
defined by key performance indicator (KPI) metrics, all while operating in
competitive environments characterized by uncertain, sparse, and stochastic
variables. To address these challenges, we introduce a novel approach combining
learnable graph-based embeddings with a planning-based latent diffusion model
(LDM). By capturing patterns and nuances underlying the interdependence of
impression opportunities and the multi-agent dynamics of the auction
environment, the graph representation enable expressive computations regarding
auto-bidding outcomes. With reward alignment techniques, the LDM's posterior is
fine-tuned to generate auto-bidding trajectories that maximize KPI metrics
while satisfying constraint thresholds. Empirical evaluations on both
real-world and synthetic auction environments demonstrate significant
improvements in auto-bidding performance across multiple common KPI metrics, as
well as accuracy in forecasting auction outcomes.","['cs.LG', 'cs.AI', 'cs.MA']",False,,,,Balanced Multi-view Clustering,Multi-agent Auto-Bidding with Latent Graph Diffusion Models
neg-d2-252,2025-01-30,,2501.18709," The time-modulated array (TMA) is a simple array architecture in which each
antenna is connected via a multi-throw switch. The switch acts as a modulator
switching state faster than the symbol rate. The phase shifting and beamforming
is achieved by a cyclic shift of the periodical modulating signal across
antennas. In this paper, the TMA mode of operation is proposed to improve the
resolution of a conventional phase shifter. The TMAs are analyzed under
constrained switching frequency being a small multiple of the symbol rate. The
presented generic signal model gives insight into the magnitude, phase and
spacing of the harmonic components generated by the quantized modulating
sequence. It is shown that the effective phase-shifting resolution can be
improved multiplicatively by the oversampling factor ($O$) at the cost of
introducing harmonics. Finally, the array tapering with an oversampled
modulating signal is proposed. The oversampling provides $O+1$ uniformly
distributed tapering amplitudes.",['eess.SP'],2501.14039," As the atomistic simulations of materials science move from traditional
potentials to machine learning interatomic potential (MLIP), the field is
entering the second phase focused on discovering and explaining new material
phenomena. While MLIP development relies on curated data and flexible datasets
from ab-initio simulations, transitioning seamlessly between ab-initio
workflows and MLIP frameworks remains challenging. A global survey was
conducted to understand the current standing (progress and bottleneck) of the
machine learning-guided materials science research. The survey responses have
been implemented to design an open-source software to reduce the access
barriers of MLIP models for the global scientific community. Here, we present
AtomProNet, an open-source Python package that automates obtaining atomic
structures, prepares and submits ab-initio jobs, and efficiently collects
batch-processed data for streamlined neural network (NN) training. Finally, we
compared empirical and start-of-the-art machine learning potential, showing the
practicality of using MLIPs based on computational time and resources.",['cond-mat.mtrl-sci'],False,,,,Beamforming with Oversampled Time-Modulated Arrays,"AtomProNet: Data flow to and from machine learning interatomic
  potentials in materials science"
neg-d2-253,2025-01-21,,2501.11994," The Single Carrier-Frequency Division Multiple Access (SC-FDMA) is a
transmission technique used in the uplink of Long Term Evolution (LTE) and 5G
systems, as it is characterized by reduced transmitted signal envelope
fluctuations in comparison to Orthogonal Frequency Division Multiplexing (OFDM)
technique used in the downlink. This allows for higher energy efficiency of
User Equipments (UEs) while maintaining sufficient signal quality, measured by
Error Vector Magnitude (EVM), at the transmitter. This paper proposes to model
a nonlinear Power Amplifier (PA) influence while optimizing the transmit power
in order to maximize the Signal to Noise and Distortion power Ratio (SNDR) at
the receiver, removing the transmitter-based EVM constraint. An analytic model
of SNDR for the OFDM system and a semi-analytical model for the SC-FDMA system
are provided. Numerical investigations show that the proposed transmit power
optimization allows for improved signal quality at the receiver for both OFDM
and SC-FDMA systems. However, SC-FDMA still outperforms OFDM in this matter.
Such a power amplifier-aware wireless transmitter optimization should be
considered to boost the performance and sustainability of next-generation
wireless systems, including Internet of Things (IoT) ones.",['cs.NI'],2503.09241," Computer agents powered by vision-language models (VLMs) have significantly
advanced human-computer interaction, enabling users to perform complex tasks
through natural language instructions. However, these agents are vulnerable to
context deception attacks, an emerging threat where adversaries embed
misleading content into the agent's operational environment, such as a pop-up
window containing deceptive instructions. Existing defenses, such as
instructing agents to ignore deceptive elements, have proven largely
ineffective. As the first systematic study on protecting computer agents, we
introduce textbf{in-context defense}, leveraging in-context learning and
chain-of-thought (CoT) reasoning to counter such attacks. Our approach involves
augmenting the agent's context with a small set of carefully curated exemplars
containing both malicious environments and corresponding defensive responses.
These exemplars guide the agent to first perform explicit defensive reasoning
before action planning, reducing susceptibility to deceptive attacks.
Experiments demonstrate the effectiveness of our method, reducing attack
success rates by 91.2% on pop-up window attacks, 74.6% on average on
environment injection attacks, while achieving 100% successful defenses against
distracting advertisements. Our findings highlight that (1) defensive reasoning
must precede action planning for optimal performance, and (2) a minimal number
of exemplars (fewer than three) is sufficient to induce an agent's defensive
behavior.",['cs.AI'],False,,,,"Power Amplifier-Aware Transmit Power Optimization for OFDM and SC-FDMA
  Systems",In-Context Defense in Computer Agents: An Empirical Study
neg-d2-254,2025-01-20,,2501.11558," Federated Learning (FL) is a distributed machine learning approach that has
emerged as an effective way to address recent privacy concerns. However, FL
introduces the need for additional security measures as FL alone is still
subject to vulnerabilities such as model and data poisoning and inference
attacks. Confidential Computing (CC) is a paradigm that, by leveraging
hardware-based trusted execution environments (TEEs), protects the
confidentiality and integrity of ML models and data, thus resulting in a
powerful ally of FL applications. Typical TEEs offer an application-isolation
level but suffer many drawbacks, such as limited available memory and debugging
and coding difficulties. The new generation of TEEs offers a virtual machine
(VM)-based isolation level, thus reducing the porting effort for existing
applications. In this work, we compare the performance of VM-based and
application-isolation level TEEs for confidential FL (CFL) applications. In
particular, we evaluate the impact of TEEs and additional security mechanisms
such as TLS (for securing the communication channel). The results, obtained
across three datasets and two deep learning models, demonstrate that the new
VM-based TEEs introduce a limited overhead (at most 1.5x), thus paving the way
to leverage public and untrusted computing environments, such as HPC facilities
or public cloud, without detriment to performance.","['cs.CR', 'cs.PF']",2502.11951," This paper dives into the exciting and rapidly growing field of quantum
computing, explaining its core ideas, current progress, and how it could
revolutionize the way we solve complex problems. It starts by breaking down the
basics, like qubits, quantum circuits, and how principles like superposition
and entanglement make quantum computers fundamentally different-and far more
powerful for certain tasks-than the classical computers we use today. We also
explore how quantum computing deals with complex problems and why it is
uniquely suited for challenges classical systems struggle to handle. A big part
of this paper focuses on Quantum Machine Learning (QML), where the strengths of
quantum computing meet the world of artificial intelligence. By processing
massive datasets and optimizing intricate algorithms, quantum systems offer new
possibilities for machine learning. We highlight different approaches to
combining quantum and classical computing, showing how they can work together
to produce faster and more accurate results. Additionally, we explore the tools
and platforms available-like TensorFlow Quantum, Qiskit and PennyLane-that are
helping researchers and developers bring these theories to life. Of course,
quantum computing has its hurdles. Challenges like scaling up hardware,
correcting errors, and keeping qubits stable are significant roadblocks. Yet,
with rapid advancements in cloud-based platforms and innovative technologies,
the potential of quantum computing feels closer than ever. This paper aims to
offer readers a clear and comprehensive introduction to quantum computing, its
role in machine learning, and the immense possibilities it holds for the future
of technology.","['cs.CE', 'cs.LG', 'quant-ph']",False,,,,"A performance analysis of VM-based Trusted Execution Environments for
  Confidential Federated Learning","Qubit-Based Framework for Quantum Machine Learning: Bridging Classical
  Data and Quantum Algorithms"
neg-d2-255,2025-03-12,,2503.09113," This paper presents a constraint-guided deep learning framework for
developing physically consistent health indicators in bearing prognostics and
health management. Conventional data-driven methods often lack physical
plausibility, while physics-based models are limited by incomplete system
knowledge. To address this, we integrate domain knowledge into deep learning
using constraints to enforce monotonicity, bound output values between 1 and 0
(representing healthy to failed states), and ensure consistency between signal
energy trends and health indicator estimates. This eliminates the need for
complex loss term balancing. We implement constraint-guided gradient descent
within an autoencoder architecture, creating a constrained autoencoder.
However, the framework is adaptable to other architectures. Using
time-frequency representations of accelerometer signals from the Pronostia
dataset, our constrained model generates smoother, more reliable degradation
profiles compared to conventional methods, aligning with expected physical
behavior. Performance is assessed using three metrics: trendability,
robustness, and consistency. Compared to a conventional baseline, the
constrained model improves all three. Another baseline, incorporating
monotonicity via a soft-ranking loss function, outperforms in trendability but
falls short in robustness and consistency. An ablation study confirms that the
monotonicity constraint enhances trendability, the boundary constraint ensures
consistency, and the energy-health consistency constraint improves robustness.
These findings highlight the effectiveness of constraint-guided deep learning
in producing reliable, physically meaningful health indicators, offering a
promising direction for future prognostic applications.","['cs.LG', 'cs.AI']",2503.14723," Code quality is of paramount importance in all types of software development
settings. Our work seeks to enable Machine Learning (ML) engineers to write
better code by helping them find and fix instances of Data Leakage in their
models. Data Leakage often results from bad practices in writing ML code. As a
result, the model effectively ''memorizes'' the data on which it trains,
leading to an overly optimistic estimate of the model performance and an
inability to make generalized predictions. ML developers must carefully
separate their data into training, evaluation, and test sets to avoid
introducing Data Leakage into their code. Training data should be used to train
the model, evaluation data should be used to repeatedly confirm a model's
accuracy, and test data should be used only once to determine the accuracy of a
production-ready model. In this paper, we develop LEAKAGEDETECTOR, a Python
plugin for the PyCharm IDE that identifies instances of Data Leakage in ML code
and provides suggestions on how to remove the leakage.",['cs.SE'],False,,,,"Constraint-Guided Learning of Data-driven Health Indicator Models: An
  Application on the Pronostia Bearing Dataset","LeakageDetector: An Open Source Data Leakage Analysis Tool in Machine
  Learning Pipelines"
neg-d2-256,2025-01-09,,2501.05555," Detecting object-level changes between two images across possibly different
views is a core task in many applications that involve visual inspection or
camera surveillance. Existing change-detection approaches suffer from three
major limitations: (1) lack of evaluation on image pairs that contain no
changes, leading to unreported false positive rates; (2) lack of
correspondences (i.e., localizing the regions before and after a change); and
(3) poor zero-shot generalization across different domains. To address these
issues, we introduce a novel method that leverages change correspondences (a)
during training to improve change detection accuracy, and (b) at test time, to
minimize false positives. That is, we harness the supervision labels of where
an object is added or removed to supervise change detectors, improving their
accuracy over previous work by a large margin. Our work is also the first to
predict correspondences between pairs of detected changes using estimated
homography and the Hungarian algorithm. Our model demonstrates superior
performance over existing methods, achieving state-of-the-art results in change
detection and change correspondence accuracy across both in-distribution and
zero-shot benchmarks.","['cs.CV', 'cs.AI']",2502.1312," Gender-inclusive language is often used with the aim of ensuring that all
individuals, regardless of gender, can be associated with certain concepts.
While psycholinguistic studies have examined its effects in relation to human
cognition, it remains unclear how Large Language Models (LLMs) process
gender-inclusive language. Given that commercial LLMs are gaining an
increasingly strong foothold in everyday applications, it is crucial to examine
whether LLMs in fact interpret gender-inclusive language neutrally, because the
language they generate has the potential to influence the language of their
users. This study examines whether LLM-generated coreferent terms align with a
given gender expression or reflect model biases. Adapting psycholinguistic
methods from French to English and German, we find that in English, LLMs
generally maintain the antecedent's gender but exhibit underlying masculine
bias. In German, this bias is much stronger, overriding all tested
gender-neutralization strategies.","['cs.CL', 'cs.AI']",False,,,,"Improving Zero-Shot Object-Level Change Detection by Incorporating
  Visual Correspondence","Adapting Psycholinguistic Research for LLMs: Gender-inclusive Language
  in a Coreference Context"
neg-d2-257,2025-03-09,,2503.06795," Femoral artery access is essential for numerous clinical procedures,
including diagnostic angiography, therapeutic catheterization, and emergency
interventions. Despite its critical role, successful vascular access remains
challenging due to anatomical variability, overlying adipose tissue, and the
need for precise ultrasound (US) guidance. Errors in needle placement can lead
to severe complications, restricting the procedure to highly skilled clinicians
in controlled hospital settings. While robotic systems have shown promise in
addressing these challenges through autonomous scanning and vessel
reconstruction, clinical translation remains limited due to reliance on
simplified phantom models that fail to capture human anatomical complexity. In
this work, we present a method for autonomous robotic US scanning of bifurcated
femoral arteries, and validate it on five vascular phantoms created from real
patient computed tomography (CT) data. Additionally, we introduce a video-based
deep learning US segmentation network tailored for vascular imaging, enabling
improved 3D arterial reconstruction. The proposed network achieves a Dice score
of 89.21% and an Intersection over Union of 80.54% on a newly developed
vascular dataset. The quality of the reconstructed artery centerline is
evaluated against ground truth CT data, demonstrating an average L2 deviation
of 0.91+/-0.70 mm, with an average Hausdorff distance of 4.36+/-1.11mm. This
study is the first to validate an autonomous robotic system for US scanning of
the femoral artery on a diverse set of patient-specific phantoms, introducing a
more advanced framework for evaluating robotic performance in vascular imaging
and intervention.","['cs.RO', 'cs.CV']",2502.17882," Scientific research is inherently global. However, the vast majority of
academic journals are published exclusively in English, creating barriers for
non-native-English-speaking researchers. In this study, we leverage large
language models (LLMs) to translate published scientific articles while
preserving their native JATS XML formatting, thereby developing a practical,
automated approach for implementation by academic journals. Using our approach,
we translate articles across multiple scientific disciplines into 28 languages.
To evaluate translation accuracy, we introduce a novel question-and-answer (QA)
benchmarking method, in which an LLM generates comprehension-based questions
from the original text and then answers them based on the translated text. Our
benchmark results show an average performance of 95.9%, showing that the key
scientific details are accurately conveyed. In a user study, we translate the
scientific papers of 15 researchers into their native languages, finding that
the authors consistently found the translations to accurately capture the
original information in their articles. Interestingly, a third of the authors
found many technical terms ""overtranslated,"" expressing a preference to keep
terminology more familiar in English untranslated. Finally, we demonstrate how
in-context learning techniques can be used to align translations with
domain-specific preferences such as mitigating overtranslation, highlighting
the adaptability and utility of LLM-driven scientific translation. The code and
translated articles are available at https://hankleid.github.io/ProjectMundo.","['cs.AI', 'cs.CL']",False,,,,"Robotic Ultrasound-Guided Femoral Artery Reconstruction of
  Anatomically-Representative Phantoms","Science Across Languages: Assessing LLM Multilingual Translation of
  Scientific Papers"
neg-d2-258,2025-02-22,,2502.16394," The growing momentum in lunar exploration programs and urgent need for robust
communication systems capable of operating in dust-laden lunar environments
necessitate comprehensive understanding of channel propagation characteristics
in lunar conditions. In this article, we present a comprehensive analysis of
terahertz (THz) channel propagation characteristics through lunar dust
environments, critical for establishing reliable communication and sensing
infrastructure on the Moon. We develop an extended Mie scattering model
incorporating the unique properties of lunar dust particles (Apollo 11 sample
10084, Apollo 14 sample 14003, and Apollo 17 sample 70051), including their
irregular morphology, dielectric characteristics, and charge-dependent
behavior. Through theoretical analysis and experimental verification, we
examine both power and bit error rate (BER) performance across varying dust
conditions. Our results reveal distinct relationships between particle charge
levels, morphological characteristics, and channel performance with power loss
patterns and BER evolution. Our findings provide essential guidelines for
developing robust lunar communication systems that integrate sensing
capabilities, contributing to the establishment of sustainable lunar
infrastructure.",['physics.app-ph'],2501.03137," We investigate the problem of synthesizing distributionally robust control
policies for stochastic systems under safety and reach-avoid specifications.
Using a game-theoretical framework, we consider the setting where the
probability distribution of the disturbance at each time step is selected from
an ambiguity set defined by the Wasserstein distance. The goal is to synthesize
a distributionally robust control policy that ensures the satisfaction
probability exceeds a specified threshold under any distribution within the
ambiguity set. First, for both safety and reach-avoid specifications, we
establish the existence of optimal policies by leveraging the dynamic
programming principles. Then we demonstrate how the associated optimization
problem can be efficiently solved using the dual representation of Wasserstein
distributionally robust optimization. Furthermore, for safety specifications in
particular, we introduce a novel concept of distributionally robust control
barrier certificates and show how these enable the efficient synthesis of
controllers through sum-of-squares programming techniques. Finally, our
experimental results reveal that incorporating distributional robustness during
the synthesis phase significantly improves the satisfaction probability during
online execution, even with limited statistical knowledge of the disturbance
distribution.","['eess.SY', 'cs.SY']",False,,,,Propagation Performance of Terahertz Channels in Lunar Dust,"Distributionally Robust Control Synthesis for Stochastic Systems with
  Safety and Reach-Avoid Specifications"
neg-d2-259,2025-02-05,,2502.03732," Anxiety, depression, and suicidality are common mental health sequelae
following concussion in youth patients, often exacerbating concussion symptoms
and prolonging recovery. Despite the critical need for early detection of these
mental health symptoms, clinicians often face challenges in accurately
collecting patients' mental health data and making clinical decision-making in
a timely manner. Today's remote patient monitoring (RPM) technologies offer
opportunities to objectively monitor patients' activities, but they were not
specifically designed for youth concussion patients; moreover, the large amount
of data collected by RPM technologies may also impose significant workloads on
clinicians to keep up with and use the data. To address these gaps, we employed
a three-stage study consisting of a formative study, interface design, and
design evaluation. We first conducted a formative study through semi-structured
interviews with six highly professional concussion clinicians and identified
clinicians' key challenges in remotely collecting patient information and
accessing patient treatment compliance. Subsequently, we proposed preliminary
clinician-facing interface designs with the integration of AI-based RPM
technologies (AI-RPM), followed by design evaluation sessions with highly
professional concussion clinicians. Clinicians underscored the value of
integrating multi-modal AI-RPM technologies to support clinicians'
decision-making while emphasizing the importance of customizable interfaces
with explainability and multiple responsible design considerations.",['cs.HC'],2503.14723," Code quality is of paramount importance in all types of software development
settings. Our work seeks to enable Machine Learning (ML) engineers to write
better code by helping them find and fix instances of Data Leakage in their
models. Data Leakage often results from bad practices in writing ML code. As a
result, the model effectively ''memorizes'' the data on which it trains,
leading to an overly optimistic estimate of the model performance and an
inability to make generalized predictions. ML developers must carefully
separate their data into training, evaluation, and test sets to avoid
introducing Data Leakage into their code. Training data should be used to train
the model, evaluation data should be used to repeatedly confirm a model's
accuracy, and test data should be used only once to determine the accuracy of a
production-ready model. In this paper, we develop LEAKAGEDETECTOR, a Python
plugin for the PyCharm IDE that identifies instances of Data Leakage in ML code
and provides suggestions on how to remove the leakage.",['cs.SE'],False,,,,"More Modality, More AI: Exploring Design Opportunities of AI-Based
  Multi-modal Remote Monitoring Technologies for Early Detection of Mental
  Health Sequelae in Youth Concussion Patients","LeakageDetector: An Open Source Data Leakage Analysis Tool in Machine
  Learning Pipelines"
neg-d2-260,2025-02-01,,2502.01679," Large Language Models (LLMs) have significantly advanced natural language
processing applications, yet their widespread use raises concerns regarding
inherent biases that may reduce utility or harm for particular social groups.
Despite the advancement in addressing LLM bias, existing research has two major
limitations. First, existing LLM bias evaluation focuses on the U.S. cultural
context, making it challenging to reveal stereotypical biases of LLMs toward
other cultures, leading to unfair development and use of LLMs. Second, current
bias evaluation often assumes models are familiar with the target social
groups. When LLMs encounter words beyond their knowledge boundaries that are
unfamiliar in their training data, they produce irrelevant results in the local
context due to hallucinations and overconfidence, which are not necessarily
indicative of inherent bias. This research addresses these limitations with a
Local Integrated Bias Recognition and Assessment Framework (LIBRA) for
measuring bias using datasets sourced from local corpora without crowdsourcing.
Implementing this framework, we develop a dataset comprising over 360,000 test
cases in the New Zealand context. Furthermore, we propose the Enhanced
Idealized CAT Score (EiCAT), integrating the iCAT score with a beyond knowledge
boundary score (bbs) and a distribution divergence-based bias measurement to
tackle the challenge of LLMs encountering words beyond knowledge boundaries.
Our results show that the BERT family, GPT-2, and Llama-3 models seldom
understand local words in different contexts. While Llama-3 exhibits larger
bias, it responds better to different cultural contexts. The code and dataset
are available at: https://github.com/ipangbo/LIBRA.","['cs.CY', 'cs.CL', 'cs.LG']",2503.15775," A fast method for calculating the reflected and transmitted waves for a given
nonuniform plane wave incident on an arbitrarily oriented and charged planar
interface between two isotropic and possibly lossy media is proposed based on
the decomposition of the complex wave vector and complex wave numbers with
respect to the unit normal vector of the interface. According to the complex
vector analysis, the exact definition of the complex angles of incidence,
reflection and refraction are presented and applied in the complex forms of
Snell's law and Fresnel equations to quickly and correctly calculate the
complex wave vectors and the complex electric fields of the reflected and
refracted waves at a charged interface where the surface charge and current
densities are considered. The calculation procedure and two practical examples
are also given to demonstrate the validity and powerfulness of the proposed
methodology.",['physics.optics'],False,,,,LIBRA: Measuring Bias of Large Language Model from a Local Context,"Fast Calculation of Nonuniform Plane Waves at Arbitrarily Oriented and
  Charged Planar Interfaces of Isotropic Lossy Media"
neg-d2-261,2025-03-11,,2503.08813," In this article we study minimal free resolutions of Gorenstein ideals of
codimension four, using methods coming from representation theory. We introduce
families of higher structure maps associated with such resolution, defined
similarly to the codimension three case. As our main application, we prove that
every Gorenstein ideal of codimension four minimally generated by six elements
is a hyperplane section of a Gorenstein ideal of codimension three,
strengthening a result by Herzog-Miller and Vasconcelos-Villarreal. We state
analogous conjectural results for ideals minimally generated by seven and eight
elements.",['math.AC'],2502.12882," We present efficient classical algorithms to approximate expectation values
and probability amplitudes in linear optical circuits. Specifically, our
classical algorithm efficiently approximates the expectation values of
observables in linear optical circuits for arbitrary product input states
within an additive error under a mild condition. This result suggests that
certain applications of linear optical circuits relying on expectation value
estimation, such as photonic variational algorithms, may face challenges in
achieving quantum advantage. In addition, the (marginal) output probabilities
of boson sampling with arbitrary product input states can be efficiently
approximated using our algorithm, implying that boson sampling can be
efficiently simulated if its output probability distribution is polynomially
sparse. Moreover, our method generalizes Gurvits's algorithm, originally
designed to approximate the permanent, to also approximate the hafnian of
complex symmetric matrices with an additive error. The algorithm also solves a
molecular vibronic spectra problem for arbitrary product input states as
precisely as boson samplers. Finally, our method extends to near-Clifford
circuits, enabling the classical approximation of their expectation values of
any observables and (marginal) output probabilities.",['quant-ph'],False,,,,"Structure theorems for Gorenstein ideals of codimension four with small
  number of generators",Efficient classical algorithms for linear optical circuits
neg-d2-262,2025-01-04,,2501.02262," The growth of a large-scale magnetic field in the Sun and stars is usually
possible when the dynamo number (D) is above a critical value Dc. As the star
ages, its rotation rate and thus D decrease. Hence, the question is how far the
solar dynamo is from the critical dynamo transition. To answer this question,
we have performed a set of simulations using Babcock-Leighton type dynamo
models at different values of dynamo supercriticality and analyzed various
features of magnetic cycle. By comparing the recovery rates of the dynamo from
the Maunder minimum and statistics (numbers and durations) of the grand minima
and maxima with that of observations and we show that the solar dynamo is only
about two times critical and thus not highly supercritical. The observed
correlation between the polar field proxy and the following cycle amplitudes
and Gnevyshev-Ohl rule are also compatible with this conclusion.","['astro-ph.SR', 'physics.plasm-ph', 'physics.space-ph']",2502.16101," Retrieval-augmented generation (RAG) has shown impressive capabilities in
mitigating hallucinations in large language models (LLMs). However, LLMs
struggle to handle misleading retrievals and often fail to maintain their own
reasoning when exposed to conflicting or selectively-framed evidence, making
them vulnerable to real-world misinformation. In such real-world retrieval
scenarios, misleading and conflicting information is rampant, particularly in
the political domain, where evidence is often selectively framed, incomplete,
or polarized. However, existing RAG benchmarks largely assume a clean retrieval
setting, where models succeed by accurately retrieving and generating answers
from gold-standard documents. This assumption fails to align with real-world
conditions, leading to an overestimation of RAG system performance. To bridge
this gap, we introduce RAGuard, a fact-checking dataset designed to evaluate
the robustness of RAG systems against misleading retrievals. Unlike prior
benchmarks that rely on synthetic noise, our dataset constructs its retrieval
corpus from Reddit discussions, capturing naturally occurring misinformation.
It categorizes retrieved evidence into three types: supporting, misleading, and
irrelevant, providing a realistic and challenging testbed for assessing how
well RAG systems navigate different retrieval information. Our benchmark
experiments reveal that when exposed to misleading retrievals, all tested
LLM-powered RAG systems perform worse than their zero-shot baselines (i.e., no
retrieval at all), highlighting their susceptibility to noisy environments. To
the best of our knowledge, RAGuard is the first benchmark to systematically
assess RAG robustness against misleading evidence. We expect this benchmark
will drive future research toward improving RAG systems beyond idealized
datasets, making them more reliable for real-world applications.","['cs.AI', 'cs.IR']",False,,,,"Analyses of features of magnetic cycles at different amounts of dynamo
  supercriticality: Solar dynamo is about two times critical","Worse than Zero-shot? A Fact-Checking Dataset for Evaluating the
  Robustness of RAG Against Misleading Retrievals"
neg-d2-263,2025-03-13,,2503.10865," Predicting the outcomes of species invasions is a central goal of ecology, a
task made especially challenging due to ecological feedbacks. To address this,
we develop a general theory of ecological invasions applicable to a wide
variety of ecological models: including Lotka-Volterra models, consumer
resource models, and models with cross feeding. Importantly, our framework
remains valid even when invading evolved (non-random) communities and accounts
for invasion-driven species extinctions. We derive analytical expressions
relating invasion fitness to invader abundance, shifts in the community, and
extinction probabilities. These results can be understood through a new
quantity we term ``dressed invasion fitness'', which augments the traditional
notion of invasion fitness by incorporating ecological feedbacks. We apply our
theory to analyze short-term evolutionary dynamics through a series of
invasions by mutants whose traits are correlated with an existing parent. We
demonstrate that, generically, mutants and parents can coexist, often by
driving the extinction of low-abundance species. We validate theoretical
predictions against experimental datasets spanning ecosystems from plants to
microbial protists. Our work highlights the central role of ecological
feedbacks in shaping community responses to invasions and mutations, suggesting
that parent-mutant coexistence is widespread in eco-evolutionary dynamics.","['q-bio.PE', 'cond-mat.dis-nn', 'cond-mat.stat-mech']",2502.15504," In this paper, we show a more concise and high level proof than the original
one, derived by researcher Bart Jacobs, for the following theorem: in the
context of Bayesian update rules for learning or updating internal states that
produce predictions, the relative entropy between the observations and the
predictions is reduced when applying Jeffrey's update rule to update the
internal state.","['stat.ML', 'cs.CR']",False,,,,"A theory of ecological invasions and its implications for
  eco-evolutionary dynamics",Jeffrey's update rule as a minimizer of Kullback-Leibler divergence
neg-d2-264,2025-01-15,,2501.09156," Introduction: Substance use disorders (SUDs) have emerged as a pressing
public health crisis in the United States, with adolescent substance use often
leading to SUDs in adulthood. Effective strategies are needed to prevent this
progression. To help in filling this need, we develop a novel and the
first-ever absolute risk prediction model for cannabis use disorder (CUD) for
adolescent or young adult cannabis users.
  Methods: We train a Bayesian machine learning model that provides a
personalized CUD absolute risk for adolescent or young adult cannabis users
using data from the National Longitudinal Study of Adolescent to Adult Health.
Model performance is assessed using 5-fold cross-validation (CV) with area
under the curve (AUC) and ratio of the expected to observed number of cases
(E/O). External validation of the final model is conducted using two
independent datasets.
  Results: The proposed model has five risk factors: biological sex,
delinquency, and scores on personality traits of conscientiousness,
neuroticism, and openness. For predicting CUD risk within five years of first
cannabis use, AUC and E/O, computed via 5-fold CV, were 0.68 and 0.95,
respectively. For the same type of prediction in external validation, AUC
values were 0.64 and 0.75, with E/O values of 0.98 and 1, indicating good
discrimination and calibration performances of the model.
  Discussion and Conclusion: The proposed model is the first absolute risk
prediction model for an SUD. It can aid clinicians in identifying
adolescent/youth substance users at a high risk of developing CUD in future for
clinically appropriate interventions.","['stat.AP', 'stat.ML']",2503.15988," Spectra of vibrational overtone and combination bands from vibrational ground
state of HCNH+ were measured using an action spectroscopy technique with active
background suppression in a cryogenic 22 pole radio frequency ion trap
apparatus. Spectroscopic constants for the upper vibrational levels of the
transitions were determined with vibrational band origins being 6846.77981(90)
$\text{cm}^{-1}$ ($2\nu_1$ , NH stretch), 6640.47624(43) $\text{cm}^{-1}$
($\nu_1 + \nu_2$), 6282.03578(63) $\text{cm}^{-1}$ ($2\nu_2$, CH stretch), and
6588.4894(20) $\text{cm}^{-1}$ ($\nu_2 + \nu_3 + 2\nu_5^0$). State of the art
ab initio VCI calculations up to 10000 $\text{cm}^{-1}$ complement the
experimental data.","['astro-ph.GA', 'physics.atom-ph', 'physics.plasm-ph']",False,,,,"Absolute Risk Prediction for Cannabis Use Disorder Using Bayesian
  Machine Learning",Rovibrational Overtone and Combination Bands of the HCNH+ Ion
neg-d2-265,2025-01-23,,2501.13597," Spectral clustering is a powerful technique for clustering high-dimensional
data, utilizing graph-based representations to detect complex, non-linear
structures and non-convex clusters. The construction of a similarity graph is
essential for ensuring accurate and effective clustering, making graph
structure learning (GSL) central for enhancing spectral clustering performance
in response to the growing demand for scalable solutions. Despite advancements
in GSL, there is a lack of comprehensive surveys specifically addressing its
role within spectral clustering. To bridge this gap, this survey presents a
comprehensive review of spectral clustering methods, emphasizing on the
critical role of GSL. We explore various graph construction techniques,
including pairwise, anchor, and hypergraph-based methods, in both fixed and
adaptive settings. Additionally, we categorize spectral clustering approaches
into single-view and multi-view frameworks, examining their applications within
one-step and two-step clustering processes. We also discuss multi-view
information fusion techniques and their impact on clustering data. By
addressing current challenges and proposing future research directions, this
survey provides valuable insights for advancing spectral clustering
methodologies and highlights the pivotal role of GSL in tackling large-scale
and high-dimensional data clustering tasks.",['cs.LG'],2501.1293," By the planarity rank of a semigroup variety we mean the largest number of
generators of a free semigroup of a variety with respect to which the semigroup
admits a planar Cayley graph. Since the time when L.M.Martynov formulated the
problem of describing the planarity ranks of semigroup varieties, many specific
results have been obtained in this direction. A modular variety of semigroups
is a variety of semigroups with a modular lattice of subvarieties. In this
paper, we calculate the exact values of the planarity ranks of an infinite
countable set of all possible modular varieties of semigroups. It turns out
that these values do not exceed 3. Machine calculations are mostly used in the
proof. Prover9 and Mace4 are used to check the equalities of elements of free
semigroups of varieties defined by a large number of identities. To prove the
non-planarity of graphs, the Pontryagin-Kuratovsky criterion is used, and the
Colin de Verdiere invariant is indirectly used to justify planarity.",['math.RA'],False,,,,"A Comprehensive Survey on Spectral Clustering with Graph Structure
  Learning",Planarity ranks of modular varieties of semigroups
neg-d2-266,2025-03-10,,2503.07446," Principal Component Analysis (PCA), a classical dimensionality reduction
technique, and 2D Gaussian representation, an adaptation of 3D Gaussian
Splatting for image representation, offer distinct approaches to modeling
visual data. We present EigenGS, a novel method that bridges these paradigms
through an efficient transformation pipeline connecting eigenspace and
image-space Gaussian representations. Our approach enables instant
initialization of Gaussian parameters for new images without requiring
per-image optimization from scratch, dramatically accelerating convergence.
EigenGS introduces a frequency-aware learning mechanism that encourages
Gaussians to adapt to different scales, effectively modeling varied spatial
frequencies and preventing artifacts in high-resolution reconstruction.
Extensive experiments demonstrate that EigenGS not only achieves superior
reconstruction quality compared to direct 2D Gaussian fitting but also reduces
necessary parameter count and training time. The results highlight EigenGS's
effectiveness and generalization ability across images with varying resolutions
and diverse categories, making Gaussian-based image representation both
high-quality and viable for real-time applications.",['cs.CV'],2502.08434," We analyze some aspects of the cubic action for gravity recently proposed by
Cheung and Remmen, which is a particular instance of a first order (Palatini)
action. In this approach both the spacetime metric and the connection are
treated as independent fields. We discuss its BRST invariance and compute
explicitly the one-loop contribution of quantum fluctuations around flat space,
checking that the corresponding Slavnov-Taylor identities are fulfilled.
Finally, our results on a first order action are compared with the existing
ones corresponding to a second order action.","['hep-th', 'gr-qc']",False,,,,EigenGS Representation: From Eigenspace to Gaussian Image Space,One loop analysis of the cubic action for gravity
neg-d2-267,2025-03-15,,2503.12118," We consider a periodic quantum clock based on cooperative resonance
fluorescence at zero temperature.
  In the quantum case, this system has an exact steady state and the limit
cycle appears in conditional quantum dynamics under homodyne detection. We show
that the intrinsic quantum phase diffusion on the limit cycle leads to
fluctuations in the period. By simulating the stochastic master equation for
homodyne detection, we extract the statistical properties of the clock period.
We show that the precision of the clock satisfies the quantum-thermodynamic
kinetic uncertainty relations. As energy dissipation increases, the clock
quality improves, fully validating, in a quantum stochastic system, the link
between energy dissipation and clock precision.",['quant-ph'],2501.01513," We theoretically study how the superfluid and condensate deformation of a
weakly interacting ultracold Bose gas evolve during the ramp-up of an external
weak disorder potential. Both resulting deformations turn out to consist of two
distinct contributions, namely a reversible equilibrium one, already predicted
by Huang and Meng in 1992, and a nonequilibrium dynamical one, whose magnitude
depends on the details of the ramping protocol. For the specific case of the
exponential ramp-up protocol, we are able to derive analytical time-dependent
expressions for the above quantities. After a sufficiently long time, a steady
state emerges that is generically out of equilibrium. We take the first step in
investigating its properties by studying its relaxation dynamics. In addition,
we analyze the two-time correlation function and elucidate its relation to the
equilibrium and the dynamical part of the condensate deformation.",['cond-mat.quant-gas'],False,,,,Quantum Thermodynamics on a limit cycle,"Out-of-equilibrium dynamical properties of Bose-Einstein condensates in
  a ramped up weak disorder"
neg-d2-268,2025-03-13,,2503.11721," We show that Laser Interferometer Space Antenna can uniquely identify the
sites of intermediate-mass binary black hole (IMBBH) mergers if they occur in
Active Galactic Nuclei (AGN) disks with a gas density $\rho\geq10^{-12} \, {\rm
g/cc}$ via measurement of dynamical friction effect in the gravitational
waveform. We find that even a single observation of a gravitational wave source
with a total mass of $10^3 M_{\odot}$ and a mass ratio of 2 at a luminosity
distance of 3 Gpc is sufficient to confidently associate the merger to be in an
AGN disk with a density $\sim 10^{-12} \, {\rm g/cc}$, as it allows estimation
of the density with an error bar ${\cal O}(100\%)$. This provides a new way of
inferring AGN disk densities that complement traditional X-ray observations.
Further, we find that neglecting the presence of environmental effects in the
waveform models used for parameter estimation can bias the chirp mass, mass
ratio and arrival time of a merger. If not corrected, this can significantly
impact our ability to carry out multiband data analysis of IMBBHs that combines
information from LISA and the ground-based gravitational wave detectors.","['astro-ph.HE', 'astro-ph.GA', 'gr-qc']",2503.04271," We study LLM judgments of misinformation expressed with uncertainty. Our
experiments study the response of three widely used LLMs (GPT-4o, LlaMA3,
DeepSeek-v2) to misinformation propositions that have been verified false and
then are transformed into uncertain statements according to an uncertainty
typology. Our results show that after transformation, LLMs change their
factchecking classification from false to not-false in 25% of the cases.
Analysis reveals that the change cannot be explained by predictors to which
humans are expected to be sensitive, i.e., modality, linguistic cues, or
argumentation strategy. The exception is doxastic transformations, which use
linguistic cue phrases such as ""It is believed ..."".To gain further insight, we
prompt the LLM to make another judgment about the transformed misinformation
statements that is not related to truth value. Specifically, we study LLM
estimates of the frequency with which people make the uncertain statement. We
find a small but significant correlation between judgment of fact and
estimation of frequency.","['cs.CL', 'cs.CY']",False,,,,"Identifying intermediate mass binary black hole mergers in AGN disks
  using LISA","On Fact and Frequency: LLM Responses to Misinformation Expressed with
  Uncertainty"
neg-d2-269,2025-02-21,,2502.15896," The origin of heavy r-process elements in the universe is still a matter of
great debate, with a confirmed scenario being neutron star (NS) mergers.
Additional relevant sites could be specific classes of events, such as
gamma-ray burst (GRB) Supernovae (SNe), where a central engine could push
neutron-rich material outwards, contributing to the ejecta of the massive
exploding star. Here, we investigate our ability to infer the production of
heavy elements in such scenarios, on the basis of the observed nebular
emission. We solve the steady-state ionization, level population, and thermal
balance, for optically thin ejecta in non-local thermodynamic equilibrium
(NLTE), in order to explore the role of heavy elements in cooling the gas, and
their imprint in the emergent spectrum a few hundreds days post-explosion. We
find that heavy elements would be relevant in the cooling process of the nebula
only if they account for at least $\sim1\%$ of the total ejected mass, at the
typical kinetic temperatures of a few thousands K. However, even in the absence
of such amount, a few $0.1\%$ of the total ejected mass could be instead
sufficient to leave a detectable imprint around $\sim1-10~\mathrm{\mu m}$. This
wavelength range, which would be relatively clean from features due to light
elements, would be instead robustly populated by lines from heavy elements
arising from forbidden transitions in their atomic fine structures. Hence, the
new generation of telescopes, represented by the James Webb Space Telescope
(JWST), will most likely allow for their detection.",['astro-ph.HE'],2502.01679," Large Language Models (LLMs) have significantly advanced natural language
processing applications, yet their widespread use raises concerns regarding
inherent biases that may reduce utility or harm for particular social groups.
Despite the advancement in addressing LLM bias, existing research has two major
limitations. First, existing LLM bias evaluation focuses on the U.S. cultural
context, making it challenging to reveal stereotypical biases of LLMs toward
other cultures, leading to unfair development and use of LLMs. Second, current
bias evaluation often assumes models are familiar with the target social
groups. When LLMs encounter words beyond their knowledge boundaries that are
unfamiliar in their training data, they produce irrelevant results in the local
context due to hallucinations and overconfidence, which are not necessarily
indicative of inherent bias. This research addresses these limitations with a
Local Integrated Bias Recognition and Assessment Framework (LIBRA) for
measuring bias using datasets sourced from local corpora without crowdsourcing.
Implementing this framework, we develop a dataset comprising over 360,000 test
cases in the New Zealand context. Furthermore, we propose the Enhanced
Idealized CAT Score (EiCAT), integrating the iCAT score with a beyond knowledge
boundary score (bbs) and a distribution divergence-based bias measurement to
tackle the challenge of LLMs encountering words beyond knowledge boundaries.
Our results show that the BERT family, GPT-2, and Llama-3 models seldom
understand local words in different contexts. While Llama-3 exhibits larger
bias, it responds better to different cultural contexts. The code and dataset
are available at: https://github.com/ipangbo/LIBRA.","['cs.CY', 'cs.CL', 'cs.LG']",False,,,,Modeling the emission lines from r-process elements in Supernova nebulae,LIBRA: Measuring Bias of Large Language Model from a Local Context
neg-d2-270,2025-02-19,,2503.16451," Modeling human-like action-to-reaction generation has significant real-world
applications, like human-robot interaction and games. Despite recent
advancements in single-person motion generation, it is still challenging to
well handle action-to-reaction generation, due to the difficulty of directly
predicting reaction from action sequence without prompts, and the absence of a
unified representation that effectively encodes multi-person motion. To address
these challenges, we introduce Think-Then-React (TTR), a large
language-model-based framework designed to generate human-like reactions.
First, with our fine-grained multimodal training strategy, TTR is capable to
unify two processes during inference: a thinking process that explicitly infers
action intentions and reasons corresponding reaction description, which serve
as semantic prompts, and a reacting process that predicts reactions based on
input action and the inferred semantic prompts. Second, to effectively
represent multi-person motion in language models, we propose a unified motion
tokenizer by decoupling egocentric pose and absolute space features, which
effectively represents action and reaction motion with same encoding. Extensive
experiments demonstrate that TTR outperforms existing baselines, achieving
significant improvements in evaluation metrics, such as reducing FID from 3.988
to 1.942.","['cs.HC', 'cs.AI', 'cs.RO']",2502.14344," Binary Spiking Neural Networks (BSNNs) inherit the eventdriven paradigm of
SNNs, while also adopting the reduced storage burden of binarization
techniques. These distinct advantages grant BSNNs lightweight and
energy-efficient characteristics, rendering them ideal for deployment on
resource-constrained edge devices. However, due to the binary synaptic weights
and non-differentiable spike function, effectively training BSNNs remains an
open question. In this paper, we conduct an in-depth analysis of the challenge
for BSNN learning, namely the frequent weight sign flipping problem. To
mitigate this issue, we propose an Adaptive Gradient Modulation Mechanism
(AGMM), which is designed to reduce the frequency of weight sign flipping by
adaptively adjusting the gradients during the learning process. The proposed
AGMM can enable BSNNs to achieve faster convergence speed and higher accuracy,
effectively narrowing the gap between BSNNs and their full-precision
equivalents. We validate AGMM on both static and neuromorphic datasets, and
results indicate that it achieves state-of-the-art results among BSNNs. This
work substantially reduces storage demands and enhances SNNs' inherent energy
efficiency, making them highly feasible for resource-constrained environments.",['cs.CV'],False,,,,"Think-Then-React: Towards Unconstrained Human Action-to-Reaction
  Generation","Towards Accurate Binary Spiking Neural Networks: Learning with Adaptive
  Gradient Modulation Mechanism"
neg-d2-271,2025-02-16,,2502.11337," Machine learning applications in high-stakes scenarios should always operate
under human oversight. Developing an optimal combination of human and machine
intelligence requires an understanding of their complementarities, particularly
regarding the similarities and differences in the way they make mistakes. We
perform extensive experiments in the area of face recognition and compare two
automated face recognition systems against human annotators through a
demographically balanced user study. Our research uncovers important ways in
which machine learning errors and human errors differ from each other, and
suggests potential strategies in which human-machine collaboration can improve
accuracy in face recognition.","['cs.HC', 'cs.CV', 'cs.CY']",2501.0279," Reinforcement learning from human feedback (RLHF) has been widely adopted to
align language models (LMs) with human preference. Prior RLHF works typically
take a bandit formulation, which, though intuitive, ignores the sequential
nature of LM generation and can suffer from the sparse reward issue. While
recent works propose dense token-level RLHF, treating each token as an action
may be oversubtle to proper reward assignment. In this paper, we seek to get
the best of both by training and utilizing a segment-level reward model, which
assigns a reward to each semantically complete text segment that spans over a
short sequence of tokens. For reward learning, our method allows dynamic text
segmentation and compatibility with standard sequence-preference datasets. For
effective RL-based LM training against segment reward, we generalize the
classical scalar bandit reward normalizers into location-aware normalizer
functions and interpolate the segment reward for further densification. With
these designs, our method performs competitively on three popular RLHF
benchmarks for LM policy: AlpacaEval 2.0, Arena-Hard, and MT-Bench. Ablation
studies are conducted to further demonstrate our method.","['cs.CL', 'cs.AI']",False,,,,A Comparison of Human and Machine Learning Errors in Face Recognition,"Segmenting Text and Learning Their Rewards for Improved RLHF in Language
  Model"
neg-d2-272,2025-02-03,,2502.01526," It was recently found that connection coefficients of the Heun equation can
be derived in closed form using crossing symmetry in two-dimensional Liouville
theory via the Nekrasov-Shatashvili functions. In this work, we systematize
this approach to second-order linear ODEs of Fuchsian type, which arise in the
description of N = 2, four-dimensional quiver gauge theories. After presenting
the general procedure, we focus on the specific case of Fuchsian equations with
five regular singularities and present some applications to black hole
perturbation theory. First, we consider a massive scalar perturbation of the
Schwarzschild black hole in AdS7. Next, we analyze vector type perturbations of
the Reissner-Nordstr\""om-AdS5 black hole. We also discuss the implications of
our results in the context of the AdS/CFT correspondence and present explicit
results in the large spin limit, where we make connection with the light-cone
bootstrap. Furthermore, using the spectral network technology, we identify the
region of the moduli space in Seiberg-Witten theory that is relevant for the
study of black hole quasinormal modes. Our results suggest that, in some cases,
this region corresponds to the strong-coupling regime, highlighting the
potential applicability of the conformal GMN TBA framework to address scenarios
where the gravitational dictionary implies that the instanton counting
parameters are not parametrically small.","['hep-th', 'gr-qc', 'math-ph', 'math.MP']",2503.07383," Diverse usage patterns induce complex and variable aging behaviors in
lithium-ion batteries, complicating accurate health diagnosis and prognosis.
Separate diagnostic cycles are often used to untangle the battery's current
state of health from prior complex aging patterns. However, these same
diagnostic cycles alter the battery's degradation trajectory, are
time-intensive, and cannot be practically performed in onboard applications. In
this work, we leverage portions of operational measurements in combination with
an interpretable machine learning model to enable rapid, onboard battery health
diagnostics and prognostics without offline diagnostic testing and the
requirement of historical data. We integrate mechanistic constraints within an
encoder-decoder architecture to extract electrode states in a physically
interpretable latent space and enable improved reconstruction of the
degradation path. The health diagnosis model framework can be flexibly applied
across diverse application interests with slight fine-tuning. We demonstrate
the versatility of this model framework by applying it to three battery-cycling
datasets consisting of 422 cells under different operating conditions,
highlighting the utility of an interpretable diagnostic-free, onboard battery
diagnosis and prognosis model.","['eess.SY', 'cs.LG', 'cs.SY']",False,,,,"On quivers, spectral networks and black holes",Diagnostic-free onboard battery health assessment
neg-d2-273,2025-03-20,,2503.16237," Next-generation telescopes will bring groundbreaking discoveries but they
will also present new technological challenges. The Square Kilometre Array
Observatory (SKAO) will be one of the most demanding scientific
infrastructures, with a projected data output of 700 PB per year to be
distributed to a network of SKA Regional Centres. Current tools are not fully
suited to manage such massive data volumes, therefore, new research is required
to transform science archives from data providers into service providers. In
this paper we examine how a science archive can deliver advanced visualisation
capabilities for the SKA science archive. In particular, we have conducted a
thorough exploration of existing visualisation software for astronomy and other
fields to identify tools capable of addressing Big Data requirements. Using
selected technologies, we have developed a prototype archive that provides
access to interactive visualisations of 3D radio data through web-based
interfaces, adhering to International Virtual Observatory Alliance (IVOA)
recommendations to favour interoperability and Open Science practices. In
addition, we discuss how current IVOA recommendations support these
visualisation capabilities and how they could be expanded. Our prototype
archive includes a service to generate 3D models on the fly as a server
operation, enabling remote visualisations in a flexible manner; for instance, a
set of parameters can be used to customise the models and their visualisation.
We have used SKA precursor and pathfinder data to test its usability and
scalability, concluding that remote visualisation is a viable solution for
handling high-volume data. However, our prototype is constrained by memory
limitations, requiring techniques to reduce memory usage.","['astro-ph.IM', 'astro-ph.GA']",2501.04737," The proposed Habitable Worlds Observatory is intended to observe the
atmospheres of nearby terrestrial exoplanets with a resolution greater than
that of any previous instrument. While these observations present a substantial
opportunity for astrobiology, they also incur the risk of false positives and
false negatives. Here, we explore the use of systems science (in the form of
network theory and thermochemical kinetics) to mitigate these risks, and
briefly describe the technical specifications HWO would require in order to use
these methodologies.","['astro-ph.IM', 'astro-ph.EP']",False,,,,"3D radio data visualisation in open science platforms for
  next-generation observatories","Network and Kinetics-based Biosignatures: Implications for the Putative
  Habitable World Observatory Design"
neg-d2-274,2025-01-31,,2502.0011," The improved sensitivity of interferometric facilities to the 21-cm line of
atomic hydrogen (HI) enables studies of its properties in galaxies beyond the
local Universe. In this work, we perform a 21 cm line spectral stacking
analysis combining the MIGHTEE and CHILES surveys in the COSMOS field to derive
a robust HI-stellar mass relation at z=0.36. In particular, by stacking
thousands of star-forming galaxies subdivided into stellar mass bins, we
optimize the signal-to-noise ratio of targets and derive mean HI masses in the
different stellar mass intervals for the investigated galaxy population. We
combine spectra from the two surveys, estimate HI masses, and derive the
scaling relation log10(MHI) = (0.32 +- 0.04)log10(M*) + (6.65 +- 0.36). Our
findings indicate that galaxies at z=0.36 are HI richer than those at z=0, but
HI poorer than those at z=1, with a slope consistent across redshift,
suggesting that stellar mass does not significantly affect HI exchange
mechanisms. We also observe a slower growth rate HI relative to the molecular
gas, supporting the idea that the accretion of cold gas is slower than the rate
of consumption of molecular gas to form stars. This study contributes to
understanding the role of atomic gas in galaxy evolution and sets the stage for
future development of the field in the upcoming SKA era.",['astro-ph.GA'],2503.13664," Defects and interfaces are essential to understand the properties of matter.
However, studying their dynamics in the quantum regime remains a challenge in
particular concerning the regime of two spatial dimensions. Recently, it has
been shown that a quantum counterpart of the hard-disk problem on a lattice
yields defects and interfaces, which are stable just due to quantum effects
while they delocalize and dissolve classically. Here, we study in more detail
the properties of defects and interfaces in this quantum hard-disk problem with
a particular emphasis on the stability of these quantum effects upon including
perturbations. Specifically, we introduce short-range soft-core interactions
between the hard disks. From both analytical arguments and numerical
simulations we find that large classes of defects and interfaces remain stable
even under such perturbations suggesting that the quantum nature of the
dynamics exhibits a large range of robustness. Our findings demonstrate the
stability and non-classical behavior of quantum interface dynamics, offering
insights into the dynamics of two-dimensional quantum matter and establishing
the quantum hard-disk model as a platform for studying unconventional
constrained quantum dynamics.","['quant-ph', 'cond-mat.stat-mech']",False,,,,"New constraints on the evolution of the MHI-M* scaling relation
  combining CHILES and MIGHTEE-HI data",Dynamics of defects and interfaces for interacting quantum hard disks
neg-d2-275,2025-03-08,,2503.10658," The limitations sections of scientific articles play a crucial role in
highlighting the boundaries and shortcomings of research, thereby guiding
future studies and improving research methods. Analyzing these limitations
benefits researchers, reviewers, funding agencies, and the broader academic
community. We introduce LimTopic, a strategy where Topic generation in
Limitation sections in scientific articles with Large Language Models (LLMs).
Here, each topic contains the title and Topic Summary. This study focuses on
effectively extracting and understanding these limitations through topic
modeling and text summarization, utilizing the capabilities of LLMs. We
extracted limitations from research articles and applied an LLM-based topic
modeling integrated with the BERtopic approach to generate a title for each
topic and Topic Sentences. To enhance comprehension and accessibility, we
employed LLM-based text summarization to create concise and generalizable
summaries for each topic Topic Sentences and produce a Topic Summary. Our
experimentation involved prompt engineering, fine-tuning LLM and BERTopic, and
integrating BERTopic with LLM to generate topics, titles, and a topic summary.
We also experimented with various LLMs with BERTopic for topic modeling and
various LLMs for text summarization tasks. Our results showed that the
combination of BERTopic and GPT 4 performed the best in terms of silhouette and
coherence scores in topic modeling, and the GPT4 summary outperformed other LLM
tasks as a text summarizer.","['cs.CL', 'cs.LG']",2501.04201," Weyl semimetal (WSM) thin films possess unique electronic properties that
differ from bulk materials. In this article, we study the nonreciprocal
ballistic transport of the WSM thin films caused by surface modification. We
find that the surface states contribute predominantly to the nonreciprocity,
while the bulk states provide a negative correction. Our calculation shows a
kind of quantum size effect that the nonreciprocal signal decreases as the WSM
film becomes thicker, and diverges when the Fermi energy is near the bottom of
a sub-band. On the other hand, it is found that the density of states in
multi-layer systems possesses some properties roughly independent of thickness.
A single-variable theory is developed to explain it",['cond-mat.mes-hall'],False,,,,"LimTopic: LLM-based Topic Modeling and Text Summarization for Analyzing
  Scientific Articles limitations","Nonreciprocal ballistic transport in multi-layer Weyl Semimetal films
  with surface engineering"
neg-d2-276,2025-02-17,,2502.11524," In this paper we deal with generalizations of the Mahler volume product for
log-concave functions. We show that the polarity transform $\mathcal A$ can be
rescaled so that the Mahler product it induces has upper and lower bounds of
the same asymptotics. We discuss a similar result for the $\mathcal J$
transform.
  As an application, we extend the K\""onig-Milman duality of entropy result to
the class of geometric log-concave functions.",['math.FA'],2502.1612," Data-driven inverse optimization seeks to estimate unknown parameters in an
optimization model from observations of optimization solutions. Many existing
methods are ineffective in handling noisy and suboptimal solution observations
and also suffer from computational challenges. In this paper, we build a
connection between inverse optimization and the Fenchel-Young (FY) loss
originally designed for structured prediction, proposing a FY loss approach to
data-driven inverse optimization. This new approach is amenable to efficient
gradient-based optimization, hence much more efficient than existing methods.
We provide theoretical guarantees for the proposed method and use extensive
simulation and real-data experiments to demonstrate its significant advantage
in parameter estimation accuracy, decision error and computational speed.","['math.OC', 'stat.ML']",False,,,,The Scaled Polarity transform and related inequalities,A Fenchel-Young Loss Approach to Data-Driven Inverse Optimization
neg-d2-277,2025-01-16,,2501.09551," The prediction of solar irradiance enhances reliability in photovoltaic (PV)
solar plant generation and grid integration. In Colombia, PV plants face
penalties if energy production deviates beyond governmental thresholds from
intraday market offers. This research employs Long Short-Term Memory (LSTM) and
Bidirectional-LSTM (Bi-LSTM) models, utilizing meteorological data from a PV
plant in El Paso, Cesar, Colombia, to predict solar irradiance with a 6-hour
horizon and 10-minute resolution. While Bi-LSTM showed superior performance,
the LSTM model achieved comparable results with significantly reduced training
time (6 hours versus 18 hours), making it computationally advantageous. The
LSTM predictions were averaged to create an hourly resolution model, evaluated
using Mean Absolute Error, Root-Mean-Square Error, Normalized Root-Mean-Square
Error, and Mean Absolute Percentage Error metrics. Comparison with the Global
Forecast System (GFS) revealed similar performance, with both models
effectively capturing daily solar irradiance patterns. The forecast model
integrates with an Object-Oriented power production model, enabling accurate
energy offers in the intraday market while minimizing penalty costs.","['cs.LG', 'cs.SY', 'eess.SP', 'eess.SY']",2503.03974," Voter registration systems are a critical - and surprisingly understudied -
element of most high-stakes elections. Despite a history of targeting by
adversaries, relatively little academic work has been done to increase
visibility into how voter registration systems keep voters' data secure,
accurate, and up to date. Enhancing transparency and verifiability could help
election officials and the public detect and mitigate risks to this essential
component of electoral processes worldwide.
  This work introduces cryptographic verifiability for voter registration
systems. Based on consultation with diverse expert stakeholders that support
elections systems, we precisely define the requirements for cryptographic
verifiability in voter registration and systematize the practical challenges
that must be overcome for near-term deployment.
  We then introduce VRLog, the first system to bring strong verifiability to
voter registration. VRLog enables election officials to provide a transparent
log that (1) allows voters to verify that their registration data has not been
tampered with and (2) allows the public to monitor update patterns and database
consistency. We also introduce VRLog$^x$, an enhancement to VRLog that offers
cryptographic privacy to voter deduplication between jurisdictions - a common
maintenance task currently performed in plaintext or using trusted third
parties. Our designs rely on standard, efficient cryptographic primitives, and
are backward compatible with existing voter registration systems. Finally, we
provide an open-source implementation of VRLog and benchmarks to demonstrate
that the system is practical - capable of running on low-cost commodity
hardware and scaling to support databases the size of the largest U.S. state
voter registration systems.",['cs.CR'],False,,,,"Intra-day Solar and Power Forecast for Optimization of Intraday Market
  Participation",Cryptographic Verifiability for Voter Registration Systems
neg-d2-278,2025-03-20,,2503.16003," Mechano-bactericidal (MB) surfaces have been proposed as an emerging strategy
for preventing biofilm formation. Unlike antibiotics and metal ions that
chemically interfere with cellular processes, MB nanostructures cause physical
damage to the bacteria. The antibacterial performance of artificial MB surfaces
relies on rational control of surface features, which is difficult to achieve
for large surfaces in real-life applications. Herein, we report a facile and
scalable method for fabricating MB surfaces based on metal-organic frameworks
(MOFs) using epitaxial MOF-on-MOF hybrids as building blocks with nanopillars
of less than 5 nm tip diameter, 200 nm base diameter, and 300 nm length. Two
methods of MOF surface assembly, in-situ growth and ex-situ dropcasting, result
in surfaces with nanopillars in different orientations, both presenting MB
actions (bactericidal efficiency of 83% for E. coli). Distinct MB mechanisms,
including stretching, impaling, and apoptosis-like death induced by mechanical
injury are discussed with the observed bacterial morphology on the obtained MOF
surfaces.",['physics.bio-ph'],2501.14005," Deep-learning-based face recognition (FR) systems are susceptible to
adversarial examples in both digital and physical domains. Physical attacks
present a greater threat to deployed systems as adversaries can easily access
the input channel, allowing them to provide malicious inputs to impersonate a
victim. This paper addresses the limitations of existing projector-camera-based
adversarial light attacks in practical FR setups. By incorporating device-aware
adaptations into the digital attack algorithm, such as resolution-aware and
color-aware adjustments, we mitigate the degradation from digital to physical
domains. Experimental validation showcases the efficacy of our proposed
algorithm against real and spoof adversaries, achieving high physical
similarity scores in FR models and state-of-the-art commercial systems. On
average, there is only a 14% reduction in scores from digital to physical
attacks, with high attack success rate in both white- and black-box scenarios.","['cs.CV', 'cs.AI']",False,,,,"Mechano-Bactericidal Surfaces Achieved by Epitaxial Growth of
  Metal-Organic Frameworks","Device-aware Optical Adversarial Attack for a Portable Projector-camera
  System"
neg-d2-279,2025-01-31,,2501.19402," We consider the homogeneous mean-field Bose gas at positive temperature. We
show that spontaneous $U(1)$ symmetry breaking occurs if and only if the system
displays Bose-Einstein condensation in the sense that the one-particle density
matrix of the Gibbs state has a macroscopic eigenvalue.","['math-ph', 'math.AP', 'math.MP', 'quant-ph']",2503.02077," Designing effective reward functions in multi-agent reinforcement learning
(MARL) is a significant challenge, often leading to suboptimal or misaligned
behaviors in complex, coordinated environments. We introduce Multi-agent
Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality (M3HF),
a novel framework that integrates multi-phase human feedback of mixed quality
into the MARL training process. By involving humans with diverse expertise
levels to provide iterative guidance, M3HF leverages both expert and non-expert
feedback to continuously refine agents' policies. During training, we
strategically pause agent learning for human evaluation, parse feedback using
large language models to assign it appropriately and update reward functions
through predefined templates and adaptive weight by using weight decay and
performance-based adjustments. Our approach enables the integration of nuanced
human insights across various levels of quality, enhancing the interpretability
and robustness of multi-agent cooperation. Empirical results in challenging
environments demonstrate that M3HF significantly outperforms state-of-the-art
methods, effectively addressing the complexities of reward design in MARL and
enabling broader human participation in the training process.","['cs.MA', 'cs.AI', 'cs.LG']",False,,,,A note on spontaneous symmetry breaking in the mean-field Bose gas,"M3HF: Multi-agent Reinforcement Learning from Multi-phase Human Feedback
  of Mixed Quality"
neg-d2-280,2025-02-25,,2502.17894," Object fetching from cluttered shelves is an important capability for robots
to assist humans in real-world scenarios. Achieving this task demands robotic
behaviors that prioritize safety by minimizing disturbances to surrounding
objects, an essential but highly challenging requirement due to restricted
motion space, limited fields of view, and complex object dynamics. In this
paper, we introduce FetchBot, a sim-to-real framework designed to enable
zero-shot generalizable and safety-aware object fetching from cluttered shelves
in real-world settings. To address data scarcity, we propose an efficient
voxel-based method for generating diverse simulated cluttered shelf scenes at
scale and train a dynamics-aware reinforcement learning (RL) policy to generate
object fetching trajectories within these scenes. This RL policy, which
leverages oracle information, is subsequently distilled into a vision-based
policy for real-world deployment. Considering that sim-to-real discrepancies
stem from texture variations mostly while from geometric dimensions rarely, we
propose to adopt depth information estimated by full-fledged depth foundation
models as the input for the vision-based policy to mitigate sim-to-real gap. To
tackle the challenge of limited views, we design a novel architecture for
learning multi-view representations, allowing for comprehensive encoding of
cluttered shelf scenes. This enables FetchBot to effectively minimize
collisions while fetching objects from varying positions and depths, ensuring
robust and safety-aware operation. Both simulation and real-robot experiments
demonstrate FetchBot's superior generalization ability, particularly in
handling a broad range of real-world scenarios, includ","['cs.RO', 'cs.CV']",2501.04201," Weyl semimetal (WSM) thin films possess unique electronic properties that
differ from bulk materials. In this article, we study the nonreciprocal
ballistic transport of the WSM thin films caused by surface modification. We
find that the surface states contribute predominantly to the nonreciprocity,
while the bulk states provide a negative correction. Our calculation shows a
kind of quantum size effect that the nonreciprocal signal decreases as the WSM
film becomes thicker, and diverges when the Fermi energy is near the bottom of
a sub-band. On the other hand, it is found that the density of states in
multi-layer systems possesses some properties roughly independent of thickness.
A single-variable theory is developed to explain it",['cond-mat.mes-hall'],False,,,,FetchBot: Object Fetching in Cluttered Shelves via Zero-Shot Sim2Real,"Nonreciprocal ballistic transport in multi-layer Weyl Semimetal films
  with surface engineering"
neg-d2-281,2025-02-28,,2502.2122," Explainable AI (XAI) is concerned with how to make AI models more
understandable to people. To date these explanations have predominantly been
technocentric - mechanistic or productivity oriented. This paper introduces the
Explainable AI for the Arts (XAIxArts) manifesto to provoke new ways of
thinking about explainability and AI beyond technocentric discourses.
Manifestos offer a means to communicate ideas, amplify unheard voices, and
foster reflection on practice. To supports the co-creation and revision of the
XAIxArts manifesto we combine a World Caf\'e style discussion format with a
living manifesto to question four core themes: 1) Empowerment, Inclusion, and
Fairness; 2) Valuing Artistic Practice; 3) Hacking and Glitches; and 4)
Openness. Through our interactive living manifesto experience we invite
participants to actively engage in shaping this XIAxArts vision within the CHI
community and beyond.","['cs.HC', 'cs.AI']",2503.0067," Anomaly detection in videos is a challenging task as anomalies in different
videos are of different kinds. Therefore, a promising way to approach video
anomaly detection is by learning the non-anomalous nature of the video at hand.
To this end, we propose a one-class few-shot learning driven transformer based
approach for anomaly detection in videos that is self-context aware. Features
from the first few consecutive non-anomalous frames in a video are used to
train the transformer in predicting the non-anomalous feature of the subsequent
frame. This takes place under the attention of a self-context learned from the
input features themselves. After the learning, given a few previous frames, the
video-specific transformer is used to infer if a frame is anomalous or not by
comparing the feature predicted by it with the actual. The effectiveness of the
proposed method with respect to the state-of-the-art is demonstrated through
qualitative and quantitative results on different standard datasets. We also
study the positive effect of the self-context used in our approach.",['cs.CV'],False,,,,XAIxArts Manifesto: Explainable AI for the Arts,"Transformer Based Self-Context Aware Prediction for Few-Shot Anomaly
  Detection in Videos"
neg-d2-282,2025-02-12,,2502.08768," In this paper a novel frequency-scalable rotary platform design is introduced
which allows for flexible directional channel measurements using different
types of antennas, and which can also be used with frequency extenders for
measurements up to the THz region. The measurement platform has been applied to
measure the channel properties including the direction of arrival at the FR3
frequency 14 GHz and in the D-band at 160 GHz in a large hall indoor
environment with LOS distances up to 40 m. The results show very good agreement
of strong path components for both frequencies as well as interesting
dependencies of delay spread, angular spread, and Ricean K- factor on distance
and frequency and can be used to parameterize a path loss model.",['eess.SP'],2501.15223," We propose an efficient and interpretable neural network with a novel
activation function called the weighted Lehmer transform. This new activation
function enables adaptive feature selection and extends to the complex domain,
capturing phase-sensitive and hierarchical relationships within data. Notably,
it provides greater interpretability and transparency compared to existing
machine learning models, facilitating a deeper understanding of its
functionality and decision-making processes. We analyze the mathematical
properties of both real-valued and complex-valued Lehmer activation units and
demonstrate their applications in modeling nonlinear interactions. Empirical
evaluations demonstrate that our proposed neural network achieves competitive
accuracy on benchmark datasets with significantly improved computational
efficiency. A single layer of real-valued or complex-valued Lehmer activation
units is shown to deliver state-of-the-art performance, balancing efficiency
with interpretability.","['cs.LG', 'cs.AI']",False,,,,"Instantaneous directional channel measurements at 14 GHz and 160 GHz via
  a virtual circular array","Efficient and Interpretable Neural Networks Using Complex Lehmer
  Transform"
neg-d2-283,2025-02-14,,2502.1037," We provide explicit formulas for the Alexander polynomial of Pretzel knots
and establish several immediate corollaries, including the characterization of
Pretzel knots with a trivial Alexander polynomial.",['math.GT'],2501.02844," Text classification is a fundamental task in data mining, pivotal to various
applications such as tabular understanding and recommendation. Although neural
network-based models, such as CNN and BERT, have demonstrated remarkable
performance in text classification, their effectiveness heavily relies on
abundant labeled training data. This dependency makes these models less
effective in dynamic few-shot text classification, where labeled data is
scarce, and new target labels frequently appear based on application needs.
Recently, large language models (LLMs) have shown promise due to their
extensive pretraining and contextual understanding ability. Current approaches
provide LLMs with text inputs, candidate labels, and additional side
information (e.g., descriptions) to classify texts. However, their
effectiveness is hindered by the increased input size and the noise introduced
through side information processing. To address these limitations, we propose a
graph-based online retrieval-augmented generation framework, namely GORAG, for
dynamic few-shot text classification. Rather than treating each input
independently, GORAG constructs and maintains a weighted graph by extracting
side information across all target texts. In this graph, text keywords and
labels are represented as nodes, with edges indicating the correlations between
them. To model these correlations, GORAG employs an edge weighting mechanism to
prioritize the importance and reliability of extracted information and
dynamically retrieves relevant context using a minimum-cost spanning tree
tailored for each text input. Empirical evaluations demonstrate that GORAG
outperforms existing approaches by providing more comprehensive and precise
contextual information.","['cs.CL', 'cs.IR', 'cs.LG']",False,,,,Explicit Formulas for the Alexander Polynomial of Pretzel Knots,"Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text
  Classification"
neg-d2-284,2025-03-22,,2503.17709," GUI agents hold significant potential to enhance the experience and
efficiency of human-device interaction. However, current methods face
challenges in generalizing across applications (apps) and tasks, primarily due
to two fundamental limitations in existing datasets. First, these datasets
overlook developer-induced structural variations among apps, limiting the
transferability of knowledge across diverse software environments. Second, many
of them focus solely on navigation tasks, which restricts their capacity to
represent comprehensive software architectures and complex user interactions.
To address these challenges, we introduce GUI-Xplore, a dataset meticulously
designed to enhance cross-application and cross-task generalization via an
exploration-and-reasoning framework. GUI-Xplore integrates pre-recorded
exploration videos providing contextual insights, alongside five hierarchically
structured downstream tasks designed to comprehensively evaluate GUI agent
capabilities. To fully exploit GUI-Xplore's unique features, we propose
Xplore-Agent, a GUI agent framework that combines Action-aware GUI Modeling
with Graph-Guided Environment Reasoning. Further experiments indicate that
Xplore-Agent achieves a 10% improvement over existing methods in unfamiliar
environments, yet there remains significant potential for further enhancement
towards truly generalizable GUI agents.","['cs.CV', 'cs.AI']",2503.05812," Frontier AI models -- highly capable foundation models at the cutting edge of
AI development -- may pose severe risks to public safety, human rights,
economic stability, and societal value in the coming years. These risks could
arise from deliberate adversarial misuse, system failures, unintended cascading
effects, or simultaneous failures across multiple models.
  In response to such risks, at the AI Seoul Summit in May 2024, 16 global AI
industry organizations signed the Frontier AI Safety Commitments, and 27
nations and the EU issued a declaration on their intent to define these
thresholds. To fulfill these commitments, organizations must determine and
disclose ``thresholds at which severe risks posed by a model or system, unless
adequately mitigated, would be deemed intolerable.''
  To assist in setting and operationalizing intolerable risk thresholds, we
outline key principles and considerations; for example, to aim for ``good, not
perfect'' thresholds in the face of limited data on rapidly advancing AI
capabilities and consequently evolving risks. We also propose specific
threshold recommendations, including some detailed case studies, for a subset
of risks across eight risk categories: (1) Chemical, Biological, Radiological,
and Nuclear (CBRN) Weapons, (2) Cyber Attacks, (3) Model Autonomy, (4)
Persuasion and Manipulation, (5) Deception, (6) Toxicity, (7) Discrimination,
and (8) Socioeconomic Disruption. Our goal is to serve as a starting point or
supplementary resource for policymakers and industry leaders, encouraging
proactive risk management that prioritizes preventing intolerable risks (ex
ante) rather than merely mitigating them after they occur (ex post).","['cs.CY', 'cs.AI', 'cs.CR', 'cs.HC', 'cs.LG']",False,,,,GUI-Xplore: Empowering Generalizable GUI Agents with One Exploration,Intolerable Risk Threshold Recommendations for Artificial Intelligence
neg-d2-285,2025-03-17,,2503.13435," With the rapid development of 3D reconstruction technology, research in 4D
reconstruction is also advancing, existing 4D reconstruction methods can
generate high-quality 4D scenes. However, due to the challenges in acquiring
multi-view video data, the current 4D reconstruction benchmarks mainly display
actions performed in place, such as dancing, within limited scenarios. In
practical scenarios, many scenes involve wide-range spatial movements,
highlighting the limitations of existing 4D reconstruction datasets.
Additionally, existing 4D reconstruction methods rely on deformation fields to
estimate the dynamics of 3D objects, but deformation fields struggle with
wide-range spatial movements, which limits the ability to achieve high-quality
4D scene reconstruction with wide-range spatial movements. In this paper, we
focus on 4D scene reconstruction with significant object spatial movements and
propose a novel 4D reconstruction benchmark, WideRange4D. This benchmark
includes rich 4D scene data with large spatial variations, allowing for a more
comprehensive evaluation of the generation capabilities of 4D generation
methods. Furthermore, we introduce a new 4D reconstruction method, Progress4D,
which generates stable and high-quality 4D results across various complex 4D
scene reconstruction tasks. We conduct both quantitative and qualitative
comparison experiments on WideRange4D, showing that our Progress4D outperforms
existing state-of-the-art 4D reconstruction methods. Project:
https://github.com/Gen-Verse/WideRange4D",['cs.CV'],2503.13568," Autonomous mobile robots are widely used for navigation, transportation, and
inspection tasks indoors and outdoors. In practical situations of limited
satellite signals or poor lighting conditions, navigation depends only on
inertial sensors. In such cases, the navigation solution rapidly drifts due to
inertial measurement errors. In this work, we propose WMINet a wheel-mounted
inertial deep learning approach to estimate the mobile robot's position based
only on its inertial sensors. To that end, we merge two common practical
methods to reduce inertial drift: a wheel-mounted approach and driving the
mobile robot in periodic trajectories. Additionally, we enforce a wheelbase
constraint to further improve positioning performance. To evaluate our proposed
approach we recorded using the Rosbot-XL a wheel-mounted initial dataset
totaling 190 minutes, which is made publicly available. Our approach
demonstrated a 66\% improvement over state-of-the-art approaches. As a
consequence, our approach enables navigation in challenging environments and
bridges the pure inertial gap. This enables seamless robot navigation using
only inertial sensors for short periods.","['cs.RO', 'cs.AI', 'cs.LG']",False,,,,"WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range
  Movements and Scenes","WMINet: A Wheel-Mounted Inertial Learning Approach For Mobile-Robot
  Positioning"
neg-d2-286,2025-03-04,,2503.02432," We analyse light Higgs scalar and pseudoscalar associated hadro-production in
the 2-Higgs Doublet Model (2HDM) Type-X (or lepton-specific) within the
parameter space allowed by theoretical self-consistency requirements as well as
the latest experimental constraints from the Large Hadron Collider (LHC),
precision data and $B$ physics. Over the viable regions of such a scenario, the
Standard Model-like Higgs boson discovered at the LHC in 2012 is the heavier
CP-even state $H$. Furthermore, in the Type-X scenario, due to large
$\tan\beta$, the lighter Higgs scalar $h$ and the pseudoscalar $A$ mainly decay
into two $\tau$ leptons. Therefore, we concentrate on analysing the signal
process $pp\to Z^{*} \to hA\to \tau^{+}\tau^{-}\tau^{+}\tau^{-}\to \ell
\nu_\ell \ell \nu_\ell \tau_h \tau_h$ (where $\ell= e, \mu$ whereas $\tau_h$
represents the hadronic decay of the $\tau$) and explore the feasibility of
conducting such a search at the LHC with a centre-of-mass energy of
$\sqrt{s}~=$ 14 TeV and a luminosity of $L~=~300~fb^{-1}$. To suppress the huge
SM background, we confine ourselves to consider the fraction of signal events
with two same-sign $\tau$ leptons further decaying into same-sign leptons while
the other two $\tau$ leptons decay hadronically. We find that a combination of
kinematical selection and machine learning (ML) analysis will yields
significant sensitivity to this process at the end of the LHC Run 3.",['hep-ph'],2501.03756," Interior models of gas giants in the Solar System traditionally assume a
fully convective molecular hydrogen envelope. However, recent observations from
the Juno mission suggest a possible depletion of alkali metals in Jupiter's
molecular hydrogen envelope, indicating that a stable radiative layer could
exist at the kilobar level. Recent studies propose that deep stable layers help
reconcile various Jupiter observations, including its atmospheric water and CO
abundances and the depth of its zonal winds. However, opacity tables used to
infer stable layers are often outdated and incomplete, leaving the precise
molecular hydrogen envelope composition required for a deep radiative zone
uncertain. In this paper, we determine atmospheric compositions that can lead
to the formation of a radiative zone at the kilobar level in Jupiter and Saturn
today. We computed radiative opacity tables covering pressures up to $10^5$
bar, including the most abundant molecules present in the gas giants of the
Solar System, as well as contributions from free electrons, metal hydrides,
oxides, and atomic species, using the most up-to-date line lists published in
the literature. These tables were used to calculate Rosseland-mean opacities
for the molecular hydrogen envelopes of Jupiter and Saturn, which were then
compared to the critical mean opacity required to maintain convection. We find
that the presence of a radiative zone is controlled by the existence of K, Na,
and NaH in the atmosphere of Jupiter and Saturn. For Jupiter, the elemental
abundance of K and Na must be less than $\sim 10^{-3}$ times solar to form a
radiative zone. In contrast, for Saturn, the required abundance for K and Na is
below $\sim 10^{-4}$ times solar.",['astro-ph.EP'],False,,,,"Analysis of the $ q\bar q\to Z^* \to hA \to4\tau$ process within the
  lepton-specific 2HDM at the LHC","Conditions for radiative zones in the molecular hydrogen envelope of
  Jupiter and Saturn: The role of alkali metals"
neg-d2-287,2025-03-13,,2503.10589," Recent advances in video generation can produce realistic, minute-long
single-shot videos with scalable diffusion transformers. However, real-world
narrative videos require multi-shot scenes with visual and dynamic consistency
across shots. In this work, we introduce Long Context Tuning (LCT), a training
paradigm that expands the context window of pre-trained single-shot video
diffusion models to learn scene-level consistency directly from data. Our
method expands full attention mechanisms from individual shots to encompass all
shots within a scene, incorporating interleaved 3D position embedding and an
asynchronous noise strategy, enabling both joint and auto-regressive shot
generation without additional parameters. Models with bidirectional attention
after LCT can further be fine-tuned with context-causal attention, facilitating
auto-regressive generation with efficient KV-cache. Experiments demonstrate
single-shot models after LCT can produce coherent multi-shot scenes and exhibit
emerging capabilities, including compositional generation and interactive shot
extension, paving the way for more practical visual content creation. See
https://guoyww.github.io/projects/long-context-video/ for more details.",['cs.CV'],2501.05114," A growing number of directly-imaged companions have been recently
characterised, with robust constraints on carbon-to-oxygen ratios and even
isotopic ratios. Many companions and isolated targets have also shown spectral
variability. In this work we observed the super-Jupiter AB~Pictoris~b across
four consecutive nights using VLT/CRIRES+ as part of the ESO SupJup survey,
exploring how the constraints on chemical composition and temperature profile
change over time using spectral line shape variations between nights. We
performed atmospheric retrievals of the high-resolution observations and found
broadly consistent results across all four nights, but there were differences
for some parameters. We clearly detect H$_2$O, $^{12}$CO and $^{13}$CO in each
night, but abundances varied by $\sim2\sigma$, which was correlated to the deep
atmosphere temperature profiles. We also found differences in the
$^{12}$C$/^{13}$C ratios in each night by up to $\sim3\sigma$, which seemed to
be correlated with the cloud deck pressure. Our combined retrieval
simultaneously analysing all nights together constrained broadly the average of
each night individually, with the C/O$=0.59\pm0.01$, consistent with solar
composition, and $^{12}$C$/^{13}$C~$ = 102\pm8$, slightly higher than the ISM
and Solar System values. We also find a low projected rotational velocity,
suggesting that AB~Pictoris~b is either intrinsically a slow rotator due to its
young age or that the spin axis is observed pole-on with a $\sim90^\circ$
misalignment with its orbit inclination. Future observations will be able to
further explore the variability and orbit of AB~Pictoris~b as well as for other
companions.",['astro-ph.EP'],False,,,,Long Context Tuning for Video Generation,"The ESO SupJup Survey V: Exploring Atmospheric Variability and Orbit of
  the Super-Jupiter AB Pictoris b with CRIRES+"
neg-d2-288,2025-01-22,,2501.12915," In this paper, we treat minimal left-invariant unit vector fields on
oscillator group and their relations with the ones that define a harmonic map.
Particularly, if all structure constants of the oscillator group are equal to
each other, then all unit left invariant vector fields that define a harmonic
map into the unit tangent bundle with Sasaki metric are minimal.",['math.DG'],2503.16003," Mechano-bactericidal (MB) surfaces have been proposed as an emerging strategy
for preventing biofilm formation. Unlike antibiotics and metal ions that
chemically interfere with cellular processes, MB nanostructures cause physical
damage to the bacteria. The antibacterial performance of artificial MB surfaces
relies on rational control of surface features, which is difficult to achieve
for large surfaces in real-life applications. Herein, we report a facile and
scalable method for fabricating MB surfaces based on metal-organic frameworks
(MOFs) using epitaxial MOF-on-MOF hybrids as building blocks with nanopillars
of less than 5 nm tip diameter, 200 nm base diameter, and 300 nm length. Two
methods of MOF surface assembly, in-situ growth and ex-situ dropcasting, result
in surfaces with nanopillars in different orientations, both presenting MB
actions (bactericidal efficiency of 83% for E. coli). Distinct MB mechanisms,
including stretching, impaling, and apoptosis-like death induced by mechanical
injury are discussed with the observed bacterial morphology on the obtained MOF
surfaces.",['physics.bio-ph'],False,,,,Minimal unit vector fields on oscillator groups,"Mechano-Bactericidal Surfaces Achieved by Epitaxial Growth of
  Metal-Organic Frameworks"
neg-d2-289,2025-02-24,,2502.17776," Tip-of-the-tongue (TOT) search occurs when a user struggles to recall a
specific identifier, such as a document title. While common, existing search
systems often fail to effectively support TOT scenarios. Research on TOT
retrieval is further constrained by the challenge of collecting queries, as
current approaches rely heavily on community question-answering (CQA) websites,
leading to labor-intensive evaluation and domain bias. To overcome these
limitations, we introduce two methods for eliciting TOT queries - leveraging
large language models (LLMs) and human participants - to facilitate simulated
evaluations of TOT retrieval systems. Our LLM-based TOT user simulator
generates synthetic TOT queries at scale, achieving high correlations with how
CQA-based TOT queries rank TOT retrieval systems when tested in the Movie
domain. Additionally, these synthetic queries exhibit high linguistic
similarity to CQA-derived queries. For human-elicited queries, we developed an
interface that uses visual stimuli to place participants in a TOT state,
enabling the collection of natural queries. In the Movie domain, system rank
correlation and linguistic similarity analyses confirm that human-elicited
queries are both effective and closely resemble CQA-based queries. These
approaches reduce reliance on CQA-based data collection while expanding
coverage to underrepresented domains, such as Landmark and Person. LLM-elicited
queries for the Movie, Landmark, and Person domains have been released as test
queries in the TREC 2024 TOT track, with human-elicited queries scheduled for
inclusion in the TREC 2025 TOT track. Additionally, we provide source code for
synthetic query generation and the human query collection interface, along with
curated visual stimuli used for eliciting TOT queries.","['cs.IR', 'cs.CL', 'cs.HC']",2503.02077," Designing effective reward functions in multi-agent reinforcement learning
(MARL) is a significant challenge, often leading to suboptimal or misaligned
behaviors in complex, coordinated environments. We introduce Multi-agent
Reinforcement Learning from Multi-phase Human Feedback of Mixed Quality (M3HF),
a novel framework that integrates multi-phase human feedback of mixed quality
into the MARL training process. By involving humans with diverse expertise
levels to provide iterative guidance, M3HF leverages both expert and non-expert
feedback to continuously refine agents' policies. During training, we
strategically pause agent learning for human evaluation, parse feedback using
large language models to assign it appropriately and update reward functions
through predefined templates and adaptive weight by using weight decay and
performance-based adjustments. Our approach enables the integration of nuanced
human insights across various levels of quality, enhancing the interpretability
and robustness of multi-agent cooperation. Empirical results in challenging
environments demonstrate that M3HF significantly outperforms state-of-the-art
methods, effectively addressing the complexities of reward design in MARL and
enabling broader human participation in the training process.","['cs.MA', 'cs.AI', 'cs.LG']",False,,,,Tip of the Tongue Query Elicitation for Simulated Evaluation,"M3HF: Multi-agent Reinforcement Learning from Multi-phase Human Feedback
  of Mixed Quality"
neg-d2-290,2025-02-25,,2502.181," A graph $G$ is $\mathcal S_3$-connected if, for any mapping $\beta : V (G)
\mapsto {\mathbb Z}_3$ with $\sum_{v\in V(G)} \beta(v)\equiv 0\pmod3$, there
exists a strongly connected orientation $D$ satisfying
$d^{+}_D(v)-d^{-}_D(v)\equiv \beta(v)\pmod{3}$ for any $v \in V(G)$. It is
known that $\mathcal S_3$-connected graphs are contractible configurations for
the property of flow index strictly less than three. In this paper, we provide
a complete characterization of graphic sequences that have an
$\mathcal{S}_{3}$-connected realization: A graphic sequence $\pi=(d_1,\,
\ldots,\, d_n )$ has an $\mathcal S_3$-connected realization if and only if
$\min \{d_1,\, \ldots,\, d_n\} \ge 4$ and $\sum^n_{i=1}d_i \ge 6n - 4$.
Consequently, every graphic sequence $\pi=(d_1,\, \ldots,\, d_n )$ with $\min
\{d_1,\, \ldots,\, d_n\} \ge 6$ has a realization $G$ with flow index strictly
less than three. This supports a conjecture of Li, Thomassen, Wu and Zhang
[European J. Combin., 70 (2018) 164-177] that every $6$-edge-connected graph
has flow index strictly less than three.",['math.CO'],2503.05656," This article proposes a roadmap to address the current challenges in
small-scale testbeds for Connected and Automated Vehicles (CAVs) and robot
swarms. The roadmap is a joint effort of participants in the workshop ""1st
Workshop on Small-Scale Testbeds for Connected and Automated Vehicles and Robot
Swarms,"" held on June 2 at the IEEE Intelligent Vehicles Symposium (IV) 2024 in
Jeju, South Korea. The roadmap contains three parts: 1) enhancing accessibility
and diversity, especially for underrepresented communities, 2) sharing best
practices for the development and maintenance of testbeds, and 3) connecting
testbeds through an abstraction layer to support collaboration. The workshop
features eight invited speakers, four contributed papers [1]-[4], and a
presentation of a survey paper on testbeds [5]. The survey paper provides an
online comparative table of more than 25 testbeds, available at
https://bassamlab.github.io/testbeds-survey. The workshop's own website is
available at https://cpm-remote.lrt.unibw-muenchen.de/iv24-workshop.","['cs.RO', 'cs.MA']",False,,,,Realizing degree sequences with $\mathcal S_3$-connected graphs,"Small-Scale Testbeds for Connected and Automated Vehicles and Robot
  Swarms: Challenges and a Roadmap"
neg-d2-291,2025-01-16,,2501.09748," In recent years, numerical simulations have become indispensable for
addressing complex astrophysical problems. The MagnetoHydroDynamics (MHD)
framework represents a key tool for investigating the dynamical evolution of
astrophysical plasmas, which are described as a set of partial differential
equations that enforce the conservation of mass, momentum, and energy, along
with Maxwell's equation for the evolution of the electromagnetic fields. Due to
the high nonlinearity of the MHD equations (regardless of their specifications,
e.g., classical/relativistic or ideal/resistive), a general analytical solution
is precluded, making the numerical approach crucial. Numerical simulations
usually end up producing large sets of data files and their scientific analysis
leans on dedicated software designed for data visualization. However, in order
to encompass all of the code output features, specialized tools focusing on the
numerical code may represent a more versatile and built-in tool. Here, we
present PyPLUTO, a Python package tailored for efficient loading, manipulation,
and visualization of outputs produced with the PLUTO code (Mignone et al.,
2007; Mignone et al., 2012). PyPLUTO uses memory mapping to optimize data
loading and provides general routines for data manipulation and visualization.
PyPLUTO also supports the particle modules of the PLUTO code, enabling users to
load and visualize particles, such as cosmic rays (Mignone et al., 2018),
Lagrangian (Vaidya et al., 2018), or dust (Mignone et al., 2019) particles,
from hybrid simulations. A dedicated Graphical User Interface (GUI) simplifies
the generation of single-subplot figures, making PyPLUTO a powerful yet
user-friendly toolkit for astrophysical data analysis.",['astro-ph.IM'],2501.09083," Observations of the circumgalactic medium (CGM) often display coincident
absorption from species with widely varying ionization states, providing direct
evidence for complex, multiphase interactions. Motivated by these measurements,
we perform a series of cloud-crushing simulations that model cold clouds
traveling through the hot CGM. We analyze the ion distributions of these
clouds, generate mock absorption spectra, and study their implications on
quasar (QSO) absorption observations. Our results show interesting multiphase
features, in which ions with significantly different ionization potentials
exist in the same absorber and share similar spectral features. However, our
simulations are unable to explain high ions like O \textsc{vi} and their
coexistence with lower ions that appear in many observed QSO absorption
systems.",['astro-ph.GA'],False,,,,PyPLUTO: a data analysis Python package for the PLUTO code,Ion densities of cold clouds driven by galactic outflows
neg-d2-292,2025-03-05,,2503.03925," In recent years, attempts have been made to extend ISS small-gain theorems
from finite networks to countably infinite, locally finite networks. Under
specific assumptions about the interconnection gains and the ISS formulation,
corresponding infinite-dimensional small-gain results have been proven.
However, concerning these assumptions, the results are still too narrow to be
considered a full extension of the state-of-the-art for finite networks. We
take a step to closing this gap by a thorough investigation of various monotone
operators associated with an infinite network and a specific ISS formulation.
Our results shed more light on the theory of finite networks, yield complete
characterizations of the small-gain condition for specific ISS formulations,
and show which obstacles still have to be overcome to obtain a complete theory
for the most general case.","['math.OC', 'math.DS']",2502.10411," Personalised education is one of the domains that can greatly benefit from
the most recent advances in Artificial Intelligence (AI) and Large Language
Models (LLM). However, it is also one of the most challenging applications due
to the cognitive complexity of teaching effectively while personalising the
learning experience to suit independent learners. We hypothesise that one
promising approach to excelling in such demanding use cases is using a
\emph{society of minds}. In this chapter, we present TrueReason, an exemplar
personalised learning system that integrates a multitude of specialised AI
models that can mimic micro skills that are composed together by a LLM to
operationalise planning and reasoning. The architecture of the initial
prototype is presented while describing two micro skills that have been
incorporated in the prototype. The proposed system demonstrates the first step
in building sophisticated AI systems that can take up very complex cognitive
tasks that are demanded by domains such as education.","['cs.CY', 'cs.AI', 'cs.CL', 'cs.IR', 'cs.MA']",False,,,,The Small-Gain Condition for Infinite Networks,"TrueReason: An Exemplar Personalised Learning System Integrating
  Reasoning with Foundational Models"
neg-d2-293,2025-03-17,,2503.12982," Cooperative perception can increase the view field and decrease the occlusion
of an ego vehicle, hence improving the perception performance and safety of
autonomous driving. Despite the success of previous works on cooperative object
detection, they mostly operate on dense Bird's Eye View (BEV) feature maps,
which are computationally demanding and can hardly be extended to long-range
detection problems. More efficient fully sparse frameworks are rarely explored.
In this work, we design a fully sparse framework, SparseAlign, with three key
features: an enhanced sparse 3D backbone, a query-based temporal context
learning module, and a robust detection head specially tailored for sparse
features. Extensive experimental results on both OPV2V and DairV2X datasets
show that our framework, despite its sparsity, outperforms the state of the art
with less communication bandwidth requirements. In addition, experiments on the
OPV2Vt and DairV2Xt datasets for time-aligned cooperative object detection also
show a significant performance gain compared to the baseline works.",['cs.CV'],2501.03038," Automatic Music Transcription (AMT), aiming to get musical notes from raw
audio, typically uses frame-level systems with piano-roll outputs or language
model (LM)-based systems with note-level predictions. However, frame-level
systems require manual thresholding, while the LM-based systems struggle with
long sequences. In this paper, we propose a hybrid method combining pre-trained
roll-based encoders with an LM decoder to leverage the strengths of both
methods. Besides, our approach employs a hierarchical prediction strategy,
first predicting onset and pitch, then velocity, and finally offset. The
hierarchical prediction strategy reduces computational costs by breaking down
long sequences into different hierarchies. Evaluated on two benchmark
roll-based encoders, our method outperforms traditional piano-roll outputs 0.01
and 0.022 in onset-offset-velocity F1 score, demonstrating its potential as a
performance-enhancing plug-in for arbitrary roll-based music transcription
encoder.","['cs.SD', 'cs.AI', 'cs.LG', 'eess.AS']",False,,,,SparseAlign: A Fully Sparse Framework for Cooperative Object Detection,"Piano Transcription by Hierarchical Language Modeling with Pretrained
  Roll-based Encoders"
neg-d2-294,2025-03-16,,2503.12662," This paper introduces TuneNSearch, a hybrid transfer learning and local
search approach for addressing different variants of vehicle routing problems
(VRP). Recently, multi-task learning has gained much attention for solving VRP
variants. However, this adaptability often compromises the performance of the
models. To address this challenge, we first pre-train a reinforcement learning
model on the multi-depot VRP, followed by a short fine-tuning phase to adapt it
to different variants. By leveraging the complexity of the multi-depot VRP, the
pre-trained model learns richer node representations and gains more
transferable knowledge compared to models trained on simpler routing problems,
such as the traveling salesman problem. TuneNSearch employs, in the first
stage, a Transformer-based architecture, augmented with a residual edge-graph
attention network to capture the impact of edge distances and residual
connections between layers. This architecture allows for a more precise capture
of graph-structured data, improving the encoding of VRP's features. After
inference, our model is also coupled with a second stage composed of a local
search algorithm, which yields substantial performance gains with minimal
computational overhead added. Results show that TuneNSearch outperforms many
existing state-of-the-art models trained for each VRP variant, requiring only
one-fifth of the training epochs. Our approach demonstrates strong
generalization, achieving high performance across different tasks,
distributions and problem sizes, thus addressing a long-standing gap in the
literature.",['cs.LG'],2501.09741," Consider a sample of size $N$ from a population governed by a hierarchical
species sampling model. We study the large $N$ asymptotic behavior of the
number ${\bf K}_N$ of clusters and the number ${\bf M}_{r,N}$ of clusters with
frequency $r$ in the sample. In particular, we show almost sure and $L^p$
convergence for ${\bf M}_{r,N}$, obtain Gaussian fluctuation theorems for ${\bf
K}_N$, and establish large deviation principles for both ${\bf K}_N$ and ${\bf
M}_{r,N}$. Our approach relies on a random sample size representation of the
number of clusters through the corresponding non-hierarchical species sampling
model.",['math.PR'],False,,,,"TuneNSearch: a hybrid transfer learning and local search approach for
  solving vehicle routing problems",Asymptotic behavior of clusters in hierarchical species sampling models
neg-d2-295,2025-02-06,,2502.04436," We report the masses, sizes, and orbital properties of 86 planets orbiting 55
stars observed by NASA's K2 Mission with follow-up Doppler measurements by the
HIRES spectrometer at the W. M. Keck Observatory and the Automated Planet
Finder at Lick Observatory. Eighty-one of the planets were discovered from
their transits in the K2 photometry, while five were found based on subsequent
Doppler measurements of transiting planet host stars. The sizes of the
transiting planets range from Earth-size to larger than Jupiter (1-3 REarth is
typical), while the orbital periods range from less than a day to a few months.
For 32 of the planets, the Doppler signal was detected with significance
greater than 5-sigma (51 were detected with >3-sigma significance). An
important characteristic of this catalog is the use of uniform analysis
procedures to determine stellar and planetary properties. This includes the
transit search and fitting procedures applied to the K2 photometry, the Doppler
fitting techniques applied to the radial velocities, and the spectral modeling
to determine bulk stellar parameters. Such a uniform treatment will make the
catalog useful for statistical studies of the masses, densities, and system
architectures of exoplanetary systems. This work also serves as a data release
for all previously unpublished RVs and associated stellar activity indicators
obtained by our team for these systems, along with derived stellar and planet
parameters.","['astro-ph.EP', 'astro-ph.IM', 'astro-ph.SR']",2503.01924," Adversarial robustness is a critical challenge in deploying deep neural
networks for real-world applications. While adversarial training is a widely
recognized defense strategy, most existing studies focus on balanced datasets,
overlooking the prevalence of long-tailed distributions in real-world data,
which significantly complicates robustness. This paper provides a comprehensive
analysis of adversarial training under long-tailed distributions and identifies
limitations in the current state-of-the-art method, AT-BSL, in achieving robust
performance under such conditions. To address these challenges, we propose a
novel training framework, TAET, which integrates an initial stabilization phase
followed by a stratified equalization adversarial training phase. Additionally,
prior work on long-tailed robustness has largely ignored the crucial evaluation
metric of balanced accuracy. To bridge this gap, we introduce the concept of
balanced robustness, a comprehensive metric tailored for assessing robustness
under long-tailed distributions. Extensive experiments demonstrate that our
method surpasses existing advanced defenses, achieving significant improvements
in both memory and computational efficiency. This work represents a substantial
advancement in addressing robustness challenges in real-world applications. Our
code is available at:
https://github.com/BuhuiOK/TAET-Two-Stage-Adversarial-Equalization-Training-on-Long-Tailed-Distributions.","['cs.LG', 'cs.AI', 'stat.ML']",False,,,,"Planet Masses, Radii, and Orbits from NASA's K2 Mission","TAET: Two-Stage Adversarial Equalization Training on Long-Tailed
  Distributions"
neg-d2-296,2025-03-20,,2503.15879," Non-factoid question-answering (NFQA) poses a significant challenge due to
its open-ended nature, diverse intents, and the need for multi-aspect
reasoning, which renders conventional factoid QA approaches, including
retrieval-augmented generation (RAG), inadequate. Unlike factoid questions,
non-factoid questions (NFQs) lack definitive answers and require synthesizing
information from multiple sources across various reasoning dimensions. To
address these limitations, we introduce Typed-RAG, a type-aware multi-aspect
decomposition framework within the RAG paradigm for NFQA. Typed-RAG classifies
NFQs into distinct types -- such as debate, experience, and comparison -- and
applies aspect-based decomposition to refine retrieval and generation
strategies. By decomposing multi-aspect NFQs into single-aspect sub-queries and
aggregating the results, Typed-RAG generates more informative and contextually
relevant responses. To evaluate Typed-RAG, we introduce Wiki-NFQA, a benchmark
dataset covering diverse NFQ types. Experimental results demonstrate that
Typed-RAG outperforms baselines, thereby highlighting the importance of
type-aware decomposition for effective retrieval and generation in NFQA. Our
code and dataset are available at https://github.com/TeamNLP/Typed-RAG.","['cs.CL', 'cs.IR']",2502.14312," The aim of this paper is to extend Washburn's capillary rise equation by
incorporating a slip condition at the pipe wall. The governing equation is
derived using fundamental principles from continuum mechanics. A new scaling is
introduced, allowing for a systematic analysis of different flow regimes. We
prove the global-in-time existence and uniqueness of a bounded positive
solution to Washburn's equation that includes the slip parameter, as well as
the continuous dependence of the solution in the maximum norm on the initial
data. Thus, the initial-value problem for Washburn's equation is shown to be
well-posed in the sense of Hadamard. Additionally, we show that the unique
equilibrium solution may be reached either monotonically or in an oscillatory
fashion, similarly to the no-slip case. Finally, we determine the basin of
attraction for the system, ensuring that the equilibrium state will be reached
from the initial data we impose. These results hold for any positive value of
the nondimensional slip parameter in the model, and for all values of the ratio
$h_0/h_e$ in the range $[0,3/2]$, where $h_0$ is the initial height of the
fluid column and $h_e$ is its equilibrium height.","['math.AP', 'math-ph', 'math.MP']",False,,,,"Typed-RAG: Type-aware Multi-Aspect Decomposition for Non-Factoid
  Question Answering","Modelling Capillary Rise with a Slip Boundary Condition: Well-posedness
  and Long-time Dynamics of Solutions to Washburn's Equation"
neg-d2-297,2025-02-10,,2502.06974," We characterise when a rank $n$ generalised Baumslag-Solitar group is CAT(0)
and when it is biautomatic.",['math.GR'],2503.05488," Document Key Information Extraction (KIE) is a technology that transforms
valuable information in document images into structured data, and it has become
an essential function in industrial settings. However, current evaluation
metrics of this technology do not accurately reflect the critical attributes of
its industrial applications. In this paper, we present KIEval, a novel
application-centric evaluation metric for Document KIE models. Unlike prior
metrics, KIEval assesses Document KIE models not just on the extraction of
individual information (entity) but also of the structured information
(grouping). Evaluation of structured information provides assessment of
Document KIE models that are more reflective of extracting grouped information
from documents in industrial settings. Designed with industrial application in
mind, we believe that KIEval can become a standard evaluation metric for
developing or applying Document KIE models in practice. The code will be
publicly available.",['cs.CL'],False,,,,Higher-rank GBS groups: non-positive curvature and biautomaticity,KIEval: Evaluation Metric for Document Key Information Extraction
neg-d2-298,2025-01-14,,2501.08533," The complexity of mathematical models describing respiratory mechanics has
grown in recent years, however, parameter identifiability of such models has
only been studied in the last decade in the context of observable data. This
study investigates parameter identifiability of a nonlinear respiratory
mechanics model tuned to the physiology of an extremely preterm infant, using
global Morris screening, local deterministic sensitivity analysis, and singular
value decomposition-based subset selection. The model predicts airflow and
dynamic pulmonary volumes and pressures under varying levels of continuous
positive airway pressure, and a range of parameters characterizing both
surfactant-treated and surfactant-deficient lung. Sensitivity analyses
indicated eleven parameters influence model outputs over the range of
continuous positive airway pressure and lung health scenarios. The model was
adapted to data from a spontaneously breathing 1 kg infant using gradient-based
optimization to estimate the parameter subset characterizing the patient's
state of health.",['q-bio.TO'],2502.19477," Effective field theories (EFTs) parametrize our ignorance of the underlying
UV theory through their Wilson coefficients. However, not all values of these
coefficients are consistent with fundamental physical principles. In this
paper, we explore the consequences of imposing causal propagation on the
comoving curvature perturbation in the EFT of inflation, particularly its
impact on the primordial power spectrum and the effective sound speed
$c_s^\text{eff}$. We investigate scenarios where $c_s^\text{eff}$ undergoes a
transition, remaining consistent with CMB constraints at early times but later
experiencing a drastic change, becoming highly subluminal. Such scenarios allow
the primordial power spectrum to grow at small scales, potentially leading to
the formation of primordial black holes or the generation of scalar-induced
gravitational waves. We find the generic feature that in a causal theory,
luminal sound speeds imply a free theory, effectively constraining the
dynamics. Additionally, we obtain that when considering natural values for the
Wilson coefficients, maintaining the validity of the EFT and the weakly coupled
regime, and enforcing causal propagation of the EFT modes, the power spectrum
cannot increase drastically. This imposes significant constraints on the
parameter space of models aiming to produce such features.","['hep-th', 'astro-ph.CO']",False,,,,"Practical parameter identifiability of respiratory mechanics in the
  extremely preterm infant",Causality Bounds on the Primordial Power Spectrum
neg-d2-299,2025-03-15,,2503.12051," Large language models (LLMs) have made tremendous progress in recent years,
but low-resource languages, such as Tibetan, remain significantly
underrepresented in their evaluation. Despite Tibetan being spoken by over
seven million people, it has largely been neglected in the development and
assessment of LLMs. To address this gap, we present TLUE (A Tibetan Language
Understanding Evaluation Benchmark), the first large-scale benchmark for
assessing LLMs' capabilities in Tibetan. TLUE comprises two major components:
(1) a comprehensive multi-task understanding benchmark spanning 5 domains and
67 subdomains, and (2) a safety benchmark covering 7 subdomains. We evaluate a
diverse set of state-of-the-art LLMs. Experimental results demonstrate that
most LLMs perform below the random baseline, highlighting the considerable
challenges LLMs face in processing Tibetan, a low-resource language. TLUE
provides an essential foundation for driving future research and progress in
Tibetan language understanding and underscores the need for greater inclusivity
in LLM development.",['cs.CL'],2502.03152," In this work, we have carried out lattice simulations of $(2+1)$-flavor QCD
using highly improved staggered quarks at the physical pion mass on $32^3
\times 8$ and $48^3 \times 12$ lattices, with magnetic field strengths ranging
up to 0.8 GeV$^2$ and nonzero baryon chemical potentials employing the Taylor
expansion framework. We present lattice QCD continuum estimate results, along
with the magnetized hadron resonance and ideal gas comparisons, for the
leading-order Taylor expansion coefficients for bulk thermodynamic quantities
such as pressure, number density, energy density, and entropy density, focusing
on the significant impact of strong magnetic fields.","['hep-lat', 'hep-ph', 'hep-th']",False,,,,TLUE: A Tibetan Language Understanding Evaluation Benchmark,"QCD Equation of State with Strong Magnetic Fields and Nonzero Baryon
  Density"
neg-d2-300,2025-01-09,,2501.05015," Adversarial attacks are allegedly unnoticeable. Prior studies have designed
attack noticeability measures on graphs, primarily using statistical tests to
compare the topology of original and (possibly) attacked graphs. However, we
observe two critical limitations in the existing measures. First, because the
measures rely on simple rules, attackers can readily enhance their attacks to
bypass them, reducing their attack ""noticeability"" and, yet, maintaining their
attack performance. Second, because the measures naively leverage global
statistics, such as degree distributions, they may entirely overlook attacks
until severe perturbations occur, letting the attacks be almost ""totally
unnoticeable."" To address the limitations, we introduce HideNSeek, a learnable
measure for graph attack noticeability. First, to mitigate the bypass problem,
HideNSeek learns to distinguish the original and (potential) attack edges using
a learnable edge scorer (LEO), which scores each edge on its likelihood of
being an attack. Second, to mitigate the overlooking problem, HideNSeek
conducts imbalance-aware aggregation of all the edge scores to obtain the final
noticeability score. Using six real-world graphs, we empirically demonstrate
that HideNSeek effectively alleviates the observed limitations, and LEO (i.e.,
our learnable edge scorer) outperforms eleven competitors in distinguishing
attack edges under five different attack methods. For an additional
application, we show that LEO boost the performance of robust GNNs by removing
attack-like edges.","['cs.LG', 'cs.AI']",2501.04946," The least trimmed squares (LTS) estimator is a renowned robust alternative to
the classic least squares estimator and is popular in location, regression,
machine learning, and AI literature. Many studies exist on LTS, including its
robustness, computation algorithms, extension to non-linear cases, asymptotics,
etc. The LTS has been applied in the penalized regression in a high-dimensional
real-data sparse-model setting where dimension $p$ (in thousands) is much
larger than sample size $n$ (in tens, or hundreds). In such a practical
setting, the sample size $n$ often is the count of sub-population that has a
special attribute (e.g. the count of patients of Alzheimer's, Parkinson's,
Leukemia, or ALS, etc.) among a population with a finite fixed size N.
Asymptotic analysis assuming that $n$ tends to infinity is not practically
convincing and legitimate in such a scenario. A non-asymptotic or finite sample
analysis will be more desirable and feasible.
  This article establishes some finite sample (non-asymptotic) error bounds for
estimating and predicting based on LTS with high probability for the first
time.","['stat.ML', 'cs.LG']",False,,,,"On Measuring Unnoticeability of Graph Adversarial Attacks: Observations,
  New Measure, and Applications","Non-asymptotic analysis of the performance of the penalized least
  trimmed squares in sparse models"
neg-d2-301,2025-01-10,,2501.05881," Infrared spectroscopy, e.g., with JWST, provides a glimpse into the chemical
inventory of the innermost region of protoplanetary discs, where terrestrial
planets eventually form. The chemical make-up of regions inside snowlines is
connected to the material drifting from the outer regions, which can be modeled
with dust evolution models. However, infrared observations are limited by the
high dust extinction in the inner disc, and only probes the abundances of
gaseous species in the disc surface layers. As a result, the bulk mass of
delivered volatiles is not directly relatable to what is measured through
infrared spectra. In this paper, we investigate how the delivery of dust and
ice after prolonged pebble drift affects the observable reservoir of water
vapor in the inner disc. We develop a 1+1D approach based on dust evolution
models to determine the delivery and distribution of vapor compared to the
height of the $\tau = 1$ surface in the dust continuum. We find that the
observable column density of water vapor at wavelengths probed by JWST spans
many orders of magnitude over time, exhibiting different radial profiles
depending on dust properties, drift rate, and local processing. In the presence
of a traffic-jam effect inside the snowline, the observable vapor reservoir
appears constant in time despite the ongoing delivery by pebble drift, such
that water is effectively smuggled unnoticed. Differences in measured column
densities then originate not only from variations in bulk vapor content, but
also from differences in the properties and distribution of dust particles.",['astro-ph.EP'],2502.09905," Abdominal aortic aneurysm (AAA) is a life-threatening condition involving the
permanent dilation of the aorta, often detected incidentally through imaging
for some other condition. The standard clinical approach to managing AAA
follows a one-size-fits-all model based on aneurysm size and growth rate,
leading to underestimation or overestimation of rupture risk in individual
patients. The widely studied stress-based rupture risk estimation using
computational biomechanics requires wall strength information. However,
non-invasive methods for local patient-specific wall strength measurement have
not yet been developed. Recently, we introduced an image-based approach for
patient-specific, in vivo, non-invasive AAA kinematic analysis using
time-resolved 3D computed tomography angiography (4D-CTA) images to measure
wall strain throughout the cardiac cycle. In the present study, we integrated
wall tension computation and strain measurement to develop a novel measure of
local structural integrity of AAA wall - Relative Structural Integrity Index
(RSII), independent of material properties and thickness of the wall and
conditions of blood pressure measurement. Our methods provide a visual map of
AAA wall structural integrity for individual patients using only their medical
images and blood pressure data. We applied our methods to twelve patients.
Additionally, we compared our measure of structural integrity of aneurysmal and
non-aneurysmal aortas. Our results show similar values of the wall structural
integrity measure across the patients, indicating the reliability of our
methods. In line with experimental observations reported in the literature, our
analysis revealed that localized low stiffness areas are primarily found in the
most dilated AAA regions. Our results clearly demonstrate that the AAA wall is
stiffer than the non-aneurysmal aorta.",['cs.CE'],False,,,,"Smuggling unnoticed: Towards a 2D view of water and dust delivery to the
  inner regions of protoplanetary discs","Towards personalised assessment of abdominal aortic aneurysm structural
  integrity"
neg-d2-302,2025-02-05,,2502.03188," Code-switching (CS) remains a significant challenge in Natural Language
Processing (NLP), mainly due a lack of relevant data. In the context of the
contact between the Basque and Spanish languages in the north of the Iberian
Peninsula, CS frequently occurs in both formal and informal spontaneous
interactions. However, resources to analyse this phenomenon and support the
development and evaluation of models capable of understanding and generating
code-switched language for this language pair are almost non-existent. We
introduce a first approach to develop a naturally sourced corpus for
Basque-Spanish code-switching. Our methodology consists of identifying CS texts
from previously available corpora using language identification models, which
are then manually validated to obtain a reliable subset of CS instances. We
present the properties of our corpus and make it available under the name
Euska\~nolDS.","['cs.CL', 'cs.AI']",2502.13793," Green hydrogen is likely to play a major role in decarbonising the aviation
industry. It is crucial to understand the effects of microstructure on hydrogen
redistribution, which may be implicated in the embrittlement of candidate fuel
system metals. We have developed a stochastic multiscale finite element
modelling framework that integrates micromechanical and hydrogen transport
models, such that the dominant microstructural effects can be efficiently
accounted for at millimetre length scales. Our results show that microstructure
has a significant effect on hydrogen localisation in elastically anisotropic
materials, which exhibit an interesting interplay between microstructure and
millimetre-scale hydrogen redistribution at various loading rates. Considering
316L stainless steel and nickel, a direct comparison of model predictions
against experimental hydrogen embrittlement data reveals that the reported
sensitivity to loading rate is strongly linked with rate-dependent grain scale
diffusion. These findings highlight the need to incorporate microstructural
characteristics in the design of hydrogen resistant materials.",['cond-mat.mtrl-sci'],False,,,,"Euska\~nolDS: A Naturally Sourced Corpus for Basque-Spanish
  Code-Switching","The link between Microstructural Heterogeneity, Diffusivity, and
  Hydrogen Embrittlement"
neg-d2-303,2025-03-06,,2503.04962," Topological insulators (TIs) are intriguing materials for advanced computing
applications based on spintronics because they can host robust spin effects.
For instance, TIs have intrinsically large spin generation enabled by their
large spin-orbit coupling. Furthermore, topological surface states (TSS) with
spin-momentum locking and Dirac dispersion lead to long spin diffusion. Future
spintronic device technology will require scalable film growth of high-quality
material. We grow epitaxial films of Bi$_{1-x}$Sb$_x$Te$_{3-y}$Se$_y$ (BSTS, $x
= 0.58, y = 1$) and confirm the gapless band structure with optimal doping
using angle-resolved photoelectron spectra. The temperature dependence of
longitudinal resistivity shows bulk transport is suppressed as temperature is
decreased, and at low temperature surface transport dominates. We evaluate the
spin transport properties in BSTS without using ferromagnetic tunnel contacts
via a non-local resistance experiment as a function of temperature and applied
charge current. As expected, these experiments reveal the necessity of
decreasing the bulk conduction to best enhance the spin transport. In the TSS,
we find high efficiency of charge-to-spin conversion (spin Hall angle,
$\theta_{SH} \approx 1$) and spin diffusion over several microns. Further
development of high-quality TIs will make them viable candidates for efficient
and lossless spintronics.","['cond-mat.mes-hall', 'physics.app-ph']",2502.1894," Evaluating the pedagogical capabilities of AI-based tutoring models is
critical for making guided progress in the field. Yet, we lack a reliable,
easy-to-use, and simple-to-run evaluation that reflects the pedagogical
abilities of models. To fill this gap, we present MathTutorBench, an
open-source benchmark for holistic tutoring model evaluation. MathTutorBench
contains a collection of datasets and metrics that broadly cover tutor
abilities as defined by learning sciences research in dialog-based teaching. To
score the pedagogical quality of open-ended teacher responses, we train a
reward model and show it can discriminate expert from novice teacher responses
with high accuracy. We evaluate a wide set of closed- and open-weight models on
MathTutorBench and find that subject expertise, indicated by solving ability,
does not immediately translate to good teaching. Rather, pedagogy and subject
expertise appear to form a trade-off that is navigated by the degree of
tutoring specialization of the model. Furthermore, tutoring appears to become
more challenging in longer dialogs, where simpler questioning strategies begin
to fail. We release the benchmark, code, and leaderboard openly to enable rapid
benchmarking of future models.","['cs.CL', 'cs.AI', 'cs.LG']",False,,,,Intrinsic Spin Transport in a Topological Insulator Thin Film,"MathTutorBench: A Benchmark for Measuring Open-ended Pedagogical
  Capabilities of LLM Tutors"
neg-d2-304,2025-03-01,,2503.00647," Efficient coverage of unknown environments requires robots to adapt their
paths in real time based on on-board sensor data. In this paper, we introduce
CAP, a connectivity-aware hierarchical coverage path planning algorithm for
efficient coverage of unknown environments. During online operation, CAP
incrementally constructs a coverage guidance graph to capture essential
information about the environment. Based on the updated graph, the hierarchical
planner determines an efficient path to maximize global coverage efficiency and
minimize local coverage time. The performance of CAP is evaluated and compared
with five baseline algorithms through high-fidelity simulations as well as
robot experiments. Our results show that CAP yields significant improvements in
coverage time, path length, and path overlap ratio.",['cs.RO'],2503.12647," Molecular communication (MC) offers a groundbreaking approach to
communication inspired by biological signaling. It is particularly suited for
environments where traditional electromagnetic methods fail, such as fluid
mediums or within the human body. This study focuses on addressing a major
challenge in MC systems: inter symbol interference (ISI), which arises due to
the random, diffusive propagation of molecules. We propose a novel technique
that leverages transmission shaping to mitigate ISI effectively by designing
optimal transmission pulse (or sequence) for symbols. Our approach centers on
solving a multi-objective optimization problem that aims to maximize the
separability of individual symbol's responses within the symbol duration while
matching the interference caused by molecular spillover for all symbols. By
making ISI of each symbol similar, the approach reduces the effect of previous
symbols and thus not require any adaptive computations. We introduce a
geometric analogy involving two families of ellipses to derive the optimal
solution. Analytical insights are supported by numerical simulations to design
optimized transmission profiles to enhance the resilience toward ISI. The
proposed transmission shaping method is evaluated through symbol error rate
(SER). These results mark a significant step forward in developing robust and
efficient MC systems, opening doors to advanced applications in bio-inspired
and nano-scale communication technologies.","['q-bio.MN', 'eess.SP']",False,,,,"CAP: A Connectivity-Aware Hierarchical Coverage Path Planning Algorithm
  for Unknown Environments using Coverage Guidance Graph","Optimal Transmission Sequence Design with ISI Matching in Molecular
  Communication"
neg-d2-305,2025-01-13,,2501.0747," The study of numerical rounding errors is often greatly simplified in the
analytical treatment of mathematical problems, or even entirely separated from
it. In sampling theory, for instance, it is standard to assume the availability
of an orthonormal basis for computations, ensuring that numerical errors are
negligible. In reality, however, this assumption is often unmet. In this paper,
we discard it and demonstrate the advantages of integrating numerical insights
more deeply into sampling theory. To clearly pinpoint when the numerical
phenomena play a significant role, we introduce the concept of numerical
redundancy. A set of functions is numerically redundant if it spans a
lower-dimensional space when analysed numerically rather than analytically.
This property makes it generally impossible to compute the best approximation
of a function in its span using finite precision. In contrast,
$\ell^2$-regularized approximations are computable and, therefore, form the
foundation of many practical methods. Regularization generally reduces accuracy
compared to the best approximation, but our analysis shows that there is a
benefit: it also significantly reduces the amount of data needed for accurate
approximation. Furthermore, we present a constructive method for optimally
selecting data points for $L^2$-approximations, explicitly accounting for the
effects of regularization. The results are illustrated for two common scenarios
that lead to numerical redundancy: (1) approximations on irregular domains and
(2) approximations that incorporate specific features of the function to be
approximated. In doing so, we obtain new results on random sampling for Fourier
extension frames. Finally, we establish that regularization is implicit in
numerical orthogonalization of a numerically redundant set, indicating that its
analysis cannot be bypassed in a much broader range of methods.","['math.NA', 'cs.NA']",2501.01515," Motivated by deep learning regimes with multiple interacting yet distinct
model components, we introduce learning diagrams, graphical depictions of
training setups that capture parameterized learning as data rather than code. A
learning diagram compiles to a unique loss function on which component models
are trained. The result of training on this loss is a collection of models
whose predictions ``agree"" with one another. We show that a number of popular
learning setups such as few-shot multi-task learning, knowledge distillation,
and multi-modal learning can be depicted as learning diagrams. We further
implement learning diagrams in a library that allows users to build diagrams of
PyTorch and Flux.jl models. By implementing some classic machine learning use
cases, we demonstrate how learning diagrams allow practitioners to build
complicated models as compositions of smaller components, identify
relationships between workflows, and manipulate models during or after
training. Leveraging a category theoretic framework, we introduce a rigorous
semantics for learning diagrams that puts such operations on a firm
mathematical foundation.","['cs.LG', 'cs.AI', 'cs.PL', 'math.CT']",False,,,,Sampling Theory for Function Approximation with Numerical Redundancy,"DiagrammaticLearning: A Graphical Language for Compositional Training
  Regimes"
neg-d2-306,2025-01-20,,2501.11698," A next-generation medium-energy gamma-ray telescope targeting the MeV range
would address open questions in astrophysics regarding how extreme conditions
accelerate cosmic-ray particles, produce relativistic jet outflows, and more.
One concept, AMEGO-X, relies upon the mission-enabling CMOS Monolithic Active
Pixel Sensor silicon chip AstroPix. AstroPix is designed for space-based use,
featuring low noise, low power consumption, and high scalability. Desired
performance of the device include an energy resolution of 5 keV (or 10% FWHM)
at 122 keV and a dynamic range per-pixel of 25-700 keV, enabled by the addition
of a high-voltage bias to each pixel which supports a depletion depth of 500
um. This work reports on the status of the AstroPix development process with
emphasis on the current version under test, version three (v3), and highlights
of version two (v2). Version 3 achieves energy resolution of 10.4 +\- 3.2 % at
59.5 keV and 94 +\- 6 um depletion in a low-resistivity test silicon substrate.","['astro-ph.IM', 'physics.ins-det']",2501.19176," Full-Field Digital Mammography (FFDM) is the primary imaging modality for
routine breast cancer screening; however, its effectiveness is limited in
patients with dense breast tissue or fibrocystic conditions. Contrast-Enhanced
Spectral Mammography (CESM), a second-level imaging technique, offers enhanced
accuracy in tumor detection. Nonetheless, its application is restricted due to
higher radiation exposure, the use of contrast agents, and limited
accessibility. As a result, CESM is typically reserved for select cases,
leaving many patients to rely solely on FFDM despite the superior diagnostic
performance of CESM. While biopsy remains the gold standard for definitive
diagnosis, it is an invasive procedure that can cause discomfort for patients.
We introduce a multimodal, multi-view deep learning approach for virtual
biopsy, integrating FFDM and CESM modalities in craniocaudal and mediolateral
oblique views to classify lesions as malignant or benign. To address the
challenge of missing CESM data, we leverage generative artificial intelligence
to impute CESM images from FFDM scans. Experimental results demonstrate that
incorporating the CESM modality is crucial to enhance the performance of
virtual biopsy. When real CESM data is missing, synthetic CESM images proved
effective, outperforming the use of FFDM alone, particularly in multimodal
configurations that combine FFDM and CESM modalities. The proposed approach has
the potential to improve diagnostic workflows, providing clinicians with
augmented intelligence tools to improve diagnostic accuracy and patient care.
Additionally, as a contribution to the research community, we publicly release
the dataset used in our experiments, facilitating further advancements in this
field.","['eess.IV', 'cs.AI', 'cs.CV']",False,,,,"AstroPix: A Pixelated HVCMOS Sensor for Space-Based Gamma-Ray
  Measurement","Augmented Intelligence for Multimodal Virtual Biopsy in Breast Cancer
  Using Generative Artificial Intelligence"
neg-d2-307,2025-02-20,,2502.14673," Deploying ASR models at an industrial scale poses significant challenges in
hardware resource management, especially for long-form transcription tasks
where audio may last for hours. Large Conformer models, despite their
capabilities, are limited to processing only 15 minutes of audio on an 80GB
GPU. Furthermore, variable input lengths worsen inefficiencies, as standard
batching leads to excessive padding, increasing resource consumption and
execution time. To address this, we introduce ChunkFormer, an efficient ASR
model that uses chunk-wise processing with relative right context, enabling
long audio transcriptions on low-memory GPUs. ChunkFormer handles up to 16
hours of audio on an 80GB GPU, 1.5x longer than the current state-of-the-art
FastConformer, while also boosting long-form transcription performance with up
to 7.7% absolute reduction on word error rate and maintaining accuracy on
shorter tasks compared to Conformer. By eliminating the need for padding in
standard batching, ChunkFormer's masked batching technique reduces execution
time and memory usage by more than 3x in batch processing, substantially
reducing costs for a wide range of ASR systems, particularly regarding GPU
resources for models serving in real-world applications.","['cs.SD', 'eess.AS']",2501.16011," Legal texts, characterized by complex and specialized terminology, present a
significant challenge for Language Models. Adding an underrepresented language,
such as Spanish, to the mix makes it even more challenging. While pre-trained
models like XLM-RoBERTa have shown capabilities in handling multilingual
corpora, their performance on domain specific documents remains underexplored.
This paper presents the development and evaluation of MEL, a legal language
model based on XLM-RoBERTa-large, fine-tuned on legal documents such as BOE
(Bolet\'in Oficial del Estado, the Spanish oficial report of laws) and congress
texts. We detail the data collection, processing, training, and evaluation
processes. Evaluation benchmarks show a significant improvement over baseline
models in understanding the legal Spanish language. We also present case
studies demonstrating the model's application to new legal texts, highlighting
its potential to perform top results over different NLP tasks.",['cs.CL'],False,,,,"ChunkFormer: Masked Chunking Conformer For Long-Form Speech
  Transcription",MEL: Legal Spanish Language Model
neg-d2-308,2025-03-02,,2503.0081," In our quest for a reinforcement learning (RL) algorithm that is both
practical and provably optimal, we introduce EQO (Exploration via
Quasi-Optimism). Unlike existing minimax optimal approaches, EQO avoids
reliance on empirical variances and employs a simple bonus term proportional to
the inverse of the state-action visit count. Central to EQO is the concept of
quasi-optimism, where estimated values need not be fully optimistic, allowing
for a simpler yet effective exploration strategy. The algorithm achieves the
sharpest known regret bound for tabular RL under the mildest assumptions,
proving that fast convergence can be attained with a practical and
computationally efficient approach. Empirical evaluations demonstrate that EQO
consistently outperforms existing algorithms in both regret performance and
computational efficiency, providing the best of both theoretical soundness and
practical effectiveness.","['cs.LG', 'stat.ML']",2502.10411," Personalised education is one of the domains that can greatly benefit from
the most recent advances in Artificial Intelligence (AI) and Large Language
Models (LLM). However, it is also one of the most challenging applications due
to the cognitive complexity of teaching effectively while personalising the
learning experience to suit independent learners. We hypothesise that one
promising approach to excelling in such demanding use cases is using a
\emph{society of minds}. In this chapter, we present TrueReason, an exemplar
personalised learning system that integrates a multitude of specialised AI
models that can mimic micro skills that are composed together by a LLM to
operationalise planning and reasoning. The architecture of the initial
prototype is presented while describing two micro skills that have been
incorporated in the prototype. The proposed system demonstrates the first step
in building sophisticated AI systems that can take up very complex cognitive
tasks that are demanded by domains such as education.","['cs.CY', 'cs.AI', 'cs.CL', 'cs.IR', 'cs.MA']",False,,,,Minimax Optimal Reinforcement Learning with Quasi-Optimism,"TrueReason: An Exemplar Personalised Learning System Integrating
  Reasoning with Foundational Models"
neg-d2-309,2025-03-14,,2503.11889," Background: The nuclear shell model offers realistic predictions of nuclear
structure starting from (quasi-) proton and neutron degrees of freedom, but
relies on coupling constants (interaction matrix elements) that must be fit to
experiment. To extend the shell model's applicability across the nuclear chart,
and specifically toward the driplines, we must first be able to efficiently
test new interaction matrix elements and assign credible uncertainties.
  Purpose: We develop and test a framework to efficiently fit new shell model
interactions and obtain credible uncertainties. We further demonstrate its use
by validating the uncertainty estimates of the known \textit{sd}-shell
effective interactions.
  Methods: We use eigenvector continuation to emulate solutions to the exact
shell model. First, we use the emulator to replicate earlier results using a
well-known linear-combination chi-squared minimization algorithm. Then, we
employ a modern Markov Chain Monte Carlo method to test for nonlinearities in
the observable posterior distributions, which previous sensitivity analyses
precluded.
  Results: The emulator reproduces the USDB interaction within a small margin
of error, allowing for the quantification of the matrix element uncertainty.
However, we find that to obtain credible predictive intervals the model defect
of the shell model itself, rather than experimental or emulator
uncertainty/error, must be taken into account.
  Conclusions: Eigenvector continuation can be used to accelerate fitting shell
model interactions. We confirm that the linear approximation used to develop
interactions in the past is indeed sufficient. However, we find that typical
assumptions about the likelihood function must be modified in order to obtain a
credible uncertainty-quantified interaction.",['nucl-th'],2503.09329," This work explores an extension of ML-optimized piecewise polynomial
approximation by incorporating energy optimization as an additional objective.
Traditional closed-form solutions enable continuity and approximation targets
but lack flexibility in accommodating complex optimization goals. By leveraging
modern gradient descent optimizers within TensorFlow, we introduce a framework
that minimizes total curvature in cam profiles, leading to smoother motion and
reduced energy consumption for input data that is unfavorable for sole
approximation and continuity optimization. Experimental results confirm the
effectiveness of this approach, demonstrating its potential to improve
efficiency in scenarios where input data is noisy or suboptimal for
conventional methods.",['cs.LG'],False,,,,Towards shell model interactions with credible uncertainties,"Energy Optimized Piecewise Polynomial Approximation Utilizing Modern
  Machine Learning Optimizers"
neg-d2-310,2025-03-17,,2503.12861," Let $p>3$ be a prime, $a_1,a_2,a_3\in\Bbb Z$ and let
$N_p(x^3+a_1x^2+a_2x+a_3)$ denote the number of solutions to the congruence
$x^3+a_1x^2+a_2x+a_3\equiv 0\pmod p$. In this paper, we give an explicit
criterion for $N_p(x^3+a_1x^2+a_2x+a_3)=3$ via binary quadratic forms.",['math.NT'],2501.05555," Detecting object-level changes between two images across possibly different
views is a core task in many applications that involve visual inspection or
camera surveillance. Existing change-detection approaches suffer from three
major limitations: (1) lack of evaluation on image pairs that contain no
changes, leading to unreported false positive rates; (2) lack of
correspondences (i.e., localizing the regions before and after a change); and
(3) poor zero-shot generalization across different domains. To address these
issues, we introduce a novel method that leverages change correspondences (a)
during training to improve change detection accuracy, and (b) at test time, to
minimize false positives. That is, we harness the supervision labels of where
an object is added or removed to supervise change detectors, improving their
accuracy over previous work by a large margin. Our work is also the first to
predict correspondences between pairs of detected changes using estimated
homography and the Hungarian algorithm. Our model demonstrates superior
performance over existing methods, achieving state-of-the-art results in change
detection and change correspondence accuracy across both in-distribution and
zero-shot benchmarks.","['cs.CV', 'cs.AI']",False,,,,Cubic congruences and binary quadratic forms,"Improving Zero-Shot Object-Level Change Detection by Incorporating
  Visual Correspondence"
neg-d2-311,2025-03-21,,2503.17342," We present an updated reconstruction of the dark energy equation of state,
$w(a)$, using the newly released DESI DR2 Baryon Acoustic Oscillation (BAO)
data in combination with Pantheon+ and DES5Y Type Ia supernovae measurements,
respectively. Building on our previous analysis in arXiv:2503.08658, which
employed a nonparametric flexknot reconstruction approach, we examine whether
the evidence for dynamical dark energy persists with the improved precision of
the DESI DR2 dataset. We find that while the overall qualitative structure of
$w(a)$ remains consistent with our earlier findings, the statistical support
for dynamical dark energy is reduced when considering DESI DR2 data alone,
particularly for more complex flexknot models with higher numbers of knots.
However, the evidence for simpler dynamical models, such as $w$CDM and CPL
(which correspond to $n=1$ and $n=2$ knots respectively), increases relative to
$\Lambda$CDM with DESI DR2 alone, consistent with previous DESI analyses. When
combined with Pantheon+ data, the conclusions remain broadly consistent with
our earlier work, but the inclusion of DES5Y supernovae data leads to an
increase of preference for flexknot models with more than two knots, placing
$w$CDM and CPL on par with $\Lambda$CDM.",['astro-ph.CO'],2503.0139," Crash consistency is essential for applications that must persist data.
Crash-consistency testing has been commonly applied to find crash-consistency
bugs in applications. The crash-state space grows exponentially as the number
of operations in the program increases, necessitating techniques for pruning
the search space. However, state-of-the-art crash-state space pruning is far
from ideal. Some techniques look for known buggy patterns or bound the
exploration for efficiency, but they sacrifice coverage and may miss bugs
lodged deep within applications. Other techniques eliminate redundancy in the
search space by skipping identical crash states, but they still fail to scale
to larger applications.
  In this work, we propose representative testing: a new crash-state space
reduction strategy that achieves high scalability and high coverage. Our key
observation is that the consistency of crash states is often correlated, even
if those crash states are not identical. We build Pathfinder, a
crash-consistency testing tool that implements an update behaviors-based
heuristic to approximate a small set of representative crash states.
  We evaluate Pathfinder on POSIX-based and MMIO-based applications, where it
finds 18 (7 new) bugs across 8 production-ready systems. Pathfinder scales more
effectively to large applications than prior works and finds 4x more bugs in
POSIX-based applications and 8x more bugs in MMIO-based applications compared
to state-of-the-art systems.","['cs.OS', 'cs.PL', 'cs.SE']",False,,,,"Comparison of dynamical dark energy with {\Lambda}CDM in light of DESI
  DR2","Scalable and Accurate Application-Level Crash-Consistency Testing via
  Representative Testing"
neg-d2-312,2025-02-28,,2503.00329," Visual embedding models excel at zero-shot tasks like visual retrieval and
classification. However, these models cannot be used for tasks that contain
ambiguity or require user instruction. These tasks necessitate a multimodal
embedding model, which outputs embeddings that combine visual and natural
language input. Existing CLIP-based approaches embed images and text
independently, and fuse the result. We find that this results in weak
interactions between modalities, and poor user control over the representation.
We introduce ABC, an open-source multimodal embedding model that uses a
vision-language model backbone to deeply integrate image features with natural
language instructions. ABC achieves bestfor-size performance on MSCOCO
image-to-text retrieval and is the top performing model on classification and
VQA tasks in the Massive Multimodal Embedding Benchmark. With a strongly
unified vision-language representation, ABC can use natural language to solve
subtle and potentially ambiguous visual retrieval problems. To evaluate this
capability, we design CtrlBench, a benchmark that requires interleaving textual
instructions with image content for correct retrieval. ABC advances the state
of multimodal embeddings by offering high-quality representations and flexible
natural language control. Our model and datasets are available at our project
page.","['cs.CV', 'cs.LG']",2502.08388," In this paper, we study the shadow and observational image of the Kerr-like
Loop Quantum Gravity (LQG) inspired black bounce with the help of the celestial
light source and the thin disk source by employing the backward ray-tracing
method. The results indicate that both the LQG parameter alpha and the rotation
parameter a contribute to a reduction in the shadow size; however, the
influence of a is predominant, while the effect of alpha circular orbit. One
can find that the correlation parameter (a, alpha), along with the observer's
inclination angle, affect the image's asymmetry and the distortion of the inner
shadow. As the inclination increases, the direct and lensed images diverge,
creating a structure resembling a hat. Meanwhile, we also investigate the
redshift distribution of the direct lensed images of the accretion disk under
different parameters and observation angle. The results show that the
distribution of redshift and observed intensity is obviously related to the
behavior of accretion flow. These results may provide a potential approach to
limit black hole parameters, detect quantum gravity effects, and distinguish
the LQG black hole from other black hole models.",['gr-qc'],False,,,,ABC: Achieving Better Control of Multimodal Embeddings using VLMs,"The shadow and accretion disk images of the rotation loop quantum black
  bounce"
neg-d2-313,2025-02-05,,2502.03209," Domain walls formed during a phase transition in a simple field theory model
with $\mathbb{Z}_2$ symmetry in a periodic box have been demonstrated to
annihilate as fast as causality allows and their area density scales $\propto
t^{-1}$. We have performed numerical simulations of the dynamics of domain
walls in the Two-Higgs Doublet Model (2HDM) where the potential has
$\mathbb{Z}_2$ symmetry in two spatial dimensions. We observed significant
differences with the standard case. Although the extreme long-time limit is the
same for the $\approx 10^{5}$ sets of random initial configurations analysed,
the percolation process is much slower due to the formation of long-lived
loops. We suggest that this is due to the build up of superconducting currents
on the walls which could lead ultimately to stationary configurations known as
Kinky Vortons. We discuss the relevance of these findings for the production of
Vortons in three spatial dimensions.","['hep-ph', 'astro-ph.CO', 'hep-th']",2502.17968," We consider the defocusing Calogero--Moser derivative nonlinear
Schr{\""o}dinger equation\begin{align*}i \partial_{t} u+\partial_{x}^2 u-2\Pi
D\left(|u|^{2}\right)u=0, \quad (t,x ) \in \mathbb{R} \times
\mathbb{R}\end{align*}posed on $E := \left\{u \in L^{\infty}(\mathbb{R}): u'
\in L^{2}(\mathbb{R}), u'' \in L^{2}(\mathbb{R}), |u|^{2}-1 \in
L^{2}(\mathbb{R})\right\}$. We prove the global well-posedness of this equation
in $E$. Moreover, we give an explicit formula for the chiral solution to this
equation.",['math.AP'],False,,,,Percolation of Domain Walls in the Two-Higgs Doublet Model,"The defocusing Calogero--Moser derivative nonlinear Schr{\""o}dinger
  equation with a nonvanishing condition at infinity"
neg-d2-314,2025-03-18,,2503.14296," We establish a complete Widder Theory for the fractional fast diffusion
equation. Our work focuses on nonnegative solutions satisfying a certain
integral size condition at infinity. We prove that these solutions possess a
Radon measure as initial trace, and prove the existence and uniqueness of
solutions originating from such initial data. The uniqueness result is the main
issue. Most of its difficulty comes from the singular character of the
nonlinearity.",['math.AP'],2501.11864," Thorough simulation testing is crucial for validating the correct behavior of
small Uncrewed Aerial Systems (sUAS) across multiple scenarios, including
adverse weather conditions (such as wind, and fog), diverse settings (hilly
terrain, or urban areas), and varying mission profiles (surveillance,
tracking). While various sUAS simulation tools exist to support developers, the
entire process of creating, executing, and analyzing simulation tests remains a
largely manual and cumbersome task. Developers must identify test scenarios,
set up the simulation environment, integrate the System under Test (SuT) with
simulation tools, formulate mission plans, and collect and analyze results.
These labor-intensive tasks limit the ability of developers to conduct
exhaustive testing across a wide range of scenarios. To alleviate this problem,
in this paper, we propose AutoSimTest, a Large Language Model (LLM)-driven
framework, where multiple LLM agents collaborate to support the sUAS simulation
testing process. This includes: (1) creating test scenarios that subject the
SuT to unique environmental contexts; (2) preparing the simulation environment
as per the test scenario; (3) generating diverse sUAS missions for the SuT to
execute; and (4) analyzing simulation results and providing an interactive
analytics interface. Further, the design of the framework is flexible for
creating and testing scenarios for a variety of sUAS use cases, simulation
tools, and SuT input requirements. We evaluated our approach by (a) conducting
simulation testing of PX4 and ArduPilot flight-controller-based SuTs, (b)
analyzing the performance of each agent, and (c) gathering feedback from sUAS
developers. Our findings indicate that AutoSimTest significantly improves the
efficiency and scope of the sUAS testing process, allowing for more
comprehensive and varied scenario evaluations while reducing the manual effort.",['cs.SE'],False,,,,Fractional fast diffusion with initial data a Radon measure,"LLM-Agents Driven Automated Simulation Testing and Analysis of small
  Uncrewed Aerial Systems"
neg-d2-315,2025-03-03,,2503.05805," This paper proposes a diffusion-based auto-bidding framework that leverages
graph representations to model large-scale auction environments. In such
settings, agents must dynamically optimize bidding strategies under constraints
defined by key performance indicator (KPI) metrics, all while operating in
competitive environments characterized by uncertain, sparse, and stochastic
variables. To address these challenges, we introduce a novel approach combining
learnable graph-based embeddings with a planning-based latent diffusion model
(LDM). By capturing patterns and nuances underlying the interdependence of
impression opportunities and the multi-agent dynamics of the auction
environment, the graph representation enable expressive computations regarding
auto-bidding outcomes. With reward alignment techniques, the LDM's posterior is
fine-tuned to generate auto-bidding trajectories that maximize KPI metrics
while satisfying constraint thresholds. Empirical evaluations on both
real-world and synthetic auction environments demonstrate significant
improvements in auto-bidding performance across multiple common KPI metrics, as
well as accuracy in forecasting auction outcomes.","['cs.LG', 'cs.AI', 'cs.MA']",2502.13997," Style transfer enables the seamless integration of artistic styles from a
style image into a content image, resulting in visually striking and
aesthetically enriched outputs. Despite numerous advances in this field,
existing methods did not explicitly focus on the signature style, which
represents the distinct and recognizable visual traits of the image such as
geometric and structural patterns, color palettes and brush strokes etc. In
this paper, we introduce SigStyle, a framework that leverages the semantic
priors that embedded in a personalized text-to-image diffusion model to capture
the signature style representation. This style capture process is powered by a
hypernetwork that efficiently fine-tunes the diffusion model for any given
single style image. Style transfer then is conceptualized as the reconstruction
process of content image through learned style tokens from the personalized
diffusion model. Additionally, to ensure the content consistency throughout the
style transfer process, we introduce a time-aware attention swapping technique
that incorporates content information from the original image into the early
denoising steps of target image generation. Beyond enabling high-quality
signature style transfer across a wide range of styles, SigStyle supports
multiple interesting applications, such as local style transfer, texture
transfer, style fusion and style-guided text-to-image generation. Quantitative
and qualitative evaluations demonstrate our approach outperforms existing style
transfer methods for recognizing and transferring the signature styles.",['cs.GR'],False,,,,Multi-agent Auto-Bidding with Latent Graph Diffusion Models,SigStyle: Signature Style Transfer via Personalized Text-to-Image Models
neg-d2-316,2025-01-29,,2501.17673," Enhancement of radiative coupling efficiency between out-of-plane excitonic
emitters in an indium selenide (InSe) film and an integrated waveguide formed
by silicon (Si) Mie-resonant nanodisks is experimentally studied.
Photoluminescence power at the resonant waveguide output is increased by~2.5
times at 950~nm in comparison with the case of a conventional rib waveguide of
the same geometrical parameters due to the efficient excitation of Mie-type
magnetic dipole resonances in individual nanoparticles. These results show
inspiring possibilities for creating new on-chip light emitters for various
integrated photonics applications.","['physics.optics', 'cond-mat.mes-hall']",2503.16601," We introduce SOFIA, a Mathematica package that automatizes the computation of
singularities of Feynman integrals, based on new theoretical understanding of
their analytic structure. Given a Feynman diagram, SOFIA generates a list of
potential singularities along with a candidate symbol alphabet. The package
also provides a comprehensive set of tools for analyzing the analytic
properties of Feynman integrals and related objects, such as cosmological and
energy correlators. We showcase its capabilities by reproducing known results
and predicting singularities and symbol alphabets of Feynman integrals at and
beyond the high-precision frontier.",['hep-th'],False,,,,"Mie-resonant silicon waveguide for efficient coupling with excitonic
  emitters in InSe",SOFIA: Singularities of Feynman Integrals Automatized
neg-d2-317,2025-01-06,,2501.03038," Automatic Music Transcription (AMT), aiming to get musical notes from raw
audio, typically uses frame-level systems with piano-roll outputs or language
model (LM)-based systems with note-level predictions. However, frame-level
systems require manual thresholding, while the LM-based systems struggle with
long sequences. In this paper, we propose a hybrid method combining pre-trained
roll-based encoders with an LM decoder to leverage the strengths of both
methods. Besides, our approach employs a hierarchical prediction strategy,
first predicting onset and pitch, then velocity, and finally offset. The
hierarchical prediction strategy reduces computational costs by breaking down
long sequences into different hierarchies. Evaluated on two benchmark
roll-based encoders, our method outperforms traditional piano-roll outputs 0.01
and 0.022 in onset-offset-velocity F1 score, demonstrating its potential as a
performance-enhancing plug-in for arbitrary roll-based music transcription
encoder.","['cs.SD', 'cs.AI', 'cs.LG', 'eess.AS']",2503.0748," We investigate the dynamics of small inertial particles in a two-dimensional,
steady Taylor-Green vortex flow. A classic study by Taylor (2022) showed that
heavy inertial point particles (having density parameter R = 1) are trapped by
the flow separatrices when the particle Stokes number St, which measures the
particle's inertia, is less than 1/4. Here, we consider finitely dense
particles, incorporating the previously neglected effects of added mass and the
Boussinesq-Basset history force. Using linear stability analysis near
stagnation points, we determine the critical parametric conditions in the St-R
plane that leads to particle trapping within vortex cells. We identify
additional stagnation points perceived by inertial particles, beyond the
traditional ones at vortex cell corners, when the added mass effect is
included, and we analyze their stability. Numerical analysis of the full
nonlinear system confirms the existence of distinct particle
behaviours--trapped, diffusive, and ballistic--depending on initial conditions,
consistent with Nath et al. (2024), with modifications due to added mass
effect. We delineate the regions in the St-R plane where these behaviours
dominate based on the prominent particle dynamics. However, when both the
history force and added mass effect are included, all particles exhibit
ballistic motion regardless of St and R.",['physics.flu-dyn'],False,,,,"Piano Transcription by Hierarchical Language Modeling with Pretrained
  Roll-based Encoders","Trapping and Transport of Inertial Particles in a Taylor-Green Vortex:
  Effects of Added Mass and History Force"
neg-d2-318,2025-03-03,,2503.02086," We report the discovery of an intriguing, low-mass galaxy-scale strong-lens
system in the SMACS J0723.3-7327 galaxy cluster. By modeling James Webb Space
Telescope imaging and Very Large Telescope Multi-Unit Spectroscopic Explorer
spectroscopic data, we find that the lens is cluster member galaxy at $z=0.397$
with an Einstein radius of $0^{\prime \prime}.424$ $\pm$ $0^{\prime
\prime}.012$, stellar mass of $M_* = (3.3 \pm 0.8) \times 10^{10} M_\odot$,
half-light radius of $\sim 1$ kpc, and central stellar velocity dispersion of
$140 \pm 6$ km s$^{-1}$. This lens galaxy is one of the few strong lens
galaxies known to date that have stellar mass as low as $M_* \sim 10^{10.5}
M_\odot$, offering an exceptional opportunity to peek into the population of
low-mass galaxies that has largely remained unexplored in the context of
strong-lensing studies. This strong lens system can also assist in assessing
the systematic uncertainty in the lens modeling of cluster member galaxies.",['astro-ph.GA'],2502.14105," Conformal prediction provides a powerful framework for constructing
prediction intervals with finite-sample guarantees, yet its robustness under
distribution shifts remains a significant challenge. This paper addresses this
limitation by modeling distribution shifts using L\'evy-Prokhorov (LP)
ambiguity sets, which capture both local and global perturbations. We provide a
self-contained overview of LP ambiguity sets and their connections to popular
metrics such as Wasserstein and Total Variation. We show that the link between
conformal prediction and LP ambiguity sets is a natural one: by propagating the
LP ambiguity set through the scoring function, we reduce complex
high-dimensional distribution shifts to manageable one-dimensional distribution
shifts, enabling exact quantification of worst-case quantiles and coverage.
Building on this analysis, we construct robust conformal prediction intervals
that remain valid under distribution shifts, explicitly linking LP parameters
to interval width and confidence levels. Experimental results on real-world
datasets demonstrate the effectiveness of the proposed approach.","['stat.ML', 'cs.LG', 'math.ST', 'stat.ME', 'stat.TH']",False,,,,Discovery of A Low-mass Strong-lens System in SMACS J0723.3-7327,"Conformal Prediction under L\'evy-Prokhorov Distribution Shifts:
  Robustness to Local and Global Perturbations"
neg-d2-319,2025-03-21,,2503.17516," We obtain a complete description of semi-Fredholm spectra of operators of the
form $(Tf)(z) = w(z)f(B(z)$ acting on the disc algebra in the case when $B$ is
either elliptic or double parabolic finite Blaschke product of degree $d \geq
2$ and $w$ has no zeros on the unit circle. In the case when $B$ has zeros on
the unit circle we provide only some partial results. Our results hint on the
possibility of interesting connections between the spectral properties of
weighted composition operators and complex dynamics.",['math.SP'],2502.06179," Driver decision quality in take-overs is critical for effective
human-Autonomous Driving System (ADS) collaboration. However, current research
lacks detailed analysis of its variations. This paper introduces two
metrics--Actual Achieved Gain (AAG) and Optimal Perceived Gain (OPG)--to assess
decision quality, with OPG representing optimal decisions and AAG reflecting
actual outcomes. Both are calculated as weighted averages of perceived gains
and losses, influenced by ADS accuracy. Study 1 (N=315) used a 21-point
Thurstone scale to measure perceived gains and losses-key components of AAG and
OPG-across typical tasks: route selection, overtaking, and collision avoidance.
Studies 2 (N=54) and 3 (N=54) modeled decision quality under varying ADS
accuracy and decision time. Results show with sufficient time (>3.5s), AAG
converges towards OPG, indicating rational decision-making, while limited time
leads to intuitive and deterministic choices. Study 3 also linked AAG-OPG
deviations to irrational behaviors. An intervention study (N=8) and a pilot
(N=4) employing voice alarms and multi-modal alarms based on these deviations
demonstrated AAG's potential to improve decision quality.",['cs.HC'],False,,,,"Spectrum of weighted composition operators. Part XI. The essential
  spectra of some weighted composition operators on the disc algebra","Actual Achieved Gain and Optimal Perceived Gain: Modeling Human
  Take-over Decisions Towards Automated Vehicles' Suggestions"
neg-d2-320,2025-02-05,,2502.03761," Unmanned aerial vehicles (UAVs) are widely used for object detection.
However, the existing UAV-based object detection systems are subject to severe
challenges, namely, their limited computation, energy and communication
resources, which limits the achievable detection performance. To overcome these
challenges, a UAV cognitive semantic communication system is proposed by
exploiting a knowledge graph. Moreover, we design a multi-scale codec for
semantic compression to reduce data transmission volume while guaranteeing
detection performance. Considering the complexity and dynamicity of UAV
communication scenarios, a signal-to-noise ratio (SNR) adaptive module with
robust channel adaptation capability is introduced. Furthermore, an object
detection scheme is proposed by exploiting the knowledge graph to overcome
channel noise interference and compression distortion. Simulation results
conducted on the practical aerial image dataset demonstrate that our proposed
semantic communication system outperforms benchmark systems in terms of
detection accuracy, communication robustness, and computation efficiency,
especially in dealing with low bandwidth compression ratios and low SNR
regimes.",['eess.SP'],2501.09954," Design space exploration (DSE) plays a crucial role in enabling custom
hardware architectures, particularly for emerging applications like AI, where
optimized and specialized designs are essential. With the growing complexity of
deep neural networks (DNNs) and the introduction of advanced foundational
models (FMs), the design space for DNN accelerators is expanding at an
exponential rate. Additionally, this space is highly non-uniform and
non-convex, making it increasingly difficult to navigate and optimize.
Traditional DSE techniques rely on search-based methods, which involve
iterative sampling of the design space to find the optimal solution. However,
this process is both time-consuming and often fails to converge to the global
optima for such design spaces. Recently, AIrchitect v1, the first attempt to
address the limitations of search-based techniques, transformed DSE into a
constant-time classification problem using recommendation networks. In this
work, we propose AIrchitect v2, a more accurate and generalizable
learning-based DSE technique applicable to large-scale design spaces that
overcomes the shortcomings of earlier approaches. Specifically, we devise an
encoder-decoder transformer model that (a) encodes the complex design space
into a uniform intermediate representation using contrastive learning and (b)
leverages a novel unified representation blending the advantages of
classification and regression to effectively explore the large DSE space
without sacrificing accuracy. Experimental results evaluated on 10^5 real DNN
workloads demonstrate that, on average, AIrchitect v2 outperforms existing
techniques by 15% in identifying optimal design points. Furthermore, to
demonstrate the generalizability of our method, we evaluate performance on
unseen model workloads (LLMs) and attain a 1.7x improvement in inference
latency on the identified hardware architecture.","['cs.LG', 'cs.AI', 'cs.AR']",False,,,,"UAV Cognitive Semantic Communications Enabled by Knowledge Graph for
  Robust Object Detection","AIRCHITECT v2: Learning the Hardware Accelerator Design Space through
  Unified Representations"
neg-d2-321,2025-01-29,,2501.17746," In vehicle-to-everything (V2X) applications, roadside units (RSUs) can be
tasked with both sensing and communication functions to enable sensing-assisted
communications. Recent studies have demonstrated that distance, angle, and
velocity information obtained through sensing can be leveraged to reduce the
overhead associated with communication beam tracking. In this work, we extend
this concept to scenarios involving multiple distributed RSUs and distributed
MIMO (multiple-input multiple-output) systems. We derive the state evolution
model, formulate the extended Kalman-filter equations, and implement predictive
beamforming for distributed MIMO. Simulation results indicate that, when
compared with a co-located massive MIMO antenna array, distributed antennas
lead to more uniform and robust sensing performance, coverage, and data rates,
while the vehicular user is in motion.","['eess.SP', 'cs.IT', 'math.IT']",2502.10699," Contextual memory integration remains a high challenge in the development of
language models, particularly in tasks that require maintaining coherence over
extended sequences. Traditional approaches, such as self-attention mechanisms
and memory-augmented architectures, often prioritize short-term dependencies,
leading to fragmentation and inconsistency in long-range contextual
understanding. Inspired by principles of synaptic plasticity observed in
biological neural systems, a novel mechanism, Synaptic Resonance, is introduced
to dynamically reinforce relevant memory pathways during training and
inference. Unlike static memory representations, this mechanism continuously
adjusts synaptic weight matrices based on contextual relevance, allowing for
improved information retention without excessive computational overhead.
Evaluations conducted on an open-source language model demonstrate reductions
in perplexity, enhancements in contextual coherence, and increased robustness
against input noise, highlighting the effectiveness of reinforcement-driven
memory modulation. Comparative analysis against baseline models further reveals
that the proposed approach achieves higher memory retention efficiency while
maintaining computational feasibility. The architectural modifications
integrate seamlessly into existing transformer-based frameworks, ensuring
stable convergence and efficient inference without sacrificing scalability.
Applications benefiting from improved long-term contextual consistency, such as
dialogue systems and document summarization, stand to gain from this approach.
Empirical findings suggest that dynamically reinforced memory pathways offer a
promising alternative to conventional memory mechanisms, addressing
longstanding limitations in extended sequence modeling.","['cs.CL', 'cs.AI', 'cs.NE']",False,,,,Predictive Beamforming with Distributed MIMO,"Exploring Synaptic Resonance in Large Language Models: A Novel Approach
  to Contextual Memory Integration"
neg-d2-322,2025-03-19,,2503.1528," We develop a system of non-linear stochastic evolution equations that
describes the continuous measurements of quantum systems with mixed initial
state. We address quantum systems with unbounded Hamiltonians and unbounded
interaction operators. Using arguments of the theory of quantum measurements we
derive a system of stochastic interacting wave functions (SIWF for short) that
models the continuous monitoring of quantum systems. We prove the existence and
uniqueness of the solution to this system under conditions general enough for
the applications. We obtain that the mixed state generated by the SIWF at any
time does not depend on the initial state, and satisfies the diffusive
stochastic quantum master equation, which is also known as Belavkin equation.
We present two physical examples. In one, the SIWF becomes a system of
non-linear stochastic partial differential equations. In the other, we deal
with a model of a circuit quantum electrodynamics.","['math-ph', 'math.MP', 'math.PR', 'quant-ph']",2501.08106," The method of H- photoionization is interesting for laser assisted charge
exchange injection. In this paper, the model and computation of photoionization
of negative hydrogen ion by using strong lasers is considered. The development
of this work is motivated by using pure lasers for photodetachment of electron
from negative hydrogen ion when it is not convenient or not possible to use
stripping magnet. Herein we develop a method of calculation of high efficiency
photoionization using time dependent wave equation with application of powerful
lasers. We compare this precise method of calculation with simplified method of
calculation through linear model of cross section interaction. Another
mechanism of photodetachment through excitation of the Feshbach resonance is
also considered.","['physics.atom-ph', 'physics.acc-ph']",False,,,,"System of stochastic interacting wave functions that model quantum
  measurements",Photodetachment of negative hydrogen ion beam
neg-d2-323,2025-01-01,,2501.00761," Ferroelectric polarization switching in electrically controlled van der Waals
multiferroic tunnel junctions (vdW-MFTJs) causes atomic migration, compromising
device stability and fatigue resistance. Here we propose a fully magnetically
controlled vdW-MFTJ based on a \(\mathrm{CrBr_3/MnPSe_3/CrBr_3}\) vertical
heterostructure, which achieves ferroelectric polarization reversal without
relying on atomic migration driven by inversion symmetry breaking. Using
first-principles calculations, we investigate the spin-polarized quantum
transport properties of the proposed structure. By integrating asymmetric
PtTe$_2$/alkali-metal (Li/Na/K)-doped/intercalated CrBr$_3$ electrodes, the
device demonstrates exceptional performance, with a maximum tunneling
magnetoresistance (TMR) exceeding $8.1\times10^5$\% and tunneling
electroresistance (TER) reaching 2499\%, while the spin-filtering channels can
be flexibly controlled by the magnetization direction of the magnetic free
layer, achieving perfect spin-filtering over a broad bias voltage range.
Applying an external bias voltage further enhances these metrics, increasing
TMR to $3.6\times 10^7$\% and TER to 9990\%. Notably, a pronounced negative
differential resistance (NDR) effect is observed, yielding an unprecedented
peak-to-valley ratio (PVR) of $9.55\times10^9$\%, representing the highest
value reported for vertical tunnel junctions. These extraordinary
characteristics highlight the potential of vdW-MFTJs for ultra-efficient
electronic switching, a key feature for next-generation spintronic devices. Our
findings provide a solid theoretical foundation for designing and developing
high-performance magnetic storage and logic technologies.",['cond-mat.mtrl-sci'],2502.06274," Drug-side effect research is vital for understanding adverse reactions
arising in complex multi-drug therapies. However, the scarcity of higher-order
datasets that capture the combinatorial effects of multiple drugs severely
limits progress in this field. Existing resources such as TWOSIDES primarily
focus on pairwise interactions. To fill this critical gap, we introduce HODDI,
the first Higher-Order Drug-Drug Interaction Dataset, constructed from U.S.
Food and Drug Administration (FDA) Adverse Event Reporting System (FAERS)
records spanning the past decade, to advance computational pharmacovigilance.
HODDI contains 109,744 records involving 2,506 unique drugs and 4,569 unique
side effects, specifically curated to capture multi-drug interactions and their
collective impact on adverse effects. Comprehensive statistical analyses
demonstrate HODDI's extensive coverage and robust analytical metrics, making it
a valuable resource for studying higher-order drug relationships. Evaluating
HODDI with multiple models, we found that simple Multi-Layer Perceptron (MLP)
can outperform graph models, while hypergraph models demonstrate superior
performance in capturing complex multi-drug interactions, further validating
HODDI's effectiveness. Our findings highlight the inherent value of
higher-order information in drug-side effect prediction and position HODDI as a
benchmark dataset for advancing research in pharmacovigilance, drug safety, and
personalized medicine. The dataset and codes are available at
https://github.com/TIML-Group/HODDI.","['cs.LG', 'cs.AI', 'q-bio.MN']",False,,,,"Giant Nonvolatile Multistate Resistance with Fully Magnetically
  Controlled in van der Waals Multiferroic Tunnel Junctions","HODDI: A Dataset of High-Order Drug-Drug Interactions for Computational
  Pharmacovigilance"
neg-d2-324,2025-03-01,,2503.0055," \abstract{Urban scaling theories posit that larger cities exhibit
disproportionately higher levels of socioeconomic activity and human
interactions. Yet, evidence from developing contexts (especially those marked
by stark socioeconomic disparities) remains limited. To address this gap, we
analyse a month-long dataset of 3.1~billion voice-call records from Brazil's
100 most populous cities, providing a continental-scale test of urban scaling
laws. We measure interactions using two complementary proxies: the number of
phone-based contacts (voice-call degrees) and the number of trips inferred from
consecutive calls in distinct locations. Our findings reveal clear superlinear
relationships in both metrics, indicating that larger urban centres exhibit
intensified remote communication and physical mobility. We further observe that
gross domestic product (GDP) also scales superlinearly with population,
consistent with broader claims that economic output grows faster than city
size. Conversely, the number of antennas required per user scales sublinearly,
suggesting economies of scale in telecommunications infrastructure. Although
the dataset covers a single provider, its widespread coverage in major cities
supports the robustness of the results. We nonetheless discuss potential
biases, including city-specific marketing campaigns and predominantly prepaid
users, as well as the open question of whether higher interaction drives wealth
or vice versa. Overall, this study enriches our understanding of urban scaling,
emphasising how communication and mobility jointly shape the socioeconomic
landscapes of rapidly growing cities.",['physics.soc-ph'],2503.09425," In arXiv:1303.3724, the authors provide an axiomatic way of constructing new
polynomially bounded o-minimal structures. However, all of the structures
satisfying these axioms must also have smooth cell-decomposition. In this
paper, we generalize their approach by allowing weakly smooth germs into the
construction. In particular, we showed in arXiv:2501.17583 that the o-minimal
structure constructed in [O. Le Gal, J.-P. Rolin. ""An o-minimal structure which
does not admit $C^\infty$ cellular decomposition"" Ann. Inst. Fourier 59 (2009),
pp 543-562] satisfies the assumptions of our theorem.",['math.LO'],False,,,,"Validating Urban Scaling Laws through Mobile Phone Data: A
  Continental-Scale Analysis of Brazil's Largest Cities","Quasianalytic algebras with weakly smooth germs generate o-minimal
  structures"
neg-d2-325,2025-01-24,,2501.14717," Recent advances in natural language processing have leveraged instruction
tuning to enhance Large Language Models (LLMs) for table-related tasks.
However, previous works train different base models with different training
data, lacking an apples-to-apples comparison across the result table LLMs. To
address this, we fine-tune base models from the Mistral, OLMo, and Phi families
on existing public training datasets. Our replication achieves performance on
par with or surpassing existing table LLMs, establishing new state-of-the-art
performance on Hitab, a table question-answering dataset. More importantly,
through systematic out-of-domain evaluation, we decouple the contributions of
training data and the base model, providing insight into their individual
impacts. In addition, we assess the effects of table-specific instruction
tuning on general-purpose benchmarks, revealing trade-offs between
specialization and generalization.",['cs.CL'],2501.03756," Interior models of gas giants in the Solar System traditionally assume a
fully convective molecular hydrogen envelope. However, recent observations from
the Juno mission suggest a possible depletion of alkali metals in Jupiter's
molecular hydrogen envelope, indicating that a stable radiative layer could
exist at the kilobar level. Recent studies propose that deep stable layers help
reconcile various Jupiter observations, including its atmospheric water and CO
abundances and the depth of its zonal winds. However, opacity tables used to
infer stable layers are often outdated and incomplete, leaving the precise
molecular hydrogen envelope composition required for a deep radiative zone
uncertain. In this paper, we determine atmospheric compositions that can lead
to the formation of a radiative zone at the kilobar level in Jupiter and Saturn
today. We computed radiative opacity tables covering pressures up to $10^5$
bar, including the most abundant molecules present in the gas giants of the
Solar System, as well as contributions from free electrons, metal hydrides,
oxides, and atomic species, using the most up-to-date line lists published in
the literature. These tables were used to calculate Rosseland-mean opacities
for the molecular hydrogen envelopes of Jupiter and Saturn, which were then
compared to the critical mean opacity required to maintain convection. We find
that the presence of a radiative zone is controlled by the existence of K, Na,
and NaH in the atmosphere of Jupiter and Saturn. For Jupiter, the elemental
abundance of K and Na must be less than $\sim 10^{-3}$ times solar to form a
radiative zone. In contrast, for Saturn, the required abundance for K and Na is
below $\sim 10^{-4}$ times solar.",['astro-ph.EP'],False,,,,"Towards Better Understanding Table Instruction Tuning: Decoupling the
  Effects from Data versus Models","Conditions for radiative zones in the molecular hydrogen envelope of
  Jupiter and Saturn: The role of alkali metals"
neg-d2-326,2025-03-12,,2503.09241," Computer agents powered by vision-language models (VLMs) have significantly
advanced human-computer interaction, enabling users to perform complex tasks
through natural language instructions. However, these agents are vulnerable to
context deception attacks, an emerging threat where adversaries embed
misleading content into the agent's operational environment, such as a pop-up
window containing deceptive instructions. Existing defenses, such as
instructing agents to ignore deceptive elements, have proven largely
ineffective. As the first systematic study on protecting computer agents, we
introduce textbf{in-context defense}, leveraging in-context learning and
chain-of-thought (CoT) reasoning to counter such attacks. Our approach involves
augmenting the agent's context with a small set of carefully curated exemplars
containing both malicious environments and corresponding defensive responses.
These exemplars guide the agent to first perform explicit defensive reasoning
before action planning, reducing susceptibility to deceptive attacks.
Experiments demonstrate the effectiveness of our method, reducing attack
success rates by 91.2% on pop-up window attacks, 74.6% on average on
environment injection attacks, while achieving 100% successful defenses against
distracting advertisements. Our findings highlight that (1) defensive reasoning
must precede action planning for optimal performance, and (2) a minimal number
of exemplars (fewer than three) is sufficient to induce an agent's defensive
behavior.",['cs.AI'],2501.18771," Data contamination -- the accidental consumption of evaluation examples
within the pre-training data -- can undermine the validity of evaluation
benchmarks. In this paper, we present a rigorous analysis of the effects of
contamination on language models at 1B and 8B scales on the machine translation
task. Starting from a carefully decontaminated train-test split, we
systematically introduce contamination at various stages, scales, and data
formats to isolate its effect and measure its impact on performance metrics.
Our experiments reveal that contamination with both source and target
substantially inflates BLEU scores, and this inflation is 2.5 times larger (up
to 30 BLEU points) for 8B compared to 1B models. In contrast, source-only and
target-only contamination generally produce smaller, less consistent
over-estimations. Finally, we study how the temporal distribution and frequency
of contaminated samples influence performance over-estimation across languages
with varying degrees of data resources.","['cs.CL', 'cs.AI']",False,,,,In-Context Defense in Computer Agents: An Empirical Study,"Overestimation in LLM Evaluation: A Controlled Large-Scale Study on Data
  Contamination's Impact on Machine Translation"
neg-d2-327,2025-01-24,,2501.14703," A comprehensive study on thermomechanical processing of pure Mg was conducted
through sequential hot extrusion, hot rolling, and cold drawing operations.
Three different extrusion ratios (6:1, 25:1, and 39:1) were investigated at
350{\deg}C, revealing that 39:1 ratio produced an optimal bimodal grain
structure with beneficial twin morphology. Subsequently, hot rolling
experiments were performed at varying linear speeds (26- and 130-mm s-1) and
interpass annealing times (2.5 and 10 minutes). Results demonstrated that
higher rolling speeds led to finer microstructure, while longer interpass
annealing times resulted in reduced twin fraction and more inhomogeneous
microstructure. The processed material was then subjected to cold drawing with
approximately 12% true strain per pass. Different annealing conditions
(275{\deg}C and 375{\deg}C for 2.5-10 minutes) between drawing passes were
evaluated. Analysis showed that annealing at 375{\deg}C for 2.5-5 minutes
provided optimal softening for subsequent deformation. Fracture analysis
revealed a mixed ductile-brittle behavior, with twin-matrix interfaces serving
as preferred crack propagation paths This study establishes optimal processing
parameters for pure Mg wire production, highlighting the critical role of twin
characteristics and restoration processes in determining material formability
during multi-step thermomechanical processing.",['cond-mat.mtrl-sci'],2502.14277," GK Persei, an old nova and intermediate polar (IP), exhibited a dwarf nova
(DN) outburst in 2010. This outburst was extensively observed by the Neil
Gehrels Swift Observatory, beginning 1.95 days after the eruption and
continuing until 13.9 days before the maximum of the outburst in the optical.
In this paper, we present timing and spectral analyses, comparing the results
with those of other outbursts. We confirm the spin modulation in the 2 $-$ 10
keV X-ray range with a period of $P_{\rm WD} = 351.325(9)$ s. Additionally, we
detected spin modulation in the 0.3 $-$ 2 keV band during the second half of
the observations, a feature not seen in the 2015 and 2018 outbursts. This
finding suggests that the soft X-ray emission in GK Per may originate partly
near the magnetic poles and partly from a wind or circumstellar material.","['astro-ph.HE', 'astro-ph.SR']",False,,,,"Thermomechanical Processing of Pure Magnesium: Hot Extrusion, Hot
  Rolling and Cold Drawing","Timing and spectral analysis of GK Persei during the 2010 dwarf nova
  outburst"
neg-d2-328,2025-01-24,,2501.14402," Context: Microservice-based systems have established themselves in the
software industry. However, sustainability-related legislation and the growing
costs of energy-hungry software increase the importance of energy efficiency
for these systems. While some proposals for architectural tactics and patterns
exist, their effectiveness as well as potential trade-offs on other quality
attributes (QAs) remain unclear.
  Goal: We therefore aim to study the effectiveness of microservices tactics
and patterns to reduce energy consumption, as well as potential trade-offs with
performance and maintainability.
  Method: Using the open-source Online Boutique system, we conducted a
controlled experiment with three tactics and three patterns, and analyzed the
impact of each technique compared to a baseline. We also tested with three
levels of simulated request loads (low, medium, high).
  Results: Request load moderated the effectiveness of reducing energy
consumption. All techniques (tactics and patterns) reduced the energy
consumption for at least one load level, up to 5.6%. For performance, the
techniques could negatively impact response time by increasing it by up to
25.9%, while some also decreased it by up to 72.5%. Two techniques increased
the throughput, by 1.9% and 34.0%. For maintainability, three techniques had a
negative, one a positive, and two no impact.
  Conclusion: Some techniques reduced energy consumption while also improving
performance. However, these techniques usually involved a trade-off in
maintainability, e.g., via more code duplication and module coupling. Overall,
all techniques significantly reduced energy consumption at higher loads, but
most of them sacrificed one of the other QAs. This highlights that the real
challenge is not simply reducing energy consumption of microservices, but to
achieve energy efficiency.",['cs.SE'],2501.01654," A fundamental alcove $\mathcal{A}$ is a tile in a paving of a vector space
$V$ by an affine reflection group $W_{\mathrm{aff}}$. Its geometry encodes
essential features of $W_{\mathrm{aff}}$, such as its affine Dynkin diagram
$\widetilde{D}$ and fundamental group $\Omega$. In this article we investigate
its full isometry group $\mathrm{Aut}(\mathcal{A})$. It is well known that the
isometry group of a regular polyhedron is generated by hyperplane reflections
on its faces. Being a simplex, an alcove $\mathcal{A}$ is the simplest of
polyhedra, nevertheless it is seldom a regular one. In our first main result we
show that $\mathrm{Aut}(\mathcal{A})$ is isomorphic to
$\mathrm{Aut}(\widetilde{D})$. Building on this connection, we establish that
$\mathrm{Aut}(\mathcal{A})$ is an abstract Coxeter group, with generators given
by affine isometric involutions of the ambient space. Although these
involutions are seldom reflections, our second main result leverages them to
construct, by slicing the Komrakov--Premet fundamental polytope $\mathcal{K}$
for the action of $\Omega$, a family of fundamental polytopes for the action of
$\mathrm{Aut}(\mathcal{A})$ on $\mathcal{A}$, whose vertices are contained in
the vertices of $\mathcal{K}$ and whose faces are parametrized by the so-called
balanced minuscule roots, which we introduce here. In an appendix, we discuss
some related negative results on stratified centralizers and equivariant
triangulations.","['math.CO', 'math.GR']",False,,,,"On the Effectiveness of Microservices Tactics and Patterns to Reduce
  Energy Consumption: An Experimental Study on Trade-Offs",Fundamental polytope for the isometry group of an alcove
neg-d2-329,2025-02-25,,2502.18254," We study the normalized solutions to the following Choquard equation
  \begin{equation*}
  \aligned &-\Delta u + \lambda u =\mu g(u) + \gamma (I_\alpha *
|u|^{\frac{N+\alpha}{N}})|u|^{\frac{N+\alpha}{N}-2}u & \text{in\ \ }
\mathbb{R}^N \endaligned
  \end{equation*} under the $L^2$-norm constraint $\|u\|_2=c$. Here $\gamma>0$,
$ N\geq 1$, $\alpha\in(0,N)$, $I_{\alpha}$ is the Riesz potential, and the
unknown $\lambda$ appears as a Lagrange multiplier. In a mass supercritical
setting on $g$, we find regions in the $(c,\mu)$--parameter space such that the
corresponding equation admits a positive radial ground state solution. To
overcome the lack of compactness resulting from the nonlocal term, we present a
novel compactness lemma and some prior energy estimate. These results are even
new for the power type nonlinearity $g(u)= |u|^{q-2}u$ with
$2+\frac{4}{N}<q<2^*$ ($2^*:=\frac{2N}{N-2}$, if $N\geq 3$ and $2^* = \infty$,
if $N=1, 2$). We also show that as $\mu$ or $c$ tends to $0$ (resp. $\mu$ or
$c$ tends to $+\infty$), after a suitable rescaling the ground state solutions
converge in $H^1(\RN)$ to a particular solution of the limit equations.
Further, we study the non-existence and multiplicity of positive radial
solutions to \begin{equation*}
  -\Delta u + u = \eta |u|^{q-2}u + (I_\alpha *
|u|^{\frac{N+\alpha}{N}})|u|^{\frac{N+\alpha}{N}-2}u, \quad \text{in}\ \ \RN
\end{equation*} where $N \geq 1$, $ 2< q<2^*$ and $\eta>0$. Based on some
analytical ideas the limit behaviors of the normalized solutions, we verify
some threshold regions of $\eta$ such that the corresponding equation has no
positive least action solution or admits multiple positive solutions. To the
best of our knowledge, this seems to be the first result concerning the
non-existence and multiplicity of positive solutions to Choquard type equations
involving the lower critical exponent.",['math.AP'],2502.13005," Characterizing bipolaron binding, and understanding how it depends on
electron-phonon interaction, is crucial to unraveling the nature of emergent
many-body states in strongly interacting electron-phonon systems. So far, most
studies of bipolarons have been limited to the Holstein model, in which the
coupling constant is momentum-independent. The paradigmatic example of
momentum-dependent electron-phonon interaction comes from the system in which
phonon distortions modify electron hopping, the SSH model. Already individual
polarons in the SSH model are richer than the Holstein model counterparts, and
feature a phase transition into the finite momentum ground state with
increasing electron-phonon interaction. In this paper, we use a variational
approach to study bipolarons in the one-dimensional SSH model and discuss their
ground state, dispersion, and excitation spectra. We explore the full parameter
range of the system, including the adiabatic regime of slow phonons, which was
inaccessible to previous theoretical studies. In agreement with earlier
studies, we find that in the anti-adiabatic strongly interacting regime,
bipolarons have low effective mass. By contrast, in the adiabatic case, we find
that increasing electron-phonon interactions results in an exponential increase
of the bipolaron mass. We establish the existence of multiple branches of bound
excited states of SSH bipolaron and discuss the signatures of these bound
states in dynamics. We show that in the anti-adiabatic regime, response
functions obey a parity selection rule, that imposes symmetry constraints on
the excitation spectra and provides a clear signature of SSH bipolarons.",['cond-mat.str-el'],False,,,,"Normalized solutions to lower critical Choquard equation in
  mass-supercritical setting",Bipolaron dynamics in the one-dimensional SSH model
neg-d2-330,2025-01-08,,2501.05487," The field of Artificial Intelligence (AI) continues to drive transformative
innovations, with significant progress in conversational interfaces, autonomous
vehicles, and intelligent content creation. Since the launch of ChatGPT in late
2022, the rise of Generative AI has marked a pivotal era, with the term Large
Language Models (LLMs) becoming a ubiquitous part of daily life. LLMs have
demonstrated exceptional capabilities in tasks such as text summarization, code
generation, and creative writing. However, these models are inherently limited
by their token-level processing, which restricts their ability to perform
abstract reasoning, conceptual understanding, and efficient generation of
long-form content. To address these limitations, Meta has introduced Large
Concept Models (LCMs), representing a significant shift from traditional
token-based frameworks. LCMs use concepts as foundational units of
understanding, enabling more sophisticated semantic reasoning and context-aware
decision-making. Given the limited academic research on this emerging
technology, our study aims to bridge the knowledge gap by collecting,
analyzing, and synthesizing existing grey literature to provide a comprehensive
understanding of LCMs. Specifically, we (i) identify and describe the features
that distinguish LCMs from LLMs, (ii) explore potential applications of LCMs
across multiple domains, and (iii) propose future research directions and
practical strategies to advance LCM development and adoption.",['cs.CL'],2501.06785," Understanding objects in 3D at the part level is essential for humans and
robots to navigate and interact with the environment. Current datasets for
part-level 3D object understanding encompass a limited range of categories. For
instance, the ShapeNet-Part and PartNet datasets only include 16, and 24 object
categories respectively. The 3DCoMPaT dataset, specifically designed for
compositional understanding of parts and materials, contains only 42 object
categories. To foster richer and fine-grained part-level 3D understanding, we
introduce 3DCoMPaT200, a large-scale dataset tailored for compositional
understanding of object parts and materials, with 200 object categories with
$\approx$5 times larger object vocabulary compared to 3DCoMPaT and $\approx$ 4
times larger part categories. Concretely, 3DCoMPaT200 significantly expands
upon 3DCoMPaT, featuring 1,031 fine-grained part categories and 293 distinct
material classes for compositional application to 3D object parts.
Additionally, to address the complexities of compositional 3D modeling, we
propose a novel task of Compositional Part Shape Retrieval using ULIP to
provide a strong 3D foundational model for 3D Compositional Understanding. This
method evaluates the model shape retrieval performance given one, three, or six
parts described in text format. These results show that the model's performance
improves with an increasing number of style compositions, highlighting the
critical role of the compositional dataset. Such results underscore the
dataset's effectiveness in enhancing models' capability to understand complex
3D shapes from a compositional perspective. Code and Data can be found at
http://github.com/3DCoMPaT200/3DCoMPaT200","['cs.CV', 'cs.CL']",False,,,,The Future of AI: Exploring the Potential of Large Concept Models,"3DCoMPaT200: Language-Grounded Compositional Understanding of Parts and
  Materials of 3D Shapes"
neg-d2-331,2025-01-31,,2501.19382," In this paper, we propose a novel loop closure detection algorithm that uses
graph attention neural networks to encode semantic graphs to perform place
recognition and then use semantic registration to estimate the 6 DoF relative
pose constraint. Our place recognition algorithm has two key modules, namely, a
semantic graph encoder module and a graph comparison module. The semantic graph
encoder employs graph attention networks to efficiently encode spatial,
semantic and geometric information from the semantic graph of the input point
cloud. We then use self-attention mechanism in both node-embedding and
graph-embedding steps to create distinctive graph vectors. The graph vectors of
the current scan and a keyframe scan are then compared in the graph comparison
module to identify a possible loop closure. Specifically, employing the
difference of the two graph vectors showed a significant improvement in
performance, as shown in ablation studies. Lastly, we implemented a semantic
registration algorithm that takes in loop closure candidate scans and estimates
the relative 6 DoF pose constraint for the LiDAR SLAM system. Extensive
evaluation on public datasets shows that our model is more accurate and robust,
achieving 13% improvement in maximum F1 score on the SemanticKITTI dataset,
when compared to the baseline semantic graph algorithm. For the benefit of the
community, we open-source the complete implementation of our proposed algorithm
and custom implementation of semantic registration at
https://github.com/crepuscularlight/SemanticLoopClosure","['cs.CV', 'cs.RO']",2502.09522," Synchronizing words in classical automata theory provide a mechanism to reset
any state of a deterministic automaton to a specific target state via a
carefully chosen finite sequence of transition rules. In this work, we extend
the concept of synchronizing words to quantum information theory. Specifically,
we show that with only two quantum channels, it is possible to bring an
arbitrary qutrit state close to a designated target state. Furthermore, we
demonstrate that following this reset, any pure real qutrit state can be
closely approximated using the same two channels. These findings establish a
quantum analogue of synchronizing words, highlighting their potential
applications in constructing minimal sets of universal quantum gates capable of
both resetting and preparing arbitrary states.",['quant-ph'],False,,,,"LiDAR Loop Closure Detection using Semantic Graphs with Graph Attention
  Networks",Quantum Synchronizing Words: Resetting and Preparing Qutrit States
neg-d2-332,2025-03-07,,2503.06055," Recent approaches to the theory of dynamic programming view dynamic programs
as families of policy operators acting on partially ordered sets. In this
paper, we extend these ideas by shifting from arbitrary partially ordered sets
to ordered vector space. The advantage of working in this setting is that
ordered vector spaces have well integrated algebric and order structure, which
leads to sharper fixed point results. These fixed point results can then be
exploited to obtain strong optimality properties. We illustrate our results
through a range of applications, including new findings for several useful
models.",['math.OC'],2502.12683," We investigate the nuclear Stark effect induced in hydrogen-like atomic
nuclei under super-intense laser fields. Since laser wavelengths are generally
larger than nuclear dimensions, direct laser-nucleus interaction is unfeasible.
Instead, this effect is induced indirectly through electron oscillations in the
laser field, which produce a periodic electric field that shifts the nuclear
energy levels. Using perturbation theory, we derive an expression for the
energy shift and dynamic polarizability of the nucleus as a function of laser
parameters. Our findings reveal that the Nuclear Stark effect can be controlled
by adjusting the laser frequency and intensity, potentially enabling
applications in nuclear and quantum optical systems.",['nucl-th'],False,,,,Dynamic Programming in Ordered Vector Space,"AC nuclear Stark effect in H-atom via super-intense laser-atom
  interaction"
neg-d2-333,2025-01-26,,2501.15719," Following a method introduced by Thomas-Vasquez and developed by Grundman, we
prove that many Hilbert modular threefolds of arithmetic genus $0$ and $1$ are
of general type, and that some are of nonnegative Kodaira dimension. The new
ingredient is a detailed study of the geometry and combinatorics of totally
positive integral elements $x$ of a fractional ideal $I$ in a totally real
number field $K$ with the property that $\mathop{\mathrm{tr}} xy <
\mathop{\mathrm{min}} I \mathop{\mathrm{tr}} y$ for some $y \gg 0 \in K$.","['math.NT', 'math.AG']",2502.09337," This thesis is an exposition of the author's contribution on effective
descent morphisms in various categories of generalized categorical structures.
It consists of: Chapter 1, where an elementary description of descent theory
and the content of each remaining chapter is provided, supplemented with
references; Chapter 2, consisting of various descent theoretical definitions
and results employed in the remainder of this work; four chapters, each
corresponding to an article written by the author during the period of his PhD
studies.",['math.CT'],False,,,,The Kodaira dimension of Hilbert modular threefolds,Some aspects of descent theory and applications
neg-d2-334,2025-02-19,,2502.13997," Style transfer enables the seamless integration of artistic styles from a
style image into a content image, resulting in visually striking and
aesthetically enriched outputs. Despite numerous advances in this field,
existing methods did not explicitly focus on the signature style, which
represents the distinct and recognizable visual traits of the image such as
geometric and structural patterns, color palettes and brush strokes etc. In
this paper, we introduce SigStyle, a framework that leverages the semantic
priors that embedded in a personalized text-to-image diffusion model to capture
the signature style representation. This style capture process is powered by a
hypernetwork that efficiently fine-tunes the diffusion model for any given
single style image. Style transfer then is conceptualized as the reconstruction
process of content image through learned style tokens from the personalized
diffusion model. Additionally, to ensure the content consistency throughout the
style transfer process, we introduce a time-aware attention swapping technique
that incorporates content information from the original image into the early
denoising steps of target image generation. Beyond enabling high-quality
signature style transfer across a wide range of styles, SigStyle supports
multiple interesting applications, such as local style transfer, texture
transfer, style fusion and style-guided text-to-image generation. Quantitative
and qualitative evaluations demonstrate our approach outperforms existing style
transfer methods for recognizing and transferring the signature styles.",['cs.GR'],2502.1037," We provide explicit formulas for the Alexander polynomial of Pretzel knots
and establish several immediate corollaries, including the characterization of
Pretzel knots with a trivial Alexander polynomial.",['math.GT'],False,,,,SigStyle: Signature Style Transfer via Personalized Text-to-Image Models,Explicit Formulas for the Alexander Polynomial of Pretzel Knots
neg-d2-335,2025-02-24,,2502.17715," Effective conversational systems are expected to dynamically generate
contextual follow-up questions to elicit new information while maintaining the
conversation flow. While humans excel at asking diverse and informative
questions by intuitively assessing both obtained and missing information,
existing models often fall short of human performance on this task. To mitigate
this, we propose a method that generates diverse and informative questions
based on targeting unanswered information using a hypothetical LLM-generated
""comprehensive answer"". Our method is applied to augment an existing follow-up
questions dataset. The experimental results demonstrate that language models
fine-tuned on the augmented datasets produce follow-up questions of
significantly higher quality and diversity. This promising approach could be
effectively adopted to future work to augment information-seeking dialogues for
reducing ambiguities and improving the accuracy of LLM answers.","['cs.CL', 'cs.AI', 'cs.HC']",2503.11495," Human processes video reasoning in a sequential spatio-temporal reasoning
logic, we first identify the relevant frames (""when"") and then analyse the
spatial relationships (""where"") between key objects, and finally leverage these
relationships to draw inferences (""what""). However, can Video Large Language
Models (Video-LLMs) also ""reason through a sequential spatio-temporal logic"" in
videos? Existing Video-LLM benchmarks primarily focus on assessing object
presence, neglecting relational reasoning. Consequently, it is difficult to
measure whether a model truly comprehends object interactions (actions/events)
in videos or merely relies on pre-trained ""memory"" of co-occurrences as biases
in generating answers. In this work, we introduce a Video Spatio-Temporal
Reasoning (V-STaR) benchmark to address these shortcomings. The key idea is to
decompose video understanding into a Reverse Spatio-Temporal Reasoning (RSTR)
task that simultaneously evaluates what objects are present, when events occur,
and where they are located while capturing the underlying Chain-of-thought
(CoT) logic. To support this evaluation, we construct a dataset to elicit the
spatial-temporal reasoning process of Video-LLMs. It contains coarse-to-fine
CoT questions generated by a semi-automated GPT-4-powered pipeline, embedding
explicit reasoning chains to mimic human cognition. Experiments from 14
Video-LLMs on our V-STaR reveal significant gaps between current Video-LLMs and
the needs for robust and consistent spatio-temporal reasoning.",['cs.CV'],False,,,,"Bridging Information Gaps with Comprehensive Answers: Improving the
  Diversity and Informativeness of Follow-Up Questions",V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning
neg-d2-336,2025-02-13,,2502.09485," We develop a geometric flow framework to investigate the following two
classical shape functionals: the torsional rigidity and the first Dirichlet
eigenvalue of the Laplacian. First, by constructing novel deformation paths
governed by stretching flows, we prove several new monotonicity properties of
the torsional rigidity and the first eigenvalue along the evolutions restricted
to triangles and rhombuses. These results also lead to new and simpler proofs
of some known results, unifying and extending prior symmetrization-based
proofs. Second, utilizing the mean curvature flow, we give a new proof of the
Saint-Venant inequality for smooth convex bodies. This might represent the
first flow-based proof to establish geometric functional inequalities that
couple both the domain and the state function associated with it. Third, by
discovering a gradient norm inequality for the sides of rectangles, we prove
monotonicity and stronger rigidity results of the torsional rigidity on
rectangles.",['math.AP'],2503.15548," The widespread adoption of Retrieval-Augmented Generation (RAG) systems in
real-world applications has heightened concerns about the confidentiality and
integrity of their proprietary knowledge bases. These knowledge bases, which
play a critical role in enhancing the generative capabilities of Large Language
Models (LLMs), are increasingly vulnerable to breaches that could compromise
sensitive information. To address these challenges, this paper proposes an
advanced encryption methodology designed to protect RAG systems from
unauthorized access and data leakage. Our approach encrypts both textual
content and its corresponding embeddings prior to storage, ensuring that all
data remains securely encrypted. This mechanism restricts access to authorized
entities with the appropriate decryption keys, thereby significantly reducing
the risk of unintended data exposure. Furthermore, we demonstrate that our
encryption strategy preserves the performance and functionality of RAG
pipelines, ensuring compatibility across diverse domains and applications. To
validate the robustness of our method, we provide comprehensive security proofs
that highlight its resilience against potential threats and vulnerabilities.
These proofs also reveal limitations in existing approaches, which often lack
robustness, adaptability, or reliance on open-source models. Our findings
suggest that integrating advanced encryption techniques into the design and
deployment of RAG systems can effectively enhance privacy safeguards. This
research contributes to the ongoing discourse on improving security measures
for AI-driven services and advocates for stricter data protection standards
within RAG architectures.","['cs.CR', 'cs.AI']",False,,,,Flow approach on the monotonicity of shape functionals,Privacy-Aware RAG: Secure and Isolated Knowledge Retrieval
neg-d2-337,2025-02-25,,2502.18261," Traditionally, the impact of minimum wages on employment has been studied,
and it is generally believed to have a negative effect. Yet, some recent
studies have shown that the impact of minimum wages on employment can sometimes
be positive. In addition, certain recent proposals set a higher minimum wage
than the wage earned by some high-productivity workers. However, the impact of
minimum wages on employment has been primarily studied on low-skilled workers,
whereas there is limited research on high-skilled workers. To address this gap
and examine the effects of minimum wages on high-productivity workers'
employment, I construct a macroeconomic model incorporating productivity
fluctuations, incomplete markets, directed search, and on-the-job search and
compare the steady-state distributions between the baseline model and the model
with a minimum wage. As a result, binding minimum wages increase the
unemployment rate of both low and high-productivity workers.","['econ.GN', 'q-fin.EC']",2503.06914," A low-energy neutral quasiparticle in a fractional quantum Hall system
appears in the latter's energy spectrum on a sphere as a series of many-body
excited states labeled by the angular momentum $L$ and whose energy is a smooth
function of $L$ in the limit of large sphere radius. We argue that the
signature of a nonvanishing spin (intrinsic angular momentum) $s$ of the
quasiparticle is the absence, in this series, of states with total angular
momentum less than $s$.We reinterpret the missing of certain states, observed
in an exact-diagonalization calculation of the spectrum of the $\nu=7/3$ FQH
state in a wide quantum well as well as in many proposed wave functions for the
excited states as a consequence of the spin-2 nature of the zero-momentum
magnetoroton.","['cond-mat.str-el', 'cond-mat.mes-hall', 'hep-th']",False,,,,"The effect of minimum wages on employment in the presence of
  productivity fluctuations","Spin of fractional quantum Hall neutral modes and ""missing states"" on a
  sphere"
neg-d2-338,2025-02-25,,2502.18043," We analyze (p,t) two-neutron transfer reactions in a semi-microscopic model.
The overlap integrals of the target nucleus are calculated in a microscopic
cluster model. The Resonating Group Method (RGM) assumes a cluster structure of
the nucleus, and is well adapted to halo nuclei since the long-range part of
the wave function is accurately described. We focus on (p,t) reactions
involving 6He and 11Li, which are well known core+n+n halo nuclei. The RGM is
based on a nucleon-nucleon interaction, and therefore does not involve any
fitting procedure. It also provides overlap integrals of excited states of the
core nucleus. We present overlap integrals and spectroscopic factors of 6He and
11Li. We compute the 6He(p,t)alpha and 11Li(p,t)9Li cross sections at the DWBA,
and compare them with experiments. For 11Li we also determine the 11Li(p,t)9Li*
cross section which involves the first excited states of 9Li. A fair agreement
with experiment is obtained, considering that no parameter is adjusted.",['nucl-th'],2501.0726," 3D semantic scene completion is critical for multiple downstream tasks in
autonomous systems. It estimates missing geometric and semantic information in
the acquired scene data. Due to the challenging real-world conditions, this
task usually demands complex models that process multi-modal data to achieve
acceptable performance. We propose a unique neural model, leveraging advances
from the state space and diffusion generative modeling to achieve remarkable 3D
semantic scene completion performance with monocular image input. Our technique
processes the data in the conditioned latent space of a variational autoencoder
where diffusion modeling is carried out with an innovative state space
technique. A key component of our neural network is the proposed Skimba (Skip
Mamba) denoiser, which is adept at efficiently processing long-sequence data.
The Skimba diffusion model is integral to our 3D scene completion network,
incorporating a triple Mamba structure, dimensional decomposition residuals and
varying dilations along three directions. We also adopt a variant of this
network for the subsequent semantic segmentation stage of our method. Extensive
evaluation on the standard SemanticKITTI and SSCBench-KITTI360 datasets show
that our approach not only outperforms other monocular techniques by a large
margin, it also achieves competitive performance against stereo methods. The
code is available at https://github.com/xrkong/skimba","['cs.CV', 'cs.AI']",False,,,,"Microscopic study of halo nuclei through (p,t) reactions",Skip Mamba Diffusion for Monocular 3D Semantic Scene Completion
neg-d2-339,2025-03-06,,2503.04199," RGB-Thermal fusion is a potential solution for various weather and light
conditions in challenging scenarios. However, plenty of studies focus on
designing complex modules to fuse different modalities. With the widespread
application of large language models (LLMs), valuable information can be more
effectively extracted from natural language. Therefore, we aim to leverage the
advantages of large language models to design a structurally simple and highly
adaptable multimodal fusion model architecture. We proposed MultimodAl
Segmentation with TExt PRompts (MASTER) architecture, which integrates LLM into
the fusion of RGB-Thermal multimodal data and allows complex query text to
participate in the fusion process. Our model utilizes a dual-path structure to
extract information from different modalities of images. Additionally, we
employ LLM as the core module for multimodal fusion, enabling the model to
generate learnable codebook tokens from RGB, thermal images, and textual
information. A lightweight image decoder is used to obtain semantic
segmentation results. The proposed MASTER performs exceptionally well in
benchmark tests across various automated driving scenarios, yielding promising
results.","['cs.CV', 'cs.AI']",2503.00329," Visual embedding models excel at zero-shot tasks like visual retrieval and
classification. However, these models cannot be used for tasks that contain
ambiguity or require user instruction. These tasks necessitate a multimodal
embedding model, which outputs embeddings that combine visual and natural
language input. Existing CLIP-based approaches embed images and text
independently, and fuse the result. We find that this results in weak
interactions between modalities, and poor user control over the representation.
We introduce ABC, an open-source multimodal embedding model that uses a
vision-language model backbone to deeply integrate image features with natural
language instructions. ABC achieves bestfor-size performance on MSCOCO
image-to-text retrieval and is the top performing model on classification and
VQA tasks in the Massive Multimodal Embedding Benchmark. With a strongly
unified vision-language representation, ABC can use natural language to solve
subtle and potentially ambiguous visual retrieval problems. To evaluate this
capability, we design CtrlBench, a benchmark that requires interleaving textual
instructions with image content for correct retrieval. ABC advances the state
of multimodal embeddings by offering high-quality representations and flexible
natural language control. Our model and datasets are available at our project
page.","['cs.CV', 'cs.LG']",False,,,,MASTER: Multimodal Segmentation with Text Prompts,ABC: Achieving Better Control of Multimodal Embeddings using VLMs
neg-d2-340,2025-01-09,,2501.05505," The study of regular black holes and black hole mimickers as alternatives to
standard black holes has recently gained significant attention, driven both by
the need to extend general relativity to describe black hole interiors, and by
recent advances in observational technologies. Despite considerable progress in
this field, significant challenges remain in identifying and characterizing
physically well-motivated classes of regular black holes and black hole
mimickers. This report provides an overview of these challenges, and outlines
some of the promising research directions -- as discussed during a week-long
focus programme held at the Institute for Fundamental Physics of the Universe
(IFPU) in Trieste from November 11th to 15th, 2024.","['gr-qc', 'astro-ph.HE']",2501.0317," Microwave shielding is an important technique that can suppress the losses
that arise from collisions of ultracold polar molecules. It has been
instrumental in achieving molecular Bose-Einstein condensation (BEC) for NaCs
[Bigagli et al., Nature 631, 289 (2024)]. We demonstrate that microwave
shielding is universal, in the sense that the 2-body collision properties of
different molecules are very similar when expressed in suitable reduced units
of length and energy. This applies to rate coefficients for inelastic
scattering and loss, to scattering lengths, and to the properties of 2-molecule
bound states. We also explore the small deviations from universality that arise
at very large Rabi frequencies. In general, the collision properties are
near-universal except when the Rabi frequency exceeds a few percent of the
molecular rotational constant. The universality extends to elliptically
polarized microwaves and to combinations of multiple fields. Our results
indicate that the methods that have been used to achieve BEC for NaCs can be
transferred directly to most other polar molecules.","['cond-mat.quant-gas', 'physics.atom-ph']",False,,,,Towards a Non-singular Paradigm of Black Hole Physics,Universality in the microwave shielding of ultracold polar molecules
neg-d2-341,2025-01-09,,2501.05392," We study the dynamics of a qubit system interacting with thermalized
bath-ancilla spins via a repeated interaction scheme. Considering generic
initial conditions for the system and employing a Heisenberg-type interaction
between the system and the ancillas, we analytically prove the following: (i)
The population and coherences of the system qubit evolve independently toward a
nonequilibrium steady-state solution, which is diagonal in the qubit's energy
eigenbasis. The population relaxes to this state geometrically, whereas the
coherences decay through a more compound behavior. (ii) In the long time limit,
the system approaches a steady state that generally differs from the thermal
state of the ancilla. We derive this steady-state solution and show its
dependence on the interaction parameters and collision frequency. (iii) We
bound the number of interaction steps required to achieve the steady state
within a specified error tolerance, and we evaluate the energetic cost
associated with the process. Our key finding is that deterministic
system-ancilla interactions do not typically result in the system thermalizing
to the thermal state of the ancilla. Instead, they generate a distinct
nonequilibrium steady state, which we explicitly derive. However, we also
identify an operational regime that leads to thermalization with a few long and
possibly randomized collisions.",['quant-ph'],2502.10559," Accurate morphometric assessment of cartilage-such as thickness/volume-via
MRI is essential for monitoring knee osteoarthritis. Segmenting cartilage
remains challenging and dependent on extensive expert-annotated datasets, which
are heavily subjected to inter-reader variability. Recent advancements in
Visual Foundational Models (VFM), especially memory-based approaches, offer
opportunities for improving generalizability and robustness. This study
introduces a deep learning (DL) method for cartilage and meniscus segmentation
from 3D MRIs using interactive, memory-based VFMs. To improve spatial awareness
and convergence, we incorporated a Hybrid Shuffling Strategy (HSS) during
training and applied a segmentation mask propagation technique to enhance
annotation efficiency. We trained four AI models-a CNN-based 3D-VNet, two
automatic transformer-based models (SaMRI2D and SaMRI3D), and a
transformer-based promptable memory-based VFM (SAMRI-2)-on 3D knee MRIs from
270 patients using public and internal datasets and evaluated on 57 external
cases, including multi-radiologist annotations and different data acquisitions.
Model performance was assessed against reference standards using Dice Score
(DSC) and Intersection over Union (IoU), with additional morphometric
evaluations to further quantify segmentation accuracy. SAMRI-2 model, trained
with HSS, outperformed all other models, achieving an average DSC improvement
of 5 points, with a peak improvement of 12 points for tibial cartilage. It also
demonstrated the lowest cartilage thickness errors, reducing discrepancies by
up to threefold. Notably, SAMRI-2 maintained high performance with as few as
three user clicks per volume, reducing annotation effort while ensuring
anatomical precision. This memory-based VFM with spatial awareness offers a
novel approach for reliable AI-assisted knee MRI segmentation, advancing DL in
musculoskeletal imaging.","['eess.IV', 'cs.AI', 'cs.CV']",False,,,,"Equilibrium and nonequilibrium steady states with the repeated
  interaction protocol: Relaxation dynamics and energetic cost","SAMRI-2: A Memory-based Model for Cartilage and Meniscus Segmentation in
  3D MRIs of the Knee Joint"
neg-d2-342,2025-03-05,,2503.03348," This article presents a composite nonlinear feedback (CNF) control method
using self-triggered (ST) adaptive dynamic programming (ADP) algorithm in a
human-machine shared steering framework. For the overall system dynamics, a
two-degrees-of-freedom (2-DOF) vehicle model is established and a two-point
preview driver model is adopted. A dynamic authority allocation strategy based
on cooperation level is proposed to combine the steering input of the human
driver and the automatic controller. To make further improvements in the
controller design, three main contributions are put forward. Firstly, the CNF
controller is designed for trajectory tracking control with refined transient
performance. Besides, the self-triggered rule is applied such that the system
will update in discrete times to save computing resources and increase
efficiency. Moreover, by introducing the data-based ADP algorithm, the optimal
control problem can be solved through iteration using system input and output
information, reducing the need for accurate knowledge of system dynamics. The
effectiveness of the proposed control method is validated through
Carsim-Simulink co-simulations in diverse driving scenarios.","['eess.SY', 'cs.SY']",2501.15898," The homotopy category of a model structure on a weakly idempotent complete
additive category is proved to be equivalent to the additive quotient of the
category of cofibrant-fibrant objects with respect to the subcategory of
cofibrant-fibrant-trivial objects. A model structure on pointed category is
fibrant, if every object is a fibrant object. Fibrant model structures is
explicitly described by trivial cofibrations, and also by fibrations. Fibrantly
weak factorization systems are introduced, fibrant model structures are
constructed via fibrantly weak factorization systems, and a one-one
correspondence between fibrantly weak factorization systems and fibrant model
structures is given. Applications are given to rediscover the $\omega$-model
structures and the $\mathcal W$-model structures, and their relations with
exact model structures are discussed.",['math.RT'],False,,,,"Composite Nonlinear Trajectory Tracking Control of Co-Driving Vehicles
  Using Self-Triggered Adaptive Dynamic Programming",Homotopy categories and fibrant model structures
neg-d2-343,2025-01-31,,2501.19226," In this paper, we explore a taxonomy of connectivity for space-like
structures. It is inspired by isolating posets of connected pieces of a space
and examining its embedding in the ambient space. The taxonomy includes in its
scope all standard notions of connectivity in point-set and point-free
contexts, such as connectivity in graphs and hypergraphs (as well as
k-connectivity in graphs), connectivity and path-connectivity in topology, and
connectivity of elements in a frame.","['math.GN', 'math.CT', 'math.RA']",2501.04946," The least trimmed squares (LTS) estimator is a renowned robust alternative to
the classic least squares estimator and is popular in location, regression,
machine learning, and AI literature. Many studies exist on LTS, including its
robustness, computation algorithms, extension to non-linear cases, asymptotics,
etc. The LTS has been applied in the penalized regression in a high-dimensional
real-data sparse-model setting where dimension $p$ (in thousands) is much
larger than sample size $n$ (in tens, or hundreds). In such a practical
setting, the sample size $n$ often is the count of sub-population that has a
special attribute (e.g. the count of patients of Alzheimer's, Parkinson's,
Leukemia, or ALS, etc.) among a population with a finite fixed size N.
Asymptotic analysis assuming that $n$ tends to infinity is not practically
convincing and legitimate in such a scenario. A non-asymptotic or finite sample
analysis will be more desirable and feasible.
  This article establishes some finite sample (non-asymptotic) error bounds for
estimating and predicting based on LTS with high probability for the first
time.","['stat.ML', 'cs.LG']",False,,,,What is Connectivity?,"Non-asymptotic analysis of the performance of the penalized least
  trimmed squares in sparse models"
neg-d2-344,2025-02-04,,2502.02204," This study represents a first attempt to build a backcasting methodology to
identify the optimal policy roadmaps in transport systems. Specifically, it
considers a passenger car fleet subsystem, modelling its evolution and
greenhouse gas emissions. The policy decision under consideration is the
monetary incentive to the purchase of electric vehicles. This process is cast
as an optimal control problem with the objective to minimize the total budget
of the state and reach a desired CO$_2$ target. A case study applied to
Metropolitan France is presented to illustrate the approach. Additionally,
alternative policy scenarios are also analyzed.","['math.OC', 'cs.SY', 'eess.SY']",2503.12751," We present R3-Avatar, incorporating a temporal codebook, to overcome the
inability of human avatars to be both animatable and of high-fidelity rendering
quality. Existing video-based reconstruction of 3D human avatars either focuses
solely on rendering, lacking animation support, or learns a pose-appearance
mapping for animating, which degrades under limited training poses or complex
clothing. In this paper, we adopt a ""record-retrieve-reconstruct"" strategy that
ensures high-quality rendering from novel views while mitigating degradation in
novel poses. Specifically, disambiguating timestamps record temporal appearance
variations in a codebook, ensuring high-fidelity novel-view rendering, while
novel poses retrieve corresponding timestamps by matching the most similar
training poses for augmented appearance. Our R3-Avatar outperforms cutting-edge
video-based human avatar reconstruction, particularly in overcoming visual
quality degradation in extreme scenarios with limited training human poses and
complex clothing.",['cs.CV'],False,,,,"Backcasting Policies in Transport Systems as an Optimal Control Problem
  : An Example with Electric Vehicle Purchase Incentives","R3-Avatar: Record and Retrieve Temporal Codebook for Reconstructing
  Photorealistic Human Avatars"
neg-d2-345,2025-02-03,,2502.01312," Category-level object pose estimation aims to recover the rotation,
translation and size of unseen instances within predefined categories. In this
task, deep neural network-based methods have demonstrated remarkable
performance. However, previous studies show they suffer from spurious
correlations raised by ""unclean"" confounders in models, hindering their
performance on novel instances with significant variations. To address this
issue, we propose CleanPose, a novel approach integrating causal learning and
knowledge distillation to enhance category-level pose estimation. To mitigate
the negative effect of unobserved confounders, we develop a causal inference
module based on front-door adjustment, which promotes unbiased estimation by
reducing potential spurious correlations. Additionally, to further improve
generalization ability, we devise a residual-based knowledge distillation
method that has proven effective in providing comprehensive category
information guidance. Extensive experiments across multiple benchmarks
(REAL275, CAMERA25 and HouseCat6D) hightlight the superiority of proposed
CleanPose over state-of-the-art methods. Code will be released.",['cs.CV'],2503.14714," This paper revises aesthetics theory through the lens of authenticity and
investigates practical applications using a co-design approach. We encourage
designers to include ordinary clients as co-creators in the co-design process,
guiding them in expressing their aesthetics, values, and preferences while
stimulating their creativity. This paper proposes a bespoke design process
framework for authenticity aesthetics that incorporates empathy, defining,
ideating, prototyping, and testing. This framework delineates the roles and
responsibilities of clients and designers at different phases and highlights
evolving material mediums that enable their communication. The paper concludes
by reflecting on consumerist aesthetics, advocating for designers to focus on
the insights of ordinary clients, design for their authentic uniqueness, and
recognize the broad prospects of bespoke design methods.",['cs.HC'],False,,,,"CleanPose: Category-Level Object Pose Estimation via Causal Learning and
  Knowledge Distillation","Authenticity as Aesthetics: Enabling the Client to Dominate
  Decision-making in Co-design"
neg-d2-346,2025-03-04,,2503.02768," We develop a denotational model for programs that have standard programming
constructs such as conditionals and while-loops, as well as probabilistic and
concurrent commands. Whereas semantic models for languages with either
concurrency or randomization are well studied, their combination is limited to
languages with bounded loops. Our work is the first to consider both
randomization and concurrency for a language with unbounded looping constructs.
The interaction between Boolean tests (arising from the control flow
structures), probabilistic actions, and concurrent execution creates challenges
in generalizing previous work on pomsets and convex languages, prominent models
for those effects, individually. To illustrate the generality of our model, we
show that it recovers a typical powerdomain semantics for concurrency, as well
as the convex powerset semantics for probabilistic nondeterminism.","['cs.PL', 'cs.LO']",2503.14187," In this paper, we consider the inviscid limit problem to the higher
dimensional incompressible Navier-Stokes equations in the whole space. It was
proved in \cite[J. Funct. Anal., 276 (2019)]{GZ} that given initial data
$u_0\in B^{s}_{p,r}$ with $1\leq r<\infty$, the solutions of the Navier-Stokes
equations converge strongly in $B^{s}_{p,r}$ to the Euler equations as the
viscosity parameter tends to zero. In the case when $r=\infty$, we prove the
failure of the $B^{s}_{p,\infty}$-convergence of the Navier-Stokes equations
toward the Euler equations in the inviscid limit.",['math.AP'],False,,,,Denotational Semantics for Probabilistic and Concurrent Programs,"Non-convergence of the Navier-Stokes equations toward the Euler
  equations in weak Besov spaces"
neg-d2-347,2025-03-17,,2503.12885," Image-conditioned generation methods, such as depth- and canny-conditioned
approaches, have demonstrated remarkable abilities for precise image synthesis.
However, existing models still struggle to accurately control the content of
multiple instances (or regions). Even state-of-the-art models like FLUX and
3DIS face challenges, such as attribute leakage between instances, which limits
user control. To address these issues, we introduce DreamRenderer, a
training-free approach built upon the FLUX model. DreamRenderer enables users
to control the content of each instance via bounding boxes or masks, while
ensuring overall visual harmony. We propose two key innovations: 1) Bridge
Image Tokens for Hard Text Attribute Binding, which uses replicated image
tokens as bridge tokens to ensure that T5 text embeddings, pre-trained solely
on text data, bind the correct visual attributes for each instance during Joint
Attention; 2) Hard Image Attribute Binding applied only to vital layers.
Through our analysis of FLUX, we identify the critical layers responsible for
instance attribute rendering and apply Hard Image Attribute Binding only in
these layers, using soft binding in the others. This approach ensures precise
control while preserving image quality. Evaluations on the COCO-POS and
COCO-MIG benchmarks demonstrate that DreamRenderer improves the Image Success
Ratio by 17.7% over FLUX and enhances the performance of layout-to-image models
like GLIGEN and 3DIS by up to 26.8%. Project Page:
https://limuloo.github.io/DreamRenderer/.",['cs.CV'],2502.07366," A holobiont is made up of a host organism together with its microbiota. In
the context of animal breeding, as the holobiont can be viewed as the single
unit upon which selection operates, integrating microbiota data into genomic
prediction models may be a promising approach to improve predictions of
phenotypic and genetic values. Nevertheless, there is a paucity of hologenomic
transgenerational data to address this hypothesis, and thus to fill this gap,
we propose a new simulation framework. Our approach, an R Implementation of a
Transgenerational Hologenomic Model-based Simulator (RITHMS) is an open-source
package, builds upon the MoBPS package and incorporates distinctive
characteristics of the microbiota, notably vertical and horizontal transmission
as well as modulation due to the environment and host genetics. In addition,
RITHMS can account for a variety of selection strategies and is adaptable to
different genetic architectures. We simulated transgenerational hologenomic
data using RITHMS under a wide variety of scenarios, varying heritability,
microbiability, and microbiota heritability. We found that simulated data
accurately reflected expected characteristics, notably based on microbial
diversity metrics, correlation between taxa, modulation of vertical and
horizontal transmission, response to environmental effects and the evolution of
phenotypic values depending on selection strategy. Our results support the
relevance of our simulation framework and illustrate its possible use for
building a selection index balancing genetic gain and microbial diversity.
RITHMS is an advanced, flexible tool for generating transgenerational
hologenomic data that incorporate the complex interplay between genetics,
microbiota and environment.",['stat.ME'],False,,,,"DreamRenderer: Taming Multi-Instance Attribute Control in Large-Scale
  Text-to-Image Models","RITHMS : An advanced stochastic framework for the simulation of
  transgenerational hologenomic data"
neg-d2-348,2025-02-21,,2502.15239," Quantum computers require both scalability and high performance for practical
applications. While semiconductor quantum dots are promising candidates for
quantum bits, the complexity of measurement setups poses an important challenge
for scaling up these devices. Here, radio-frequency system-on-chip (RFSoC)
technology is exepcted for a promising approach that combines scalability with
flexibility. In this paper, we demonstrate RF reflectometry in gate-defined
bilayer graphene quantum devices using RFSoC-based measurement architecture. By
controlling the confinement strength through gate voltages, we achieve both
Fabry-P\'erot interferometer and quantum dot operations in a single device.
Although impedance matching conditions currently limit the measurement
sensitivity, we identify pathways for optimization through tunnel barrier
engineering and resonator design. These results represent a step toward
integrating high-bandwidth measurements with scalable quantum devices.",['cond-mat.mes-hall'],2503.18182," In this work, we apply topic modeling using Non-Negative Matrix Factorization
(NMF) on the COVID-19 Open Research Dataset (CORD-19) to uncover the underlying
thematic structure and its evolution within the extensive body of COVID-19
research literature. NMF factorizes the document-term matrix into two
non-negative matrices, effectively representing the topics and their
distribution across the documents. This helps us see how strongly documents
relate to topics and how topics relate to words. We describe the complete
methodology which involves a series of rigorous pre-processing steps to
standardize the available text data while preserving the context of phrases,
and subsequently feature extraction using the term frequency-inverse document
frequency (tf-idf), which assigns weights to words based on their frequency and
rarity in the dataset. To ensure the robustness of our topic model, we conduct
a stability analysis. This process assesses the stability scores of the NMF
topic model for different numbers of topics, enabling us to select the optimal
number of topics for our analysis. Through our analysis, we track the evolution
of topics over time within the CORD-19 dataset. Our findings contribute to the
understanding of the knowledge structure of the COVID-19 research landscape,
providing a valuable resource for future research in this field.",['cs.CL'],False,,,,"RFSoC-based radio-frequency reflectometry in gate-defined bilayer
  graphene quantum devices","Exploring Topic Trends in COVID-19 Research Literature using
  Non-Negative Matrix Factorization"
neg-d2-349,2025-03-22,,2503.17779," Over-extended Kac-Moody algebras contain so-called gradient structures - a
gl(d)-covariant level decomposition of the algebra contains strings of modules
at different levels that can be interpreted as spatial gradients. We present an
algebraic origin for this phenomenon, based on the recently introduced Lie
algebra extension of an over-extended Kac-Moody algebra by its fundamental
module, appearing in tensor hierarchy algebra super-extensions of over-extended
Kac-Moody algebras. The extensions are described in terms of Lie algebra
cohomology, vanishing for finite-dimensional simple Lie algebras, but
non-vanishing in relevant infinite-dimensional cases. The extension is
described in a few different gradings, where it is given a covariant
description with respect to different subalgebras. We expect the results to be
important for the connection between extended geometry and cosmological
billiards.","['hep-th', 'math.RT']",2502.08772," Protein flexibility, measured by the B-factor or Debye-Waller factor, is
essential for protein functions such as structural support, enzyme activity,
cellular communication, and molecular transport. Theoretical analysis and
prediction of protein flexibility are crucial for protein design, engineering,
and drug discovery. In this work, we introduce the persistent sheaf Laplacian
(PSL), an effective tool in topological data analysis, to model and analyze
protein flexibility. By representing the local topology and geometry of protein
atoms through the multiscale harmonic and non-harmonic spectra of PSLs, the
proposed model effectively captures protein flexibility and provides accurate,
robust predictions of protein B-factors. Our PSL model demonstrates an increase
in accuracy of 32% compared to the classical Gaussian network model (GNM) in
predicting B-factors for a dataset of 364 proteins. Additionally, we construct
a blind machine learning prediction method utilizing global and local protein
features. Extensive computations and comparisons validate the effectiveness of
the proposed PSL model for B-factor predictions.","['q-bio.BM', 'q-bio.QM']",False,,,,Gradient structures from extensions of over-extended Kac-Moody algebras,Persistent Sheaf Laplacian Analysis of Protein Flexibility
neg-d2-350,2025-03-10,,2503.07211," A striking feature of cavity quantum electrodynamics is the existence of
atom-photon bound states, which typically form when the coupling between the
atom and its environment are strong enough that after de-excitation the atom
can ``grab'' an emitted photon and re-absorb it, resulting in a virtual cloud
surrounding the atom. Here we will demonstrate the existence of bound states
that instead form in the case of weak coupling. Specifically, we show that when
a quantum emitter is weakly coupled to a structured reservoir exhibiting
topologically-protected surface states, hybridizations between these states and
the emitter can form, resulting in mid-gap bound states. We illustrate this
using a semi-infinite extension of the Su-Schrieffer-Heeger (SSH) model as our
reservoir. First, we diagonalize the bare semi-infinite SSH chain and reveal a
winding number that predicts only the edge state on the finite side of the
chain survives the semi-infinite extension. Then, after coupling the quantum
emitter to this end of the chain, we analyze the modified emitter spectrum and
reveal the existence of bound states in three parameter regions. Two of these
represent the usual strong-coupling bound states, while the third gives the
weak-coupling bound states with eigenvalue appearing in the SSH band gap and
which exhibit partial sublattice localization. We demonstrate that oscillations
between the weak-coupling bound states can be used to transfer the particle
from the emitter into the lattice in a predictable and reversible manner.",['quant-ph'],2503.13535," The advent of generative artificial intelligence (GAI) has brought about a
notable surge in the field of education. The use of GAI to support learning is
becoming increasingly prevalent among students. However, the manner and extent
of its utilisation vary considerably from one individual to another. And
researches about student's utilisation and perceptions of GAI remains
relatively scarce. To gain insight into the issue, this paper proposed a
hybrid-survey method to examine the impact of GAI on students across four
different grades in six key areas (LIPSAL): learning interest, independent
learning, problem solving, self-confidence, appropriate use, and learning
enjoyment. Firstly, through questionnaire, we found that among LIPSAL, GAI has
the greatest impact on the concept of appropriate use, the lowest level of
learning interest and self-confidence. Secondly, a comparison of four grades
revealed that the high and low factors of LIPSAL exhibited grade-related
variation, and college students exhibited a higher level than high school
students across LIPSAL. Thirdly, through interview, the students demonstrated a
comprehensive understanding of the application of GAI. We found that students
have a positive attitude towards GAI and are very willing to use it, which is
why GAI has grown so rapidly in popularity. They also told us prospects and
challenges in using GAI. In the future, as GAI matures technologically, it will
have an greater impact on students. These findings may help better understand
usage by different students and inform future research in digital education.","['cs.CY', 'cs.AI']",False,,,,Weak-coupling bound states in semi-infinite topological waveguide QED,"Unlocking Learning Potentials: The Transformative Effect of Generative
  AI in Education Across Grade Levels"
neg-d2-351,2025-01-22,,2501.13207," Observations of pressure modes ($p$-modes) in stars have enabled profound
insights into stellar properties, and theoretical stellar evolution and
oscillation models are integral to these inferences. However, modeling
uncertainties are often overlooked, even as they can rival or exceed
observational uncertainties. In this study, we quantify, for the first time,
the impact of structural resolution choices in 1D stellar evolution
calculations on predicted $p$-mode frequencies across the HR diagram, using
\texttt{MESA} and \texttt{GYRE}. We present measurements of resolution-based
modeling uncertainty for a range of solar-like, upper main-sequence, and Mira
oscillators and compare these directly to TESS observational uncertainties. We
demonstrate that resolution-driven uncertainties can significantly influence
theoretical predictions and in some cases overwhelm observational uncertainties
by orders of magnitude: while solar-like oscillators typically have fractional,
resolution-based uncertainties at or below 1\% of the test frequency,
fractional uncertainties in Miras were as large as 20\%. We also find that the
location and morphology of the RGB bump and red clump are impacted
substantially by resolution uncertainty. Stellar ages are impacted at the 10\%
level for young main-sequence stars, and the model-based correction factor for
the $\Delta\nu$--$\sqrt{\rho}$ scaling relation is impacted at the 2\% level.
Our results underscore the need to incorporate modeling uncertainties into
asteroseismic analyses and provide a reference framework for observers
evaluating the reliability of theoretical models.",['astro-ph.SR'],2501.03864," Consider the nonlinear stochastic heat equation $$
  \frac{\partial u (t,x)}{\partial t}=\frac{\partial^2 u (t,x)}{\partial x^2}+
\sigma(u (t,x))\dot{W}(t,x),\quad t> 0,\,
  x\in \mathbb{R}, $$ where $\dot W$ is a Gaussian noise which is white in time
and has the covariance of a fractional Brownian motion with Hurst parameter
$H\in(\frac 14,\frac 12)$ in the space variable. When $\sigma(0)=0$, the
well-posedness of the solution and its H\""older continuity have been proved by
Hu et al. \cite{HHLNT2017}. In this paper, we study the asymptotic properties
of the temporal gradient $u(t+\varepsilon, x)-u(t, x)$ at any fixed $t \ge 0$
and $x\in \mathbb R$, as $\varepsilon\downarrow 0$. As applications, we deduce
Khintchine's law of iterated logarithm, Chung's law of iterated logarithm, and
a result on the $q$-variations of the temporal process $\{u(t, x)\}_{t \ge 0}$,
where $x\in \mathbb R$ is fixed.",['math.PR'],False,,,,"Beyond MESA Defaults: The Impact of Structural Resolution Uncertainty in
  p-mode Asteroseismology","Temporal regularity for the stochastic heat equation with rough
  dependence in space"
neg-d2-352,2025-02-21,,2502.1598," Text-to-SQL models, which parse natural language (NL) questions to executable
SQL queries, are increasingly adopted in real-world applications. However,
deploying such models in the real world often requires adapting them to the
highly specialized database schemas used in specific applications. We find that
existing text-to-SQL models experience significant performance drops when
applied to new schemas, primarily due to the lack of domain-specific data for
fine-tuning. This data scarcity also limits the ability to effectively evaluate
model performance in new domains. Continuously obtaining high-quality
text-to-SQL data for evolving schemas is prohibitively expensive in real-world
scenarios. To bridge this gap, we propose SQLsynth, a human-in-the-loop
text-to-SQL data annotation system. SQLsynth streamlines the creation of
high-quality text-to-SQL datasets through human-LLM collaboration in a
structured workflow. A within-subjects user study comparing SQLsynth with
manual annotation and ChatGPT shows that SQLsynth significantly accelerates
text-to-SQL data annotation, reduces cognitive load, and produces datasets that
are more accurate, natural, and diverse. Our code is available at
https://github.com/adobe/nl_sql_analyzer.","['cs.HC', 'cs.AI', 'cs.DB']",2502.11343," In this paper, a new class of structured polynomials, which we dub the {\it
separable plus lower degree {\rm (SPLD in short)} polynomials}, is introduced.
The formal definition of an SPLD polynomial, which extends the concept of the
SPQ polynomial (Ahmadi et al. in Math Oper Res 48:1316--1343, 2023), is
defined. A type of bounded degree SOS hierarchy (BSOS-SPLD) is proposed to
efficiently solve the optimization problems with SPLD polynomials, and several
numerical examples are performed much better than the bounded degree SOS
hierarchy (Lasserre et al. in EURO J Comput Optim 5:87--117, 2017). An exact
SOS relaxation for a class of convex SPLD polynomial optimization problems is
proposed. Finally, an application of SPLD polynomials to polynomial regression
problems in statistics is presented.",['math.OC'],False,,,,"Text-to-SQL Domain Adaptation via Human-LLM Collaborative Data
  Annotation",SPLD polynomial optimization and bounded degree SOS hierarchies
neg-d2-353,2025-02-04,,2502.02503," In this paper, we demonstrate that in many NP-complete variants of the stable
matching problem, such as the Stable Hypergraph Matching problem, the Stable
Multicommodity Flow problem, and the College Admission problem with common
quotas, a near-feasible stable solution - that is, a solution which is stable,
but may slightly violate some capacities - always exists. Our results provide
strong theoretical guarantees that even under complex constraints, stability
can be restored with minimal capacity modifications.
  To achieve this, we present an iterative rounding algorithm that starts from
a stable fractional solution and systematically adjusts capacities to ensure
the existence of an integral stable solution. This approach leverages Scarf's
algorithm to compute an initial fractional stable solution, which serves as the
foundation for our rounding process. Notably, in the case of the Stable
Fixtures problem, where a stable fractional matching can be computed
efficiently, our method runs in polynomial time.
  These findings have significant practical implications for market design,
college admissions, and other real-world allocation problems, where small
adjustments to institutional constraints can guarantee stable and implementable
outcomes.",['cs.GT'],2501.02262," The growth of a large-scale magnetic field in the Sun and stars is usually
possible when the dynamo number (D) is above a critical value Dc. As the star
ages, its rotation rate and thus D decrease. Hence, the question is how far the
solar dynamo is from the critical dynamo transition. To answer this question,
we have performed a set of simulations using Babcock-Leighton type dynamo
models at different values of dynamo supercriticality and analyzed various
features of magnetic cycle. By comparing the recovery rates of the dynamo from
the Maunder minimum and statistics (numbers and durations) of the grand minima
and maxima with that of observations and we show that the solar dynamo is only
about two times critical and thus not highly supercritical. The observed
correlation between the polar field proxy and the following cycle amplitudes
and Gnevyshev-Ohl rule are also compatible with this conclusion.","['astro-ph.SR', 'physics.plasm-ph', 'physics.space-ph']",False,,,,Near-Feasible Solutions to Complex Stable Matching Problems,"Analyses of features of magnetic cycles at different amounts of dynamo
  supercriticality: Solar dynamo is about two times critical"
neg-d2-354,2025-02-09,,2502.06117," Dynamic graph clustering aims to detect and track time-varying clusters in
dynamic graphs, revealing the evolutionary mechanisms of complex real-world
dynamic systems. Matrix factorization-based methods are promising approaches
for this task; however, these methods often struggle with scalability and can
be time-consuming when applied to large-scale dynamic graphs. Moreover, they
tend to lack robustness and are vulnerable to real-world noisy data. To address
these issues, we make three key contributions. First, to improve scalability,
we propose temporal separated matrix factorization, where a single matrix is
divided into multiple smaller matrices for independent factorization, resulting
in faster computation. Second, to improve robustness, we introduce
bi-clustering regularization, which jointly optimizes graph embedding and
clustering, thereby filtering out noisy features from the graph embeddings.
Third, to further enhance effectiveness and efficiency, we propose selective
embedding updating, where we update only the embeddings of dynamic nodes while
the embeddings of static nodes are fixed among different timestamps.
Experimental results on six synthetic and five real-world benchmarks
demonstrate the scalability, robustness and effectiveness of our proposed
method. Source code is available at https://github.com/Clearloveyuan/DyG-MF.","['cs.LG', 'cs.AI', 'stat.ML']",2502.01694," A key paradigm to improve the reasoning capabilities of large language models
(LLMs) is to allocate more inference-time compute to search against a verifier
or reward model. This process can then be utilized to refine the pretrained
model or distill its reasoning patterns into more efficient models. In this
paper, we study inference-time compute by viewing chain-of-thought (CoT)
generation as a metastable Markov process: easy reasoning steps (e.g.,
algebraic manipulations) form densely connected clusters, while hard reasoning
steps (e.g., applying a relevant theorem) create sparse, low-probability edges
between clusters, leading to phase transitions at longer timescales. Under this
framework, we prove that implementing a search protocol that rewards sparse
edges improves CoT by decreasing the expected number of steps to reach
different clusters. In contrast, we establish a limit on reasoning capability
when the model is restricted to local information of the pretrained graph. We
also show that the information gained by search can be utilized to obtain a
better reasoning model: (1) the pretrained model can be directly finetuned to
favor sparse edges via policy gradient methods, and moreover (2) a compressed
metastable representation of the reasoning dynamics can be distilled into a
smaller, more efficient model.","['cs.AI', 'cs.LG', 'stat.ML']",False,,,,Revisiting Dynamic Graph Clustering via Matrix Factorization,"Metastable Dynamics of Chain-of-Thought Reasoning: Provable Benefits of
  Search, RL and Distillation"
neg-d2-355,2025-02-17,,2502.11911," In recent studies, analogs of the electronic Quantum Spin-Hall Effect have
been explored within photonic crystals that incorporate spatial symmetries,
especially those with $ C_{6v} $ symmetry, where $ \mathbb{Z}_2 $ topological
invariants are enforced by crystalline symmetry. These photonic crystals
possess bulk states with well-defined pseudospins and exhibit helical edge
states, closely resembling their electronic counterparts. However, achieving
$\mathbb{Z}_2$ topological protection in a square lattice photonic crystal
remains great theoretical and experimental challange. In this work, we propose
a single material photonic crystal structure based on a $ C_4 $ lattice that
supports partially $ \mathbb{Z}_2 $-protected edge modes. We show that this
structure can host photonic band-gap that hosts $ \mathbb{Z}_2 $-like modes,
enabling perfect transmission in waveguide applications. Furthermore, we
investigate the robustness of these modes against structural defects and
directional turns, highlighting the distinctions between full $ \mathbb{Z}_2 $
topological protection and partial topological protection. Finally, we analyze
the impact of the number of elementary cells surrounding the interface on the
formation and stability of these protected modes.",['physics.optics'],2502.00447," One of the most often used methods of summing divergent series in physics is
the Borel-type summation with control parameters improving convergence, which
are defined by some optimization conditions. The well known annoying problem in
this procedure is the occurrence of multiple solutions for control parameters.
We suggest a method for resolving this problem, based on the minimization of
cost functional. Control parameters can be introduced by employing the
Borel-Leroy or Mittag-Leffler transforms. Also, two novel transformations are
proposed using fractional integrals and fractional derivatives. New cost
functionals are advanced, based on lasso and ridge selection criteria, and
their performance is studied for a number of models. The developed method is
shown to provide good accuracy for the calculated quantities.","['math-ph', 'cond-mat.stat-mech', 'hep-ph', 'math.MP']",False,,,,Partial Topological Protection in C4 Lattices for Optical Communications,"Resolving the Problem of Multiple Control Parameters in Optimized
  Borel-Type Summation"
neg-d2-356,2025-02-12,,2502.08388," In this paper, we study the shadow and observational image of the Kerr-like
Loop Quantum Gravity (LQG) inspired black bounce with the help of the celestial
light source and the thin disk source by employing the backward ray-tracing
method. The results indicate that both the LQG parameter alpha and the rotation
parameter a contribute to a reduction in the shadow size; however, the
influence of a is predominant, while the effect of alpha circular orbit. One
can find that the correlation parameter (a, alpha), along with the observer's
inclination angle, affect the image's asymmetry and the distortion of the inner
shadow. As the inclination increases, the direct and lensed images diverge,
creating a structure resembling a hat. Meanwhile, we also investigate the
redshift distribution of the direct lensed images of the accretion disk under
different parameters and observation angle. The results show that the
distribution of redshift and observed intensity is obviously related to the
behavior of accretion flow. These results may provide a potential approach to
limit black hole parameters, detect quantum gravity effects, and distinguish
the LQG black hole from other black hole models.",['gr-qc'],2503.05805," This paper proposes a diffusion-based auto-bidding framework that leverages
graph representations to model large-scale auction environments. In such
settings, agents must dynamically optimize bidding strategies under constraints
defined by key performance indicator (KPI) metrics, all while operating in
competitive environments characterized by uncertain, sparse, and stochastic
variables. To address these challenges, we introduce a novel approach combining
learnable graph-based embeddings with a planning-based latent diffusion model
(LDM). By capturing patterns and nuances underlying the interdependence of
impression opportunities and the multi-agent dynamics of the auction
environment, the graph representation enable expressive computations regarding
auto-bidding outcomes. With reward alignment techniques, the LDM's posterior is
fine-tuned to generate auto-bidding trajectories that maximize KPI metrics
while satisfying constraint thresholds. Empirical evaluations on both
real-world and synthetic auction environments demonstrate significant
improvements in auto-bidding performance across multiple common KPI metrics, as
well as accuracy in forecasting auction outcomes.","['cs.LG', 'cs.AI', 'cs.MA']",False,,,,"The shadow and accretion disk images of the rotation loop quantum black
  bounce",Multi-agent Auto-Bidding with Latent Graph Diffusion Models
neg-d2-357,2025-03-09,,2503.06822," Clustering is a fundamental task in network analysis, essential for
uncovering hidden structures within complex systems. Edge clustering, which
focuses on relationships between nodes rather than the nodes themselves, has
gained increased attention in recent years. However, existing edge clustering
algorithms often overlook the significance of edge weights, which can represent
the strength or capacity of connections, and fail to account for noisy
edges--connections that obscure the true structure of the network. To address
these challenges, the Weighted Edge Clustering Adjusting for Noise (WECAN)
model is introduced. This novel algorithm integrates edge weights into the
clustering process and includes a noise component that filters out spurious
edges. WECAN offers a data-driven approach to distinguishing between meaningful
and noisy edges, avoiding the arbitrary thresholding commonly used in network
analysis. Its effectiveness is demonstrated through simulation studies and
applications to real-world datasets, showing significant improvements over
traditional clustering methods. Additionally, the R package ``WECAN'' has been
developed to facilitate its practical implementation.",['stat.CO'],2502.20639," Federated Learning (FL) facilitates collaborative training of a shared global
model without exposing clients' private data. In practical FL systems, clients
(e.g., edge servers, smartphones, and wearables) typically have disparate
system resources. Conventional FL, however, adopts a one-size-fits-all
solution, where a homogeneous large global model is transmitted to and trained
on each client, resulting in an overwhelming workload for less capable clients
and starvation for other clients. To address this issue, we propose FedConv, a
client-friendly FL framework, which minimizes the computation and memory burden
on resource-constrained clients by providing heterogeneous customized
sub-models. FedConv features a novel learning-on-model paradigm that learns the
parameters of the heterogeneous sub-models via convolutional compression.
Unlike traditional compression methods, the compressed models in FedConv can be
directly trained on clients without decompression. To aggregate the
heterogeneous sub-models, we propose transposed convolutional dilation to
convert them back to large models with a unified size while retaining
personalized information from clients. The compression and dilation processes,
transparent to clients, are optimized on the server leveraging a small public
dataset. Extensive experiments on six datasets demonstrate that FedConv
outperforms state-of-the-art FL systems in terms of model accuracy (by more
than 35% on average), computation and communication overhead (with 33% and 25%
reduction, respectively).","['cs.LG', 'cs.AI']",False,,,,Model-based edge clustering for weighted networks with a noise component,"FedConv: A Learning-on-Model Paradigm for Heterogeneous Federated
  Clients"
neg-d2-358,2025-02-18,,2502.13384," The derivative of a polynomial with all zeros on the unit circle has the
zeros of its derivative on or inside the unit circle. It has been observed that
in many cases the zeros of the derivative have a bimodal distribution: there
are two smaller circles near which it is more likely to find those zeros. We
identify the likely source of the second mode. This idea is supported with
numerical examples involving the characteristic polynomials of random unitary
matrices.","['math.CV', 'math.NT']",2502.1598," Text-to-SQL models, which parse natural language (NL) questions to executable
SQL queries, are increasingly adopted in real-world applications. However,
deploying such models in the real world often requires adapting them to the
highly specialized database schemas used in specific applications. We find that
existing text-to-SQL models experience significant performance drops when
applied to new schemas, primarily due to the lack of domain-specific data for
fine-tuning. This data scarcity also limits the ability to effectively evaluate
model performance in new domains. Continuously obtaining high-quality
text-to-SQL data for evolving schemas is prohibitively expensive in real-world
scenarios. To bridge this gap, we propose SQLsynth, a human-in-the-loop
text-to-SQL data annotation system. SQLsynth streamlines the creation of
high-quality text-to-SQL datasets through human-LLM collaboration in a
structured workflow. A within-subjects user study comparing SQLsynth with
manual annotation and ChatGPT shows that SQLsynth significantly accelerates
text-to-SQL data annotation, reduces cognitive load, and produces datasets that
are more accurate, natural, and diverse. Our code is available at
https://github.com/adobe/nl_sql_analyzer.","['cs.HC', 'cs.AI', 'cs.DB']",False,,,,The bimodal distribution in the derivative of unitary polynomials,"Text-to-SQL Domain Adaptation via Human-LLM Collaborative Data
  Annotation"
neg-d2-359,2025-03-12,,2503.09809," Thom polynomials provide universal formulas for the fundamental class of
singularity loci in terms of characteristic classes. Ohmoto extended this
notion to SSM-Thom polynomials, which refine this description by capturing the
richer Segre-Schwartz-MacPherson (SSM) class of singularity loci. While
previous methods for computing SSM-Thom polynomials relied on intricate
geometric arguments, we introduce a more efficient approach that depends solely
on the symmetries of singularities. Our method is inspired by connections to
Geometric Representation Theory, particularly the interpolation properties of
Maulik-Okounkov stable envelopes. By formulating SSM analogs of these axioms
within a degree-bounded framework, we obtain new computational tools for
SSM-Thom polynomials. We also present explicit examples of SSM-Thom
polynomials, and illustrate their applications in enumerative geometry and
singularity theory.",['math.AG'],2501.0969," We revisit the theory of operator-valued free convolution powers given by a
completely positive map $\eta$. We first give a general result, with a new
analytic proof, that the $\eta$-convolution power of the law of $X$ is realized
by $V^*XV$ for any operator $V$ satisfying certain conditions, which unifies
Nica and Speicher's construction in the scalar-valued setting and
Shlyakhtenko's construction in the operator-valued setting. Second, we provide
an analog, for the setting of $\eta$-valued convolution powers, of the analytic
subordination for conditional expectations that holds for additive free
convolution. Finally, we describe a Hilbert-space manipulation that explains
the equivalence between the $n$-fold additive free convolution and the
convolution power with respect to $\eta = n \operatorname{id}$.",['math.OA'],False,,,,Interpolation characterization of higher Thom polynomials,"Operator models and analytic subordination for operator-valued free
  convolution powers"
neg-d2-360,2025-01-27,,2501.15898," The homotopy category of a model structure on a weakly idempotent complete
additive category is proved to be equivalent to the additive quotient of the
category of cofibrant-fibrant objects with respect to the subcategory of
cofibrant-fibrant-trivial objects. A model structure on pointed category is
fibrant, if every object is a fibrant object. Fibrant model structures is
explicitly described by trivial cofibrations, and also by fibrations. Fibrantly
weak factorization systems are introduced, fibrant model structures are
constructed via fibrantly weak factorization systems, and a one-one
correspondence between fibrantly weak factorization systems and fibrant model
structures is given. Applications are given to rediscover the $\omega$-model
structures and the $\mathcal W$-model structures, and their relations with
exact model structures are discussed.",['math.RT'],2503.07754," The physical properties of stellar atmospheres in rapidly rotating massive
stars, such as Be stars, are critical to understanding their evolution and
their role as progenitors of supernovae. These stars, which often have
near-critical rotation, exhibit equatorial stretching and gravity darkening,
which significantly complicates the determination of parameters such as the
inclination angle. Be stars, characterized by their extreme rotational
velocities, serve as excellent candidates for exploring these phenomena.
However, fundamental quantities such as polar and equatorial radii and
inclination angles are typically derived from interferometry, which applies
only to a limited number of stars. This study aims to enhance the determination
of inclination angles for Be stars using the ZPEKTR spectral synthesis code. By
incorporating advanced models of gravity darkening and stellar deformation, we
evaluated the effectiveness of this method with a sample of ten Be stars from
the BeSOS database, comparing results with established interferometric data.
Methods. We used the ZPEKTR code to model the effects of stellar oblateness and
gravity darkening on spectral lines, focusing on the HeI 4471 line. We applied
a chi-squared test minimization approach to identify the best-fitting models,
and we evaluated the inclination angles derived against interferometric
measurements. Our analysis reveals a robust linear correlation between the
inclination angles derived from ZPEKTR and using interferometric techniques,
which demonstrates an excellent agreement. The ZPEKTR code effectively models
high rotational velocity effects, providing precise stellar parameter
determinations. The results underscore the potential of advanced spectroscopic
techniques to yield inclination measurements comparable to interferometry,
which offers a pathway to studying distant massive stars.",['astro-ph.SR'],False,,,,Homotopy categories and fibrant model structures,Unveiling stellar spin: Determining inclination angles in Be stars
neg-d2-361,2025-02-03,,2502.01302," Dynamical tide consists of various waves that can resonate with orbital
motion. We test this coupling of dynamical tide and orbital motion using a
simple two-dimensional shallow water model, which can be applied to a rocky
planet covered with thin ocean or atmosphere. Then we take the earth-moon
system as a fiducial model to calculate the tidal resonances and orbital
evolution. We find that tidal dissipation can even increase with increasing
orbital separation because of the coupling of dynamical tide and orbital
motion. We draw the conclusion that the coupling is not negligible to study the
orbital evolution on secular timescale.","['astro-ph.EP', 'astro-ph.SR', 'physics.geo-ph']",2502.20342," Sensing technology is an important aspect of information processing. Current
development in artificial intelligence systems (especially those aimed at
medical and environmental applications) requires a lot of data on the chemical
composition of biological fluids or environmental samples. These complex
matrices require advanced sensing devices, and photoelectrochemical ones seem
to have potential to overcome at least some of the obstacles. Furthermore, the
development of artificial intelligence (AI) technology for autonomous robotics
requires technology mimicking human senses, also those operating at the
molecular level, such as gustation and olfaction. Again, photoelectrochemical
sensing can provide some suitable solutions. In this review, we introduce the
idea of integration of photoelectrochemical sensors with some unconventional
computing paradigm - reservoir computing. This approach should not only boost
the performance of the sensors itself, but also open new pathways through
science. Integration of sensing devices with computing systems will also
contribute to a better understanding (or at least mimicking) of the human
senses and neuromorphic sensory information processing. Although reservoir
systems can be considered magic ""black boxes"" and their operation is at the
same time simple and hard to comprehend, this combination is expected to open a
new era of effective information harvesting and processing systems.","['physics.ins-det', 'cs.ET']",False,,,,Coupling of dynamical tide and orbital motion,"Reservoir Computing and Photoelectrochemical Sensors: A Marriage of
  Convenience"
neg-d2-362,2025-01-06,,2501.0346," We extend the result of Blumberg and Mandell on K-theoretic Tate-Poitou
duality at odd primes which serves as a spectral refinement of the classical
arithmetic Tate-Poitou duality. The duality is formulated for the
$K(1)$-localized algebraic K-theory of the ring of $p$-integers in a number
field and its completion using the $\bZ_p$-Anderson duality. This paper
completes the picture by addressing the prime 2, where the real embeddings of
number fields introduce extra complexities. As an application, we identify the
homotopy type at prime 2 of the homotopy fiber of the cyclotomic trace for the
sphere spectrum in terms of the algebraic K-theory of the integers.","['math.KT', 'math.AT']",2503.12233," A robust full-space physical layer security (PLS) transmission scheme is
proposed in this paper considering the full-space wiretapping challenge of
wireless networks supported by simultaneous transmitting and reflecting
reconfigurable intelligent surface (STAR-RIS). Different from the existing
schemes, the proposed PLS scheme takes account of the uncertainty on the
eavesdropper's position within the 360$^\circ$ service area offered by the
STAR-RIS. Specifically, the large system analytical method is utilized to
derive the asymptotic expression of the average security rate achieved by the
security user, considering that the base station (BS) only has the statistical
information of the eavesdropper's channel state information (CSI) and the
uncertainty of its location. To evaluate the effectiveness of the proposed PLS
scheme, we first formulate an optimization problem aimed at maximizing the
weighted sum rate of the security user and the public user. This optimization
is conducted under the power allocation constraint, and some practical
limitations for STAR-RIS implementation, through jointly designing the active
and passive beamforming variables. A novel iterative algorithm based on the
minimum mean-square error (MMSE) and cross-entropy optimization (CEO) methods
is proposed to effectively address the established non-convex optimization
problem with discrete variables. Simulation results indicate that the proposed
robust PLS scheme can effectively mitigate the information leakage across the
entire coverage area of the STAR-RIS-assisted system, leading to superior
performance gain when compared to benchmark schemes encompassing traditional
RIS-aided scheme.","['cs.IT', 'eess.SP', 'math.IT']",False,,,,K-theoretic Tate-Poitou duality at prime 2,"Robust Full-Space Physical Layer Security for STAR-RIS-Aided Wireless
  Networks: Eavesdropper with Uncertain Location and Channel"
neg-d2-363,2025-02-25,,2502.17968," We consider the defocusing Calogero--Moser derivative nonlinear
Schr{\""o}dinger equation\begin{align*}i \partial_{t} u+\partial_{x}^2 u-2\Pi
D\left(|u|^{2}\right)u=0, \quad (t,x ) \in \mathbb{R} \times
\mathbb{R}\end{align*}posed on $E := \left\{u \in L^{\infty}(\mathbb{R}): u'
\in L^{2}(\mathbb{R}), u'' \in L^{2}(\mathbb{R}), |u|^{2}-1 \in
L^{2}(\mathbb{R})\right\}$. We prove the global well-posedness of this equation
in $E$. Moreover, we give an explicit formula for the chiral solution to this
equation.",['math.AP'],2503.01474," Interactive navigation is crucial in scenarios where proactively interacting
with objects can yield shorter paths, thus significantly improving traversal
efficiency. Existing methods primarily focus on using the robot body to
relocate large obstacles (which could be comparable to the size of a robot).
However, they prove ineffective in narrow or constrained spaces where the
robot's dimensions restrict its manipulation capabilities. This paper
introduces a novel interactive navigation framework for legged manipulators,
featuring an active arm-pushing mechanism that enables the robot to reposition
movable obstacles in space-constrained environments. To this end, we develop a
reinforcement learning-based arm-pushing controller with a two-stage reward
strategy for large-object manipulation. Specifically, this strategy first
directs the manipulator to a designated pushing zone to achieve a kinematically
feasible contact configuration. Then, the end effector is guided to maintain
its position at appropriate contact points for stable object displacement while
preventing toppling. The simulations validate the robustness of the arm-pushing
controller, showing that the two-stage reward strategy improves policy
convergence and long-term performance. Real-world experiments further
demonstrate the effectiveness of the proposed navigation framework, which
achieves shorter paths and reduced traversal time. The open-source project can
be found at
https://github.com/Zhihaibi/Interactive-Navigation-for-legged-manipulator.git.",['cs.RO'],False,,,,"The defocusing Calogero--Moser derivative nonlinear Schr{\""o}dinger
  equation with a nonvanishing condition at infinity","Interactive Navigation for Legged Manipulators with Learned Arm-Pushing
  Controller"
neg-d2-364,2025-02-19,,2502.14105," Conformal prediction provides a powerful framework for constructing
prediction intervals with finite-sample guarantees, yet its robustness under
distribution shifts remains a significant challenge. This paper addresses this
limitation by modeling distribution shifts using L\'evy-Prokhorov (LP)
ambiguity sets, which capture both local and global perturbations. We provide a
self-contained overview of LP ambiguity sets and their connections to popular
metrics such as Wasserstein and Total Variation. We show that the link between
conformal prediction and LP ambiguity sets is a natural one: by propagating the
LP ambiguity set through the scoring function, we reduce complex
high-dimensional distribution shifts to manageable one-dimensional distribution
shifts, enabling exact quantification of worst-case quantiles and coverage.
Building on this analysis, we construct robust conformal prediction intervals
that remain valid under distribution shifts, explicitly linking LP parameters
to interval width and confidence levels. Experimental results on real-world
datasets demonstrate the effectiveness of the proposed approach.","['stat.ML', 'cs.LG', 'math.ST', 'stat.ME', 'stat.TH']",2501.02139," The space writhe of a knot is a property of its three-dimensional embedding
that contains information about its underlying topology, but the correspondence
between space writhe and other topological invariants is not fully understood.
We perform Langevin dynamics simulations of knotted semiflexible polymers and
measure their ensemble average space writhe. We show that for all knots up to
10 crossings, alternating and non-alternating, the average space writhe is
almost equal to that of the tightest known configuration of the same knot, with
minor differences. Using this equivalence, we show that for more complex knots
with up to 38 crossings, the average space writhe is strongly correlated with
the signature of the knot. This establishes that the connection between
signature and space writhe holds at larger crossing numbers.","['cond-mat.soft', 'math.GT']",False,,,,"Conformal Prediction under L\'evy-Prokhorov Distribution Shifts:
  Robustness to Local and Global Perturbations",The space writhes and signatures of polymer knots
neg-d2-365,2025-01-23,,2501.13988," Most studies on environmental perception for autonomous vehicles (AVs) focus
on urban traffic environments, where the objects/stuff to be perceived are
mainly from man-made scenes and scalable datasets with dense annotations can be
used to train supervised learning models. By contrast, it is hard to densely
annotate a large-scale off-road driving dataset manually due to the inherently
unstructured nature of off-road environments. In this paper, we propose a
Multimodal Contrastive Representation Learning approach for Off-Road
environmental perception, namely MCRL4OR. This approach aims to jointly learn
three encoders for processing visual images, locomotion states, and control
actions by aligning the locomotion states with the fused features of visual
images and control actions within a contrastive learning framework. The
causation behind this alignment strategy is that the inertial locomotion state
is the result of taking a certain control action under the current
landform/terrain condition perceived by visual sensors. In experiments, we
pre-train the MCRL4OR with a large-scale off-road driving dataset and adopt the
learned multimodal representations for various downstream perception tasks in
off-road driving scenarios. The superior performance in downstream tasks
demonstrates the advantages of the pre-trained multimodal representations. The
codes can be found in \url{https://github.com/1uciusy/MCRL4OR}.","['cs.RO', 'cs.AI', 'cs.CV']",2502.12602," This paper presents a novel learning-based approach to dynamic robot-to-human
handover, addressing the challenges of delivering objects to a moving receiver.
We hypothesize that dynamic handover, where the robot adjusts to the receiver's
movements, results in more efficient and comfortable interaction compared to
static handover, where the receiver is assumed to be stationary. To validate
this, we developed a nonparametric method for generating continuous handover
motion, conditioned on the receiver's movements, and trained the model using a
dataset of 1,000 human-to-human handover demonstrations. We integrated
preference learning for improved handover effectiveness and applied impedance
control to ensure user safety and adaptiveness. The approach was evaluated in
both simulation and real-world settings, with user studies demonstrating that
dynamic handover significantly reduces handover time and improves user comfort
compared to static methods. Videos and demonstrations of our approach are
available at https://zerotohero7886.github.io/dyn-r2h-handover .",['cs.RO'],False,,,,"MCRL4OR: Multimodal Contrastive Representation Learning for Off-Road
  Environmental Perception",Learning-based Dynamic Robot-to-Human Handover
neg-d2-366,2025-01-27,,2501.16412," Neutrino masses may have evolved dynamically throughout the history of the
Universe, potentially leading to a mass spectrum distinct from the normal or
inverted ordering observed today. While cosmological measurements constrain the
total energy density of neutrinos, they are not directly sensitive to a
dynamically changing mass ordering unless future surveys achieve exceptional
precision in detecting the distinct imprints of each mass eigenstate on
large-scale structures. In this work, we investigate the impact of a dynamic
neutrino mass spectrum on the diffuse supernova neutrino background (DSNB),
which is composed of neutrinos from all supernova explosions throughout cosmic
history and is on the verge of experimental detection. Since neutrino
oscillations are highly sensitive to the mass spectrum, we show that the
electron neutrino survival probability carries distinct signatures of the
evolving neutrino mass spectrum. Our results indicate that the resulting
modifications to the DSNB spectrum would exhibit unique energy-dependent
features. These features are distinguishable from the effects of significant
astrophysical uncertainties, providing a potential avenue for probing the
dynamic nature of neutrino masses.",['hep-ph'],2502.19632," We present extensive proper motion measurements of the Crab Nebula made from
Canada-France-Hawaii Telescope MegaPrime/MegaCam images taken in 2007, 2016,
and 2019. A total of 19974 proper motion vectors with uncertainty
$<10$\,mas\,yr$^{-1}$ located over the majority of the Crab Nebula are used to
map the supernova remnant's two-dimensional expansion properties that reflect
the dynamics of the original explosion, acceleration of ejecta imparted by
spin-down energy from the pulsar, and interaction between the ejecta and
surrounding cicumstellar material (CSM). The average convergence date we derive
is 1105.5 $\pm$ 0.5 CE, which is 15-35 yr earlier compared to most previous
estimates. We find that it varies as a function of position angle around the
nebula, with the earliest date and smallest proper motions measured along the
equator defined by the east and west bays. The lower acceleration of material
along the equatorial plane may be indicative of the supernova's interaction
with a disk-like CSM geometry. Comparing our measurements to previous
analytical solutions of the Crab's expansion and our own numerical simulation
using the moving mesh hydrodynamics code \texttt{Sprout}, we conclude that the
ejecta have relaxed closer to homologous expansion than expected for the
commonly adopted pulsar spindown age of $\tau \sim 700$ yr and a pulsar wind
nebula (PWN) still evolving inside the flat part of the ejecta density profile.
These findings provide further evidence that the PWN has broken out of the
inner flat part of the supernova ejecta density profile and has experienced
``blowout''.","['astro-ph.HE', 'astro-ph.GA', 'astro-ph.SR']",False,,,,"Dynamic Neutrino Mass Ordering and Its Imprint on the Diffuse Supernova
  Neutrino Background",The Non-Uniform Expansion of the Crab Nebula
neg-d2-367,2025-02-03,,2502.01219," Dynamical systems can be coupled in a manner that is designed to drive the
resulting dynamics onto a specified lower dimensional submanifold in the phase
space of the combined system. On the submanifold, the variables of the two
systems have a well-defined unique functional relationship. This process can
thus be viewed as a control technique that ensures generalized synchronization.
Depending on the nature of the dynamical systems and the specified submanifold,
different coupling functions can be derived in order to achieve a desired
control objective. We discuss the circuit implementations of this strategy in
representative examples of coupled chaotic dynamical systems, namely Lorenz
oscillators",['nlin.CD'],2502.12408," Recent advances in speech foundation models are largely driven by scaling
both model size and data, enabling them to perform a wide range of tasks,
including speech recognition. Traditionally, ASR models are evaluated using
metrics like Word Error Rate (WER) and Character Error Rate (CER), which depend
on ground truth labels. As a result of limited labeled data from diverse
domains and testing conditions, the true generalization capabilities of these
models beyond standard benchmarks remain unclear. Moreover, labeling data is
both costly and time-consuming. To address this, we propose a novel label-free
approach for approximating ASR performance metrics, eliminating the need for
ground truth labels. Our method utilizes multimodal embeddings in a unified
space for speech and transcription representations, combined with a
high-quality proxy model to compute proxy metrics. These features are used to
train a regression model to predict key ASR metrics like Word Error Rate (WER)
and Character Error Rate (CER). We experiment with over 40 models across 14
datasets representing both standard and in-the-wild testing conditions. Our
results show that we approximate the metrics within a single-digit absolute
difference across all experimental configurations, outperforming the most
recent baseline by more than 50\%.",['cs.CL'],False,,,,Control Strategy for Generalized Synchrony in Coupled Dynamical Systems,On the Robust Approximation of ASR Metrics
neg-d2-368,2025-01-30,,2501.18203," Extended warranties (EWs) are significant source of revenue for
capital-intensive products like automobiles. Such products consist of multiple
subsystems, providing flexibility in EW customization, for example, bundling a
tailored set of subsystems in an EW contract. This, in turn, enables the
creation of a service menu with different EW contract options. From the
perspective of a third-party EW provider servicing a fleet of automobile
brands, we develop a novel model to jointly optimize the design and pricing of
EWs in order to maximize the profit. Specifically, the problem is to determine
which contracts should be included in the EW menu and identify the appropriate
price for each contract. As the complexity of the joint optimization problem
increases exponentially with the number of subsystems, two solution approaches
are devised to solve the problem. The first approach is based on a
mixed-integer second-order cone programming reformulation, which guarantees
optimality but is applicable only for a small number of subsystems. The second
approach utilizes a two-step iteration process, offering enhanced computational
efficiency in scenarios with a large number of subsystems. Through numerical
experiments, the effectiveness of our model is validated, particularly in
scenarios characterized by high failure rates and a large number of subsystems.","['math.OC', 'cs.SY', 'eess.SY']",2502.1718," How much energy is required to unbind baryons from the cosmological
structures that originally bind them? This tutorial article explains why trying
to answer this question using just a halo model can be misleading. Instead, it
recommends parsing the universe into ``bound domains,'' which are the
gravitationally bound structures that ultimately become widely separated
islands as the universe evolves. It explains why a bound domain's potential
well was about as deep ~1 Gyr after the Big Bang as it is now, and it outlines
how future research might take advantage of a bound-domain approach to make
progress on some open questions about the baryon distributions in and around
galaxy groups and clusters.","['hep-ph', 'astro-ph.GA']",False,,,,"Joint Design and Pricing of Extended Warranties for Multiple Automobiles
  with Different Price Bands",Bound Domains
neg-d2-369,2025-01-13,,2501.07269," Let $k\leq n$ be positive integers and $\mathbb{Z}_{n}$ be the set of
integers modulo $n$. A conjecture of Baranyai from 1974 asks for a
decomposition of $k$-element subsets of $\mathbb{Z}_{n}$ into particular
families of sets called ""wreaths"". We approach this conjecture from a new
algebraic angle by introducing the key object of this paper, the wreath matrix
$M$. As our first result, we establish that Baranyai's conjecture is equivalent
to the existence of a particular vector in the kernel of $M$. We then employ
results from representation theory to study $M$ and its spectrum in detail. In
particular, we find all eigenvalues of $M$ and their multiplicities, and
identify several families of vectors which lie in the kernel of $M$.","['math.CO', 'math.RT']",2503.14893," Life cycle assessment (LCA) is a methodology for holistically measuring the
environmental impact of a product from initial manufacturing to end-of-life
disposal. However, the extent to which LCA informs the design of computing
devices remains unclear. To understand how this information is collected and
applied, we interviewed 17 industry professionals with experience in LCA or
electronics design, systematically coded the interviews, and investigated
common themes. These themes highlight the challenge of LCA data collection and
reveal distributed decision-making processes where responsibility for
sustainable design choices, and their associated costs, is often ambiguous. Our
analysis identifies opportunities for HCI technologies to support LCA
computation and its integration into the design process to facilitate
sustainability-oriented decision-making. While this work provides a nuanced
discussion about sustainable design in the information and communication
technologies (ICT) hardware industry, we hope our insights will also be
valuable to other sectors.",['cs.HC'],False,,,,The wreath matrix,"Incorporating Sustainability in Electronics Design: Obstacles and
  Opportunities"
neg-d2-370,2025-03-12,,2503.0993," This paper presents a nonlinear control strategy for an aerial cooperative
payload transportation system consisting of two quadrotor UAVs rigidly
connected to a payload. The system includes human physical interaction
facilitated by an admittance control. The proposed control framework integrates
an adaptive Backstepping controller for the position subsystem and a Fast
Nonsingular Terminal Sliding Mode Control (FNTSMC) for the attitude subsystem
to ensure asymptotic stabilization. The admittance controller interprets the
interaction forces from the human operator, generating reference trajectories
for the position controller to ensure accurate tracking of the operator's
guidance. The system aims to assist humans in payload transportation, providing
both stability and responsiveness. The robustness and effectiveness of the
proposed control scheme in maintaining system stability and performance under
various conditions are presented.","['eess.SY', 'cs.SY']",2502.10561," Landmarks are critical in navigation, supporting self-orientation and mental
model development. Similar to sighted people, people with low vision (PLV)
frequently look for landmarks via visual cues but face difficulties identifying
some important landmarks due to vision loss. We first conducted a formative
study with six PLV to characterize their challenges and strategies in landmark
selection, identifying their unique landmark categories (e.g., area
silhouettes, accessibility-related objects) and preferred landmark
augmentations. We then designed VisiMark, an AR interface that supports
landmark perception for PLV by providing both overviews of space structures and
in-situ landmark augmentations. We evaluated VisiMark with 16 PLV and found
that VisiMark enabled PLV to perceive landmarks they preferred but could not
easily perceive before, and changed PLV's landmark selection from only
visually-salient objects to cognitive landmarks that are more important and
meaningful. We further derive design considerations for AR-based landmark
augmentation systems for PLV.",['cs.HC'],False,,,,"Human Physical Interaction based on UAV Cooperative Payload
  Transportation System using Adaptive Backstepping and FNTSMC","VisiMark: Characterizing and Augmenting Landmarks for People with Low
  Vision in Augmented Reality to Support Indoor Navigation"
neg-d2-371,2025-02-26,,2502.19454," Recent developments in Video Diffusion Models (VDMs) have demonstrated
remarkable capability to generate high-quality video content. Nonetheless, the
potential of VDMs for creating transparent videos remains largely uncharted. In
this paper, we introduce TransVDM, the first diffusion-based model specifically
designed for transparent video generation. TransVDM integrates a Transparent
Variational Autoencoder (TVAE) and a pretrained UNet-based VDM, along with a
novel Alpha Motion Constraint Module (AMCM). The TVAE captures the alpha
channel transparency of video frames and encodes it into the latent space of
the VDMs, facilitating a seamless transition to transparent video diffusion
models. To improve the detection of transparent areas, the AMCM integrates
motion constraints from the foreground within the VDM, helping to reduce
undesirable artifacts. Moreover, we curate a dataset containing 250K
transparent frames for training. Experimental results demonstrate the
effectiveness of our approach across various benchmarks.",['cs.GR'],2502.00447," One of the most often used methods of summing divergent series in physics is
the Borel-type summation with control parameters improving convergence, which
are defined by some optimization conditions. The well known annoying problem in
this procedure is the occurrence of multiple solutions for control parameters.
We suggest a method for resolving this problem, based on the minimization of
cost functional. Control parameters can be introduced by employing the
Borel-Leroy or Mittag-Leffler transforms. Also, two novel transformations are
proposed using fractional integrals and fractional derivatives. New cost
functionals are advanced, based on lasso and ridge selection criteria, and
their performance is studied for a number of models. The developed method is
shown to provide good accuracy for the calculated quantities.","['math-ph', 'cond-mat.stat-mech', 'hep-ph', 'math.MP']",False,,,,"TransVDM: Motion-Constrained Video Diffusion Model for Transparent Video
  Synthesis","Resolving the Problem of Multiple Control Parameters in Optimized
  Borel-Type Summation"
neg-d2-372,2025-02-11,,2502.07691," We study a possibility of measuring the time-resolved second-order
autocorrelation function of one of two beams generated in type-II parametric
downconversion by means of temporal magnification of this beam, bringing its
correlation time from the picosecond to the nanosecond scale, which can be
resolved by modern photodetectors. We show that such a measurement enables one
to infer directly the degree of global coherence of that beam, which is linked
by a simple relation to the number of modes characterizing the entanglement
between the two generated beams. We illustrate the proposed method by an
example of photon pairs generated in a periodically poled KTP crystal with a
symmetric group velocity matching for various durations of the pump pulse,
resulting in different numbers of modes. Our theoretical model also shows that
the magnified double-heralded autocorrelation function of one beam exhibits a
local maximum around zero delay time, corresponding to photon bunching at a
short time scale.",['quant-ph'],2502.09528," Machine learning algorithms have enabled high quality stereo depth estimation
to run on Augmented and Virtual Reality (AR/VR) devices. However, high energy
consumption across the full image processing stack prevents stereo depth
algorithms from running effectively on battery-limited devices. This paper
introduces SteROI-D, a full stereo depth system paired with a mapping
methodology. SteROI-D exploits Region-of-Interest (ROI) and temporal sparsity
at the system level to save energy. SteROI-D's flexible and heterogeneous
compute fabric supports diverse ROIs. Importantly, we introduce a systematic
mapping methodology to effectively handle dynamic ROIs, thereby maximizing
energy savings. Using these techniques, our 28nm prototype SteROI-D design
achieves up to 4.35x reduction in total system energy compared to a baseline
ASIC.","['cs.CV', 'cs.AR']",False,,,,"Time-resolved second-order autocorrelation function of parametric
  downconversion","SteROI-D: System Design and Mapping for Stereo Depth Inference on
  Regions of Interest"
neg-d2-373,2025-03-21,,2503.17466," This paper provides a complete characterization of global hypoellipticity and
solvability with loss of derivatives for Fourier multiplier operators on the
$n$-dimensional torus. We establish necessary and sufficient conditions for
these properties and examine their connections with classical notions of global
hypoellipticity and solvability, particularly in relation to the closedness of
the operator's range.
  As an application, we explore the interplay between these properties and
number theory in the context of differential operators on the two-torus.
Specifically, we prove that the loss of derivatives in the solvability of the
vector field $\partial_{x_1} - \alpha \partial_{x_2}$ is precisely determined
by the well-known irrationality measure $\mu(\alpha)$ of its coefficient
$\alpha$. Furthermore, we analyze the wave operator $\partial_{x_1}^2 - \eta^2
\Delta_{\mathbb{T}^n}$ and show how the loss of derivatives depends explicitly
on the parameter $\eta > 0$.",['math.AP'],2501.1747," We explore how to detect the large quantum fluctuations in the throat of a
near-extremal black hole, where the dynamics are governed by the Schwarzian
theory. To this end, we scatter a low-frequency wave of a massless, minimal
scalar off the black hole and calculate the absorption cross-section. In the
semiclassical regime, where the Schwarzian is weakly coupled, we recover the
universal result that the cross-section equals the horizon area. However, in
the strongly coupled regime, where quantum fluctuations dominate, we find that
the absorption cross-section exceeds the semiclassical prediction. This result
may seem counterintuitive, given that the density of black hole states is
suppressed in this regime. Nevertheless, two effects outweigh this suppression.
First, quantum fluctuations enhance absorption transitions between individual
states, with the effect becoming stronger closer to the ground state. Second,
these fluctuations significantly reduce stimulated emission. We conclude that a
measurement showing an enhanced absorption cross-section serves as a clear
signature of the large quantum fluctuations in the geometry.","['hep-th', 'gr-qc']",False,,,,"Global Hypoellipticity and Solvability with Loss of Derivatives on the
  Torus",Quantum Cross-section of Near-extremal Black Holes
neg-d2-374,2025-03-02,,2503.01039," Superradiance can cause the axion cloud around a rotating black hole to reach
extremely high densities, and the decay of these axions can produce a powerful
laser. The electric field of these lasers is strong enough that the Schwinger
effect may become significant, resulting in the production of an
electron-positron plasma. We explore the dynamics between axion lasers and this
electron-positron plasma. While there are several mechanisms by which the
inclusion of a plasma can impact the laser's behavior, the most significant of
these mechanisms is that the electron-positron plasma imparts an effective mass
on the photon. As the plasma frequency increases, axion decay becomes
energetically unfavorable, up to the point where the axion no longer decays
into photons, shutting off the laser. We find that the impact of the
electron-positron plasma on the dynamics of the system depend heavily on the
parameters, specifically the axion mass $m_\phi$ and the superradiant coupling
$\alpha$, and that we may divide parameter space into three regimes: the
unenhanced, enhanced, and unstable regimes. In the unenhanced and enhanced
regime, the system will eventually settle into an equilibrium state, emitting a
laser of constant luminosity while the number of axions remains constant. In
the unenhanced regime, this equilibrium state can be calculated while
neglecting the effects of Schwinger production; in the enhanced regime, the
equilibrium luminosity is slightly larger than what it would be without
Schwinger production. In the unstable regime, the electron-positron plasma
suppresses axion decay to the point where the system is never able to reach
equilibrium; instead, the axions continue to grow superradiantly. In all three
cases, the production of superradiant axions will eventually cause the black
hole to spin down to the point where superradiance ceases.","['hep-ph', 'astro-ph.HE', 'gr-qc', 'hep-th']",2501.12282," This work shows new results on the complexity of games Jelly-No and Hanano
with various constraints on the size of the board and number of colours. Hanano
and Jelly-No are one-player, 2D side-view puzzle games with a dynamic board
consisting of coloured, movable blocks disposed on platforms. These blocks can
be moved by the player and are subject to gravity. Both games somehow vary in
their gameplay, but the goal is always to move the coloured blocks in order to
reach a specific configuration and make them interact with each other or with
other elements of the game. In Jelly-No the goal is to merge all coloured
blocks of a same colour, which also happens when they make contact. In Hanano
the goal is to make all the coloured blocks bloom by making contact with
flowers of the same colour. Jelly-No was proven by Chao Yang to be NP-Complete
under the restriction that all movable blocks are the same colour and NP-Hard
for more colours. Hanano was proven by Michael C. Chavrimootoo to be
PSPACE-Complete under the restriction that all movable blocks are the same
colour. However, the question whether Jelly-No for more than one colours is
also PSPACE-complete or if it too stays in NP was left open. In this paper, we
settle this question, proving that Jelly-No is PSPACE-Complete with an
unbounded number of colours. We further show that, if we allow black jellies
(that is, jellies that do not need to be merged), the game is PSPACE-complete
even for one colour. We further show that one-colour Jelly-No and Hanano remain
NP-Hard even if the width or the height of the board are small constants.",['cs.CC'],False,,,,The Role of the Schwinger Effect in Superradiant Axion Lasers,Complexity of Jelly-No and Hanano games with various constraints
neg-d2-375,2025-03-14,,2503.11955," We introduce a one parameter deformation of Zwegers' multivariable
$\mu$-function by applying iterations of the $q$-Borel summation method, which
is also a multivariate analogue of the generalized $\mu$-function introduced by
the authors. For this deformed multivariable $\mu$-function, we give some
formulas, for example, forward shift formula, translation and
$\mathfrak{S}_{N+1}$-symmetry. Further we mention modular formulas for the
Zwegers' original multivariable $\mu$-function.",['math.CA'],2501.08206," This paper proposes SAT-based techniques to calculate a specific normal form
of a given finite mathematical structure (model). The normal form is obtained
by permuting the domain elements so that the representation of the structure is
lexicographically smallest possible. Such a normal form is of interest to
mathematicians as it enables easy cataloging of algebraic structures. In
particular, two structures are isomorphic precisely when their normal forms are
the same. This form is also natural to inspect as mathematicians have been
using it routinely for many decades.
  We develop a novel approach where a SAT solver is used in a black-box fashion
to compute the smallest representative. The approach constructs the
representative gradually and searches the space of possible isomorphisms,
requiring a small number of variables. However, the approach may lead to a
large number of SAT calls and therefore we devise propagation techniques to
reduce this number. The paper focuses on finite structures with a single binary
operation (encompassing groups, semigroups, etc.). However, the approach is
generalizable to arbitrary finite structures. We provide an implementation of
the proposed algorithm and evaluate it on a variety of algebraic structures.",['cs.LO'],False,,,,A generalization of Zwegers' multivariable $\mu$-function,SAT-Based Techniques for Lexicographically Smallest Finite Models
neg-d2-376,2025-01-24,,2501.14415," Let $k$ be a field of characteristic zero. Let $m$ and $\alpha$ be positive
integers. For $n\geq 2$, let $R_n=k[x_1,x_2,\dots,x_n]$ with the $k$-derivation
$d_n$ given by
$d_n=(1-x_1x_2^{\alpha})\partial_{x_1}+x_1^m\partial_{x_2}+x_2\partial_{x_3}+\dots+x_{n-1}\partial_{x_n}$.
We prove that for integers $m\geq 2$ and $\alpha \geq 1$, $d_n$ is a simple
derivation on $R_n$ and $d_n(R_n)$ contains no units. This generalizes a result
of D. A. Jordan. We also show that the isotropy group of $d_n$ is conjugate to
a subgroup of translations.",['math.AC'],2503.02086," We report the discovery of an intriguing, low-mass galaxy-scale strong-lens
system in the SMACS J0723.3-7327 galaxy cluster. By modeling James Webb Space
Telescope imaging and Very Large Telescope Multi-Unit Spectroscopic Explorer
spectroscopic data, we find that the lens is cluster member galaxy at $z=0.397$
with an Einstein radius of $0^{\prime \prime}.424$ $\pm$ $0^{\prime
\prime}.012$, stellar mass of $M_* = (3.3 \pm 0.8) \times 10^{10} M_\odot$,
half-light radius of $\sim 1$ kpc, and central stellar velocity dispersion of
$140 \pm 6$ km s$^{-1}$. This lens galaxy is one of the few strong lens
galaxies known to date that have stellar mass as low as $M_* \sim 10^{10.5}
M_\odot$, offering an exceptional opportunity to peek into the population of
low-mass galaxies that has largely remained unexplored in the context of
strong-lensing studies. This strong lens system can also assist in assessing
the systematic uncertainty in the lens modeling of cluster member galaxies.",['astro-ph.GA'],False,,,,"Classes of simple derivations on polynomial rings $k[x_1,x_2, \ldots
  ,x_n]$",Discovery of A Low-mass Strong-lens System in SMACS J0723.3-7327
neg-d2-377,2025-03-19,,2503.1535," In the early stages of star formation, boundary layer accretion, where
protostars accrete material from disks extending down to their surfaces, plays
a crucial role. Understanding how a magneto-rotational-instability (MRI)-active
disk connects to a protostar's surface remains a significant challenge. To
investigate the mechanisms of mass and angular momentum transfer, we develop a
global, three-dimensional magnetohydrodynamic model of boundary layer accretion
around a magnetized, convective low-mass protostar. Our results reveal that
angular momentum transport mechanisms transition significantly from the outer
MRI-active disk to the protostellar surface. Various mechanisms--MRI, spiral
shocks, coronal accretion, jets, and disk winds--contribute to angular momentum
transfer, resulting in three distinct disk structures: (1) the MRI-active disk,
(2) the transition layer, and (3) the boundary layer. The simulated protostar
is strongly magnetized due to the accumulation of the disk fields, wrapping by
disk toroidal fields, and stellar dynamo activity. Magnetic concentrations
analogous to starspots form on the protostar and interact with the rotating
disk gas to generate spiral shocks. These shocks play a key role in driving
accretion. These findings demonstrate the necessity of global MHD models for a
comprehensive understanding of angular momentum transport. Additionally, we
identify explosive events triggered by magnetic reconnection in both the
protostar and the disk atmosphere. We also find decretion flows in the disk
midplane, which may be important for the radial transport of refractory
materials, such as Calcium-Aluminium-rich Inclusions (CAIs) precursor gas, to
the outer disk.","['astro-ph.SR', 'astro-ph.EP', 'astro-ph.HE']",2501.03818," We study the Dirichlet dynamical zeta function $\eta_D(s)$ for billiard flow
corresponding to several strictly convex disjoint obstacles. For large ${\rm
Re}\: s$ we have $\eta_D(s) =\sum_{n= 1}^{\infty} a_n e^{-\lambda_n s}, \: a_n
\in \mathbb R$ and $\eta_D$ admits a meromorphic continuation to $\mathbb C$.
We obtain some conditions of the frequencies $\lambda_n$ and some sums of
coefficients $a_n$ which imply that $\eta_D$ cannot be prolonged as entire
function.","['math.DS', 'math.NT']",False,,,,"Connecting a Magnetized Disk to a Convective Low-mass Protostar: A
  Global Three-dimensional Model of Boundary Layer Accretion",Dirichlet dynamical zeta function for billiard flow
neg-d2-378,2025-01-15,,2501.0863," The paper is concerned with the effect of the spatio-temporal heterogeneity
on the principal eigenvalue of some linear time-periodic parabolic system.
Various asymptotic behaviors of the principal eigenvalue and its monotonicity,
as a function of the diffusion rate and frequency, are first derived. In
particular, some singular behaviors of the principal eigenvalues are observed
when both diffusion rate and frequency approach zero, with some scalar
time-periodic Hamilton-Jacobi equation as the limiting equation. Furthermore,
we completely classify the topological structures of the level sets for the
principal eigenvalues in the plane of frequency and diffusion rate. Our results
not only generalize most of the findings in [S. Liu and Y. Lou, J. Funct.
Anal., 282 (2022), 109338] for scalar periodic-parabolic operators, but also
reveal more rich global information, for time-periodic parabolic systems, on
the dependence of the principal eigenvalues upon the spatio-temporal
heterogeneity.",['math.AP'],2502.01891," Human label variation (HLV) challenges the standard assumption that a
labelled instance has a single ground truth, instead embracing the natural
variation in human annotation to train and evaluate models. While various
training methods and metrics for HLV have been proposed, it is still unclear
which methods and metrics perform best in what settings. We propose new
evaluation metrics for HLV leveraging fuzzy set theory. Since these new
proposed metrics are differentiable, we then in turn experiment with employing
these metrics as training objectives. We conduct an extensive study over 6 HLV
datasets testing 14 training methods and 6 evaluation metrics. We find that
training on either disaggregated annotations or soft labels performs best
across metrics, outperforming training using the proposed training objectives
with differentiable metrics. We also show that our proposed soft metric is more
interpretable and correlates best with human preference.","['cs.LG', 'cs.CL']",False,,,,"On principal eigenvalues of linear time-periodic parabolic systems:
  symmetric mutation case",Training and Evaluating with Human Label Variation: An Empirical Study
neg-d2-379,2025-02-20,,2502.14812," We introduce the Byzantine Selection Problem, living at the intersection of
game theory and fault-tolerant distributed computing. Here, an event organizer
is presented with a group of $n$ agents, and wants to select $\ell < n$ of them
to form a team. For these purposes, each agent $i$ self-reports a positive
skill value $v_i$, and a team's value is the sum of its members' skill values.
Ideally, the value of the team should be as large as possible, which can be
easily achieved by selecting agents with the highest $\ell$ skill values.
However, an unknown subset of at most $t < n$ agents are byzantine and hence
not to be trusted, rendering their true skill values as $0$. In the spirit of
the distributed computing literature, the identity of the byzantine agents is
not random but instead chosen by an adversary aiming to minimize the value of
the chosen team. Can we still select a team with good guarantees in this
adversarial setting? As it turns out, deterministically, it remains optimal to
select agents with the highest $\ell$ values. Yet, if $t \geq \ell$, the
adversary can choose to make all selected agents byzantine, leading to a team
of value zero. To provide meaningful guarantees, one hence needs to allow for
randomization, in which case the expected value of the selected team needs to
be maximized, assuming again that the adversary plays to minimize it. For this
case, we provide linear-time randomized algorithms that maximize the expected
value of the selected team.","['cs.GT', 'cs.DC', 'cs.DS']",2501.03859," In this paper, we present a novel synergistic framework for learning shape
estimation and a shape-aware whole-body control policy for tendon-driven
continuum robots. Our approach leverages the interaction between two Augmented
Neural Ordinary Differential Equations (ANODEs) -- the Shape-NODE and
Control-NODE -- to achieve continuous shape estimation and shape-aware control.
The Shape-NODE integrates prior knowledge from Cosserat rod theory, allowing it
to adapt and account for model mismatches, while the Control-NODE uses this
shape information to optimize a whole-body control policy, trained in a Model
Predictive Control (MPC) fashion. This unified framework effectively overcomes
limitations of existing data-driven methods, such as poor shape awareness and
challenges in capturing complex nonlinear dynamics. Extensive evaluations in
both simulation and real-world environments demonstrate the framework's robust
performance in shape estimation, trajectory tracking, and obstacle avoidance.
The proposed method consistently outperforms state-of-the-art end-to-end,
Neural-ODE, and Recurrent Neural Network (RNN) models, particularly in terms of
tracking accuracy and generalization capabilities.",['cs.RO'],False,,,,Byzantine Game Theory: Sun Tzus Boxes,"A Synergistic Framework for Learning Shape Estimation and Shape-Aware
  Whole-Body Control Policy for Continuum Robots"
neg-d2-380,2025-02-06,,2502.044," Multimodal Federated Learning (MFL) enables multiple clients to
collaboratively train models on multimodal data while ensuring clients'
privacy. However, modality and task heterogeneity hinder clients from learning
a unified representation, weakening local model generalization, especially in
MFL with mixed modalities where only some clients have multimodal data. In this
work, we propose an Adaptive prototype-based Multimodal Federated Learning
(AproMFL) framework for mixed modalities and heterogeneous tasks to address the
aforementioned issues. Our AproMFL transfers knowledge through
adaptively-constructed prototypes without a prior public dataset. Clients
adaptively select prototype construction methods in line with tasks; server
converts client prototypes into unified multimodal prototypes and aggregates
them to form global prototypes, avoid clients keeping unified labels. We divide
the model into various modules and only aggregate mapping modules to reduce
communication and computation overhead. To address aggregation issues in
heterogeneity, we develop a client relationship graph-based scheme to
dynamically adjust aggregation weights. Extensive experiments on representative
datasets evidence effectiveness of AproMFL.","['cs.LG', 'cs.AI', 'cs.CR', 'cs.MM']",2502.13819," Let $A$ be an $n\times n$ random matrix with independent, identically
distributed mean 0, variance 1 subgaussian entries. We prove that $$
\mathbb{P}(A\text{ has distinct singular values})\geq 1-e^{-cn} $$ for some
$c>0$, confirming a conjecture of Vu. This result is then generalized to
singular values of rectangular random matrices with i.i.d. entries.
  We also prove that for two fixed real numbers $\lambda_1,\lambda_2$ with a
sufficient lower bound on $|\lambda_1-\lambda_2|$, we have a joint singular
value small ball estimate for any $\epsilon>0$ $$
\mathbb{P}(\sigma_{min}(A-\lambda_1I_n)\leq\epsilon
n^{-1/2},\sigma_{min}(A-\lambda_2I_n)\leq\epsilon n^{-1/2})\leq
C\epsilon^2+e^{-cn}, $$ where $\sigma_{min}(A)$ is the minimal singular value
of a square matrix $A$ and $I_n$ is the identity matrix. For much smaller
$|\lambda_1-\lambda_2|$ we derive a similar estimate with $C$ replaced by
$C\sqrt{n}/|\lambda_1-\lambda_2|$. This generalizes the one-point estimate of
Rudelson and Vershynin, which proves $\mathbb{P}(\sigma_{min}(A)\leq \epsilon
n^{-1/2})\leq C\epsilon+e^{-cn}$. Analogous two-point bounds are proven when
$A$ has i.i.d. real and complex parts, with $\epsilon^4$ in place of
$\epsilon^2$ on the right hand side of the estimate and for any complex numbers
$\lambda_1,\lambda_2$. These two point estimates can be used to derive strong
anticoncentration bounds for an arbitrary linear combination of two eigenvalues
of $A$.",['math.PR'],False,,,,"Adaptive Prototype Knowledge Transfer for Federated Learning with Mixed
  Modalities and Heterogeneous Tasks","Simplicity of singular value spectrum of random matrices and two-point
  quantitative invertibility"
neg-d2-381,2025-03-11,,2503.09028," Integration of Inverter-based Resources (IBRs) such as solar-powered plants
which lack the intrinsic characteristics such as the inertial response of the
traditional synchronous-generator (SG) based sources presents a new challenge
in the form of analyzing the grid stability under their presence. For example,
solar power is available for approximately from 9 AM-5 PM. However, the result
of the rise in power consumption after 6 PM and the reverting back to the
non-renewable source of power generation during that period puts immense stress
on the grid, testing the ramp limitations of the SGs. Failure to meet the
required power demand due to SG ramp limitations leads to failure of the power
grid and other catastrophes. Numerous mitigation techniques exist in order to
address the ramping issues with adding the energy storage elements (ESE) to the
grid being one. ESEs have higher ramping capabilities compared to the
traditional SGs. Also, the ESEs can store the energy and supply it to the grid
when required making them extremely responsive to high ramp situations.
However, the rate of degradation of the ESEs is faster than the SGs. This
raises an important issue of addressing the degradation of the ESEs while
meeting the required power demand objectives and constraints. This work
proposes a battery degradation-aware model predictive energy management
strategy and it is tested via a numerical simulation on multiple physical
systems such as Shipboard Power Systems (SPS). Moreover, the risk arising due
to the fault in the IBR is also studied by means of a numerical simulation.
Overall, the goal of this study is to make the existing power grid more robust,
resilient, and risk-free from component degradation and eventual failures.",['math.OC'],2501.0726," 3D semantic scene completion is critical for multiple downstream tasks in
autonomous systems. It estimates missing geometric and semantic information in
the acquired scene data. Due to the challenging real-world conditions, this
task usually demands complex models that process multi-modal data to achieve
acceptable performance. We propose a unique neural model, leveraging advances
from the state space and diffusion generative modeling to achieve remarkable 3D
semantic scene completion performance with monocular image input. Our technique
processes the data in the conditioned latent space of a variational autoencoder
where diffusion modeling is carried out with an innovative state space
technique. A key component of our neural network is the proposed Skimba (Skip
Mamba) denoiser, which is adept at efficiently processing long-sequence data.
The Skimba diffusion model is integral to our 3D scene completion network,
incorporating a triple Mamba structure, dimensional decomposition residuals and
varying dilations along three directions. We also adopt a variant of this
network for the subsequent semantic segmentation stage of our method. Extensive
evaluation on the standard SemanticKITTI and SSCBench-KITTI360 datasets show
that our approach not only outperforms other monocular techniques by a large
margin, it also achieves competitive performance against stereo methods. The
code is available at https://github.com/xrkong/skimba","['cs.CV', 'cs.AI']",False,,,,"Degradation-based Energy Management for Microgrids in the Presence of
  Energy Storage Elements",Skip Mamba Diffusion for Monocular 3D Semantic Scene Completion
neg-d2-382,2025-02-04,,2502.02531," We theoretically characterize gradient descent dynamics in deep linear
networks trained at large width from random initialization and on large
quantities of random data. Our theory captures the ``wider is better"" effect of
mean-field/maximum-update parameterized networks as well as hyperparameter
transfer effects, which can be contrasted with the neural-tangent
parameterization where optimal learning rates shift with model width. We
provide asymptotic descriptions of both non-residual and residual neural
networks, the latter of which enables an infinite depth limit when branches are
scaled as $1/\sqrt{\text{depth}}$. We also compare training with one-pass
stochastic gradient descent to the dynamics when training data are repeated at
each iteration. Lastly, we show that this model recovers the accelerated power
law training dynamics for power law structured data in the rich regime observed
in recent works.","['cs.LG', 'cond-mat.dis-nn', 'stat.ML']",2503.1204," Recently, Griffin, Ono, and Tsai examined the distribution of the number of
$t$-hooks in partitions of $n$, which was later followed by the work of Craig,
Ono, and Singh on the distribution of the number of $t$-hooks in self-conjugate
partitions of $n$. Motivated by these studies, in this paper, we further
investigate the number of $t$-hooks in some subsets of partitions. More
specifically, we obtain the generating functions for the number of $t$-hooks in
doubled distinct partitions and the number of $t$-shifted hooks in strict
partitions. Based on these generating functions, we prove that the number of
$t$-hooks in doubled distinct partitions and the number of $t$-shifted hooks in
strict partitions are both asymptotically normally distributed.","['math.CO', 'math.NT']",False,,,,"Deep Linear Network Training Dynamics from Random Initialization: Data,
  Width, Depth, and Hyperparameter Transfer",On the distribution of $t$-hooks of doubled distinct partitions
neg-d2-383,2025-02-20,,2502.14277," GK Persei, an old nova and intermediate polar (IP), exhibited a dwarf nova
(DN) outburst in 2010. This outburst was extensively observed by the Neil
Gehrels Swift Observatory, beginning 1.95 days after the eruption and
continuing until 13.9 days before the maximum of the outburst in the optical.
In this paper, we present timing and spectral analyses, comparing the results
with those of other outbursts. We confirm the spin modulation in the 2 $-$ 10
keV X-ray range with a period of $P_{\rm WD} = 351.325(9)$ s. Additionally, we
detected spin modulation in the 0.3 $-$ 2 keV band during the second half of
the observations, a feature not seen in the 2015 and 2018 outbursts. This
finding suggests that the soft X-ray emission in GK Per may originate partly
near the magnetic poles and partly from a wind or circumstellar material.","['astro-ph.HE', 'astro-ph.SR']",2503.15196," Ultra-short Period exoplanets (USPs) like 55 Cnc e, hosting dayside magma
oceans, present unique opportunities to study surface-atmosphere interactions.
The composition of a vaporised mineral atmosphere enveloping the dayside is
dictated by that of the surface magma ocean, which in turn is sensitive to its
oxygen fugacity ($f$O$_2$). Observability estimations and characterisation of
the atmospheric emission of 55 Cnc e have mostly remained limited to low
spectral resolution space-based studies. Here, we aim to examine ground-based
high-resolution observabilities of a diverse set of mineral atmospheres
produced across a grid of mantle $f$O$_2$s varying over 12 orders of magnitude.
We assume a Bulk Silicate Earth mantle composition and a substellar dayside
temperature of T = 2500K in the near infrared wavelength (NIR) region. This
spectral range is often featureless for this class of atmospheres at
low-resolution. Coupling our newly developed simulator for synthesising
realistic observations from high-resolution ground-based spectrographs (Ratri)
to a pre-developed high-resolution cross-correlation spectroscopy (HRCCS)
analysis pipeline (Upamana), we find that this array of mineral atmospheres
would all be detectable with 11 hours of observing time of the dayside of 55
Cnc e with CARMENES and each individual scenario can be correctly
differentiated within 1$\sigma$. Our analysis is readily able to distinguish
between a planet with an Earth-like redox state (with $f$O$_2$ $\sim$3.5
log$_{10}$ units above the iron-w\""ustite, IW buffer) from a Mercury-like
planet ($f$O$_2$ $\sim$5 log$_{10}$ units below IW). We thus conclude that the
HRCCS technique holds promise for cataloguing the diversity of redox states
among the rocky exoplanetary population.",['astro-ph.EP'],False,,,,"Timing and spectral analysis of GK Persei during the 2010 dwarf nova
  outburst","Detectability of oxygen fugacity regimes in the magma ocean world 55
  Cancri e at high spectral resolution"
neg-d2-384,2025-01-27,,2501.17202," An ideal multimodal agent should be aware of the quality of its input
modalities. Recent advances have enabled large language models (LLMs) to
incorporate auditory systems for handling various speech-related tasks.
However, most audio LLMs remain unaware of the quality of the speech they
process. This limitation arises because speech quality evaluation is typically
excluded from multi-task training due to the lack of suitable datasets. To
address this, we introduce the first natural language-based speech evaluation
corpus, generated from authentic human ratings. In addition to the overall Mean
Opinion Score (MOS), this corpus offers detailed analysis across multiple
dimensions and identifies causes of quality degradation. It also enables
descriptive comparisons between two speech samples (A/B tests) with human-like
judgment. Leveraging this corpus, we propose an alignment approach with LLM
distillation (ALLD) to guide the audio LLM in extracting relevant information
from raw speech and generating meaningful responses. Experimental results
demonstrate that ALLD outperforms the previous state-of-the-art regression
model in MOS prediction, with a mean square error of 0.17 and an A/B test
accuracy of 98.6%. Additionally, the generated responses achieve BLEU scores of
25.8 and 30.2 on two tasks, surpassing the capabilities of task-specific
models. This work advances the comprehensive perception of speech signals by
audio LLMs, contributing to the development of real-world auditory and sensory
intelligent agents.","['cs.SD', 'cs.CL', 'eess.AS']",2501.17705," Integrating high-dimensional, heterogeneous data from multi-site cohort
studies with complex hierarchical structures poses significant feature
selection and prediction challenges. We extend the Bayesian Integrative
Analysis and Prediction (BIP) framework to enable simultaneous feature
selection and outcome modeling in data of nested hierarchical structure. We
apply the proposed Bayesian Integrative Mixed Modeling (BIPmixed) framework to
the Adolescent Brain Cognitive Development (ABCD) Study, leveraging multi-view
data, including structural and functional MRI and early life adversity (ELA)
metrics, to identify relevant features and predict the behavioral outcome.
BIPmixed incorporates 2-level nested random effects, to enhance
interpretability and make predictions in hierarchical data settings. Simulation
studies illustrate BIPmixed's robustness in distinct random effect settings,
highlighting its use for complex study designs. Our findings suggest that
BIPmixed effectively integrates multi-view data while accounting for nested
sampling, making it a valuable tool for analyzing large-scale studies with
hierarchical data.","['stat.ME', 'stat.AP']",False,,,,Audio Large Language Models Can Be Descriptive Speech Quality Evaluators,"A Bayesian Integrative Mixed Modeling Framework for Analysis of the
  Adolescent Brain and Cognitive Development Study"
neg-d2-385,2025-03-05,,2503.03665," Metal intercalation in epitaxial graphene enables the emergence of
proximity-induced superconductivity and modified quantum transport properties.
However, systematic transport studies of intercalated graphene have been
hindered by challenges in device fabrication, including processing-induced
deintercalation and instability under standard lithographic techniques. Here,
we introduce a lithographically controlled intercalation approach that enables
the scalable fabrication of gallium-intercalated quasi-freestanding bilayer
graphene (QFBLG) Hall bar devices. By integrating lithographic structuring with
subsequent intercalation through dedicated intercalation channels, this method
ensures precise control over metal incorporation while preserving device
integrity. Magneto-transport measurements reveal superconductivity with a
critical temperature Tc,onset ~ 3.5 K and the occurrence of a transverse
resistance, including both symmetric and antisymmetric field components, which
is attributed to the symmetric-in-field component to non-uniform currents.
These results establish an advanced fabrication method for intercalated
graphene devices, providing access to systematic investigations of confined 2D
superconductivity and emergent electronic phases in van der Waals
heterostructures.","['cond-mat.mtrl-sci', 'cond-mat.mes-hall']",2502.19475," We study the response of mono-energetic stellar populations with initially
isotropic kinematics to impulsive and adiabatic changes to an underlying dark
matter potential. Half-light radii expand and velocity dispersions decrease as
enclosed dark matter is removed. The details of this expansion and cooling
depend on the time scale on which the underlying potential changes. In the
adiabatic regime, the product of half-light radius and average velocity
dispersion is conserved. We show that the stellar populations maintain
centrally isotropic kinematics throughout their adiabatic evolution, and their
densities can be approximated by a family of analytical radial profiles.
Metallicity gradients within the galaxy flatten as dark matter is slowly
removed. In the case of strong impulsive perturbations, stellar populations
develop power-law-like density tails with radially biased kinematics. We show
that the distribution of stellar binding energies within the dark matter halo
substantially widens after an impulsive perturbation, no matter the sign of the
perturbation. This allows initially energetically separated stellar populations
to mix, to the extent that previously chemo-dynamically distinct populations
may masquerade as a single population with large metallicity and energy spread.
Finally, we show that in response to an impulsive perturbation, stellar
populations that are deeply embedded in cored dark matter halos undergo a
series of damped oscillations before reaching a virialised equilibrium state,
driven by inefficient phase mixing in the harmonic potentials of cored halos.
This slow return to equilibrium adds substantial systematic uncertainty to
dynamical masses estimated from Jeans modeling or the virial theorem.",['astro-ph.GA'],False,,,,"Lithographically-controlled liquid metal diffusion in graphene:
  Fabrication and magneto-transport signatures of superconductivity",Impulsive mixing of stellar populations in dwarf spheroidal galaxies
neg-d2-386,2025-01-27,,2501.16196," Going beyond short-range interactions, we explore the role of long-range
interactions in the extended XY model for transferring quantum states through
evolution. In particular, employing a spin-1/2 chain with interactions decaying
as a power law, we demonstrate that long-range interactions significantly
enhance the efficiency of a quantum state transfer (QST) protocol, reducing the
minimum time required to achieve fidelity beyond the classical limit. Our study
identifies the long-range regime as providing an optimal balance between
interaction range and transfer efficiency, outperforming the protocol with the
short-range interacting model. Our detailed analysis reveals the impact of
system parameters, such as anisotropy, magnetic field strength, and
coordination number, on QST dynamics. Specifically, we find that intermediate
coordination numbers lead to a faster and more reliable state transfer, while
extreme values diminish performance. Further, we exhibit that the presence of
long-range interactions also improves the achievable fidelity, mitigating its
decline associated with increasing system-size.","['quant-ph', 'cond-mat.quant-gas', 'cond-mat.str-el']",2503.17516," We obtain a complete description of semi-Fredholm spectra of operators of the
form $(Tf)(z) = w(z)f(B(z)$ acting on the disc algebra in the case when $B$ is
either elliptic or double parabolic finite Blaschke product of degree $d \geq
2$ and $w$ has no zeros on the unit circle. In the case when $B$ has zeros on
the unit circle we provide only some partial results. Our results hint on the
possibility of interesting connections between the spectral properties of
weighted composition operators and complex dynamics.",['math.SP'],False,,,,Expediting quantum state transfer through long-range extended XY model,"Spectrum of weighted composition operators. Part XI. The essential
  spectra of some weighted composition operators on the disc algebra"
neg-d2-387,2025-01-27,,2501.15917," This article presents a novel perspective to model and simulate
reconfigurable intelligent surface (RIS)-assisted communication systems.
Traditional methods in antenna design often rely on array method to simulate,
whereas communication system modeling tends to idealize antenna behavior.
Neither approach sufficiently captures the detailed characteristics of
RIS-assisted communication. To address this limitation, we propose a
comprehensive simulation framework that jointly models RIS antenna design and
the communication process. This framework simulates the entire communication
pipeline, encompassing signal generation, modulation, propagation, RIS-based
radiation, signal reception, alignment, demodulation, decision, and processing.
Using a QPSK-modulated signal for validation, we analyze system performance and
investigate the relationship between bit error rate (BER), aperture fill time,
array size, and baseband symbol frequency. The results indicate that larger
array sizes and higher baseband symbol frequencies exacerbate aperture fill
time effects, leading to increased BER. Furthermore, we examine BER variation
with respect to signal-to-noise ratio (SNR) and propose an optimal
matching-based alignment algorithm, which significantly reduces BER compared to
conventional pilot-based alignment methods. This work demonstrates the entire
process of RIS communication, and reveals the source of bit errors, which
provides valuable insights into the design and performance optimization of
RIS-assisted communication systems.",['physics.app-ph'],2503.01053," Each period, two players bargain over a unit of surplus. Each player chooses
between remaining flexible and committing to a take-it-or-leave-it offer at a
cost. If players' committed demands are incompatible, then the current-period
surplus is destroyed in the conflict. When both players are flexible, the
surplus is split according to the status quo, which is the division in the last
period where there was no conflict. We show that when players are patient and
the cost of commitment is small, there exist a class of symmetric Markov
Perfect equilibria that are asymptotically efficient and renegotiation proof,
in which players commit to fair demands in almost all periods.","['econ.TH', 'cs.GT']",False,,,,"RIS Assisted Wireless Communication: Advanced Modeling, Simulation, and
  Analytical Insights","Commitment, Conflict, and Status Quo in Bargaining"
neg-d2-388,2025-01-30,,2501.18743," In this paper, we introduced circuits for three- and four-particle quantum
systems to generate W states with any arbitrary coefficients and phases.
Subsequently, each qubit was transmitted separately through a four-qubit
entangled channel. Before transmission, the sender performed pre-processing on
their qubits to minimize the resources required for transmission. Additionally,
the receiver applied post-processing using the ancilla qubit(s) to recover the
final states. To further improve efficiency, it is preferable to implement the
protocol in a bidirectional manner, as this allows the unknown qubits initially
held by the users to be utilized ancilla qubit(s). Finally, we compared our
protocol with similar works and validated the correctness of the protocol by
simulating it using Qiskit, a tool provided by IBM.",['quant-ph'],2501.09839," We prove two theorems about tree-decompositions in the setting of coarse
graph theory. First, we show that a graph $G$ admits a tree-decomposition in
which each bag is contained in the union of a bounded number of balls of
bounded radius, if and only if $G$ admits a quasi-isometry to a graph with
bounded tree-width. (The ``if'' half is easy, but the ``only if'' half is
challenging.) This generalizes a recent result of Berger and Seymour,
concerning tree-decompositions when each bag has bounded radius.
  Second, we show that if $G$ admits a quasi-isometry $\phi$ to a graph $H$ of
bounded path-width, then $G$ admits a quasi-isometry (with error only an
additive constant) to a graph of bounded path-width. Indeed, we will show a
much stronger statement: that we can assign a non-negative integer length to
each edge of $H$, such that the same function $\phi$ is a quasi-isometry (with
error only an additive constant) to this weighted version of $H$.",['math.CO'],False,,,,Generation and Teleportation of three and four particle W state,Coarse tree-width
neg-d2-389,2025-03-24,,2503.18678," Suffering from performance bottlenecks in passively detecting high-quality
Deepfake images due to the advancement of generative models, proactive
perturbations offer a promising approach to disabling Deepfake manipulations by
inserting signals into benign images. However, existing proactive perturbation
approaches remain unsatisfactory in several aspects: 1) visual degradation due
to direct element-wise addition; 2) limited effectiveness against face swapping
manipulation; 3) unavoidable reliance on white- and grey-box settings to
involve generative models during training. In this study, we analyze the
essence of Deepfake face swapping and argue the necessity of protecting source
identities rather than target images, and we propose NullSwap, a novel
proactive defense approach that cloaks source image identities and nullifies
face swapping under a pure black-box scenario. We design an Identity Extraction
module to obtain facial identity features from the source image, while a
Perturbation Block is then devised to generate identity-guided perturbations
accordingly. Meanwhile, a Feature Block extracts shallow-level image features,
which are then fused with the perturbation in the Cloaking Block for image
reconstruction. Furthermore, to ensure adaptability across different identity
extractors in face swapping algorithms, we propose Dynamic Loss Weighting to
adaptively balance identity losses. Experiments demonstrate the outstanding
ability of our approach to fool various identity recognition models,
outperforming state-of-the-art proactive perturbations in preventing face
swapping models from generating images with correct source identities.",['cs.CV'],2503.15988," Spectra of vibrational overtone and combination bands from vibrational ground
state of HCNH+ were measured using an action spectroscopy technique with active
background suppression in a cryogenic 22 pole radio frequency ion trap
apparatus. Spectroscopic constants for the upper vibrational levels of the
transitions were determined with vibrational band origins being 6846.77981(90)
$\text{cm}^{-1}$ ($2\nu_1$ , NH stretch), 6640.47624(43) $\text{cm}^{-1}$
($\nu_1 + \nu_2$), 6282.03578(63) $\text{cm}^{-1}$ ($2\nu_2$, CH stretch), and
6588.4894(20) $\text{cm}^{-1}$ ($\nu_2 + \nu_3 + 2\nu_5^0$). State of the art
ab initio VCI calculations up to 10000 $\text{cm}^{-1}$ complement the
experimental data.","['astro-ph.GA', 'physics.atom-ph', 'physics.plasm-ph']",False,,,,NullSwap: Proactive Identity Cloaking Against Deepfake Face Swapping,Rovibrational Overtone and Combination Bands of the HCNH+ Ion
neg-d2-390,2025-03-21,,2503.17472," Our goal is to estimate the total gas mass in the direction of the Central
Molecular Zone (CMZ), quantify the various uncertainties associated, and
discuss the implications for the estimates of CR energy densities and dust
opacities. The $H_{\rm{I}}$ 21 cm line and the carbon monoxide isotopes
($^{12}\rm{CO}$, $^{13}\rm{CO}$ and $\rm{C}^{18}\rm{O}$) line emission maps are
used to derive the total gas column density. The gas in the CMZ is separated
from the disk contribution in position and velocity thanks to its different
properties in term of velocity dispersion and brightness ratio of CO isotopes.
The variations of the $X_{\rm{CO}}$ factors are modelled relying on both
theoretical trends from simulations and empirical corrections. We use the new
gas column density estimated together with gamma-ray and dust emission
measurements to derive the CR energy density and dust opacities, respectively.
The $X_{\rm{CO}}$ values in the CMZ range from $(0.32 - 1.37) \ \times$
$10^{20}$ cm$^{-2}$ K$^{-1}$ km$^{-1}$ s, with a distribution that is highly
asymmetric and skewed. The median value is $ \rm{\overline{X}_{CO}^{CMZ}} =
0.39 \ \times$ $10^{20}$ cm$^{-2}$ K$^{-1}$ km$^{-1}$ s. The total gas mass in
the CMZ is estimated to be $2.3_{-0.3}^{+0.3}\times10^{7} \; \rm{M_{\odot}}$
with $\sim 10 \%$ contribution from the atomic phase. Without removing the disk
contamination the total mass is about twice higher, and the atomic gas fraction
increases to $\sim30\%$. The cosmic-ray (CR) energy density in the CMZ,
assuming a 1/r profile, is higher by a factor of two compared to the previous
calculations at TeV energies. Using molecular gas tracers which probes only the
densest molecular cores leads to an overestimation of the CR energy density,
while ignoring the foreground/background contribution leads to an
underestimation of the CR energy density in the CMZ.","['astro-ph.GA', 'astro-ph.HE']",2502.01005," Cryogenic solid neon has recently emerged as a pristine solid host for single
electron qubits. At ~10 mK temperatures, electron-on-solid-neon (eNe) charge
qubits have exhibited exceptionally long coherence times and high operation
fidelities. To advance this platform towards a scalable quantum information
architecture, systematic characterization of its noise feature is imperative.
Here, we show the remarkable resilience of solid neon against charge and
thermal noises when eNe qubits are operated away from the charge-insensitive
sweet-spot and at elevated temperatures. Without optimizing neon growth, the
measured charge (voltage) noise on solid neon is already orders of magnitude
lower than that in most stringently grown semiconductors, rivaling the best
records to date. Up to 400 mK, the eNe charge qubits operated at ~5 GHz can
maintain their echo coherence times over 1 microsecond. These observations
highlight solid neon as an ideal host for quantum information processing at
higher temperatures and larger scales.","['quant-ph', 'cond-mat.mes-hall']",False,,,,"Cosmic rays, gas and dust in the Central Molecular Zone I -- $X_{CO}$
  factors, cosmic-ray densities and dust opacities",Noise-resilient solid host for electron qubits above 100 mK
neg-d2-391,2025-02-03,,2502.06806," Many commercial Large Language Models (LLMs) are often closed-source,
limiting developers to prompt tuning for aligning content generation with
specific applications. While these models currently do not provide access to
token logits, we argue that if such access were available, it would enable more
powerful adaptation techniques beyond prompt engineering. In this paper, we
propose a token-level probability reweighting framework that, given access to
logits and a small amount of task-specific data, can effectively steer
black-box LLMs toward application-specific content generation. Our approach
views next-token prediction through the lens of supervised classification. We
show that aligning black-box LLMs with task-specific data can be formulated as
a label noise correction problem, leading to \emph{Plugin} model -- an
autoregressive probability reweighting model that operates solely on logits. We
provide theoretical justification for why reweighting logits alone is
sufficient for task adaptation. Extensive experiments with multiple datasets,
LLMs, and reweighting models demonstrate the effectiveness of our method,
advocating for broader access to token logits in closed-source models.","['cs.LG', 'cs.AI', 'cs.CL']",2501.08747," We prove that, to every abstract group $G$, we can associate a sequence of
graphs $\Gamma_n$ such that the automorphism group of $\Gamma_n$ is isomorphic
to $G$ and the genus of $\Gamma_n$ is an unbounded function of $n$.","['math.GR', 'math.CO']",False,,,,Logits are All We Need to Adapt Closed Models,"Every group is the automorphism group of a graph with arbitrarily large
  genus"
neg-d2-392,2025-02-20,,2502.14798," X
  = S, Se): A Sustainable Alternative in Chalcogenide Perovskites; The quest for environmentally benign and stable optoelectronic materials has
intensified, and chalcogenide perovskites (CPs) have emerged as promising
candidates owing to their non-toxic composition, stability, small bandgaps,
large absorption coefficients. However, a detailed theoretical study of
excitonic and polaronic properties of these materials remains underexplored due
to high computational demands. Herein, we present a comprehensive theoretical
investigation of Germanium-based CPs, AGeX$_{3}$ (A = Ca, Sr, Ba; X = S, Se),
which adopt distorted perovskite structures (\beta-phase) with an orthorhombic
crystal structure (space group : Pnma) by utilizing state-of-the-art density
functional theory (DFT), density functional perturbation theory (DFPT), and
many-body perturbation theory (GW and Bethe-Salpeter equation). Our
calculations reveal that these materials are thermodynamically and mechanically
stable, with the bandgaps calculated using G$_{0}$W$_{0}$@PBE ranging from
0.646 to 2.001 eV - suitable for optoelectronic devices. We analyze the ionic
and electronic contributions to dielectric screening using DFPT and BSE
methods, finding that the electronic component dominates. The exciton binding
energies range from 0.03 to 73.63 meV, indicating efficient exciton
dissociation under ambient conditions. Additionally, these perovskites exhibit
low to high polaronic mobilities (1.67-167.65 cm$^{2}$V$^{-1}$s$^{-1}$),
exceeding many lead-free CPs and halide perovskites due to reduced
carrier-phonon interactions. The unique combination of wide tunable bandgaps,
low exciton binding energies, and enhanced charge-carrier mobility highlights
AGeX$_{3}$ as a potential material for next-generation optoelectronic
applications. These compounds are stable, high-performing, and eco-friendly,
showing great promise for experimental realization and device integration.",['cond-mat.mtrl-sci'],2501.06778," In this study, we analyze the observational images of a Konoplya-Zhidenko
rotating non-Kerr black hole, wherein a thin accretion disk, serving as the
sole background light source, is situated on the equatorial plane of the black
hole. The inner boundary of the thin accretion disk extends to the event
horizon, and the accretion material in the disk exhibits two different motion
behaviors, that is, it moves along the critical plunging orbit inside the
innermost stable circular orbit (ISCO) and follows the Keplerian orbit outside
the ISCO. The shadow image is captured on the imaging plane of a zero angular
momentum observer utilizing advanced fisheye camera ray-tracing techniques. The
results demonstrate that an image consistently reveals a dark region encircled
by a narrow photon ring, which is called the inner shadow. At low observation
inclination angles, the observation intensity is highly concentrated, with the
lensed image of accretion disk being superimposed on the direct image. As
observation inclination angle increases, the direct and lensed images gradually
separate, becoming distinctly distinguishable and forming a hat-like structure.
Furthermore, variations in the parameter space and observation angle will
influence pertinent image characteristics, including image symmetry, the range
or deformation degree of the inner shadow. We further examined the distinctive
characteristics of images observed in both prograde and retrograde accretion
disk scenarios. Subsequently, we also examined the redshift distribution on the
disk. The findings indicate that while variations in relevant parameters do
influence the redshift distribution, the primary factor is the change in
observational inclination. The observer can detect both redshift and blueshift
phenomena on the screen when viewed at a higher observation angle.","['astro-ph.HE', 'gr-qc']",False,,,,"Unlocking the Optoelectronic Potential of AGeX$_{3}$ (A = Ca, Sr, Ba","Optical appearance of the Konoplya-Zhidenko rotating non-Kerr black hole
  surrounded by a thin accretion disk"
neg-d2-393,2025-01-20,,2501.11446," This work considers a system coupling a viscous Burgers equation (aimed to
describe a simplified model of $1D$ fluid flow) with the ODE describing the
motion of a point mass moving inside the fluid. The point mass is possibly
under the action of a feedback control. Our main contributions are that we
prove two global exponential stability results. More precisely, we first show
that the velocity field corresponding to the free dynamics case is globally
exponentially stable. We next show that, in the presence of the feedback
control both the velocity field and the distance from the mass point to a
prescribed target position decay exponentially. The proofs of these results
heavily rely on the use of a special test function allowing both to prove that
the mass point stays away from the boundary and to construct a perturbed
Lyapunov function.","['math.AP', 'math.OC']",2503.02846," Large language models (LLMs) exhibit hallucinations (i.e., unfaithful or
nonsensical information) when serving as AI assistants in various domains.
Since hallucinations always come with truthful content in the LLM responses,
previous factuality alignment methods that conduct response-level preference
learning inevitably introduced noises during training. Therefore, this paper
proposes a fine-grained factuality alignment method based on Direct Preference
Optimization (DPO), called Mask-DPO. Incorporating sentence-level factuality as
mask signals, Mask-DPO only learns from factually correct sentences in the
preferred samples and prevents the penalty on factual contents in the not
preferred samples, which resolves the ambiguity in the preference learning.
Extensive experimental results demonstrate that Mask-DPO can significantly
improve the factuality of LLMs responses to questions from both in-domain and
out-of-domain datasets, although these questions and their corresponding topics
are unseen during training. Only trained on the ANAH train set, the score of
Llama3.1-8B-Instruct on the ANAH test set is improved from 49.19% to 77.53%,
even surpassing the score of Llama3.1-70B-Instruct (53.44%), while its
FactScore on the out-of-domain Biography dataset is also improved from 30.29%
to 39.39%. We further study the generalization property of Mask-DPO using
different training sample scaling strategies and find that scaling the number
of topics in the dataset is more effective than the number of questions. We
provide a hypothesis of what factual alignment is doing with LLMs, on the
implication of this phenomenon, and conduct proof-of-concept experiments to
verify it. We hope the method and the findings pave the way for future research
on scaling factuality alignment.",['cs.CL'],False,,,,"Global Exponential Stabilization for a Simplified Fluid-Particle
  Interaction System",Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs
neg-d2-394,2025-01-06,,2501.0279," Reinforcement learning from human feedback (RLHF) has been widely adopted to
align language models (LMs) with human preference. Prior RLHF works typically
take a bandit formulation, which, though intuitive, ignores the sequential
nature of LM generation and can suffer from the sparse reward issue. While
recent works propose dense token-level RLHF, treating each token as an action
may be oversubtle to proper reward assignment. In this paper, we seek to get
the best of both by training and utilizing a segment-level reward model, which
assigns a reward to each semantically complete text segment that spans over a
short sequence of tokens. For reward learning, our method allows dynamic text
segmentation and compatibility with standard sequence-preference datasets. For
effective RL-based LM training against segment reward, we generalize the
classical scalar bandit reward normalizers into location-aware normalizer
functions and interpolate the segment reward for further densification. With
these designs, our method performs competitively on three popular RLHF
benchmarks for LM policy: AlpacaEval 2.0, Arena-Hard, and MT-Bench. Ablation
studies are conducted to further demonstrate our method.","['cs.CL', 'cs.AI']",2502.07547," In machine learning practice, early stopping has been widely used to
regularize models and can save computational costs by halting the training
process when the model's performance on a validation set stops improving.
However, conventional early stopping applies the same stopping criterion to all
instances without considering their individual learning statuses, which leads
to redundant computations on instances that are already well-learned. To
further improve the efficiency, we propose an Instance-dependent Early Stopping
(IES) method that adapts the early stopping mechanism from the entire training
set to the instance level, based on the core principle that once the model has
mastered an instance, the training on it should stop. IES considers an instance
as mastered if the second-order differences of its loss value remain within a
small range around zero. This offers a more consistent measure of an instance's
learning status compared with directly using the loss value, and thus allows
for a unified threshold to determine when an instance can be excluded from
further backpropagation. We show that excluding mastered instances from
backpropagation can increase the gradient norms, thereby accelerating the
decrease of the training loss and speeding up the training process. Extensive
experiments on benchmarks demonstrate that IES method can reduce
backpropagation instances by 10%-50% while maintaining or even slightly
improving the test accuracy and transfer learning performance of a model.",['cs.LG'],False,,,,"Segmenting Text and Learning Their Rewards for Improved RLHF in Language
  Model",Instance-dependent Early Stopping
neg-d2-395,2025-03-02,,2503.01064," Large language models (LLMs) can answer questions and reason about complex
tasks, also from the scientific domain. We assess several multimodal LLMs
(MLLMs) on ScienceQA and find that Gemini models show the highest accuracy with
little context, and the highest textual similarity to human explanations with
richer context. Adapter-tuning of smaller MLLMs did not lead to any reliable
performance. Training from Gemini outputs consistently underperformed training
from the original data.","['cs.CL', 'cs.AI', 'cs.CV']",2501.12637," Neural Radiance Fields (NeRF) has achieved superior performance in novel view
synthesis and 3D scene representation, but its practical applications are
hindered by slow convergence and reliance on dense training views. To this end,
we present DWTNeRF, a unified framework based on Instant-NGP's fast-training
hash encoding. It is coupled with regularization terms designed for few-shot
NeRF, which operates on sparse training views. Our DWTNeRF additionally
includes a novel Discrete Wavelet loss that allows explicit prioritization of
low frequencies directly in the training objective, reducing few-shot NeRF's
overfitting on high frequencies in earlier training stages. We also introduce a
model-based approach, based on multi-head attention, that is compatible with
INGP, which are sensitive to architectural changes. On the 3-shot LLFF
benchmark, DWTNeRF outperforms Vanilla INGP by 15.07% in PSNR, 24.45% in SSIM
and 36.30% in LPIPS. Our approach encourages a re-thinking of current few-shot
approaches for fast-converging implicit representations like INGP or 3DGS.",['cs.CV'],False,,,,Scientific Reasoning: Assessment of Multimodal Generative LLMs,"DWTNeRF: Boosting Few-shot Neural Radiance Fields via Discrete Wavelet
  Transform"
neg-d2-396,2025-02-05,,2502.03152," In this work, we have carried out lattice simulations of $(2+1)$-flavor QCD
using highly improved staggered quarks at the physical pion mass on $32^3
\times 8$ and $48^3 \times 12$ lattices, with magnetic field strengths ranging
up to 0.8 GeV$^2$ and nonzero baryon chemical potentials employing the Taylor
expansion framework. We present lattice QCD continuum estimate results, along
with the magnetized hadron resonance and ideal gas comparisons, for the
leading-order Taylor expansion coefficients for bulk thermodynamic quantities
such as pressure, number density, energy density, and entropy density, focusing
on the significant impact of strong magnetic fields.","['hep-lat', 'hep-ph', 'hep-th']",2502.13911," In recent years, ultra-high dose rate (FLASH) radiotherapy has become a novel
cancer treatment technique because of its similar tumor-killing efficacy as
conventional particle therapy while significantly protecting normal tissues.
However, due to the limitation of particle number, achieving FLASH condition in
a compact heavy-ion synchrotron requires a short extraction time of tens of
milliseconds, which is challenging for the conventional RF-KO method. To tackle
this challenge, we introduce autoresonance into the third-order resonant
extraction for the first time, offering an alternative to the conventional
approach of merely increasing the excitation strength. By leveraging a strong
detuning effect, a frequency sweeping excitation with small amplitude can drive
the entire beam into the autoresonant state, thus enabling rapid beam
extraction within a single sweeping period. Compared with the conventional
method, this innovative method requires only the addition of an octupole
magnet. At the same time, it shows that the conventional RF-KO method has a
high autoresonance threshold, so that only a small number of particles that
meet the threshold can be excited to large amplitude and be extracted in each
sweeping period. In this paper, the autoresonance threshold of a particle in
the presence of sextupole and octupole magnetic fields is analyzed, and the
single particle simulation shows good agreement with the theoretical formula.
Furthermore, the autoresonance based rapid extraction process is simulated and
studied, revealing the possibility of millisecond scale beam extraction.","['physics.acc-ph', 'physics.med-ph']",False,,,,"QCD Equation of State with Strong Magnetic Fields and Nonzero Baryon
  Density",Application of autoresonance in rapid beam extraction of synchrotrons
neg-d2-397,2025-02-26,,2502.19477," Effective field theories (EFTs) parametrize our ignorance of the underlying
UV theory through their Wilson coefficients. However, not all values of these
coefficients are consistent with fundamental physical principles. In this
paper, we explore the consequences of imposing causal propagation on the
comoving curvature perturbation in the EFT of inflation, particularly its
impact on the primordial power spectrum and the effective sound speed
$c_s^\text{eff}$. We investigate scenarios where $c_s^\text{eff}$ undergoes a
transition, remaining consistent with CMB constraints at early times but later
experiencing a drastic change, becoming highly subluminal. Such scenarios allow
the primordial power spectrum to grow at small scales, potentially leading to
the formation of primordial black holes or the generation of scalar-induced
gravitational waves. We find the generic feature that in a causal theory,
luminal sound speeds imply a free theory, effectively constraining the
dynamics. Additionally, we obtain that when considering natural values for the
Wilson coefficients, maintaining the validity of the EFT and the weakly coupled
regime, and enforcing causal propagation of the EFT modes, the power spectrum
cannot increase drastically. This imposes significant constraints on the
parameter space of models aiming to produce such features.","['hep-th', 'astro-ph.CO']",2502.13384," The derivative of a polynomial with all zeros on the unit circle has the
zeros of its derivative on or inside the unit circle. It has been observed that
in many cases the zeros of the derivative have a bimodal distribution: there
are two smaller circles near which it is more likely to find those zeros. We
identify the likely source of the second mode. This idea is supported with
numerical examples involving the characteristic polynomials of random unitary
matrices.","['math.CV', 'math.NT']",False,,,,Causality Bounds on the Primordial Power Spectrum,The bimodal distribution in the derivative of unitary polynomials
neg-d2-398,2025-03-21,,2503.17117," The search for exoplanets is an active field in astronomy, with direct
imaging as one of the most challenging methods due to faint exoplanet signals
buried within stronger residual starlight. Successful detection requires
advanced image processing to separate the exoplanet signal from this nuisance
component. This paper presents a novel statistical model that captures nuisance
fluctuations using a multi-scale approach, leveraging problem symmetries and a
joint spectral channel representation grounded in physical principles. Our
model integrates into an interpretable, end-to-end learnable framework for
simultaneous exoplanet detection and flux estimation. The proposed algorithm is
evaluated against the state of the art using datasets from the SPHERE
instrument operating at the Very Large Telescope (VLT). It significantly
improves the precision-recall trade-off, notably on challenging datasets that
are otherwise unusable by astronomers. The proposed approach is computationally
efficient, robust to varying data quality, and well suited for large-scale
observational surveys.","['astro-ph.IM', 'astro-ph.EP', 'cs.CV', 'cs.LG', 'stat.AP']",2502.16534," Large Language Models (LLMs) are becoming increasingly capable across global
languages. However, the ability to communicate across languages does not
necessarily translate to appropriate cultural representations. A key concern is
US-centric bias, where LLMs reflect US rather than local cultural values. We
propose a novel methodology that compares LLM-generated response distributions
against population-level opinion data from the World Value Survey across four
languages (Danish, Dutch, English, and Portuguese). Using a rigorous linear
mixed-effects regression framework, we compare two families of models: Google's
Gemma models (2B--27B parameters) and successive iterations of OpenAI's
turbo-series. Across the families of models, we find no consistent
relationships between language capabilities and cultural alignment. While the
Gemma models have a positive correlation between language capability and
cultural alignment across languages, the OpenAI models do not. Importantly, we
find that self-consistency is a stronger predictor of multicultural alignment
than multilingual capabilities. Our results demonstrate that achieving
meaningful cultural alignment requires dedicated effort beyond improving
general language capabilities.","['cs.CL', 'cs.AI', 'cs.CY']",False,,,,"A New Statistical Model of Star Speckles for Learning to Detect and
  Characterize Exoplanets in Direct Imaging Observations","Multilingual != Multicultural: Evaluating Gaps Between Multilingual
  Capabilities and Cultural Alignment in LLMs"
neg-d2-399,2025-03-14,,2503.11065," Deep reinforcement learning (DRL) has had success in virtual and simulated
domains, but due to key differences between simulated and real-world
environments, DRL-trained policies have had limited success in real-world
applications. To assist researchers to bridge the \textit{sim-to-real gap}, in
this paper, we describe a low-cost physical inverted pendulum apparatus and
software environment for exploring sim-to-real DRL methods. In particular, the
design of our apparatus enables detailed examination of the delays that arise
in physical systems when sensing, communicating, learning, inferring and
actuating. Moreover, we wish to improve access to educational systems, so our
apparatus uses readily available materials and parts to reduce cost and
logistical barriers. Our design shows how commercial, off-the-shelf electronics
and electromechanical and sensor systems, combined with common metal
extrusions, dowel and 3D printed couplings provide a pathway for affordable
physical DRL apparatus. The physical apparatus is complemented with a simulated
environment implemented using a high-fidelity physics engine and OpenAI Gym
interface.","['cs.LG', 'cs.AI', 'cs.RO', 'cs.SY', 'eess.SY']",2503.12674," The entanglement spectra for a subsystem in a spin chain fine-tuned to a
quantum-critical point contains signatures of the underlying quantum field
theory that governs its low-energy properties. For an open chain with given
boundary conditions described by a 2D conformal field theory~(CFT), the
entanglement spectrum of the left/right half of the system coincides with a
boundary CFT spectrum, where one of the boundary conditions arise due to the
`entanglement cut'. The latter has been argued to be conformal and has been
numerically found to be the `free' boundary condition for Ising, Potts and free
boson theories. For these models, the `free' boundary condition for the lattice
degree of freedom has a counterpart in the continuum theory. However, this is
not true in general. Here, this question is analyzed for the unitary minimal
models of 2D CFTs using the density matrix renormalization group technique. The
entanglement spectra are computed for blocks of spins in open chains of A-type
restricted solid-on-solid models with identical boundary conditions at the
ends. The imposed boundary conditions are realized exactly for these lattice
models due to their integrable nature. The obtained entanglement spectra are in
good agreement with certain boundary CFT spectra. The boundary condition for
the entanglement cut is found to be conformal and to coincide with the one with
the highest boundary entropy. This identification enables determination of the
exponents governing the unusual corrections to the entanglement entropy from
the CFT partition functions. These are compared with numerical results.","['quant-ph', 'hep-th', 'math-ph', 'math.MP']",False,,,,"Low-cost Real-world Implementation of the Swing-up Pendulum for Deep
  Reinforcement Learning Experiments","Boundary Conditions for the Entanglement Cut in 2D Conformal Field
  Theories"
neg-d2-400,2025-03-06,,2503.04907," Galactic weak-scale Dark Matter (DM) particles annihilating into lepton-rich
channels not only produce gamma-rays via prompt radiation but also generate
abundant energetic electrons and positrons, which subsequently emit through
bremsstrahlung or inverse Compton scattering (collectively called
`secondary-radiation photons'). While the prompt gamma-rays concentrate at
high-energy, the secondary emission falls in the MeV range, which a number of
upcoming experiments (AMEGO, E-ASTROGAM, MAST...) will probe. We investigate
the sensitivity of these future telescopes for weak-scale DM, focusing for
definiteness on observations of the galactic center. We find that they have the
potential of probing a wide region of the DM parameter space which is currently
unconstrained. Namely, in rather optimistic configurations, future MeV
telescopes could probe thermally-produced DM with a mass up to the TeV range,
or GeV DM with an annihilation cross section 2 to 3 orders of magnitude smaller
than the current bounds, precisely thanks to the significant leverage provided
by their sensitivity to secondary emissions. We comment on astrophysical and
methodological uncertainties, and compare with the reach of high-energy gamma
ray experiments.","['hep-ph', 'astro-ph.CO', 'astro-ph.GA']",2503.12453," Previous works studied how deep neural networks (DNNs) perceive image content
in terms of their biases towards different image cues, such as texture and
shape. Previous methods to measure shape and texture biases are typically
style-transfer-based and limited to DNNs for image classification. In this
work, we provide a new evaluation procedure consisting of 1) a
cue-decomposition method that comprises two AI-free data pre-processing methods
extracting shape and texture cues, respectively, and 2) a novel
cue-decomposition shape bias evaluation metric that leverages the
cue-decomposition data. For application purposes we introduce a corresponding
cue-decomposition robustness metric that allows for the estimation of the
robustness of a DNN w.r.t. image corruptions. In our numerical experiments, our
findings for biases in image classification DNNs align with those of previous
evaluation metrics. However, our cue-decomposition robustness metric shows
superior results in terms of estimating the robustness of DNNs. Furthermore,
our results for DNNs on the semantic segmentation datasets Cityscapes and
ADE20k for the first time shed light into the biases of semantic segmentation
DNNs.",['cs.CV'],False,,,,Prospects of future MeV telescopes in probing weak-scale Dark Matter,"Shape Bias and Robustness Evaluation via Cue Decomposition for Image
  Classification and Segmentation"
neg-d2-401,2025-01-15,,2501.09083," Observations of the circumgalactic medium (CGM) often display coincident
absorption from species with widely varying ionization states, providing direct
evidence for complex, multiphase interactions. Motivated by these measurements,
we perform a series of cloud-crushing simulations that model cold clouds
traveling through the hot CGM. We analyze the ion distributions of these
clouds, generate mock absorption spectra, and study their implications on
quasar (QSO) absorption observations. Our results show interesting multiphase
features, in which ions with significantly different ionization potentials
exist in the same absorber and share similar spectral features. However, our
simulations are unable to explain high ions like O \textsc{vi} and their
coexistence with lower ions that appear in many observed QSO absorption
systems.",['astro-ph.GA'],2502.1133," System messages play a crucial role in interactions with large language
models (LLMs), often serving as prompts to initiate conversations. Through
system messages, users can assign specific roles, perform intended tasks,
incorporate background information, specify various output formats and
communication styles. Despite such versatility, publicly available data are
often lack system messages and subject to strict license constraints in the
industry field. Manual labeling of publicly available data with system messages
that align with user instructions demands significant resources. In view of
such challenges, our work introduces SysGen, a pipeline for generating system
messages with better aligned assistant responses from the supervised
fine-tuning dataset without system messages. Training on SysGen data has
demonstrated substantial improvements in the alignment of model responses with
system messages and user instructions, as demonstrated across various
open-source models on the Multifacet benchmark, while maintaining minimal
impact on other unseen benchmarks such as Open LLM Leaderboard 2. Our
qualitative analysis highlights the importance of diverse system messages to
ensure better adaptability across different contexts.","['cs.CL', 'cs.AI']",False,,,,Ion densities of cold clouds driven by galactic outflows,System Message Generation for User Preferences using Open-Source Models
neg-d2-402,2025-01-30,,2501.18884," We show that iteration of a few ( $\sim N^{1/4}$) unitary steps of Grover's
algorithm suffices to perfectly prepare a Dicke state of $N$ atoms in a cavity.
We also show that a few subsequent Grover steps can be employed to generate GHZ
and Cat states. The Grover iteration is physically realized by global qubit
rotations and by the phase shift of single photons reflected on the cavity. Our
protocols are deterministic and require no individual addressing of the atoms.
A detailed error analysis accounting for spatial mode matching of the photon to
the cavity, spontaneous emission, mirror scattering, and the finite bandwidth
of the photon mode is used to predict the fidelity of the prepared states as a
function of system parameters and atom-cavity cooperativity. The fidelity can
be increased by heralding on detection of the reflected photon.","['quant-ph', 'physics.atom-ph']",2503.14296," We establish a complete Widder Theory for the fractional fast diffusion
equation. Our work focuses on nonnegative solutions satisfying a certain
integral size condition at infinity. We prove that these solutions possess a
Radon measure as initial trace, and prove the existence and uniqueness of
solutions originating from such initial data. The uniqueness result is the main
issue. Most of its difficulty comes from the singular character of the
nonlinearity.",['math.AP'],False,,,,Deterministic carving of quantum states with Grover's algorithm,Fractional fast diffusion with initial data a Radon measure
neg-d2-403,2025-03-04,,2503.16474," This paper presents Matrix, an advanced AI-powered framework designed for
real-time 3D object generation in Augmented Reality (AR) environments. By
integrating a cutting-edge text-to-3D generative AI model, multilingual
speech-to-text translation, and large language models (LLMs), the system
enables seamless user interactions through spoken commands. The framework
processes speech inputs, generates 3D objects, and provides object
recommendations based on contextual understanding, enhancing AR experiences. A
key feature of this framework is its ability to optimize 3D models by reducing
mesh complexity, resulting in significantly smaller file sizes and faster
processing on resource-constrained AR devices. Our approach addresses the
challenges of high GPU usage, large model output sizes, and real-time system
responsiveness, ensuring a smoother user experience. Moreover, the system is
equipped with a pre-generated object repository, further reducing GPU load and
improving efficiency. We demonstrate the practical applications of this
framework in various fields such as education, design, and accessibility, and
discuss future enhancements including image-to-3D conversion, environmental
object detection, and multimodal support. The open-source nature of the
framework promotes ongoing innovation and its utility across diverse
industries.","['cs.HC', 'cs.AI']",2503.05812," Frontier AI models -- highly capable foundation models at the cutting edge of
AI development -- may pose severe risks to public safety, human rights,
economic stability, and societal value in the coming years. These risks could
arise from deliberate adversarial misuse, system failures, unintended cascading
effects, or simultaneous failures across multiple models.
  In response to such risks, at the AI Seoul Summit in May 2024, 16 global AI
industry organizations signed the Frontier AI Safety Commitments, and 27
nations and the EU issued a declaration on their intent to define these
thresholds. To fulfill these commitments, organizations must determine and
disclose ``thresholds at which severe risks posed by a model or system, unless
adequately mitigated, would be deemed intolerable.''
  To assist in setting and operationalizing intolerable risk thresholds, we
outline key principles and considerations; for example, to aim for ``good, not
perfect'' thresholds in the face of limited data on rapidly advancing AI
capabilities and consequently evolving risks. We also propose specific
threshold recommendations, including some detailed case studies, for a subset
of risks across eight risk categories: (1) Chemical, Biological, Radiological,
and Nuclear (CBRN) Weapons, (2) Cyber Attacks, (3) Model Autonomy, (4)
Persuasion and Manipulation, (5) Deception, (6) Toxicity, (7) Discrimination,
and (8) Socioeconomic Disruption. Our goal is to serve as a starting point or
supplementary resource for policymakers and industry leaders, encouraging
proactive risk management that prioritizes preventing intolerable risks (ex
ante) rather than merely mitigating them after they occur (ex post).","['cs.CY', 'cs.AI', 'cs.CR', 'cs.HC', 'cs.LG']",False,,,,"From Voices to Worlds: Developing an AI-Powered Framework for 3D Object
  Generation in Augmented Reality",Intolerable Risk Threshold Recommendations for Artificial Intelligence
neg-d2-404,2025-03-01,,2503.00398," The physical origins of the diverse emission-line asymmetries observed in the
spectra of active galactic nuclei (AGNs) remain incompletely understood.
Monitoring the temporal variations of line profiles offers a promising approach
to investigating the underlying physics. In this study, we present an analysis
of the broad H$\beta$ emission line profiles of eight AGNs observed from the
end of 2016 to May 2023 as part of the reverberation mapping campaign titled
""Monitoring AGNs with H$\beta$ Asymmetry"" (MAHA), utilizing data obtained from
the Wyoming Infrared Observatory (WIRO) 2.3-meter telescope. We measure the
temporal variations of line asymmetry, width, and central velocity shift for
the eight objects. Our findings reveal that the variation in asymmetry is
positively correlated with H$\beta$ flux in five of the eight objects, while
the remaining objects exhibit negative or complex correlations. Furthermore, we
observe anti-correlations between line width and H$\beta$ flux for most
objects, indicating the presence of the ""breathing"" phenomenon in their
H$\beta$ emission lines. In contrast, two objects demonstrate an
""anti-breathing"" phenomenon or complex behavior. We discuss the physical
origins of the temporal variations in line profiles and propose the possibility
of decomposing the variations in H$\beta$ asymmetry and width into components:
one that corresponds to short-term variations in H$\beta$ flux and another that
reflects long-term variations in continuum light curves, perhaps driven by
radiation pressure.",['astro-ph.GA'],2503.06953," Autonomous and targeted underwater visual monitoring and exploration using
Autonomous Underwater Vehicles (AUVs) can be a challenging task due to both
online and offline constraints. The online constraints comprise limited onboard
storage capacity and communication bandwidth to the surface, whereas the
offline constraints entail the time and effort required for the selection of
desired key frames from the video data. An example use case of targeted
underwater visual monitoring is finding the most interesting visual frames of
fish in a long sequence of an AUV's visual experience. This challenge of
targeted informative sampling is further aggravated in murky waters with poor
visibility. In this paper, we present MERLION, a novel framework that provides
semantically aligned and visually enhanced summaries for murky underwater
marine environment monitoring and exploration. Specifically, our framework
integrates (a) an image-text model for semantically aligning the visual samples
to the users' needs, (b) an image enhancement model for murky water visual data
and (c) an informative sampler for summarizing the monitoring experience. We
validate our proposed MERLION framework on real-world data with user studies
and present qualitative and quantitative results using our evaluation metric
and show improved results compared to the state-of-the-art approaches. We have
open-sourced the code for MERLION at the following link
https://github.com/MARVL-Lab/MERLION.git.",['cs.RO'],False,,,,"Monitoring AGNs with H$\beta$ Asymmetry. V. Long-term Variation and
  Evolution of the Broad H$\beta$ Emission-Line Profiles","MERLION: Marine ExploRation with Language guIded Online iNformative
  Visual Sampling and Enhancement"
neg-d2-405,2025-01-13,,2501.07334," The steadily increasing utilization of data-driven methods and approaches in
areas that handle sensitive personal information such as in law enforcement
mandates an ever increasing effort in these institutions to comply with data
protection guidelines. In this work, we present a system for automatically
anonymizing images of scanned documents, reducing manual effort while ensuring
data protection compliance. Our method considers the viability of further
forensic processing after anonymization by minimizing automatically redacted
areas by combining automatic detection of sensitive regions with knowledge from
a manually anonymized reference document. Using a self-supervised image model
for instance retrieval of the reference document, our approach requires only
one anonymized example to efficiently redact all documents of the same type,
significantly reducing processing time. We show that our approach outperforms
both a purely automatic redaction system and also a naive copy-paste scheme of
the reference anonymization to other documents on a hand-crafted dataset of
ground truth redactions.","['cs.AI', 'cs.CV']",2502.18852," Full Waveform Inversion (FWI) reconstructs high-resolution subsurface models
via multi-variate optimization but faces challenges with solver selection and
data availability. Deep Learning (DL) offers a promising alternative, bridging
data-driven and physics-based methods. While FWI in DL has been explored in the
time domain, the pseudo-spectral approach remains underutilized, despite its
success in classical FWI.
  This thesis integrates pseudo-spectral FWI into DL, formulating both
data-driven and theory-guided approaches using Deep Neural Networks (DNNs) and
Recurrent Neural Networks (RNNs). These methods were theoretically derived,
tested on synthetic and Marmousi datasets, and compared with deterministic and
time-domain approaches.
  Results show that data-driven pseudo-spectral DNNs outperform classical FWI
in deeper and over-thrust regions due to their global approximation capability.
Theory-guided RNNs yield greater accuracy, with lower error and better fault
identification. While DNNs excel in velocity contrast recovery, RNNs provide
superior edge definition and stability in shallow and deep sections.
  Beyond enhancing FWI performance, this research identifies broader
applications of DL-based inversion and outlines future directions for these
frameworks.","['physics.geo-ph', 'cs.LG']",False,,,,Anonymization of Documents for Law Enforcement with Machine Learning,"Data-Driven and Theory-Guided Pseudo-Spectral Seismic Imaging Using Deep
  Neural Network Architectures"
neg-d2-406,2025-03-10,,2503.06914," A low-energy neutral quasiparticle in a fractional quantum Hall system
appears in the latter's energy spectrum on a sphere as a series of many-body
excited states labeled by the angular momentum $L$ and whose energy is a smooth
function of $L$ in the limit of large sphere radius. We argue that the
signature of a nonvanishing spin (intrinsic angular momentum) $s$ of the
quasiparticle is the absence, in this series, of states with total angular
momentum less than $s$.We reinterpret the missing of certain states, observed
in an exact-diagonalization calculation of the spectrum of the $\nu=7/3$ FQH
state in a wide quantum well as well as in many proposed wave functions for the
excited states as a consequence of the spin-2 nature of the zero-momentum
magnetoroton.","['cond-mat.str-el', 'cond-mat.mes-hall', 'hep-th']",2501.04483," The Ethereum blockchain has a \emph{gas system} that associates operations
with a cost in gas units. Two central concepts of this system are the \emph{gas
limit} assigned by the issuer of a transaction and the \emph{gas used} by a
transaction. The former is a budget that must not be exhausted before the
completion of the transaction execution; otherwise, the execution fails.
Therefore, it seems rather essential to determine the \emph{minimum gas limit}
that ensures the execution of a transaction will not abort due to the lack of
gas. Despite its practical relevance, this concept has not been properly
addressed. In the literature, gas used and minimum gas limit are conflated.
This paper proposes a precise notion of minimum gas limit and how it can differ
from gas used by a transaction; this is also demonstrated with a quantitative
study on real transactions of the Ethereum blockchain. Another significant
contribution is the proposition of a fairly precise estimator for each of the
two metrics. Again, the confusion between these concepts has led to the
creation of estimators only for the gas used by a transaction. We demonstrate
that the minimum gas limit for the state of the Ethereum blockchain (after the
block) $t$ can serve as a near-perfect estimation for the execution of the
transaction at block $t + \Delta$, where $\Delta \leq 11$; the same holds for
estimating gas used. These precise estimators can be very valuable in helping
the users predict the gas budget of transactions and developers in optimising
their smart contracts; over and underestimating gas used and minimum gas limit
can lead to a number of practical issues. Overall, this paper serves as an
important reference for blockchain developers and users as to how the gas
system really works.","['cs.SE', 'cs.CE', 'cs.DC', 'cs.ET', 'cs.NI']",False,,,,"Spin of fractional quantum Hall neutral modes and ""missing states"" on a
  sphere","Demystification and Near-perfect Estimation of Minimum Gas Limit and Gas
  Used for Ethereum Smart Contracts"
neg-d2-407,2025-01-29,,2501.17967," We present a new framework for the fast solution of inhomogeneous elliptic
boundary value problems in domains with smooth boundaries. High-order solvers
based on adaptive box codes or the fast Fourier transform can efficiently treat
the volumetric inhomogeneity, but require care to be taken near the boundary to
ensure that the volume data is globally smooth. We avoid function extension or
cut-cell quadratures near the boundary by dividing the domain into two regions:
a bulk region away from the boundary that is efficiently treated with a
truncated free-space box code, and a variable-width boundary-conforming strip
region that is treated with a spectral collocation method and accompanying fast
direct solver. Particular solutions in each region are then combined with
Laplace layer potentials to yield the global solution. The resulting solver has
an optimal computational complexity of $O(N)$ for an adaptive discretization
with $N$ degrees of freedom. With an efficient two-dimensional (2D)
implementation we demonstrate adaptive resolution of volumetric data, boundary
data, and geometric features across a wide range of length scales, to typically
10-digit accuracy. The cost of all boundary corrections remains small relative
to that of the bulk box code. The extension to 3D is expected to be
straightforward in many cases because the strip ``thickens'' an existing
boundary quadrature.","['math.NA', 'cs.NA']",2502.21199," We analyze a subclass of Ising models in the context of credit risk, focusing
on Dandelion models when the correlations $\rho$ between the central node and
each non-central node are negative. We establish the possible range of values
for $\rho$ and derive an explicit formula linking the correlation between any
pair of non-central nodes to $\rho$. The paper concludes with a simulation
study.",['stat.AP'],False,,,,"A fully adaptive, high-order, fast Poisson solver for complex
  two-dimensional geometries",Negative correlations in Ising models of credit risk
neg-d2-408,2025-03-19,,2503.1502," This study presents a shaped reset feedback control strategy to enhance the
performance of precision motion systems. The approach utilizes a phase-lead
compensator as a shaping filter to tune the phase of reset instants, thereby
shaping the nonlinearity in the first-order reset control. {The design achieves
either an increased phase margin while maintaining gain properties or improved
gain without sacrificing phase margin, compared to reset control without the
shaping filter.} Then, frequency-domain design procedures are provided for both
Clegg Integrator (CI)-based and First-Order Reset Element (FORE)-based reset
control systems. Finally, the effectiveness of the proposed strategy is
demonstrated through two experimental case studies on a precision motion stage.
In the first case, the shaped reset control leverages phase-lead benefits to
achieve zero overshoot in the transient response. In the second case, the
shaped reset control strategy enhances the gain advantages of the previous
reset element, resulting in improved steady-state performance, including better
tracking precision and disturbance rejection, while reducing overshoot for an
improved transient response.","['eess.SY', 'cs.SY']",2503.16494," In this paper, we examine the wide-ranging impact of artificial intelligence
on society, focusing on its potential to both help and harm global equity,
cognitive abilities, and economic stability. We argue that while artificial
intelligence offers significant opportunities for progress in areas like
healthcare, education, and scientific research, its rapid growth -- mainly
driven by private companies -- may worsen global inequalities, increase
dependence on automated systems for cognitive tasks, and disrupt established
economic paradigms. We emphasize the critical need for strong governance and
ethical guidelines to tackle these issues, urging the academic community to
actively participate in creating policies that ensure the benefits of
artificial intelligence are shared fairly and its risks are managed
effectively.","['physics.soc-ph', 'cs.CY']",False,,,,"Enhancing Reset Control Phase with Lead Shaping Filters: Applications to
  Precision Motion Systems","The impact of artificial intelligence: from cognitive costs to global
  inequality"
neg-d2-409,2025-01-28,,2501.16839," Among generative neural models, flow matching techniques stand out for their
simple applicability and good scaling properties. Here, velocity fields of
curves connecting a simple latent and a target distribution are learned. Then
the corresponding ordinary differential equation can be used to sample from a
target distribution, starting in samples from the latent one. This paper
reviews from a mathematical point of view different techniques to learn the
velocity fields of absolutely continuous curves in the Wasserstein geometry. We
show how the velocity fields can be characterized and learned via i) transport
plans (couplings) between latent and target distributions, ii) Markov kernels
and iii) stochastic processes, where the latter two include the coupling
approach, but are in general broader. Besides this main goal, we show how flow
matching can be used for solving Bayesian inverse problems, where the
definition of conditional Wasserstein distances plays a central role. Finally,
we briefly address continuous normalizing flows and score matching techniques,
which approach the learning of velocity fields of curves from other directions.","['cs.LG', 'math.PR']",2503.0067," Anomaly detection in videos is a challenging task as anomalies in different
videos are of different kinds. Therefore, a promising way to approach video
anomaly detection is by learning the non-anomalous nature of the video at hand.
To this end, we propose a one-class few-shot learning driven transformer based
approach for anomaly detection in videos that is self-context aware. Features
from the first few consecutive non-anomalous frames in a video are used to
train the transformer in predicting the non-anomalous feature of the subsequent
frame. This takes place under the attention of a self-context learned from the
input features themselves. After the learning, given a few previous frames, the
video-specific transformer is used to infer if a frame is anomalous or not by
comparing the feature predicted by it with the actual. The effectiveness of the
proposed method with respect to the state-of-the-art is demonstrated through
qualitative and quantitative results on different standard datasets. We also
study the positive effect of the self-context used in our approach.",['cs.CV'],False,,,,"Flow Matching: Markov Kernels, Stochastic Processes and Transport Plans","Transformer Based Self-Context Aware Prediction for Few-Shot Anomaly
  Detection in Videos"
neg-d2-410,2025-03-06,,2503.0441," This article provides an introduction to nuclear magnetic resonance
spectroscopy in pulsed magnetic fields (PFNMR), focusing on its capabilities,
applications, and future developments in research involving high magnetic
fields. It highlights the significance of PFNMR in enhancing the understanding
of solid-state materials, with particular emphasis on those exhibiting complex
interactions and strong electronic correlations. Several technical aspects are
discussed, including the challenges associated with high-frequency NMR
experiments. The power of PFNMR is showcased through several examples,
including studies on the topical materials LiCuVO$_4$, SrCu$_2$(BO$_3$)$_2$,
and CeIn$_3$, offering insights into their magnetic and electronic properties
at high magnetic fields. The article also discusses possible future directions
for the technique, including improvements in PFNMR instrumentation and the
exploration of materials under extreme conditions. This exposition underscores
the role of PFNMR in advancing the frontiers of materials-science research.",['cond-mat.str-el'],2501.15216," Color centers in hexagonal boron nitride (hBN) are promising candidates as
quantum light sources for future technologies. In this work, we utilize a
scattering-type near-field optical microscope (s-SNOM) to study the
photoluminescence (PL) emission characteristics of such quantum emitters in
metalorganic vapor phase epitaxy grown hBN. On the one hand, we demonstrate
direct near-field optical excitation and emission through interaction with the
nanofocus of the tip resulting in a sub-diffraction limited tip-enhanced PL
hotspot. On the other hand, we show that indirect excitation and emission via
scattering from the tip significantly increases the recorded PL intensity. This
demonstrates that the tip-assisted PL (TAPL) process efficiently guides the
generated light to the detector. We apply the TAPL method to map the in-plane
dipole orientations of the hBN color centers on the nanoscale. This work
promotes the widely available s-SNOM approach to applications in the quantum
domain including characterization and optical control.","['cond-mat.mes-hall', 'physics.optics']",False,,,,Nuclear magnetic resonance spectroscopy in pulsed magnetic fields,"Nanoscale resolved mapping of the dipole emission of hBN color centers
  with a scattering-type scanning near-field optical microscope"
neg-d2-411,2025-01-04,,2501.02325," Modern sampling methods create ensembles of district maps that score well on
discrete compactness scores, whereas the Polsby-Popper and other shape-based
scores remain highly relevant for building fair maps and litigating unfair
ones. The aim of this paper is twofold. First, we introduce population-weighted
versions of shape-based scores and show a precise sense in which this
interpolates between shape-based and discrete scores. Second, we introduce a
modification of the ReCom sampling method that produces ensembles of maps with
improved shape-based compactness scores.","['physics.soc-ph', 'cs.CV']",2502.14314," You Look Only Once (YOLO) models have been widely used for building real-time
object detectors across various domains. With the increasing frequency of new
YOLO versions being released, key questions arise. Are the newer versions
always better than their previous versions? What are the core innovations in
each YOLO version and how do these changes translate into real-world
performance gains? In this paper, we summarize the key innovations from YOLOv1
to YOLOv11, introduce a comprehensive benchmark called ODverse33, which
includes 33 datasets spanning 11 diverse domains (Autonomous driving,
Agricultural, Underwater, Medical, Videogame, Industrial, Aerial, Wildlife,
Retail, Microscopic, and Security), and explore the practical impact of model
improvements in real-world, multi-domain applications through extensive
experimental results. We hope this study can provide some guidance to the
extensive users of object detection models and give some references for future
real-time object detector development.",['cs.CV'],False,,,,Revisiting Compactness for District Plans,"ODVerse33: Is the New YOLO Version Always Better? A Multi Domain
  benchmark from YOLO v5 to v11"
neg-d2-412,2025-01-20,,2501.11354," Recently, we have witnessed the rapid development of large language models,
which have demonstrated excellent capabilities in the downstream task of code
generation. However, despite their potential, LLM-based code generation still
faces numerous technical and evaluation challenges, particularly when embedded
in real-world development. In this paper, we present our vision for current
research directions, and provide an in-depth analysis of existing studies on
this task. We propose a six-layer vision framework that categorizes code
generation process into distinct phases, namely Input Phase, Orchestration
Phase, Development Phase, and Validation Phase. Additionally, we outline our
vision workflow, which reflects on the currently prevalent frameworks. We
systematically analyse the challenges faced by large language models, including
those LLM-based agent frameworks, in code generation tasks. With these, we
offer various perspectives and actionable recommendations in this area. Our aim
is to provide guidelines for improving the reliability, robustness and
usability of LLM-based code generation systems. Ultimately, this work seeks to
address persistent challenges and to provide practical suggestions for a more
pragmatic LLM-based solution for future code generation endeavors.","['cs.SE', 'cs.AI']",2501.11876," Surface-from-gradients (SfG) aims to recover a three-dimensional (3D) surface
from its gradients. Traditional methods encounter significant challenges in
achieving high accuracy and handling high-resolution inputs, particularly
facing the complex nature of discontinuities and the inefficiencies associated
with large-scale linear solvers. Although recent advances in deep learning,
such as photometric stereo, have enhanced normal estimation accuracy, they do
not fully address the intricacies of gradient-based surface reconstruction. To
overcome these limitations, we propose a Fourier neural operator-based
Numerical Integration Network (FNIN) within a two-stage optimization framework.
In the first stage, our approach employs an iterative architecture for
numerical integration, harnessing an advanced Fourier neural operator to
approximate the solution operator in Fourier space. Additionally, a
self-learning attention mechanism is incorporated to effectively detect and
handle discontinuities. In the second stage, we refine the surface
reconstruction by formulating a weighted least squares problem, addressing the
identified discontinuities rationally. Extensive experiments demonstrate that
our method achieves significant improvements in both accuracy and efficiency
compared to current state-of-the-art solvers. This is particularly evident in
handling high-resolution images with complex data, achieving errors of fewer
than 0.1 mm on tested objects.",['cs.CV'],False,,,,"Towards Advancing Code Generation with Large Language Models: A Research
  Roadmap","FNIN: A Fourier Neural Operator-based Numerical Integration Network for
  Surface-form-gradients"
neg-d2-413,2025-02-14,,2502.10222," If $A \colon D(A) \subset \mathcal{H} \to \mathcal{H}$ is an unbounded
Fredholm operator of index $0$ on a Hilbert space $\mathcal{H}$ with a dense
domain $D(A)$, then its spectrum is either discrete or the entire complex
plane. This spectral dichotomy plays a central role in the study of
\textit{magic angles} in twisted bilayer graphene.
  This paper proves that if such operators (with certain additional
assumptions) are perturbed by certain random trace-class operators, their
spectrum is discrete with high probability.","['math.SP', 'math-ph', 'math.MP']",2501.17484," We present a method for solving a large-scale stochastic capacity expansion
problem which explicitly considers reliability constraints, in particular
constraints on expected energy not served. Our method tackles this problem by a
Lagrange relaxation of the expected energy not served constraints. We solve the
relaxed formulation in an iterative manner, using a subgradient-based method.
Each iteration requires the solution of a stochastic capacity expansion
problem, for which we implement a subgradient decomposition scheme in a
high-performance computing infrastructure. We apply the proposed methodology on
the Economic Viability Assessment model that is used by ENTSO-E in the annual
European Resource Adequacy Assessment, extended to include explicit reliability
constraints. The approach is able to solve this model achieving a 1.3%
optimality gap. We compare our approach against accounting for reliability
through penalizing load shedding at VOLL, and find that the former results in
1.6% savings in total cost. We are also able to quantify the cost savings from
allowing some load curtailment in the capacity planning process, which ranges
from 1.6 to 6% in the cases analyzed.","['eess.SY', 'cs.SY']",False,,,,Spectral Instability of Random Fredholm Operators,"Capacity Expansion Planning under Uncertainty subject to Expected Energy
  Not Served Constraints"
neg-d2-414,2025-01-06,,2501.0317," Microwave shielding is an important technique that can suppress the losses
that arise from collisions of ultracold polar molecules. It has been
instrumental in achieving molecular Bose-Einstein condensation (BEC) for NaCs
[Bigagli et al., Nature 631, 289 (2024)]. We demonstrate that microwave
shielding is universal, in the sense that the 2-body collision properties of
different molecules are very similar when expressed in suitable reduced units
of length and energy. This applies to rate coefficients for inelastic
scattering and loss, to scattering lengths, and to the properties of 2-molecule
bound states. We also explore the small deviations from universality that arise
at very large Rabi frequencies. In general, the collision properties are
near-universal except when the Rabi frequency exceeds a few percent of the
molecular rotational constant. The universality extends to elliptically
polarized microwaves and to combinations of multiple fields. Our results
indicate that the methods that have been used to achieve BEC for NaCs can be
transferred directly to most other polar molecules.","['cond-mat.quant-gas', 'physics.atom-ph']",2503.12075," We develop a structure theory for the limit of $SU(2)$ $G_2$-monopoles (resp.
Calabi-Yau monopoles) on a principal $SU(2)$-bundle over an asymptotically
conical $G_2$-manifolds (resp. Calabi-Yau 3-folds) as the mass parameter tends
to infinity, while the topologial data for the bundle stays fixed. We show how
to extract a singular abelian $G_2$-monopole (resp. Calabi-Yau monopole) with
Dirac singularity along a calibrated cycle in the large mass limit, and we
prove an energy identity for monopole bubbles.",['math.DG'],False,,,,Universality in the microwave shielding of ultracold polar molecules,The large mass limit of $G_2$ and Calabi-Yau monopoles
neg-d2-415,2025-02-09,,2502.0592," This paper studies the implementation of Bayes correlated equilibria in
symmetric Bayesian nonatomic games, using direct information structures and
obedient strategies. The main results demonstrate full implementation in a
class of games with positive cost externalities. Specifically, if the game
admits a strictly convex potential in every state, then for every Bayes
correlated equilibrium outcome with finite support and rational action
distributions, there exists a direct information structure that implements this
outcome under all equilibria. When the potential is only weakly convex, we show
that all equilibria implement the same expected social cost. Additionally, all
Bayes correlated equilibria, including those with infinite support or
irrational action distributions, are approximately implemented.",['econ.TH'],2502.20506," This study examines the mid-infrared properties of Giant HII (GHII) regions
in the Milky Way's Central Molecular Zone (CMZ) -- Sgr B1, Sgr B2, and Sgr C --
using SOFIA-FORCAST imaging at 25 and 37 microns. It compares these
mid-infrared data with previous multi-wavelength observations to explore their
present star formation activity and global properties. The study identifies 77
massive young stellar object (MYSO) candidates in and around the three regions.
Sgr B2 appears to host the youngest MYSOs and have much higher extinction than
the other regions, containing several radio sources not detected in the
mid-infrared even at 37 microns. Meanwhile, cm radio continuum regions of Sgr
B1 shows remarkable correspondence to its mid-infrared emission. Sgr C has
fewer confirmed MYSOs, and seems to have a higher fraction of low-mass young
stellar objects and contamination from more evolved interloper/foreground
stars. Derived MYSO densities are consistent with GHII regions elsewhere in the
Galactic plane, though the CMZ GHII regions appear to have less prolific
present star formation overall. Unlike Sgr B2, the cm continuum emission in Sgr
B1 and Sgr C GHII regions appears to be absent cold dust and molecular gas,
suggesting environmental differences, possibly driven by turbulence and rapid
dynamical changes near the Galactic Center. Furthermore, unlike typical GHII
regions, Sgr B1 and Sgr C are significantly ionized by evolved interloper
stars, which likely did not form within these regions. In these ways, Sgr B1
and Sgr C deviate from classical GHII region behavior, thus potentially
representing a new category of GHII region or challenging their classification
as GHII regions.","['astro-ph.GA', 'astro-ph.SR']",False,,,,Full Implementation via Information Design in Nonatomic Games,"Surveying the Giant HII Regions of the Milky Way with SOFIA: VII.
  Galactic Center Regions Sgr B1, Sgr B2, and Sgr C"
neg-d2-416,2025-01-08,,2501.04653," Three-dimensional quantum spin liquids have remained elusive, hindered by
reduced quantum fluctuations from larger lattice connectivity inherent to
high-dimensional systems. Here, we investigate the remarkable persistence of
dynamical short-range magnetic correlations in the nearly body-centered cubic
garnet NaCa$_2$Cu$_2$(VO$_4$)$_3$ down to $T = 50$ mK, two orders of magnitude
below its Curie-Weiss temperature. Using a combination of neutron and muon
spectroscopies plus numerical simulations, we demonstrate that a spin-liquid
phase emerges from the interplay of strongly frustrated exchange interactions
and subtle temperature-dependent Jahn-Teller spin-lattice effects.","['cond-mat.str-el', 'cond-mat.mtrl-sci']",2501.15687," In this work we study the problem of user association and resource allocation
to maximize the proportional fairness of a wireless network with limited
backhaul capacity. The optimal solution of this problem requires solving a
mixed integer non-linear programming problem which generally cannot be solved
in real time. We propose instead to model the problem as a potential game,
which decreases dramatically the computational complexity and obtains a user
association and resource allocation close to the optimal solution.
Additionally, the use of a game-theoretic approach allows an efficient
distribution of the computational burden among the computational resources of
the network.",['cs.NI'],False,,,,"Three-dimensional spin liquid state in the frustrated $S = 1/2$
  Heisenberg garnet NaCa$_{2}$Cu$_{2}$(VO$_{4}$)$_{3}$","Joint Cell Selection and Resource Allocation Games with Backhaul
  Constraints"
neg-d2-417,2025-03-19,,2503.1656," Alzheimer's disease and related dementias (AD/ADRD) represent a growing
healthcare crisis affecting over 6 million Americans. While genetic factors
play a crucial role, emerging research reveals that social determinants of
health (SDOH) significantly influence both the risk and progression of
cognitive functioning, such as cognitive scores and cognitive decline. This
report examines how these social, environmental, and structural factors impact
cognitive health trajectories, with a particular focus on Hispanic populations,
who face disproportionate risk for AD/ADRD. Using data from the Mexican Health
and Aging Study (MHAS) and its cognitive assessment sub study (Mex-Cog), we
employed ensemble of regression trees models to predict 4-year and 9-year
cognitive scores and cognitive decline based on SDOH. This approach identified
key predictive SDOH factors to inform potential multilevel interventions to
address cognitive health disparities in this population.","['q-bio.QM', 'cs.LG', 'stat.AP']",2503.09533," Designing strategyproof mechanisms for multi-facility location that optimize
social costs based on agent preferences had been challenging due to the
extensive domain knowledge required and poor worst-case guarantees. Recently,
deep learning models have been proposed as alternatives. However, these models
require some domain knowledge and extensive hyperparameter tuning as well as
lacking interpretability, which is crucial in practice when transparency of the
learned mechanisms is mandatory. In this paper, we introduce a novel approach,
named LLMMech, that addresses these limitations by incorporating large language
models (LLMs) into an evolutionary framework for generating interpretable,
hyperparameter-free, empirically strategyproof, and nearly optimal mechanisms.
Our experimental results, evaluated on various problem settings where the
social cost is arbitrarily weighted across agents and the agent preferences may
not be uniformly distributed, demonstrate that the LLM-generated mechanisms
generally outperform existing handcrafted baselines and deep learning models.
Furthermore, the mechanisms exhibit impressive generalizability to
out-of-distribution agent preferences and to larger instances with more agents.",['cs.LG'],False,,,,"Early Prediction of Alzheimer's and Related Dementias: A Machine
  Learning Approach Utilizing Social Determinants of Health Data",Large Language Models for Multi-Facility Location Mechanism Design
neg-d2-418,2025-02-23,,2502.16534," Large Language Models (LLMs) are becoming increasingly capable across global
languages. However, the ability to communicate across languages does not
necessarily translate to appropriate cultural representations. A key concern is
US-centric bias, where LLMs reflect US rather than local cultural values. We
propose a novel methodology that compares LLM-generated response distributions
against population-level opinion data from the World Value Survey across four
languages (Danish, Dutch, English, and Portuguese). Using a rigorous linear
mixed-effects regression framework, we compare two families of models: Google's
Gemma models (2B--27B parameters) and successive iterations of OpenAI's
turbo-series. Across the families of models, we find no consistent
relationships between language capabilities and cultural alignment. While the
Gemma models have a positive correlation between language capability and
cultural alignment across languages, the OpenAI models do not. Importantly, we
find that self-consistency is a stronger predictor of multicultural alignment
than multilingual capabilities. Our results demonstrate that achieving
meaningful cultural alignment requires dedicated effort beyond improving
general language capabilities.","['cs.CL', 'cs.AI', 'cs.CY']",2503.02324," The ability of large language models to solve complex mathematical problems
has progressed significantly, particularly for tasks requiring advanced
reasoning. However, the scarcity of sufficiently challenging problems,
particularly at the Olympiad level, hinders further advancements. In this work,
we introduce PromptCoT, a novel approach for automatically generating
high-quality Olympiad-level math problems. The proposed method synthesizes
complex problems based on mathematical concepts and the rationale behind
problem construction, emulating the thought processes of experienced problem
designers. We provide a theoretical analysis demonstrating that an optimal
rationale should maximize both the likelihood of rationale generation given the
associated concepts and the likelihood of problem generation conditioned on
both the rationale and the concepts. Our method is evaluated on standard
benchmarks including GSM8K, MATH-500, and AIME2024, where it consistently
outperforms existing problem generation methods. Furthermore, we demonstrate
that PromptCoT exhibits superior data scalability, consistently maintaining
high performance as the dataset size increases, outperforming the baselines.
The implementation is available at https://github.com/zhaoxlpku/PromptCoT.","['cs.CL', 'cs.AI', 'cs.LG']",False,,,,"Multilingual != Multicultural: Evaluating Gaps Between Multilingual
  Capabilities and Cultural Alignment in LLMs","PromptCoT: Synthesizing Olympiad-level Problems for Mathematical
  Reasoning in Large Language Models"
neg-d2-419,2025-03-03,,2503.02031," This study utilised the dynamics of five time-varying models to estimate six
essential features of financial return volatility that are relevant for robust
risk management. These features include pronounced persistence, mean reversion,
leverage effect or volatility asymmetry, conditional skewness, conditional
fat-tailedness, and the long memory behaviour of volatility decomposition into
long-term and short-term components. Both simulation and empirical evidence are
provided. Through the applications of these models using the S&P Indian index,
the study shows that the market returns are characterised by these volatility
features. Our findings from the long-memory behaviour revealed that although
the response to shocks is greater in the short-term component, it is however
short-lived. On the contrary, despite a high degree of persistence in the
long-term component, market information or unexpected news arrival only has a
low long-run impact on the market. Based on this, the long-run investment risks
within the Indian stock market seem to be under control. Hence, our findings
suggest that rational investors should try to stay calm with the arrival of
unexpected news in the market because the long-run effect of such news will not
be severe, and the market will eventually return to its normal state.",['stat.AP'],2501.06429," Vertical Federated Learning (VFL) is a well-known FL variant that enables
multiple parties to collaboratively train a model without sharing their raw
data. Existing VFL approaches focus on overlapping samples among different
parties, while their performance is constrained by the limited number of these
samples, leaving numerous non-overlapping samples unexplored. Some previous
work has explored techniques for imputing missing values in samples, but often
without adequate attention to the quality of the imputed samples. To address
this issue, we propose a Reliable Imputed-Sample Assisted (RISA) VFL framework
to effectively exploit non-overlapping samples by selecting reliable imputed
samples for training VFL models. Specifically, after imputing non-overlapping
samples, we introduce evidence theory to estimate the uncertainty of imputed
samples, and only samples with low uncertainty are selected. In this way,
high-quality non-overlapping samples are utilized to improve VFL model.
Experiments on two widely used datasets demonstrate the significant performance
gains achieved by the RISA, especially with the limited overlapping samples,
e.g., a 48% accuracy gain on CIFAR-10 with only 1% overlapping samples.","['cs.LG', 'stat.ML']",False,,,,"A Comparative Modelling of Essential Characteristics of Volatility:
  Simulation and Empirical Study",Reliable Imputed-Sample Assisted Vertical Federated Learning
neg-d2-420,2025-02-03,,2502.01283," We describe the dynamics of a detector modeled by a harmonic oscillator
coupled with an otherwise free quantum field in a curved spacetime in terms of
covariant equations of motion leading to local observables. To achieve this, we
derive and renormalize the integro-differential equation that governs the
detector pointer-variable dynamics, introducing phenomenological parameters
such as a dispersion coefficient and a Lamb-shift parameter. Our formal
solution, expressed in terms of Green's functions, allows for the covariant,
and causal analysis of induced observables on the field. This formalism can be
used for instance to detect non-Gaussianities present in the field's state.","['gr-qc', 'hep-th', 'quant-ph']",2501.0588," Designing efficient neural networks for embedded devices is a critical
challenge, particularly in applications requiring real-time performance, such
as aerial imaging with drones and UAVs for emergency responses. In this work,
we introduce TakuNet, a novel light-weight architecture which employs
techniques such as depth-wise convolutions and an early downsampling stem to
reduce computational complexity while maintaining high accuracy. It leverages
dense connections for fast convergence during training and uses 16-bit
floating-point precision for optimization on embedded hardware accelerators.
Experimental evaluation on two public datasets shows that TakuNet achieves
near-state-of-the-art accuracy in classifying aerial images of emergency
situations, despite its minimal parameter count. Real-world tests on embedded
devices, namely Jetson Orin Nano and Raspberry Pi, confirm TakuNet's
efficiency, achieving more than 650 fps on the 15W Jetson board, making it
suitable for real-time AI processing on resource-constrained platforms and
advancing the applicability of drones in emergency scenarios. The code and
implementation details are publicly released.","['cs.CV', 'cs.PF']",False,,,,Covariant non-perturbative pointer variables for quantum fields,"TakuNet: an Energy-Efficient CNN for Real-Time Inference on Embedded UAV
  systems in Emergency Response Scenarios"
neg-d2-421,2025-03-20,,2503.16601," We introduce SOFIA, a Mathematica package that automatizes the computation of
singularities of Feynman integrals, based on new theoretical understanding of
their analytic structure. Given a Feynman diagram, SOFIA generates a list of
potential singularities along with a candidate symbol alphabet. The package
also provides a comprehensive set of tools for analyzing the analytic
properties of Feynman integrals and related objects, such as cosmological and
energy correlators. We showcase its capabilities by reproducing known results
and predicting singularities and symbol alphabets of Feynman integrals at and
beyond the high-precision frontier.",['hep-th'],2503.09695," Recent work has shown that the triangular lattice spin-$1/2$ $J_1$-$J_2$
Heisenberg and XXZ antiferromagnets may exhibit coplanar or supersolid orders
proximate to a gapless Dirac spin liquid phase. We explore a distinct
$SU(2N)\!\!\times\!\!SU(M)$ fermionic parton approach, complemented by
variational Monte Carlo calculations for the spin-$1/2$ model, to study the
phase diagram of these models. We also calculate their dynamical spin response
including parton interactions within a random phase approximation, and discuss
implications for neutron scattering on triangular lattice cobaltates
Ba$_3$CoSb$_2$O$_9$, Na$_2$BaCo(PO$_4$)$_2$, K$_2$Co(SeO$_3$)$_2$,
Rb$_2$Co(SeO$_3$)$_2$, and Yb-based magnet KYbSe$_2$.",['cond-mat.str-el'],False,,,,SOFIA: Singularities of Feynman Integrals Automatized,"Modified large-$N$ approach to gapless spin liquids, magnetic orders,
  and dynamics: Application to triangular lattice antiferromagnets"
neg-d2-422,2025-02-11,,2502.07733," We present {\it JWST} observations of a gravitationally-lensed, extremely
metal-poor galaxy at redshift $z=8.203\pm 0.001$ from the CANUCS survey. Based
on the low oxygen to Balmer line ratios we infer a gas-phase metallicity of
$12+{\rm log(O/H)}=6.85$ (1.4\% solar), making CANUCS-A370-z8-LAE the most
metal-poor galaxy known at $z>7$. With a high H$\beta$ equivalent width of
$225\pm50$\,\AA\ and a half-light radius of only $r_{\rm hl} = 38 ^{+3}_{-19}
$\,pc, the galaxy has a high star-formation-rate density of $50 -
100\,M_{\odot}$\,yr$^{-1}$\,kpc$^{-2}$. The galaxy shows high equivalent width
Lyman-$\alpha$ emission with an inferred Lyman-$\alpha$ escape fraction of
$0.21 \pm 0.05$. The high escape fraction of Lyman-$\alpha$ is likely due to
the compact starbursting nature of the galaxy combined with its location in an
overdensity traced by at least two other galaxies spectroscopically confirmed
to lie within $\delta z = 0.01$ that have helped to reionize the environment.
The low metallicity of CANUCS-A370-z8-LAE is best explained by a model where
infalling metal-poor gas dilutes the interstellar medium, rather than being a
young galaxy forming its first stellar populations.",['astro-ph.GA'],2502.08654," Entropy and its various generalizations are important in many fields,
including mathematical statistics, communication theory, physics and computer
science, for characterizing the amount of information associated with a
probability distribution. In this paper we propose goodness-of-fit statistics
for the multivariate Student and multivariate Pearson type II distributions,
based on the maximum entropy principle and a class of estimators for Renyi
entropy based on nearest neighbour distances. We prove the L2-consistency of
these statistics using results on the subadditivity of Euclidean functionals on
nearest neighbour graphs, and investigate their rate of convergence and
asymptotic distribution using Monte Carlo methods.","['stat.ME', 'math.ST', 'stat.TH']",False,,,,"In Search of the First Stars: An Ultra-Compact and Very Low Metallicity
  Lyman-$\alpha$ Emitter Deep Within the Epoch of Reionization",Statistical tests based on Renyi entropy estimation
neg-d2-423,2025-03-10,,2503.07754," The physical properties of stellar atmospheres in rapidly rotating massive
stars, such as Be stars, are critical to understanding their evolution and
their role as progenitors of supernovae. These stars, which often have
near-critical rotation, exhibit equatorial stretching and gravity darkening,
which significantly complicates the determination of parameters such as the
inclination angle. Be stars, characterized by their extreme rotational
velocities, serve as excellent candidates for exploring these phenomena.
However, fundamental quantities such as polar and equatorial radii and
inclination angles are typically derived from interferometry, which applies
only to a limited number of stars. This study aims to enhance the determination
of inclination angles for Be stars using the ZPEKTR spectral synthesis code. By
incorporating advanced models of gravity darkening and stellar deformation, we
evaluated the effectiveness of this method with a sample of ten Be stars from
the BeSOS database, comparing results with established interferometric data.
Methods. We used the ZPEKTR code to model the effects of stellar oblateness and
gravity darkening on spectral lines, focusing on the HeI 4471 line. We applied
a chi-squared test minimization approach to identify the best-fitting models,
and we evaluated the inclination angles derived against interferometric
measurements. Our analysis reveals a robust linear correlation between the
inclination angles derived from ZPEKTR and using interferometric techniques,
which demonstrates an excellent agreement. The ZPEKTR code effectively models
high rotational velocity effects, providing precise stellar parameter
determinations. The results underscore the potential of advanced spectroscopic
techniques to yield inclination measurements comparable to interferometry,
which offers a pathway to studying distant massive stars.",['astro-ph.SR'],2501.09864," Recent findings in orbitronics pointed out large current-induced torques
originating, in the current understanding, from incident orbital currents.
These are generated by orbital Rashba-Edelstein effect (OREE) produced at the
interface between some light metal and oxides films e.g. by naturally oxidized
copper layer (Cu*). In the present work, by using second harmonic Hall
techniques, we determine the ratio of orbital vs spin currents exerting torques
on thin transition metals Co ferromagnet in systems using an orbit-to-spin Pt
converter as interlayer with Cu*. Our results quantifying damping like torques
show that both orbital and spin currents are enhanced in these systems.
Moreover, the experimental determination of the decoherence length in a sample
series with varying Co thickness clearly demonstrates the interfacial
generation of the orbital currents in Cu* by Orbital Rashba-Edelstein effects
(REE) leading to subsequent magnetic torque in Co over a typical lengthscale of
several nanometers","['cond-mat.mtrl-sci', 'cond-mat.mes-hall']",False,,,,Unveiling stellar spin: Determining inclination angles in Be stars,"Quantitative analysis of vectorial torques in thin 3d Co ferromagnet
  using orbital-spin conversion"
neg-d2-424,2025-03-03,,2503.01396," Copious mobile operating systems exist in the market, but Android remains the
user's choice. Meanwhile, its growing popularity has also attracted malware
developers. Researchers have proposed various static solutions for Android
malware detection. However, stealthier malware evade static analysis. This
raises the need for a robust Android malware detection system capable of
dealing with advanced threats and overcoming the shortcomings of static
analysis.
  Hence, this work proposes a dynamic analysis-based Android malware detection
system, CorrNetDroid, that works over network traffic flows. Many traffic
features exhibit overlapping ranges in normal and malware datasets. Therefore,
we first rank the features using two statistical measures, crRelevance and
Normalized Mean Residue Similarity (NMRS), to assess feature-class and
feature-feature correlations. Thereafter, we introduce a novel
correlation-based feature selection algorithm that applies NMRS on crRelevance
rankings to identify the optimal feature subset for Android malware detection.
  Experimental results highlight that our model effectively reduces the feature
set while detecting Android malware with 99.50 percent accuracy when
considering only two network traffic features. Furthermore, our experiments
demonstrate that the NMRS-based algorithm on crRelevance rankings outperforms
statistical tests such as chi-square, ANOVA, Mann-Whitney U test, and
Kruskal-Wallis test. In addition, our model surpasses various state-of-the-art
Android malware detection techniques in terms of detection accuracy.","['cs.CR', 'cs.MM']",2503.11036," We theoretically propose a method to generate topological spin textures by
irradiating a classical spin system with a linearly polarized AC electric
field. To this end, we investigate non-equilibrium steady states in a classical
Heisenberg model with frustrated exchange interactions on a two-dimensional
triangular lattice by numerically solving the Landau-Lifshitz-Gilbert equation
at zero temperature. Our results reveal that the linearly polarized AC
electric-field irradiation induces a topological phase transition from a
single-Q spiral state to a bimeron crystal with the skyrmion number of one in
the low-frequency regime. Furthermore, we show that the obtained bimeron
crystal remains relatively stable against both easy-axis and easy-plane
single-ion anisotropies.","['cond-mat.str-el', 'cond-mat.mes-hall']",False,,,,"CorrNetDroid: Android Malware Detector leveraging a Correlation-based
  Feature Selection for Network Traffic features","Bimeron Crystals by a Linearly Polarized AC Electric Field in Frustrated
  Magnets"
neg-d2-425,2025-01-06,,2501.02822," Road damage detection and assessment are crucial components of infrastructure
maintenance. However, current methods often struggle with detecting multiple
types of road damage in a single image, particularly at varying scales. This is
due to the lack of road datasets with various damage types having varying
scales. To overcome this deficiency, first, we present a novel dataset called
Diverse Road Damage Dataset (DRDD) for road damage detection that captures the
diverse road damage types in individual images, addressing a crucial gap in
existing datasets. Then, we provide our model, RDD4D, that exploits Attention4D
blocks, enabling better feature refinement across multiple scales. The
Attention4D module processes feature maps through an attention mechanism
combining positional encoding and ""Talking Head"" components to capture local
and global contextual information. In our comprehensive experimental analysis
comparing various state-of-the-art models on our proposed, our enhanced model
demonstrated superior performance in detecting large-sized road cracks with an
Average Precision (AP) of 0.458 and maintained competitive performance with an
overall AP of 0.445. Moreover, we also provide results on the CrackTinyNet
dataset; our model achieved around a 0.21 increase in performance. The code,
model weights, dataset, and our results are available on
\href{https://github.com/msaqib17/Road_Damage_Detection}{https://github.com/msaqib17/Road\_Damage\_Detection}.","['cs.CV', 'cs.AI', 'cs.RO']",2502.13968," Separable 3D reconstruction of multiple objects from multi-view RGB images --
resulting in two different 3D shapes for the two objects with a clear
separation between them -- remains a sparsely researched problem. It is
challenging due to severe mutual occlusions and ambiguities along the objects'
interaction boundaries. This paper investigates the setting and introduces a
new neuro-implicit method that can reconstruct the geometry and appearance of
two objects undergoing close interactions while disjoining both in 3D, avoiding
surface inter-penetrations and enabling novel-view synthesis of the observed
scene. The framework is end-to-end trainable and supervised using a novel
alpha-blending regularisation that ensures that the two geometries are well
separated even under extreme occlusions. Our reconstruction method is
markerless and can be applied to rigid as well as articulated objects. We
introduce a new dataset consisting of close interactions between a human and an
object and also evaluate on two scenes of humans performing martial arts. The
experiments confirm the effectiveness of our framework and substantial
improvements using 3D and novel view synthesis metrics compared to several
existing approaches applicable in our setting.",['cs.CV'],False,,,,RDD4D: 4D Attention-Guided Road Damage Detection And Classification,"Betsu-Betsu: Multi-View Separable 3D Reconstruction of Two Interacting
  Objects"
neg-d2-426,2025-01-19,,2501.11126," Multi-antenna coded caching (CC) with multicast beamforming typically relies
on a complex successive interference cancellation (SIC) structure to decode a
superposition of multiple streams received by each user. Signal-level CC
schemes require the regeneration and cancellation of interfering signals at the
physical layer of each receiver, which complicates practical implementations.
To address this, we propose a bit-level multicast scheduling scheme enabling
linear, SIC-free decoding of parallel streams by repeatedly transmitting data
terms with linearly independent coefficients. Two reference strategies and a
novel sparse strategy are considered for constructing the coefficient matrix.
The reference cases include the random strategy, which lacks control over
matrix construction, and the equal-distant strategy, which balances users'
interference and data terms equally. In contrast, the sparse strategy minimizes
the number of multicast streams transmitted in parallel during each interval.
This approach simplifies both the decoding process and the beamforming design
by decoupling the desired data terms for each user and reducing the number of
SINR constraints, respectively. To further enhance the symmetric rate, a
successive projection algorithm is applied to exploit channel properties and
optimize user ordering. With the coefficient matrix and optimized user ordering
in place, multicast beamformers are devised to aggregate desired data from
relevant multicast streams. Numerical simulations validate the effectiveness of
the sparse strategy and user scheduling, demonstrating significant gains in
symmetric rate.","['cs.IT', 'math.IT']",2503.12966," Score-based generative models achieve state-of-the-art sampling performance
by denoising a distribution perturbed by Gaussian noise. In this paper, we
focus on a single deterministic denoising step, and compare the optimal
denoiser for the quadratic loss, we name ''full-denoising'', to the alternative
''half-denoising'' introduced by Hyv{\""a}rinen (2024). We show that looking at
the performances in term of distance between distribution tells a more nuanced
story, with different assumptions on the data leading to very different
conclusions. We prove that half-denoising is better than full-denoising for
regular enough densities, while full-denoising is better for singular densities
such as mixtures of Dirac measures or densities supported on a low-dimensional
subspace. In the latter case, we prove that full-denoising can alleviate the
curse of dimensionality under a linear manifold hypothesis.","['cs.LG', 'stat.ML']",False,,,,SIC-free Multicast Scheduling for Multi-antenna Coded Caching,"Optimal Denoising in Score-Based Generative Models: The Role of Data
  Regularity"
neg-d2-427,2025-01-16,,2501.09864," Recent findings in orbitronics pointed out large current-induced torques
originating, in the current understanding, from incident orbital currents.
These are generated by orbital Rashba-Edelstein effect (OREE) produced at the
interface between some light metal and oxides films e.g. by naturally oxidized
copper layer (Cu*). In the present work, by using second harmonic Hall
techniques, we determine the ratio of orbital vs spin currents exerting torques
on thin transition metals Co ferromagnet in systems using an orbit-to-spin Pt
converter as interlayer with Cu*. Our results quantifying damping like torques
show that both orbital and spin currents are enhanced in these systems.
Moreover, the experimental determination of the decoherence length in a sample
series with varying Co thickness clearly demonstrates the interfacial
generation of the orbital currents in Cu* by Orbital Rashba-Edelstein effects
(REE) leading to subsequent magnetic torque in Co over a typical lengthscale of
several nanometers","['cond-mat.mtrl-sci', 'cond-mat.mes-hall']",2502.07338," Ultrafast electron microscopy aims for imaging transient phenomena occurring
on nanoscale. One of its goals is to visualize localized optical and plasmonic
modes generated by coherent excitation in the vicinity of various types of
nanostructures. Such imaging capability was enabled by photon-induced
near-field optical microscopy, which is based on spectral filtering of
electrons inelastically scattered due to the stimulated interaction with the
near-field. Here we report on the development of ultrafast 4D scanning
transmission electron microscopy, which allows us to image the transverse
components of the optical near-field while avoiding the need of electron
spectral filtering. We demonstrate that this method is capable of imaging
optical near-fields of a tungsten nanotip and ponderomotive potential of an
optical standing wave with a spatial resolution of 21 nm.","['physics.optics', 'physics.ins-det']",False,,,,"Quantitative analysis of vectorial torques in thin 3d Co ferromagnet
  using orbital-spin conversion","Ultrafast 4D scanning transmission electron microscopy for imaging of
  localized optical fields"
neg-d2-428,2025-01-21,,2501.12282," This work shows new results on the complexity of games Jelly-No and Hanano
with various constraints on the size of the board and number of colours. Hanano
and Jelly-No are one-player, 2D side-view puzzle games with a dynamic board
consisting of coloured, movable blocks disposed on platforms. These blocks can
be moved by the player and are subject to gravity. Both games somehow vary in
their gameplay, but the goal is always to move the coloured blocks in order to
reach a specific configuration and make them interact with each other or with
other elements of the game. In Jelly-No the goal is to merge all coloured
blocks of a same colour, which also happens when they make contact. In Hanano
the goal is to make all the coloured blocks bloom by making contact with
flowers of the same colour. Jelly-No was proven by Chao Yang to be NP-Complete
under the restriction that all movable blocks are the same colour and NP-Hard
for more colours. Hanano was proven by Michael C. Chavrimootoo to be
PSPACE-Complete under the restriction that all movable blocks are the same
colour. However, the question whether Jelly-No for more than one colours is
also PSPACE-complete or if it too stays in NP was left open. In this paper, we
settle this question, proving that Jelly-No is PSPACE-Complete with an
unbounded number of colours. We further show that, if we allow black jellies
(that is, jellies that do not need to be merged), the game is PSPACE-complete
even for one colour. We further show that one-colour Jelly-No and Hanano remain
NP-Hard even if the width or the height of the board are small constants.",['cs.CC'],2503.0748," We investigate the dynamics of small inertial particles in a two-dimensional,
steady Taylor-Green vortex flow. A classic study by Taylor (2022) showed that
heavy inertial point particles (having density parameter R = 1) are trapped by
the flow separatrices when the particle Stokes number St, which measures the
particle's inertia, is less than 1/4. Here, we consider finitely dense
particles, incorporating the previously neglected effects of added mass and the
Boussinesq-Basset history force. Using linear stability analysis near
stagnation points, we determine the critical parametric conditions in the St-R
plane that leads to particle trapping within vortex cells. We identify
additional stagnation points perceived by inertial particles, beyond the
traditional ones at vortex cell corners, when the added mass effect is
included, and we analyze their stability. Numerical analysis of the full
nonlinear system confirms the existence of distinct particle
behaviours--trapped, diffusive, and ballistic--depending on initial conditions,
consistent with Nath et al. (2024), with modifications due to added mass
effect. We delineate the regions in the St-R plane where these behaviours
dominate based on the prominent particle dynamics. However, when both the
history force and added mass effect are included, all particles exhibit
ballistic motion regardless of St and R.",['physics.flu-dyn'],False,,,,Complexity of Jelly-No and Hanano games with various constraints,"Trapping and Transport of Inertial Particles in a Taylor-Green Vortex:
  Effects of Added Mass and History Force"
neg-d2-429,2025-02-25,,2502.1814," Trace conjunction integrals are introduced and studied. They appear in trace
conjunction inequalities which unify the Hardy inequality on a halfspace and
the classical Gagliardo trace inequality. At the endpoint they satisfy a
Bourgain-Brezis-Mironescu formula for smooth maps, which raises some new open
problems.",['math.FA'],2503.04199," RGB-Thermal fusion is a potential solution for various weather and light
conditions in challenging scenarios. However, plenty of studies focus on
designing complex modules to fuse different modalities. With the widespread
application of large language models (LLMs), valuable information can be more
effectively extracted from natural language. Therefore, we aim to leverage the
advantages of large language models to design a structurally simple and highly
adaptable multimodal fusion model architecture. We proposed MultimodAl
Segmentation with TExt PRompts (MASTER) architecture, which integrates LLM into
the fusion of RGB-Thermal multimodal data and allows complex query text to
participate in the fusion process. Our model utilizes a dual-path structure to
extract information from different modalities of images. Additionally, we
employ LLM as the core module for multimodal fusion, enabling the model to
generate learnable codebook tokens from RGB, thermal images, and textual
information. A lightweight image decoder is used to obtain semantic
segmentation results. The proposed MASTER performs exceptionally well in
benchmark tests across various automated driving scenarios, yielding promising
results.","['cs.CV', 'cs.AI']",False,,,,Trace conjunction inequalities,MASTER: Multimodal Segmentation with Text Prompts
neg-d2-430,2025-03-15,,2503.12159," In the context of increasing global climate change, decarbonizing the
residential building sector is crucial for sustainable development. This study
is the first to analyze the role of various influencing factors in carbon
intensity changes using the decomposing structural decomposition (DSD) to
assess and compare the potential and effectiveness of electrifying end-use
activities during the operational phase of residential buildings worldwide for
decarbonization. The results show that (1) while the electrification rate
varied in its impact on emissions across different countries and regions, the
overall increase in electrification contributed to higher carbon intensity. In
contrast, changes in the emission factor of electricity generally made a
positive contribution to emission reduction globally. (2) The global
electrification level has significantly increased, with the electrification
rate rising from 29.9% in 2000 to 40.1% in 2021. A 39.8% increase in the
electricity-related carbon emissions of global residential buildings was
observed, increasing from 1452 MtCO2 to 2032 MtCO2, 2000-2021. (3) From 2000 to
2021, electrification of space heating was the main contributor to carbon
reduction, whereas the contributions of electrification to cooling and lighting
were relatively limited. Emission reductions from appliances and others
remained stable. The electrification of water heating and cooking had varying
effects on emission reductions in different countries. Furthermore, this study
proposes a series of electrification decarbonization strategies. Overall, this
study analyzes and contrasts decarbonization efforts from building
electrification at the global and regional levels, explores the key motivations
behind these efforts to aid national net-zero emission targets and accelerate
the transition of the global residential building sector toward a
carbon-neutral future.",['physics.soc-ph'],2502.20974," Open-world continual learning (OWCL) adapts to sequential tasks with open
samples, learning knowledge incrementally while preventing forgetting. However,
existing OWCL still requires a large amount of labeled data for training, which
is often impractical in real-world applications. Given that new
categories/entities typically come with limited annotations and are in small
quantities, a more realistic situation is OWCL with scarce labeled data, i.e.,
few-shot training samples. Hence, this paper investigates the problem of
open-world few-shot continual learning (OFCL), challenging in (i) learning
unbounded tasks without forgetting previous knowledge and avoiding overfitting,
(ii) constructing compact decision boundaries for open detection with limited
labeled data, and (iii) transferring knowledge about knowns and unknowns and
even update the unknowns to knowns once the labels of open samples are learned.
In response, we propose a novel OFCL framework that integrates three key
components: (1) an instance-wise token augmentation (ITA) that represents and
enriches sample representations with additional knowledge, (2) a margin-based
open boundary (MOB) that supports open detection with new tasks emerge over
time, and (3) an adaptive knowledge space (AKS) that endows unknowns with
knowledge for the updating from unknowns to knowns. Finally, extensive
experiments show the proposed OFCL framework outperforms all baselines
remarkably with practical importance and reproducibility. The source code is
released at https://github.com/liyj1201/OFCL.","['cs.LG', 'cs.AI']",False,,,,"Paving the way to carbon neutrality: Evaluating the decarbonization of
  residential building electrification worldwide","Improving Open-world Continual Learning under the Constraints of Scarce
  Labeled Data"
neg-d2-431,2025-01-24,,2501.14948," Histopathology, particularly hematoxylin and eosin (H\&E) staining, plays a
critical role in diagnosing and characterizing pathological conditions by
highlighting tissue morphology. However, H\&E-stained images inherently lack
molecular information, requiring costly and resource-intensive methods like
spatial transcriptomics to map gene expression with spatial resolution. To
address these challenges, we introduce HECLIP (Histology-Enhanced Contrastive
Learning for Imputation of Profiles), an innovative deep learning framework
that bridges the gap between histological imaging and molecular profiling.
HECLIP is specifically designed to infer gene expression profiles directly from
H\&E-stained images, eliminating the need for expensive spatial transcriptomics
assays. HECLIP leverages an advanced image-centric contrastive loss function to
optimize image representation learning, ensuring that critical morphological
patterns in histology images are effectively captured and translated into
accurate gene expression profiles. This design enhances the predictive power of
the image modality while minimizing reliance on gene expression data. Through
extensive benchmarking on publicly available datasets, HECLIP demonstrates
superior performance compared to existing approaches, delivering robust and
biologically meaningful predictions. Detailed ablation studies further
underscore its effectiveness in extracting molecular insights from histology
images. Additionally, HECLIP's scalable and cost-efficient approach positions
it as a transformative tool for both research and clinical applications,
driving advancements in precision medicine. The source code for HECLIP is
openly available at https://github.com/QSong-github/HECLIP.","['cs.CE', 'q-bio.QM']",2501.04329," While most existing neural image compression (NIC) and neural video
compression (NVC) methodologies have achieved remarkable success, their
optimization is primarily focused on human visual perception. However, with the
rapid development of artificial intelligence, many images and videos will be
used for various machine vision tasks. Consequently, such existing compression
methodologies cannot achieve competitive performance in machine vision. In this
work, we introduce an efficient adaptive compression (EAC) method tailored for
both human perception and multiple machine vision tasks. Our method involves
two key modules: 1), an adaptive compression mechanism, that adaptively selects
several subsets from latent features to balance the optimizations for multiple
machine vision tasks (e.g., segmentation, and detection) and human vision. 2),
a task-specific adapter, that uses the parameter-efficient delta-tuning
strategy to stimulate the comprehensive downstream analytical networks for
specific machine vision tasks. By using the above two modules, we can optimize
the bit-rate costs and improve machine vision performance. In general, our
proposed EAC can seamlessly integrate with existing NIC (i.e., Ball\'e2018, and
Cheng2020) and NVC (i.e., DVC, and FVC) methods. Extensive evaluation on
various benchmark datasets (i.e., VOC2007, ILSVRC2012, VOC2012, COCO, UCF101,
and DAVIS) shows that our method enhances performance for multiple machine
vision tasks while maintaining the quality of human vision.",['cs.CV'],False,,,,"HECLIP: Histology-Enhanced Contrastive Learning for Imputation of
  Transcriptomics Profiles","An Efficient Adaptive Compression Method for Human Perception and
  Machine Vision Tasks"
neg-d2-432,2025-03-12,,2503.09457," The effective conductivity ($T^{eff}$) of 2D and 3D Random Resistor Networks
(RRNs) with random edge conductivity is studied. The combined influence of
geometrical disorder, which controls the overall connectivity of the medium and
leads to percolation effects, and conductivity randomness is investigated. A
formula incorporating connectivity aspects and second-order averaging methods,
widely used in the stochastic hydrology community, is derived and extrapolated
to higher orders using a power averaging formula based on a mean-field
argument. This approach highlights the role of the so-called resistance
distance introduced by graph theorists. Simulations are performed on various
RRN geometries constructed from 2D and 3D bond-percolation lattices. The
results confirm the robustness of the power averaging technique and the
relevance of the mean-field assumption.",['cond-mat.dis-nn'],2502.07126," We propose to relax traditional axioms in decision theory by incorporating a
measurement, or degree, of satisfaction. For example, if the independence axiom
of expected utility theory is violated, we can measure the size of the
violation. This measure allows us to derive an approximation guarantee for a
utility representation that aligns with the unmodified version of the axiom.
Almost satisfying the axiom implies, then, a utility that is near a utility
representation. We develop specific examples drawn from expected utility theory
under risk and uncertainty.",['econ.TH'],False,,,,Effective conductivity of conduit networks with random conductivities,"Decision theory and the ""almost implies near"" phenomenon"
neg-d2-433,2025-01-07,,2501.03578," We theoretically propose a circuit of the four-body coupler for
superconducting qubits based on Josephson parametric oscillators (JPOs). Our
coupler for the four-body interaction has a superconducting loop, similar to a
capacitively shunted flux qubit, where an external magnetic flux set to half a
flux quantum is threaded. This coupler circuit is a specific setup of the
circuit called superconducting nonlinear asymmetric inductive elements (SNAIL)
and also is a generalization of the previously proposed one for the four-body
interaction of JPOs. We clarify roles of circuit parameters in the four-body
interaction and, in particular, show that the four-body coupling constant in
our circuit can be significantly increased by tuning capacitance of the coupler
or the area ratio of the Josephson junctions of the coupler.",['quant-ph'],2503.0081," In our quest for a reinforcement learning (RL) algorithm that is both
practical and provably optimal, we introduce EQO (Exploration via
Quasi-Optimism). Unlike existing minimax optimal approaches, EQO avoids
reliance on empirical variances and employs a simple bonus term proportional to
the inverse of the state-action visit count. Central to EQO is the concept of
quasi-optimism, where estimated values need not be fully optimistic, allowing
for a simpler yet effective exploration strategy. The algorithm achieves the
sharpest known regret bound for tabular RL under the mildest assumptions,
proving that fast convergence can be attained with a practical and
computationally efficient approach. Empirical evaluations demonstrate that EQO
consistently outperforms existing algorithms in both regret performance and
computational efficiency, providing the best of both theoretical soundness and
practical effectiveness.","['cs.LG', 'stat.ML']",False,,,,"Four-body coupler for superconducting qubits based on Josephson
  parametric oscillators",Minimax Optimal Reinforcement Learning with Quasi-Optimism
neg-d2-434,2025-02-22,,2502.16351," Neural radiance field (NeRF) research has made significant progress in
modeling static video content captured in the wild. However, current models and
rendering processes rarely consider scenes captured underwater, which are
useful for studying and filming ocean life. They fail to address visual
artifacts unique to underwater scenes, such as moving fish and suspended
particles. This paper introduces a novel NeRF renderer and optimization scheme
for an implicit MLP-based NeRF model. Our renderer reduces the influence of
floaters and moving objects that interfere with static objects of interest by
estimating a single surface per ray. We use a Gaussian weight function with a
small offset to ensure that the transmittance of the surrounding media remains
constant. Additionally, we enhance our model with a depth-based scaling
function to upscale gradients for near-camera volumes. Overall, our method
outperforms the baseline Nerfacto by approximately 7.5\% and SeaThru-NeRF by
6.2% in terms of PSNR. Subjective evaluation also shows a significant reduction
of artifacts while preserving details of static targets and background compared
to the state of the arts.",['cs.CV'],2501.09803," Estimating the shortest travel time and providing route recommendation
between different locations in a city or region can quantitatively measure the
conditions of the transportation network during or after extreme events. One
common approach is to use Dijkstra's Algorithm, which produces the shortest
path as well as the shortest distance. However, this option is computationally
expensive when applied to large-scale networks. This paper proposes a novel
fast framework based on graph neural networks (GNNs) which approximate the
single-source shortest distance between pairs of locations, and predict the
single-source shortest path subsequently. We conduct multiple experiments on
synthetic graphs of different size to demonstrate the feasibility and
computational efficiency of the proposed model. In real-world case studies, we
also applied the proposed method of flood risk analysis of coastal urban areas
to calculate delays in evacuation to public shelters during hurricanes. The
results indicate the accuracy and computational efficiency of the GNN model,
and its potential for effective implementation in emergency planning and
management.",['cs.LG'],False,,,,"AquaNeRF: Neural Radiance Fields in Underwater Media with Distractor
  Removal","Graph Neural Networks for Travel Distance Estimation and Route
  Recommendation Under Probabilistic Hazards"
neg-d2-435,2025-01-17,,2501.10327," We construct a mod $\ell$ congruence between a Klingen Eisenstein series
(associated to a classical newform $\phi$ of weight $k$) and a Siegel cusp form
$f$ with irreducible Galois representation. We use this congruence to show
non-vanishing of the Bloch-Kato Selmer group $H^1_f(\mathbf{Q},
\textrm{ad}^0\rho_{\phi}(2-k)\otimes \mathbf{Q}_{\ell}/\mathbf{Z}_{\ell})$
under certain assumptions and provide an example. We then prove an $R=dvr$
theorem for the Fontaine-Laffaille universal deformation ring of
$\overline{\rho}_f$ under some assumptions, in particular, that the residual
Selmer group $H^1_f(\mathbf{Q}, \textrm{ad}^0\overline{\rho}_{\phi}(k-2))$ is
cyclic. For this we prove a result about extensions of Fontaine-Laffaille
modules. We end by formulating conditions for when $H^1_f(\mathbf{Q},
\textrm{ad}^0\overline{\rho}_{\phi}(k-2))$ is non-cyclic and the Eisenstein
ideal is non-principal.",['math.NT'],2502.08768," In this paper a novel frequency-scalable rotary platform design is introduced
which allows for flexible directional channel measurements using different
types of antennas, and which can also be used with frequency extenders for
measurements up to the THz region. The measurement platform has been applied to
measure the channel properties including the direction of arrival at the FR3
frequency 14 GHz and in the D-band at 160 GHz in a large hall indoor
environment with LOS distances up to 40 m. The results show very good agreement
of strong path components for both frequencies as well as interesting
dependencies of delay spread, angular spread, and Ricean K- factor on distance
and frequency and can be used to parameterize a path loss model.",['eess.SP'],False,,,,Klingen Eisenstein series congruences and modularity,"Instantaneous directional channel measurements at 14 GHz and 160 GHz via
  a virtual circular array"
neg-d2-436,2025-03-19,,2503.14963," Multimodal contrastive learning (MCL) advances in aligning different
modalities and generating multimodal representations in a joint space. By
leveraging contrastive learning across diverse modalities, large-scale
multimodal data enhances representational quality. However, a critical yet
often overlooked challenge remains: multimodal data is rarely collected in a
single process, and training from scratch is computationally expensive.
Instead, emergent multimodal data can be used to optimize existing models
gradually, \textit{i.e.}, models are trained on a sequence of modality pair
data. We define this problem as Continual Multimodal Contrastive Learning
(CMCL), an underexplored yet crucial research direction at the intersection of
multimodal and continual learning. In this paper, we formulate CMCL through two
specialized principles of stability and plasticity. We theoretically derive a
novel optimization-based method, which projects updated gradients from dual
sides onto subspaces where any gradient is prevented from interfering with the
previously learned knowledge. Two upper bounds provide theoretical insights on
both stability and plasticity in our solution. Beyond our theoretical
contributions, we conduct experiments on multiple datasets by comparing our
method against advanced continual learning baselines. The empirical results
further support our claims and demonstrate the efficacy of our method. The code
will be publicly available.",['cs.LG'],2503.09731," We conduct two searches for continuous, nearly monochromatic gravitational
waves originating from the central compact objects in the supernova remnants
Cassiopeia A and Vela Jr. using public LIGO data. The search for Cassiopeia A
targets signal frequencies between 20 Hz and 400 Hz; the Vela Jr. search
between 400 Hz and 1700 Hz, and both investigate the broadest set of waveforms
ever considered with highly sensitive deterministic search methods. Above 1500
Hz the Vela Jr. search is the most sensitive carried out thus far, improving on
previous results by over 300\%. Above 976 Hz these results improve on existing
ones by 50\%. In all we investigate over $10^{18}$ waveforms, leveraging the
computational power donated by thousands of Einstein@Home volunteers. We
perform a 4-stage follow-up on more than 6 million waveforms. None of the
considered waveforms survives the follow-up scrutiny, indicating no significate
detection candidate. Our null results constrain the maximum amplitude of
continuous signals as a function of signal frequency from the targets. The most
stringent 90\% confidence upper limit for Cas A is $h_0^{90 \%}\approx
7.3\times10^{-26}$ near 200 Hz, and for Vela Jr. it is $h_0^{90 \%}\approx
8.9\times10^{-26}$ near 400 Hz. Translated into upper limits on the ellipticity
and r-mode amplitude, our results probe physically interesting regions: for
example the ellipticity of Vela Jr. is constrained to be smaller than $10^{-7}$
across the frequency band, with a tighter constraint of less than
$2\times10^{-8}$ at the highest frequencies.","['gr-qc', 'hep-ph']",False,,,,Continual Multimodal Contrastive Learning,"Results from an Einstein@Home search for continuous gravitational waves
  from Cassiopeia A and Vela Jr. using LIGO O2 data"
neg-d2-437,2025-02-22,,2502.16326," Background: Multi-collimator proton minibeam radiation therapy (MC-pMBRT) has
recently emerged as a versatile technique for dose shaping, enabling
peak-valley dose patterns in organs-at-risk (OAR) while maintaining a uniform
dose distribution in tumor. MC-pMBRT leverages a set of generic multi-slit
collimators (MSC) with varying center-to-center distances. However, the current
method for minibeam aperture optimization (MAO), i.e., the selection of MSC per
beam angle, is manual and heuristic, resulting in computational inefficiencies
and no guarantee of optimality. This work introduces a novel mixed integer
programming (MIP) approach to MAO for optimizing MC-pMBRT plan quality.
Methods: The proposed MIP approach jointly optimizes dose distributions,
peak-to-valley dose ratio (PVDR), and selects the optimal set of MSC per beam
angle. The optimization problem includes decision variables for MSC selection
per beam angle and spot weights. The proposed MIP approach is a two-step
process: Step1: the binary variables are optimally determined to select MSC for
each beam angle; Step 2: the continuous variables are solved to determine the
spot weights. Both steps utilize iterative convex relaxation and the
alternating direction method of multipliers to solve the problems. Results: The
proposed MIP method for MAO (MIP-MAO) was validated against the conventional
heuristic method (CONV) for MC-pMBRT treatment planning. Results indicate that
MIP-MAO enhances the conformity index (CI) for the target and improves PVDR for
OAR. For instance, in a head-and-neck case, CI improved from 0.61 (CONV) to
0.70 (MIP-MAO); in an abdomen case, CI improved from 0.78 (CONV) to 0.83
(MIP-MAO). Additionally, MIP-MAO reduced mean doses in the body and OAR.
Conclusions: A novel MIP approach for MAO in MC-pMBRT is presented, showing
demonstrated improvements in plan quality and PVDR compared to the heuristic
method.",['physics.med-ph'],2501.01838," The kagome lattice has garnered significant attention due to its ability to
host quantum spin Fermi liquid states. Recently, the combination of unique
lattice geometry, electron-electron correlations, and adjustable magnetism in
solid kagome materials has led to the discovery of numerous fascinating quantum
properties. These include unconventional superconductivity, charge and spin
density waves (CDW/SDW), pair density waves (PDW), and Chern insulator phases.
These emergent states are closely associated with the distinctive
characteristics of the kagome lattice's electronic structure, such as van Hove
singularities, Dirac fermions, and flat bands, which can exhibit exotic
quasi-particle excitations under different symmetries and magnetic conditions.
Recently, various quantum kagome materials have been developed, typically
consisting of kagome layers stacked along the $z$-axis with atoms either
filling the geometric centers of the kagome lattice or embedded between the
layers. In this topical review, we begin by introducing the fundamental
properties of several kagome materials. To gain an in-depth understanding of
the relationship between topology and correlation, we then discuss the complex
phenomena observed in these systems. These include the simplest kagome metal
$T_3X$, kagome intercalation metal $TX$, and the ternary compounds $AT_6X_6$
and $RT_3X_5$ ($A$ = Li, Mg, Ca, or rare earth; $T$ = V, Cr, Mn, Fe, Co, Ni;
$X$ = Sn, Ge; $R$ = K, Rb, Cs). Finally, we provide a perspective on future
experimental work in this field.","['cond-mat.str-el', 'cond-mat.mtrl-sci', 'cond-mat.supr-con']",False,,,,"A mixed integer programming approach to minibeam aperture optimization
  for multi-collimator proton minibeam radiotherapy",Electronic band structures of topological kagome materials
neg-d2-438,2025-02-26,,2502.1894," Evaluating the pedagogical capabilities of AI-based tutoring models is
critical for making guided progress in the field. Yet, we lack a reliable,
easy-to-use, and simple-to-run evaluation that reflects the pedagogical
abilities of models. To fill this gap, we present MathTutorBench, an
open-source benchmark for holistic tutoring model evaluation. MathTutorBench
contains a collection of datasets and metrics that broadly cover tutor
abilities as defined by learning sciences research in dialog-based teaching. To
score the pedagogical quality of open-ended teacher responses, we train a
reward model and show it can discriminate expert from novice teacher responses
with high accuracy. We evaluate a wide set of closed- and open-weight models on
MathTutorBench and find that subject expertise, indicated by solving ability,
does not immediately translate to good teaching. Rather, pedagogy and subject
expertise appear to form a trade-off that is navigated by the degree of
tutoring specialization of the model. Furthermore, tutoring appears to become
more challenging in longer dialogs, where simpler questioning strategies begin
to fail. We release the benchmark, code, and leaderboard openly to enable rapid
benchmarking of future models.","['cs.CL', 'cs.AI', 'cs.LG']",2501.07515," Evolutionary and bioinspired computation are crucial for efficiently
addressing complex optimization problems across diverse application domains. By
mimicking processes observed in nature, like evolution itself, these algorithms
offer innovative solutions beyond the reach of traditional optimization
methods. They excel at finding near-optimal solutions in large, complex search
spaces, making them invaluable in numerous fields. However, both areas are
plagued by challenges at their core, including inadequate benchmarking,
problem-specific overfitting, insufficient theoretical grounding, and
superfluous proposals justified only by their biological metaphor. This
overview recapitulates and analyzes in depth the criticisms concerning the lack
of innovation and rigor in experimental studies within the field. To this end,
we examine the judgmental positions of the existing literature in an informed
attempt to guide the research community toward directions of solid contribution
and advancement in these areas. We summarize guidelines for the design of
evolutionary and bioinspired optimizers, the development of experimental
comparisons, and the derivation of novel proposals that take a step further in
the field. We provide a brief note on automating the process of creating these
algorithms, which may help align metaheuristic optimization research with its
primary objective (solving real-world problems), provided that our identified
pathways are followed. Our conclusions underscore the need for a sustained push
towards innovation and the enforcement of methodological rigor in prospective
studies to fully realize the potential of these advanced computational
techniques.","['cs.NE', 'cs.AI']",False,,,,"MathTutorBench: A Benchmark for Measuring Open-ended Pedagogical
  Capabilities of LLM Tutors","The Paradox of Success in Evolutionary and Bioinspired Optimization:
  Revisiting Critical Issues, Key Studies, and Methodological Pathways"
neg-d2-439,2025-01-16,,2501.09367," Large language models (LLMs), while driving a new wave of interactive AI
applications across numerous domains, suffer from high inference costs and
heavy cloud dependency. Motivated by the redundancy phenomenon in linguistics,
we propose a progressive inference paradigm over cloud and edge, i.e., firstly
generating the sketch of the answer by LLMs at cloud, and then conducting
parallel extension to fill in details by small models (SLMs) at edge.
Progressive inference offers potential benefits to improve throughput and
reduce inference latency while facing key implementation challenges, including
decreased response quality from SLMs, a tradeoff between the brevity and
comprehensiveness of sketches, as well as increased latency caused by network
transmission and edge inference. In this work, we propose and implement PICE,
an LLM serving system with semantic-level cloud-edge collaboration, enhancing
inference throughput and quality through dynamic inference task scheduling,
ensemble learning, and parallel edge inference. Extensive testbed experiments
illustrate that our approach achieves $1.5-2\times$ throughput enhancement and
up to 43% latency reduction, while also potentially enhancing the quality
compared to SOTA systems.",['cs.DC'],2501.18627," We present a fast and simple technique to convert images into an emissive
surface-based scene representation. Building on existing emissive volume
reconstruction algorithms, we introduce a subtle yet impactful modification of
the loss function requiring changes to only a few lines of code: instead of
integrating the radiance field along rays and supervising the resulting images,
we project the training images into the scene to directly supervise the
spatio-directional radiance field.
  The primary outcome of this change is the complete removal of alpha blending
and ray marching from the image formation model, instead moving these steps
into the loss computation. In addition to promoting convergence to surfaces,
this formulation assigns explicit semantic meaning to 2D subsets of the
radiance field, turning them into well-defined emissive surfaces. We finally
extract a level set from this representation, which results in a high-quality
emissive surface model.
  Our method retains much of the speed and quality of the baseline algorithm.
For instance, a suitably modified variant of Instant~NGP maintains comparable
computational efficiency, while achieving an average PSNR that is only 0.1 dB
lower. Most importantly, our method generates explicit surfaces in place of an
exponential volume, doing so with a level of simplicity not seen in prior work.","['cs.GR', 'cs.CV']",False,,,,"PICE: A Semantic-Driven Progressive Inference System for LLM Serving in
  Cloud-Edge Networks","A Radiance Field Loss for Fast and Simple Emissive Surface
  Reconstruction"
neg-d2-440,2025-03-07,,2503.0591," The current method for forensic analysis of bullet comparison relies on
manual examination by forensic examiners to determine if bullets were
discharged from the same firearm. This process is highly subjective, prompting
the development of algorithmic methods to provide objective statistical support
for comparisons. However, a gap exists between the technical understanding of
these algorithms and the typical background of many forensic examiners. We
present a visualization tool designed to bridge this gap, allowing for the
presentation of statistical information in a more familiar format to forensic
professionals. The forensic bullet comparison visualizer (FBCV) features a
variety of plots that will enable the user to examine every step of the
algorithmic comparison process. We demonstrate the utility of the FBCV by
applying it to data from the Houston Science Lab, where it helped identify an
error in the comparison process caused by mislabeling. This tool can be used
for future investigations, such as examining how distance between shots affects
scores. The FBCV offers a user-friendly way to convey complex statistical
information to forensic examiners, facilitating their understanding and
utilization of algorithmic comparison methods.",['stat.AP'],2501.04483," The Ethereum blockchain has a \emph{gas system} that associates operations
with a cost in gas units. Two central concepts of this system are the \emph{gas
limit} assigned by the issuer of a transaction and the \emph{gas used} by a
transaction. The former is a budget that must not be exhausted before the
completion of the transaction execution; otherwise, the execution fails.
Therefore, it seems rather essential to determine the \emph{minimum gas limit}
that ensures the execution of a transaction will not abort due to the lack of
gas. Despite its practical relevance, this concept has not been properly
addressed. In the literature, gas used and minimum gas limit are conflated.
This paper proposes a precise notion of minimum gas limit and how it can differ
from gas used by a transaction; this is also demonstrated with a quantitative
study on real transactions of the Ethereum blockchain. Another significant
contribution is the proposition of a fairly precise estimator for each of the
two metrics. Again, the confusion between these concepts has led to the
creation of estimators only for the gas used by a transaction. We demonstrate
that the minimum gas limit for the state of the Ethereum blockchain (after the
block) $t$ can serve as a near-perfect estimation for the execution of the
transaction at block $t + \Delta$, where $\Delta \leq 11$; the same holds for
estimating gas used. These precise estimators can be very valuable in helping
the users predict the gas budget of transactions and developers in optimising
their smart contracts; over and underestimating gas used and minimum gas limit
can lead to a number of practical issues. Overall, this paper serves as an
important reference for blockchain developers and users as to how the gas
system really works.","['cs.SE', 'cs.CE', 'cs.DC', 'cs.ET', 'cs.NI']",False,,,,Interactive Visualization Framework for Forensic Bullet Comparisons,"Demystification and Near-perfect Estimation of Minimum Gas Limit and Gas
  Used for Ethereum Smart Contracts"
neg-d2-441,2025-03-10,,2503.06953," Autonomous and targeted underwater visual monitoring and exploration using
Autonomous Underwater Vehicles (AUVs) can be a challenging task due to both
online and offline constraints. The online constraints comprise limited onboard
storage capacity and communication bandwidth to the surface, whereas the
offline constraints entail the time and effort required for the selection of
desired key frames from the video data. An example use case of targeted
underwater visual monitoring is finding the most interesting visual frames of
fish in a long sequence of an AUV's visual experience. This challenge of
targeted informative sampling is further aggravated in murky waters with poor
visibility. In this paper, we present MERLION, a novel framework that provides
semantically aligned and visually enhanced summaries for murky underwater
marine environment monitoring and exploration. Specifically, our framework
integrates (a) an image-text model for semantically aligning the visual samples
to the users' needs, (b) an image enhancement model for murky water visual data
and (c) an informative sampler for summarizing the monitoring experience. We
validate our proposed MERLION framework on real-world data with user studies
and present qualitative and quantitative results using our evaluation metric
and show improved results compared to the state-of-the-art approaches. We have
open-sourced the code for MERLION at the following link
https://github.com/MARVL-Lab/MERLION.git.",['cs.RO'],2503.04313," Infinitesimals have seen ups and downs in their tumultuous history. In the
18th century, d'Alembert set the tone by describing infinitesimals as chimeras.
Some adversaries of infinitesimals, including Moigno and Connes, picked up on
the term. We highlight the work of Cauchy, No\""el, Poisson and Riemann. We also
chronicle reactions by Moigno, Lamarle and Cantor, and signal the start of a
revival with Peano.",['math.HO'],False,,,,"MERLION: Marine ExploRation with Language guIded Online iNformative
  Visual Sampling and Enhancement",Episodes from the history of infinitesimals
neg-d2-442,2025-02-02,,2502.00858," Effective integration of AI agents into daily life requires them to
understand and adapt to individual human preferences, particularly in
collaborative roles. Although recent studies on embodied intelligence have
advanced significantly, they typically adopt generalized approaches that
overlook personal preferences in planning. We address this limitation by
developing agents that not only learn preferences from few demonstrations but
also learn to adapt their planning strategies based on these preferences. Our
research leverages the observation that preferences, though implicitly
expressed through minimal demonstrations, can generalize across diverse
planning scenarios. To systematically evaluate this hypothesis, we introduce
Preference-based Planning (PbP) benchmark, an embodied benchmark featuring
hundreds of diverse preferences spanning from atomic actions to complex
sequences. Our evaluation of SOTA methods reveals that while symbol-based
approaches show promise in scalability, significant challenges remain in
learning to generate and execute plans that satisfy personalized preferences.
We further demonstrate that incorporating learned preferences as intermediate
representations in planning significantly improves the agent's ability to
construct personalized plans. These findings establish preferences as a
valuable abstraction layer for adaptive planning, opening new directions for
research in preference-guided plan generation and execution.","['cs.AI', 'cs.HC']",2501.16637," In this note, we review the latest qualitative results, referring to the
Li\'enard Equation, in the framework of non-conformable, generalized and
fractional differential operators.",['math.GM'],False,,,,Learning to Plan with Personalized Preferences,On the Li\'enard's type equation: an icon of the Nonlinear Analysis
neg-d2-443,2025-01-24,,2501.14369," Research on continual learning in multi-modal tasks has been receiving
increasing attention. However, most existing work overlooks the explicit
cross-modal and cross-task interactions. In this paper, we innovatively propose
the Low-rank Prompt Interaction (LPI) to address this general problem of
multi-modal understanding, which considers both cross-modal and cross-task
interactions. Specifically, as for the former, we employ multi-modal
correlation modules for corresponding Transformer layers. Considering that the
training parameters scale to the number of layers and tasks, we propose
low-rank interaction-augmented decomposition to avoid memory explosion while
enhancing the cross-modal association through sharing and separating
common-specific low-rank factors. In addition, due to the multi-modal semantic
differences carried by the low-rank initialization, we adopt hierarchical
low-rank contrastive learning to ensure training robustness. As for the latter,
we initially employ a visual analysis and identify that different tasks have
clear distinctions in proximity. Therefore, we introduce explicit task
contrastive constraints in the prompt learning process based on task semantic
distances. Experiments on two retrieval tasks show performance improvements
with the introduction of a minimal number of parameters, demonstrating the
effectiveness of our method. Code is available at
https://github.com/Kelvin-ywc/LPI.",['cs.CV'],2501.13297," Multi-modal retrieval-augmented Question Answering (MRAQA), integrating text
and images, has gained significant attention in information retrieval (IR) and
natural language processing (NLP). Traditional ranking methods rely on small
encoder-based language models, which are incompatible with modern decoder-based
generative large language models (LLMs) that have advanced various NLP tasks.
To bridge this gap, we propose RAMQA, a unified framework combining
learning-to-rank methods with generative permutation-enhanced ranking
techniques. We first train a pointwise multi-modal ranker using LLaVA as the
backbone. Then, we apply instruction tuning to train a LLaMA model for
re-ranking the top-k documents using an innovative autoregressive multi-task
learning approach. Our generative ranking model generates re-ranked document
IDs and specific answers from document candidates in various permutations.
Experiments on two MRAQA benchmarks, WebQA and MultiModalQA, show significant
improvements over strong baselines, highlighting the effectiveness of our
approach. Code and data are available at: https://github.com/TonyBY/RAMQA","['cs.CL', 'cs.AI', 'cs.IR', 'cs.LG']",False,,,,Low-rank Prompt Interaction for Continual Vision-Language Retrieval,"RAMQA: A Unified Framework for Retrieval-Augmented Multi-Modal Question
  Answering"
neg-d2-444,2025-03-17,,2503.12791," The occurrence of a topological phase transition can be demonstrated by a
direct observation of a change in the topological invariant. For holographic
topological semimetals, a topological Hamiltonian method needs to be employed
to calculate the topological invariants due to the strong coupling nature of
the system. We calculate the topological invariants for the holographic Weyl
semimetal and the holographic Weyl-$\mathrm Z_2$ semimetal, which correspond to
the chiral charge and the spin-Chern number, respectively. This is achieved by
probing fermions within the system and deriving the topological Hamiltonian
from the zero-frequency Green's function. In both cases, we have identified an
effective band structure characterized by an infinite number of Weyl or
$\mathrm Z_2$ nodes, a distinctive feature of holographic systems different
from weakly coupled systems. The topological invariants of these nodes are
computed numerically and found to be nonzero, thereby confirming the
topologically nontrivial nature of these nodes.",['hep-th'],2502.09434," Diffusion models, known for their tremendous ability to generate high-quality
samples, have recently raised concerns due to their data memorization behavior,
which poses privacy risks. Recent methods for memory mitigation have primarily
addressed the issue within the context of the text modality in cross-modal
generation tasks, restricting their applicability to specific conditions. In
this paper, we propose a novel method for diffusion models from the perspective
of visual modality, which is more generic and fundamental for mitigating
memorization. Directly exposing visual data to the model increases memorization
risk, so we design a framework where models learn through proxy model
parameters instead. Specially, the training dataset is divided into multiple
shards, with each shard training a proxy model, then aggregated to form the
final model. Additionally, practical analysis of training losses illustrates
that the losses for easily memorable images tend to be obviously lower. Thus,
we skip the samples with abnormally low loss values from the current mini-batch
to avoid memorizing. However, balancing the need to skip memorization-prone
samples while maintaining sufficient training data for high-quality image
generation presents a key challenge. Thus, we propose IET-AGC+, which
redistributes highly memorizable samples between shards, to mitigate these
samples from over-skipping. Furthermore, we dynamically augment samples based
on their loss values to further reduce memorization. Extensive experiments and
analysis on four datasets show that our method successfully reduces memory
capacity while maintaining performance. Moreover, we fine-tune the pre-trained
diffusion models, e.g., Stable Diffusion, and decrease the memorization score
by 46.7\%, demonstrating the effectiveness of our method. Code is available in:
https://github.com/liuxiao-guan/IET_AGC.",['cs.CV'],False,,,,Topological invariant for holographic Weyl-$\mathrm Z_2$ semimetal,"Redistribute Ensemble Training for Mitigating Memorization in Diffusion
  Models"
neg-d2-445,2025-03-06,,2503.04312," The magneto-optical effects (MOEs), as a fundamental physical phenomenon, can
reveal the electronic structures of materials. The related probing methods are
widely used in the study of magnetic materials. However, space-time inversion
($\mathcal{PT}$) symmetric antiferromagnets were previously believed to be
magneto-optically inactive. Here, we point out that this traditional
understanding is incorrect. Based on our generic formulas and symmetry
analysis, we find that in $\mathcal{PT}$-symmetric antiferromagnets, it is the
quantum metric, i.e., the real part of the quantum geometry, that induces MOEs.
Combining a tight-binding model and first-principles calculations, we confirm
this observation by showing MOEs in the $\mathcal{PT}$-symmetric
antiferromagnet. Our work demonstrates that $\mathcal{PT}$-symmetric
antiferromagnets previously thought to lack MOEs can indeed exhibit MOEs and
greatly broaden the research on MOEs.","['cond-mat.mtrl-sci', 'cond-mat.mes-hall', 'physics.comp-ph']",2503.12751," We present R3-Avatar, incorporating a temporal codebook, to overcome the
inability of human avatars to be both animatable and of high-fidelity rendering
quality. Existing video-based reconstruction of 3D human avatars either focuses
solely on rendering, lacking animation support, or learns a pose-appearance
mapping for animating, which degrades under limited training poses or complex
clothing. In this paper, we adopt a ""record-retrieve-reconstruct"" strategy that
ensures high-quality rendering from novel views while mitigating degradation in
novel poses. Specifically, disambiguating timestamps record temporal appearance
variations in a codebook, ensuring high-fidelity novel-view rendering, while
novel poses retrieve corresponding timestamps by matching the most similar
training poses for augmented appearance. Our R3-Avatar outperforms cutting-edge
video-based human avatar reconstruction, particularly in overcoming visual
quality degradation in extreme scenarios with limited training human poses and
complex clothing.",['cs.CV'],False,,,,"Quantum metric induced magneto-optical effects in
  $\mathcal{PT}$-symmetric antiferromagnets","R3-Avatar: Record and Retrieve Temporal Codebook for Reconstructing
  Photorealistic Human Avatars"
neg-d2-446,2025-02-11,,2502.0755," Near the center of our Milky Way is a bar-like structure and the so-called
Expanding 3-kpc arms. We currently have limited knowledge of this important
region, since we are about 8.2 kpc from the center and cannot directly observe
it at optical wavelengths, owing to strong extinction from interstellar dust.
Here we present extremely precise VLBI measurements of water maser sources from
the BeSSeL Survey, where extinction is not a problem, which accurately
determine the 3-dimensional locations and motions of three massive young stars.
Combined with previous measurements, these stars delineate a trail of orbits
outlining the Milky Way's Galactic Bar. We present the first measurements
capturing the dynamics of quasi-elliptical (X1) orbits around the Galactic Bar.
Our findings provide evidence substantiating the existence of such orbits
populated by massive young stars. Our measurements of the position and velocity
of a number of massive young stars, previously identified with the Expanding
3-kpc arms, show that they are more likely located in the X1 orbits about the
Galactic Bar. Also, some stars previously assigned to the Norma spiral arm
appear to be in these orbits, which suggests that this spiral arm does not
extend past the end of the bar.",['astro-ph.GA'],2502.19921," Deep learning models lack shift invariance, making them sensitive to input
shifts that cause changes in output. While recent techniques seek to address
this for images, our findings show that these approaches fail to provide
shift-invariance in time series, where the data generation mechanism is more
challenging due to the interaction of low and high frequencies. Worse, they
also decrease performance across several tasks. In this paper, we propose a
novel differentiable bijective function that maps samples from their
high-dimensional data manifold to another manifold of the same dimension,
without any dimensional reduction. Our approach guarantees that samples -- when
subjected to random shifts -- are mapped to a unique point in the manifold
while preserving all task-relevant information without loss. We theoretically
and empirically demonstrate that the proposed transformation guarantees
shift-invariance in deep learning models without imposing any limits to the
shift. Our experiments on six time series tasks with state-of-the-art methods
show that our approach consistently improves the performance while enabling
models to achieve complete shift-invariance without modifying or imposing
restrictions on the model's topology. The source code is available on
\href{https://github.com/eth-siplab/Shifting-the-Paradigm}{GitHub}.",['cs.LG'],False,,,,"The Expanding 3-Kiloparsec Arms are neither Expanding nor Spiral Arms,
  but X1 Orbits driven by the Galactic Bar","Shifting the Paradigm: A Diffeomorphism Between Time Series Data
  Manifolds for Achieving Shift-Invariancy in Deep Learning"
neg-d2-447,2025-01-07,,2501.03818," We study the Dirichlet dynamical zeta function $\eta_D(s)$ for billiard flow
corresponding to several strictly convex disjoint obstacles. For large ${\rm
Re}\: s$ we have $\eta_D(s) =\sum_{n= 1}^{\infty} a_n e^{-\lambda_n s}, \: a_n
\in \mathbb R$ and $\eta_D$ admits a meromorphic continuation to $\mathbb C$.
We obtain some conditions of the frequencies $\lambda_n$ and some sums of
coefficients $a_n$ which imply that $\eta_D$ cannot be prolonged as entire
function.","['math.DS', 'math.NT']",2502.18157," Snow avalanches present significant risks to human life and infrastructure,
particularly in mountainous regions, making effective monitoring crucial.
Traditional monitoring methods, such as field observations, are limited by
accessibility, weather conditions, and cost. Satellite-borne Synthetic Aperture
Radar (SAR) data has become an important tool for large-scale avalanche
detection, as it can capture data in all weather conditions and across remote
areas. However, traditional processing methods struggle with the complexity and
variability of avalanches. This chapter reviews the application of deep
learning for detecting and segmenting snow avalanches from SAR data. Early
efforts focused on the binary classification of SAR images, while recent
advances have enabled pixel-level segmentation, providing greater accuracy and
spatial resolution. A case study using Sentinel-1 SAR data demonstrates the
effectiveness of deep learning models for avalanche segmentation, achieving
superior results over traditional methods. We also present an extension of this
work, testing recent state-of-the-art segmentation architectures on an expanded
dataset of over 4,500 annotated SAR images. The best-performing model among
those tested was applied for large-scale avalanche detection across the whole
of Norway, revealing important spatial and temporal patterns over several
winter seasons.","['cs.CV', 'cs.AI', 'eess.IV']",False,,,,Dirichlet dynamical zeta function for billiard flow,Monitoring snow avalanches from SAR data with deep learning
neg-d2-448,2025-01-15,,2501.09231," Quantum mechanical invariance principles dictate the most general operator
structure that can be present in the nucleon-nucleon (NN) interaction. Five
independent operators appear in the on-shell NN amplitude together with five
corresponding coefficient functions. The usual choice for these coefficient
functions is known as the NN Wolfenstein amplitudes. We analyze the
order-by-order convergence of each of the five NN Wolfenstein amplitudes
predicted by a semi-local coordinate space potential implementation of chiral
effective field theory ($\chi$EFT). We do this at laboratory kinetic energies
between 25 and 200 MeV for both neutron-proton and proton-proton scattering.
Our analysis uses the Gaussian-Process methods developed by the BUQEYE
collaboration to describe the contributions of each $\chi$EFT order, and so
yields truncation uncertainties for each Wolfenstein amplitude that are
correlated across scattering angles. We combine information on the size of
different orders in the EFT to infer the $\chi$EFT breakdown scale for each
amplitude, finding, on average, $\Lambda_b$ between 750 and 800 MeV. With this
choice of $\Lambda_b$, the EFT truncation uncertainties cover both higher-order
results and empirical Wolfenstein amplitudes well for all orders other than the
leading order.","['nucl-th', 'nucl-ex']",2503.08892," Methods for computing the integral of the Planck blackbody function over a
finite spectral range, the so-called incomplete Planck integral, are necessary
to perform multigroup radiative transfer calculations. We present a comparison,
in terms of speed and accuracy, of a wide array of approaches to numerically
evaluating these integrals. Our results indicate that a direct rational
polynomial approximation to these integrals has the best combination of
accuracy and efficiency. We also present for the first time a derivation of the
polylogarithm form of these integrals and show that modern approaches to
polylogarithm evaluation are suitable for numerically evaluating incomplete
Planck integrals. This article is dedicated to Prof. B.D. Ganapol, the
Transport Cowboy, on the occasion of his retirement.",['physics.comp-ph'],False,,,,"Order-by-order uncertainties of nucleon-nucleon Wolfenstein amplitudes
  in chiral effective field theory","Fast, Accurate Numerical Evaluation of Incomplete Planck Integrals"
neg-d2-449,2025-02-15,,2503.00006," We define various type of states on implicative involutive BE algebras
(Jauch-Piron state, (P)-state, (B)-state, subadditive state, valuation), and we
investigate the relationships between these states. Moreover, we introduce the
unital, full and rich sets of states, and we prove certain properties involving
these notions. In the case when an implicative involutive BE algebra possesses
a rich or a full set of states, we prove that it is an implicative-orthomodular
lattice. If an implicative involutive BE algebra possesses a rich set of
(P)-states or a full set of valuations, then it is an implicative-Boolean
algebra. Additionally, based on their deductive systems, we give
characterizations of implicative-orthomodular lattices and implicative-Boolean
algebras.",['math.QA'],2501.02817," Given a pair of time series, we study how the periodicity of one influences
the periodicity of the other. There are several known methods to measure the
similarity between a pair of time series, such as cross-correlation, coherence,
cross-recurrence, and dynamic time warping. But we have yet to find any
measures with theoretical stability results.
  Persistence homology has been utilized to construct a scoring function with
theoretical guarantees of stability that quantifies the periodicity of a single
univariate time series f1, denoted score(f1). Building on this concept, we
propose a conditional periodicity score that quantifies the periodicity of one
univariate time series f1 given another f2, denoted score(f1|f2), and derive
theoretical stability results for the same. With the use of dimension reduction
in mind, we prove a new stability result for score(f1|f2) under principal
component analysis (PCA) when we use the projections of the time series
embeddings onto their respective first K principal components. We show that the
change in our score is bounded by a function of the eigenvalues corresponding
to the remaining (unused) N-K principal components and hence is small when the
first K principal components capture most of the variation in the time series
embeddings. Finally we derive a lower bound on the minimum embedding dimension
to use in our pipeline which guarantees that any two such embeddings give
scores that are within a given epsilon of each other.
  We present a procedure for computing conditional periodicity scores and
implement it on several pairs of synthetic signals. We experimentally compare
our similarity measure to the most-similar statistical measure of
cross-recurrence, and show the increased accuracy and stability of our score
when predicting and measuring whether or not the periodicities of two time
series are similar.","['math.AT', 'math.ST', 'stat.ML', 'stat.TH']",False,,,,Classification of states on certain orthomodular structures,"A Stable Measure for Conditional Periodicity of Time Series using
  Persistent Homology"
neg-d2-450,2025-03-19,,2503.15213," Automatic non-cooperative analysis of intercepted radar signals is essential
for intelligent equipment in both military and civilian domains. Accurate
modulation identification and parameter estimation enable effective signal
classification, threat assessment, and the development of countermeasures. In
this paper, we propose a symbolic approach for radar signal recognition and
parameter estimation based on a vision-language model that combines
context-free grammar with time-frequency representation of radar waveforms. The
proposed model, called Sig2text, leverages the power of vision transformers for
time-frequency feature extraction and transformer-based decoders for symbolic
parsing of radar waveforms. By treating radar signal recognition as a parsing
problem, Sig2text can effectively recognize and parse radar waveforms with
different modulation types and parameters. We evaluate the performance of
Sig2text on a synthetic radar signal dataset and demonstrate its effectiveness
in recognizing and parsing radar waveforms with varying modulation types and
parameters. The training code of the model is available at
https://github.com/Na-choneko/sig2text.",['eess.SP'],2501.0387," Slot and intent detection (SID) is a classic natural language understanding
task. Despite this, research has only more recently begun focusing on SID for
dialectal and colloquial varieties. Many approaches for low-resource scenarios
have not yet been applied to dialectal SID data, or compared to each other on
the same datasets. We participate in the VarDial 2025 shared task on slot and
intent detection in Norwegian varieties, and compare multiple set-ups: varying
the training data (English, Norwegian, or dialectal Norwegian), injecting
character-level noise, training on auxiliary tasks, and applying Layer
Swapping, a technique in which layers of models fine-tuned on different
datasets are assembled into a model. We find noise injection to be beneficial
while the effects of auxiliary tasks are mixed. Though some experimentation was
required to successfully assemble a model from layers, it worked surprisingly
well; a combination of models trained on English and small amounts of dialectal
data produced the most robust slot predictions. Our best models achieve 97.6%
intent accuracy and 85.6% slot F1 in the shared task.",['cs.CL'],False,,,,"Sig2text, a Vision-language model for Non-cooperative Radar Signal
  Parsing","Add Noise, Tasks, or Layers? MaiNLP at the VarDial 2025 Shared Task on
  Norwegian Dialectal Slot and Intent Detection"
neg-d2-451,2025-03-10,,2503.0748," We investigate the dynamics of small inertial particles in a two-dimensional,
steady Taylor-Green vortex flow. A classic study by Taylor (2022) showed that
heavy inertial point particles (having density parameter R = 1) are trapped by
the flow separatrices when the particle Stokes number St, which measures the
particle's inertia, is less than 1/4. Here, we consider finitely dense
particles, incorporating the previously neglected effects of added mass and the
Boussinesq-Basset history force. Using linear stability analysis near
stagnation points, we determine the critical parametric conditions in the St-R
plane that leads to particle trapping within vortex cells. We identify
additional stagnation points perceived by inertial particles, beyond the
traditional ones at vortex cell corners, when the added mass effect is
included, and we analyze their stability. Numerical analysis of the full
nonlinear system confirms the existence of distinct particle
behaviours--trapped, diffusive, and ballistic--depending on initial conditions,
consistent with Nath et al. (2024), with modifications due to added mass
effect. We delineate the regions in the St-R plane where these behaviours
dominate based on the prominent particle dynamics. However, when both the
history force and added mass effect are included, all particles exhibit
ballistic motion regardless of St and R.",['physics.flu-dyn'],2501.12432," Although current Large Language Models (LLMs) exhibit impressive
capabilities, performing complex real-world tasks still requires tool learning.
Mainstream methods, such as CoT/ReAct, rely on step-by-step tool invocation to
interact with external environments, but they are limited in perceptual scope
and lack adequate task-planning capability. To address these limitations, other
studies introduce the first Search-based Decision Tree (DFSDT), which still
suffers from the high computational cost. In this paper, we introduce a novel
parallel tool invocation paradigm, DTA-Llama (Divide-Then-Aggregate Llama).
First, we transform traditional tree-based tool search paths into Directed
Acyclic Graph (DAG) structure, generating a high-quality parallel tool
invocation dataset. The DTA-Llama is then trained on the dataset to learn to
iteratively divide the current task into several parallel tool invocation
sub-tasks and aggregate the invocation results to decide the next actions.
Furthermore, we introduce an efficient inference framework inspired by the
Process/Threads mechanism when applying the DTA-Llama to practical tasks.
Experimental results show that our approach substantially enhances task
performance while reducing token consumption and inference time. Llama2-7B,
using our method, is comparable to the official parallel function calling
method of GPT-3.5. The relevant code, dataset, and model weights are available
at https://corn0205.github.io/","['cs.LG', 'cs.AI', 'cs.CL']",False,,,,"Trapping and Transport of Inertial Particles in a Taylor-Green Vortex:
  Effects of Added Mass and History Force","Divide-Then-Aggregate: An Efficient Tool Learning Method via Parallel
  Tool Invocation"
neg-d2-452,2025-03-18,,2503.14206," In this paper, we study the linear stability of Couette flow for 2D
compressible Navier-Stokes-Poisson system at high Reynolds number in the domain
$\mathbb{T}\times\mathbb{R}$ with initial perturbation in Sobolev spaces. We
establish the upper bounds for the solutions of linearized system near Couette
flow. In particular, we show that the irrotational component of the
perturbation may have a transient growth, after which it decays exponentially.",['math.AP'],2503.14077," The radiative open circuit voltage loss in a solar cell occurs because the
absorptance spectrum near the band gap shows gradual increase rather than sharp
step function like transition. This broadening effect has been attributed to
band gap fluctuations and or to Urbach tails. In this report, we use modelling
based on Planck s generalized law to distinguish between these two effects. Our
results demonstrate that Urbach tails have only a minimal effect on the
absorptance edge broadening and clarify that even an ideal direct semiconductor
with no band gap fluctuations shows broadening at the absorptance onset.
Furthermore, state of the art inorganic thin film solar cells often incorporate
a band gap gradient across their thickness, which can further contribute to
absorptance broadening. Using Cu(In,Ga)Se2 (CIGSe) absorbers as a case study,
we perform a comprehensive analysis of voltage losses through absolute
photoluminescence and electroluminescence spectroscopy, combined with
photospectrometry and high-spatial-resolution cathodoluminescence measurements.
We find that the loss analysis based on the combination of radiative,
generation and non-radiative losses is complete. Samples with a graded band gap
profile show more pronounced broadening of the absorptance onset and up to 16
mV higher radiative losses compared to the samples with uniform band gap. There
is indication, that band gap-graded samples also have larger lateral band gap
inhomogeneity.",['cond-mat.mtrl-sci'],False,,,,"Linear stability analysis of the Couette flow for 2D compressible
  Navier-Stokes-Poisson system","The effect of a band gap gradient on the radiative losses in the open
  circuit voltage of solar cells"
neg-d2-453,2025-03-04,,2503.02324," The ability of large language models to solve complex mathematical problems
has progressed significantly, particularly for tasks requiring advanced
reasoning. However, the scarcity of sufficiently challenging problems,
particularly at the Olympiad level, hinders further advancements. In this work,
we introduce PromptCoT, a novel approach for automatically generating
high-quality Olympiad-level math problems. The proposed method synthesizes
complex problems based on mathematical concepts and the rationale behind
problem construction, emulating the thought processes of experienced problem
designers. We provide a theoretical analysis demonstrating that an optimal
rationale should maximize both the likelihood of rationale generation given the
associated concepts and the likelihood of problem generation conditioned on
both the rationale and the concepts. Our method is evaluated on standard
benchmarks including GSM8K, MATH-500, and AIME2024, where it consistently
outperforms existing problem generation methods. Furthermore, we demonstrate
that PromptCoT exhibits superior data scalability, consistently maintaining
high performance as the dataset size increases, outperforming the baselines.
The implementation is available at https://github.com/zhaoxlpku/PromptCoT.","['cs.CL', 'cs.AI', 'cs.LG']",2501.02822," Road damage detection and assessment are crucial components of infrastructure
maintenance. However, current methods often struggle with detecting multiple
types of road damage in a single image, particularly at varying scales. This is
due to the lack of road datasets with various damage types having varying
scales. To overcome this deficiency, first, we present a novel dataset called
Diverse Road Damage Dataset (DRDD) for road damage detection that captures the
diverse road damage types in individual images, addressing a crucial gap in
existing datasets. Then, we provide our model, RDD4D, that exploits Attention4D
blocks, enabling better feature refinement across multiple scales. The
Attention4D module processes feature maps through an attention mechanism
combining positional encoding and ""Talking Head"" components to capture local
and global contextual information. In our comprehensive experimental analysis
comparing various state-of-the-art models on our proposed, our enhanced model
demonstrated superior performance in detecting large-sized road cracks with an
Average Precision (AP) of 0.458 and maintained competitive performance with an
overall AP of 0.445. Moreover, we also provide results on the CrackTinyNet
dataset; our model achieved around a 0.21 increase in performance. The code,
model weights, dataset, and our results are available on
\href{https://github.com/msaqib17/Road_Damage_Detection}{https://github.com/msaqib17/Road\_Damage\_Detection}.","['cs.CV', 'cs.AI', 'cs.RO']",False,,,,"PromptCoT: Synthesizing Olympiad-level Problems for Mathematical
  Reasoning in Large Language Models",RDD4D: 4D Attention-Guided Road Damage Detection And Classification
neg-d2-454,2025-02-10,,2502.06209," This paper introduces a cost-efficient active learning (AL) framework for
classification, featuring a novel query design called candidate set query.
Unlike traditional AL queries requiring the oracle to examine all possible
classes, our method narrows down the set of candidate classes likely to include
the ground-truth class, significantly reducing the search space and labeling
cost. Moreover, we leverage conformal prediction to dynamically generate small
yet reliable candidate sets, adapting to model enhancement over successive AL
rounds. To this end, we introduce an acquisition function designed to
prioritize data points that offer high information gain at lower cost.
Empirical evaluations on CIFAR-10, CIFAR-100, and ImageNet64x64 demonstrate the
effectiveness and scalability of our framework. Notably, it reduces labeling
cost by 42% on ImageNet64x64.","['cs.LG', 'cs.CV']",2501.01623," The ISCHEMIA Trial randomly assigned patients with ischemic heart disease to
an invasive treatment strategy centered on revascularization with a control
group assigned non-invasive medical therapy. As is common in such ``strategy
trials,'' many participants assigned to treatment remained untreated while many
assigned to control crossed over into treatment. Intention-to-treat (ITT)
analyses of strategy trials preserve randomization-based comparisons, but ITT
effects are diluted by non-compliance. Conventional per-protocol analyses that
condition on treatment received are likely biased by discarding random
assignment. In trials where compliance choices are made shortly after
assignment, instrumental variables (IV) methods solve both problems --
recovering an undiluted average causal effect of treatment for treated subjects
who comply with trial protocol. In ISCHEMIA, however, some controls were
revascularized as long as five years after random assignment. This paper
extends the IV framework for strategy trials, allowing for such dynamic
non-random compliance behavior. IV estimates of long-run revascularization
effects on quality of life are markedly larger than previously reported ITT and
per-protocol estimates. We also show how to estimate complier characteristics
in a dynamic-treatment setting. These estimates reveal increasing selection
bias in naive time-varying per-protocol estimates of revascularization effects.
Compliers have baseline health similar to that of the study population, while
control-group crossovers are far sicker.",['econ.EM'],False,,,,Enhancing Cost Efficiency in Active Learning with Candidate Set Query,"Instrumental Variables with Time-Varying Exposure: New Estimates of
  Revascularization Effects on Quality of Life"
neg-d2-455,2025-02-25,,2502.18385," Phononic materials are crucial for developing efficient, robust mechanical
waveguides with strong transport properties, enabling advances in sensing,
signal processing, energy harvesting, and microfluidics. A key motivation is
their integration into monolithic systems for on-chip applications. While
topological phononic materials developed in the past decade offer
unidirectional edge states immune to backscattering, their integration requires
large volumes to control localized small volumes' transport properties,
limiting their efficiency and application in modern phononic circuits. The
recently introduced chiral anomalous bulk states (CABSs) combine the advantages
of topological materials with innovative boundary designs, overcoming
transmission limitations and ensuring full material utilization for superior
wave propagation. Here, we present the first on-chip monolithic CABS device
integrated on a suspended LiNbO3 thin film. This breakthrough enables the
creation of phononic waveguides with unmatched unidirectionality, low loss, and
high transmission efficiency, seamlessly integrated with broadband
piezoelectric transducers, and showcasing their potential for high-fidelity,
broad-bandwidth microwave signal transmission. Additionally, we exploit the
slow-wave characteristics of CABSs for delay lines and high-density signal
processing. Tailoring wave propagation through boundary engineering opens a new
paradigm for phononic/photonic device design, with implications across
microelectronics, high-frequency communications, radar, and advanced sensing
technologies. The work sets the stage for the future development of highly
scalable, multifunctional, and robust phononic systems, unlocking new avenues
for integrated acoustic technologies.","['physics.app-ph', 'cond-mat.mes-hall', 'cond-mat.mtrl-sci']",2503.02414," 3D models are widely used in various industries, and mesh data has become an
indispensable part of 3D modeling because of its unique advantages. Mesh data
can provide an intuitive and practical expression of rich 3D information.
However, its disordered, irregular data structure and complex surface
information make it challenging to apply with deep learning models directly.
Traditional mesh data processing methods often rely on mesh models with many
limitations, such as manifold, which restrict their application scopes in
reality and do not fully utilize the advantages of mesh models. This paper
proposes a novel end-to-end framework for addressing the challenges associated
with deep learning in mesh models centered around graph neural networks (GNN)
and is titled InfoGNN. InfoGNN treats the mesh model as a graph, which enables
it to handle irregular mesh data efficiently. Moreover, we propose InfoConv and
InfoMP modules, which utilize the position information of the points and fully
use the static information such as face normals, dihedral angles, and dynamic
global feature information to fully use all kinds of data. In addition, InfoGNN
is an end-to-end framework, and we simplify the network design to make it more
efficient, paving the way for efficient deep learning of complex 3D models. We
conducted experiments on several publicly available datasets, and the results
show that InfoGNN achieves excellent performance in mesh classification and
segmentation tasks.","['cs.CV', 'cs.LG']",False,,,,"Monolithic On-Chip Phononic Chiral Anomalous Bulk States on LiNbO3
  Thin-films",InfoGNN: End-to-end deep learning on mesh via graph neural networks
neg-d2-456,2025-02-06,,2502.04659," Blockchains have revolutionized decentralized applications, with
composability enabling atomic, trustless interactions across smart contracts.
However, layer 2 (L2) scalability solutions like rollups introduce
fragmentation and hinder composability. Current cross-chain protocols,
including atomic swaps, bridges, and shared sequencers, lack the necessary
coordination mechanisms or rely on trust assumptions, and are thus not
sufficient to support full cross-rollup composability. This paper presents
$\mathsf{CRATE}$, a secure protocol for cross-rollup composability that ensures
all-or-nothing and serializable execution of cross-rollup transactions (CRTs).
$\mathsf{CRATE}$ supports rollups on distinct layer 1 (L1) chains, achieves
finality in 4 rounds on L1, and only relies on the underlying L1s and the
liveness of L2s. We introduce two formal models for CRTs, define atomicity
within them, and formally prove the security of $\mathsf{CRATE}$. We also
provide an implementation of $\mathsf{CRATE}$ along with a cross-rollup flash
loan application; our experiments demonstrate that $\mathsf{CRATE}$ is
practical in terms of gas usage on L1.",['cs.CR'],2502.19152," We investigate the entanglement properties of the Quantum Six-Vertex Model on
a cylinder, focusing on the Shannon-Renyi entropy in the limit of Renyi order
$n = \infty$. This entropy, calculated from the ground state amplitudes of the
equivalent XXZ spin-1/2 chain, allows us to determine the Renyi entanglement
entropy of the corresponding Rokhsar-Kivelson wavefunctions, which describe the
ground states of certain conformal quantum critical points. Our analysis
reveals a novel logarithmic correction to the expected entanglement scaling
when the system size is odd. This anomaly arises from the geometric frustration
of spin configurations imposed by periodic boundary conditions on odd-sized
chains. We demonstrate that the scaling prefactor of this logarithmic term is
directly related to the compactification radius of the low-energy bosonic field
theory description, or equivalently, the Luttinger parameter. Thus, this
correction provides a direct probe of the underlying Conformal Field Theory
(CFT) describing the critical point. Our findings highlight the crucial role of
system size parity in determining the entanglement properties of this model and
offer insights into the interplay between geometry, frustration, and
criticality.","['quant-ph', 'cond-mat.str-el']",False,,,,$\mathsf{CRATE}$: Cross-Rollup Atomic Transaction Execution,Oddities in the Entanglement Scaling of the Quantum Six-Vertex Model
neg-d2-457,2025-01-21,,2501.12082," Wide-angle video is favored for its wide viewing angle and ability to capture
a large area of scenery, making it an ideal choice for sports and adventure
recording. However, wide-angle video is prone to deformation, exposure and
other distortions, resulting in poor video quality and affecting the perception
and experience, which may seriously hinder its application in fields such as
competitive sports. Up to now, few explorations focus on the quality assessment
issue of wide-angle video. This deficiency primarily stems from the absence of
a specialized dataset for wide-angle videos. To bridge this gap, we construct
the first Multi-annotated and multi-modal Wide-angle Video quality assessment
(MWV) dataset. Then, the performances of state-of-the-art video quality methods
on the MWV dataset are investigated by inter-dataset testing and intra-dataset
testing. Experimental results show that these methods impose significant
limitations on their applicability.","['cs.CV', 'eess.IV']",2501.0958," Stellar-mass and supermassive black holes abound in the Universe, whereas
intermediate-mass black holes (IMBHs) of ~10^2-10^5 solar masses in between are
largely missing observationally, with few cases found only. Here we report the
real-time discovery of a long-duration X-ray transient, EP240222a, accompanied
by an optical flare with prominent H and He emission lines revealed by prompt
follow-up observations. Its observed properties evidence an IMBH located
unambiguously in the halo of a nearby galaxy and flaring by tidally disrupting
a star -- the only confirmed off-nucleus IMBH-tidal disruption event so far.
This work demonstrates the potential of sensitive time-domain X-ray surveys,
complemented by timely multi-wavelength follow-ups, in probing IMBHs, their
environments, demographics, origins and connections to stellar-mass and
supermassive black holes.","['astro-ph.HE', 'astro-ph.GA']",False,,,,"A Multi-annotated and Multi-modal Dataset for Wide-angle Video Quality
  Assessment","An Intermediate-mass Black Hole Lurking in A Galactic Halo Caught Alive
  during Outburst"
neg-d2-458,2025-02-28,,2502.21272," Let $K = \mathbb{R}$ or $\mathbb{C}$. An $n$-element subset $A$ of $K$ is a
$B_h$-set if every element of $K$ has at most one representation as the sum of
$h$ not necessarily distinct elements of $A$. Associated to the $B_h$ set $A =
\{a_1,\ldots, a_n\}$ are the $B_h$-vectors $\mathbf{a} = (a_1,\ldots, a_n)$ in
$K^n$. This paper proves that ``almost all'' $n$-element subsets of $K$ are
$B_h$-sets in the sense that the set of all $B_h$-vectors is a dense open
subset of $K^n$.","['math.CO', 'math.NT']",2502.07856," In applications of diffusion models, controllable generation is of practical
significance, but is also challenging. Current methods for controllable
generation primarily focus on modifying the score function of diffusion models,
while Mean Reverting (MR) Diffusion directly modifies the structure of the
stochastic differential equation (SDE), making the incorporation of image
conditions simpler and more natural. However, current training-free fast
samplers are not directly applicable to MR Diffusion. And thus MR Diffusion
requires hundreds of NFEs (number of function evaluations) to obtain
high-quality samples. In this paper, we propose a new algorithm named MaRS (MR
Sampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time
SDE and the probability flow ordinary differential equation (PF-ODE) associated
with MR Diffusion, and derive semi-analytical solutions. The solutions consist
of an analytical function and an integral parameterized by a neural network.
Based on this solution, we can generate high-quality samples in fewer steps.
Our approach does not require training and supports all mainstream
parameterizations, including noise prediction, data prediction and velocity
prediction. Extensive experiments demonstrate that MR Sampler maintains high
sampling quality with a speedup of 10 to 20 times across ten different image
restoration tasks. Our algorithm accelerates the sampling procedure of MR
Diffusion, making it more practical in controllable generation.","['cs.CV', 'cs.AI', 'cs.LG']",False,,,,$B_h$-sets of real and complex numbers,"MaRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE
  Solvers"
neg-d2-459,2025-02-08,,2502.05549," The problem ""A general characterization of uniqueness polynomial for
non-critically injective polynomials"" has been remained open since the last two
decades. In this paper, we explore this open problem. To this end, we initiate
a new approach that also includes critically injective polynomials. We provide
this characterization for both the complex and p-adic cases. We also provide
various examples as an application of our results along with the verification
of the existing examples. Consequently, we find examples of unique range sets
generated by non-critically injective polynomials with least cardinalities
achieved so far and one of these results is sharp with respect to all the
available formulas in the literature. Furthermore, we cover the part of least
degree uniqueness polynomials. In this part, we also provide some sharp bounds.",['math.CV'],2503.07414," This study aims to develop a cost-effective microgrid design that optimally
balances the economic feasibility, reliability, efficiency, and environmental
impact in a grid-tied community microgrid. A multi-objective optimization
framework is employed, integrating HOMER Pro for system sizing with deep
reinforcement learning (DRL). Sensitivity analyses are conducted to evaluate
the system performance under varying load demand and renewable energy
fluctuations, while an economic sensitivity assessment examines the impact of
electricity prices and capital costs on the Levelized Cost of Energy (LCOE).
The proposed microgrid configuration achieves high reliability, satisfying 100%
of the load, even under adverse weather conditions. The proposed framework
attains an efficiency of 91.99% while maintaining a carbon footprint of 302,747
kg/year, which is approximately 95% lower than that of the grid system. The
economic analysis indicates a net present cost (NPC) of $4.83M with a
competitive LCOE of $0.208/kWh. In addition, the operation cost is $201,473 per
year with a capital investment of $1.42M, rendering it a financially viable
alternative to conventional grid-dependent systems.This work can be valuable in
identifying effective solutions for supplying reliable and cost-effective power
to regional and remote areas.","['eess.SY', 'cs.SY']",False,,,,"On the characterization of uniqueness polynomials: both complex and
  p-adic versions",Cost-Effective Design of Grid-tied Community Microgrid
neg-d2-460,2025-03-20,,2503.16733," Using optical time series with Telescopi Joan Or\'o (TJO), Gaia, TESS, and
NEOWISE archival data, we performed a variability study on the candidate
bloated massive young stellar object (MYSO) IRAS 19520+2759. This is the first
time that a bloated star candidate has been tested for the theoretically
predicted periodic variability. The source is found to be variable at optical
and mid-infrared wavelengths and classified as a long-period variable MYSO. The
observed TJO data gives a period of the source of $\sim$ 270$\pm$40 days (in
the Rc band) and $\sim$ 270$\pm$50 days (in the Ic band), which is very close
to the value predicted by the theoretical Period-Luminosity relation for a
bloated young star of $\sim 10^5 L\odot$. Additionally, a large period of
$\sim$ 460$\pm$80 days (in the G band) and $\sim$ 440$\pm$70 (in the Rp band)
is also visible in the Gaia light curve. The physical parameters of the source,
such as mass, radius, and accretion rate, based on the theoretical predictions
for the spherical accretion case and corresponding to a period of 270--460
days, are $\sim 24$--28$\,M\odot$, $\sim 650$--900$\,R\odot$ and $\sim
(6$--$9)\times10^{-3}\,M\odot yr^{-1}$. However, these numbers are very
sensitive to the effective temperatures assumed in the models. Additionally,
these values strongly depend on the geometry of accretion and could
significantly decrease for the case of a MYSO accreting through a disc. The
observed periodic variability, the observed colour trend, and the nature of the
variability are found to be consistent with the pulsational model for a bloated
MYSO.",['astro-ph.SR'],2502.11967," High intensity coherent light can dress matter, realizing new hybrid phases
that are not accessible in equilibrium. This effect results from the coherent
interaction between Bloch states inside the solid and the periodic field of
impinging photons which produces hybrid light-matter states called
Floquet-Bloch states that can alter properties of the solid. Optically inducing
a topological state in a semiconductor using so-called Floquet engineering is
an exciting prospect. However, it has not been realized, despite its
theoretical prediction more than 10 years ago. Here we show that an
ultrashort-lived topological state that is absent at equilibrium in the ground
state of SnTe can be created with femtosecond light pulses. This occurs when
the photoexcitation is similar in energy with the band gap of this polar
semiconductor. We observe a concomitant renormalization of the band dispersions
that reveals the generation of Floquet states connecting to the topological
state. We therefore provide the first direct experimental observation of a
Floquet topological state and propose that it is driven by a light-induced band
inversion in SnTe. Our discovery opens the way for controlling optically
on-demand the topological properties of semiconductors.",['cond-mat.str-el'],False,,,,"Testing the bloated star hypothesis in the massive young stellar object
  IRAS 19520+2759 through optical and infrared variability",Floquet topological state induced by light-driven band inversion in SnTe
neg-d2-461,2025-03-14,,2503.11461," Navigating unknown three-dimensional (3D) rugged environments is challenging
for multi-robot systems. Traditional discrete systems struggle with rough
terrain due to limited individual mobility, while modular systems--where rigid,
controllable constraints link robot units--improve traversal but suffer from
high control complexity and reduced flexibility. To address these limitations,
we propose the Multi-Robot System with Controllable Weak Constraints (MRS-CWC),
where robot units are connected by constraints with dynamically adjustable
stiffness. This adaptive mechanism softens or stiffens in real-time during
environmental interactions, ensuring a balance between flexibility and
mobility. We formulate the system's dynamics and control model and evaluate
MRS-CWC against six baseline methods and an ablation variant in a benchmark
dataset with 100 different simulation terrains. Results show that MRS-CWC
achieves the highest navigation completion rate and ranks second in success
rate, efficiency, and energy cost in the highly rugged terrain group,
outperforming all baseline methods without relying on environmental modeling,
path planning, or complex control. Even where MRS-CWC ranks second, its
performance is only slightly behind a more complex ablation variant with
environmental modeling and path planning. Finally, we develop a physical
prototype and validate its feasibility in a constructed rugged environment. For
videos, simulation benchmarks, and code, please visit
https://wyd0817.github.io/project-mrs-cwc/.","['cs.RO', 'cs.MA']",2502.06011," Off-policy evaluation (OPE) is a critical challenge in robust decision-making
that seeks to assess the performance of a new policy using data collected under
a different policy. However, the existing OPE methodologies suffer from several
limitations arising from statistical uncertainty as well as causal
considerations. In this thesis, we address these limitations by presenting
three different works. Firstly, we consider the problem of high variance in the
importance-sampling-based OPE estimators. We introduce the Marginal Ratio (MR)
estimator, a novel OPE method that reduces variance by focusing on the marginal
distribution of outcomes rather than direct policy shifts, improving robustness
in contextual bandits. Next, we propose Conformal Off-Policy Prediction (COPP),
a principled approach for uncertainty quantification in OPE that provides
finite-sample predictive intervals, ensuring robust decision-making in
risk-sensitive applications. Finally, we address causal unidentifiability in
off-policy decision-making by developing novel bounds for sequential decision
settings, which remain valid under arbitrary unmeasured confounding. We apply
these bounds to assess the reliability of digital twin models, introducing a
falsification framework to identify scenarios where model predictions diverge
from real-world behaviour. Our contributions provide new insights into robust
decision-making under uncertainty and establish principled methods for
evaluating policies in both static and dynamic settings.","['stat.ML', 'cs.LG']",False,,,,"MRS-CWC: A Weakly Constrained Multi-Robot System with Controllable
  Constraint Stiffness for Mobility and Navigation in Unknown 3D Rough
  Environments","Uncertainty Quantification and Causal Considerations for Off-Policy
  Decision Making"
neg-d2-462,2025-03-17,,2503.13051," Sorting and permutation learning are key concepts in optimization and machine
learning, especially when organizing high-dimensional data into meaningful
spatial layouts. The Gumbel-Sinkhorn method, while effective, requires N*N
parameters to determine a full permutation matrix, making it computationally
expensive for large datasets. Low-rank matrix factorization approximations
reduce memory requirements to 2MN (with M << N), but they still struggle with
very large problems. SoftSort, by providing a continuous relaxation of the
argsort operator, allows differentiable 1D sorting, but it faces challenges
with multidimensional data and complex permutations. In this paper, we present
a novel method for learning permutations using only N parameters, which
dramatically reduces storage costs. Our approach builds on SoftSort, but
extends it by iteratively shuffling the N indices of the elements to be sorted
through a separable learning process. This modification significantly improves
sorting quality, especially for multidimensional data and complex optimization
criteria, and outperforms pure SoftSort. Our method offers improved memory
efficiency and scalability compared to existing approaches, while maintaining
high-quality permutation learning. Its dramatically reduced memory requirements
make it particularly well-suited for large-scale optimization tasks, such as
""Self-Organizing Gaussians"", where efficient and scalable permutation learning
is critical.","['cs.LG', 'cs.CV', 'stat.ML']",2503.14669," This paper presents a reinforcement learning-based neuroadaptive control
framework for robotic manipulators operating under deferred constraints. The
proposed approach improves traditional barrier Lyapunov functions by
introducing a smooth constraint enforcement mechanism that offers two key
advantages: (i) it minimizes control effort in unconstrained regions and
progressively increases it near constraints, improving energy efficiency, and
(ii) it enables gradual constraint activation through a prescribed-time
shifting function, allowing safe operation even when initial conditions violate
constraints. To address system uncertainties and improve adaptability, an
actor-critic reinforcement learning framework is employed. The critic network
estimates the value function, while the actor network learns an optimal control
policy in real time, enabling adaptive constraint handling without requiring
explicit system modeling. Lyapunov-based stability analysis guarantees the
boundedness of all closed-loop signals. The effectiveness of the proposed
method is validated through numerical simulations.","['cs.RO', 'cs.SY', 'eess.SY']",False,,,,"Permutation Learning with Only N Parameters: From SoftSort to
  Self-Organizing Gaussians","Reinforcement Learning-Based Neuroadaptive Control of Robotic
  Manipulators under Deferred Constraints"
neg-d2-463,2025-03-04,,2503.02731," This study investigates the novelty of the crystalline and electronic
structure of (Mg,Ti)-doped ZnO and the co-doped Zn1-x-yMgxTiyO structures using
Gaussian and plane-wave basis sets, as implemented in the CP2K code. The goal
of incorporating low concentration of Mg and Ti into ZnO is to influence its
electronic properties without significantly altering its geometrical and
crystalline structure. Within the framework of density functional theory (DFT),
we analyze various doped and co-doped configurations. Our results show that
Ti-doped ZnO exhibits an indirect band gap, while Mg doping preserves the
direct semiconductor behavior of ZnO structure, with an increase in band gap
energy. Additionally, the co-doped Zn1-x-yMgxTiyO system, at varying
concentrations of Ti and Mg, displays minimal lattice deformation. These
findings suggest that this material could be a promising candidate for
transparent electronic devices, highlighting the importance of understanding
the electronic structure of ZnO to optimize its physical properties.","['cond-mat.mtrl-sci', 'physics.chem-ph']",2503.09241," Computer agents powered by vision-language models (VLMs) have significantly
advanced human-computer interaction, enabling users to perform complex tasks
through natural language instructions. However, these agents are vulnerable to
context deception attacks, an emerging threat where adversaries embed
misleading content into the agent's operational environment, such as a pop-up
window containing deceptive instructions. Existing defenses, such as
instructing agents to ignore deceptive elements, have proven largely
ineffective. As the first systematic study on protecting computer agents, we
introduce textbf{in-context defense}, leveraging in-context learning and
chain-of-thought (CoT) reasoning to counter such attacks. Our approach involves
augmenting the agent's context with a small set of carefully curated exemplars
containing both malicious environments and corresponding defensive responses.
These exemplars guide the agent to first perform explicit defensive reasoning
before action planning, reducing susceptibility to deceptive attacks.
Experiments demonstrate the effectiveness of our method, reducing attack
success rates by 91.2% on pop-up window attacks, 74.6% on average on
environment injection attacks, while achieving 100% successful defenses against
distracting advertisements. Our findings highlight that (1) defensive reasoning
must precede action planning for optimal performance, and (2) a minimal number
of exemplars (fewer than three) is sufficient to induce an agent's defensive
behavior.",['cs.AI'],False,,,,"New investigation of the electronic and structural properties of
  (Mg,Ti)-doped and co-doped ZnO structures: A DFT and DFT+U study",In-Context Defense in Computer Agents: An Empirical Study
neg-d2-464,2025-01-14,,2501.08486," The past decade has seen a rise in the use of Machine Learning methods in the
study of young stellar evolution. This trend has led to a growing need for a
comprehensive database of young stellar objects (YSO) that goes beyond
survey-specific biases and that can be employed for training, validation, and
refining the physical interpretation of machine learning outcomes. We reviewed
the literature focused on the Orion Star Formation complex (OSFC) to compile a
thorough catalogue of previously identified YSO candidates in the region
including the curation of observables relevant to probe their youth. Starting
from the NASA/ADS database, we assembled YSO candidates from more than 200
peer-reviewed publications. We collated data products relevant to the study of
YSOs into a dedicated catalogue, which was complemented with data from large
photometric and spectroscopic surveys and in the Strasbourg astronomical Data
Center. We also added significant value to the catalogue by homogeneously
deriving YSO infrared classification labels and through a comprehensive
curation of labels concerning sources' multiplicity. Finally, we used a
panchromatic approach to derive the probabilities that the sources in our
catalogue were contaminant extragalactic sources or giant stars. We present the
NEMESIS catalogue of YSOs for the OSFC, which includes data collated for 27879
sources covering the whole mass spectrum and the various stages of pre-Main
Sequence evolution from protostars to diskless young stars. The catalogue
includes a collection of panchromatic photometric data processed into spectral
energy distributions, stellar parameters, infrared classes, equivalent widths
of emission lines related to YSOs accretion and star-disk interaction, and
absorption lines such as lithium and lines related to source's gravity, X-ray
emission observables, photometric variability observables, and multiplicity
labels.","['astro-ph.SR', 'astro-ph.GA']",2501.04085," We present the Cosmic Evolution Early Release Science (CEERS) Survey, a 77.2
hour Director's Discretionary Early Release Science Program. CEERS
demonstrates, tests, and validates efficient extragalactic surveys using
coordinated, overlapping parallel observations with the JWST instrument suite,
including NIRCam and MIRI imaging, NIRSpec low (R~100) and medium (R~1000)
resolution spectroscopy, and NIRCam slitless grism (R~1500) spectroscopy. CEERS
targets the Hubble Space Telescope-observed region of the Extended Groth Strip
(EGS) field, supported by a rich set of multiwavelength data. CEERS facilitated
immediate community science in both of the extragalactic core JWST science
drivers ``First Light"" and ``Galaxy Assembly,"" including: 1) The discovery and
characterization of large samples of galaxies at z >~ 10 from ~90 arcmin^2 of
NIRCam imaging, constraining their abundance and physical nature; 2) Deep
spectra of >1000 galaxies, including dozens of galaxies at 6<z<10, enabling
redshift measurements and constraints on the physical conditions of
star-formation and black hole growth via line diagnostics; 3) Quantifying the
first bulge, bar and disk structures at z>3; and 4) Characterizing galaxy
mid-IR emission with MIRI to study dust-obscured star-formation and
supermassive black hole growth at z~1-3. As a legacy product for the community,
the CEERS team has provided several data releases, accompanied by detailed
notes on the data reduction procedures and notebooks to aid in reproducibility.
In addition to an overview of the survey and quality of the data, we provide
science highlights from the first two years with CEERS data.",['astro-ph.GA'],False,,,,"The NEMESIS Catalogue of Young Stellar Objects for the Orion Star
  Formation Complex. I. General description of data curation",The Cosmic Evolution Early Release Science Survey (CEERS)
neg-d2-465,2025-01-03,,2501.01838," The kagome lattice has garnered significant attention due to its ability to
host quantum spin Fermi liquid states. Recently, the combination of unique
lattice geometry, electron-electron correlations, and adjustable magnetism in
solid kagome materials has led to the discovery of numerous fascinating quantum
properties. These include unconventional superconductivity, charge and spin
density waves (CDW/SDW), pair density waves (PDW), and Chern insulator phases.
These emergent states are closely associated with the distinctive
characteristics of the kagome lattice's electronic structure, such as van Hove
singularities, Dirac fermions, and flat bands, which can exhibit exotic
quasi-particle excitations under different symmetries and magnetic conditions.
Recently, various quantum kagome materials have been developed, typically
consisting of kagome layers stacked along the $z$-axis with atoms either
filling the geometric centers of the kagome lattice or embedded between the
layers. In this topical review, we begin by introducing the fundamental
properties of several kagome materials. To gain an in-depth understanding of
the relationship between topology and correlation, we then discuss the complex
phenomena observed in these systems. These include the simplest kagome metal
$T_3X$, kagome intercalation metal $TX$, and the ternary compounds $AT_6X_6$
and $RT_3X_5$ ($A$ = Li, Mg, Ca, or rare earth; $T$ = V, Cr, Mn, Fe, Co, Ni;
$X$ = Sn, Ge; $R$ = K, Rb, Cs). Finally, we provide a perspective on future
experimental work in this field.","['cond-mat.str-el', 'cond-mat.mtrl-sci', 'cond-mat.supr-con']",2502.14277," GK Persei, an old nova and intermediate polar (IP), exhibited a dwarf nova
(DN) outburst in 2010. This outburst was extensively observed by the Neil
Gehrels Swift Observatory, beginning 1.95 days after the eruption and
continuing until 13.9 days before the maximum of the outburst in the optical.
In this paper, we present timing and spectral analyses, comparing the results
with those of other outbursts. We confirm the spin modulation in the 2 $-$ 10
keV X-ray range with a period of $P_{\rm WD} = 351.325(9)$ s. Additionally, we
detected spin modulation in the 0.3 $-$ 2 keV band during the second half of
the observations, a feature not seen in the 2015 and 2018 outbursts. This
finding suggests that the soft X-ray emission in GK Per may originate partly
near the magnetic poles and partly from a wind or circumstellar material.","['astro-ph.HE', 'astro-ph.SR']",False,,,,Electronic band structures of topological kagome materials,"Timing and spectral analysis of GK Persei during the 2010 dwarf nova
  outburst"
neg-d2-466,2025-01-16,,2501.09588," Transformer architectures have become the standard neural network model for
various machine learning applications including natural language processing and
computer vision. However, the compute and memory requirements introduced by
transformer models make them challenging to adopt for edge applications.
Furthermore, fine-tuning pre-trained transformers (e.g., foundation models) is
a common task to enhance the model's predictive performance on specific
tasks/applications. Existing transformer accelerators are oblivious to
complexities introduced by fine-tuning. In this paper, we propose the design of
a three-dimensional (3D) heterogeneous architecture referred to as Atleus that
incorporates heterogeneous computing resources specifically optimized to
accelerate transformer models for the dual purposes of fine-tuning and
inference. Specifically, Atleus utilizes non-volatile memory and systolic array
for accelerating transformer computational kernels using an integrated 3D
platform. Moreover, we design a suitable NoC to achieve high performance and
energy efficiency. Finally, Atleus adopts an effective quantization scheme to
support model compression. Experimental results demonstrate that Atleus
outperforms existing state-of-the-art by up to 56x and 64.5x in terms of
performance and energy efficiency respectively","['cs.AR', 'cs.LG']",2501.1632," This paper is the first in a series dedicated to computing the integral Chow
rings of the moduli stacks of Prym pairs. In this work, we compute the Chow
ring for Prym pairs arising from a single pair of Weierstrass points and from
at most $(g-1)/2 $ pairs when the genus $g$ of the curve is odd.","['math.AG', 'math.RT']",False,,,,"Atleus: Accelerating Transformers on the Edge Enabled by 3D
  Heterogeneous Manycore Architectures","The Integral Chow Rings of the Moduli Stacks of Hyperelliptic Prym Pairs
  I"
neg-d2-467,2025-03-11,,2503.08447," It has been found that the gluon density inside the proton grows rapidly at
small momentum fractions. Quantum Chromodynamics (QCD) predicts that this
growth can be regulated by nonlinear effects, ultimately leading to gluon
saturation. Within the color glass condensate framework, nonlinear QCD effects
are predicted to suppress and broaden back-to-back angular correlations in
collisions involving heavy nuclei. While suppression has been observed in
various experiments in $d/p$$+$A collisions compared to $p$$+$$p$ collisions,
the predicted broadening remains unobserved. This study investigates the
contributions of intrinsic transverse momentum ($k_T$), which is associated
with saturation physics, as well as parton showers and transverse motion from
fragmentation ($p_T^{\mathrm{frag}}$), which are not saturation dependent, to
the width of the correlation function. Our findings show that the
non-saturation dependent effects, especially the initial-state parton shower
and $p_T^{\mathrm{frag}}$, which occur independently of the collision system,
smear the back-to-back correlation more than gluon saturation does, making the
broadening phenomenon difficult to observe.","['hep-ph', 'nucl-ex', 'nucl-th']",2503.12146," Let $\mathcal{D}_{n} \subset \mathbb{N}$ denote the set of the $\tau(n)$
divisors of $n$. We study the function $$ D_{n}(X,Y):=|\{d \in
\mathcal{D}_{n}:\ X \le d \le X+Y\}| $$ for $Y \le X$.",['math.NT'],False,,,,"Investigating the broadening phenomenon in two-particle correlations
  induced by gluon saturation",Divisors of an Integer in a Short Interval
neg-d2-468,2025-02-03,,2502.01491," In this work, we explore how instance-level memorization in the teacher
Neural Machine Translation (NMT) model gets inherited by the student model in
sequence-level knowledge distillation (SeqKD). We find that despite not
directly seeing the original training data, students memorize more than
baseline models (models of the same size, trained on the original data) -- 3.4%
for exact matches and 57% for extractive memorization -- and show increased
hallucination rates. Further, under this SeqKD setting, we also characterize
how students behave on specific training data subgroups, such as subgroups with
low quality and specific counterfactual memorization (CM) scores, and find that
students exhibit amplified denoising on low-quality subgroups. Finally, we
propose a modification to SeqKD named Adaptive-SeqKD, which intervenes in SeqKD
to reduce memorization and hallucinations. Overall, we recommend caution when
applying SeqKD: students inherit both their teachers' superior performance and
their fault modes, thereby requiring active monitoring.",['cs.CL'],2502.13005," Characterizing bipolaron binding, and understanding how it depends on
electron-phonon interaction, is crucial to unraveling the nature of emergent
many-body states in strongly interacting electron-phonon systems. So far, most
studies of bipolarons have been limited to the Holstein model, in which the
coupling constant is momentum-independent. The paradigmatic example of
momentum-dependent electron-phonon interaction comes from the system in which
phonon distortions modify electron hopping, the SSH model. Already individual
polarons in the SSH model are richer than the Holstein model counterparts, and
feature a phase transition into the finite momentum ground state with
increasing electron-phonon interaction. In this paper, we use a variational
approach to study bipolarons in the one-dimensional SSH model and discuss their
ground state, dispersion, and excitation spectra. We explore the full parameter
range of the system, including the adiabatic regime of slow phonons, which was
inaccessible to previous theoretical studies. In agreement with earlier
studies, we find that in the anti-adiabatic strongly interacting regime,
bipolarons have low effective mass. By contrast, in the adiabatic case, we find
that increasing electron-phonon interactions results in an exponential increase
of the bipolaron mass. We establish the existence of multiple branches of bound
excited states of SSH bipolaron and discuss the signatures of these bound
states in dynamics. We show that in the anti-adiabatic regime, response
functions obey a parity selection rule, that imposes symmetry constraints on
the excitation spectra and provides a clear signature of SSH bipolarons.",['cond-mat.str-el'],False,,,,"Memorization Inheritance in Sequence-Level Knowledge Distillation for
  Neural Machine Translation",Bipolaron dynamics in the one-dimensional SSH model
neg-d2-469,2025-03-17,,2503.15548," The widespread adoption of Retrieval-Augmented Generation (RAG) systems in
real-world applications has heightened concerns about the confidentiality and
integrity of their proprietary knowledge bases. These knowledge bases, which
play a critical role in enhancing the generative capabilities of Large Language
Models (LLMs), are increasingly vulnerable to breaches that could compromise
sensitive information. To address these challenges, this paper proposes an
advanced encryption methodology designed to protect RAG systems from
unauthorized access and data leakage. Our approach encrypts both textual
content and its corresponding embeddings prior to storage, ensuring that all
data remains securely encrypted. This mechanism restricts access to authorized
entities with the appropriate decryption keys, thereby significantly reducing
the risk of unintended data exposure. Furthermore, we demonstrate that our
encryption strategy preserves the performance and functionality of RAG
pipelines, ensuring compatibility across diverse domains and applications. To
validate the robustness of our method, we provide comprehensive security proofs
that highlight its resilience against potential threats and vulnerabilities.
These proofs also reveal limitations in existing approaches, which often lack
robustness, adaptability, or reliance on open-source models. Our findings
suggest that integrating advanced encryption techniques into the design and
deployment of RAG systems can effectively enhance privacy safeguards. This
research contributes to the ongoing discourse on improving security measures
for AI-driven services and advocates for stricter data protection standards
within RAG architectures.","['cs.CR', 'cs.AI']",2501.01515," Motivated by deep learning regimes with multiple interacting yet distinct
model components, we introduce learning diagrams, graphical depictions of
training setups that capture parameterized learning as data rather than code. A
learning diagram compiles to a unique loss function on which component models
are trained. The result of training on this loss is a collection of models
whose predictions ``agree"" with one another. We show that a number of popular
learning setups such as few-shot multi-task learning, knowledge distillation,
and multi-modal learning can be depicted as learning diagrams. We further
implement learning diagrams in a library that allows users to build diagrams of
PyTorch and Flux.jl models. By implementing some classic machine learning use
cases, we demonstrate how learning diagrams allow practitioners to build
complicated models as compositions of smaller components, identify
relationships between workflows, and manipulate models during or after
training. Leveraging a category theoretic framework, we introduce a rigorous
semantics for learning diagrams that puts such operations on a firm
mathematical foundation.","['cs.LG', 'cs.AI', 'cs.PL', 'math.CT']",False,,,,Privacy-Aware RAG: Secure and Isolated Knowledge Retrieval,"DiagrammaticLearning: A Graphical Language for Compositional Training
  Regimes"
neg-d2-470,2025-03-01,,2503.00593," On electron kinetic scales, ions and electrons decouple, and electron
velocity shear on electron inertial length $\sim d_e$ can trigger
electromagnetic (EM) electron Kelvin-Helmholtz instability (EKHI). In this
paper, we present an analytic study of EM EKHI in an inviscid collisionless
plasma with a step-function electron shear flow. We show that in incompressible
collisionless plasma the ideal electron frozen-in condition $\mathbf{E} +
\mathbf{v}_e \times \mathbf{B}/c = 0$ must be broken for the EM EKHI to occur.
In a step-function electron shear flow, the ideal electron frozen-in condition
is replaced by magnetic flux conservation, i.e., $\nabla \times (\mathbf{E} +
\mathbf{v}_e\times \mathbf{B}/c) = 0$, resulting in a dispersion relation
similar to that of the standard ideal and incompressible magnetohydrodynamics
KHI. The magnetic field parallel to the electron streaming suppresses the EM
EKHI due to magnetic tension. The threshold for the EM mode of the EKHI is
$(\mathbf{k}\cdot\Delta\mathbf{U}_e)^2>\frac{n_{e1}+n_{e2}}{n_{e1}
n_{e2}}[n_{e1}(\mathbf{v}_{Ae1}\cdot\mathbf{k})^2+n_{e2}(\mathbf{v}_{Ae2}\cdot\mathbf{k})^2]$,
where $\mathbf{v}_{Ae} =\mathbf{B}/(4\pi m_e n_e)^{1/2}$, $\Delta\mathbf{U}_e$
and $n_e$ are the electron streaming velocity shear and densities,
respectively. The growth rate of the EM mode is $\gamma_{em} \sim \Omega_{ce}$,
the electron gyro-frequency.","['astro-ph.SR', 'astro-ph.HE', 'physics.plasm-ph']",2501.04112," In this work, we provide the first example of an infinite family of branch
groups in the class of non-contracting self-similar groups. We show that these
groups are very strongly fractal, not regular branch, and of exponential
growth. Further, we prove that these groups do not have the congruence subgroup
property by explicitly calculating the structure of their rigid kernels. This
class of groups is also the first example of branch groups with non-torsion
rigid kernels. As a consequence of these results, we also determine the
Hausdorff dimension of these groups.",['math.GR'],False,,,,Electromagnetic Electron Kelvin-Helmholtz Instability,A Class of Non-Contracting Branch Groups with Non-Torsion Rigid Kernels
neg-d2-471,2025-03-01,,2503.00562," H. Lamb considered the classical dynamics of a vibrating particle embedded in
an elastic medium before the development of quantum theory. Lamb was interested
in how the back-action of the elastic waves generated can damp the vibrations
of the particle. We propose a quantum version of Lamb's model. We show that
this model is exactly solvable by using a multimode Bogoliubov transformation.
We show that the exact system ground state is a multimode squeezed vacuum
state, and we obtain the exact Bogoliubov frequencies by numerically solving a
nonlinear integral equation. A closed-form expression for the damping rate of
the particle is obtained, and we find that it agrees with the result obtained
by perturbation theory for coupling strength below a critical value. The model
is found to break down for coupling strength above the critical value where the
lowest Bogoliubov frequency vanishes. We show that the addition of an
anharmonic elastic term is sufficient to stabilize the system in this strong
coupling regime.","['quant-ph', 'cond-mat.mes-hall']",2503.07355," This paper contains a review of the theoretical foundations of Clifford
algebras, spinors and spinor bundles in the so-called co-frame formalism. A
compact index-free notation is introduced, along with a series of identities
useful for computations in supergravity theories.","['math-ph', 'hep-th', 'math.MP']",False,,,,Quantum Lamb model,Tools for Supergravity in the spin coframe formalism
neg-d2-472,2025-02-07,,2502.05238," In this work, we study neutrino spin oscillations in the case when they are
gravitationally scattered off a rotating Kerr black hole surrounded by a thick
magnetized accretion disk. We consider only toroidal magnetic field inside the
disk. Neutrino spin precession is caused by the interaction of the neutrino
magnetic moment with the magnetic field in the disk. Our treatment of the spin
oscillations of the observed neutrino fluxes is based on numerical simulations
of the propagation of a large number of incoming test neutrinos using High
Performance Parallel Computing. We briefly discuss our results and their
applications in the observations of astrophysical neutrinos.","['hep-ph', 'astro-ph.HE', 'gr-qc']",2503.09358," Standardization of clinical reports is crucial for improving the quality of
healthcare and facilitating data integration. The lack of unified standards,
including format, terminology, and style, is a great challenge in clinical
fundus diagnostic reports, which increases the difficulty for large language
models (LLMs) to understand the data. To address this, we construct a bilingual
standard terminology, containing fundus clinical terms and commonly used
descriptions in clinical diagnosis. Then, we establish two models,
RetSTA-7B-Zero and RetSTA-7B. RetSTA-7B-Zero, fine-tuned on an augmented
dataset simulating clinical scenarios, demonstrates powerful standardization
behaviors. However, it encounters a challenge of limitation to cover a wider
range of diseases. To further enhance standardization performance, we build
RetSTA-7B, which integrates a substantial amount of standardized data generated
by RetSTA-7B-Zero along with corresponding English data, covering diverse
complex clinical scenarios and achieving report-level standardization for the
first time. Experimental results demonstrate that RetSTA-7B outperforms other
compared LLMs in bilingual standardization task, which validates its superior
performance and generalizability. The checkpoints are available at
https://github.com/AB-Story/RetSTA-7B.","['cs.CL', 'cs.AI']",False,,,,Neutrino spin oscillations near a black hole,"RetSTA: An LLM-Based Approach for Standardizing Clinical Fundus Image
  Reports"
neg-d2-473,2025-03-21,,2503.17066," We consider in this work a $2$-dimensional $3$-wave kinetic equation
describing the dynamics of the thermal cloud outside a Bose-Einstein
Condensate. We construct global non-radial mild solutions for the equation.
Those mild solutions are the summation of Dirac masses on circles. We prove
that in each spatial direction, either Dirac masses at the origin, which are
the so-called Bose-Einstein condensates, can be formed in finite time or the
solutions converge to Bose-Einstein condensates as time evolves to infinity. We
also describe a dynamics of the formation of the Bose-Einstein condensates
latter case. In this case, on each direction, the solutions accumulate around
circles close to the origin at growth rates at least linearly in time.",['math.AP'],2501.01623," The ISCHEMIA Trial randomly assigned patients with ischemic heart disease to
an invasive treatment strategy centered on revascularization with a control
group assigned non-invasive medical therapy. As is common in such ``strategy
trials,'' many participants assigned to treatment remained untreated while many
assigned to control crossed over into treatment. Intention-to-treat (ITT)
analyses of strategy trials preserve randomization-based comparisons, but ITT
effects are diluted by non-compliance. Conventional per-protocol analyses that
condition on treatment received are likely biased by discarding random
assignment. In trials where compliance choices are made shortly after
assignment, instrumental variables (IV) methods solve both problems --
recovering an undiluted average causal effect of treatment for treated subjects
who comply with trial protocol. In ISCHEMIA, however, some controls were
revascularized as long as five years after random assignment. This paper
extends the IV framework for strategy trials, allowing for such dynamic
non-random compliance behavior. IV estimates of long-run revascularization
effects on quality of life are markedly larger than previously reported ITT and
per-protocol estimates. We also show how to estimate complier characteristics
in a dynamic-treatment setting. These estimates reveal increasing selection
bias in naive time-varying per-protocol estimates of revascularization effects.
Compliers have baseline health similar to that of the study population, while
control-group crossovers are far sicker.",['econ.EM'],False,,,,"Formation of condensations for non-radial solutions to 3-wave kinetic
  equations","Instrumental Variables with Time-Varying Exposure: New Estimates of
  Revascularization Effects on Quality of Life"
neg-d2-474,2025-01-03,,2501.02139," The space writhe of a knot is a property of its three-dimensional embedding
that contains information about its underlying topology, but the correspondence
between space writhe and other topological invariants is not fully understood.
We perform Langevin dynamics simulations of knotted semiflexible polymers and
measure their ensemble average space writhe. We show that for all knots up to
10 crossings, alternating and non-alternating, the average space writhe is
almost equal to that of the tightest known configuration of the same knot, with
minor differences. Using this equivalence, we show that for more complex knots
with up to 38 crossings, the average space writhe is strongly correlated with
the signature of the knot. This establishes that the connection between
signature and space writhe holds at larger crossing numbers.","['cond-mat.soft', 'math.GT']",2501.01003," 3D Gaussian Splatting (3DGS) techniques have achieved satisfactory 3D scene
representation. Despite their impressive performance, they confront challenges
due to the limitation of structure-from-motion (SfM) methods on acquiring
accurate scene initialization, or the inefficiency of densification strategy.
In this paper, we introduce a novel framework EasySplat to achieve high-quality
3DGS modeling. Instead of using SfM for scene initialization, we employ a novel
method to release the power of large-scale pointmap approaches. Specifically,
we propose an efficient grouping strategy based on view similarity, and use
robust pointmap priors to obtain high-quality point clouds and camera poses for
3D scene initialization. After obtaining a reliable scene structure, we propose
a novel densification approach that adaptively splits Gaussian primitives based
on the average shape of neighboring Gaussian ellipsoids, utilizing KNN scheme.
In this way, the proposed method tackles the limitation on initialization and
optimization, leading to an efficient and accurate 3DGS modeling. Extensive
experiments demonstrate that EasySplat outperforms the current state-of-the-art
(SOTA) in handling novel view synthesis.",['cs.CV'],False,,,,The space writhes and signatures of polymer knots,EasySplat: View-Adaptive Learning makes 3D Gaussian Splatting Easy
neg-d2-475,2025-01-16,,2501.0961," This study explores the applications of the Prouhet-Thue-Morse (PTM) sequence
in quantum computing, highlighting its mathematical elegance and practical
relevance. We demonstrate the critical role of the PTM sequence in quantum
error correction, in noise-resistant quantum memories, and in providing
insights into quantum chaos. Notably, we demonstrate how the PTM sequence
naturally appears in Ising X-X interacting systems, leading to a proposed
robust encoding of quantum memories in such systems. Furthermore, connections
to number theory, including the Riemann zeta function, bridge quantum computing
with pure mathematics. Our findings emphasize the PTM sequence's importance in
understanding the mathematical structure of quantum computing systems and the
development of the full potential of quantum technologies and invite further
interdisciplinary research.","['quant-ph', 'math-ph', 'math.MP']",2503.1702," Quantum kernels quantify similarity between data points by measuring the
inner product between quantum states, computed through quantum circuit
measurements. By embedding data into quantum systems, quantum kernel feature
maps, that may be classically intractable to compute, could efficiently exploit
high-dimensional Hilbert spaces to capture complex patterns. However, designing
effective quantum feature maps remains a major challenge. Many quantum kernels,
such as the fidelity kernel, suffer from exponential concentration, leading to
near-identity kernel matrices that fail to capture meaningful data correlations
and lead to overfitting and poor generalization. In this paper, we propose a
novel strategy for constructing quantum kernels that achieve good
generalization performance, drawing inspiration from benign overfitting in
classical machine learning. Our approach introduces the concept of local-global
quantum kernels, which combine two complementary components: a local quantum
kernel based on measurements of small subsystems and a global quantum kernel
derived from full-system measurements. Through numerical experiments, we
demonstrate that local-global quantum kernels exhibit benign overfitting,
supporting the effectiveness of our approach in enhancing quantum kernel
methods.","['quant-ph', 'cs.LG', 'stat.ML']",False,,,,"Elucidating the Physical and Mathematical Properties of the
  Prouhet-Thue-Morse Sequence in Quantum Computing",Benign Overfitting with Quantum Kernels
neg-d2-476,2025-01-16,,2501.09803," Estimating the shortest travel time and providing route recommendation
between different locations in a city or region can quantitatively measure the
conditions of the transportation network during or after extreme events. One
common approach is to use Dijkstra's Algorithm, which produces the shortest
path as well as the shortest distance. However, this option is computationally
expensive when applied to large-scale networks. This paper proposes a novel
fast framework based on graph neural networks (GNNs) which approximate the
single-source shortest distance between pairs of locations, and predict the
single-source shortest path subsequently. We conduct multiple experiments on
synthetic graphs of different size to demonstrate the feasibility and
computational efficiency of the proposed model. In real-world case studies, we
also applied the proposed method of flood risk analysis of coastal urban areas
to calculate delays in evacuation to public shelters during hurricanes. The
results indicate the accuracy and computational efficiency of the GNN model,
and its potential for effective implementation in emergency planning and
management.",['cs.LG'],2502.03732," Anxiety, depression, and suicidality are common mental health sequelae
following concussion in youth patients, often exacerbating concussion symptoms
and prolonging recovery. Despite the critical need for early detection of these
mental health symptoms, clinicians often face challenges in accurately
collecting patients' mental health data and making clinical decision-making in
a timely manner. Today's remote patient monitoring (RPM) technologies offer
opportunities to objectively monitor patients' activities, but they were not
specifically designed for youth concussion patients; moreover, the large amount
of data collected by RPM technologies may also impose significant workloads on
clinicians to keep up with and use the data. To address these gaps, we employed
a three-stage study consisting of a formative study, interface design, and
design evaluation. We first conducted a formative study through semi-structured
interviews with six highly professional concussion clinicians and identified
clinicians' key challenges in remotely collecting patient information and
accessing patient treatment compliance. Subsequently, we proposed preliminary
clinician-facing interface designs with the integration of AI-based RPM
technologies (AI-RPM), followed by design evaluation sessions with highly
professional concussion clinicians. Clinicians underscored the value of
integrating multi-modal AI-RPM technologies to support clinicians'
decision-making while emphasizing the importance of customizable interfaces
with explainability and multiple responsible design considerations.",['cs.HC'],False,,,,"Graph Neural Networks for Travel Distance Estimation and Route
  Recommendation Under Probabilistic Hazards","More Modality, More AI: Exploring Design Opportunities of AI-Based
  Multi-modal Remote Monitoring Technologies for Early Detection of Mental
  Health Sequelae in Youth Concussion Patients"
neg-d2-477,2025-01-24,,2501.14421," Isolation levels, consistency guarantees among concurrently execution
transactions in local- and distributed systems, have been formalized in a
number of models. Thus far, no model can reason about executable
implementations of databases or local transaction libraries providing weak
isolation levels. Weak isolation levels are characterized by being highly
concurrent and, unlike their stronger counterpart serializability, they are not
equivalent to the consistency guarantees provided by a transaction library
implemented using a global lock. In this paper, we formalize three weak
isolation levels in separation logic, namely read uncommitted, read committed,
and snapshot isolation. We define modular separation logic specifications that
are independent of the underlying transaction library implementation.
Historically, isolation levels have been specified using examples of executions
between concurrent transactions that are not allowed to occur, and we
demonstrate that our specifications correctly prohibit such examples. To show
that our specifications are realizable, we formally verify that an executable
implementation of a key-value database running the multi-version concurrency
control algorithm from the original snapshot isolation paper satisfies our
specification of snapshot isolation. Moreover, we prove implications between
the specifications -- snapshot isolation implies read committed and read
committed implies read uncommitted -- and thus the verification effort of the
database serves as proof that all of our specifications are realizable. All
results are mechanised in the Coq proof assistant on top of the Iris separation
logic framework.","['cs.PL', 'cs.LO']",2502.01058," Leveraging the sensitive dependence of a giant atom's radiation rate on its
frequency [A. F. Kockum, $et~al$., Phys. Rev. A 90, 013837 (2014)], we propose
an effective magnetometer model based on single giant emitter. In this model,
the emitter's frequency is proportional to the applied bias magnetic field. The
self-interference effect causes the slope of the dissipation spectrum to vary
linearly with the number of emitter-coupling points. The giant emitter
magnetometer achieves a sensitivity as high as $10^{-8}-10^{-9}\,{\rm
T/\sqrt{Hz}}$, demonstrating the significant advantages of the
self-interference effect compared to small emitters. We hope our proposal will
expand the applications of giant emitters in precision measurement and
magnetometry.",['quant-ph'],False,,,,Reasoning about Weak Isolation Levels in Separation Logic,Giant emitter magnetometer
neg-d2-478,2025-01-02,,2501.01186," A summary of recent contributions in the field of rough partial differential
equations is given. For that purpose we rely on the formalism of ``unbounded
rough driver''. We present applications to concrete models including
Landau-Lifshitz-Gilbert, Navier-Stokes and Euler equations.","['math.AP', 'math.PR']",2503.12647," Molecular communication (MC) offers a groundbreaking approach to
communication inspired by biological signaling. It is particularly suited for
environments where traditional electromagnetic methods fail, such as fluid
mediums or within the human body. This study focuses on addressing a major
challenge in MC systems: inter symbol interference (ISI), which arises due to
the random, diffusive propagation of molecules. We propose a novel technique
that leverages transmission shaping to mitigate ISI effectively by designing
optimal transmission pulse (or sequence) for symbols. Our approach centers on
solving a multi-objective optimization problem that aims to maximize the
separability of individual symbol's responses within the symbol duration while
matching the interference caused by molecular spillover for all symbols. By
making ISI of each symbol similar, the approach reduces the effect of previous
symbols and thus not require any adaptive computations. We introduce a
geometric analogy involving two families of ellipses to derive the optimal
solution. Analytical insights are supported by numerical simulations to design
optimized transmission profiles to enhance the resilience toward ISI. The
proposed transmission shaping method is evaluated through symbol error rate
(SER). These results mark a significant step forward in developing robust and
efficient MC systems, opening doors to advanced applications in bio-inspired
and nano-scale communication technologies.","['q-bio.MN', 'eess.SP']",False,,,,"Unbounded rough drivers, rough PDEs and applications","Optimal Transmission Sequence Design with ISI Matching in Molecular
  Communication"
neg-d2-479,2025-01-08,,2501.0429," Although the star formation process has been studied for decades, many
important aspects of the physics involved remain unsolved. Recent advancement
of instrumentation in the infrared, far-infrared and sub-millimetre wavelength
regimes have contributed to a significantly improved understanding of processes
in the interstellar medium (ISM) leading to star formation. The future of
research on the ISM and star formation looks exciting with instruments like the
JWST, ALMA, etc., already contributing to the topic by gathering
high-resolution high-sensitivity data and with several larger ground- and
space-bound facilities either being planned or constructed. India has a sizable
number of astronomers engaged in research on topics related to the ISM and star
formation. In this white paper invited by the Astronomical Society of India to
prepare a vision document for Indian astronomy, we review the Indian
contributions to the global understanding of the star formation process and
suggest areas that require focused efforts both in creating observing
facilities and in theoretical front in India, in order to improve the impact of
our research in the coming decades.",['astro-ph.GA'],2502.0613," While recent Large Vision-Language Models (LVLMs) have shown remarkable
performance in multi-modal tasks, they are prone to generating hallucinatory
text responses that do not align with the given visual input, which restricts
their practical applicability in real-world scenarios. In this work, inspired
by the observation that the text-to-image generation process is the inverse of
image-conditioned response generation in LVLMs, we explore the potential of
leveraging text-to-image generative models to assist in mitigating
hallucinations in LVLMs. We discover that generative models can offer valuable
self-feedback for mitigating hallucinations at both the response and token
levels. Building on this insight, we introduce self-correcting Decoding with
Generative Feedback (DeGF), a novel training-free algorithm that incorporates
feedback from text-to-image generative models into the decoding process to
effectively mitigate hallucinations in LVLMs. Specifically, DeGF generates an
image from the initial response produced by LVLMs, which acts as an auxiliary
visual reference and provides self-feedback to verify and correct the initial
response through complementary or contrastive decoding. Extensive experimental
results validate the effectiveness of our approach in mitigating diverse types
of hallucinations, consistently surpassing state-of-the-art methods across six
benchmarks. Code is available at https://github.com/zhangce01/DeGF.","['cs.CV', 'cs.CL']",False,,,,"Research on the Interstellar Medium and Star Formation in the Galaxy: An
  Indian Perspective","Self-Correcting Decoding with Generative Feedback for Mitigating
  Hallucinations in Large Vision-Language Models"
neg-d2-480,2025-03-18,,2503.14797," With the widespread consumption of AI-generated content, there has been an
increased focus on developing automated tools to verify the factual accuracy of
such content. However, prior research and tools developed for fact verification
treat it as a binary classification or a linear regression problem. Although
this is a useful mechanism as part of automatic guardrails in systems, we argue
that such tools lack transparency in the prediction reasoning and diversity in
source evidence to provide a trustworthy user experience. We develop
Facts&Evidence - an interactive and transparent tool for user-driven
verification of complex text. The tool facilitates the intricate
decision-making involved in fact-verification, presenting its users a breakdown
of complex input texts to visualize the credibility of individual claims along
with an explanation of model decisions and attribution to multiple, diverse
evidence sources. Facts&Evidence aims to empower consumers of machine-generated
text and give them agency to understand, verify, selectively trust and use such
text.",['cs.CL'],2502.13384," The derivative of a polynomial with all zeros on the unit circle has the
zeros of its derivative on or inside the unit circle. It has been observed that
in many cases the zeros of the derivative have a bimodal distribution: there
are two smaller circles near which it is more likely to find those zeros. We
identify the likely source of the second mode. This idea is supported with
numerical examples involving the characteristic polynomials of random unitary
matrices.","['math.CV', 'math.NT']",False,,,,"FACTS&EVIDENCE: An Interactive Tool for Transparent Fine-Grained Factual
  Verification of Machine-Generated Text",The bimodal distribution in the derivative of unitary polynomials
neg-d2-481,2025-02-27,,2502.20639," Federated Learning (FL) facilitates collaborative training of a shared global
model without exposing clients' private data. In practical FL systems, clients
(e.g., edge servers, smartphones, and wearables) typically have disparate
system resources. Conventional FL, however, adopts a one-size-fits-all
solution, where a homogeneous large global model is transmitted to and trained
on each client, resulting in an overwhelming workload for less capable clients
and starvation for other clients. To address this issue, we propose FedConv, a
client-friendly FL framework, which minimizes the computation and memory burden
on resource-constrained clients by providing heterogeneous customized
sub-models. FedConv features a novel learning-on-model paradigm that learns the
parameters of the heterogeneous sub-models via convolutional compression.
Unlike traditional compression methods, the compressed models in FedConv can be
directly trained on clients without decompression. To aggregate the
heterogeneous sub-models, we propose transposed convolutional dilation to
convert them back to large models with a unified size while retaining
personalized information from clients. The compression and dilation processes,
transparent to clients, are optimized on the server leveraging a small public
dataset. Extensive experiments on six datasets demonstrate that FedConv
outperforms state-of-the-art FL systems in terms of model accuracy (by more
than 35% on average), computation and communication overhead (with 33% and 25%
reduction, respectively).","['cs.LG', 'cs.AI']",2503.16733," Using optical time series with Telescopi Joan Or\'o (TJO), Gaia, TESS, and
NEOWISE archival data, we performed a variability study on the candidate
bloated massive young stellar object (MYSO) IRAS 19520+2759. This is the first
time that a bloated star candidate has been tested for the theoretically
predicted periodic variability. The source is found to be variable at optical
and mid-infrared wavelengths and classified as a long-period variable MYSO. The
observed TJO data gives a period of the source of $\sim$ 270$\pm$40 days (in
the Rc band) and $\sim$ 270$\pm$50 days (in the Ic band), which is very close
to the value predicted by the theoretical Period-Luminosity relation for a
bloated young star of $\sim 10^5 L\odot$. Additionally, a large period of
$\sim$ 460$\pm$80 days (in the G band) and $\sim$ 440$\pm$70 (in the Rp band)
is also visible in the Gaia light curve. The physical parameters of the source,
such as mass, radius, and accretion rate, based on the theoretical predictions
for the spherical accretion case and corresponding to a period of 270--460
days, are $\sim 24$--28$\,M\odot$, $\sim 650$--900$\,R\odot$ and $\sim
(6$--$9)\times10^{-3}\,M\odot yr^{-1}$. However, these numbers are very
sensitive to the effective temperatures assumed in the models. Additionally,
these values strongly depend on the geometry of accretion and could
significantly decrease for the case of a MYSO accreting through a disc. The
observed periodic variability, the observed colour trend, and the nature of the
variability are found to be consistent with the pulsational model for a bloated
MYSO.",['astro-ph.SR'],False,,,,"FedConv: A Learning-on-Model Paradigm for Heterogeneous Federated
  Clients","Testing the bloated star hypothesis in the massive young stellar object
  IRAS 19520+2759 through optical and infrared variability"
neg-d2-482,2025-02-16,,2502.1133," System messages play a crucial role in interactions with large language
models (LLMs), often serving as prompts to initiate conversations. Through
system messages, users can assign specific roles, perform intended tasks,
incorporate background information, specify various output formats and
communication styles. Despite such versatility, publicly available data are
often lack system messages and subject to strict license constraints in the
industry field. Manual labeling of publicly available data with system messages
that align with user instructions demands significant resources. In view of
such challenges, our work introduces SysGen, a pipeline for generating system
messages with better aligned assistant responses from the supervised
fine-tuning dataset without system messages. Training on SysGen data has
demonstrated substantial improvements in the alignment of model responses with
system messages and user instructions, as demonstrated across various
open-source models on the Multifacet benchmark, while maintaining minimal
impact on other unseen benchmarks such as Open LLM Leaderboard 2. Our
qualitative analysis highlights the importance of diverse system messages to
ensure better adaptability across different contexts.","['cs.CL', 'cs.AI']",2502.03605," Device sizing is crucial for meeting performance specifications in
operational transconductance amplifiers (OTAs), and this work proposes an
automated sizing framework based on a transformer model. The approach first
leverages the driving-point signal flow graph (DP-SFG) to map an OTA circuit
and its specifications into transformer-friendly sequential data. A specialized
tokenization approach is applied to the sequential data to expedite the
training of the transformer on a diverse range of OTA topologies, under
multiple specifications. Under specific performance constraints, the trained
transformer model is used to accurately predict DP-SFG parameters in the
inference phase. The predicted DP-SFG parameters are then translated to
transistor sizes using a precomputed look-up table-based approach inspired by
the gm/Id methodology. In contrast to previous conventional or
machine-learning-based methods, the proposed framework achieves significant
improvements in both speed and computational efficiency by reducing the need
for expensive SPICE simulations within the optimization loop; instead, almost
all SPICE simulations are confined to the one-time training phase. The method
is validated on a variety of unseen specifications, and the sizing solution
demonstrates over 90% success in meeting specifications with just one SPICE
simulation for validation, and 100% success with 3-5 additional SPICE
simulations.",['cs.AR'],False,,,,System Message Generation for User Preferences using Open-Source Models,"Accelerating OTA Circuit Design: Transistor Sizing Based on a
  Transformer Model and Precomputed Lookup Tables"
neg-d2-483,2025-03-24,,2503.18396," We study quantum geometric effects in time-dependent quantum many-body
systems quenched from integrable systems through a unitary transformation whose
phase operator is linear in time. We establish a theorem stating that the Berry
connection matrix thus all associated geometric quantities of the
time-dependent many-body system, can be precisely characterized by excitations
up to two-particle processes derived from the quantum integrable system. This
geometric characterization provides a powerful lens for analyzing dynamical
transitions in driven many-body settings. To illustrate the many-body geometric
influence, we analyze a prototypical time-dependent Ising chain subjected to
both a small longitudinal field and a slowly rotating transverse field, whose
low-energy physics in the scaling limit is instantaneously governed by the
quantum $E_8$ integrable field theory. Focusing on the quantum geometric
potential (QGP), we show the QGP continuously suppresses the instantaneous
energy gaps with decreasing longitudinal field, thereby enhancing many-body
Landau-Zener tunneling as evidenced by the Loschmidt echo and its associated
spectral entropy. The critical threshold for the longitudinal field strength is
determined, where the spectral entropy linearly increases with system size and
exhbits hyperscaling behavior when approaching to the threshold. When the
longitudinal field passes the threshold and decreases toward zero, the QGP
continuously leads to vanishing instantaneous energy gaps involving more
low-energy excitations, resulting in increasing spectral entropy indicative of
many-body Landau-Zener tunneling. Our results unveil telltale quantum geometric
signatures in time-dependent many-body systems, elucidating the intricate
interplay between quantum geometry and dynamics.","['cond-mat.str-el', 'cond-mat.stat-mech', 'quant-ph']",2503.17547," Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting
neural networks by extracting the concepts represented in their activations.
However, choosing the size of the SAE dictionary (i.e. number of learned
concepts) creates a tension: as dictionary size increases to capture more
relevant concepts, sparsity incentivizes features to be split or absorbed into
more specific features, leaving high-level features missing or warped. We
introduce Matryoshka SAEs, a novel variant that addresses these issues by
simultaneously training multiple nested dictionaries of increasing size,
forcing the smaller dictionaries to independently reconstruct the inputs
without using the larger dictionaries. This organizes features hierarchically -
the smaller dictionaries learn general concepts, while the larger dictionaries
learn more specific concepts, without incentive to absorb the high-level
features. We train Matryoshka SAEs on Gemma-2-2B and TinyStories and find
superior performance on sparse probing and targeted concept erasure tasks, more
disentangled concept representations, and reduced feature absorption. While
there is a minor tradeoff with reconstruction performance, we believe
Matryoshka SAEs are a superior alternative for practical tasks, as they enable
training arbitrarily large SAEs while retaining interpretable features at
different levels of abstraction.","['cs.LG', 'cs.AI']",False,,,,"Quantum Geometry and Many-Body Landau-Zener Tunneling in Time-dependent
  Quantum Systems with Instantaneous Quantum Integrability",Learning Multi-Level Features with Matryoshka Sparse Autoencoders
neg-d2-484,2025-01-08,,2501.04483," The Ethereum blockchain has a \emph{gas system} that associates operations
with a cost in gas units. Two central concepts of this system are the \emph{gas
limit} assigned by the issuer of a transaction and the \emph{gas used} by a
transaction. The former is a budget that must not be exhausted before the
completion of the transaction execution; otherwise, the execution fails.
Therefore, it seems rather essential to determine the \emph{minimum gas limit}
that ensures the execution of a transaction will not abort due to the lack of
gas. Despite its practical relevance, this concept has not been properly
addressed. In the literature, gas used and minimum gas limit are conflated.
This paper proposes a precise notion of minimum gas limit and how it can differ
from gas used by a transaction; this is also demonstrated with a quantitative
study on real transactions of the Ethereum blockchain. Another significant
contribution is the proposition of a fairly precise estimator for each of the
two metrics. Again, the confusion between these concepts has led to the
creation of estimators only for the gas used by a transaction. We demonstrate
that the minimum gas limit for the state of the Ethereum blockchain (after the
block) $t$ can serve as a near-perfect estimation for the execution of the
transaction at block $t + \Delta$, where $\Delta \leq 11$; the same holds for
estimating gas used. These precise estimators can be very valuable in helping
the users predict the gas budget of transactions and developers in optimising
their smart contracts; over and underestimating gas used and minimum gas limit
can lead to a number of practical issues. Overall, this paper serves as an
important reference for blockchain developers and users as to how the gas
system really works.","['cs.SE', 'cs.CE', 'cs.DC', 'cs.ET', 'cs.NI']",2503.17085," Artificial intelligence (AI) systems powered by large language models have
become increasingly prevalent in modern society, enabling a wide range of
applications through natural language interaction. As AI agents proliferate in
our daily lives, their generic and uniform expressiveness presents a
significant limitation to their appeal and adoption. Personality expression
represents a key prerequisite for creating more human-like and distinctive AI
systems. We show that AI models can express deterministic and consistent
personalities when instructed using established psychological frameworks, with
varying degrees of accuracy depending on model capabilities. We find that more
advanced models like GPT-4o and o1 demonstrate the highest accuracy in
expressing specified personalities across both Big Five and Myers-Briggs
assessments, and further analysis suggests that personality expression emerges
from a combination of intelligence and reasoning capabilities. Our results
reveal that personality expression operates through holistic reasoning rather
than question-by-question optimization, with response-scale metrics showing
higher variance than test-scale metrics. Furthermore, we find that model
fine-tuning affects communication style independently of personality expression
accuracy. These findings establish a foundation for creating AI agents with
diverse and consistent personalities, which could significantly enhance
human-AI interaction across applications from education to healthcare, while
additionally enabling a broader range of more unique AI agents. The ability to
quantitatively assess and implement personality expression in AI systems opens
new avenues for research into more relatable, trustworthy, and ethically
designed AI.","['cs.LG', 'cs.AI', 'cs.CY', 'cs.HC']",False,,,,"Demystification and Near-perfect Estimation of Minimum Gas Limit and Gas
  Used for Ethereum Smart Contracts","Deterministic AI Agent Personality Expression through Standard
  Psychological Diagnostics"
neg-d2-485,2025-02-17,,2502.12129," In this work, we address the lossy quantum-classical source coding with the
quantum side-information (QC-QSI) problem. The task is to compress the
classical information about a quantum source, obtained after performing a
measurement while incurring a bounded reconstruction error. Here, the decoder
is allowed to use the side information to recover the classical data obtained
from measurements on the source states. We introduce a new formulation based on
a backward (posterior) channel, replacing the single-letter distortion
observable with a single-letter posterior channel to capture reconstruction
error. Unlike the rate-distortion framework, this formulation imposes a block
error constraint. An analogous formulation is developed for lossy classical
source coding with classical side information (C-CSI) problem. We derive an
inner bound on the asymptotic performance limit in terms of single-letter
quantum and classical mutual information quantities of the given posterior
channel for QC-QSI and C-CSI cases, respectively. Furthermore, we establish a
connection between rate-distortion and rate-channel theory, showing that a
rate-channel compression protocol attains the optimal rate-distortion function
for a specific distortion measure and level.","['cs.IT', 'math.IT', 'quant-ph']",2501.17673," Enhancement of radiative coupling efficiency between out-of-plane excitonic
emitters in an indium selenide (InSe) film and an integrated waveguide formed
by silicon (Si) Mie-resonant nanodisks is experimentally studied.
Photoluminescence power at the resonant waveguide output is increased by~2.5
times at 950~nm in comparison with the case of a conventional rib waveguide of
the same geometrical parameters due to the efficient excitation of Mie-type
magnetic dipole resonances in individual nanoparticles. These results show
inspiring possibilities for creating new on-chip light emitters for various
integrated photonics applications.","['physics.optics', 'cond-mat.mes-hall']",False,,,,When Wyner and Ziv Met Bayes in Quantum-Classical Realm,"Mie-resonant silicon waveguide for efficient coupling with excitonic
  emitters in InSe"
neg-d2-486,2025-01-13,,2501.07724," Introduced with the advent of statistical wireless channel models for high
mobility communications and having a profound role in communication-centric
(CC) integrated sensing and communications (ISAC), the doubly-dispersive (DD)
channel structure has long been heralded as a useful tool enabling the capture
of the most important fading effects undergone by an arbitrary time-domain
transmit signal propagating through some medium. However, the incorporation of
this model into multiple-input multiple-output (MIMO) system setups, relying on
the recent paradigm-shifting transceiver architecture based on stacked
intelligent metasurfaces (SIM), in an environment with reconfigurable
intelligent surfaces (RISs) remains an open problem due to the many intricate
details that have to be accounted for. In this paper, we fill this gap by
introducing a novel DD MIMO channel model that incorporates an arbitrary number
of RISs in the ambient, as well as SIMs equipping both the transmitter and
receiver. We then discuss how the proposed metasurfaces-parametrized DD (MPDD)
channel model can be seamlessly applied to waveforms that are known to perform
well in DD environments, namely, orthogonal frequency division multiplexing
(OFDM), orthogonal time frequency space (OTFS), and affine frequency division
multiplexing (AFDM), with each having their own inherent advantages and
disadvantages. An illustrative application of the programmable functionality of
the proposed model is finally presented to showcase its potential for boosting
the performance of the aforementioned waveforms. Our numerical results indicate
that the design of waveforms suitable to mitigating the effects of DD channels
is significantly impacted by the emerging SIM technology.",['eess.SP'],2503.1535," In the early stages of star formation, boundary layer accretion, where
protostars accrete material from disks extending down to their surfaces, plays
a crucial role. Understanding how a magneto-rotational-instability (MRI)-active
disk connects to a protostar's surface remains a significant challenge. To
investigate the mechanisms of mass and angular momentum transfer, we develop a
global, three-dimensional magnetohydrodynamic model of boundary layer accretion
around a magnetized, convective low-mass protostar. Our results reveal that
angular momentum transport mechanisms transition significantly from the outer
MRI-active disk to the protostellar surface. Various mechanisms--MRI, spiral
shocks, coronal accretion, jets, and disk winds--contribute to angular momentum
transfer, resulting in three distinct disk structures: (1) the MRI-active disk,
(2) the transition layer, and (3) the boundary layer. The simulated protostar
is strongly magnetized due to the accumulation of the disk fields, wrapping by
disk toroidal fields, and stellar dynamo activity. Magnetic concentrations
analogous to starspots form on the protostar and interact with the rotating
disk gas to generate spiral shocks. These shocks play a key role in driving
accretion. These findings demonstrate the necessity of global MHD models for a
comprehensive understanding of angular momentum transport. Additionally, we
identify explosive events triggered by magnetic reconnection in both the
protostar and the disk atmosphere. We also find decretion flows in the disk
midplane, which may be important for the radial transport of refractory
materials, such as Calcium-Aluminium-rich Inclusions (CAIs) precursor gas, to
the outer disk.","['astro-ph.SR', 'astro-ph.EP', 'astro-ph.HE']",False,,,,"A Doubly-Dispersive MIMO Channel Model Parametrized with Stacked
  Intelligent Metasurfaces","Connecting a Magnetized Disk to a Convective Low-mass Protostar: A
  Global Three-dimensional Model of Boundary Layer Accretion"
neg-d2-487,2025-03-08,,2503.06144," A fundamental limitation of traditional Neural Networks (NN) in predictive
modelling is their inability to quantify uncertainty in their outputs. In
critical applications like positioning systems, understanding the reliability
of predictions is critical for constructing confidence intervals, early warning
systems, and effectively propagating results. For instance, Precise Point
Positioning in satellite navigation heavily relies on accurate error models for
ancillary data (orbits, clocks, ionosphere, and troposphere) to compute precise
error estimates. In addition, these uncertainty estimates are needed to
establish robust protection levels in safety critical applications.
  To address this challenge, the main objectives of this paper aims at
exploring a potential framework capable of providing both point estimates and
associated uncertainty measures of ionospheric Vertical Total Electron Content
(VTEC). In this context, Probabilistic Neural Networks (PNNs) offer a promising
approach to achieve this goal. However, constructing an effective PNN requires
meticulous design of hidden and output layers, as well as careful definition of
prior and posterior probability distributions for network weights and biases.
  A key finding of this study is that the uncertainty provided by the PNN model
in VTEC estimates may be systematically underestimated. In low-latitude areas,
the actual error was observed to be as much as twice the model's estimate. This
underestimation is expected to be more pronounced during solar maximum,
correlating with increased VTEC values.","['eess.SP', 'cs.AI']",2501.13336," Adversarial training and adversarial purification are two effective and
practical defense methods to enhance a model's robustness against adversarial
attacks. However, adversarial training necessitates additional training, while
adversarial purification suffers from low time efficiency. More critically,
current defenses are designed under the perturbation-based adversarial threat
model, which is ineffective against the recently proposed unrestricted
adversarial attacks. In this paper, we propose an effective and efficient
adversarial defense method that counters both perturbation-based and
unrestricted adversarial attacks. Our defense is inspired by the observation
that adversarial attacks are typically located near the decision boundary and
are sensitive to pixel changes. To address this, we introduce adversarial
anti-aliasing to mitigate adversarial modifications. Additionally, we propose
adversarial super-resolution, which leverages prior knowledge from clean
datasets to benignly recover images. These approaches do not require additional
training and are computationally efficient without calculating gradients.
Extensive experiments against both perturbation-based and unrestricted
adversarial attacks demonstrate that our defense method outperforms
state-of-the-art adversarial purification methods.","['cs.CV', 'eess.IV']",False,,,,"Exploring the usage of Probabilistic Neural Networks for Ionospheric
  electron density estimation",Gradient-Free Adversarial Purification with Diffusion Models
neg-d2-488,2025-02-20,,2502.15125," Let $T(f)$ denote the Littlewood--Paley square operators, including the
Littlewood--Paley $\mathcal{G}$-function $\mathcal{G}(f)$, Lusin's area
integral $\mathcal{S}(f)$ and Stein's function
$\mathcal{G}^{\ast}_{\lambda}(f)$ with $\lambda>2$. We establish the
boundedness of Littlewood--Paley square operators on the weighted spaces
$\mathrm{BMO}(\omega)$ with $\omega\in A_1$. The weighted space
$\mathrm{BLO}(\omega)$ (the space of functions with bounded lower oscillation)
is introduced and studied in this paper. This new space is a proper subspace of
$\mathrm{BMO}(\omega)$. It is proved that if $T(f)(x_0)$ is finite for a single
point $x_0\in\mathbb R^n$, then $T(f)(x)$ is finite almost everywhere in
$\mathbb R^n$. Moreover, it is shown that $T(f)$ is bounded from
$\mathrm{BMO}(\omega)$ into $\mathrm{BLO}(\omega)$, provided that $\omega\in
A_1$. The corresponding John--Nirenberg inequality suitable for the space
$\mathrm{BLO}(\omega)$ with $\omega\in A_1$ is discussed. Based on this, the
equivalent characterization of the space $\mathrm{BLO}(\omega)$ is also given.","['math.CA', 'math.FA']",2501.08234," This paper addresses a critical challenge in the high-speed passenger railway
industry: designing effective dynamic pricing strategies in the context of
competing and cooperating operators. To address this, a multi-agent
reinforcement learning (MARL) framework based on a non-zero-sum Markov game is
proposed, incorporating random utility models to capture passenger decision
making. Unlike prior studies in areas such as energy, airlines, and mobile
networks, dynamic pricing for railway systems using deep reinforcement learning
has received limited attention. A key contribution of this paper is a
parametrisable and versatile reinforcement learning simulator designed to model
a variety of railway network configurations and demand patterns while enabling
realistic, microscopic modelling of user behaviour, called RailPricing-RL. This
environment supports the proposed MARL framework, which models heterogeneous
agents competing to maximise individual profits while fostering cooperative
behaviour to synchronise connecting services. Experimental results validate the
framework, demonstrating how user preferences affect MARL performance and how
pricing policies influence passenger choices, utility, and overall system
dynamics. This study provides a foundation for advancing dynamic pricing
strategies in railway systems, aligning profitability with system-wide
efficiency, and supporting future research on optimising pricing policies.","['cs.LG', 'cs.AI', 'cs.MA']",False,,,,Weighted BMO-BLO estimates for Littlewood--Paley square operators,"Dynamic Pricing in High-Speed Railways Using Multi-Agent Reinforcement
  Learning"
neg-d2-489,2025-03-18,,2503.14237," Popular video training methods mainly operate on a fixed number of tokens
sampled from a predetermined spatiotemporal grid, resulting in sub-optimal
accuracy-computation trade-offs due to inherent video redundancy. They also
lack adaptability to varying computational budgets for downstream tasks,
hindering applications of the most competitive model in real-world scenes. We
thus propose a new test setting, Token Optimization, for maximized input
information across budgets, which optimizes the size-limited set of input
tokens through token selection from more suitably sampled videos. To this end,
we propose a novel augmentation tool termed Flux. By making the sampling grid
flexible and leveraging token selection, it is easily adopted in most popular
video training frameworks, boosting model robustness with nearly no additional
cost. We integrate Flux in large-scale video pre-training, and the resulting
FluxViT establishes new state-of-the-art results across extensive tasks at
standard costs. Notably, with 1/4 tokens only, it can still match the
performance of previous state-of-the-art models with Token Optimization,
yielding nearly 90\% savings. All models and data are available at
https://github.com/OpenGVLab/FluxViT.",['cs.CV'],2502.09485," We develop a geometric flow framework to investigate the following two
classical shape functionals: the torsional rigidity and the first Dirichlet
eigenvalue of the Laplacian. First, by constructing novel deformation paths
governed by stretching flows, we prove several new monotonicity properties of
the torsional rigidity and the first eigenvalue along the evolutions restricted
to triangles and rhombuses. These results also lead to new and simpler proofs
of some known results, unifying and extending prior symmetrization-based
proofs. Second, utilizing the mean curvature flow, we give a new proof of the
Saint-Venant inequality for smooth convex bodies. This might represent the
first flow-based proof to establish geometric functional inequalities that
couple both the domain and the state function associated with it. Third, by
discovering a gradient norm inequality for the sides of rectangles, we prove
monotonicity and stronger rigidity results of the torsional rigidity on
rectangles.",['math.AP'],False,,,,Make Your Training Flexible: Towards Deployment-Efficient Video Models,Flow approach on the monotonicity of shape functionals
neg-d2-490,2025-03-16,,2503.12647," Molecular communication (MC) offers a groundbreaking approach to
communication inspired by biological signaling. It is particularly suited for
environments where traditional electromagnetic methods fail, such as fluid
mediums or within the human body. This study focuses on addressing a major
challenge in MC systems: inter symbol interference (ISI), which arises due to
the random, diffusive propagation of molecules. We propose a novel technique
that leverages transmission shaping to mitigate ISI effectively by designing
optimal transmission pulse (or sequence) for symbols. Our approach centers on
solving a multi-objective optimization problem that aims to maximize the
separability of individual symbol's responses within the symbol duration while
matching the interference caused by molecular spillover for all symbols. By
making ISI of each symbol similar, the approach reduces the effect of previous
symbols and thus not require any adaptive computations. We introduce a
geometric analogy involving two families of ellipses to derive the optimal
solution. Analytical insights are supported by numerical simulations to design
optimized transmission profiles to enhance the resilience toward ISI. The
proposed transmission shaping method is evaluated through symbol error rate
(SER). These results mark a significant step forward in developing robust and
efficient MC systems, opening doors to advanced applications in bio-inspired
and nano-scale communication technologies.","['q-bio.MN', 'eess.SP']",2501.03859," In this paper, we present a novel synergistic framework for learning shape
estimation and a shape-aware whole-body control policy for tendon-driven
continuum robots. Our approach leverages the interaction between two Augmented
Neural Ordinary Differential Equations (ANODEs) -- the Shape-NODE and
Control-NODE -- to achieve continuous shape estimation and shape-aware control.
The Shape-NODE integrates prior knowledge from Cosserat rod theory, allowing it
to adapt and account for model mismatches, while the Control-NODE uses this
shape information to optimize a whole-body control policy, trained in a Model
Predictive Control (MPC) fashion. This unified framework effectively overcomes
limitations of existing data-driven methods, such as poor shape awareness and
challenges in capturing complex nonlinear dynamics. Extensive evaluations in
both simulation and real-world environments demonstrate the framework's robust
performance in shape estimation, trajectory tracking, and obstacle avoidance.
The proposed method consistently outperforms state-of-the-art end-to-end,
Neural-ODE, and Recurrent Neural Network (RNN) models, particularly in terms of
tracking accuracy and generalization capabilities.",['cs.RO'],False,,,,"Optimal Transmission Sequence Design with ISI Matching in Molecular
  Communication","A Synergistic Framework for Learning Shape Estimation and Shape-Aware
  Whole-Body Control Policy for Continuum Robots"
neg-d2-491,2025-03-12,,2503.09476," In this paper,we propose a Multi-Objective Sequential Quadratic Programming
(MOSQP) algorithm for constrained multi-objective optimization problems,basd on
a low-order smooth penalty function as the merit function for line search. The
algorithm constructs single-objective optimization subproblems based on each
objective function, solves quadratic programming (QP) subproblems to obtain
descent directions for expanding the iterative point set within the feasible
region, and filters non-dominated points after expansion. A new QP problem is
then formulated using information from all objective functions to derive
descent directions. The Armijo step size rule is employed for line search,
combined with Powell's correction formula (1978) for B iteration updates. If QP
subproblems is infesible, the negative gradient of the merit function is
adopted as the search direction. The algorithm is proven to converge to an
approximate Pareto front for constrained multi-objective optimization. Finally,
numerical experiments are performed for specific multi-objective optimization
problems.",['math.OC'],2502.11071," The paper gives a bound on the generalization error of the Gibbs algorithm,
which recovers known data-independent bounds for the high temperature range and
extends to the low-temperature range, where generalization depends critically
on the data-dependent loss-landscape. It is shown, that with high probability
the generalization error of a single hypothesis drawn from the Gibbs posterior
decreases with the total prior volume of all hypotheses with similar or smaller
empirical error. This gives theoretical support to the belief in the benefit of
flat minima. The zero temperature limit is discussed and the bound is extended
to a class of similar stochastic algorithms.","['cs.LG', 'stat.ML']",False,,,,"A Multi-objective Sequential Quadratic Programming Algorithm Based on
  Low-order Smooth Penalty Function","Generalization of the Gibbs algorithm with high probability at low
  temperatures"
neg-d2-492,2025-03-10,,2503.07355," This paper contains a review of the theoretical foundations of Clifford
algebras, spinors and spinor bundles in the so-called co-frame formalism. A
compact index-free notation is introduced, along with a series of identities
useful for computations in supergravity theories.","['math-ph', 'hep-th', 'math.MP']",2501.11089," Strontium titanate (STO) possesses promising properties for applications in
thermoelectricity, catalysis, fuel cells, and more, but its performance is
highly dependent on stoichiometry and impurity levels. While atom probe
tomography (APT) can provide detailed three-dimensional atomic-scale chemical
information, STO specimens have been challenging to analyze due to premature
specimen fracture. In this study, we show that by applying a thin metal coating
to atom probe tips, STO specimens can be analyzed with nearly 100% success.
Using this approach, we investigate both undoped STO and 1 at% Nb-doped STO,
achieving sufficient sensitivity to detect Nb concentrations as low as 0.7 at%.
This work establishes a reliable APT method for high-resolution chemical
analysis of STO at the nanoscale.",['cond-mat.mtrl-sci'],False,,,,Tools for Supergravity in the spin coframe formalism,"Advancing Atom Probe Tomography of SrTiO$_3$: Measurement Methodology
  and Impurity Detection Limits"
neg-d2-493,2025-01-30,,2501.1878," Collision-resistant cryptographic hash functions (CRHs) are crucial for
security in modern systems but are optimized for standard CPUs. While heavily
used in zero-knowledge proof (ZKP) applications, traditional CRHs are
inefficient in the ZK domain. ZK-friendly hashes have been developed but
struggle on consumer hardware due to a lack of specialized ZK-specific
hardware. To address this, we present HashEmAll, a novel collection of
FPGA-based realizations of three ZK-friendly hash functions: Griffin,
Rescue-Prime, and Reinforced Concrete. Each hash offers different optimization
focuses, allowing users to choose based on the constraints of their
applications. Through our ZK-optimized arithmetic functions on reconfigurable
hardware, HashEmAll outperforms CPU implementations by up to $23\times$ with
lower power consumption and compatibility with accessible FPGAs.","['cs.CR', 'cs.AR']",2501.04866," M dwarfs are the most common stars in the galaxy, with long lifespans, a high
occurrence rate of rocky planets, and close-in habitable zones. However, high
stellar activity in the form of frequent flaring and any associated coronal
mass ejections may drive atmospheric escape with the bombardment of radiation
and high-energy particles, drastically impacting the habitability of these
systems. The stellar latitude where flares and coronal mass ejections occur
determines the space weather that exoplanets are subject to, with high-energy
particle events associated with equatorial flares producing significant
atmospheric erosion. However, the flaring latitudes for M dwarfs remain largely
unconstrained. To aid in the effort to locate these flaring regions we explore
the applicability of flare occultations using optical photometry to identify
the latitudes of flares. As a planet transits in front of an ongoing flare the
timing and geometry of the transit can be used to constrain the latitude and
longitude of the flare. We predict the probability of detecting an occultation
for known transiting planets and eclipsing binaries. From this, we estimate
3-22 detectable occultations exist within the TESS primary mission photometry,
with the majority occurring in eclipsing binary observations. To demonstrate
this technique, we analyze a candidate flare occultation event for the
eclipsing binary CM Draconis.","['astro-ph.EP', 'astro-ph.SR']",False,,,,"Gotta Hash 'Em All! Speeding Up Hash Functions for Zero-Knowledge Proof
  Applications",Identifying Flare Locations Through Exoplanet Transit Occultations
neg-d2-494,2025-01-12,,2501.06778," In this study, we analyze the observational images of a Konoplya-Zhidenko
rotating non-Kerr black hole, wherein a thin accretion disk, serving as the
sole background light source, is situated on the equatorial plane of the black
hole. The inner boundary of the thin accretion disk extends to the event
horizon, and the accretion material in the disk exhibits two different motion
behaviors, that is, it moves along the critical plunging orbit inside the
innermost stable circular orbit (ISCO) and follows the Keplerian orbit outside
the ISCO. The shadow image is captured on the imaging plane of a zero angular
momentum observer utilizing advanced fisheye camera ray-tracing techniques. The
results demonstrate that an image consistently reveals a dark region encircled
by a narrow photon ring, which is called the inner shadow. At low observation
inclination angles, the observation intensity is highly concentrated, with the
lensed image of accretion disk being superimposed on the direct image. As
observation inclination angle increases, the direct and lensed images gradually
separate, becoming distinctly distinguishable and forming a hat-like structure.
Furthermore, variations in the parameter space and observation angle will
influence pertinent image characteristics, including image symmetry, the range
or deformation degree of the inner shadow. We further examined the distinctive
characteristics of images observed in both prograde and retrograde accretion
disk scenarios. Subsequently, we also examined the redshift distribution on the
disk. The findings indicate that while variations in relevant parameters do
influence the redshift distribution, the primary factor is the change in
observational inclination. The observer can detect both redshift and blueshift
phenomena on the screen when viewed at a higher observation angle.","['astro-ph.HE', 'gr-qc']",2501.04279," In daily domestic settings, frequently used objects like cups often have
unfixed positions and multiple instances within the same category, and their
carriers frequently change as well. As a result, it becomes challenging for a
robot to efficiently navigate to a specific instance. To tackle this challenge,
the robot must capture and update scene changes and plans continuously.
However, current object navigation approaches primarily focus on the semantic
level and lack the ability to dynamically update scene representation. In
contrast, this paper captures the relationships between frequently used objects
and their static carriers. It constructs an open-vocabulary
Carrier-Relationship Scene Graph (CRSG) and updates the carrying status during
robot navigation to reflect the dynamic changes of the scene. Based on the
CRSG, we further propose an instance navigation strategy that models the
navigation process as a Markov Decision Process. At each step, decisions are
informed by the Large Language Model's commonsense knowledge and
visual-language feature similarity. We designed a series of long-sequence
navigation tasks for frequently used everyday items in the Habitat simulator.
The results demonstrate that by updating the CRSG, the robot can efficiently
navigate to moved targets. Additionally, we deployed our algorithm on a real
robot and validated its practical effectiveness. The project page can be found
here: https://OpenIN-nav.github.io.",['cs.RO'],False,,,,"Optical appearance of the Konoplya-Zhidenko rotating non-Kerr black hole
  surrounded by a thin accretion disk","OpenIN: Open-Vocabulary Instance-Oriented Navigation in Dynamic Domestic
  Environments"
neg-d2-495,2025-03-08,,2503.06396," The problem of finding a minimum vertex cover (MVC) in a graph is a
well-known NP-hard problem with significant practical applications in
optimization and scheduling. Its complexity, combined with the increasing scale
of problems, underscores the need for efficient and effective algorithms.
However, existing heuristic algorithms for MVC often rely on simplistic
initialization strategies and overlook the impact of edge attributes and
neighborhood information on vertex selection. In this paper, we introduce
GCNIVC, a novel heuristic search algorithm designed to address the limitations
of existing methods for solving MVC problems in large-scale graphs. Our
approach features two main innovations. First, it utilizes a Graph
Convolutional Network (GCN) to capture the global structure of graphs, which
enables the generation of high-quality initial solutions that enhance the
efficiency of the subsequent search process. Second, GCNIVC introduces a new
heuristic that employs three containers and the concept of double-covered edges
(dc-edges), improving search efficiency and providing greater flexibility for
adding and removing operations based on edge attributes. Through extensive
experiments on benchmark datasets, we demonstrate that GCNIVC outperforms
state-of-the-art MVC algorithms in terms of both accuracy and efficiency. Our
results highlight the effectiveness of GCNIVC's GCN-assisted initialization and
its edge-informed search strategy. This study not only advances the
understanding of MVC problem-solving but also contributes a new tool for
addressing large-scale graph optimization challenges.",['cs.AI'],2503.17813," Current frameworks for evaluating security bug severity, such as the Common
Vulnerability Scoring System (CVSS), prioritize the ratio of exploitability to
impact. This paper suggests that the above approach measures the ""known knowns""
but inadequately addresses the ""known unknowns"" especially when there exist
multiple possible exploit paths and side effects, which introduce significant
uncertainty. This paper introduces the concept of connectedness, which measures
how strongly a security bug is connected with different entities, thereby
reflecting the uncertainty of impact and the exploit potential. This work
highlights the critical but underappreciated role connectedness plays in
severity assessments.",['cs.CR'],False,,,,"Optimizing Minimum Vertex Cover Solving via a GCN-assisted Heuristic
  Algorithm","Connectedness: a dimension of security bug severity assessment for
  measuring uncertainty"
neg-d2-496,2025-03-15,,2503.12301," Large Language Models (LLMs) have made significant strides in generating
human-like responses, largely due to preference alignment techniques. However,
these methods often assume unbiased human feedback, which is rarely the case in
real-world scenarios. This paper introduces Content-Aware Noise-Resilient
Preference Optimization (CNRPO), a novel framework that addresses multiple
sources of content-dependent noise in preference learning. CNRPO employs a
multi-objective optimization approach to separate true preferences from
content-aware noises, effectively mitigating their impact. We leverage backdoor
attack mechanisms to efficiently learn and control various noise sources within
a single model. Theoretical analysis and extensive experiments on different
synthetic noisy datasets demonstrate that CNRPO significantly improves
alignment with primary human preferences while controlling for secondary noises
and biases, such as response length and harmfulness.","['cs.LG', 'cs.CL']",2503.05214," The growing rate of chronic wound occurrence, especially in patients with
diabetes, has become a concerning trend in recent years. Chronic wounds are
difficult and costly to treat, and have become a serious burden on health care
systems worldwide. Chronic wounds can have devastating consequences for the
patient, with infection often leading to reduced quality of life and increased
mortality risk. Innovative deep learning methods for the detection and
monitoring of such wounds have the potential to reduce the impact to both
patient and clinician. We present a novel multimodal segmentation method which
allows for the introduction of patient metadata into the training workflow
whereby the patient data are expressed as Gaussian random fields. Our results
indicate that the proposed method improved performance when utilising multiple
models, each trained on different metadata categories. Using the Diabetic Foot
Ulcer Challenge 2022 test set, when compared to the baseline results
(intersection over union = 0.4670, Dice similarity coefficient = 0.5908) we
demonstrate improvements of +0.0220 and +0.0229 for intersection over union and
Dice similarity coefficient respectively. This paper presents the first study
to focus on integrating patient data into a chronic wound segmentation
workflow. Our results show significant performance gains when training
individual models using specific metadata categories, followed by average
merging of prediction masks using distance transforms. All source code for this
study is available at:
https://github.com/mmu-dermatology-research/multimodal-grf","['eess.IV', 'cs.CV']",False,,,,"One Goal, Many Challenges: Robust Preference Optimization Amid
  Content-Aware and Multi-Source Noise","Gaussian Random Fields as an Abstract Representation of Patient Metadata
  for Multimodal Medical Image Segmentation"
neg-d2-497,2025-02-27,,2502.20342," Sensing technology is an important aspect of information processing. Current
development in artificial intelligence systems (especially those aimed at
medical and environmental applications) requires a lot of data on the chemical
composition of biological fluids or environmental samples. These complex
matrices require advanced sensing devices, and photoelectrochemical ones seem
to have potential to overcome at least some of the obstacles. Furthermore, the
development of artificial intelligence (AI) technology for autonomous robotics
requires technology mimicking human senses, also those operating at the
molecular level, such as gustation and olfaction. Again, photoelectrochemical
sensing can provide some suitable solutions. In this review, we introduce the
idea of integration of photoelectrochemical sensors with some unconventional
computing paradigm - reservoir computing. This approach should not only boost
the performance of the sensors itself, but also open new pathways through
science. Integration of sensing devices with computing systems will also
contribute to a better understanding (or at least mimicking) of the human
senses and neuromorphic sensory information processing. Although reservoir
systems can be considered magic ""black boxes"" and their operation is at the
same time simple and hard to comprehend, this combination is expected to open a
new era of effective information harvesting and processing systems.","['physics.ins-det', 'cs.ET']",2503.17201," The commodity and widespread use of online shopping are having an
unprecedented impact on climate, with emission figures from key actors that are
easily comparable to those of a large-scale metropolis. Despite online shopping
being fueled by recommender systems (RecSys) algorithms, the role and potential
of the latter in promoting more sustainable choices is little studied. One of
the main reasons for this could be attributed to the lack of a dataset
containing carbon footprint emissions for the items. While building such a
dataset is a rather challenging task, its presence is pivotal for opening the
doors to novel perspectives, evaluations, and methods for RecSys research. In
this paper, we target this bottleneck and study the environmental role of
RecSys algorithms. First, we mine a dataset that includes carbon footprint
emissions for its items. Then, we benchmark conventional RecSys algorithms in
terms of accuracy and sustainability as two faces of the same coin. We find
that RecSys algorithms optimized for accuracy overlook greenness and that
longer recommendation lists are greener but less accurate. Then, we show that a
simple reranking approach that accounts for the item's carbon footprint can
establish a better trade-off between accuracy and greenness. This reranking
approach is modular, ready to use, and can be applied to any RecSys algorithm
without the need to alter the underlying mechanisms or retrain models. Our
results show that a small sacrifice of accuracy can lead to significant
improvements of recommendation greenness across all algorithms and list
lengths. Arguably, this accuracy-greenness trade-off could even be seen as an
enhancement of user satisfaction, particularly for purpose-driven users who
prioritize the environmental impact of their choices. We anticipate this work
will serve as the starting point for studying RecSys for more sustainable
recommendations.",['cs.IR'],False,,,,"Reservoir Computing and Photoelectrochemical Sensors: A Marriage of
  Convenience","Towards Carbon Footprint-Aware Recommender Systems for Greener Item
  Recommendation"
neg-d2-498,2025-03-11,,2503.08661," This paper proposes a task-oriented co-design framework that integrates
communication, computing, and control to address the key challenges of
bandwidth limitations, noise interference, and latency in mission-critical
industrial Cyber-Physical Systems (CPS). To improve communication efficiency
and robustness, we design a task-oriented Joint Source-Channel Coding (JSCC)
using Information Bottleneck (IB) to enhance data transmission efficiency by
prioritizing task-specific information. To mitigate the perceived End-to-End
(E2E) delays, we develop a Delay-Aware Trajectory-Guided Control Prediction
(DTCP) strategy that integrates trajectory planning with control prediction,
predicting commands based on E2E delay. Moreover, the DTCP is co-designed with
task-oriented JSCC, focusing on transmitting task-specific information for
timely and reliable autonomous driving. Experimental results in the CARLA
simulator demonstrate that, under an E2E delay of 1 second (20 time slots), the
proposed framework achieves a driving score of 48.12, which is 31.59 points
higher than using Better Portable Graphics (BPG) while reducing bandwidth usage
by 99.19%.","['cs.IT', 'cs.CV', 'eess.IV', 'math.IT']",2502.19921," Deep learning models lack shift invariance, making them sensitive to input
shifts that cause changes in output. While recent techniques seek to address
this for images, our findings show that these approaches fail to provide
shift-invariance in time series, where the data generation mechanism is more
challenging due to the interaction of low and high frequencies. Worse, they
also decrease performance across several tasks. In this paper, we propose a
novel differentiable bijective function that maps samples from their
high-dimensional data manifold to another manifold of the same dimension,
without any dimensional reduction. Our approach guarantees that samples -- when
subjected to random shifts -- are mapped to a unique point in the manifold
while preserving all task-relevant information without loss. We theoretically
and empirically demonstrate that the proposed transformation guarantees
shift-invariance in deep learning models without imposing any limits to the
shift. Our experiments on six time series tasks with state-of-the-art methods
show that our approach consistently improves the performance while enabling
models to achieve complete shift-invariance without modifying or imposing
restrictions on the model's topology. The source code is available on
\href{https://github.com/eth-siplab/Shifting-the-Paradigm}{GitHub}.",['cs.LG'],False,,,,"Task-Oriented Co-Design of Communication, Computing, and Control for
  Edge-Enabled Industrial Cyber-Physical Systems","Shifting the Paradigm: A Diffeomorphism Between Time Series Data
  Manifolds for Achieving Shift-Invariancy in Deep Learning"
neg-d2-499,2025-01-08,,2501.04946," The least trimmed squares (LTS) estimator is a renowned robust alternative to
the classic least squares estimator and is popular in location, regression,
machine learning, and AI literature. Many studies exist on LTS, including its
robustness, computation algorithms, extension to non-linear cases, asymptotics,
etc. The LTS has been applied in the penalized regression in a high-dimensional
real-data sparse-model setting where dimension $p$ (in thousands) is much
larger than sample size $n$ (in tens, or hundreds). In such a practical
setting, the sample size $n$ often is the count of sub-population that has a
special attribute (e.g. the count of patients of Alzheimer's, Parkinson's,
Leukemia, or ALS, etc.) among a population with a finite fixed size N.
Asymptotic analysis assuming that $n$ tends to infinity is not practically
convincing and legitimate in such a scenario. A non-asymptotic or finite sample
analysis will be more desirable and feasible.
  This article establishes some finite sample (non-asymptotic) error bounds for
estimating and predicting based on LTS with high probability for the first
time.","['stat.ML', 'cs.LG']",2502.14693," Recent advancements in large language models (LLMs) have shown remarkable
potential in automating machine learning tasks. However, existing LLM-based
agents often struggle with low-diversity and suboptimal code generation. While
recent work has introduced Monte Carlo Tree Search (MCTS) to address these
issues, limitations persist in the quality and diversity of thoughts generated,
as well as in the scalar value feedback mechanisms used for node selection. In
this study, we introduce Introspective Monte Carlo Tree Search (I-MCTS), a
novel approach that iteratively expands tree nodes through an introspective
process that meticulously analyzes solutions and results from parent and
sibling nodes. This facilitates a continuous refinement of the node in the
search tree, thereby enhancing the overall decision-making process.
Furthermore, we integrate a Large Language Model (LLM)-based value model to
facilitate direct evaluation of each node's solution prior to conducting
comprehensive computational rollouts. A hybrid rewarding mechanism is
implemented to seamlessly transition the Q-value from LLM-estimated scores to
actual performance scores. This allows higher-quality nodes to be traversed
earlier. Applied to the various ML tasks, our approach demonstrates a 6%
absolute improvement in performance compared to the strong open-source AutoML
agents, showcasing its effectiveness in enhancing agentic AutoML systems.
Resource available at https://github.com/jokieleung/I-MCTS",['cs.CL'],False,,,,"Non-asymptotic analysis of the performance of the penalized least
  trimmed squares in sparse models","I-MCTS: Enhancing Agentic AutoML via Introspective Monte Carlo Tree
  Search"
neg-d2-500,2025-03-21,,2503.16947," We review exceptional field theories as the duality-covariant reformulation
of maximal supergravity theories in ten and eleven dimensions, that make the
underlying exceptional symmetries explicit. Beyond their structural role in
unifying the various maximal supergravities, we illustrate how they also
provide access to very efficient techniques for tackling concrete computational
problems in supergravity.",['hep-th'],2503.01064," Large language models (LLMs) can answer questions and reason about complex
tasks, also from the scientific domain. We assess several multimodal LLMs
(MLLMs) on ScienceQA and find that Gemini models show the highest accuracy with
little context, and the highest textual similarity to human explanations with
richer context. Adapter-tuning of smaller MLLMs did not lead to any reliable
performance. Training from Gemini outputs consistently underperformed training
from the original data.","['cs.CL', 'cs.AI', 'cs.CV']",False,,,,Exceptional field theories,Scientific Reasoning: Assessment of Multimodal Generative LLMs
neg-d2-501,2025-01-26,,2501.15454," Exemplar-free class incremental learning (EF-CIL) is a nontrivial task that
requires continuously enriching model capability with new classes while
maintaining previously learned knowledge without storing and replaying any old
class exemplars. An emerging theory-guided framework for CIL trains
task-specific models for a shared network, shifting the pressure of forgetting
to task-id prediction. In EF-CIL, task-id prediction is more challenging due to
the lack of inter-task interaction (e.g., replays of exemplars). To address
this issue, we conduct a theoretical analysis of the importance and feasibility
of preserving a discriminative and consistent feature space, upon which we
propose a novel method termed DCNet. Concretely, it progressively maps class
representations into a hyperspherical space, in which different classes are
orthogonally distributed to achieve ample inter-class separation. Meanwhile, it
also introduces compensatory training to adaptively adjust supervision
intensity, thereby aligning the degree of intra-class aggregation. Extensive
experiments and theoretical analysis verified the superiority of the proposed
DCNet.",['cs.CV'],2503.10865," Predicting the outcomes of species invasions is a central goal of ecology, a
task made especially challenging due to ecological feedbacks. To address this,
we develop a general theory of ecological invasions applicable to a wide
variety of ecological models: including Lotka-Volterra models, consumer
resource models, and models with cross feeding. Importantly, our framework
remains valid even when invading evolved (non-random) communities and accounts
for invasion-driven species extinctions. We derive analytical expressions
relating invasion fitness to invader abundance, shifts in the community, and
extinction probabilities. These results can be understood through a new
quantity we term ``dressed invasion fitness'', which augments the traditional
notion of invasion fitness by incorporating ecological feedbacks. We apply our
theory to analyze short-term evolutionary dynamics through a series of
invasions by mutants whose traits are correlated with an existing parent. We
demonstrate that, generically, mutants and parents can coexist, often by
driving the extinction of low-abundance species. We validate theoretical
predictions against experimental datasets spanning ecosystems from plants to
microbial protists. Our work highlights the central role of ecological
feedbacks in shaping community responses to invasions and mutations, suggesting
that parent-mutant coexistence is widespread in eco-evolutionary dynamics.","['q-bio.PE', 'cond-mat.dis-nn', 'cond-mat.stat-mech']",False,,,,"On the Discrimination and Consistency for Exemplar-Free Class
  Incremental Learning","A theory of ecological invasions and its implications for
  eco-evolutionary dynamics"
neg-d2-502,2025-02-13,,2502.09236," The overarching, broad topic of my research are advancements in the area of
safety-critical, cyber-physical systems (CPS) development with emphasis on
validation and verification. The particular focus of my research is the early
validation of high-level requirements on CPS. My current approach for tackling
this problem is transforming the requirements into Event Calculus and
subsequently reasoning about them using ASP solvers such as the grounding-free
s(CASP). Below, I discuss my research, its current state, and the open issues
that are still left to tackle. The first results of my work will be presented
in a paper that was accepted for ICLP'24, which is my first paper in this area.","['cs.LO', 'cs.SE']",2503.12662," This paper introduces TuneNSearch, a hybrid transfer learning and local
search approach for addressing different variants of vehicle routing problems
(VRP). Recently, multi-task learning has gained much attention for solving VRP
variants. However, this adaptability often compromises the performance of the
models. To address this challenge, we first pre-train a reinforcement learning
model on the multi-depot VRP, followed by a short fine-tuning phase to adapt it
to different variants. By leveraging the complexity of the multi-depot VRP, the
pre-trained model learns richer node representations and gains more
transferable knowledge compared to models trained on simpler routing problems,
such as the traveling salesman problem. TuneNSearch employs, in the first
stage, a Transformer-based architecture, augmented with a residual edge-graph
attention network to capture the impact of edge distances and residual
connections between layers. This architecture allows for a more precise capture
of graph-structured data, improving the encoding of VRP's features. After
inference, our model is also coupled with a second stage composed of a local
search algorithm, which yields substantial performance gains with minimal
computational overhead added. Results show that TuneNSearch outperforms many
existing state-of-the-art models trained for each VRP variant, requiring only
one-fifth of the training epochs. Our approach demonstrates strong
generalization, achieving high performance across different tasks,
distributions and problem sizes, thus addressing a long-standing gap in the
literature.",['cs.LG'],False,,,,Early Validation of High-level Requirements on Cyber-Physical Systems,"TuneNSearch: a hybrid transfer learning and local search approach for
  solving vehicle routing problems"
neg-d2-503,2025-03-04,,2503.03004," In this paper, we propose a new construction of vertex algebras using the
Deligne category. This approach provides a rigorous framework for defining the
so-called large $N$ vertex algebra, which has appeared in recent physics
literatures. We first define the notion of a vertex algebra in a symmetric
monoidal category and extend familiar constructions in ordinary vertex algebras
to this broader categorical context. As an application, we consider a
$\beta\gamma$ vertex algebra in the Deligne category and construct the large N
vertex algebra from it. We study some simple properties of this vertex algebra
and analyze a certain vertex Poisson algebra limit.","['math.QA', 'hep-th', 'math-ph', 'math.MP']",2502.12705," Two-dimensional (2D) layered nanomaterials heterostructures, arising from the
combination of 2D materials with other low-dimensional species, feature large
surface area to volume ratio, which provides a high density of active sites for
catalytic ap-plications and in particular for (photo)electrocatalysis (PEC).
Meanwhile, their unique electronic band structure and high electrical
conductivity enable efficient charge transfer (CT) between the active material
and the substrate, which is essential for catalytic activity. In recent years,
researchers have demonstrated the potential of a range of 2D material
interfaces, such as graphene, graphitic carbon nitride (g-C3N4), metal
chalcogenides (MCs), and MXenes, for (photo)electrocatalytic applica-tions. For
instance, MCs such as MoS2 and WS2 have shown excellent catalytic activity for
hydrogen evolution, while gra-phene and MXenes have been used for the reduction
of carbon dioxide to higher value chemicals. However, despite their great
potential, there are still major challenges that need to be addressed in order
to fully realize the potential of 2D materials for PEC. For example, their
stability under harsh reaction conditions, as well as their scalability for
large-scale production are important factors to be considered. Generating
heterojunctions (HJs) by combining 2D layered structures with other
na-nomaterials is a promising method to improve the photoelectrocatalytic
properties of the former. In this review, we inspect thoroughly the recent
literature, to demonstrate the significant potential that arises from utilizing
2D layered heterostructures in PEC processes across a broad spectrum of
applications, from energy conversion and storage to environmental remediation.
With the ongoing research and development, it is likely that the potential of
these materials will be fully expressed in the near future.",['cond-mat.mtrl-sci'],False,,,,Large $N$ Vertex Algebras via Deligne Category,2D Layered Heterojunctions for Photoelectrocatalysis
neg-d2-504,2025-02-25,,2502.1861," Isotopes in the region of the nuclear chart below $^{68}\mathrm{Ni}$ have
been the subject of intense experimental and theoretical effort due to the
potential onset of a new ``island of inversion'' when crossing the harmonic
oscillator subshell closure at $N = 40$. We have measured the masses of
$^{64-68}\textrm{Mn}$ using TITAN's multiple-reflection time-of-flight mass
spectrometer, resulting in the first precision mass measurements of
$^{67}\mathrm{Mn}$ and $^{68}\mathrm{Mn}$. These results are compared to
\textit{ab initio} calculations and modern shell model calculations and show an
increase in collectivity approaching $N=40$.",['nucl-ex'],2501.04968," $r$-mode oscillations in rotating neutron stars are a source of continuous
gravitational radiation. We investigate the excitation of $r$-modes by the
mechanical impact on the neutron star surface of stochastically accreted clumps
of matter, assuming that the Chandrasekhar-Friedman-Schutz instability is not
triggered. The star is idealised as a slowly-rotating, unmagnetised,
one-component fluid with a barotropic equation of state in Newtonian gravity.
It is found that the $r$-mode amplitude depends weakly on the equation of state
but sensitively on the rotation frequency $\nu_{\rm s}$. The gravitational wave
strain implicitly depends on the equation of state through the damping
timescale. The root-mean-square strain is $h_{\rm rms} \approx 10^{-35}
(\nu_{\rm s}/ 10 {\rm Hz})^{2} (R_*/10 {\rm km})^2 (\Delta t_{\rm acc}/1 {\rm
yr})^{1/2} (f_{\rm acc}/1 {\rm kHz})^{-1/2} (\dot{M}/10^{-8} \text{M}_{\odot}
\text{yr}^{-1}) (v/0.4c) (d/1 {\rm kpc})^{-1}$, which is comparable to the
strain from $g$-, $p$- and $f$-modes excited by stochastic accretion, where
$R_*$ is the radius of the star, $\Delta t_{\rm acc}$ is the uninterrupted
duration of an accretion episode, $f_{\rm acc}$ is the mean clump impact
frequency, $\dot{M}$ is the accretion rate, $v$ is the impact speed, and $d$ is
the distance of the star from the Earth. An observational test is proposed,
based on the temporal autocorrelation function of the gravitational wave
signal, to discern whether the Chandrasekhar-Friedman-Schutz instability
switches on and coexists with impact-excited $r$-modes before or during a
gravitational wave observation.","['astro-ph.HE', 'astro-ph.SR', 'gr-qc']",False,,,,"Exploring the Onset of Collectivity Approaching N=40 through Manganese
  Masses","Gravitational waves from r-mode oscillations of stochastically accreting
  neutron stars"
neg-d2-505,2025-02-03,,2502.01058," Leveraging the sensitive dependence of a giant atom's radiation rate on its
frequency [A. F. Kockum, $et~al$., Phys. Rev. A 90, 013837 (2014)], we propose
an effective magnetometer model based on single giant emitter. In this model,
the emitter's frequency is proportional to the applied bias magnetic field. The
self-interference effect causes the slope of the dissipation spectrum to vary
linearly with the number of emitter-coupling points. The giant emitter
magnetometer achieves a sensitivity as high as $10^{-8}-10^{-9}\,{\rm
T/\sqrt{Hz}}$, demonstrating the significant advantages of the
self-interference effect compared to small emitters. We hope our proposal will
expand the applications of giant emitters in precision measurement and
magnetometry.",['quant-ph'],2502.01679," Large Language Models (LLMs) have significantly advanced natural language
processing applications, yet their widespread use raises concerns regarding
inherent biases that may reduce utility or harm for particular social groups.
Despite the advancement in addressing LLM bias, existing research has two major
limitations. First, existing LLM bias evaluation focuses on the U.S. cultural
context, making it challenging to reveal stereotypical biases of LLMs toward
other cultures, leading to unfair development and use of LLMs. Second, current
bias evaluation often assumes models are familiar with the target social
groups. When LLMs encounter words beyond their knowledge boundaries that are
unfamiliar in their training data, they produce irrelevant results in the local
context due to hallucinations and overconfidence, which are not necessarily
indicative of inherent bias. This research addresses these limitations with a
Local Integrated Bias Recognition and Assessment Framework (LIBRA) for
measuring bias using datasets sourced from local corpora without crowdsourcing.
Implementing this framework, we develop a dataset comprising over 360,000 test
cases in the New Zealand context. Furthermore, we propose the Enhanced
Idealized CAT Score (EiCAT), integrating the iCAT score with a beyond knowledge
boundary score (bbs) and a distribution divergence-based bias measurement to
tackle the challenge of LLMs encountering words beyond knowledge boundaries.
Our results show that the BERT family, GPT-2, and Llama-3 models seldom
understand local words in different contexts. While Llama-3 exhibits larger
bias, it responds better to different cultural contexts. The code and dataset
are available at: https://github.com/ipangbo/LIBRA.","['cs.CY', 'cs.CL', 'cs.LG']",False,,,,Giant emitter magnetometer,LIBRA: Measuring Bias of Large Language Model from a Local Context
neg-d2-506,2025-03-13,,2503.10174," This paper presents a novel sensitivity-based distributed programming (SBDP)
approach for non-convex, large-scale nonlinear programs (NLP). The algorithm
relies on first-order sensitivities to cooperatively solve the central NLP in a
distributed manner with only neighbor-to-neighbor communication and
parallelizable local computations. The scheme is based on primal decomposition
and offers minimal algorithmic complexity. We derive sufficient local
convergence conditions for non-convex problems. Furthermore, we consider the
SBDP method in a distributed optimal control context and derive favorable
convergence properties in this setting. We illustrate these theoretical
findings and the performance of the proposed algorithm with simulations of
various distributed optimization and control problems.",['math.OC'],2502.03432," We present a formalization of Borel determinacy in the Lean 4 theorem prover.
The formalization includes a definition of Gale-Stewart games and a proof of
Martin's theorem stating that Borel games are determined. The proof closely
follows Martin's ""A purely inductive proof of Borel determinacy"".",['math.LO'],False,,,,Sensitivity-Based Distributed Programming for Non-Convex Optimization,A formalization of Borel determinacy in Lean
neg-d2-507,2025-02-05,,2502.03605," Device sizing is crucial for meeting performance specifications in
operational transconductance amplifiers (OTAs), and this work proposes an
automated sizing framework based on a transformer model. The approach first
leverages the driving-point signal flow graph (DP-SFG) to map an OTA circuit
and its specifications into transformer-friendly sequential data. A specialized
tokenization approach is applied to the sequential data to expedite the
training of the transformer on a diverse range of OTA topologies, under
multiple specifications. Under specific performance constraints, the trained
transformer model is used to accurately predict DP-SFG parameters in the
inference phase. The predicted DP-SFG parameters are then translated to
transistor sizes using a precomputed look-up table-based approach inspired by
the gm/Id methodology. In contrast to previous conventional or
machine-learning-based methods, the proposed framework achieves significant
improvements in both speed and computational efficiency by reducing the need
for expensive SPICE simulations within the optimization loop; instead, almost
all SPICE simulations are confined to the one-time training phase. The method
is validated on a variety of unseen specifications, and the sizing solution
demonstrates over 90% success in meeting specifications with just one SPICE
simulation for validation, and 100% success with 3-5 additional SPICE
simulations.",['cs.AR'],2503.14876," Supervised deep learning methods have been successful in the field of high
energy physics, and the trend within the field is to move away from high level
reconstructed variables to lower level, higher dimensional features. Supervised
methods require labelled data, which is typically provided by a simulator. As
the number of features increases, simulation accuracy decreases, leading to
greater domain shift between training and testing data when using lower-level
features. This work demonstrates that the classification without labels
paradigm can be used to remove the need for background simulation when training
supervised classifiers. This can result in classifiers with higher performance
on real data than those trained on simulated data.",['hep-ph'],False,,,,"Accelerating OTA Circuit Design: Transistor Sizing Based on a
  Transformer Model and Precomputed Lookup Tables",Strong CWoLa: Binary Classification Without Background Simulation
neg-d2-508,2025-02-13,,2502.09337," This thesis is an exposition of the author's contribution on effective
descent morphisms in various categories of generalized categorical structures.
It consists of: Chapter 1, where an elementary description of descent theory
and the content of each remaining chapter is provided, supplemented with
references; Chapter 2, consisting of various descent theoretical definitions
and results employed in the remainder of this work; four chapters, each
corresponding to an article written by the author during the period of his PhD
studies.",['math.CT'],2503.12885," Image-conditioned generation methods, such as depth- and canny-conditioned
approaches, have demonstrated remarkable abilities for precise image synthesis.
However, existing models still struggle to accurately control the content of
multiple instances (or regions). Even state-of-the-art models like FLUX and
3DIS face challenges, such as attribute leakage between instances, which limits
user control. To address these issues, we introduce DreamRenderer, a
training-free approach built upon the FLUX model. DreamRenderer enables users
to control the content of each instance via bounding boxes or masks, while
ensuring overall visual harmony. We propose two key innovations: 1) Bridge
Image Tokens for Hard Text Attribute Binding, which uses replicated image
tokens as bridge tokens to ensure that T5 text embeddings, pre-trained solely
on text data, bind the correct visual attributes for each instance during Joint
Attention; 2) Hard Image Attribute Binding applied only to vital layers.
Through our analysis of FLUX, we identify the critical layers responsible for
instance attribute rendering and apply Hard Image Attribute Binding only in
these layers, using soft binding in the others. This approach ensures precise
control while preserving image quality. Evaluations on the COCO-POS and
COCO-MIG benchmarks demonstrate that DreamRenderer improves the Image Success
Ratio by 17.7% over FLUX and enhances the performance of layout-to-image models
like GLIGEN and 3DIS by up to 26.8%. Project Page:
https://limuloo.github.io/DreamRenderer/.",['cs.CV'],False,,,,Some aspects of descent theory and applications,"DreamRenderer: Taming Multi-Instance Attribute Control in Large-Scale
  Text-to-Image Models"
neg-d2-509,2025-01-20,,2501.11876," Surface-from-gradients (SfG) aims to recover a three-dimensional (3D) surface
from its gradients. Traditional methods encounter significant challenges in
achieving high accuracy and handling high-resolution inputs, particularly
facing the complex nature of discontinuities and the inefficiencies associated
with large-scale linear solvers. Although recent advances in deep learning,
such as photometric stereo, have enhanced normal estimation accuracy, they do
not fully address the intricacies of gradient-based surface reconstruction. To
overcome these limitations, we propose a Fourier neural operator-based
Numerical Integration Network (FNIN) within a two-stage optimization framework.
In the first stage, our approach employs an iterative architecture for
numerical integration, harnessing an advanced Fourier neural operator to
approximate the solution operator in Fourier space. Additionally, a
self-learning attention mechanism is incorporated to effectively detect and
handle discontinuities. In the second stage, we refine the surface
reconstruction by formulating a weighted least squares problem, addressing the
identified discontinuities rationally. Extensive experiments demonstrate that
our method achieves significant improvements in both accuracy and efficiency
compared to current state-of-the-art solvers. This is particularly evident in
handling high-resolution images with complex data, achieving errors of fewer
than 0.1 mm on tested objects.",['cs.CV'],2502.05238," In this work, we study neutrino spin oscillations in the case when they are
gravitationally scattered off a rotating Kerr black hole surrounded by a thick
magnetized accretion disk. We consider only toroidal magnetic field inside the
disk. Neutrino spin precession is caused by the interaction of the neutrino
magnetic moment with the magnetic field in the disk. Our treatment of the spin
oscillations of the observed neutrino fluxes is based on numerical simulations
of the propagation of a large number of incoming test neutrinos using High
Performance Parallel Computing. We briefly discuss our results and their
applications in the observations of astrophysical neutrinos.","['hep-ph', 'astro-ph.HE', 'gr-qc']",False,,,,"FNIN: A Fourier Neural Operator-based Numerical Integration Network for
  Surface-form-gradients",Neutrino spin oscillations near a black hole
neg-d2-510,2025-03-05,,2503.04028," Stress-stress correlations in crystalline solids with long-range order can be
straightforwardly derived using elasticity theory. In contrast, the `emergent
elasticity' of amorphous solids, rigid materials characterized by an underlying
disordered structure, defies direct explanation within traditional theoretical
frameworks. To address this challenge, tensor gauge theories have been recently
proposed as a promising approach to describe the emergent elasticity of
disordered solids and predict their stress-stress correlations. In this work,
we revisit this problem in two-dimensional amorphous and crystalline solids by
employing a canonical elasticity theory approach, supported by experimental and
simulation data. We demonstrate that, with respect to static stress-stress
correlations, the response of a 2D disordered solid is indistinguishable from
that of a 2D isotropic crystalline solid and it is well predicted by vanilla
elasticity theory. Moreover, we show that the presence of pinch-point
singularities in the stress response is not an exclusive feature of amorphous
solids. Our results confirm previous observations about the universal character
of static stress-stress correlations in crystalline and amorphous packings.","['cond-mat.soft', 'cond-mat.mtrl-sci', 'cond-mat.stat-mech']",2502.11843," Large Language Models (LLMs) are widely used as conversational agents,
exploiting their capabilities in various sectors such as education, law,
medicine, and more. However, LLMs are often subjected to context-shifting
behaviour, resulting in a lack of consistent and interpretable
personality-aligned interactions. Adherence to psychological traits lacks
comprehensive analysis, especially in the case of dyadic (pairwise)
conversations. We examine this challenge from two viewpoints, initially using
two conversation agents to generate a discourse on a certain topic with an
assigned personality from the OCEAN framework (Openness, Conscientiousness,
Extraversion, Agreeableness, and Neuroticism) as High/Low for each trait. This
is followed by using multiple judge agents to infer the original traits
assigned to explore prediction consistency, inter-model agreement, and
alignment with the assigned personality. Our findings indicate that while LLMs
can be guided toward personality-driven dialogue, their ability to maintain
personality traits varies significantly depending on the combination of models
and discourse settings. These inconsistencies emphasise the challenges in
achieving stable and interpretable personality-aligned interactions in LLMs.","['cs.CL', 'cs.AI', 'cs.SI']",False,,,,"Stress-stress correlations in two-dimensional amorphous and crystalline
  solids",Can LLM Agents Maintain a Persona in Discourse?
neg-d2-511,2025-01-29,,2501.17464," We propose a new methodology to simulate the discounted penalty applied to a
wind-farm operator by violating ramp-rate limitation policies. It is assumed
that the operator manages a wind turbine plugged into a battery, which either
provides or stores energy on demand to avoid ramp-up and ramp-down events. The
battery stages, namely charging, discharging, or neutral, are modeled as a
semi-Markov process. During each charging/discharging period, the energy
stored/supplied is assumed to follow a modified Brownian bridge that depends on
three parameters. We prove the validity of our methodology by testing the model
on 10 years of real wind-power data and comparing real versus simulated
results.",['stat.AP'],2501.09839," We prove two theorems about tree-decompositions in the setting of coarse
graph theory. First, we show that a graph $G$ admits a tree-decomposition in
which each bag is contained in the union of a bounded number of balls of
bounded radius, if and only if $G$ admits a quasi-isometry to a graph with
bounded tree-width. (The ``if'' half is easy, but the ``only if'' half is
challenging.) This generalizes a recent result of Berger and Seymour,
concerning tree-decompositions when each bag has bounded radius.
  Second, we show that if $G$ admits a quasi-isometry $\phi$ to a graph $H$ of
bounded path-width, then $G$ admits a quasi-isometry (with error only an
additive constant) to a graph of bounded path-width. Indeed, we will show a
much stronger statement: that we can assign a non-negative integer length to
each edge of $H$, such that the same function $\phi$ is a quasi-isometry (with
error only an additive constant) to this weighted version of $H$.",['math.CO'],False,,,,"Modelling a storage system of a wind farm with a ramp-rate limitation: a
  semi-Markov modulated Brownian bridge approach",Coarse tree-width
neg-d2-512,2025-03-15,,2503.13529," This study investigates the impact of artificial intelligence (AI) technology
on cross-border trade using a qualitative content analysis approach. By
synthesizing existing empirical studies, we aim to quantify the overall effect
of AI on trade flows and identify the key moderating and mediating variables.
Besides, our results show that AI adoption significantly increases trade
volumes in Southeast Asia. Likewise, these effects are stronger in regions with
advanced technological infrastructure and favorable regulatory frameworks. In
addition, Trade firm size partially mediates the relationship between AI
technology and trade performance. Furthermore, this study draws on several key
theoretical frameworks that provide a comprehensive understanding of the
mechanisms through which AI technology is affecting cross-border trade in
Southeast Asia. The primary theories used in this research include the
technology, organization, and environment (TOE) framework, the diffuse
innovation (DOI) theory, Dynamic Capabilities Theory, Comparative Advantage
Theory, Network theory, Transaction Cost Economics (TCE), the resource-based
view, and the institution theory. Consequently, this study contributes to the
existing literature by providing a comprehensive analysis of the role of AI in
international trade and highlighting the importance of contextual factors in
maximizing the benefits of AI. Thus, our findings underscore the need for
favorable policies and robust infrastructure to facilitate AI-driven trade
growth. A discussion of limitations and future research directions will also be
part of the report in Southeast Asia Trade.","['econ.GN', 'q-fin.EC']",2501.01193," We contribute to the lively debate in current scholarship on the Leibnizian
calculus. In a recent text, Arthur and Rabouin argue that non-Archimedean
continua are incompatible with Leibniz's concepts of number, quantity and
magnitude.
  They allege that Leibniz viewed infinitesimals as contradictory, and claim to
deduce such a conclusion from an analysis of the Leibnizian definition of
quantity. However, their argument is marred by numerous errors, deliberate
omissions, and misrepresentations, stemming in a number of cases from flawed
analyses in their earlier publications.
  We defend the thesis, traceable to the classic study by Henk Bos, that
Leibniz used genuine infinitesimals, which he viewed as fictional mathematical
entities (and not merely shorthand for talk about more ordinary quantities) on
par with negatives and imaginaries.",['math.HO'],False,,,,"The impact of artificial intelligence technology on cross-border trade
  in Southeast Asia: A meta-analytic approach",Leibniz's contested infinitesimals: Further depictions
neg-d2-513,2025-01-06,,2501.02987," Wall shear stress (WSS) is a crucial hemodynamic quantity extensively studied
in cardiovascular research, yet its numerical computation is not
straightforward. This work aims to compare WSS results obtained from two
different finite element discretizations, quantify the differences between
continuous and discontinuous stresses, and introduce a novel method for WSS
evaluation through the formulation of a boundary-flux problem. Two benchmark
problems are considered - a 2D Stokes flow on a unit square and a 3D Poiseuille
flow through a cylindrical pipe. These are followed by investigations of
steady-state Navier-Stokes flow in two patient-specific aneurysms. The study
focuses on P1/P1 stabilized and Taylor-Hood P2/P1 mixed finite elements for
velocity and pressure. WSS is computed using either the proposed boundary-flux
method or as a projection of tangential traction onto First order Lagrange
(P1), Discontinuous Galerkin first order (DG-1), or Discontinuous Galerkin zero
order (DG-0) space. For the P1/P1 stabilized element, the boundary-flux and P1
projection methods yielded equivalent results. With the P2/P1 element, the
boundary-flux evaluation demonstrated faster convergence in the Poiseuille flow
example but showed increased sensitivity to pressure field inaccuracies in
patient-specific geometries compared to the projection method. In
patient-specific cases, the P2/P1 element exhibited superior robustness to mesh
size when evaluating average WSS and low shear area (LSA), outperforming the
P1/P1 stabilized element. Projecting discontinuous finite element results into
continuous spaces can introduce artifacts, such as the Gibbs phenomenon.
Consequently, it becomes crucial to carefully select the finite element space
for boundary stress calculations - not only in applications involving WSS
computations for aneurysms.","['math.NA', 'cs.NA']",2503.02345," The detection of Alzheimer disease (AD) from clinical MRI data is an active
area of research in medical imaging. Recent advances in quantum computing,
particularly the integration of parameterized quantum circuits (PQCs) with
classical machine learning architectures, offer new opportunities to develop
models that may outperform traditional methods. However, quantum machine
learning (QML) remains in its early stages and requires further experimental
analysis to better understand its behavior and limitations. In this paper, we
propose an end to end hybrid classical quantum convolutional neural network (CQ
CNN) for AD detection using clinically formatted 3D MRI data. Our approach
involves developing a framework to make 3D MRI data usable for machine
learning, designing and training a brain tissue segmentation model (Skull Net),
and training a diffusion model to generate synthetic images for the minority
class. Our converged models exhibit potential quantum advantages, achieving
higher accuracy in fewer epochs than classical models. The proposed beta8 3
qubit model achieves an accuracy of 97.50%, surpassing state of the art (SOTA)
models while requiring significantly fewer computational resources. In
particular, the architecture employs only 13K parameters (0.48 MB), reducing
the parameter count by more than 99.99% compared to current SOTA models.
Furthermore, the diffusion-generated data used to train our quantum models, in
conjunction with real samples, preserve clinical structural standards,
representing a notable first in the field of QML. We conclude that CQCNN
architecture like models, with further improvements in gradient optimization
techniques, could become a viable option and even a potential alternative to
classical models for AD detection, especially in data limited and resource
constrained clinical settings.","['quant-ph', 'cs.AI', 'cs.CV', 'cs.LG']",False,,,,"On the numerical evaluation of wall shear stress using the finite
  element method","CQ CNN: A Hybrid Classical Quantum Convolutional Neural Network for
  Alzheimer's Disease Detection Using Diffusion Generated and U Net Segmented
  3D MRI"
neg-d2-514,2025-03-18,,2503.14366," Variational quantum algorithms (VQAs) offer a promising approach to solving
computationally demanding problems by combining parameterized quantum circuits
with classical optimization. Estimating probabilistic outcomes on quantum
hardware requires repeated measurements (shots). However, in practice, the
limited shot budget introduces significant noise in the evaluation of the
objective function. Gradient estimation in VQAs often relies on the
finite-difference, which evaluates the noisy objective function at perturbed
circuit parameter values. The accuracy of this estimation is highly dependent
on the choice of step size for these perturbations. An inappropriate step size
can exacerbate the impact of noise, causing inaccurate gradient estimates and
hindering the classical optimization in VQAs. This paper proposes QuGStep, an
algorithm that addresses the challenge of determining the appropriate step size
for finite-difference gradient estimation under a shot budget. QuGStep is
grounded in a theorem that proves the optimal step size, which accounts for the
shot budget, minimizes the error bound in gradient estimation using finite
differences. Numerical experiments approximating the ground state energy of
several molecules demonstrate that QuGStep can identify the appropriate step
size for the given shot budget to obtain effective gradient estimation.
Notably, the step size identified by QuGStep achieved convergence to the ground
state energy with over 96% fewer shots compared to using a default step size.
These findings highlight the potential of QuGStep to improve the practical
deployment and scalability of quantum computing technologies.",['quant-ph'],2503.07754," The physical properties of stellar atmospheres in rapidly rotating massive
stars, such as Be stars, are critical to understanding their evolution and
their role as progenitors of supernovae. These stars, which often have
near-critical rotation, exhibit equatorial stretching and gravity darkening,
which significantly complicates the determination of parameters such as the
inclination angle. Be stars, characterized by their extreme rotational
velocities, serve as excellent candidates for exploring these phenomena.
However, fundamental quantities such as polar and equatorial radii and
inclination angles are typically derived from interferometry, which applies
only to a limited number of stars. This study aims to enhance the determination
of inclination angles for Be stars using the ZPEKTR spectral synthesis code. By
incorporating advanced models of gravity darkening and stellar deformation, we
evaluated the effectiveness of this method with a sample of ten Be stars from
the BeSOS database, comparing results with established interferometric data.
Methods. We used the ZPEKTR code to model the effects of stellar oblateness and
gravity darkening on spectral lines, focusing on the HeI 4471 line. We applied
a chi-squared test minimization approach to identify the best-fitting models,
and we evaluated the inclination angles derived against interferometric
measurements. Our analysis reveals a robust linear correlation between the
inclination angles derived from ZPEKTR and using interferometric techniques,
which demonstrates an excellent agreement. The ZPEKTR code effectively models
high rotational velocity effects, providing precise stellar parameter
determinations. The results underscore the potential of advanced spectroscopic
techniques to yield inclination measurements comparable to interferometry,
which offers a pathway to studying distant massive stars.",['astro-ph.SR'],False,,,,"QuGStep: Refining Step Size Selection in Gradient Estimation for
  Variational Quantum Algorithms",Unveiling stellar spin: Determining inclination angles in Be stars
neg-d2-515,2025-02-26,,2502.19152," We investigate the entanglement properties of the Quantum Six-Vertex Model on
a cylinder, focusing on the Shannon-Renyi entropy in the limit of Renyi order
$n = \infty$. This entropy, calculated from the ground state amplitudes of the
equivalent XXZ spin-1/2 chain, allows us to determine the Renyi entanglement
entropy of the corresponding Rokhsar-Kivelson wavefunctions, which describe the
ground states of certain conformal quantum critical points. Our analysis
reveals a novel logarithmic correction to the expected entanglement scaling
when the system size is odd. This anomaly arises from the geometric frustration
of spin configurations imposed by periodic boundary conditions on odd-sized
chains. We demonstrate that the scaling prefactor of this logarithmic term is
directly related to the compactification radius of the low-energy bosonic field
theory description, or equivalently, the Luttinger parameter. Thus, this
correction provides a direct probe of the underlying Conformal Field Theory
(CFT) describing the critical point. Our findings highlight the crucial role of
system size parity in determining the entanglement properties of this model and
offer insights into the interplay between geometry, frustration, and
criticality.","['quant-ph', 'cond-mat.str-el']",2503.11065," Deep reinforcement learning (DRL) has had success in virtual and simulated
domains, but due to key differences between simulated and real-world
environments, DRL-trained policies have had limited success in real-world
applications. To assist researchers to bridge the \textit{sim-to-real gap}, in
this paper, we describe a low-cost physical inverted pendulum apparatus and
software environment for exploring sim-to-real DRL methods. In particular, the
design of our apparatus enables detailed examination of the delays that arise
in physical systems when sensing, communicating, learning, inferring and
actuating. Moreover, we wish to improve access to educational systems, so our
apparatus uses readily available materials and parts to reduce cost and
logistical barriers. Our design shows how commercial, off-the-shelf electronics
and electromechanical and sensor systems, combined with common metal
extrusions, dowel and 3D printed couplings provide a pathway for affordable
physical DRL apparatus. The physical apparatus is complemented with a simulated
environment implemented using a high-fidelity physics engine and OpenAI Gym
interface.","['cs.LG', 'cs.AI', 'cs.RO', 'cs.SY', 'eess.SY']",False,,,,Oddities in the Entanglement Scaling of the Quantum Six-Vertex Model,"Low-cost Real-world Implementation of the Swing-up Pendulum for Deep
  Reinforcement Learning Experiments"
neg-d2-516,2025-01-23,,2501.141," Brown dwarfs lack nuclear fusion and cool with time; the coldest known have
an effective temperature below 500 K, and are known as Y dwarfs. We present a
James Webb Space Telescope (JWST) photometric dataset of Y dwarfs: twenty-three
were imaged in wide-field mode, 20 using NIRCam with the F150W and F480M
filters, and 3 using NIRISS with the F480M filter. We present an F480M vs.
F150W $-$ F480M color-magnitude diagram for our sample, and other brown dwarfs
with F150W and F480M colors synthesized from JWST spectra by Beiler et al.
(2024). For one target, WISEA J083011.95$+$283716.0, its detection in the
near-infrared confirms it as one of the reddest Y dwarfs known, with F150W $-$
F480M $= 9.62$ mag. We provide its updated parallax and proper motion. One of
the Beiler et al. Y dwarfs, CWISEP J104756.81+545741.6, is unusually blue,
consistent with strong CO absorption seen in its spectrum which the F480M
filter is particularly sensitive to. The strong CO and the kinematics of the
object suggest it may be very low-mass and young. We update the resolved
photometry for the close binary system WISE J033605.05$-$014350.4 AB, and find
that the secondary is almost as cold as WISE 085510.83$-$071442.5, with $T_{\rm
eff} \lesssim 300$ K, however the F150W $-$ F480M color is significantly bluer,
possibly suggesting the presence of water clouds. Astrometry is measured at the
JWST epoch for the sample which is consistent with parallax and proper motion
values reported by Kirkpatrick et al. (2021) and Marocco et al. (in prep).","['astro-ph.SR', 'astro-ph.EP']",2502.09236," The overarching, broad topic of my research are advancements in the area of
safety-critical, cyber-physical systems (CPS) development with emphasis on
validation and verification. The particular focus of my research is the early
validation of high-level requirements on CPS. My current approach for tackling
this problem is transforming the requirements into Event Calculus and
subsequently reasoning about them using ASP solvers such as the grounding-free
s(CASP). Below, I discuss my research, its current state, and the open issues
that are still left to tackle. The first results of my work will be presented
in a paper that was accepted for ICLP'24, which is my first paper in this area.","['cs.LO', 'cs.SE']",False,,,,JWST 1.5 {\mu}m and 4.8 {\mu}m Photometry of Y Dwarfs,Early Validation of High-level Requirements on Cyber-Physical Systems
neg-d2-517,2025-03-12,,2503.09402," Human daily activities can be concisely narrated as sequences of routine
events (e.g., turning off an alarm) in video streams, forming an event
vocabulary. Motivated by this, we introduce VLog, a novel video understanding
framework that define video narrations as vocabulary, going beyond the typical
subword vocabularies in existing generative video-language models. Built on the
lightweight language model GPT-2, VLog feature three key innovations: (i) A
generative retrieval model, marrying language model's complex reasoning
capabilities with contrastive retrieval's efficient similarity search. (ii) A
hierarchical vocabulary derived from large-scale video narrations using our
narration pair encoding algorithm, enabling efficient indexing of specific
events (e.g., cutting a tomato) by identifying broader scenarios (e.g.,
kitchen) with expressive postfixes (e.g., by the left hand). (iii) A vocabulary
update strategy leveraging generative models to extend the vocabulary for novel
events encountered during inference. To validate our approach, we introduce
VidCap-Eval, a development set requiring concise narrations with reasoning
relationships (e.g., before and after). Experiments on EgoSchema, COIN, and
HiREST further demonstrate the effectiveness of VLog, highlighting its ability
to generate concise, contextually accurate, and efficient narrations, offering
a novel perspective on video understanding. Codes are released at
https://github.com/showlab/VLog.",['cs.CV'],2502.13481," Tagging systems play an essential role in various information retrieval
applications such as search engines and recommender systems. Recently, Large
Language Models (LLMs) have been applied in tagging systems due to their
extensive world knowledge, semantic understanding, and reasoning capabilities.
Despite achieving remarkable performance, existing methods still have
limitations, including difficulties in retrieving relevant candidate tags
comprehensively, challenges in adapting to emerging domain-specific knowledge,
and the lack of reliable tag confidence quantification. To address these three
limitations above, we propose an automatic tagging system LLM4Tag. First, a
graph-based tag recall module is designed to effectively and comprehensively
construct a small-scale highly relevant candidate tag set. Subsequently, a
knowledge-enhanced tag generation module is employed to generate accurate tags
with long-term and short-term knowledge injection. Finally, a tag confidence
calibration module is introduced to generate reliable tag confidence scores.
Extensive experiments over three large-scale industrial datasets show that
LLM4Tag significantly outperforms the state-of-the-art baselines and LLM4Tag
has been deployed online for content tagging to serve hundreds of millions of
users.",['cs.IR'],False,,,,"VLog: Video-Language Models by Generative Retrieval of Narration
  Vocabulary","LLM4Tag: Automatic Tagging System for Information Retrieval via Large
  Language Models"
neg-d2-518,2025-01-17,,2501.10182," In recent years, Semantic Communication (SemCom), which aims to achieve
efficient and reliable transmission of meaning between agents, has garnered
significant attention from both academia and industry. To ensure the security
of communication systems, encryption techniques are employed to safeguard
confidentiality and integrity. However, traditional cryptography-based
encryption algorithms encounter obstacles when applied to SemCom. Motivated by
this, this paper explores the feasibility of applying homomorphic encryption to
SemCom. Initially, we review the encryption algorithms utilized in mobile
communication systems and analyze the challenges associated with their
application to SemCom. Subsequently, we employ scale-invariant feature
transform to demonstrate that semantic features can be preserved in homomorphic
encrypted ciphertext. Based on this finding, we propose a task-oriented SemCom
scheme secured through homomorphic encryption. We design the privacy preserved
deep joint source-channel coding (JSCC) encoder and decoder, and the frequency
of key updates can be adjusted according to service requirements without
compromising transmission performance. Simulation results validate that, when
compared to plaintext images, the proposed scheme can achieve almost the same
classification accuracy performance when dealing with homomorphic ciphertext
images. Furthermore, we provide potential future research directions for
homomorphic encrypted SemCom.","['cs.CR', 'eess.SP']",2503.12937," Recent studies generally enhance MLLMs' reasoning capabilities via supervised
fine-tuning on high-quality chain-of-thought reasoning data, which often leads
models to merely imitate successful reasoning paths without understanding what
the wrong reasoning paths are. In this work, we aim to enhance the MLLMs'
reasoning ability beyond passively imitating positive reasoning paths. To this
end, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new
online reinforcement learning framework that enables MLLMs to self-improve
reasoning ability via simple, effective and dense step-wise rewarding.
Specifically, StepGRPO introduces two novel rule-based reasoning rewards:
Step-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity
Reward (StepRVR). StepRAR rewards the reasoning paths that contain necessary
intermediate reasoning steps via a soft key-step matching technique, while
StepRAR rewards reasoning paths that follow a well-structured and logically
consistent reasoning process through a reasoning completeness and logic
evaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series
of MLLMs with outstanding capabilities in step-by-step reasoning. Extensive
experiments over 8 benchmarks demonstrate the superiority of our methods.","['cs.AI', 'cs.CL', 'cs.CV', 'cs.LG']",False,,,,Secure Semantic Communication With Homomorphic Encryption,"R1-VL: Learning to Reason with Multimodal Large Language Models via
  Step-wise Group Relative Policy Optimization"
neg-d2-519,2025-02-26,,2502.19475," We study the response of mono-energetic stellar populations with initially
isotropic kinematics to impulsive and adiabatic changes to an underlying dark
matter potential. Half-light radii expand and velocity dispersions decrease as
enclosed dark matter is removed. The details of this expansion and cooling
depend on the time scale on which the underlying potential changes. In the
adiabatic regime, the product of half-light radius and average velocity
dispersion is conserved. We show that the stellar populations maintain
centrally isotropic kinematics throughout their adiabatic evolution, and their
densities can be approximated by a family of analytical radial profiles.
Metallicity gradients within the galaxy flatten as dark matter is slowly
removed. In the case of strong impulsive perturbations, stellar populations
develop power-law-like density tails with radially biased kinematics. We show
that the distribution of stellar binding energies within the dark matter halo
substantially widens after an impulsive perturbation, no matter the sign of the
perturbation. This allows initially energetically separated stellar populations
to mix, to the extent that previously chemo-dynamically distinct populations
may masquerade as a single population with large metallicity and energy spread.
Finally, we show that in response to an impulsive perturbation, stellar
populations that are deeply embedded in cored dark matter halos undergo a
series of damped oscillations before reaching a virialised equilibrium state,
driven by inefficient phase mixing in the harmonic potentials of cored halos.
This slow return to equilibrium adds substantial systematic uncertainty to
dynamical masses estimated from Jeans modeling or the virial theorem.",['astro-ph.GA'],2502.12568," Like humans, Large Language Models (LLMs) struggle to generate high-quality
long-form text that adheres to strict requirements in a single pass. This
challenge is unsurprising, as successful human writing, according to the
Cognitive Writing Theory, is a complex cognitive process involving iterative
planning, translating, reviewing, and monitoring. Motivated by these cognitive
principles, we aim to equip LLMs with human-like cognitive writing capabilities
through CogWriter, a novel training-free framework that transforms LLM
constrained long-form text generation into a systematic cognitive writing
paradigm. Our framework consists of two key modules: (1) a Planning Agent that
performs hierarchical planning to decompose the task, and (2) multiple
Generation Agents that execute these plans in parallel. The system maintains
quality via continuous monitoring and reviewing mechanisms, which evaluate
outputs against specified requirements and trigger necessary revisions.
CogWriter demonstrates exceptional performance on LongGenBench, a benchmark for
complex constrained long-form text generation. Even when using Qwen-2.5-14B as
its backbone, CogWriter surpasses GPT-4o by 22% in complex instruction
completion accuracy while reliably generating texts exceeding 10,000 words. We
hope this cognitive science-inspired approach provides a paradigm for LLM
writing advancements:
\href{https://github.com/KaiyangWan/CogWriter}{CogWriter}.","['cs.CL', 'cs.AI']",False,,,,Impulsive mixing of stellar populations in dwarf spheroidal galaxies,"A Cognitive Writing Perspective for Constrained Long-Form Text
  Generation"
neg-d2-520,2025-01-31,,2502.00284," People's opinions on a wide range of topics often evolve over time through
their interactions with others. Models of opinion dynamics primarily focus on
one-dimensional opinions which represent opinions on one topic. However,
opinions on various topics are rarely isolated; instead, they can be
interdependent and exhibit correlations. In a bounded-confidence model (BCM) of
opinion dynamics, agents influence each other's opinions only if their opinions
are sufficiently similar. We extend classical agent-based BCMs -- namely, the
Hegeselmann--Krause BCM, which has synchronous interactions, and the
Deffuant--Weisbuch BCM, which has asynchronous interactions -- to a
multidimensional setting, in which the opinions are multidimensional vectors
representing opinions of different topics and opinions on different topics are
interdependent. To measure opinion differences between agents, we introduce
topic-weighted discordance functions that account for opinion differences in
all topics. We use the regions of receptiveness to characterize the
steady-state opinion clusters and provide an analytical approach to compute
these regions. In addition, we numerically simulate our models on various
networks with initial opinions drawn from a variety of distributions. When
initial opinions are correlated across different topics, our topic-weighted
BCMs yield significantly different results in both transient and steady states
compared to baseline models, where the dynamics of each opinion topic are
independent.","['physics.soc-ph', 'cs.SI', 'math.DS']",2503.04271," We study LLM judgments of misinformation expressed with uncertainty. Our
experiments study the response of three widely used LLMs (GPT-4o, LlaMA3,
DeepSeek-v2) to misinformation propositions that have been verified false and
then are transformed into uncertain statements according to an uncertainty
typology. Our results show that after transformation, LLMs change their
factchecking classification from false to not-false in 25% of the cases.
Analysis reveals that the change cannot be explained by predictors to which
humans are expected to be sensitive, i.e., modality, linguistic cues, or
argumentation strategy. The exception is doxastic transformations, which use
linguistic cue phrases such as ""It is believed ..."".To gain further insight, we
prompt the LLM to make another judgment about the transformed misinformation
statements that is not related to truth value. Specifically, we study LLM
estimates of the frequency with which people make the uncertain statement. We
find a small but significant correlation between judgment of fact and
estimation of frequency.","['cs.CL', 'cs.CY']",False,,,,"Bounded-Confidence Models of Multi-Dimensional Opinions with
  Topic-Weighted Discordance","On Fact and Frequency: LLM Responses to Misinformation Expressed with
  Uncertainty"
neg-d2-521,2025-03-12,,2503.09972," It is known that, when $n$ is even, the number of permutations of
$\{1,2,\dots,n\}$ all of whose cycles have odd length equals the number of
those all of whose cycles have even length. Adin, Heged\H{u}s and Roichman
recently found a surprising refinement of this identity. They showed that, for
any fixed set $J$, the equality still holds when restricting to permutations
with descent set $J$ on one side, and permutations with ascent set $J$ on the
other. Their proof uses generating functions for higher Lie characters. They
also deduce a version for odd $n$.
  Here we give a bijective proof of their result. We first use known bijections
to restate the identity in terms of multisets of necklaces, and then describe a
new weight-preserving bijection between words all of whose Lyndon factors have
odd length and are distinct, and words all of whose Lyndon factors have even
length. We also show that the corresponding equality about Lyndon
factorizations has a short proof using generating functions.",['math.CO'],2502.0613," While recent Large Vision-Language Models (LVLMs) have shown remarkable
performance in multi-modal tasks, they are prone to generating hallucinatory
text responses that do not align with the given visual input, which restricts
their practical applicability in real-world scenarios. In this work, inspired
by the observation that the text-to-image generation process is the inverse of
image-conditioned response generation in LVLMs, we explore the potential of
leveraging text-to-image generative models to assist in mitigating
hallucinations in LVLMs. We discover that generative models can offer valuable
self-feedback for mitigating hallucinations at both the response and token
levels. Building on this insight, we introduce self-correcting Decoding with
Generative Feedback (DeGF), a novel training-free algorithm that incorporates
feedback from text-to-image generative models into the decoding process to
effectively mitigate hallucinations in LVLMs. Specifically, DeGF generates an
image from the initial response produced by LVLMs, which acts as an auxiliary
visual reference and provides self-feedback to verify and correct the initial
response through complementary or contrastive decoding. Extensive experimental
results validate the effectiveness of our approach in mitigating diverse types
of hallucinations, consistently surpassing state-of-the-art methods across six
benchmarks. Code is available at https://github.com/zhangce01/DeGF.","['cs.CV', 'cs.CL']",False,,,,"A bijection for descent sets of permutations with only even and only odd
  cycles","Self-Correcting Decoding with Generative Feedback for Mitigating
  Hallucinations in Large Vision-Language Models"
neg-d2-522,2025-01-21,,2501.12422," Multimodal Fake News Detection has received increasing attention recently.
Existing methods rely on independently encoded unimodal data and overlook the
advantages of capturing intra-modality relationships and integrating
inter-modal similarities using advanced techniques. To address these issues,
Cross-Modal Tri-Transformer and Metric Learning for Multimodal Fake News
Detection (CroMe) is proposed. CroMe utilizes Bootstrapping Language-Image
Pre-training with Frozen Image Encoders and Large Language Models (BLIP2) as
encoders to capture detailed text, image and combined image-text
representations. The metric learning module employs a proxy anchor method to
capture intra-modality relationships while the feature fusion module uses a
Cross-Modal and Tri-Transformer for effective integration. The final fake news
detector processes the fused features through a classifier to predict the
authenticity of the content. Experiments on datasets show that CroMe excels in
multimodal fake news detection.","['cs.LG', 'cs.AI', 'cs.CV']",2503.15246," In this paper, we propose a direct multiobject tracking (MOT) approach for
MIMO-radar signals that operates on raw sensor data via variational message
passing (VMP). Unlike classical track-before-detect (TBD) methods, which often
rely on simplified likelihood models and exclude nuisance parameters (e.g.,
object amplitudes, noise variance), our method adopts a superimposed signal
model and employs a mean-field approximation to jointly estimate both object
existence and object states. By considering correlations within in the radar
signal due to closely spaced objects and jointly estimating nuisance
parameters, the proposed method achieves robust performance for close-by
objects and in low-signal-to-noise ratio (SNR) regimes. Our numerical
evaluation based on MIMO-radar signals demonstrate that our VMP-based
direct-MOT method outperforms a detect-then-track (DTT) pipeline comprising a
super-resolution sparse Bayesian learning (SBL)-based estimation stage followed
by classical MOT using global nearest neighbour data association and a Kalman
filter.",['eess.SP'],False,,,,"CroMe: Multimodal Fake News Detection using Cross-Modal Tri-Transformer
  and Metric Learning","Variational Message Passing-based Multiobject Tracking for MIMO-Radars
  using Raw Sensor Signals"
neg-d2-523,2025-03-20,,2503.16719," Cloud services have become an essential infrastructure for enterprises and
individuals. Access to these cloud services is typically governed by Identity
and Access Management systems, where user authentication often relies on
passwords. While best practices dictate the implementation of multi-factor
authentication, it's a reality that many such users remain solely protected by
passwords. This reliance on passwords creates a significant vulnerability, as
these credentials can be compromised through various means, including
side-channel attacks. This paper exploits keyboard acoustic emanations to infer
typed natural language passphrases via unsupervised learning, necessitating no
previous training data. Whilst this work focuses on short passphrases, it is
also applicable to longer messages, such as confidential emails, where the
margin for error is much greater, than with passphrases, making the attack even
more effective in such a setting. Unlike traditional attacks that require
physical access to the target device, acoustic side-channel attacks can be
executed within the vicinity, without the user's knowledge, offering a
worthwhile avenue for malicious actors. Our findings replicate and extend
previous work, confirming that cross-correlation audio preprocessing
outperforms methods like mel-frequency-cepstral coefficients and fast-fourier
transforms in keystroke clustering. Moreover, we show that partial passphrase
recovery through clustering and a dictionary attack can enable faster than
brute-force attacks, further emphasizing the risks posed by this attack vector.",['cs.CR'],2503.04889," We classify gapped and gapless phases of non-Hermitian band structures on
two-dimensional nonorientable parameter spaces. Such spaces arise in a wide
range of physical systems in the presence of non-symmorphic parameter space
symmetries. For gapped phases, we find that nonorientable spaces provide a
natural setting for exploring fundamental structural problems in braid group
theory, such as torsion and conjugacy. Gapless phases, which host exceptional
points (EPs), explicitly violate the fermion doubling theorem, even in two-band
models. We demonstrate that EPs traversing the nonorientable parameter space
exhibit non-Abelian charge inversion. These braided phases and their
transitions leave distinct signatures in the form of bulk Fermi arc
degeneracies, offering a concrete route toward experimental realization and
verification.","['cond-mat.mes-hall', 'math-ph', 'math.MP', 'physics.optics', 'quant-ph']",False,,,,Practical Acoustic Eavesdropping On Typed Passphrases,Exceptional Topology on Nonorientable Manifolds
neg-d2-524,2025-01-08,,2501.04801," Quantum entanglement and Bell nonlocality are two phenomena that occur only
in quantum systems. In both cases, these are correlations between two
subsystems that are classically absent. Traditionally, these phenomena have
been measured in low-energy photon and electron experiments, but more recently
they have also been measured in high-energy particle collider environments. In
this work, we propose measuring the entanglement and Bell nonlocality in the
$\tau^+\tau^-$ state near and above its kinematic threshold at the Beijing
Electron Positron Collider (BEPC). We find that in the existing dataset,
entanglement is observable if systematic uncertainties are kept to 1%. In the
upcoming run between 4.0 and 5.6 GeV, the entanglement is predicted to be
measurable with a precision better than 4% and Bell nonlocality can be
established at $5\sigma$ as long as systematic uncertainty can be controlled at
level of 0.5% - 2.0%, depending on the center-of-mass energy.","['hep-ph', 'hep-ex']",2503.04098," One of the more surprising astrophysical discoveries of the last decade has
been the presence of enormous quantities of dust at megaparsec distances from
galaxies, which has important implications for galaxy evolution, the
circumgalactic and intergalactic medium, and observational cosmology. In this
work, we present a novel method for studying these vast halos of circumgalactic
dust: a maximum-likelihood estimator for dust-induced extinction of background
galaxies. This estimator can accommodate a broad range of archival photometric
data and can incorporate different dust reddening prescriptions, making it
applicable to diverse galaxy types and redshifts. We apply the estimator to the
redMaGiC catalog of luminous red galaxies, selected for their tight dispersion
in color and well-constrained photometric redshifts, and measure the resulting
extinction as a function of projected distance from WISExSuperCOSMOS and
redMaGiC foreground galaxies. We detect significant dust-induced extinction
profiles extending to at least 1 megaparsec from galactic disks, with
noticeable differences between star-forming and quiescent galaxies:
star-forming galaxies exhibit a pronounced rise in extinction within the inner
50 kiloparsecs and a steep decline beyond 1 megaparsec, while the quiescent
galaxies host little dust in the inner halo but have detectable extinction out
to 30 megaparsecs. We test the robustness of our results using star catalogs
and inverted foreground and background samples and find no evidence for
significant systematic error. Our approach provides a powerful tool for
studying the interplay between circumgalactic dust, galaxy evolution, and
large-scale structure, with potential applications in a number of astrophysical
subfields.","['astro-ph.GA', 'astro-ph.CO']",False,,,,Entanglement and Bell Nonlocality in $\tau^+ \tau^-$ at the BEPC,"A Detection of Circumgalactic Dust at Megaparsec Scales with Maximum
  Likelihood Estimation"
neg-d2-525,2025-01-26,,2501.15712," Computational modeling of cardiovascular function has become a critical part
of diagnosing, treating and understanding cardiovascular disease. Most
strategies involve constructing anatomically accurate computer models of
cardiovascular structures, which is a multistep, time-consuming process. To
improve the model generation process, we herein present SeqSeg (sequential
segmentation): a novel deep learning based automatic tracing and segmentation
algorithm for constructing image-based vascular models. SeqSeg leverages local
U-Net-based inference to sequentially segment vascular structures from medical
image volumes. We tested SeqSeg on CT and MR images of aortic and aortofemoral
models and compared the predictions to those of benchmark 2D and 3D global
nnU-Net models, which have previously shown excellent accuracy for medical
image segmentation. We demonstrate that SeqSeg is able to segment more complete
vasculature and is able to generalize to vascular structures not annotated in
the training data.","['eess.IV', 'cs.CV', 'q-bio.TO']",2501.07779," The transition route from laminar to turbulent flow in a magnetohydrodynamic
(MHD) duct with a square cross-section is investigated in the limit of low
magnetic Reynolds number. In the presence of a transverse magnetic field,
Hartmann and Shercliff layers are present on the walls orthogonal and parallel
to the field direction, respectively. We assume reflection symmetries in both
transverse directions, and investigate the competition between transition
mechanisms specific to each boundary layer using direct numerical simulations.
Independently of which wall turbulence eventually occupies, transition relies
exclusively on a tripping of the Shercliff layer by perturbations, while the
Hartmann layer plays a passive role. This is explained, using a dynamical
systems interpretation, by the spatial localization of the edge states in the
Shercliff layer at the expense of the Hartmann layer. The link between these
non-linear coherent structures and the linear optimal modes known from
non-modal stability and energy stability theory is pointed out.",['physics.flu-dyn'],False,,,,"SeqSeg: Learning Local Segments for Automatic Vascular Model
  Construction",The route to turbulence in magnetohydrodynamic square duct flow
neg-d2-526,2025-02-13,,2502.09674," Large Language Models' safety-aligned behaviors, such as refusing harmful
queries, can be represented by linear directions in activation space. Previous
research modeled safety behavior with a single direction, limiting mechanistic
understanding to an isolated safety feature. In this work, we discover that
safety-aligned behavior is jointly controlled by multi-dimensional directions.
Namely, we study the vector space of representation shifts during safety
fine-tuning on Llama 3 8B for refusing jailbreaks. By studying orthogonal
directions in the space, we first find that a dominant direction governs the
model's refusal behavior, while multiple smaller directions represent distinct
and interpretable features like hypothetical narrative and role-playing. We
then measure how different directions promote or suppress the dominant
direction, showing the important role of secondary directions in shaping the
model's refusal representation. Finally, we demonstrate that removing certain
trigger tokens in harmful queries can mitigate these directions to bypass the
learned safety capability, providing new insights on understanding safety
alignment vulnerability from a multi-dimensional perspective. Code and
artifacts are available at https://github.com/BMPixel/safety-residual-space.","['cs.CL', 'cs.AI']",2502.181," A graph $G$ is $\mathcal S_3$-connected if, for any mapping $\beta : V (G)
\mapsto {\mathbb Z}_3$ with $\sum_{v\in V(G)} \beta(v)\equiv 0\pmod3$, there
exists a strongly connected orientation $D$ satisfying
$d^{+}_D(v)-d^{-}_D(v)\equiv \beta(v)\pmod{3}$ for any $v \in V(G)$. It is
known that $\mathcal S_3$-connected graphs are contractible configurations for
the property of flow index strictly less than three. In this paper, we provide
a complete characterization of graphic sequences that have an
$\mathcal{S}_{3}$-connected realization: A graphic sequence $\pi=(d_1,\,
\ldots,\, d_n )$ has an $\mathcal S_3$-connected realization if and only if
$\min \{d_1,\, \ldots,\, d_n\} \ge 4$ and $\sum^n_{i=1}d_i \ge 6n - 4$.
Consequently, every graphic sequence $\pi=(d_1,\, \ldots,\, d_n )$ with $\min
\{d_1,\, \ldots,\, d_n\} \ge 6$ has a realization $G$ with flow index strictly
less than three. This supports a conjecture of Li, Thomassen, Wu and Zhang
[European J. Combin., 70 (2018) 164-177] that every $6$-edge-connected graph
has flow index strictly less than three.",['math.CO'],False,,,,"The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Safety
  Analysis",Realizing degree sequences with $\mathcal S_3$-connected graphs
neg-d2-527,2025-03-24,,2503.18515," We consider the following inverse problem: Suppose a $(1+1)$-dimensional wave
equation on $\mathbb R_+$ with zero initial conditions is excited with a
Neumann boundary data modelled as a white noise process. Given also the
Dirichlet data at the same point, determine the unknown first order coefficient
function of the system.
  We first establish that direct problem is well-posed. The inverse problem is
then solved by showing that correlations of the boundary data determine the
Neumann-to-Dirichlet operator in the sense of distributions, which is known to
uniquely identify the coefficient. This approach has applications in acoustic
measurements of internal cross-sections of fluid pipes such as pressurised
water supply pipes and vocal tract shape determination.","['math.AP', 'math.ST', 'stat.TH']",2502.044," Multimodal Federated Learning (MFL) enables multiple clients to
collaboratively train models on multimodal data while ensuring clients'
privacy. However, modality and task heterogeneity hinder clients from learning
a unified representation, weakening local model generalization, especially in
MFL with mixed modalities where only some clients have multimodal data. In this
work, we propose an Adaptive prototype-based Multimodal Federated Learning
(AproMFL) framework for mixed modalities and heterogeneous tasks to address the
aforementioned issues. Our AproMFL transfers knowledge through
adaptively-constructed prototypes without a prior public dataset. Clients
adaptively select prototype construction methods in line with tasks; server
converts client prototypes into unified multimodal prototypes and aggregates
them to form global prototypes, avoid clients keeping unified labels. We divide
the model into various modules and only aggregate mapping modules to reduce
communication and computation overhead. To address aggregation issues in
heterogeneity, we develop a client relationship graph-based scheme to
dynamically adjust aggregation weights. Extensive experiments on representative
datasets evidence effectiveness of AproMFL.","['cs.LG', 'cs.AI', 'cs.CR', 'cs.MM']",False,,,,"Recovering a (1+1)-dimensional wave equation from a single white noise
  boundary measurement","Adaptive Prototype Knowledge Transfer for Federated Learning with Mixed
  Modalities and Heterogeneous Tasks"
neg-d2-528,2025-03-06,,2503.04928," The evolution of automotive technologies towards more integrated and
sophisticated systems requires a shift from traditional distributed
architectures to centralized vehicle architectures. This work presents a novel
framework that addresses the increasing complexity of Software Defined Vehicles
(SDV) through a centralized approach that optimizes software and hardware
integration. Our approach introduces a scalable, modular, and secure automotive
deployment framework that leverages a hardware abstraction layer and dynamic
software deployment capabilities to meet the growing demands of the industry.
The framework supports centralized computing of vehicle functions, making
software development more dynamic and easier to update and upgrade. We
demonstrate the capabilities of our framework by implementing it in a simulated
environment where it effectively handles several automotive operations such as
lane detection, motion planning, and vehicle control. Our results highlight the
framework's potential to facilitate the development and maintenance of future
vehicles, emphasizing its adaptability to different hardware configurations and
its readiness for real-world applications. This work lays the foundation for
further exploration of robust, scalable, and secure SDV systems, setting a new
standard for future automotive architectures.",['cs.SE'],2501.04459," Despite widespread adoption of deep learning models to address a variety of
computer vision tasks, planetary science has yet to see extensive utilization
of such tools to address its unique problems. On Titan, the largest moon of
Saturn, tracking seasonal trends and weather patterns of clouds provides
crucial insights into one of the most complex climates in the Solar System, yet
much of the available image data are still analyzed in a conventional way. In
this work, we apply a Mask R-CNN trained via transfer learning to perform
instance segmentation of clouds in Titan images acquired by the Cassini
spacecraft - a previously unexplored approach to a big data problem in
planetary science. We demonstrate that an automated technique can provide
quantitative measures for clouds, such as areas and centroids, that may
otherwise be prohibitively time-intensive to produce by human mapping.
Furthermore, despite Titan specific challenges, our approach yields accuracy
comparable to contemporary cloud identification studies on Earth and other
worlds. We compare the efficiencies of human-driven versus algorithmic
approaches, showing that transfer learning provides speed-ups that may open new
horizons for data investigation for Titan. Moreover, we suggest that such
approaches have broad potential for application to similar problems in
planetary science where they are currently under-utilized. Future planned
missions to the planets and remote sensing initiatives for the Earth promise to
provide a deluge of image data in the coming years that will benefit strongly
from leveraging machine learning approaches to perform the analysis.","['astro-ph.IM', 'astro-ph.EP', 'cs.CV', 'eess.IV']",False,,,,"AUTOFRAME -- A Software-driven Integration Framework for Automotive
  Systems",Rapid Automated Mapping of Clouds on Titan With Instance Segmentation
neg-d2-529,2025-03-01,,2503.0067," Anomaly detection in videos is a challenging task as anomalies in different
videos are of different kinds. Therefore, a promising way to approach video
anomaly detection is by learning the non-anomalous nature of the video at hand.
To this end, we propose a one-class few-shot learning driven transformer based
approach for anomaly detection in videos that is self-context aware. Features
from the first few consecutive non-anomalous frames in a video are used to
train the transformer in predicting the non-anomalous feature of the subsequent
frame. This takes place under the attention of a self-context learned from the
input features themselves. After the learning, given a few previous frames, the
video-specific transformer is used to infer if a frame is anomalous or not by
comparing the feature predicted by it with the actual. The effectiveness of the
proposed method with respect to the state-of-the-art is demonstrated through
qualitative and quantitative results on different standard datasets. We also
study the positive effect of the self-context used in our approach.",['cs.CV'],2503.11955," We introduce a one parameter deformation of Zwegers' multivariable
$\mu$-function by applying iterations of the $q$-Borel summation method, which
is also a multivariate analogue of the generalized $\mu$-function introduced by
the authors. For this deformed multivariable $\mu$-function, we give some
formulas, for example, forward shift formula, translation and
$\mathfrak{S}_{N+1}$-symmetry. Further we mention modular formulas for the
Zwegers' original multivariable $\mu$-function.",['math.CA'],False,,,,"Transformer Based Self-Context Aware Prediction for Few-Shot Anomaly
  Detection in Videos",A generalization of Zwegers' multivariable $\mu$-function
neg-d2-530,2025-01-06,,2501.02963," Power prices can be forecasted using data-driven models or fundamental
models. Data-driven models learn from historical patterns, while fundamental
models simulate electricity markets. Traditionally, fundamental models have
been too computationally demanding to allow for intrinsic parameter estimation
or frequent updates, which are essential for short-term forecasting. In this
paper, we propose a novel data-driven fundamental model that combines the
strengths of both approaches. We estimate the parameters of a fully fundamental
merit order model using historical data, similar to how data-driven models
work. This removes the need for fixed technical parameters or expert
assumptions, allowing most parameters to be calibrated directly to
observations. The model is efficient enough for quick parameter estimation and
forecast generation. We apply it to forecast German day-ahead electricity
prices and demonstrate that it outperforms both classical fundamental and
purely data-driven models. The hybrid model effectively captures price
volatility and sequential price clusters, which are becoming increasingly
important with the expansion of renewable energy sources. It also provides
valuable insights, such as fuel switches, marginal power plant contributions,
estimated parameters, dispatched plants, and power generation.","['stat.AP', 'econ.EM']",2502.18423," Retrieving objects buried beneath multiple objects is not only challenging
but also time-consuming. Performing manipulation in such environments presents
significant difficulty due to complex contact relationships. Existing methods
typically address this task by sequentially grasping and removing each
occluding object, resulting in lengthy execution times and requiring
impractical grasping capabilities for every occluding object. In this paper, we
present a dexterous arm-hand system for efficient object retrieval in
multi-object stacked environments. Our approach leverages large-scale parallel
reinforcement learning within diverse and carefully designed cluttered
environments to train policies. These policies demonstrate emergent
manipulation skills (e.g., pushing, stirring, and poking) that efficiently
clear occluding objects to expose sufficient surface area of the target object.
We conduct extensive evaluations across a set of over 10 household objects in
diverse clutter configurations, demonstrating superior retrieval performance
and efficiency for both trained and unseen objects. Furthermore, we
successfully transfer the learned policies to a real-world dexterous
multi-fingered robot system, validating their practical applicability in
real-world scenarios. Videos can be found on our project website
https://ChangWinde.github.io/RetrDex.",['cs.RO'],False,,,,"A data-driven merit order: Learning a fundamental electricity price
  model","Retrieval Dexterity: Efficient Object Retrieval in Clutters with
  Dexterous Hand"
neg-d2-531,2025-02-04,,2502.0252," The use of Self-Sovereign Identity (SSI) systems for digital identity
management is gaining traction and interest. Countries such as Bhutan have
already implemented an SSI infrastructure to manage the identity of their
citizens. The EU, thanks to the revised eIDAS regulation, is opening the door
for SSI vendors to develop SSI systems for the planned EU digital identity
wallet. These developments, which fall within the sovereign domain, raise
questions about individual privacy.
  The purpose of this article is to help SSI solution designers make informed
choices to ensure that the designed solution is privacy-friendly. The
observation is that the range of possible solutions is very broad, from DID and
DID resolution methods to verifiable credential types, publicly available
information (e.g. in a blockchain), type of infrastructure, etc. As a result,
the article proposes (1) to group the elementary building blocks of a SSI
system into 5 structuring layers, (2) to analyze for each layer the privacy
implications of using the chosen building block, and (3) to provide a design
assistance dashboard that gives the complete picture of the SSI, and shows the
interdependencies between architectural choices and technical building blocks,
allowing designers to make informed choices and graphically achieve a SSI
solution that meets their need for privacy.","['cs.ET', 'cs.CY', 'cs.SE']",2503.16719," Cloud services have become an essential infrastructure for enterprises and
individuals. Access to these cloud services is typically governed by Identity
and Access Management systems, where user authentication often relies on
passwords. While best practices dictate the implementation of multi-factor
authentication, it's a reality that many such users remain solely protected by
passwords. This reliance on passwords creates a significant vulnerability, as
these credentials can be compromised through various means, including
side-channel attacks. This paper exploits keyboard acoustic emanations to infer
typed natural language passphrases via unsupervised learning, necessitating no
previous training data. Whilst this work focuses on short passphrases, it is
also applicable to longer messages, such as confidential emails, where the
margin for error is much greater, than with passphrases, making the attack even
more effective in such a setting. Unlike traditional attacks that require
physical access to the target device, acoustic side-channel attacks can be
executed within the vicinity, without the user's knowledge, offering a
worthwhile avenue for malicious actors. Our findings replicate and extend
previous work, confirming that cross-correlation audio preprocessing
outperforms methods like mel-frequency-cepstral coefficients and fast-fourier
transforms in keystroke clustering. Moreover, we show that partial passphrase
recovery through clustering and a dictionary attack can enable faster than
brute-force attacks, further emphasizing the risks posed by this attack vector.",['cs.CR'],False,,,,"Privacy by Design for Self-Sovereign Identity Systems: An in-depth
  Component Analysis completed by a Design Assistance Dashboard",Practical Acoustic Eavesdropping On Typed Passphrases
neg-d2-532,2025-02-24,,2502.17842," Semantic communication marks a new paradigm shift from bit-wise data
transmission to semantic information delivery for the purpose of bandwidth
reduction. To more effectively carry out specialized downstream tasks at the
receiver end, it is crucial to define the most critical semantic message in the
data based on the task or goal-oriented features. In this work, we propose a
novel goal-oriented communication (GO-COM) framework, namely Goal-Oriented
Semantic Variational Autoencoder (GOS-VAE), by focusing on the extraction of
the semantics vital to the downstream tasks. Specifically, we adopt a Vector
Quantized Variational Autoencoder (VQ-VAE) to compress media data at the
transmitter side. Instead of targeting the pixel-wise image data
reconstruction, we measure the quality-of-service at the receiver end based on
a pre-defined task-incentivized model. Moreover, to capture the relevant
semantic features in the data reconstruction, imitation learning is adopted to
measure the data regeneration quality in terms of goal-oriented semantics. Our
experimental results demonstrate the power of imitation learning in
characterizing goal-oriented semantics and bandwidth efficiency of our proposed
GOS-VAE.","['cs.LG', 'cs.NI']",2503.16494," In this paper, we examine the wide-ranging impact of artificial intelligence
on society, focusing on its potential to both help and harm global equity,
cognitive abilities, and economic stability. We argue that while artificial
intelligence offers significant opportunities for progress in areas like
healthcare, education, and scientific research, its rapid growth -- mainly
driven by private companies -- may worsen global inequalities, increase
dependence on automated systems for cognitive tasks, and disrupt established
economic paradigms. We emphasize the critical need for strong governance and
ethical guidelines to tackle these issues, urging the academic community to
actively participate in creating policies that ensure the benefits of
artificial intelligence are shared fairly and its risks are managed
effectively.","['physics.soc-ph', 'cs.CY']",False,,,,"Task-Driven Semantic Quantization and Imitation Learning for
  Goal-Oriented Communications","The impact of artificial intelligence: from cognitive costs to global
  inequality"
neg-d2-533,2025-02-20,,2502.14394," Recent advances in natural language processing have raised expectations for
generative models to produce coherent text across diverse language varieties.
In the particular case of the Portuguese language, the predominance of
Brazilian Portuguese corpora online introduces linguistic biases in these
models, limiting their applicability outside of Brazil. To address this gap and
promote the creation of European Portuguese resources, we developed a
cross-domain language variety identifier (LVI) to discriminate between European
and Brazilian Portuguese. Motivated by the findings of our literature review,
we compiled the PtBrVarId corpus, a cross-domain LVI dataset, and study the
effectiveness of transformer-based LVI classifiers for cross-domain scenarios.
Although this research focuses on two Portuguese varieties, our contribution
can be extended to other varieties and languages. We open source the code,
corpus, and models to foster further research in this task.",['cs.CL'],2501.08486," The past decade has seen a rise in the use of Machine Learning methods in the
study of young stellar evolution. This trend has led to a growing need for a
comprehensive database of young stellar objects (YSO) that goes beyond
survey-specific biases and that can be employed for training, validation, and
refining the physical interpretation of machine learning outcomes. We reviewed
the literature focused on the Orion Star Formation complex (OSFC) to compile a
thorough catalogue of previously identified YSO candidates in the region
including the curation of observables relevant to probe their youth. Starting
from the NASA/ADS database, we assembled YSO candidates from more than 200
peer-reviewed publications. We collated data products relevant to the study of
YSOs into a dedicated catalogue, which was complemented with data from large
photometric and spectroscopic surveys and in the Strasbourg astronomical Data
Center. We also added significant value to the catalogue by homogeneously
deriving YSO infrared classification labels and through a comprehensive
curation of labels concerning sources' multiplicity. Finally, we used a
panchromatic approach to derive the probabilities that the sources in our
catalogue were contaminant extragalactic sources or giant stars. We present the
NEMESIS catalogue of YSOs for the OSFC, which includes data collated for 27879
sources covering the whole mass spectrum and the various stages of pre-Main
Sequence evolution from protostars to diskless young stars. The catalogue
includes a collection of panchromatic photometric data processed into spectral
energy distributions, stellar parameters, infrared classes, equivalent widths
of emission lines related to YSOs accretion and star-disk interaction, and
absorption lines such as lithium and lines related to source's gravity, X-ray
emission observables, photometric variability observables, and multiplicity
labels.","['astro-ph.SR', 'astro-ph.GA']",False,,,,Enhancing Portuguese Variety Identification with Cross-Domain Approaches,"The NEMESIS Catalogue of Young Stellar Objects for the Orion Star
  Formation Complex. I. General description of data curation"
neg-d2-534,2025-03-14,,2503.11495," Human processes video reasoning in a sequential spatio-temporal reasoning
logic, we first identify the relevant frames (""when"") and then analyse the
spatial relationships (""where"") between key objects, and finally leverage these
relationships to draw inferences (""what""). However, can Video Large Language
Models (Video-LLMs) also ""reason through a sequential spatio-temporal logic"" in
videos? Existing Video-LLM benchmarks primarily focus on assessing object
presence, neglecting relational reasoning. Consequently, it is difficult to
measure whether a model truly comprehends object interactions (actions/events)
in videos or merely relies on pre-trained ""memory"" of co-occurrences as biases
in generating answers. In this work, we introduce a Video Spatio-Temporal
Reasoning (V-STaR) benchmark to address these shortcomings. The key idea is to
decompose video understanding into a Reverse Spatio-Temporal Reasoning (RSTR)
task that simultaneously evaluates what objects are present, when events occur,
and where they are located while capturing the underlying Chain-of-thought
(CoT) logic. To support this evaluation, we construct a dataset to elicit the
spatial-temporal reasoning process of Video-LLMs. It contains coarse-to-fine
CoT questions generated by a semi-automated GPT-4-powered pipeline, embedding
explicit reasoning chains to mimic human cognition. Experiments from 14
Video-LLMs on our V-STaR reveal significant gaps between current Video-LLMs and
the needs for robust and consistent spatio-temporal reasoning.",['cs.CV'],2501.11864," Thorough simulation testing is crucial for validating the correct behavior of
small Uncrewed Aerial Systems (sUAS) across multiple scenarios, including
adverse weather conditions (such as wind, and fog), diverse settings (hilly
terrain, or urban areas), and varying mission profiles (surveillance,
tracking). While various sUAS simulation tools exist to support developers, the
entire process of creating, executing, and analyzing simulation tests remains a
largely manual and cumbersome task. Developers must identify test scenarios,
set up the simulation environment, integrate the System under Test (SuT) with
simulation tools, formulate mission plans, and collect and analyze results.
These labor-intensive tasks limit the ability of developers to conduct
exhaustive testing across a wide range of scenarios. To alleviate this problem,
in this paper, we propose AutoSimTest, a Large Language Model (LLM)-driven
framework, where multiple LLM agents collaborate to support the sUAS simulation
testing process. This includes: (1) creating test scenarios that subject the
SuT to unique environmental contexts; (2) preparing the simulation environment
as per the test scenario; (3) generating diverse sUAS missions for the SuT to
execute; and (4) analyzing simulation results and providing an interactive
analytics interface. Further, the design of the framework is flexible for
creating and testing scenarios for a variety of sUAS use cases, simulation
tools, and SuT input requirements. We evaluated our approach by (a) conducting
simulation testing of PX4 and ArduPilot flight-controller-based SuTs, (b)
analyzing the performance of each agent, and (c) gathering feedback from sUAS
developers. Our findings indicate that AutoSimTest significantly improves the
efficiency and scope of the sUAS testing process, allowing for more
comprehensive and varied scenario evaluations while reducing the manual effort.",['cs.SE'],False,,,,V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning,"LLM-Agents Driven Automated Simulation Testing and Analysis of small
  Uncrewed Aerial Systems"
neg-d2-535,2025-01-27,,2501.16282," Understanding brain disorders is crucial for accurate clinical diagnosis and
treatment. Recent advances in Multimodal Large Language Models (MLLMs) offer a
promising approach to interpreting medical images with the support of text
descriptions. However, previous research has primarily focused on 2D medical
images, leaving richer spatial information of 3D images under-explored, and
single-modality-based methods are limited by overlooking the critical clinical
information contained in other modalities. To address this issue, this paper
proposes Brain-Adapter, a novel approach that incorporates an extra bottleneck
layer to learn new knowledge and instill it into the original pre-trained
knowledge. The major idea is to incorporate a lightweight bottleneck layer to
train fewer parameters while capturing essential information and utilize a
Contrastive Language-Image Pre-training (CLIP) strategy to align multimodal
data within a unified representation space. Extensive experiments demonstrated
the effectiveness of our approach in integrating multimodal data to
significantly improve the diagnosis accuracy without high computational costs,
highlighting the potential to enhance real-world diagnostic workflows.","['eess.IV', 'cs.AI', 'cs.CV']",2503.03004," In this paper, we propose a new construction of vertex algebras using the
Deligne category. This approach provides a rigorous framework for defining the
so-called large $N$ vertex algebra, which has appeared in recent physics
literatures. We first define the notion of a vertex algebra in a symmetric
monoidal category and extend familiar constructions in ordinary vertex algebras
to this broader categorical context. As an application, we consider a
$\beta\gamma$ vertex algebra in the Deligne category and construct the large N
vertex algebra from it. We study some simple properties of this vertex algebra
and analyze a certain vertex Poisson algebra limit.","['math.QA', 'hep-th', 'math-ph', 'math.MP']",False,,,,"Brain-Adapter: Enhancing Neurological Disorder Analysis with
  Adapter-Tuning Multimodal Large Language Models",Large $N$ Vertex Algebras via Deligne Category
neg-d2-536,2025-02-24,,2502.16959," IceCube measures a diffuse neutrino flux comparable to the Waxman-Bahcall
bound, which suggests the possibility that the ultra-high energy cosmic rays
(UHECRs) have a common origin with diffuse high energy neutrinos. We propose
high energy gamma-ray and/or neutrino observations toward the arrival
directions of UHECRs to search for the sources and test this possibility. We
calculate the detection probability of gamma-ray/neutrino sources, and find
that the average probability per UHECR of >10 EeV is $\sim$10% if the
sensitivity of the gamma-ray or neutrino telescope is $\sim$10$^{-12}$ erg
cm$^{-2}$s$^{-1}$ and the source number density is $\sim$10$^{-5}$ Mpc$^{-3}$.
Future gamma-ray and neutrino observations toward UHECRs, e.g., by LHAASO-WCDA,
CTA, IceCube/Gen2, are encouraged to constrain the density of UHECR sources or
even identify the sources of UHECRs.",['astro-ph.HE'],2501.08481," The evolution operator method is used to solve the generalized Fokker-Planck
equations and the generalized diffusion-wave equations in the (1+1) dimensional
space in which $x\in\mathbb{R}$ and $t\in\mathbb{R}_+$. These equations contain
either the first- or the second-time derivatives smeared by memory functions,
each of which forms an integral kernel (denoted by $f(\xi, t)$,
$\xi\in\mathbb{R}_+$) of suitable evolution operators. If memory functions in
the Laplace space are Stieltjes functions, then $f(\xi, t)$ satisfy
normalization, non-negativity, and infinite divisibility to be considered a
probability density function. The evolution operators also contain
exponential-like operators whose action on initial condition $p_0(x) > 0$ leads
to the parent process distribution functions. This makes the results fully
analogous to those obtained within the standard subordination approach. The
above conclusion is satisfied by the solution of the generalized Fokker-Planck
equation. In the case of the generalized diffusion-wave equation, to get this
property, we should employ a special class, namely ""diffusion-like"" initial
conditions. The key models of the operator method involve power-law memory
functions. It leads to the characterization of $f(\xi, t)$ by applying
one-sided stable L\'{e}vy distributions. The article also examines the
properties of evolution operators in terms of evolution and self-reproduction.","['math-ph', 'math.MP']",False,,,,"Observe Gamma-Rays and Neutrinos Associated with Ultra-High Energy
  Cosmic Rays","Operational solutions for the generalized Fokker-Planck and generalized
  diffusion-wave equations"
neg-d2-537,2025-01-23,,2501.14118," We develop a new methodology to select scenarios of DER adoption most
critical for distribution grids. Anticipating risks of future voltage and line
flow violations due to additional PV adopters is central for utility investment
planning but continues to rely on deterministic or ad hoc scenario selection.
We propose a highly efficient search framework based on multi-objective
Bayesian Optimization. We treat underlying grid stress metrics as
computationally expensive black-box functions, approximated via Gaussian
Process surrogates and design an acquisition function based on probability of
scenarios being Pareto-critical across a collection of line- and bus-based
violation objectives. Our approach provides a statistical guarantee and offers
an order of magnitude speed-up relative to a conservative exhaustive search.
Case studies on realistic feeders with 200-400 buses demonstrate the
effectiveness and accuracy of our approach.","['cs.LG', 'stat.AP', 'stat.ML']",2501.14703," A comprehensive study on thermomechanical processing of pure Mg was conducted
through sequential hot extrusion, hot rolling, and cold drawing operations.
Three different extrusion ratios (6:1, 25:1, and 39:1) were investigated at
350{\deg}C, revealing that 39:1 ratio produced an optimal bimodal grain
structure with beneficial twin morphology. Subsequently, hot rolling
experiments were performed at varying linear speeds (26- and 130-mm s-1) and
interpass annealing times (2.5 and 10 minutes). Results demonstrated that
higher rolling speeds led to finer microstructure, while longer interpass
annealing times resulted in reduced twin fraction and more inhomogeneous
microstructure. The processed material was then subjected to cold drawing with
approximately 12% true strain per pass. Different annealing conditions
(275{\deg}C and 375{\deg}C for 2.5-10 minutes) between drawing passes were
evaluated. Analysis showed that annealing at 375{\deg}C for 2.5-5 minutes
provided optimal softening for subsequent deformation. Fracture analysis
revealed a mixed ductile-brittle behavior, with twin-matrix interfaces serving
as preferred crack propagation paths This study establishes optimal processing
parameters for pure Mg wire production, highlighting the critical role of twin
characteristics and restoration processes in determining material formability
during multi-step thermomechanical processing.",['cond-mat.mtrl-sci'],False,,,,"Selecting Critical Scenarios of DER Adoption in Distribution Grids Using
  Bayesian Optimization","Thermomechanical Processing of Pure Magnesium: Hot Extrusion, Hot
  Rolling and Cold Drawing"
neg-d2-538,2025-01-06,,2501.02817," Given a pair of time series, we study how the periodicity of one influences
the periodicity of the other. There are several known methods to measure the
similarity between a pair of time series, such as cross-correlation, coherence,
cross-recurrence, and dynamic time warping. But we have yet to find any
measures with theoretical stability results.
  Persistence homology has been utilized to construct a scoring function with
theoretical guarantees of stability that quantifies the periodicity of a single
univariate time series f1, denoted score(f1). Building on this concept, we
propose a conditional periodicity score that quantifies the periodicity of one
univariate time series f1 given another f2, denoted score(f1|f2), and derive
theoretical stability results for the same. With the use of dimension reduction
in mind, we prove a new stability result for score(f1|f2) under principal
component analysis (PCA) when we use the projections of the time series
embeddings onto their respective first K principal components. We show that the
change in our score is bounded by a function of the eigenvalues corresponding
to the remaining (unused) N-K principal components and hence is small when the
first K principal components capture most of the variation in the time series
embeddings. Finally we derive a lower bound on the minimum embedding dimension
to use in our pipeline which guarantees that any two such embeddings give
scores that are within a given epsilon of each other.
  We present a procedure for computing conditional periodicity scores and
implement it on several pairs of synthetic signals. We experimentally compare
our similarity measure to the most-similar statistical measure of
cross-recurrence, and show the increased accuracy and stability of our score
when predicting and measuring whether or not the periodicities of two time
series are similar.","['math.AT', 'math.ST', 'stat.ML', 'stat.TH']",2503.01053," Each period, two players bargain over a unit of surplus. Each player chooses
between remaining flexible and committing to a take-it-or-leave-it offer at a
cost. If players' committed demands are incompatible, then the current-period
surplus is destroyed in the conflict. When both players are flexible, the
surplus is split according to the status quo, which is the division in the last
period where there was no conflict. We show that when players are patient and
the cost of commitment is small, there exist a class of symmetric Markov
Perfect equilibria that are asymptotically efficient and renegotiation proof,
in which players commit to fair demands in almost all periods.","['econ.TH', 'cs.GT']",False,,,,"A Stable Measure for Conditional Periodicity of Time Series using
  Persistent Homology","Commitment, Conflict, and Status Quo in Bargaining"
neg-d2-539,2025-02-04,,2502.02307," Despite decades of research on data collection and model architectures,
current gaze estimation models encounter significant challenges in generalizing
across diverse data domains. Recent advances in self-supervised pre-training
have shown remarkable performances in generalization across various vision
tasks. However, their effectiveness in gaze estimation remains unexplored. We
propose UniGaze, for the first time, leveraging large-scale in-the-wild facial
datasets for gaze estimation through self-supervised pre-training. Through
systematic investigation, we clarify critical factors that are essential for
effective pretraining in gaze estimation. Our experiments reveal that
self-supervised approaches designed for semantic tasks fail when applied to
gaze estimation, while our carefully designed pre-training pipeline
consistently improves cross-domain performance. Through comprehensive
experiments of challenging cross-dataset evaluation and novel protocols
including leave-one-dataset-out and joint-dataset settings, we demonstrate that
UniGaze significantly improves generalization across multiple data domains
while minimizing reliance on costly labeled data. source code and model are
available at https://github.com/ut-vision/UniGaze.",['cs.CV'],2503.02414," 3D models are widely used in various industries, and mesh data has become an
indispensable part of 3D modeling because of its unique advantages. Mesh data
can provide an intuitive and practical expression of rich 3D information.
However, its disordered, irregular data structure and complex surface
information make it challenging to apply with deep learning models directly.
Traditional mesh data processing methods often rely on mesh models with many
limitations, such as manifold, which restrict their application scopes in
reality and do not fully utilize the advantages of mesh models. This paper
proposes a novel end-to-end framework for addressing the challenges associated
with deep learning in mesh models centered around graph neural networks (GNN)
and is titled InfoGNN. InfoGNN treats the mesh model as a graph, which enables
it to handle irregular mesh data efficiently. Moreover, we propose InfoConv and
InfoMP modules, which utilize the position information of the points and fully
use the static information such as face normals, dihedral angles, and dynamic
global feature information to fully use all kinds of data. In addition, InfoGNN
is an end-to-end framework, and we simplify the network design to make it more
efficient, paving the way for efficient deep learning of complex 3D models. We
conducted experiments on several publicly available datasets, and the results
show that InfoGNN achieves excellent performance in mesh classification and
segmentation tasks.","['cs.CV', 'cs.LG']",False,,,,UniGaze: Towards Universal Gaze Estimation via Large-scale Pre-Training,InfoGNN: End-to-end deep learning on mesh via graph neural networks
neg-d2-540,2025-03-21,,2503.1702," Quantum kernels quantify similarity between data points by measuring the
inner product between quantum states, computed through quantum circuit
measurements. By embedding data into quantum systems, quantum kernel feature
maps, that may be classically intractable to compute, could efficiently exploit
high-dimensional Hilbert spaces to capture complex patterns. However, designing
effective quantum feature maps remains a major challenge. Many quantum kernels,
such as the fidelity kernel, suffer from exponential concentration, leading to
near-identity kernel matrices that fail to capture meaningful data correlations
and lead to overfitting and poor generalization. In this paper, we propose a
novel strategy for constructing quantum kernels that achieve good
generalization performance, drawing inspiration from benign overfitting in
classical machine learning. Our approach introduces the concept of local-global
quantum kernels, which combine two complementary components: a local quantum
kernel based on measurements of small subsystems and a global quantum kernel
derived from full-system measurements. Through numerical experiments, we
demonstrate that local-global quantum kernels exhibit benign overfitting,
supporting the effectiveness of our approach in enhancing quantum kernel
methods.","['quant-ph', 'cs.LG', 'stat.ML']",2502.01335," While traditional self-supervised learning methods improve performance and
robustness across various medical tasks, they rely on single-vector embeddings
that may not capture fine-grained concepts such as anatomical structures or
organs. The ability to identify such concepts and their characteristics without
supervision has the potential to improve pre-training methods, and enable novel
applications such as fine-grained image retrieval and concept-based outlier
detection. In this paper, we introduce ConceptVAE, a novel pre-training
framework that detects and disentangles fine-grained concepts from their style
characteristics in a self-supervised manner. We present a suite of loss terms
and model architecture primitives designed to discretise input data into a
preset number of concepts along with their local style. We validate ConceptVAE
both qualitatively and quantitatively, demonstrating its ability to detect
fine-grained anatomical structures such as blood pools and septum walls from 2D
cardiac echocardiographies. Quantitatively, ConceptVAE outperforms traditional
self-supervised methods in tasks such as region-based instance retrieval,
semantic segmentation, out-of-distribution detection, and object detection.
Additionally, we explore the generation of in-distribution synthetic data that
maintains the same concepts as the training data but with distinct styles,
highlighting its potential for more calibrated data generation. Overall, our
study introduces and validates a promising new pre-training technique based on
concept-style disentanglement, opening multiple avenues for developing models
for medical image analysis that are more interpretable and explainable than
black-box approaches.",['cs.CV'],False,,,,Benign Overfitting with Quantum Kernels,"ConceptVAE: Self-Supervised Fine-Grained Concept Disentanglement from 2D
  Echocardiographies"
neg-d2-541,2025-02-04,,2502.02128," We present a microscopic description of cluster emission processes within the
Cluster--Hartree--Fock (CHF) self--consistent field (SCF) theory. The starting
point is a Woods--Saxon (WS) mean field (MF) with spin--orbit and Coulomb
terms. Pairing is treated through standard Bardeen--Cooper--Schrieffer (BCS)
quasiparticles. The residual two--body interaction is given by a
density--dependent Wigner force having a Gaussian shape with a center of mass
(com) correction located in a region of low nuclear density slightly beyond the
geometrical contact radius of a system comprised from a nucleus and a surface
cluster. We show that such a description adequately reproduces the ground state
(gs) shape of a spherical nucleus while the surface correction enhances the
radial tail of single particle orbitals, thus allowing for an adequate
description of the $\alpha$-decay width for unstable systems.",['nucl-th'],2503.0139," Crash consistency is essential for applications that must persist data.
Crash-consistency testing has been commonly applied to find crash-consistency
bugs in applications. The crash-state space grows exponentially as the number
of operations in the program increases, necessitating techniques for pruning
the search space. However, state-of-the-art crash-state space pruning is far
from ideal. Some techniques look for known buggy patterns or bound the
exploration for efficiency, but they sacrifice coverage and may miss bugs
lodged deep within applications. Other techniques eliminate redundancy in the
search space by skipping identical crash states, but they still fail to scale
to larger applications.
  In this work, we propose representative testing: a new crash-state space
reduction strategy that achieves high scalability and high coverage. Our key
observation is that the consistency of crash states is often correlated, even
if those crash states are not identical. We build Pathfinder, a
crash-consistency testing tool that implements an update behaviors-based
heuristic to approximate a small set of representative crash states.
  We evaluate Pathfinder on POSIX-based and MMIO-based applications, where it
finds 18 (7 new) bugs across 8 production-ready systems. Pathfinder scales more
effectively to large applications than prior works and finds 4x more bugs in
POSIX-based applications and 8x more bugs in MMIO-based applications compared
to state-of-the-art systems.","['cs.OS', 'cs.PL', 'cs.SE']",False,,,,Emission processes in a self-consistent field,"Scalable and Accurate Application-Level Crash-Consistency Testing via
  Representative Testing"
neg-d2-542,2025-03-09,,2503.06664," High-quality, error-free datasets are a key ingredient in building reliable,
accurate, and unbiased machine learning (ML) models. However, real world
datasets often suffer from errors due to sensor malfunctions, data entry
mistakes, or improper data integration across multiple sources that can
severely degrade model performance. Detecting and correcting these issues
typically require tailor-made solutions and demand extensive domain expertise.
Consequently, automation is challenging, rendering the process labor-intensive
and tedious. In this study, we investigate whether Large Language Models (LLMs)
can help alleviate the burden of manual data cleaning. We set up an experiment
in which an LLM, paired with Python, is tasked with cleaning the training
dataset to improve the performance of a learning algorithm without having the
ability to modify the training pipeline or perform any feature engineering. We
run this experiment on multiple Kaggle datasets that have been intentionally
corrupted with errors. Our results show that LLMs can identify and correct
erroneous entries, such as illogical values or outlier, by leveraging
contextual information from other features within the same row, as well as
feedback from previous iterations. However, they struggle to detect more
complex errors that require understanding data distribution across multiple
rows, such as trends and biases.","['cs.LG', 'cs.AI']",2502.08051," We study the screening mass of the neutral rho-meson in the presence of
strong magnetic fields using the Kroll-Lee-Zumino (KLZ) model. The rho-meson
self-energy is computed at one-loop order within the lowest Landau level (LLL)
approximation, considering the magnetic field as the dominant energy scale. Due
to Lorentz symmetry breaking induced by the external field, we decompose the
self-energy into three independent tensor structures, which give rise to three
distinct modes. Additionally, the four-momentum splits into parallel and
perpendicular components, leading to two types of screening masses: the
parallel screening mass ( $p_0=0$ and $p_\perp \to 0$ ) and the perpendicular
screening mass ( $p_0=0$ and $p_\parallel \to 0$ ). Our results show that the
zero and perpendicular modes exhibit a monotonically increasing behavior with
the magnetic field strength, whereas the parallel mode remains essentially
constant. These findings provide new insights into the behavior of vector
mesons in strongly magnetized media, with implications for QCD under extreme
conditions.","['hep-ph', 'hep-th']",False,,,,Exploring LLM Agents for Cleaning Tabular Machine Learning Datasets,Screening rho-meson mass in the presence of strong magnetic fields
neg-d2-543,2025-02-05,,2502.03171," Localization methods based on holographic multiple input multiple output
(HMIMO) have gained much attention for its potential to achieve high accuracy.
By deploying multiple HMIMOs, we can improve the link quality and system
coverage. As the scale of HMIMO increases to improve beam control capability,
the near-field (NF) region of each HMIMO expands. However, existing multiple
HMIMO-enabled methods mainly focus on the far-field (FF) of each HMIMO, which
leads to low localization accuracy when applied in the NF. In this paper, a
hybrid NF and FF localization method aided by multiple RISs, a low cost
implementation of HMIMO, is proposed. In such a scenario, it is difficult to
achieve user localization and RIS optimization since the equivalent NF of all
RISs expands, which results in high complexity, and we need to handle the
interference caused by multiple RISs. To tackle this challenge, we propose a
two-phase RIS-enabled localization method that first estimate the relative
locations of the user to each RIS and fuse the results to obtain the global
estimation. In this way, the algorithm complexity is reduced. We formulate the
RIS optimization problem to keep the RIS sidelobe as low as possible to
minimize the interference. The effectiveness of the proposed method is verified
through simulations.",['eess.SP'],2503.05656," This article proposes a roadmap to address the current challenges in
small-scale testbeds for Connected and Automated Vehicles (CAVs) and robot
swarms. The roadmap is a joint effort of participants in the workshop ""1st
Workshop on Small-Scale Testbeds for Connected and Automated Vehicles and Robot
Swarms,"" held on June 2 at the IEEE Intelligent Vehicles Symposium (IV) 2024 in
Jeju, South Korea. The roadmap contains three parts: 1) enhancing accessibility
and diversity, especially for underrepresented communities, 2) sharing best
practices for the development and maintenance of testbeds, and 3) connecting
testbeds through an abstraction layer to support collaboration. The workshop
features eight invited speakers, four contributed papers [1]-[4], and a
presentation of a survey paper on testbeds [5]. The survey paper provides an
online comparative table of more than 25 testbeds, available at
https://bassamlab.github.io/testbeds-survey. The workshop's own website is
available at https://cpm-remote.lrt.unibw-muenchen.de/iv24-workshop.","['cs.RO', 'cs.MA']",False,,,,"Hybrid Near-Field and Far-Field Localization with Multiple Holographic
  MIMO Surfaces","Small-Scale Testbeds for Connected and Automated Vehicles and Robot
  Swarms: Challenges and a Roadmap"
neg-d2-544,2025-01-10,,2501.05859," In this paper, we introduce a large model-empowered streaming semantic
communication system for speech transmission across various languages, named
LSSC-ST. Specifically, we devise an edge-device collaborative semantic
communication architecture by offloading the intricate semantic extraction and
channel coding modules to edge servers, thereby reducing the computational
burden on local devices. To support multilingual speech transmission,
pre-trained large speech models are utilized to learn unified semantic features
from speech in different languages, breaking the constraint of a single input
language and enhancing the practicality of the LSSC-ST. Moreover, the input
speech is sequentially streamed into the developed system as short speech
segments, which enables low transmission latency without degrading the quality
of the produced speech. A novel dynamic speech segmentation algorithm is
proposed to further reduce the transmission latency by adaptively adjusting the
duration of speech segments. According to simulation results, the LSSC-ST
provides more accurate speech transmission and achieves a streaming manner with
lower latency compared to the existing non-streaming semantic communication
systems.",['eess.AS'],2501.14703," A comprehensive study on thermomechanical processing of pure Mg was conducted
through sequential hot extrusion, hot rolling, and cold drawing operations.
Three different extrusion ratios (6:1, 25:1, and 39:1) were investigated at
350{\deg}C, revealing that 39:1 ratio produced an optimal bimodal grain
structure with beneficial twin morphology. Subsequently, hot rolling
experiments were performed at varying linear speeds (26- and 130-mm s-1) and
interpass annealing times (2.5 and 10 minutes). Results demonstrated that
higher rolling speeds led to finer microstructure, while longer interpass
annealing times resulted in reduced twin fraction and more inhomogeneous
microstructure. The processed material was then subjected to cold drawing with
approximately 12% true strain per pass. Different annealing conditions
(275{\deg}C and 375{\deg}C for 2.5-10 minutes) between drawing passes were
evaluated. Analysis showed that annealing at 375{\deg}C for 2.5-5 minutes
provided optimal softening for subsequent deformation. Fracture analysis
revealed a mixed ductile-brittle behavior, with twin-matrix interfaces serving
as preferred crack propagation paths This study establishes optimal processing
parameters for pure Mg wire production, highlighting the critical role of twin
characteristics and restoration processes in determining material formability
during multi-step thermomechanical processing.",['cond-mat.mtrl-sci'],False,,,,Large Model Empowered Streaming Speech Semantic Communications,"Thermomechanical Processing of Pure Magnesium: Hot Extrusion, Hot
  Rolling and Cold Drawing"
neg-d2-545,2025-01-08,,2501.04329," While most existing neural image compression (NIC) and neural video
compression (NVC) methodologies have achieved remarkable success, their
optimization is primarily focused on human visual perception. However, with the
rapid development of artificial intelligence, many images and videos will be
used for various machine vision tasks. Consequently, such existing compression
methodologies cannot achieve competitive performance in machine vision. In this
work, we introduce an efficient adaptive compression (EAC) method tailored for
both human perception and multiple machine vision tasks. Our method involves
two key modules: 1), an adaptive compression mechanism, that adaptively selects
several subsets from latent features to balance the optimizations for multiple
machine vision tasks (e.g., segmentation, and detection) and human vision. 2),
a task-specific adapter, that uses the parameter-efficient delta-tuning
strategy to stimulate the comprehensive downstream analytical networks for
specific machine vision tasks. By using the above two modules, we can optimize
the bit-rate costs and improve machine vision performance. In general, our
proposed EAC can seamlessly integrate with existing NIC (i.e., Ball\'e2018, and
Cheng2020) and NVC (i.e., DVC, and FVC) methods. Extensive evaluation on
various benchmark datasets (i.e., VOC2007, ILSVRC2012, VOC2012, COCO, UCF101,
and DAVIS) shows that our method enhances performance for multiple machine
vision tasks while maintaining the quality of human vision.",['cs.CV'],2502.0709," Accurate prediction with multimodal data-encompassing tabular, textual, and
visual inputs or outputs-is fundamental to advancing analytics in diverse
application domains. Traditional approaches often struggle to integrate
heterogeneous data types while maintaining high predictive accuracy. We
introduce Generative Distribution Prediction (GDP), a novel framework that
leverages multimodal synthetic data generation-such as conditional diffusion
models-to enhance predictive performance across structured and unstructured
modalities. GDP is model-agnostic, compatible with any high-fidelity generative
model, and supports transfer learning for domain adaptation. We establish a
rigorous theoretical foundation for GDP, providing statistical guarantees on
its predictive accuracy when using diffusion models as the generative backbone.
By estimating the data-generating distribution and adapting to various loss
functions for risk minimization, GDP enables accurate point predictions across
multimodal settings. We empirically validate GDP on four supervised learning
tasks-tabular data prediction, question answering, image captioning, and
adaptive quantile regression-demonstrating its versatility and effectiveness
across diverse domains.","['stat.ML', 'cs.AI', 'cs.LG']",False,,,,"An Efficient Adaptive Compression Method for Human Perception and
  Machine Vision Tasks","Generative Distribution Prediction: A Unified Approach to Multimodal
  Learning"
neg-d2-546,2025-01-21,,2501.12166," Detecting anomalies in discrete event logs is critical for ensuring system
reliability, security, and efficiency. Traditional window-based methods for log
anomaly detection often suffer from context bias and fuzzy localization, which
hinder their ability to precisely and efficiently identify anomalies. To
address these challenges, we propose a graph-centric framework, TempoLog, which
leverages multi-scale temporal graph networks for discrete log anomaly
detection. Unlike conventional methods, TempoLog constructs continuous-time
dynamic graphs directly from event logs, eliminating the need for fixed-size
window grouping. By representing log templates as nodes and their temporal
relationships as edges, the framework dynamically captures both local and
global dependencies across multiple temporal scales. Additionally, a
semantic-aware model enhances detection by incorporating rich contextual
information. Extensive experiments on public datasets demonstrate that our
method achieves state-of-the-art performance in event-level anomaly detection,
significantly outperforming existing approaches in both accuracy and
efficiency.","['cs.SE', 'cs.LG']",2503.04667," This paper proposes a new principled multi-task representation learning
framework (InfoMTL) to extract noise-invariant sufficient representations for
all tasks. It ensures sufficiency of shared representations for all tasks and
mitigates the negative effect of redundant features, which can enhance language
understanding of pre-trained language models (PLMs) under the multi-task
paradigm. Firstly, a shared information maximization principle is proposed to
learn more sufficient shared representations for all target tasks. It can avoid
the insufficiency issue arising from representation compression in the
multi-task paradigm. Secondly, a task-specific information minimization
principle is designed to mitigate the negative effect of potential redundant
features in the input for each task. It can compress task-irrelevant redundant
information and preserve necessary information relevant to the target for
multi-task prediction. Experiments on six classification benchmarks show that
our method outperforms 12 comparative multi-task methods under the same
multi-task settings, especially in data-constrained and noisy scenarios.
Extensive experiments demonstrate that the learned representations are more
sufficient, data-efficient, and robust.","['cs.CL', 'cs.IT', 'cs.LG', 'math.IT']",False,,,,"Beyond Window-Based Detection: A Graph-Centric Framework for Discrete
  Log Anomaly Detection","An Information-theoretic Multi-task Representation Learning Framework
  for Natural Language Understanding"
neg-d2-547,2025-01-24,,2501.14385," Since searches for the low-lying excited baryons $\Xi(1/2^-)$ and
$\Sigma(1/2^-)$ are crucial to deepening our understanding of the light baryon
spectrum, we have investigated the Cabibbo-favored process $\Xi_c^+ \to \Lambda
{\bar{K}}^0 \pi^+$ by taking into account the $S$-wave pseudoscalar meson-octet
baryon interactions within the chiral unitary approach, which could dynamically
generate the resonances $\Xi(1/2^-)$ and $\Sigma(1/2^-)$. The contributions
from the excited kaons are double Cabibbo-suppressed, and the contribution from
the $\Sigma(1385)$ is also suppressed due to Korner-Pati-Woo theory, thus those
states are expected to play negligible contributions in this process. We have
predicted the $\bar{K}^0 \Lambda$ and $\pi^+\Lambda$ invariant mass
distributions, which have the clear signals of the $\Xi(1/2^-)$ and
$\Sigma(1/2^-)$. Thus, the $\Xi_c^+ \to \Lambda {\bar{K}}^0 \pi^+$ is an ideal
process to search for the low-lying baryons $\Xi(1/2^-)$ and $\Sigma(1/2^-)$,
and we make a call for a precise measurements of this process in experiments,
such as Belle II, LHCb, and the proposed Super Tau-Charm Facility (STCF).",['hep-ph'],2503.01396," Copious mobile operating systems exist in the market, but Android remains the
user's choice. Meanwhile, its growing popularity has also attracted malware
developers. Researchers have proposed various static solutions for Android
malware detection. However, stealthier malware evade static analysis. This
raises the need for a robust Android malware detection system capable of
dealing with advanced threats and overcoming the shortcomings of static
analysis.
  Hence, this work proposes a dynamic analysis-based Android malware detection
system, CorrNetDroid, that works over network traffic flows. Many traffic
features exhibit overlapping ranges in normal and malware datasets. Therefore,
we first rank the features using two statistical measures, crRelevance and
Normalized Mean Residue Similarity (NMRS), to assess feature-class and
feature-feature correlations. Thereafter, we introduce a novel
correlation-based feature selection algorithm that applies NMRS on crRelevance
rankings to identify the optimal feature subset for Android malware detection.
  Experimental results highlight that our model effectively reduces the feature
set while detecting Android malware with 99.50 percent accuracy when
considering only two network traffic features. Furthermore, our experiments
demonstrate that the NMRS-based algorithm on crRelevance rankings outperforms
statistical tests such as chi-square, ANOVA, Mann-Whitney U test, and
Kruskal-Wallis test. In addition, our model surpasses various state-of-the-art
Android malware detection techniques in terms of detection accuracy.","['cs.CR', 'cs.MM']",False,,,,"Study $\Xi_c^+ \to \Lambda {\bar{K}}^0 \pi^+$ and search for the
  low-lying baryons $\Xi(1/2^-)$ and $\Sigma(1/2^-)$","CorrNetDroid: Android Malware Detector leveraging a Correlation-based
  Feature Selection for Network Traffic features"
neg-d2-548,2025-02-12,,2502.08686," EEG signals convey important information about brain activity both in healthy
and pathological conditions. However, they are inherently noisy, which poses
significant challenges for accurate analysis and interpretation. Traditional
EEG artifact removal methods, while effective, often require extensive expert
intervention. This study presents LSTEEG, a novel LSTM-based autoencoder
designed for the detection and correction of artifacts in EEG signals.
Leveraging deep learning, particularly LSTM layers, LSTEEG captures non-linear
dependencies in sequential EEG data. LSTEEG demonstrates superior performance
in both artifact detection and correction tasks compared to other
state-of-the-art convolutional autoencoders. Our methodology enhances the
interpretability and utility of the autoencoder's latent space, enabling
data-driven automated artefact removal in EEG its application in downstream
tasks. This research advances the field of efficient and accurate multi-channel
EEG preprocessing, and promotes the implementation and usage of automated EEG
analysis pipelines for brain health applications.","['cs.LG', 'cs.AI']",2502.10682," Effective deepfake detection tools are becoming increasingly essential over
the last few years due to the growing usage of deepfakes in unethical
practices. There exists a diverse range of deepfake generation techniques,
which makes it challenging to develop an accurate universal detection
mechanism. The 2025 Signal Processing Cup (DFWild-Cup competition) provided a
diverse dataset of deepfake images, which are generated from multiple deepfake
image generators, for training machine learning model(s) to emphasize the
generalization of deepfake detection. To this end, we proposed an
ensemble-based approach that employs three different neural network
architectures: a ResNet-34-based architecture, a data-efficient image
transformer (DeiT), and an XceptionNet with Wavelet Transform to capture both
local and global features of deepfakes. We visualize the specific regions that
these models focus for classification using Grad-CAM, and empirically
demonstrate the effectiveness of these models in grouping real and fake images
into cohesive clusters using t-SNE plots. Individually, the ResNet-34
architecture has achieved 88.9% accuracy, whereas the Xception network and the
DeiT architecture have achieved 87.76% and 89.32% accuracy, respectively. With
these networks, our weighted ensemble model achieves an excellent accuracy of
93.23% on the validation dataset of the SP Cup 2025 competition. Finally, the
confusion matrix and an Area Under the ROC curve of 97.44% further confirm the
stability of our proposed method.","['cs.CV', 'cs.LG', 'eess.IV']",False,,,,EEG Artifact Detection and Correction with Deep Autoencoders,"Hybrid Deepfake Image Detection: A Comprehensive Dataset-Driven Approach
  Integrating Convolutional and Attention Mechanisms with Frequency Domain
  Features"
neg-d2-549,2025-03-01,,2503.00395," We demonstrate dynamic pressure tuning (0-6.6 GPa) of layer-hybridized
excitons in AB-stacked trilayer WSe$_2$ via diamond-anvil-cell-integrated
reflectance spectroscopy. Pressure-controlled interlayer coupling manifests in
enhanced energy-level anti-crossings and oscillator strength redistribution,
with Stark shift analysis revealing a characteristic dipole moment reduction of
11%. Notably, the hybridization strength between the intra- and interlayer
excitons triples from $\sim$10 meV to above $\sim$30 meV, exhibiting a
near-linear scaling of 3.5$\pm$0.2 meV/GPa. Spectral density simulations
resolve four distinct components, i.e., intralayer ground/excited and
interlayer ground/excited excitons, with their relative weights transitioning
from one component dominant to strongly hybridized at higher pressures. Our
findings highlight the potential for controlling excitonic properties and
engineering novel optoelectronic devices through interlayer compression.",['cond-mat.mtrl-sci'],2501.14873," Local-type primordial non-Gaussianity (PNG), predicted by many non-minimal
models of inflation, creates a scale-dependent contribution to the power
spectrum of large-scale structure (LSS) tracers. Its amplitude is characterized
by the product $b_\phi f_{\rm NL}^{\rm loc}$, where $b_\phi$ is an
astrophysical parameter dependent on the properties of the tracer. However,
$b_\phi$ exhibits significant secondary dependence on halo concentration and
other astrophysical properties, which may bias and weaken the constraints on
$f_{\rm NL}^{\rm loc}$. In this work, we demonstrate that incorporating
knowledge of the relation between Lagrangian bias parameters and $b_\phi$ can
significantly enhance PNG constraints. We employ the Hybrid Effective Field
Theory (HEFT) approach at the field-level and a linear regression model to seek
a connection between the bias parameters and $b_{\phi}$ for halo and galaxy
samples, constructed using the \textsc{AbacusSummit} simulation suite and
mimicking the luminous red galaxies (LRGs) and quasi-stellar objects (QSOs) of
the Dark Energy Spectroscopic Instrument (DESI) survey. For the fixed-mass halo
samples, our full bias model reduces the uncertainty by more than 70\%, with
most of that improvement coming from $b_\nabla$, which we find to be an
excellent proxy for concentration. For the galaxy samples, our model reduces
the uncertainty on $b_\phi$ by 80\% for all tracers. By adopting
Lagrangian-bias informed priors on the parameter $b_\phi$, future analyses can
thus constrain $f_{\rm NL}^{\rm loc}$ with less bias and smaller errors.","['astro-ph.CO', 'astro-ph.GA']",False,,,,Pressure Tuning of Layer-hybridized Excitons in Trilayer WSe2,"Refining local-type primordial non-Gaussianity: Sharpened $b_\phi$
  constraints through bias expansion"
neg-d2-550,2025-02-08,,2502.05583," Sensor placement plays a crucial role in graph signal recovery in
underdetermined systems. In this paper, we present the graph-filtered
regularized maximum likelihood (GFR-ML) estimator of graph signals, which
integrates general graph filtering with regularization to enhance signal
recovery performance under a limited number of sensors. Then, we investigate
task-based sampling allocation aimed at minimizing the mean squared error (MSE)
of the GFR-ML estimator by wisely choosing sensor placement. Since this MSE
depends on the unknown graph signals to be estimated, we propose four cost
functions for the optimization of the sampling allocation: the biased
Cram$\acute{\text{e}}$r-Rao bound (bCRB), the worst-case MSE (WC-MSE), the
Bayesian MSE (BMSE), and the worst-case BMSE (WC-BMSE), where the last two
assume a Gaussian prior. We investigate the properties of these cost functions
and develop two algorithms for their practical implementation: 1) the
straightforward greedy algorithm; and 2) the alternating projection gradient
descent (PGD) algorithm that reduces the computational complexity. Simulation
results on synthetic and real-world datasets of the IEEE 118-bus power system
and the Minnesota road network demonstrate that the proposed sampling
allocation methods reduce the MSE by up to $50\%$ compared to the common
sampling methods A-design, E-design, and LR-design in the tested scenarios.
Thus, the proposed methods improve the estimation performance and reduce the
required number of measurements in graph signal processing (GSP)-based signal
recovery in the case of underdetermined systems.",['eess.SP'],2501.09803," Estimating the shortest travel time and providing route recommendation
between different locations in a city or region can quantitatively measure the
conditions of the transportation network during or after extreme events. One
common approach is to use Dijkstra's Algorithm, which produces the shortest
path as well as the shortest distance. However, this option is computationally
expensive when applied to large-scale networks. This paper proposes a novel
fast framework based on graph neural networks (GNNs) which approximate the
single-source shortest distance between pairs of locations, and predict the
single-source shortest path subsequently. We conduct multiple experiments on
synthetic graphs of different size to demonstrate the feasibility and
computational efficiency of the proposed model. In real-world case studies, we
also applied the proposed method of flood risk analysis of coastal urban areas
to calculate delays in evacuation to public shelters during hurricanes. The
results indicate the accuracy and computational efficiency of the GNN model,
and its potential for effective implementation in emergency planning and
management.",['cs.LG'],False,,,,"Efficient Sampling Allocation Strategies for General Graph-Filter-Based
  Signal Recovery","Graph Neural Networks for Travel Distance Estimation and Route
  Recommendation Under Probabilistic Hazards"
neg-d2-551,2025-03-17,,2503.13388," In this work, we develop a novel mathematical framework for universal digital
quantum computation using algebraic probability theory. We rigorously define
quantum circuits as finite sequences of elementary quantum gates and establish
their role in implementing unitary transformations. A key result demonstrates
that every unitary matrix in \(\mathrm{U}(N)\) can be expressed as a product of
elementary quantum gates, leading to the concept of a universal dictionary for
quantum computation. We apply this framework to the construction of quantum
circuits that encode probability distributions, focusing on the Grover-Rudolph
algorithm. By leveraging controlled quantum gates and rotation matrices, we
design a quantum circuit that approximates a given probability density
function. Numerical simulations, conducted using Qiskit, confirm the
theoretical predictions and validate the effectiveness of our approach. These
results provide a rigorous foundation for quantum circuit synthesis within an
algebraic probability framework and offer new insights into the encoding of
probability distributions in quantum algorithms. Potential applications include
quantum machine learning, circuit optimization, and experimental
implementations on real quantum hardware.","['quant-ph', 'cs.NA', 'math.NA']",2501.13681," Particle discretizations of partial differential equations are advantageous
for high-dimensional kinetic models in phase space due to their better
scalability than continuum approaches with respect to dimension. Complex
processes collectively referred to as \textit{particle noise} hamper long-time
simulations with particle methods. One approach to address this problem is
particle mesh adaptivity or remapping, known as \textit{particle resampling}.
This paper introduces a resampling method that projects particles to and from a
(finite element) function space. The method is simple; using standard sparse
linear algebra and finite element techniques, it can adapt to almost any set of
new particle locations and preserves all moments up to the order of polynomial
represented exactly by the continuum function space.
  This work is motivated by the Vlasov-Maxwell-Landau model of magnetized
plasmas with up to six dimensions, $3X$ in physical space and $3V$ in velocity
space, and is developed in the context of a $1X$ + $1V$ Vlasov-Poisson model of
Landau damping with logically regular particle and continuum phase space grids.
The evaluation codes are publicly available, along with the data and
reproducibility artifacts, and developed in the PETSc numerical library
(petsc.org).","['physics.plasm-ph', 'physics.comp-ph']",False,,,,"A mathematical model for a universal digital quantum computer with an
  application to the Grover-Rudolph algorithm",A projection method for particle resampling
neg-d2-552,2025-02-13,,2502.09455," Formulas for the combined nuclear-recoil and finite-nuclear-size effects of
order $(Z\,\alpha)^5$ and $(Z\,\alpha)^6$ are derived without any expansion in
the nuclear charge radius $r_C$, making them applicable to both electronic and
muonic atoms. The obtained results are particularly relevant for high-precision
determinations of root-mean-square charge radii from muonic atom spectroscopy.
We demonstrate that calculations of the atomic isotope shift based on the
widely used Breit approximation give rise to an unphysical nuclear-size
contribution that is linear in the nuclear charge radius $r_C$ at order
$(Z\,\alpha)^5$. This spurious term vanishes in a full QED treatment, leaving
the correct contribution quadratic in $r_C$. For electronic atoms, this
quadratic term is significantly smaller than the spurious linear contribution.",['physics.atom-ph'],2502.1977," With the increasing prevalence of Web-based platforms handling vast amounts
of user data, machine unlearning has emerged as a crucial mechanism to uphold
users' right to be forgotten, enabling individuals to request the removal of
their specified data from trained models. However, the auditing of machine
unlearning processes remains significantly underexplored. Although some
existing methods offer unlearning auditing by leveraging backdoors, these
backdoor-based approaches are inefficient and impractical, as they necessitate
involvement in the initial model training process to embed the backdoors. In
this paper, we propose a TAilored Posterior diffErence (TAPE) method to provide
unlearning auditing independently of original model training. We observe that
the process of machine unlearning inherently introduces changes in the model,
which contains information related to the erased data. TAPE leverages
unlearning model differences to assess how much information has been removed
through the unlearning operation. Firstly, TAPE mimics the unlearned posterior
differences by quickly building unlearned shadow models based on first-order
influence estimation. Secondly, we train a Reconstructor model to extract and
evaluate the private information of the unlearned posterior differences to
audit unlearning. Existing privacy reconstructing methods based on posterior
differences are only feasible for model updates of a single sample. To enable
the reconstruction effective for multi-sample unlearning requests, we propose
two strategies, unlearned data perturbation and unlearned influence-based
division, to augment the posterior difference. Extensive experimental results
indicate the significant superiority of TAPE over the state-of-the-art
unlearning verification methods, at least 4.5$\times$ efficiency speedup and
supporting the auditing for broader unlearning scenarios.","['cs.CR', 'cs.LG']",False,,,,Recoil nuclear size corrections in hydrogenic systems,TAPE: Tailored Posterior Difference for Auditing of Machine Unlearning
neg-d2-553,2025-01-07,,2501.0406," In recent years, traffic flow prediction has played a crucial role in the
management of intelligent transportation systems. However, traditional
prediction methods are often limited by static spatial modeling, making it
difficult to accurately capture the dynamic and complex relationships between
time and space, thereby affecting prediction accuracy. This paper proposes an
innovative traffic flow prediction network, SFADNet, which categorizes traffic
flow into multiple traffic patterns based on temporal and spatial feature
matrices. For each pattern, we construct an independent adaptive
spatio-temporal fusion graph based on a cross-attention mechanism, employing
residual graph convolution modules and time series modules to better capture
dynamic spatio-temporal relationships under different fine-grained traffic
patterns. Extensive experimental results demonstrate that SFADNet outperforms
current state-of-the-art baselines across four large-scale datasets.",['cs.LG'],2502.07338," Ultrafast electron microscopy aims for imaging transient phenomena occurring
on nanoscale. One of its goals is to visualize localized optical and plasmonic
modes generated by coherent excitation in the vicinity of various types of
nanostructures. Such imaging capability was enabled by photon-induced
near-field optical microscopy, which is based on spectral filtering of
electrons inelastically scattered due to the stimulated interaction with the
near-field. Here we report on the development of ultrafast 4D scanning
transmission electron microscopy, which allows us to image the transverse
components of the optical near-field while avoiding the need of electron
spectral filtering. We demonstrate that this method is capable of imaging
optical near-fields of a tungsten nanotip and ponderomotive potential of an
optical standing wave with a spatial resolution of 21 nm.","['physics.optics', 'physics.ins-det']",False,,,,"SFADNet: Spatio-temporal Fused Graph based on Attention Decoupling
  Network for Traffic Prediction","Ultrafast 4D scanning transmission electron microscopy for imaging of
  localized optical fields"
neg-d2-554,2025-03-17,,2503.13214," Currently, deep learning-based methods for remote sensing pansharpening have
advanced rapidly. However, many existing methods struggle to fully leverage
feature heterogeneity and redundancy, thereby limiting their effectiveness. We
use the covariance matrix to model the feature heterogeneity and redundancy and
propose Correlation-Aware Covariance Weighting (CACW) to adjust them. CACW
captures these correlations through the covariance matrix, which is then
processed by a nonlinear function to generate weights for adjustment. Building
upon CACW, we introduce a general adaptive dual-level weighting mechanism
(ADWM) to address these challenges from two key perspectives, enhancing a wide
range of existing deep-learning methods. First, Intra-Feature Weighting (IFW)
evaluates correlations among channels within each feature to reduce redundancy
and enhance unique information. Second, Cross-Feature Weighting (CFW) adjusts
contributions across layers based on inter-layer correlations, refining the
final output. Extensive experiments demonstrate the superior performance of
ADWM compared to recent state-of-the-art (SOTA) methods. Furthermore, we
validate the effectiveness of our approach through generality experiments,
redundancy visualization, comparison experiments, key variables and complexity
analysis, and ablation studies. Our code is available at
https://github.com/Jie-1203/ADWM.","['cs.CV', 'cs.AI']",2503.08367," Occlusion is one of the fundamental challenges in crowd counting. In the
community, various data-driven approaches have been developed to address this
issue, yet their effectiveness is limited. This is mainly because most existing
crowd counting datasets on which the methods are trained are based on passive
cameras, restricting their ability to fully sense the environment. Recently,
embodied navigation methods have shown significant potential in precise object
detection in interactive scenes. These methods incorporate active camera
settings, holding promise in addressing the fundamental issues in crowd
counting. However, most existing methods are designed for indoor navigation,
showing unknown performance in analyzing complex object distribution in large
scale scenes, such as crowds. Besides, most existing embodied navigation
datasets are indoor scenes with limited scale and object quantity, preventing
them from being introduced into dense crowd analysis. Based on this, a novel
task, Embodied Crowd Counting (ECC), is proposed. We first build up an
interactive simulator, Embodied Crowd Counting Dataset (ECCD), which enables
large scale scenes and large object quantity. A prior probability distribution
that approximates realistic crowd distribution is introduced to generate
crowds. Then, a zero-shot navigation method (ZECC) is proposed. This method
contains a MLLM driven coarse-to-fine navigation mechanism, enabling active
Z-axis exploration, and a normal-line-based crowd distribution analysis method
for fine counting. Experimental results against baselines show that the
proposed method achieves the best trade-off between counting accuracy and
navigation cost.",['cs.CV'],False,,,,"A General Adaptive Dual-level Weighting Mechanism for Remote Sensing
  Pansharpening",Embodied Crowd Counting
neg-d2-555,2025-02-27,,2503.00069," Recent progress in large language models (LLMs) has focused on producing
responses that meet human expectations and align with shared values - a process
coined alignment. However, aligning LLMs remains challenging due to the
inherent disconnect between the complexity of human values and the narrow
nature of the technological approaches designed to address them. Current
alignment methods often lead to misspecified objectives, reflecting the broader
issue of incomplete contracts, the impracticality of specifying a contract
between a model developer, and the model that accounts for every scenario in
LLM alignment. In this paper, we argue that improving LLM alignment requires
incorporating insights from societal alignment frameworks, including social,
economic, and contractual alignment, and discuss potential solutions drawn from
these domains. Given the role of uncertainty within societal alignment
frameworks, we then investigate how it manifests in LLM alignment. We end our
discussion by offering an alternative view on LLM alignment, framing the
underspecified nature of its objectives as an opportunity rather than perfect
their specification. Beyond technical improvements in LLM alignment, we discuss
the need for participatory alignment interface designs.","['cs.CY', 'cs.AI', 'cs.CL']",2502.12705," Two-dimensional (2D) layered nanomaterials heterostructures, arising from the
combination of 2D materials with other low-dimensional species, feature large
surface area to volume ratio, which provides a high density of active sites for
catalytic ap-plications and in particular for (photo)electrocatalysis (PEC).
Meanwhile, their unique electronic band structure and high electrical
conductivity enable efficient charge transfer (CT) between the active material
and the substrate, which is essential for catalytic activity. In recent years,
researchers have demonstrated the potential of a range of 2D material
interfaces, such as graphene, graphitic carbon nitride (g-C3N4), metal
chalcogenides (MCs), and MXenes, for (photo)electrocatalytic applica-tions. For
instance, MCs such as MoS2 and WS2 have shown excellent catalytic activity for
hydrogen evolution, while gra-phene and MXenes have been used for the reduction
of carbon dioxide to higher value chemicals. However, despite their great
potential, there are still major challenges that need to be addressed in order
to fully realize the potential of 2D materials for PEC. For example, their
stability under harsh reaction conditions, as well as their scalability for
large-scale production are important factors to be considered. Generating
heterojunctions (HJs) by combining 2D layered structures with other
na-nomaterials is a promising method to improve the photoelectrocatalytic
properties of the former. In this review, we inspect thoroughly the recent
literature, to demonstrate the significant potential that arises from utilizing
2D layered heterostructures in PEC processes across a broad spectrum of
applications, from energy conversion and storage to environmental remediation.
With the ongoing research and development, it is likely that the potential of
these materials will be fully expressed in the near future.",['cond-mat.mtrl-sci'],False,,,,Societal Alignment Frameworks Can Improve LLM Alignment,2D Layered Heterojunctions for Photoelectrocatalysis
neg-d2-556,2025-01-23,,2502.08654," Entropy and its various generalizations are important in many fields,
including mathematical statistics, communication theory, physics and computer
science, for characterizing the amount of information associated with a
probability distribution. In this paper we propose goodness-of-fit statistics
for the multivariate Student and multivariate Pearson type II distributions,
based on the maximum entropy principle and a class of estimators for Renyi
entropy based on nearest neighbour distances. We prove the L2-consistency of
these statistics using results on the subadditivity of Euclidean functionals on
nearest neighbour graphs, and investigate their rate of convergence and
asymptotic distribution using Monte Carlo methods.","['stat.ME', 'math.ST', 'stat.TH']",2502.04365," Approximately 10% of newborns need some assistance to start breathing and 5\%
proper ventilation. It is crucial that interventions are initiated as soon as
possible after birth. Accurate documentation of Time of Birth (ToB) is thereby
essential for documenting and improving newborn resuscitation performance.
However, current clinical practices rely on manual recording of ToB, typically
with minute precision. In this study, we present an AI-driven, video-based
system for automated ToB detection using thermal imaging, designed to preserve
the privacy of healthcare providers and mothers by avoiding the use of
identifiable visual data. Our approach achieves 91.4% precision and 97.4%
recall in detecting ToB within thermal video clips during performance
evaluation. Additionally, our system successfully identifies ToB in 96% of test
cases with an absolute median deviation of 1 second compared to manual
annotations. This method offers a reliable solution for improving ToB
documentation and enhancing newborn resuscitation outcomes.","['cs.CV', 'cs.AI']",False,,,,Statistical tests based on Renyi entropy estimation,"AI-Based Thermal Video Analysis in Privacy-Preserving Healthcare: A Case
  Study on Detecting Time of Birth"
neg-d2-557,2025-02-09,,2502.06011," Off-policy evaluation (OPE) is a critical challenge in robust decision-making
that seeks to assess the performance of a new policy using data collected under
a different policy. However, the existing OPE methodologies suffer from several
limitations arising from statistical uncertainty as well as causal
considerations. In this thesis, we address these limitations by presenting
three different works. Firstly, we consider the problem of high variance in the
importance-sampling-based OPE estimators. We introduce the Marginal Ratio (MR)
estimator, a novel OPE method that reduces variance by focusing on the marginal
distribution of outcomes rather than direct policy shifts, improving robustness
in contextual bandits. Next, we propose Conformal Off-Policy Prediction (COPP),
a principled approach for uncertainty quantification in OPE that provides
finite-sample predictive intervals, ensuring robust decision-making in
risk-sensitive applications. Finally, we address causal unidentifiability in
off-policy decision-making by developing novel bounds for sequential decision
settings, which remain valid under arbitrary unmeasured confounding. We apply
these bounds to assess the reliability of digital twin models, introducing a
falsification framework to identify scenarios where model predictions diverge
from real-world behaviour. Our contributions provide new insights into robust
decision-making under uncertainty and establish principled methods for
evaluating policies in both static and dynamic settings.","['stat.ML', 'cs.LG']",2502.0439," Despite remarkable capabilities, large language models (LLMs) struggle to
continually update their knowledge without catastrophic forgetting. In
contrast, humans effortlessly integrate new information, detect conflicts with
existing beliefs, and selectively update their mental models. This paper
introduces a cognitive-inspired investigation paradigm to study continual
knowledge updating in LLMs. We implement two key components inspired by human
cognition: (1) Dissonance and Familiarity Awareness, analyzing model behavior
to classify information as novel, familiar, or dissonant; and (2) Targeted
Network Updates, which track neural activity to identify frequently used
(stubborn) and rarely used (plastic) neurons. Through carefully designed
experiments in controlled settings, we uncover a number of empirical findings
demonstrating the potential of this approach. First, dissonance detection is
feasible using simple activation and gradient features, suggesting potential
for cognitive-inspired training. Second, we find that non-dissonant updates
largely preserve prior knowledge regardless of targeting strategy, revealing
inherent robustness in LLM knowledge integration. Most critically, we discover
that dissonant updates prove catastrophically destructive to the model's
knowledge base, indiscriminately affecting even information unrelated to the
current updates. This suggests fundamental limitations in how neural networks
handle contradictions and motivates the need for new approaches to knowledge
updating that better mirror human cognitive mechanisms.","['cs.CL', 'cs.AI', 'cs.LG', 'q-bio.NC']",False,,,,"Uncertainty Quantification and Causal Considerations for Off-Policy
  Decision Making","In Praise of Stubbornness: The Case for Cognitive-Dissonance-Aware
  Knowledge Updates in LLMs"
neg-d2-558,2025-01-16,,2501.09671," Flapping-based propulsive systems rely on fluid-structure interactions to
produce thrust. At intermediate and high Reynolds numbers, vortex formation and
organization in the wake of such systems are crucial for the generation of a
propulsive force. In this work, we experimentally investigate the wake produced
by a tethered robotic fish immersed in a water tunnel. By systematically
varying the amplitude and frequency of the fish tail as well as the free-stream
speed, we are able to observe and characterize different vortex streets as a
function of the Strouhal number. The produced wakes are three-dimensional and
exhibit a classical V-shape, mainly with two oblique trains of vortex rings
convecting outward. Using two-dimensional Particle Image Velocimetry (PIV) in
the mid-span plane behind the fish and through extensive data processing of the
velocity and vorticity fields, we demonstrate the strong couplings at place
between vortex dynamics, thrust production and wake structure. We first measure
the evolution of the vortex velocity with the Strouhal number, and model it
using a momentum balance equation directly related to thrust production. We
then focus on the wake structure, such as wake angle as well as vortex ring
orientation, diameter and vorticity. The wake structure is modelled in a simple
geometrical framework where the vortex ring velocity is composed of the
free-stream speed and the ring self-advecting speed. This framework is tested
and validated by our experimental measurements as well as literature data
collapsing on master curves, highlighting a universal behavior dominated by the
Strouhal number. This allows us to establish a comprehensive understanding of
how the wake structure varies with this number and, thus, thrust production.",['physics.flu-dyn'],2501.05023," The ESA Euclid mission will survey more than 14,000 deg$^2$ of the sky in
visible and near-infrared wavelengths, mapping the extra-galactic sky to
constrain our cosmological model of the Universe. Although the survey focusses
on regions further than 15 deg from the ecliptic, it should allow for the
detection of more than about $10^5$ Solar System objects (SSOs). After
simulating the expected signal from SSOs in Euclid images acquired with the
visible camera (VIS), we describe an automated pipeline developed to detect
moving objects with an apparent velocity in the range of 0.1-10 arcsec/h,
typically corresponding to sources in the outer Solar System (from Centaurs to
Kuiper-belt objects). In particular, the proposed detection scheme is based on
Sourcextractor software and on applying a new algorithm capable of associating
moving objects amongst different catalogues. After applying a suite of filters
to improve the detection quality, we study the expected purity and completeness
of the SSO detections. We also show how a Kohonen self-organising neural
network can be successfully trained (in an unsupervised fashion) to classify
stars, galaxies, and SSOs. By implementing an early-stopping method in the
training scheme, we show that the network can be used in a predictive way,
allowing one to assign the probability of each detected object being a member
of each considered class.",['astro-ph.IM'],False,,,,"Undulatory underwater swimming: Linking vortex dynamics, thrust, and
  wake structure with a biorobotic fish","Euclid: Detecting Solar System objects in Euclid images and classifying
  them using Kohonen self-organising maps"
neg-d2-559,2025-01-18,,2501.10778," Large Neighbourhood Search (LNS) is a powerful heuristic framework for
solving Mixed-Integer Programming (MIP) problems. However, designing effective
variable selection strategies in LNS remains challenging, especially for
diverse sets of problems. In this paper, we propose an approach that integrates
Machine Learning (ML) within the destroy operator of LNS for MIPs with a focus
on minimal offline training. We implement a modular LNS matheuristic as a test
bench to compare different LNS heuristics, including our ML-enhanced LNS.
Experimental results on the MIPLIB 2017 dataset demonstrate that the
matheuristic can significantly improve the performance of state-of-the-art
solvers like Gurobi and SCIP. We conduct analyses on noisy oracles to explore
the impact of prediction accuracy on solution quality. Additionally, we develop
techniques to enhance the ML model through loss adjustments and sampling
routines. Our findings suggest that while random LNS remains competitive, our
Supervised LNS (SLNS) outperforms other baselines and helps set the foundation
for future research on ML for LNS methods that are both efficient and general.",['math.OC'],2501.05638," We show that it is NP-hard to distinguish graphs of linear mim-width at most
1211 from graphs of sim-width at least 1216. This implies that Mim-Width,
Sim-Width, One-Sided Mim-Width, and their linear counterparts are all
paraNP-complete, i.e., NP-complete to compute even when upper bounded by a
constant.","['cs.CC', 'cs.DM', 'cs.DS', 'math.CO']",False,,,,Supervised Large Neighbourhood Search for MIPs,Mim-Width is paraNP-complete
neg-d2-560,2025-01-05,,2501.02487," We report ACE++, an instruction-based diffusion framework that tackles
various image generation and editing tasks. Inspired by the input format for
the inpainting task proposed by FLUX.1-Fill-dev, we improve the Long-context
Condition Unit (LCU) introduced in ACE and extend this input paradigm to any
editing and generation tasks. To take full advantage of image generative
priors, we develop a two-stage training scheme to minimize the efforts of
finetuning powerful text-to-image diffusion models like FLUX.1-dev. In the
first stage, we pre-train the model using task data with the 0-ref tasks from
the text-to-image model. There are many models in the community based on the
post-training of text-to-image foundational models that meet this training
paradigm of the first stage. For example, FLUX.1-Fill-dev deals primarily with
painting tasks and can be used as an initialization to accelerate the training
process. In the second stage, we finetune the above model to support the
general instructions using all tasks defined in ACE. To promote the widespread
application of ACE++ in different scenarios, we provide a comprehensive set of
models that cover both full finetuning and lightweight finetuning, while
considering general applicability and applicability in vertical scenarios. The
qualitative analysis showcases the superiority of ACE++ in terms of generating
image quality and prompt following ability. Code and models will be available
on the project page: https://ali-vilab. github.io/ACE_plus_page/.",['cs.CV'],2501.07269," Let $k\leq n$ be positive integers and $\mathbb{Z}_{n}$ be the set of
integers modulo $n$. A conjecture of Baranyai from 1974 asks for a
decomposition of $k$-element subsets of $\mathbb{Z}_{n}$ into particular
families of sets called ""wreaths"". We approach this conjecture from a new
algebraic angle by introducing the key object of this paper, the wreath matrix
$M$. As our first result, we establish that Baranyai's conjecture is equivalent
to the existence of a particular vector in the kernel of $M$. We then employ
results from representation theory to study $M$ and its spectrum in detail. In
particular, we find all eigenvalues of $M$ and their multiplicities, and
identify several families of vectors which lie in the kernel of $M$.","['math.CO', 'math.RT']",False,,,,"ACE++: Instruction-Based Image Creation and Editing via Context-Aware
  Content Filling",The wreath matrix
neg-d2-561,2025-03-05,,2503.04086," Gcd-graphs represent an interesting and historically important class of
integral graphs. Since the pioneering work of Klotz and Sander, numerous
incarnations of these graphs have been explored in the literature. In this
article, we define and establish some foundational properties of gcd-graphs
defined over a general finite commutative ring. In particular, we investigate
the connectivity and diameter of these graphs. Additionally, when the ring is a
finite symmetric $\mathbb{Z}/n$-algebra, we give an explicit description of
their spectrum using the theory of Ramanujan sums that gives a unified
treatment of various results in the literature.","['math.NT', 'math.AC', 'math.CO']",2501.14118," We develop a new methodology to select scenarios of DER adoption most
critical for distribution grids. Anticipating risks of future voltage and line
flow violations due to additional PV adopters is central for utility investment
planning but continues to rely on deterministic or ad hoc scenario selection.
We propose a highly efficient search framework based on multi-objective
Bayesian Optimization. We treat underlying grid stress metrics as
computationally expensive black-box functions, approximated via Gaussian
Process surrogates and design an acquisition function based on probability of
scenarios being Pareto-critical across a collection of line- and bus-based
violation objectives. Our approach provides a statistical guarantee and offers
an order of magnitude speed-up relative to a conservative exhaustive search.
Case studies on realistic feeders with 200-400 buses demonstrate the
effectiveness and accuracy of our approach.","['cs.LG', 'stat.AP', 'stat.ML']",False,,,,On gcd-graphs over finite rings,"Selecting Critical Scenarios of DER Adoption in Distribution Grids Using
  Bayesian Optimization"
neg-d2-562,2025-01-25,,2501.15192," A subset of a topological space possesses the Baire property if it can be
  covered by an open set up to a meagre set. For the Cantor space of infinite
  words Finkel introduced the automatic Baire category where both sets, the
  open and the meagre, can be chosen to be definable by finite automata. Here
  we show that, given a Muller automaton defining the original set, resulting
  open and meagre sets can be constructed in polynomial time.
  Since the constructed sets are of simple topological structure, it is
  possible to construct not only Muller automata defining them but also the
  simpler B\""uchi automata. To this end we give, for a restricted class of
  Muller automata, a conversion to equivalent B\""uchi automata of at most
  quadratic size.",['cs.FL'],2502.12683," We investigate the nuclear Stark effect induced in hydrogen-like atomic
nuclei under super-intense laser fields. Since laser wavelengths are generally
larger than nuclear dimensions, direct laser-nucleus interaction is unfeasible.
Instead, this effect is induced indirectly through electron oscillations in the
laser field, which produce a periodic electric field that shifts the nuclear
energy levels. Using perturbation theory, we derive an expression for the
energy shift and dynamic polarizability of the nucleus as a function of laser
parameters. Our findings reveal that the Nuclear Stark effect can be controlled
by adjusting the laser frequency and intensity, potentially enabling
applications in nuclear and quantum optical systems.",['nucl-th'],False,,,,A polynomial-time algorithm for the automatic Baire property,"AC nuclear Stark effect in H-atom via super-intense laser-atom
  interaction"
neg-d2-563,2025-02-22,,2502.16101," Retrieval-augmented generation (RAG) has shown impressive capabilities in
mitigating hallucinations in large language models (LLMs). However, LLMs
struggle to handle misleading retrievals and often fail to maintain their own
reasoning when exposed to conflicting or selectively-framed evidence, making
them vulnerable to real-world misinformation. In such real-world retrieval
scenarios, misleading and conflicting information is rampant, particularly in
the political domain, where evidence is often selectively framed, incomplete,
or polarized. However, existing RAG benchmarks largely assume a clean retrieval
setting, where models succeed by accurately retrieving and generating answers
from gold-standard documents. This assumption fails to align with real-world
conditions, leading to an overestimation of RAG system performance. To bridge
this gap, we introduce RAGuard, a fact-checking dataset designed to evaluate
the robustness of RAG systems against misleading retrievals. Unlike prior
benchmarks that rely on synthetic noise, our dataset constructs its retrieval
corpus from Reddit discussions, capturing naturally occurring misinformation.
It categorizes retrieved evidence into three types: supporting, misleading, and
irrelevant, providing a realistic and challenging testbed for assessing how
well RAG systems navigate different retrieval information. Our benchmark
experiments reveal that when exposed to misleading retrievals, all tested
LLM-powered RAG systems perform worse than their zero-shot baselines (i.e., no
retrieval at all), highlighting their susceptibility to noisy environments. To
the best of our knowledge, RAGuard is the first benchmark to systematically
assess RAG robustness against misleading evidence. We expect this benchmark
will drive future research toward improving RAG systems beyond idealized
datasets, making them more reliable for real-world applications.","['cs.AI', 'cs.IR']",2501.08945," Adjusting for covariates in randomized controlled trials can enhance the
credibility and efficiency of treatment effect estimation. However, handling
numerous covariates and their complex (non-linear) transformations poses a
challenge. Motivated by the case study of the Best Apnea Interventions for
Research (BestAIR) trial data from the National Sleep Research Resource (NSRR),
where the number of covariates (p=114) is comparable to the sample size
(N=196), we propose a principled Covariate Adjustment with Variable Selection
(COADVISE) framework. COADVISE enables variable selection for covariates most
relevant to the outcome while accommodating both linear and nonlinear
adjustments. This framework ensures consistent estimates with improved
efficiency over unadjusted estimators and provides robust variance estimation,
even under outcome model misspecification. We demonstrate efficiency gains
through theoretical analysis, extensive simulations, and a re-analysis of the
BestAIR trial data to compare alternative variable selection strategies,
offering cautionary recommendations. A user-friendly R package, Coadvise, is
available to facilitate practical implementation.",['stat.ME'],False,,,,"Worse than Zero-shot? A Fact-Checking Dataset for Evaluating the
  Robustness of RAG Against Misleading Retrievals","COADVISE: Covariate Adjustment with Variable Selection in Randomized
  Controlled Trials"
neg-d2-564,2025-02-19,,2502.13688," This work addresses the $K$-user computation broadcast problem consisting of
a master node, that holds all datasets and users for a general class of
function demands, including linear and non-linear functions, over finite
fields. The master node sends a broadcast message to enable each of $K$
distributed users to compute its demanded function in an asymptotically
lossless manner with user's side information. We derive bounds on the optimal
$K$-user computation broadcast rate that allows the users to compute their
demanded functions by capturing the structures of the computations and
available side information. Our achievability scheme involves the design of a
novel graph-based coding model to build a broadcast message to meet each user's
demand, by leveraging the structural dependencies among the datasets, the user
demands, and the side information of each user, drawing on K{\""o}rner's
characteristic graph framework. The converse uses the structures of the demands
and the side information available at $K$ users to yield a tight lower bound on
the broadcast rate. With the help of examples, we demonstrate our scheme
achieves a better communication rate than the existing state of the art.","['cs.IT', 'math.IT']",2501.0387," Slot and intent detection (SID) is a classic natural language understanding
task. Despite this, research has only more recently begun focusing on SID for
dialectal and colloquial varieties. Many approaches for low-resource scenarios
have not yet been applied to dialectal SID data, or compared to each other on
the same datasets. We participate in the VarDial 2025 shared task on slot and
intent detection in Norwegian varieties, and compare multiple set-ups: varying
the training data (English, Norwegian, or dialectal Norwegian), injecting
character-level noise, training on auxiliary tasks, and applying Layer
Swapping, a technique in which layers of models fine-tuned on different
datasets are assembled into a model. We find noise injection to be beneficial
while the effects of auxiliary tasks are mixed. Though some experimentation was
required to successfully assemble a model from layers, it worked surprisingly
well; a combination of models trained on English and small amounts of dialectal
data produced the most robust slot predictions. Our best models achieve 97.6%
intent accuracy and 85.6% slot F1 in the shared task.",['cs.CL'],False,,,,Non-Linear Function Computation Broadcast,"Add Noise, Tasks, or Layers? MaiNLP at the VarDial 2025 Shared Task on
  Norwegian Dialectal Slot and Intent Detection"
neg-d2-565,2025-02-06,,2502.04565," This paper presents an implementation of machine learning model training
using private federated learning (PFL) on edge devices. We introduce a novel
framework that uses PFL to address the challenge of training a model using
users' private data. The framework ensures that user data remain on individual
devices, with only essential model updates transmitted to a central server for
aggregation with privacy guarantees. We detail the architecture of our app
selection model, which incorporates a neural network with attention mechanisms
and ambiguity handling through uncertainty management. Experiments conducted
through off-line simulations and on device training demonstrate the feasibility
of our approach in real-world scenarios. Our results show the potential of PFL
to improve the accuracy of an app selection model by adapting to changes in
user behavior over time, while adhering to privacy standards. The insights
gained from this study are important for industries looking to implement PFL,
offering a robust strategy for training a predictive model directly on edge
devices while ensuring user data privacy.","['cs.LG', 'cs.CR']",2503.13664," Defects and interfaces are essential to understand the properties of matter.
However, studying their dynamics in the quantum regime remains a challenge in
particular concerning the regime of two spatial dimensions. Recently, it has
been shown that a quantum counterpart of the hard-disk problem on a lattice
yields defects and interfaces, which are stable just due to quantum effects
while they delocalize and dissolve classically. Here, we study in more detail
the properties of defects and interfaces in this quantum hard-disk problem with
a particular emphasis on the stability of these quantum effects upon including
perturbations. Specifically, we introduce short-range soft-core interactions
between the hard disks. From both analytical arguments and numerical
simulations we find that large classes of defects and interfaces remain stable
even under such perturbations suggesting that the quantum nature of the
dynamics exhibits a large range of robustness. Our findings demonstrate the
stability and non-classical behavior of quantum interface dynamics, offering
insights into the dynamics of two-dimensional quantum matter and establishing
the quantum hard-disk model as a platform for studying unconventional
constrained quantum dynamics.","['quant-ph', 'cond-mat.stat-mech']",False,,,,Private Federated Learning In Real World Application -- A Case Study,Dynamics of defects and interfaces for interacting quantum hard disks
neg-d2-566,2025-02-15,,2502.10699," Contextual memory integration remains a high challenge in the development of
language models, particularly in tasks that require maintaining coherence over
extended sequences. Traditional approaches, such as self-attention mechanisms
and memory-augmented architectures, often prioritize short-term dependencies,
leading to fragmentation and inconsistency in long-range contextual
understanding. Inspired by principles of synaptic plasticity observed in
biological neural systems, a novel mechanism, Synaptic Resonance, is introduced
to dynamically reinforce relevant memory pathways during training and
inference. Unlike static memory representations, this mechanism continuously
adjusts synaptic weight matrices based on contextual relevance, allowing for
improved information retention without excessive computational overhead.
Evaluations conducted on an open-source language model demonstrate reductions
in perplexity, enhancements in contextual coherence, and increased robustness
against input noise, highlighting the effectiveness of reinforcement-driven
memory modulation. Comparative analysis against baseline models further reveals
that the proposed approach achieves higher memory retention efficiency while
maintaining computational feasibility. The architectural modifications
integrate seamlessly into existing transformer-based frameworks, ensuring
stable convergence and efficient inference without sacrificing scalability.
Applications benefiting from improved long-term contextual consistency, such as
dialogue systems and document summarization, stand to gain from this approach.
Empirical findings suggest that dynamically reinforced memory pathways offer a
promising alternative to conventional memory mechanisms, addressing
longstanding limitations in extended sequence modeling.","['cs.CL', 'cs.AI', 'cs.NE']",2503.12966," Score-based generative models achieve state-of-the-art sampling performance
by denoising a distribution perturbed by Gaussian noise. In this paper, we
focus on a single deterministic denoising step, and compare the optimal
denoiser for the quadratic loss, we name ''full-denoising'', to the alternative
''half-denoising'' introduced by Hyv{\""a}rinen (2024). We show that looking at
the performances in term of distance between distribution tells a more nuanced
story, with different assumptions on the data leading to very different
conclusions. We prove that half-denoising is better than full-denoising for
regular enough densities, while full-denoising is better for singular densities
such as mixtures of Dirac measures or densities supported on a low-dimensional
subspace. In the latter case, we prove that full-denoising can alleviate the
curse of dimensionality under a linear manifold hypothesis.","['cs.LG', 'stat.ML']",False,,,,"Exploring Synaptic Resonance in Large Language Models: A Novel Approach
  to Contextual Memory Integration","Optimal Denoising in Score-Based Generative Models: The Role of Data
  Regularity"
neg-d2-567,2025-01-01,,2501.01003," 3D Gaussian Splatting (3DGS) techniques have achieved satisfactory 3D scene
representation. Despite their impressive performance, they confront challenges
due to the limitation of structure-from-motion (SfM) methods on acquiring
accurate scene initialization, or the inefficiency of densification strategy.
In this paper, we introduce a novel framework EasySplat to achieve high-quality
3DGS modeling. Instead of using SfM for scene initialization, we employ a novel
method to release the power of large-scale pointmap approaches. Specifically,
we propose an efficient grouping strategy based on view similarity, and use
robust pointmap priors to obtain high-quality point clouds and camera poses for
3D scene initialization. After obtaining a reliable scene structure, we propose
a novel densification approach that adaptively splits Gaussian primitives based
on the average shape of neighboring Gaussian ellipsoids, utilizing KNN scheme.
In this way, the proposed method tackles the limitation on initialization and
optimization, leading to an efficient and accurate 3DGS modeling. Extensive
experiments demonstrate that EasySplat outperforms the current state-of-the-art
(SOTA) in handling novel view synthesis.",['cs.CV'],2501.02564," Multi-view clustering (MvC) aims to integrate information from different
views to enhance the capability of the model in capturing the underlying data
structures. The widely used joint training paradigm in MvC is potentially not
fully leverage the multi-view information, since the imbalanced and
under-optimized view-specific features caused by the uniform learning objective
for all views. For instance, particular views with more discriminative
information could dominate the learning process in the joint training paradigm,
leading to other views being under-optimized. To alleviate this issue, we first
analyze the imbalanced phenomenon in the joint-training paradigm of multi-view
clustering from the perspective of gradient descent for each view-specific
feature extractor. Then, we propose a novel balanced multi-view clustering
(BMvC) method, which introduces a view-specific contrastive regularization
(VCR) to modulate the optimization of each view. Concretely, VCR preserves the
sample similarities captured from the joint features and view-specific ones
into the clustering distributions corresponding to view-specific features to
enhance the learning process of view-specific feature extractors. Additionally,
a theoretical analysis is provided to illustrate that VCR adaptively modulates
the magnitudes of gradients for updating the parameters of view-specific
feature extractors to achieve a balanced multi-view learning procedure. In such
a manner, BMvC achieves a better trade-off between the exploitation of
view-specific patterns and the exploration of view-invariance patterns to fully
learn the multi-view information for the clustering task. Finally, a set of
experiments are conducted to verify the superiority of the proposed method
compared with state-of-the-art approaches on eight benchmark MvC datasets.","['cs.CV', 'cs.AI', 'cs.LG']",False,,,,EasySplat: View-Adaptive Learning makes 3D Gaussian Splatting Easy,Balanced Multi-view Clustering
neg-d2-568,2025-01-14,,2501.08481," The evolution operator method is used to solve the generalized Fokker-Planck
equations and the generalized diffusion-wave equations in the (1+1) dimensional
space in which $x\in\mathbb{R}$ and $t\in\mathbb{R}_+$. These equations contain
either the first- or the second-time derivatives smeared by memory functions,
each of which forms an integral kernel (denoted by $f(\xi, t)$,
$\xi\in\mathbb{R}_+$) of suitable evolution operators. If memory functions in
the Laplace space are Stieltjes functions, then $f(\xi, t)$ satisfy
normalization, non-negativity, and infinite divisibility to be considered a
probability density function. The evolution operators also contain
exponential-like operators whose action on initial condition $p_0(x) > 0$ leads
to the parent process distribution functions. This makes the results fully
analogous to those obtained within the standard subordination approach. The
above conclusion is satisfied by the solution of the generalized Fokker-Planck
equation. In the case of the generalized diffusion-wave equation, to get this
property, we should employ a special class, namely ""diffusion-like"" initial
conditions. The key models of the operator method involve power-law memory
functions. It leads to the characterization of $f(\xi, t)$ by applying
one-sided stable L\'{e}vy distributions. The article also examines the
properties of evolution operators in terms of evolution and self-reproduction.","['math-ph', 'math.MP']",2501.05241," Late gadolinium enhancement MRI (LGE MRI) is the gold standard for the
detection of myocardial scars for post myocardial infarction (MI). LGE MRI
requires the injection of a contrast agent, which carries potential side
effects and increases scanning time and patient discomfort. To address these
issues, we propose a novel framework that combines cardiac motion observed in
cine MRI with image texture information to segment the myocardium and scar
tissue in the left ventricle. Cardiac motion tracking can be formulated as a
full cardiac image cycle registration problem, which can be solved via deep
neural networks. Experimental results prove that the proposed method can
achieve scar segmentation based on non-contrasted cine images with comparable
accuracy to LGE MRI. This demonstrates its potential as an alternative to
contrast-enhanced techniques for scar detection.","['eess.IV', 'cs.CV']",False,,,,"Operational solutions for the generalized Fokker-Planck and generalized
  diffusion-wave equations","Contrast-Free Myocardial Scar Segmentation in Cine MRI using Motion and
  Texture Fusion"
neg-d2-569,2025-01-31,,2501.19317," Large language models exhibit a remarkable capacity in language generation
and comprehension. These advances enable AI systems to produce more human-like
and emotionally engaging text. However, these models rely on a large number of
parameters, requiring significant computational resources for training and
inference. In some scenarios, accessing these resources can be challenging
(e.g., budget or hardware limitations). Techniques like reducing precision bits
can make models more memory-efficient, reducing the computational resources
needed, at the cost of reduced accuracy. This paper addresses the trade-off
between different quantization values, GPU RAM utilization, and text quality in
affective text generation (e.g., ""I really enjoy running in the snow-covered
forest""). To evaluate, we use an emotion classifier and ten seed prompts to
generate affective text. We test three setups of precision bits (8, 16, and 32)
across five open-weight language models from two different families. Our
findings demonstrate that bit reductions lead to memory savings, achieving a
reduction of 76%. However, this optimization comes with a trade-off, leading to
a decrease of up to 10 pp in F1 score for larger models and an increase of 10
pp for smaller models, along with roughly double the inference time. In terms
of text quality, larger models at lower quantization levels generally
outperform smaller, higher-precision models -- while requiring similar memory.",['cs.CL'],2503.02768," We develop a denotational model for programs that have standard programming
constructs such as conditionals and while-loops, as well as probabilistic and
concurrent commands. Whereas semantic models for languages with either
concurrency or randomization are well studied, their combination is limited to
languages with bounded loops. Our work is the first to consider both
randomization and concurrency for a language with unbounded looping constructs.
The interaction between Boolean tests (arising from the control flow
structures), probabilistic actions, and concurrent execution creates challenges
in generalizing previous work on pomsets and convex languages, prominent models
for those effects, individually. To illustrate the generality of our model, we
show that it recovers a typical powerdomain semantics for concurrency, as well
as the convex powerset semantics for probabilistic nondeterminism.","['cs.PL', 'cs.LO']",False,,,,"LLM-based Affective Text Generation Quality Based on Different
  Quantization Values",Denotational Semantics for Probabilistic and Concurrent Programs
neg-d2-570,2025-01-27,,2501.15795," In industrial settings, the accurate detection of anomalies is essential for
maintaining product quality and ensuring operational safety. Traditional
industrial anomaly detection (IAD) models often struggle with flexibility and
adaptability, especially in dynamic production environments where new defect
types and operational changes frequently arise. Recent advancements in
Multimodal Large Language Models (MLLMs) hold promise for overcoming these
limitations by combining visual and textual information processing
capabilities. MLLMs excel in general visual understanding due to their training
on large, diverse datasets, but they lack domain-specific knowledge, such as
industry-specific defect tolerance levels, which limits their effectiveness in
IAD tasks. To address these challenges, we propose Echo, a novel multi-expert
framework designed to enhance MLLM performance for IAD. Echo integrates four
expert modules: Reference Extractor which provides a contextual baseline by
retrieving similar normal images, Knowledge Guide which supplies
domain-specific insights, Reasoning Expert which enables structured, stepwise
reasoning for complex queries, and Decision Maker which synthesizes information
from all modules to deliver precise, context-aware responses. Evaluated on the
MMAD benchmark, Echo demonstrates significant improvements in adaptability,
precision, and robustness, moving closer to meeting the demands of real-world
industrial anomaly detection.",['cs.CV'],2503.00843," An important unsolved problem in Diophantine number theory is to establish a
general method to effectively find all solutions to any given $S$-unit equation
with at least four terms. Although there are many works contributing to this
problem in literature, most of which handle purely exponential Diophantine
equations, it can be said that all of them only solve finitely many equations
in a natural distinction. In this paper, we study infinitely many purely
exponential Diophantine equations with four terms of consecutive bases. Our
result states that all solutions to the equation $n^x+(n+1)^y+(n+2)^z=(n+3)^w$
in positive integers $n,x,y,z,w$ with $n \equiv 3 \pmod{4}$ are given by
$(n,x,y,z,w)=(3,3,1,1,2), (3,3,3,3,3)$. The proof uses elementary congruence
arguments developed in the study of ternary case, Baker's method in both
rational and $p$-adic cases, and the algorithm of Bert\'ok and Hajdu based on a
variant of Skolem's conjecture on purely exponential equations.",['math.NT'],False,,,,"Can Multimodal Large Language Models be Guided to Improve Industrial
  Anomaly Detection?","Solving an infinite number of purely exponential Diophantine equations
  with four terms"
neg-d2-571,2025-01-13,,2501.07779," The transition route from laminar to turbulent flow in a magnetohydrodynamic
(MHD) duct with a square cross-section is investigated in the limit of low
magnetic Reynolds number. In the presence of a transverse magnetic field,
Hartmann and Shercliff layers are present on the walls orthogonal and parallel
to the field direction, respectively. We assume reflection symmetries in both
transverse directions, and investigate the competition between transition
mechanisms specific to each boundary layer using direct numerical simulations.
Independently of which wall turbulence eventually occupies, transition relies
exclusively on a tripping of the Shercliff layer by perturbations, while the
Hartmann layer plays a passive role. This is explained, using a dynamical
systems interpretation, by the spatial localization of the edge states in the
Shercliff layer at the expense of the Hartmann layer. The link between these
non-linear coherent structures and the linear optimal modes known from
non-modal stability and energy stability theory is pointed out.",['physics.flu-dyn'],2503.13154," We consider a metapopulation made up of $K$ demes, each containing $N$
individuals bearing a heritable quantitative trait. Demes are connected by
migration and undergo independent Moran processes with mutation and selection
based on trait values. Mutation and migration rates are tuned so that each deme
receives a migrant or a mutant in the same slow timescale and is thus
essentially monomorphic at all times for the trait (adaptive dynamics). In the
timescale of mutation/migration, the metapopulation can then be seen as a giant
spatial Moran model with size $K$ that we characterize. As $K\to \infty$ and
physical space becomes continuous, the empirical distribution of the trait
(over the physical and trait spaces) evolves deterministically according to an
integro-differential evolution equation. In this limit, the trait of every
migrant is drawn from this global distribution, so that conditional on its
initial state, traits from finitely many demes evolve independently
(propagation of chaos). Under mean-field dispersal, the value $X_t$ of the
trait at time $t$ and at any given location has a law denoted $\mu_t$ and a
jump kernel with two terms: a mutation-fixation term and a migration-fixation
term involving $\mu_{t-}$ (McKean-Vlasov equation). In the limit where
mutations have small effects and migration is further slowed down accordingly,
we obtain the convergence of $X$, in the new migration timescale, to the
solution of a stochastic differential equation which can be referred to as a
new canonical equation of adaptive dynamics. This equation includes an
advection term representing selection, a diffusive term due to genetic drift,
and a jump term, representing the effect of migration, to a state distributed
according to its own law.","['math.PR', 'q-bio.PE']",False,,,,The route to turbulence in magnetohydrodynamic square duct flow,"Evolution of a trait distributed over a large fragmented population:
  Propagation of chaos meets adaptive dynamics"
neg-d2-572,2025-03-16,,2503.12502," The Capacitated Location Routing Problem is an important planning and routing
problem in logistics, which generalizes the capacitated vehicle routing problem
and the uncapacitated facility location problem. In this problem, we are given
a set of depots and a set of customers where each depot has an opening cost and
each customer has a demand. The goal is to open some depots and route
capacitated vehicles from the opened depots to satisfy all customers' demand,
while minimizing the total cost. In this paper, we propose a
$4.169$-approximation algorithm for this problem, improving the best-known
$4.38$-approximation ratio. Moreover, if the demand of each customer is allowed
to be delivered by multiple tours, we propose a more refined
$4.091$-approximation algorithm. Experimental study on benchmark instances
shows that the quality of our computed solutions is better than that of the
previous algorithm and is also much closer to optimality than the provable
approximation factor.",['cs.DS'],2502.03529," Interpreting galactic luminosity requires assumptions about the galaxy-wide
initial mass function (gwIMF), often assumed invariant in most stellar
population synthesis (SPS) models. If stars form in clusters with metallicity-
and density-dependent \textit{stellar IMFs}, the integrated galaxy-wide IMF
(IGIMF) can be calculated, with its shape depending on the star formation rate
(SFR) and metallicity. The shape of the IGIMF thus depends on the star
formation rate (SFR) and metallicity. We develop the \texttt{SPS-VarIMF} code
which enables us for the first time to compute the spectra, luminosities, and
remnant populations of galaxies in the context of the varying gwIMF with time,
SFR, and an assumed metallicity. Using the \texttt{SPS-VarIMF} code one can
calculate how the interpretation from the integrated galactic light may change
if the underlying galaxy-wide IMF is assumed to be environmentally dependent
instead of being invariant. In particular, we compare the time evolution of the
galaxy color and the stellar mass-to-light ratio in different bands for the
IGIMF and invariant canonical gwIMF assuming constant and delayed-$\tau$ star
formation histories. We show that the underlying gwIMF can be determined by
examining the colors and luminosities of late-type galaxies in UV and optical
bands. On the other hand, for early-type galaxies, it is difficult to
distinguish which gwIMF is valid since adopting the different gwIMFs yields
almost identical colors. However, their gwIMF-dependent $M/L$ ratios differ by
up to an order of magnitude. Massive present-day elliptical galaxies would have
been $10^4$ times as bright as at present when they were forming.",['astro-ph.GA'],False,,,,"Enhanced Approximation Algorithms for the Capacitated Location Routing
  Problem",Stellar population synthesis models with a physically varying IMF
neg-d2-573,2025-02-09,,2502.0613," While recent Large Vision-Language Models (LVLMs) have shown remarkable
performance in multi-modal tasks, they are prone to generating hallucinatory
text responses that do not align with the given visual input, which restricts
their practical applicability in real-world scenarios. In this work, inspired
by the observation that the text-to-image generation process is the inverse of
image-conditioned response generation in LVLMs, we explore the potential of
leveraging text-to-image generative models to assist in mitigating
hallucinations in LVLMs. We discover that generative models can offer valuable
self-feedback for mitigating hallucinations at both the response and token
levels. Building on this insight, we introduce self-correcting Decoding with
Generative Feedback (DeGF), a novel training-free algorithm that incorporates
feedback from text-to-image generative models into the decoding process to
effectively mitigate hallucinations in LVLMs. Specifically, DeGF generates an
image from the initial response produced by LVLMs, which acts as an auxiliary
visual reference and provides self-feedback to verify and correct the initial
response through complementary or contrastive decoding. Extensive experimental
results validate the effectiveness of our approach in mitigating diverse types
of hallucinations, consistently surpassing state-of-the-art methods across six
benchmarks. Code is available at https://github.com/zhangce01/DeGF.","['cs.CV', 'cs.CL']",2503.1204," Recently, Griffin, Ono, and Tsai examined the distribution of the number of
$t$-hooks in partitions of $n$, which was later followed by the work of Craig,
Ono, and Singh on the distribution of the number of $t$-hooks in self-conjugate
partitions of $n$. Motivated by these studies, in this paper, we further
investigate the number of $t$-hooks in some subsets of partitions. More
specifically, we obtain the generating functions for the number of $t$-hooks in
doubled distinct partitions and the number of $t$-shifted hooks in strict
partitions. Based on these generating functions, we prove that the number of
$t$-hooks in doubled distinct partitions and the number of $t$-shifted hooks in
strict partitions are both asymptotically normally distributed.","['math.CO', 'math.NT']",False,,,,"Self-Correcting Decoding with Generative Feedback for Mitigating
  Hallucinations in Large Vision-Language Models",On the distribution of $t$-hooks of doubled distinct partitions
neg-d2-574,2025-02-05,,2502.03531," We study (i) Janus deformations and (ii) non-local double trace deformations
of a pair of CFTs, as two different ways to construct CFT duals of traversable
AdS wormholes. First, we construct a simple model of traversable wormholes by
gluing two Poincar\'e AdS geometries and BTZ black holes and compute
holographic two point functions and (pseudo) entanglement entropy. We point out
that a Janus gravity solution describes a traversable wormhole when the
deformation parameter takes imaginary values. On the other hand, we show that
double trace deformations between two decoupled CFTs can reproduce two point
functions of traversable AdS wormholes. By considering the case where the
double trace deformation is given by a non-local $T\overline{T}$ deformation,
we analyze the dual gravity which implies emergence of wormholes. We present
toy model of these deformed CFTs by using free scalars and obtain qualitative
behaviors expected for them. We argue that the crucial difference between the
two constructions is that a global time slice of wormhole is described by a
pure state for Janus deformations, while it is a mixed state for the double
trace deformations.","['hep-th', 'gr-qc', 'quant-ph']",2502.11911," In recent studies, analogs of the electronic Quantum Spin-Hall Effect have
been explored within photonic crystals that incorporate spatial symmetries,
especially those with $ C_{6v} $ symmetry, where $ \mathbb{Z}_2 $ topological
invariants are enforced by crystalline symmetry. These photonic crystals
possess bulk states with well-defined pseudospins and exhibit helical edge
states, closely resembling their electronic counterparts. However, achieving
$\mathbb{Z}_2$ topological protection in a square lattice photonic crystal
remains great theoretical and experimental challange. In this work, we propose
a single material photonic crystal structure based on a $ C_4 $ lattice that
supports partially $ \mathbb{Z}_2 $-protected edge modes. We show that this
structure can host photonic band-gap that hosts $ \mathbb{Z}_2 $-like modes,
enabling perfect transmission in waveguide applications. Furthermore, we
investigate the robustness of these modes against structural defects and
directional turns, highlighting the distinctions between full $ \mathbb{Z}_2 $
topological protection and partial topological protection. Finally, we analyze
the impact of the number of elementary cells surrounding the interface on the
formation and stability of these protected modes.",['physics.optics'],False,,,,Traversable AdS Wormhole via Non-local Double Trace or Janus Deformation,Partial Topological Protection in C4 Lattices for Optical Communications
neg-d2-575,2025-01-04,,2501.0231," Operator-splitting methods are widely used to solve differential equations,
especially those that arise from multi-scale or multi-physics models, because a
monolithic (single-method) approach may be inefficient or even infeasible. The
most common operator-splitting methods are the first-order Lie--Trotter (or
Godunov) and the second-order Strang (Strang--Marchuk) splitting methods.
High-order splitting methods with real coefficients require backward-in-time
integration in each operator and hence may be adversely impacted by instability
for certain operators such as diffusion. However, besides the method
coefficients, there are many other ancillary aspects to an overall
operator-splitting method that are important but often overlooked. For example,
the operator ordering and the choice of sub-integration methods can
significantly affect the stability and efficiency of an operator-splitting
method. In this paper, we investigate some design principles for the
construction of operator-splitting methods, including minimization of local
error measure, choice of sub-integration method, maximization of linear
stability, and minimization of overall computational cost. We propose a new
four-stage, third-order, 2-split operator-splitting method with seven
sub-integrations per step and optimized linear stability for a benchmark
problem from cardiac electrophysiology. We then propose a general principle to
further improve stability and efficiency of such operator-splitting methods by
using low-order, explicit sub-integrators for unstable sub-integrations. We
demonstrate an almost 30\% improvement in the performance of methods derived
from these design principles compared to the best-known third-order methods.","['math.NA', 'cs.NA']",2502.13026," We revisit Hawking's original derivation of the evaporation process in a
non-stationary spacetime, presenting it in a clear and pedagogical manner, with
a focus on the spherical collapse of a star into a black hole. Our analysis
highlights the underlying assumptions in the calculations, clarifying their
physical significance, potential implications, and the limitations of this
approach.",['gr-qc'],False,,,,"Improving the stability and efficiency of high-order operator-splitting
  methods",Deriving the paradox: original derivation of Hawking radiation
neg-d2-576,2025-02-17,,2502.11528," Large Language Models (LLMs) excel in handling general knowledge tasks, yet
they struggle with user-specific personalization, such as understanding
individual emotions, writing styles, and preferences. Personalized Large
Language Models (PLLMs) tackle these challenges by leveraging individual user
data, such as user profiles, historical dialogues, content, and interactions,
to deliver responses that are contextually relevant and tailored to each user's
specific needs. This is a highly valuable research topic, as PLLMs can
significantly enhance user satisfaction and have broad applications in
conversational agents, recommendation systems, emotion recognition, medical
assistants, and more. This survey reviews recent advancements in PLLMs from
three technical perspectives: prompting for personalized context (input level),
finetuning for personalized adapters (model level), and alignment for
personalized preferences (objective level). To provide deeper insights, we also
discuss current limitations and outline several promising directions for future
research. Updated information about this survey can be found at the
https://github.com/JiahongLiu21/Awesome-Personalized-Large-Language-Models.",['cs.AI'],2502.17727," The remarkable success of deep learning in recent years has prompted
applications in medical image classification and diagnosis tasks. While
classification models have demonstrated robustness in classifying simpler
datasets like MNIST or natural images such as ImageNet, this resilience is not
consistently observed in complex medical image datasets where data is more
scarce and lacks diversity. Moreover, previous findings on natural image
datasets have indicated a potential trade-off between data likelihood and
classification accuracy. In this study, we explore the use of score-based
generative models as classifiers for medical images, specifically mammographic
images. Our findings suggest that our proposed generative classifier model not
only achieves superior classification results on CBIS-DDSM, INbreast and Vin-Dr
Mammo datasets, but also introduces a novel approach to image classification in
a broader context. Our code is publicly available at
https://github.com/sushmitasarker/sgc_for_medical_image_classification",['cs.CV'],False,,,,"A Survey of Personalized Large Language Models: Progress and Future
  Directions","Can Score-Based Generative Modeling Effectively Handle Medical Image
  Classification?"
neg-d2-577,2025-02-01,,2502.00563," Recent advancements in deep neural networks have significantly enhanced the
performance of semantic segmentation. However, class imbalance and instance
imbalance remain persistent challenges, where smaller instances and thin
boundaries are often overshadowed by larger structures. To address the
multiscale nature of segmented objects, various models have incorporated
mechanisms such as spatial attention and feature pyramid networks. Despite
these advancements, most loss functions are still primarily pixel-wise, while
regional and boundary-focused loss functions often incur high computational
costs or are restricted to small-scale regions. To address this limitation, we
propose complex wavelet mutual information (CWMI) loss, a novel loss function
that leverages mutual information from subband images decomposed by a complex
steerable pyramid. The complex steerable pyramid captures features across
multiple orientations and preserves structural similarity across scales.
Meanwhile, mutual information is well-suited for capturing high-dimensional
directional features and exhibits greater noise robustness. Extensive
experiments on diverse segmentation datasets demonstrate that CWMI loss
achieves significant improvements in both pixel-wise accuracy and topological
metrics compared to state-of-the-art methods, while introducing minimal
computational overhead. The code is available at
https://anonymous.4open.science/r/CWMI-83B7/","['cs.CV', 'eess.IV']",2502.11159," Recently, the BESIII Collaboration has observed three-body decays $D_s^+\to
\eta \omega\pi^+$, $D^+\to K^0_S\pi^+\omega$ and $D^0\to K^-\pi^+\omega$. In
this work, we investigate the contributions of the subprocesses $\rho^+\to
\omega\pi^+$ in these Cabibbo-favored decays $D \to h\omega\pi$, with $\rho^+=
\{\rho(770)^+, \rho(1450)^+, \rho(770)^+\&\rho(1450)^+\}$ and $h=\{ \eta,
K^0_S, K^-\}$, by introducing these subprocesses into the decay amplitudes of
the relevant decay processes via the vector form factor $F_{\omega\pi}$
measured in the related $\tau$ and $e^+e^-$ processes; we provide the first
theoretical predictions for the branching fractions of the quasi-two-body
decays $D_s^+\to\eta[\rho^+\to]\omega\pi^+$, $D^+\to
K^0_S[\rho^+\to]\omega\pi^+$ and $D^0\to K^-[\rho^+\to]\omega\pi^+$. Our
findings reveal that the contributions from the subprocess
$\rho(770)^+\to\omega\pi^+$ are significant in these observed three-body decays
$D_s^+\to\eta \omega\pi^+$, $D^+\to K^0_S \omega\pi^+$ and $D^0\to K^-
\omega\pi^+$, notwithstanding the contributions originating from the
Breit-Wigner tail effect of $\rho(770)^+$. The numerical results of this study
suggest that these Cabibbo-favored three-body decays are dominated by the
contributions come from the $P$-wave intermediate states $\rho(770)^+$,
$\rho(1450)^+$ and their interference effects.",['hep-ph'],False,,,,"Complex Wavelet Mutual Information Loss: A Multi-Scale Loss Function for
  Semantic Segmentation","Contributions of $\rho(770,1450)\to \omega\pi$ for the Cabibbo-favored
  $D \to h\omega\pi$ decays"
neg-d2-578,2025-02-03,,2502.01731," During the accretion phase of a core-collapse supernova (SN), dark-photon
(DP) cooling can be largest in the gain layer below the stalled shock wave. In
this way, it could counter-act the usual shock rejuvenation by neutrino energy
deposition and thus prevent the explosion. This peculiar energy-loss profile
derives from the resonant nature of DP production. The largest cooling and thus
strongest constraints obtain for DP masses of 0.1-0.4 MeV, a range
corresponding to the photon plasma mass in the gain region. Electron-capture
SNe, once observationally unambiguously identified, could provide strong bounds
even down to nearly 0.01 MeV. For a coupling strength so small that
neutrino-driven explosions are expected to survive, the DP cooling of the core
is too small to modify the neutrino signal, i.e., our new argument supersedes
the traditional SN1987A cooling bound.","['hep-ph', 'astro-ph.HE']",2503.0139," Crash consistency is essential for applications that must persist data.
Crash-consistency testing has been commonly applied to find crash-consistency
bugs in applications. The crash-state space grows exponentially as the number
of operations in the program increases, necessitating techniques for pruning
the search space. However, state-of-the-art crash-state space pruning is far
from ideal. Some techniques look for known buggy patterns or bound the
exploration for efficiency, but they sacrifice coverage and may miss bugs
lodged deep within applications. Other techniques eliminate redundancy in the
search space by skipping identical crash states, but they still fail to scale
to larger applications.
  In this work, we propose representative testing: a new crash-state space
reduction strategy that achieves high scalability and high coverage. Our key
observation is that the consistency of crash states is often correlated, even
if those crash states are not identical. We build Pathfinder, a
crash-consistency testing tool that implements an update behaviors-based
heuristic to approximate a small set of representative crash states.
  We evaluate Pathfinder on POSIX-based and MMIO-based applications, where it
finds 18 (7 new) bugs across 8 production-ready systems. Pathfinder scales more
effectively to large applications than prior works and finds 4x more bugs in
POSIX-based applications and 8x more bugs in MMIO-based applications compared
to state-of-the-art systems.","['cs.OS', 'cs.PL', 'cs.SE']",False,,,,Dark Photons can Prevent Core-Collapse Supernova Explosions,"Scalable and Accurate Application-Level Crash-Consistency Testing via
  Representative Testing"
neg-d2-579,2025-01-15,,2501.08827," The key molecules such as triphosphate (ATP), glutathione (GSH), and
homocarnosine (hCs) - central to metabolic processes in the human brain remain
elusive or challenging to detect with upfield 1H-MRSI. Traditional 3D 1H-MRSI
in vivo faces challenges, including a low signal-to-noise ratio and
magnetization transfer effects with water, leading to prolonged measurement
times and reduced resolution. To address these limitations, we propose a
downfield 3D-MRSI method aimed at measuring downfield metabolites with enhanced
spatial resolution, and speed acceptable for clinical practice at 7T. The
CHEmical-shift selective Adiabatic Pulse (CHEAP) technique was integrated into
echo-planar spectroscopic imaging (EPSI) readout sequence for downfield
metabolite and water reference 3D-MRSI. Five healthy subjects and two glioma
patients were scanned to test the feasibility. In this work, CHEAP-EPSI
technique is shown to significantly enhance spatial the resolution to 0.37 ml
while simultaneously reducing the scan time to 10.5 minutes. Its distinct
advantages include low specific absorption rate, effective suppression of water
and lipid signals, and minimal baseline distortions, making it a valuable tool
for research or potentially diagnostic purposes. CHEAP-EPSI improves the
detection sensitivity of downfield metabolites like N-acetyl-aspartate (NAA+)
and DF8.18 (ATP&GSH+), and offers new possibilities for the study of metabolism
in healthy and diseased brain.",['physics.med-ph'],2501.0346," We extend the result of Blumberg and Mandell on K-theoretic Tate-Poitou
duality at odd primes which serves as a spectral refinement of the classical
arithmetic Tate-Poitou duality. The duality is formulated for the
$K(1)$-localized algebraic K-theory of the ring of $p$-integers in a number
field and its completion using the $\bZ_p$-Anderson duality. This paper
completes the picture by addressing the prime 2, where the real embeddings of
number fields introduce extra complexities. As an application, we identify the
homotopy type at prime 2 of the homotopy fiber of the cyclotomic trace for the
sphere spectrum in terms of the algebraic K-theory of the integers.","['math.KT', 'math.AT']",False,,,,"CHEmical-shift selective Adiabatic Pulse (CHEAP): Fast and High
  Resolution Downfield 3D 1H-MRSI at 7T",K-theoretic Tate-Poitou duality at prime 2
neg-d2-580,2025-02-18,,2502.12602," This paper presents a novel learning-based approach to dynamic robot-to-human
handover, addressing the challenges of delivering objects to a moving receiver.
We hypothesize that dynamic handover, where the robot adjusts to the receiver's
movements, results in more efficient and comfortable interaction compared to
static handover, where the receiver is assumed to be stationary. To validate
this, we developed a nonparametric method for generating continuous handover
motion, conditioned on the receiver's movements, and trained the model using a
dataset of 1,000 human-to-human handover demonstrations. We integrated
preference learning for improved handover effectiveness and applied impedance
control to ensure user safety and adaptiveness. The approach was evaluated in
both simulation and real-world settings, with user studies demonstrating that
dynamic handover significantly reduces handover time and improves user comfort
compared to static methods. Videos and demonstrations of our approach are
available at https://zerotohero7886.github.io/dyn-r2h-handover .",['cs.RO'],2503.17779," Over-extended Kac-Moody algebras contain so-called gradient structures - a
gl(d)-covariant level decomposition of the algebra contains strings of modules
at different levels that can be interpreted as spatial gradients. We present an
algebraic origin for this phenomenon, based on the recently introduced Lie
algebra extension of an over-extended Kac-Moody algebra by its fundamental
module, appearing in tensor hierarchy algebra super-extensions of over-extended
Kac-Moody algebras. The extensions are described in terms of Lie algebra
cohomology, vanishing for finite-dimensional simple Lie algebras, but
non-vanishing in relevant infinite-dimensional cases. The extension is
described in a few different gradings, where it is given a covariant
description with respect to different subalgebras. We expect the results to be
important for the connection between extended geometry and cosmological
billiards.","['hep-th', 'math.RT']",False,,,,Learning-based Dynamic Robot-to-Human Handover,Gradient structures from extensions of over-extended Kac-Moody algebras
neg-d2-581,2025-03-13,,2503.10512," We consider the problem of generating valid and small prediction sets by
sampling outputs (e.g., software code and natural language text) from a
black-box deep generative model for a given input (e.g., textual prompt). The
validity of a prediction set is determined by a user-defined binary
admissibility function depending on the target application. For example,
requiring at least one program in the set to pass all test cases in code
generation application. To address this problem, we develop a simple and
effective conformal inference algorithm referred to as Generative Prediction
Sets (GPS). Given a set of calibration examples and black-box access to a deep
generative model, GPS can generate prediction sets with provable guarantees.
The key insight behind GPS is to exploit the inherent structure within the
distribution over the minimum number of samples needed to obtain an admissible
output to develop a simple conformal regression approach over the minimum
number of samples. Experiments on multiple datasets for code and math word
problems using different large language models demonstrate the efficacy of GPS
over state-of-the-art methods.","['cs.LG', 'cs.AI']",2501.09954," Design space exploration (DSE) plays a crucial role in enabling custom
hardware architectures, particularly for emerging applications like AI, where
optimized and specialized designs are essential. With the growing complexity of
deep neural networks (DNNs) and the introduction of advanced foundational
models (FMs), the design space for DNN accelerators is expanding at an
exponential rate. Additionally, this space is highly non-uniform and
non-convex, making it increasingly difficult to navigate and optimize.
Traditional DSE techniques rely on search-based methods, which involve
iterative sampling of the design space to find the optimal solution. However,
this process is both time-consuming and often fails to converge to the global
optima for such design spaces. Recently, AIrchitect v1, the first attempt to
address the limitations of search-based techniques, transformed DSE into a
constant-time classification problem using recommendation networks. In this
work, we propose AIrchitect v2, a more accurate and generalizable
learning-based DSE technique applicable to large-scale design spaces that
overcomes the shortcomings of earlier approaches. Specifically, we devise an
encoder-decoder transformer model that (a) encodes the complex design space
into a uniform intermediate representation using contrastive learning and (b)
leverages a novel unified representation blending the advantages of
classification and regression to effectively explore the large DSE space
without sacrificing accuracy. Experimental results evaluated on 10^5 real DNN
workloads demonstrate that, on average, AIrchitect v2 outperforms existing
techniques by 15% in identifying optimal design points. Furthermore, to
demonstrate the generalizability of our method, we evaluate performance on
unseen model workloads (LLMs) and attain a 1.7x improvement in inference
latency on the identified hardware architecture.","['cs.LG', 'cs.AI', 'cs.AR']",False,,,,"Conformal Prediction Sets for Deep Generative Models via Reduction to
  Conformal Regression","AIRCHITECT v2: Learning the Hardware Accelerator Design Space through
  Unified Representations"
neg-d2-582,2025-01-03,,2501.01756," Stueckelberg introduced an axion like scalar field to provide mass to the
gauge electromagnetic field without breaking gauge invariance. This can be
considered as a precursor to the spontaneously broken abelian Higgs model. We
will consider its role in cosmology to provide a novel candidate to the dark
matter question. In addition its implications to deeper issues will be pointed
out.","['hep-ph', 'gr-qc', 'hep-th']",2501.18627," We present a fast and simple technique to convert images into an emissive
surface-based scene representation. Building on existing emissive volume
reconstruction algorithms, we introduce a subtle yet impactful modification of
the loss function requiring changes to only a few lines of code: instead of
integrating the radiance field along rays and supervising the resulting images,
we project the training images into the scene to directly supervise the
spatio-directional radiance field.
  The primary outcome of this change is the complete removal of alpha blending
and ray marching from the image formation model, instead moving these steps
into the loss computation. In addition to promoting convergence to surfaces,
this formulation assigns explicit semantic meaning to 2D subsets of the
radiance field, turning them into well-defined emissive surfaces. We finally
extract a level set from this representation, which results in a high-quality
emissive surface model.
  Our method retains much of the speed and quality of the baseline algorithm.
For instance, a suitably modified variant of Instant~NGP maintains comparable
computational efficiency, while achieving an average PSNR that is only 0.1 dB
lower. Most importantly, our method generates explicit surfaces in place of an
exponential volume, doing so with a level of simplicity not seen in prior work.","['cs.GR', 'cs.CV']",False,,,,Stueckelberg field and Cosmology,"A Radiance Field Loss for Fast and Simple Emissive Surface
  Reconstruction"
neg-d2-583,2025-01-15,,2501.08747," We prove that, to every abstract group $G$, we can associate a sequence of
graphs $\Gamma_n$ such that the automorphism group of $\Gamma_n$ is isomorphic
to $G$ and the genus of $\Gamma_n$ is an unbounded function of $n$.","['math.GR', 'math.CO']",2503.12698," Precision medicine in the quantitative management of chronic diseases and
oncology would be greatly improved if the Computed Tomography (CT) scan of any
patient could be segmented, parsed and analyzed in a precise and detailed way.
However, there is no such fully annotated CT dataset with all anatomies
delineated for training because of the exceptionally high manual cost, the need
for specialized clinical expertise, and the time required to finish the task.
To this end, we proposed a novel continual learning-driven CT model that can
segment complete anatomies presented using dozens of previously partially
labeled datasets, dynamically expanding its capacity to segment new ones
without compromising previously learned organ knowledge. Existing multi-dataset
approaches are not able to dynamically segment new anatomies without
catastrophic forgetting and would encounter optimization difficulty or
infeasibility when segmenting hundreds of anatomies across the whole range of
body regions. Our single unified CT segmentation model, CL-Net, can highly
accurately segment a clinically comprehensive set of 235 fine-grained
whole-body anatomies. Composed of a universal encoder, multiple optimized and
pruned decoders, CL-Net is developed using 13,952 CT scans from 20 public and
16 private high-quality partially labeled CT datasets of various vendors,
different contrast phases, and pathologies. Extensive evaluation demonstrates
that CL-Net consistently outperforms the upper limit of an ensemble of 36
specialist nnUNets trained per dataset with the complexity of 5% model size and
significantly surpasses the segmentation accuracy of recent leading Segment
Anything-style medical image foundation models by large margins. Our continual
learning-driven CL-Net model would lay a solid foundation to facilitate many
downstream tasks of oncology and chronic diseases using the most widely adopted
CT imaging.","['eess.IV', 'cs.CV']",False,,,,"Every group is the automorphism group of a graph with arbitrarily large
  genus","A Continual Learning-driven Model for Accurate and Generalizable
  Segmentation of Clinically Comprehensive and Fine-grained Whole-body
  Anatomies in CT"
neg-d2-584,2025-02-13,,2502.09028," Let $\Omega \subset \mathbb{R}$ be a nonempty and open set, then for all $f,
g, h\in \mathscr{C}^{2}(\Omega)$ we have \begin{multline*}
\frac{\mathrm{d}^{2}}{\mathrm{d} x^{2}}(f\cdot g\cdot h) -
f\frac{\mathrm{d}^{2}}{\mathrm{d} x^{2}}(g\cdot
h)-g\frac{\mathrm{d}^{2}}{\mathrm{d} x^{2}}(f\cdot
h)-h\frac{\mathrm{d}^{2}}{\mathrm{d} x^{2}}(f\cdot g) + f\cdot
g\frac{\mathrm{d}^{2}}{\mathrm{d} x^{2}} h+f\cdot
h\frac{\mathrm{d}^{2}}{\mathrm{d} x^{2}} g+g\cdot h
\frac{\mathrm{d}^{2}}{\mathrm{d} x^{2}}f=0 \end{multline*} The aim of this
paper is to consider the corresponding operator equation \[D(f\cdot g \cdot h)
- fD(g\cdot h) - gD(f\cdot h) - hD(f \cdot g) + f\cdot g D(h) + f\cdot h D(g)
+g\cdot h D(f) = 0\] for operators $D\colon \mathscr{C}^{k}(\Omega)\to
\mathscr{C}(\Omega)$, where $k$ is a given nonnegative integer and the above
identity is supposed to hold for all $f, g, h \in \mathscr{C}^{k}(\Omega)$. We
show that besides the operators of first and second derivative, there are more
solutions to this equation. Some special cases characterizing differential
operators are also studied.",['math.CA'],2503.06957," Volterra integral and integro-differential equations have been extensively
studied in both pure mathematics and applied science. In one direction,
developments in analysis have yielded far-ranging existence, uniqueness, and
regularity results. In the other, applications in science have inspired a
substantial library of practical techniques to deal with such equations.
  The present work connects these research areas by examining five large
classes of linear Volterra equations: integral and integro-differential
equations with completely monotone (CM) kernels, corresponding to linear
viscoelastic models; those with positive definite (PD) kernels, corresponding
to partially-observed quantum systems; difference equations with PD kernels; a
class of generalized delay differential equations; and a class of generalized
fractional differential equations. We develop a system of correspondences
between these problems, showing that all five can be understood within the
same, spectral theory. We leverage this theory to recover practical,
closed-form solutions of all five classes, and we show that interconversion
yields a natural, continuous involution within each class. Our work unifies
several results from science: the interconversion formula of Gross, recent
results in viscoelasticity and operator theory for integral equations of the
second type, classical formulas for Prony series and fractional differential
equations, and the convergence of Prony series to CM kernels. Finally, our
theory yields a novel, geometric construction of the regularized Hilbert
transform, extends it to a wide class of infinite measures, and reveals a
natural connection to delay and fractional differential equations.
  We leverage our theory to develop a powerful, spectral method to handle
scalar Volterra equations numerically, and illustrate it with a number of
practical examples.","['math.CA', 'math.DS', 'math.SP', 'physics.comp-ph']",False,,,,"Second-order derivations of functions spaces -- a characterization of
  second-order differential operators",A Spectral Theory of Scalar Volterra Equations
neg-d2-585,2025-01-25,,2501.15223," We propose an efficient and interpretable neural network with a novel
activation function called the weighted Lehmer transform. This new activation
function enables adaptive feature selection and extends to the complex domain,
capturing phase-sensitive and hierarchical relationships within data. Notably,
it provides greater interpretability and transparency compared to existing
machine learning models, facilitating a deeper understanding of its
functionality and decision-making processes. We analyze the mathematical
properties of both real-valued and complex-valued Lehmer activation units and
demonstrate their applications in modeling nonlinear interactions. Empirical
evaluations demonstrate that our proposed neural network achieves competitive
accuracy on benchmark datasets with significantly improved computational
efficiency. A single layer of real-valued or complex-valued Lehmer activation
units is shown to deliver state-of-the-art performance, balancing efficiency
with interpretability.","['cs.LG', 'cs.AI']",2503.17066," We consider in this work a $2$-dimensional $3$-wave kinetic equation
describing the dynamics of the thermal cloud outside a Bose-Einstein
Condensate. We construct global non-radial mild solutions for the equation.
Those mild solutions are the summation of Dirac masses on circles. We prove
that in each spatial direction, either Dirac masses at the origin, which are
the so-called Bose-Einstein condensates, can be formed in finite time or the
solutions converge to Bose-Einstein condensates as time evolves to infinity. We
also describe a dynamics of the formation of the Bose-Einstein condensates
latter case. In this case, on each direction, the solutions accumulate around
circles close to the origin at growth rates at least linearly in time.",['math.AP'],False,,,,"Efficient and Interpretable Neural Networks Using Complex Lehmer
  Transform","Formation of condensations for non-radial solutions to 3-wave kinetic
  equations"
neg-d2-586,2025-01-07,,2501.03644," Let $p$ be a prime number and $K$ a finite unramified extension of
$\mathbb{Q}_p$. If $p$ is large enough with respect to $[K:\mathbb{Q}_p]$ and
under mild genericity assumptions, we prove that the admissible smooth
representations of $\mathrm{GL}_2(K)$ that occur in Hecke eigenspaces of the
mod $p$ cohomology are of finite length. We also prove many new structural
results about these representations of $\mathrm{GL}_2(K)$ and their
subquotients.","['math.NT', 'math.RT']",2502.17875," In recent years, the fifth-generation (5G) new radio (NR) signals have
emerged as a promising supplementary resource for urban navigation. However, a
major challenge in utilizing 5G signals lies in their vulnerability to
non-line-of-sight (NLoS) propagation effects, which are especially prevalent in
urban street canyons. This paper applies the direct position estimation (DPE)
method to 5G cellular signals to mitigate the NLoS bias as well as the
multipath effects, thereby enabling precise localization in urbanized
environments. The feasibility of applying the DPE method to NR positioning is
analyzed, followed by a discussion of the tapped delay line (TDL) channel
propagation model provided by the 3rd Generation Partnership Project (3GPP).
The positioning performance is then evaluated through large-scale system-level
simulations. The simulation results demonstrate that 5G DPE achieves
satisfactory positioning accuracy in a 10 dB noisy channel, with an overall
root mean square error (RMSE) constrained within 6 m. In addition, 5G DPE
outperforms the observed time difference of arrival (OTDoA) method by 95.24% in
terms of positioning accuracy in an NLoS-dominated propagation environment.",['eess.SP'],False,,,,Finite length for unramified $\mathrm{GL}_2$,"5G Direct Position Estimation for Precise Localization in Dense Urban
  Area"
neg-d2-587,2025-02-26,,2502.18904," Commit messages concisely describe code changes in natural language and are
important for software maintenance. Several approaches have been proposed to
automatically generate commit messages, but they still suffer from critical
limitations, such as time-consuming training and poor generalization ability.
To tackle these limitations, we propose to borrow the weapon of large language
models (LLMs) and in-context learning (ICL). Our intuition is based on the fact
that the training corpora of LLMs contain extensive code changes and their
pairwise commit messages, which makes LLMs capture the knowledge about commits,
while ICL can exploit the knowledge hidden in the LLMs and enable them to
perform downstream tasks without model tuning. However, it remains unclear how
well LLMs perform on commit message generation via ICL. In this paper, we
conduct an empirical study to investigate the capability of LLMs to generate
commit messages via ICL. Specifically, we first explore the impact of different
settings on the performance of ICL-based commit message generation. We then
compare ICL-based commit message generation with state-of-the-art approaches on
a popular multilingual dataset and a new dataset we created to mitigate
potential data leakage. The results show that ICL-based commit message
generation significantly outperforms state-of-the-art approaches on subjective
evaluation and achieves better generalization ability. We further analyze the
root causes for LLM's underperformance and propose several implications, which
shed light on future research directions for using LLMs to generate commit
messages.",['cs.SE'],2501.11126," Multi-antenna coded caching (CC) with multicast beamforming typically relies
on a complex successive interference cancellation (SIC) structure to decode a
superposition of multiple streams received by each user. Signal-level CC
schemes require the regeneration and cancellation of interfering signals at the
physical layer of each receiver, which complicates practical implementations.
To address this, we propose a bit-level multicast scheduling scheme enabling
linear, SIC-free decoding of parallel streams by repeatedly transmitting data
terms with linearly independent coefficients. Two reference strategies and a
novel sparse strategy are considered for constructing the coefficient matrix.
The reference cases include the random strategy, which lacks control over
matrix construction, and the equal-distant strategy, which balances users'
interference and data terms equally. In contrast, the sparse strategy minimizes
the number of multicast streams transmitted in parallel during each interval.
This approach simplifies both the decoding process and the beamforming design
by decoupling the desired data terms for each user and reducing the number of
SINR constraints, respectively. To further enhance the symmetric rate, a
successive projection algorithm is applied to exploit channel properties and
optimize user ordering. With the coefficient matrix and optimized user ordering
in place, multicast beamformers are devised to aggregate desired data from
relevant multicast streams. Numerical simulations validate the effectiveness of
the sparse strategy and user scheduling, demonstrating significant gains in
symmetric rate.","['cs.IT', 'math.IT']",False,,,,"An Empirical Study on Commit Message Generation using LLMs via
  In-Context Learning",SIC-free Multicast Scheduling for Multi-antenna Coded Caching
neg-d2-588,2025-01-28,,2501.16805," Spoofed traffic has been identified as one of the main issues of concern for
network hygiene nowadays, as it facilitates Distributed Denial-of-Service
(DDoS) attacks by hiding their origin and complicating forensic investigations.
Some indicators of poor network hygiene are packets with Bogon or Martian
source addresses representing either misconfigurations or spoofed packets.
Despite the development of Source Address Validation (SAV) techniques and
guidelines such as BCP 38 and BCP 84, Bogons are often overlooked in the
filtering practices of network operators. This study uses traceroute
measurements from the CAIDA Ark dataset, enriched with historical BGP routing
information from RIPE RIS and RouteViews, to investigate the prevalence of
Bogon addresses over seven years (2017-2023). Our analysis reveals widespread
non-compliance with best practices, with Bogon traffic detected across
thousands of ASes. Notably, 82.69%-97.83% of CAIDA Ark vantage points observe
paths containing Bogon IPs, primarily RFC1918 addresses. Additionally, 19.70%
of all analyzed traceroutes include RFC1918 addresses, while smaller
proportions involve RFC6598 (1.50%) and RFC3927 (0.10%) addresses. We identify
more than 13,000 unique ASes transiting Bogon traffic, with only 11.64%
appearing in more than half of the measurements. Cross-referencing with the
Spoofer project and MANRS initiatives shows a concerning gap: 62.67% of ASes
that do not filter packets with Bogon sources are marked as non-spoofable,
suggesting incomplete SAV implementation. Our contributions include an
assessment of network hygiene using the transiting of Bogon packets as a
metric, an analysis of the main types of Bogon addresses found in traceroutes,
and several proposed recommendations to address the observed gaps, enforcing
the need for stronger compliance with best practices to improve global network
security.",['cs.NI'],2502.08388," In this paper, we study the shadow and observational image of the Kerr-like
Loop Quantum Gravity (LQG) inspired black bounce with the help of the celestial
light source and the thin disk source by employing the backward ray-tracing
method. The results indicate that both the LQG parameter alpha and the rotation
parameter a contribute to a reduction in the shadow size; however, the
influence of a is predominant, while the effect of alpha circular orbit. One
can find that the correlation parameter (a, alpha), along with the observer's
inclination angle, affect the image's asymmetry and the distortion of the inner
shadow. As the inclination increases, the direct and lensed images diverge,
creating a structure resembling a hat. Meanwhile, we also investigate the
redshift distribution of the direct lensed images of the accretion disk under
different parameters and observation angle. The results show that the
distribution of redshift and observed intensity is obviously related to the
behavior of accretion flow. These results may provide a potential approach to
limit black hole parameters, detect quantum gravity effects, and distinguish
the LQG black hole from other black hole models.",['gr-qc'],False,,,,"Martians Among Us: Observing Private or Reserved IPs on the Public
  Internet","The shadow and accretion disk images of the rotation loop quantum black
  bounce"
neg-d2-589,2025-03-16,,2503.12453," Previous works studied how deep neural networks (DNNs) perceive image content
in terms of their biases towards different image cues, such as texture and
shape. Previous methods to measure shape and texture biases are typically
style-transfer-based and limited to DNNs for image classification. In this
work, we provide a new evaluation procedure consisting of 1) a
cue-decomposition method that comprises two AI-free data pre-processing methods
extracting shape and texture cues, respectively, and 2) a novel
cue-decomposition shape bias evaluation metric that leverages the
cue-decomposition data. For application purposes we introduce a corresponding
cue-decomposition robustness metric that allows for the estimation of the
robustness of a DNN w.r.t. image corruptions. In our numerical experiments, our
findings for biases in image classification DNNs align with those of previous
evaluation metrics. However, our cue-decomposition robustness metric shows
superior results in terms of estimating the robustness of DNNs. Furthermore,
our results for DNNs on the semantic segmentation datasets Cityscapes and
ADE20k for the first time shed light into the biases of semantic segmentation
DNNs.",['cs.CV'],2502.18601," The rapid advancements in data-driven methodologies have underscored the
critical importance of ensuring data quality. Consequently, detecting
out-of-distribution (OOD) data has emerged as an essential task to maintain the
reliability and robustness of data-driven models, in general, and machine and
deep learning models, in particular. In this study, we leveraged the convex
hull property of a dataset and the fact that anomalies highly contribute to the
increase of the CH's volume to propose a novel anomaly detection algorithm. Our
algorithm computes the CH's volume as an increasing number of data points are
removed from the dataset to define a decision line between OOD and
in-distribution data points. We compared the proposed algorithm to seven widely
used anomaly detection algorithms over ten datasets, showing comparable results
for state-of-the-art (SOTA) algorithms. Moreover, we show that with a
computationally cheap and simple check, one can detect datasets that are
well-suited for the proposed algorithm which outperforms the SOTA anomaly
detection algorithms.",['cs.LG'],False,,,,"Shape Bias and Robustness Evaluation via Cue Decomposition for Image
  Classification and Segmentation",Tighten The Lasso: A Convex Hull Volume-based Anomaly Detection Method
neg-d2-590,2025-03-10,,2503.06957," Volterra integral and integro-differential equations have been extensively
studied in both pure mathematics and applied science. In one direction,
developments in analysis have yielded far-ranging existence, uniqueness, and
regularity results. In the other, applications in science have inspired a
substantial library of practical techniques to deal with such equations.
  The present work connects these research areas by examining five large
classes of linear Volterra equations: integral and integro-differential
equations with completely monotone (CM) kernels, corresponding to linear
viscoelastic models; those with positive definite (PD) kernels, corresponding
to partially-observed quantum systems; difference equations with PD kernels; a
class of generalized delay differential equations; and a class of generalized
fractional differential equations. We develop a system of correspondences
between these problems, showing that all five can be understood within the
same, spectral theory. We leverage this theory to recover practical,
closed-form solutions of all five classes, and we show that interconversion
yields a natural, continuous involution within each class. Our work unifies
several results from science: the interconversion formula of Gross, recent
results in viscoelasticity and operator theory for integral equations of the
second type, classical formulas for Prony series and fractional differential
equations, and the convergence of Prony series to CM kernels. Finally, our
theory yields a novel, geometric construction of the regularized Hilbert
transform, extends it to a wide class of infinite measures, and reveals a
natural connection to delay and fractional differential equations.
  We leverage our theory to develop a powerful, spectral method to handle
scalar Volterra equations numerically, and illustrate it with a number of
practical examples.","['math.CA', 'math.DS', 'math.SP', 'physics.comp-ph']",2501.09559," One crucial and basic method for disclosing a secret to every participant in
quantum cryptography is quantum secret sharing. Numerous intricate protocols,
including secure multiparty summation, multiplication, sorting, voting, and
more, can be designed with it. A quantum secret sharing protocol with a $(t,n)$
threshold approach and modulo d, where t and n represent the threshold number
of participants and the total number of participants, respectively was recently
discussed by Song et al. Kao et al. notes that without the information of other
participants, the secret in Song {\em et al.'s}protocol cannot be
reconstructed. We address a protocol that solves this issue in this paper.","['quant-ph', 'cs.CR']",False,,,,A Spectral Theory of Scalar Volterra Equations,Threshold Quantum Secret Sharing
neg-d2-591,2025-01-08,,2501.04968," $r$-mode oscillations in rotating neutron stars are a source of continuous
gravitational radiation. We investigate the excitation of $r$-modes by the
mechanical impact on the neutron star surface of stochastically accreted clumps
of matter, assuming that the Chandrasekhar-Friedman-Schutz instability is not
triggered. The star is idealised as a slowly-rotating, unmagnetised,
one-component fluid with a barotropic equation of state in Newtonian gravity.
It is found that the $r$-mode amplitude depends weakly on the equation of state
but sensitively on the rotation frequency $\nu_{\rm s}$. The gravitational wave
strain implicitly depends on the equation of state through the damping
timescale. The root-mean-square strain is $h_{\rm rms} \approx 10^{-35}
(\nu_{\rm s}/ 10 {\rm Hz})^{2} (R_*/10 {\rm km})^2 (\Delta t_{\rm acc}/1 {\rm
yr})^{1/2} (f_{\rm acc}/1 {\rm kHz})^{-1/2} (\dot{M}/10^{-8} \text{M}_{\odot}
\text{yr}^{-1}) (v/0.4c) (d/1 {\rm kpc})^{-1}$, which is comparable to the
strain from $g$-, $p$- and $f$-modes excited by stochastic accretion, where
$R_*$ is the radius of the star, $\Delta t_{\rm acc}$ is the uninterrupted
duration of an accretion episode, $f_{\rm acc}$ is the mean clump impact
frequency, $\dot{M}$ is the accretion rate, $v$ is the impact speed, and $d$ is
the distance of the star from the Earth. An observational test is proposed,
based on the temporal autocorrelation function of the gravitational wave
signal, to discern whether the Chandrasekhar-Friedman-Schutz instability
switches on and coexists with impact-excited $r$-modes before or during a
gravitational wave observation.","['astro-ph.HE', 'astro-ph.SR', 'gr-qc']",2501.06778," In this study, we analyze the observational images of a Konoplya-Zhidenko
rotating non-Kerr black hole, wherein a thin accretion disk, serving as the
sole background light source, is situated on the equatorial plane of the black
hole. The inner boundary of the thin accretion disk extends to the event
horizon, and the accretion material in the disk exhibits two different motion
behaviors, that is, it moves along the critical plunging orbit inside the
innermost stable circular orbit (ISCO) and follows the Keplerian orbit outside
the ISCO. The shadow image is captured on the imaging plane of a zero angular
momentum observer utilizing advanced fisheye camera ray-tracing techniques. The
results demonstrate that an image consistently reveals a dark region encircled
by a narrow photon ring, which is called the inner shadow. At low observation
inclination angles, the observation intensity is highly concentrated, with the
lensed image of accretion disk being superimposed on the direct image. As
observation inclination angle increases, the direct and lensed images gradually
separate, becoming distinctly distinguishable and forming a hat-like structure.
Furthermore, variations in the parameter space and observation angle will
influence pertinent image characteristics, including image symmetry, the range
or deformation degree of the inner shadow. We further examined the distinctive
characteristics of images observed in both prograde and retrograde accretion
disk scenarios. Subsequently, we also examined the redshift distribution on the
disk. The findings indicate that while variations in relevant parameters do
influence the redshift distribution, the primary factor is the change in
observational inclination. The observer can detect both redshift and blueshift
phenomena on the screen when viewed at a higher observation angle.","['astro-ph.HE', 'gr-qc']",False,,,,"Gravitational waves from r-mode oscillations of stochastically accreting
  neutron stars","Optical appearance of the Konoplya-Zhidenko rotating non-Kerr black hole
  surrounded by a thin accretion disk"
neg-d2-592,2025-02-28,,2503.00319," Electrical control of quantum magnetic states is essential in spintronic
science. Initial studies on the ferromagnetic state control were extended to
collinear antiferromagnets and, more recently, noncollinear antiferromagnets.
However, electrical control mechanisms of such exotic magnetic states remain
poorly understood. Here, we report the first experimental and theoretical
example of the current control of helical antiferromagnets, arising from the
competition between collinear antiferromagnetic exchange and interlayer
Dzyaloshinskii-Moriya interaction in new van-der-Waals (vdW) material
Ni1/3NbS2. Due to the intrinsic broken inversion symmetry, an in-plane current
generates spin-orbit torque that, in turn, interacts directly with the helical
antiferromagnetic order. Our theoretical analyses indicate that a weak
ferromagnetic order coexists due to the Dzyaloshinskii-Moriya interaction,
mediating the spin-orbit torque to collectively rotate the helical
antiferromagnetic order. Our Ni1/3NbS2 nanodevice experiments produce
current-dependent resistance change consistent with the theoretical prediction.
This work widens our understanding of the electrical control of helical
antiferromagnets and promotes vdW quantum magnets as interesting material
platforms for electrical control.","['cond-mat.mtrl-sci', 'cond-mat.other', 'physics.app-ph', 'quant-ph']",2502.13101," The integration of Artificial Intelligence (AI) in urban governance presents
significant opportunities to transform decision-making and enhance
accountability. The paper highlights AI's potential to reposition human
discretion and reshape specific types of accountability, elevating the
decision-making capabilities of both frontline bureaucrats and managers while
ensuring ethical standards and public trust are maintained. While AI can
enhance bureaucratic flexibility and efficiency, its integration will also
necessitate new governance frameworks to mitigate risks associated with uneven
capacity distribution, ethical concerns, and public trust. Following the
literature review and theoretical discussion, this study introduces a set of
guiding principles for AI-assisted urban governance, emphasizing equitable AI
deployment, adaptive administrative structures, robust data governance,
transparent human-AI collaboration, and citizen engagement in oversight
mechanisms. By critically evaluating AI's dual role in expanding discretion and
reinforcing accountability, this paper advances a framework for responsible AI
adoption, ensuring that urban governance remains adaptive, transparent, and
aligned with public values.",['cs.CY'],False,,,,"Current-driven collective control of helical spin texture in van der
  Waals antiferromagnet","AI and the Transformation of Accountability and Discretion in Urban
  Governance"
neg-d2-593,2025-01-08,,2501.04637," A new effective theory framework for fluctuating hydrodynamics in the
relativistic regime is derived using standard thermodynamical principles and
general properties of non-equilibrium stochastic dynamics. For the first time,
we establish clear and concise conditions for ensuring that the resulting
effective theories are causal, stable, and well-posed within general
relativity. These properties are independent of spacetime foliation and are
valid in the full nonlinear regime. Out-of-equilibrium fluctuations are
constrained by a relativistically covariant version of Crooks fluctuation
theorem, which determines how the entropy production is distributed even when
the system is driven by an external force. This leads to an emerging
$\mathbb{Z}_2$ symmetry responsible for imposing fluctuation-dissipation
relations for n-point correlation functions, which matches the standard
constraints for the Schwinger-Keldysh effective action.","['nucl-th', 'hep-ph', 'hep-th']",2502.10722," Modern processors widely equip the Performance Monitoring Unit (PMU) to
collect various architecture and microarchitecture events. Software developers
often utilize the PMU to enhance program's performance, but the potential side
effects that arise from its activation are often disregarded. In this paper, we
find that the PMU can be employed to retrieve instruction operands. Based on
this discovery, we introduce PMU-Data, a novel category of side-channel attacks
aimed at leaking secret by identifying instruction operands with PMU.
  To achieve the PMU-Data attack, we develop five gadgets to encode the
confidential data into distinct data-related traces while maintaining the
control-flow unchanged. We then measure all documented PMU events on three
physical machines with different processors while those gadgets are performing.
We successfully identify two types of vulnerable gadgets caused by DIV and MOV
instructions. Additionally, we discover 40 vulnerable PMU events that can be
used to carry out the PMU-Data attack. We through real experiments to
demonstrate the perniciousness of the PMU-Data attack by implementing three
attack goals: (1) leaking the kernel data illegally combined with the transient
execution vulnerabilities including Meltdown, Spectre, and Zombieload; (2)
building a covert-channel to secretly transfer data; (3) extracting the secret
data protected by the Trusted Execution Environment (TEE) combined with the
Zombieload vulnerability.",['cs.CR'],False,,,,"Effective action for relativistic hydrodynamics from Crooks fluctuation
  theorem",PMU-Data: Data Traces Could be Distinguished
neg-d2-594,2025-01-20,,2501.11724," This work introduces and investigates the function $J(G) =
\frac{\text{Nil}(G)}{L(G)}$, where $\text{Nil}(G)$ denotes the number of
nilpotent subgroups and $L(G)$ the total number of subgroups of a finite group
$G$. The function $J(G)$, defined over the interval $(0,1]$, serves as a tool
to analyze structural patterns in finite groups, particularly within
non-nilpotent families such as supersolvable and dihedral groups. Analytical
results demonstrate the product density of $J(G)$ values in $(0,1]$,
highlighting its distribution across products of dihedral groups. Additionally,
a probabilistic analysis was conducted, and based on extensive computational
simulations, it was conjectured that the sample mean of $J(G)$ values converges
in distribution to the standard normal distribution, in accordance with the
Central Limit Theorem, as the sample size increases. These findings expand the
understanding of multiplicative functions in group theory, offering novel
insights into the structural and probabilistic behavior of finite groups.","['math.GR', 'math.PR']",2501.17673," Enhancement of radiative coupling efficiency between out-of-plane excitonic
emitters in an indium selenide (InSe) film and an integrated waveguide formed
by silicon (Si) Mie-resonant nanodisks is experimentally studied.
Photoluminescence power at the resonant waveguide output is increased by~2.5
times at 950~nm in comparison with the case of a conventional rib waveguide of
the same geometrical parameters due to the efficient excitation of Mie-type
magnetic dipole resonances in individual nanoparticles. These results show
inspiring possibilities for creating new on-chip light emitters for various
integrated photonics applications.","['physics.optics', 'cond-mat.mes-hall']",False,,,,Proportion of Nilpotent Subgroups in Finite Groups and Their Properties,"Mie-resonant silicon waveguide for efficient coupling with excitonic
  emitters in InSe"
neg-d2-595,2025-03-23,,2503.1799," Complex question-answering (QA) systems face significant challenges in
retrieving and reasoning over information that addresses multi-faceted queries.
While large language models (LLMs) have advanced the reasoning capabilities of
these systems, the bounded-recall problem persists, where procuring all
relevant documents in first-stage retrieval remains a challenge. Missing
pertinent documents at this stage leads to performance degradation that cannot
be remedied in later stages, especially given the limited context windows of
LLMs which necessitate high recall at smaller retrieval depths. In this paper,
we introduce SUNAR, a novel approach that leverages LLMs to guide a
Neighborhood Aware Retrieval process. SUNAR iteratively explores a neighborhood
graph of documents, dynamically promoting or penalizing documents based on
uncertainty estimates from interim LLM-generated answer candidates. We validate
our approach through extensive experiments on two complex QA datasets. Our
results show that SUNAR significantly outperforms existing retrieve-and-reason
baselines, achieving up to a 31.84% improvement in performance over existing
state-of-the-art methods for complex QA.",['cs.IR'],2502.08622," Drought is a frequent and costly natural disaster in California, with major
negative impacts on agricultural production and water resource availability,
particularly groundwater. This study investigated the performance of applying
different machine learning approaches to predicting the U.S. Drought Monitor
classification in California. Four approaches were used: a convolutional neural
network (CNN), random forest, XGBoost, and long short term memory (LSTM)
recurrent neural network, and compared to a baseline persistence model. We
evaluated the models' performance in predicting severe drought (USDM drought
category D2 or higher) using a macro F1 binary classification metric. The LSTM
model emerged as the top performer, followed by XGBoost, CNN, and random
forest. Further evaluation of our results at the county level suggested that
the LSTM model would perform best in counties with more consistent drought
patterns and where severe drought was more common, and the LSTM model would
perform worse where drought scores increased rapidly. Utilizing 30 weeks of
historical data, the LSTM model successfully forecasted drought scores for a
12-week period with a Mean Absolute Error (MAE) of 0.33, equivalent to less
than half a drought category on a scale of 0 to 5. Additionally, the LSTM
achieved a macro F1 score of 0.9, indicating high accuracy in binary
classification for severe drought conditions. Evaluation of different window
and future horizon sizes in weeks suggested that at least 24 weeks of data
would result in the best performance, with best performance for shorter horizon
sizes, particularly less than eight weeks.",['cs.LG'],False,,,,"SUNAR: Semantic Uncertainty based Neighborhood Aware Retrieval for
  Complex QA",Forecasting Drought Using Machine Learning in California
neg-d2-596,2025-02-14,,2502.0995," Duminil-Copin and Manolescu (2022) recently proved the scaling relations for
planar Fortuin-Kasteleyn (FK) percolation. In particular, they showed that the
one-arm exponent and the mixing rate exponent are sufficient to derive the
other near-critical exponents. The scaling limit of critical FK percolation is
conjectured to be a conformally invariant random collection of loops called the
conformal loop ensemble (CLE). In this paper, we define the CLE analog of the
mixing rate exponent. Assuming the convergence of FK percolation to CLE, we
show that the mixing rate exponent for FK percolation agrees with that of CLE.
We prove that the CLE$_\kappa$ mixing rate exponent equals $\frac{3
\kappa}{8}-1$, thereby answering Question 3 of Duminil-Copin and Manolescu
(2022). The derivation of the CLE exponent is based on an exact formula for the
Radon-Nikodym derivative between the marginal laws of the odd-level and
even-level CLE loops, which is obtained from the coupling between Liouville
quantum gravity and CLE.","['math.PR', 'math-ph', 'math.MP']",2501.18993," Image Super-Resolution (ISR) has seen significant progress with the
introduction of remarkable generative models. However, challenges such as the
trade-off issues between fidelity and realism, as well as computational
complexity, have also posed limitations on their application. Building upon the
tremendous success of autoregressive models in the language domain, we propose
\textbf{VARSR}, a novel visual autoregressive modeling for ISR framework with
the form of next-scale prediction. To effectively integrate and preserve
semantic information in low-resolution images, we propose using prefix tokens
to incorporate the condition. Scale-aligned Rotary Positional Encodings are
introduced to capture spatial structures and the diffusion refiner is utilized
for modeling quantization residual loss to achieve pixel-level fidelity.
Image-based Classifier-free Guidance is proposed to guide the generation of
more realistic images. Furthermore, we collect large-scale data and design a
training process to obtain robust generative priors. Quantitative and
qualitative results show that VARSR is capable of generating high-fidelity and
high-realism images with more efficiency than diffusion-based methods. Our
codes will be released at https://github.com/qyp2000/VARSR.",['cs.CV'],False,,,,Mixing rate exponent of planar Fortuin-Kasteleyn percolation,Visual Autoregressive Modeling for Image Super-Resolution
neg-d2-597,2025-02-19,,2502.13506," Negation is a fundamental aspect of human communication, yet it remains a
challenge for Language Models (LMs) in Information Retrieval (IR). Despite the
heavy reliance of modern neural IR systems on LMs, little attention has been
given to their handling of negation. In this study, we reproduce and extend the
findings of NevIR, a benchmark study that revealed most IR models perform at or
below the level of random ranking when dealing with negation. We replicate
NevIR's original experiments and evaluate newly developed state-of-the-art IR
models. Our findings show that a recently emerging category - listwise Large
Language Model (LLM) rerankers - outperforms other models but still
underperforms human performance. Additionally, we leverage ExcluIR, a benchmark
dataset designed for exclusionary queries with extensive negation, to assess
the generalizability of negation understanding. Our findings suggest that
fine-tuning on one dataset does not reliably improve performance on the other,
indicating notable differences in their data distributions. Furthermore, we
observe that only cross-encoders and listwise LLM rerankers achieve reasonable
performance across both negation tasks.",['cs.IR'],2501.00873," Capitalizing on the complementary advantages of generative and discriminative
models has always been a compelling vision in machine learning, backed by a
growing body of research. This work discloses the hidden semantic structure
within score-based generative models, unveiling their potential as effective
discriminative priors. Inspired by our theoretical findings, we propose DUSA to
exploit the structured semantic priors underlying diffusion score to facilitate
the test-time adaptation of image classifiers or dense predictors. Notably,
DUSA extracts knowledge from a single timestep of denoising diffusion, lifting
the curse of Monte Carlo-based likelihood estimation over timesteps. We
demonstrate the efficacy of our DUSA in adapting a wide variety of competitive
pre-trained discriminative models on diverse test-time scenarios. Additionally,
a thorough ablation study is conducted to dissect the pivotal elements in DUSA.
Code is publicly available at https://github.com/BIT-DA/DUSA.","['cs.CV', 'cs.LG']",False,,,,Reproducing NevIR: Negation in Neural Information Retrieval,"Exploring Structured Semantic Priors Underlying Diffusion Score for
  Test-time Adaptation"
neg-d2-598,2025-01-25,,2501.15299," We present a bioassay platform that leverages the lasing threshold
distribution in a microlaser ensemble (ME), consisting of hundreds of
individual microlasers, to measure analyte concentrations in solution. An ME is
formed by placing dye-doped microbeads in a micro Fabry-Perot cavity.
Microbeads are surface modified with biorecognition molecules to capture
analytes, while the quenchers resulting from the presence of the analytes on
the microbeads' surfaces increase the lasing thresholds of microlasers. Since
the number of analytes varies from one microbead (or microlaser) to another due
to the randomness in binding processes, a distribution of the analytes (and
hence the quenchers) in the ME is created, which in turn leads to a lasing
threshold distribution in the ME. Experimentally, multiple pumping energy
densities are used to probe the lasing threshold distribution. A theoretical
model is developed to map the lasing threshold distribution to analyte
distribution in the ME, and then to recover the analyte concentration in
solution. Using streptavidin and interleukin-6 as a model system, our platform
achieves a detection limit of 0.1 pg/mL and a dynamic range exceeding five
orders of magnitude, showing that the ME quenching method can provide a high
sensitivity with a superior dynamic range.","['physics.optics', 'physics.app-ph']",2503.06795," Femoral artery access is essential for numerous clinical procedures,
including diagnostic angiography, therapeutic catheterization, and emergency
interventions. Despite its critical role, successful vascular access remains
challenging due to anatomical variability, overlying adipose tissue, and the
need for precise ultrasound (US) guidance. Errors in needle placement can lead
to severe complications, restricting the procedure to highly skilled clinicians
in controlled hospital settings. While robotic systems have shown promise in
addressing these challenges through autonomous scanning and vessel
reconstruction, clinical translation remains limited due to reliance on
simplified phantom models that fail to capture human anatomical complexity. In
this work, we present a method for autonomous robotic US scanning of bifurcated
femoral arteries, and validate it on five vascular phantoms created from real
patient computed tomography (CT) data. Additionally, we introduce a video-based
deep learning US segmentation network tailored for vascular imaging, enabling
improved 3D arterial reconstruction. The proposed network achieves a Dice score
of 89.21% and an Intersection over Union of 80.54% on a newly developed
vascular dataset. The quality of the reconstructed artery centerline is
evaluated against ground truth CT data, demonstrating an average L2 deviation
of 0.91+/-0.70 mm, with an average Hausdorff distance of 4.36+/-1.11mm. This
study is the first to validate an autonomous robotic system for US scanning of
the femoral artery on a diverse set of patient-specific phantoms, introducing a
more advanced framework for evaluating robotic performance in vascular imaging
and intervention.","['cs.RO', 'cs.CV']",False,,,,"Sensitive bioassay with an ultra-large dynamic range via microlaser
  ensemble quenching","Robotic Ultrasound-Guided Femoral Artery Reconstruction of
  Anatomically-Representative Phantoms"
neg-d2-599,2025-02-18,,2502.12769," In the age of misinformation, hallucination -- the tendency of Large Language
Models (LLMs) to generate non-factual or unfaithful responses -- represents the
main risk for their global utility. Despite LLMs becoming increasingly
multilingual, the vast majority of research on detecting and quantifying LLM
hallucination are (a) English-centric and (b) focus on machine translation (MT)
and summarization, tasks that are less common ``in the wild'' than open
information seeking. In contrast, we aim to quantify the extent of LLM
hallucination across languages in knowledge-intensive long-form question
answering. To this end, we train a multilingual hallucination detection model
and conduct a large-scale study across 30 languages and 6 open-source LLM
families. We start from an English hallucination detection dataset and rely on
MT to generate (noisy) training data in other languages. We also manually
annotate gold data for five high-resource languages; we then demonstrate, for
these languages, that the estimates of hallucination rates are similar between
silver (LLM-generated) and gold test sets, validating the use of silver data
for estimating hallucination rates for other languages. For the final rates
estimation, we build a knowledge-intensive QA dataset for 30 languages with
LLM-generated prompts and Wikipedia articles as references. We find that, while
LLMs generate longer responses with more hallucinated tokens for
higher-resource languages, there is no correlation between length-normalized
hallucination rates of languages and their digital representation. Further, we
find that smaller LLMs exhibit larger hallucination rates than larger models.","['cs.CL', 'cs.AI']",2503.18396," We study quantum geometric effects in time-dependent quantum many-body
systems quenched from integrable systems through a unitary transformation whose
phase operator is linear in time. We establish a theorem stating that the Berry
connection matrix thus all associated geometric quantities of the
time-dependent many-body system, can be precisely characterized by excitations
up to two-particle processes derived from the quantum integrable system. This
geometric characterization provides a powerful lens for analyzing dynamical
transitions in driven many-body settings. To illustrate the many-body geometric
influence, we analyze a prototypical time-dependent Ising chain subjected to
both a small longitudinal field and a slowly rotating transverse field, whose
low-energy physics in the scaling limit is instantaneously governed by the
quantum $E_8$ integrable field theory. Focusing on the quantum geometric
potential (QGP), we show the QGP continuously suppresses the instantaneous
energy gaps with decreasing longitudinal field, thereby enhancing many-body
Landau-Zener tunneling as evidenced by the Loschmidt echo and its associated
spectral entropy. The critical threshold for the longitudinal field strength is
determined, where the spectral entropy linearly increases with system size and
exhbits hyperscaling behavior when approaching to the threshold. When the
longitudinal field passes the threshold and decreases toward zero, the QGP
continuously leads to vanishing instantaneous energy gaps involving more
low-energy excitations, resulting in increasing spectral entropy indicative of
many-body Landau-Zener tunneling. Our results unveil telltale quantum geometric
signatures in time-dependent many-body systems, elucidating the intricate
interplay between quantum geometry and dynamics.","['cond-mat.str-el', 'cond-mat.stat-mech', 'quant-ph']",False,,,,"How Much Do LLMs Hallucinate across Languages? On Multilingual
  Estimation of LLM Hallucination in the Wild","Quantum Geometry and Many-Body Landau-Zener Tunneling in Time-dependent
  Quantum Systems with Instantaneous Quantum Integrability"
neg-d2-600,2025-01-25,,2501.15216," Color centers in hexagonal boron nitride (hBN) are promising candidates as
quantum light sources for future technologies. In this work, we utilize a
scattering-type near-field optical microscope (s-SNOM) to study the
photoluminescence (PL) emission characteristics of such quantum emitters in
metalorganic vapor phase epitaxy grown hBN. On the one hand, we demonstrate
direct near-field optical excitation and emission through interaction with the
nanofocus of the tip resulting in a sub-diffraction limited tip-enhanced PL
hotspot. On the other hand, we show that indirect excitation and emission via
scattering from the tip significantly increases the recorded PL intensity. This
demonstrates that the tip-assisted PL (TAPL) process efficiently guides the
generated light to the detector. We apply the TAPL method to map the in-plane
dipole orientations of the hBN color centers on the nanoscale. This work
promotes the widely available s-SNOM approach to applications in the quantum
domain including characterization and optical control.","['cond-mat.mes-hall', 'physics.optics']",2503.08224," Reconstructing animatable and high-quality 3D head avatars from monocular
videos, especially with realistic relighting, is a valuable task. However, the
limited information from single-view input, combined with the complex head
poses and facial movements, makes this challenging. Previous methods achieve
real-time performance by combining 3D Gaussian Splatting with a parametric head
model, but the resulting head quality suffers from inaccurate face tracking and
limited expressiveness of the deformation model. These methods also fail to
produce realistic effects under novel lighting conditions. To address these
issues, we propose HRAvatar, a 3DGS-based method that reconstructs
high-fidelity, relightable 3D head avatars. HRAvatar reduces tracking errors
through end-to-end optimization and better captures individual facial
deformations using learnable blendshapes and learnable linear blend skinning.
Additionally, it decomposes head appearance into several physical properties
and incorporates physically-based shading to account for environmental
lighting. Extensive experiments demonstrate that HRAvatar not only reconstructs
superior-quality heads but also achieves realistic visual effects under varying
lighting conditions.",['cs.CV'],False,,,,"Nanoscale resolved mapping of the dipole emission of hBN color centers
  with a scattering-type scanning near-field optical microscope",HRAvatar: High-Quality and Relightable Gaussian Head Avatar
neg-d2-601,2025-03-12,,2503.09731," We conduct two searches for continuous, nearly monochromatic gravitational
waves originating from the central compact objects in the supernova remnants
Cassiopeia A and Vela Jr. using public LIGO data. The search for Cassiopeia A
targets signal frequencies between 20 Hz and 400 Hz; the Vela Jr. search
between 400 Hz and 1700 Hz, and both investigate the broadest set of waveforms
ever considered with highly sensitive deterministic search methods. Above 1500
Hz the Vela Jr. search is the most sensitive carried out thus far, improving on
previous results by over 300\%. Above 976 Hz these results improve on existing
ones by 50\%. In all we investigate over $10^{18}$ waveforms, leveraging the
computational power donated by thousands of Einstein@Home volunteers. We
perform a 4-stage follow-up on more than 6 million waveforms. None of the
considered waveforms survives the follow-up scrutiny, indicating no significate
detection candidate. Our null results constrain the maximum amplitude of
continuous signals as a function of signal frequency from the targets. The most
stringent 90\% confidence upper limit for Cas A is $h_0^{90 \%}\approx
7.3\times10^{-26}$ near 200 Hz, and for Vela Jr. it is $h_0^{90 \%}\approx
8.9\times10^{-26}$ near 400 Hz. Translated into upper limits on the ellipticity
and r-mode amplitude, our results probe physically interesting regions: for
example the ellipticity of Vela Jr. is constrained to be smaller than $10^{-7}$
across the frequency band, with a tighter constraint of less than
$2\times10^{-8}$ at the highest frequencies.","['gr-qc', 'hep-ph']",2502.13199," GitHub Copilot is transforming software development by automating tasks and
boosting productivity through AI-driven code generation. In this paper, we
con-duct a literature survey to synthesize insights on Copilot's impact on
productivity and security. We review academic journal databases, industry
reports, and official docu-mentation to highlight key findings and challenges.
While Copilot accelerates coding and prototyping, concerns over security
vulnerabilities and intellectual property risks persist. Drawing from the
literature, we provide a perspective on best practices and future directions
for responsible AI adoption in software engineering, offering action-able
insights for developers and organizations to integrate Copilot effectively
while maintaining high standards of quality and security.","['cs.SE', 'cs.AI']",False,,,,"Results from an Einstein@Home search for continuous gravitational waves
  from Cassiopeia A and Vela Jr. using LIGO O2 data","The Role of GitHub Copilot on Software Development: A Perspec-tive on
  Productivity, Security, Best Practices and Future Directions"
neg-d2-602,2025-03-06,,2503.04098," One of the more surprising astrophysical discoveries of the last decade has
been the presence of enormous quantities of dust at megaparsec distances from
galaxies, which has important implications for galaxy evolution, the
circumgalactic and intergalactic medium, and observational cosmology. In this
work, we present a novel method for studying these vast halos of circumgalactic
dust: a maximum-likelihood estimator for dust-induced extinction of background
galaxies. This estimator can accommodate a broad range of archival photometric
data and can incorporate different dust reddening prescriptions, making it
applicable to diverse galaxy types and redshifts. We apply the estimator to the
redMaGiC catalog of luminous red galaxies, selected for their tight dispersion
in color and well-constrained photometric redshifts, and measure the resulting
extinction as a function of projected distance from WISExSuperCOSMOS and
redMaGiC foreground galaxies. We detect significant dust-induced extinction
profiles extending to at least 1 megaparsec from galactic disks, with
noticeable differences between star-forming and quiescent galaxies:
star-forming galaxies exhibit a pronounced rise in extinction within the inner
50 kiloparsecs and a steep decline beyond 1 megaparsec, while the quiescent
galaxies host little dust in the inner halo but have detectable extinction out
to 30 megaparsecs. We test the robustness of our results using star catalogs
and inverted foreground and background samples and find no evidence for
significant systematic error. Our approach provides a powerful tool for
studying the interplay between circumgalactic dust, galaxy evolution, and
large-scale structure, with potential applications in a number of astrophysical
subfields.","['astro-ph.GA', 'astro-ph.CO']",2502.1977," With the increasing prevalence of Web-based platforms handling vast amounts
of user data, machine unlearning has emerged as a crucial mechanism to uphold
users' right to be forgotten, enabling individuals to request the removal of
their specified data from trained models. However, the auditing of machine
unlearning processes remains significantly underexplored. Although some
existing methods offer unlearning auditing by leveraging backdoors, these
backdoor-based approaches are inefficient and impractical, as they necessitate
involvement in the initial model training process to embed the backdoors. In
this paper, we propose a TAilored Posterior diffErence (TAPE) method to provide
unlearning auditing independently of original model training. We observe that
the process of machine unlearning inherently introduces changes in the model,
which contains information related to the erased data. TAPE leverages
unlearning model differences to assess how much information has been removed
through the unlearning operation. Firstly, TAPE mimics the unlearned posterior
differences by quickly building unlearned shadow models based on first-order
influence estimation. Secondly, we train a Reconstructor model to extract and
evaluate the private information of the unlearned posterior differences to
audit unlearning. Existing privacy reconstructing methods based on posterior
differences are only feasible for model updates of a single sample. To enable
the reconstruction effective for multi-sample unlearning requests, we propose
two strategies, unlearned data perturbation and unlearned influence-based
division, to augment the posterior difference. Extensive experimental results
indicate the significant superiority of TAPE over the state-of-the-art
unlearning verification methods, at least 4.5$\times$ efficiency speedup and
supporting the auditing for broader unlearning scenarios.","['cs.CR', 'cs.LG']",False,,,,"A Detection of Circumgalactic Dust at Megaparsec Scales with Maximum
  Likelihood Estimation",TAPE: Tailored Posterior Difference for Auditing of Machine Unlearning
neg-d2-603,2025-01-28,,2501.17363," The difficulty of assessing the state lies in a little predictable change in
the dimension of a dynamic system under the influence of internal changes and
environmental parameters. In the work, the state of such a system is estimated
by the method of integral indicators. The application of the method of integral
indicators allowed us to evaluate the activity of an enterprise. In the present
work, the method of integrated indicators is used to assess the control of a
digital copy (enterprise).",['math.OC'],2503.12638," The emergence of 6G wireless networks demands solutions that seamlessly
integrate communication and sensing. This letter proposes a novel waveform
design for joint sensing and communication (JSAC) systems, combining
single-carrier interleaved frequency division multiplexing (SC-IFDM), a 5G
communication candidate signal, with frequency modulated continuous wave
(FMCW), widely used for sensing. The proposed waveform leverages the sparse
nature of FMCW within SC-IFDM to achieve orthogonal integration in three steps:
SC-IFDM symbols are allocated alongside the sparse FMCW, followed by the
SC-IFDM transform into the time domain, and a cyclic prefix (CP) is applied in
which phase shifts are introduced to the FMCW. Additionally, an enhanced
channel estimation method is incorporated to boost system performance.
Simulation results demonstrate the proposed waveform's ability to deliver
high-resolution sensing and superior communication performance, surpassing
traditional multicarrier designs.",['eess.SP'],False,,,,"Assessment various control methods a digital copy of enterprise by
  integral indicator",Single-Carrier Waveform Design for Joint Sensing and Communication
neg-d2-604,2025-02-27,,2502.19893," The transferable neural network (TransNet) is a two-layer shallow neural
network with pre-determined and uniformly distributed neurons in the hidden
layer, and the least-squares solvers can be particularly used to compute the
parameters of its output layer when applied to the solution of partial
differential equations. In this paper, we integrate the TransNet technique with
the nonoverlapping domain decomposition and the interface conditions to develop
a novel multiple transferable neural network (Multi-TransNet) method for
solving elliptic interface problems, which typically contain discontinuities in
both solutions and their derivatives across interfaces. We first propose an
empirical formula for the TransNet to characterize the relationship between the
radius of the domain-covering ball, the number of hidden-layer neurons, and the
optimal neuron shape. In the Multi-TransNet method, we assign each subdomain
one distinct TransNet with an adaptively determined number of hidden-layer
neurons to maintain the globally uniform neuron distribution across the entire
computational domain, and then unite all the subdomain TransNets together by
incorporating the interface condition terms into the loss function. The
empirical formula is also extended to the Multi-TransNet and further employed
to estimate appropriate neuron shapes for the subdomain TransNets, greatly
reducing the parameter tuning cost. Additionally, we propose a normalization
approach to adaptively select the weighting parameters for the terms in the
loss function. Ablation studies and extensive experiments with comparison tests
on different types of elliptic interface problems with low to high contrast
diffusion coefficients in two and three dimensions are carried out to
numerically demonstrate the superior accuracy, efficiency, and robustness of
the proposed Multi-TransNet method.","['math.NA', 'cs.LG', 'cs.NA']",2503.06055," Recent approaches to the theory of dynamic programming view dynamic programs
as families of policy operators acting on partially ordered sets. In this
paper, we extend these ideas by shifting from arbitrary partially ordered sets
to ordered vector space. The advantage of working in this setting is that
ordered vector spaces have well integrated algebric and order structure, which
leads to sharper fixed point results. These fixed point results can then be
exploited to obtain strong optimality properties. We illustrate our results
through a range of applications, including new findings for several useful
models.",['math.OC'],False,,,,"A Multiple Transferable Neural Network Method with Domain Decomposition
  for Elliptic Interface Problems",Dynamic Programming in Ordered Vector Space
neg-d2-605,2025-01-30,,2501.18262," A novel strategy is proposed to improve the accuracy of state estimation and
reconstruction from low-fidelity models and sparse data from sensors. This
strategy combines ensemble Data Assimilation (DA) and Machine Learning (ML)
tools, exploiting their complementary features. ML techniques rely on the data
produced by DA methods during analysis phases to train physics-informed
corrective algorithms, which are then coupled with the low-fidelity models when
data from sensors is unavailable. The methodology is validated via the analysis
of the turbulent plane channel flow test case for $Re_\tau \approx 550$. Here,
the low-fidelity model consists of coarse-grained simulations coupled with the
Immersed Boundary Method (IBM), while observation is sampled by a highly
refined body-fitted calculation. The analysis demonstrates the capabilities of
the algorithm based on DA and ML to accurately predict the flow features with
significantly reduced computational costs. This approach exhibits potential for
future synergistic applications of DA and ML, leveraging the robustness and
efficiency of ML models alongside the physical interpretability ensured by DA
algorithms.",['physics.flu-dyn'],2501.0779," Robust Bayesian inference using density power divergence (DPD) has emerged as
a promising approach for handling outliers in statistical estimation. While the
DPD-based posterior offers theoretical guarantees for robustness, its practical
implementation faces significant computational challenges, particularly for
general parametric models with intractable integral terms. These challenges
become especially pronounced in high-dimensional settings where traditional
numerical integration methods prove inadequate and computationally expensive.
We propose a novel sampling methodology that addresses these limitations by
integrating the loss-likelihood bootstrap with a stochastic gradient descent
algorithm specifically designed for DPD-based estimation. Our approach enables
efficient and scalable sampling from DPD-based posteriors for a broad class of
parametric models, including those with intractable integrals, and we further
extend it to accommodate generalized linear models. Through comprehensive
simulation studies, we demonstrate that our method efficiently samples from
DPD-based posteriors, offering superior computational scalability compared to
conventional methods, particularly in high-dimensional settings. The results
also highlight its ability to handle complex parametric models with intractable
integral terms.",['stat.ME'],False,,,,"Enhanced State Estimation for turbulent flows combining Ensemble Data
  Assimilation and Machine Learning","Sampling from Density power divergence-based Generalized posterior
  distribution via Stochastic optimization"
neg-d2-606,2025-02-05,,2502.03616," Noncooperative multi-agent systems often face coordination challenges due to
conflicting preferences among agents. In particular, agents acting in their own
self-interest can settle on different equilibria, leading to suboptimal
outcomes or even safety concerns. We propose an algorithm named trading auction
for consensus (TACo), a decentralized approach that enables noncooperative
agents to reach consensus without communicating directly or disclosing private
valuations. TACo facilitates coordination through a structured trading-based
auction, where agents iteratively select choices of interest and provably reach
an agreement within an a priori bounded number of steps. A series of numerical
experiments validate that the termination guarantees of TACo hold in practice,
and show that TACo achieves a median performance that minimizes the total cost
across all agents, while allocating resources significantly more fairly than
baseline approaches.","['cs.GT', 'cs.MA']",2503.11721," We show that Laser Interferometer Space Antenna can uniquely identify the
sites of intermediate-mass binary black hole (IMBBH) mergers if they occur in
Active Galactic Nuclei (AGN) disks with a gas density $\rho\geq10^{-12} \, {\rm
g/cc}$ via measurement of dynamical friction effect in the gravitational
waveform. We find that even a single observation of a gravitational wave source
with a total mass of $10^3 M_{\odot}$ and a mass ratio of 2 at a luminosity
distance of 3 Gpc is sufficient to confidently associate the merger to be in an
AGN disk with a density $\sim 10^{-12} \, {\rm g/cc}$, as it allows estimation
of the density with an error bar ${\cal O}(100\%)$. This provides a new way of
inferring AGN disk densities that complement traditional X-ray observations.
Further, we find that neglecting the presence of environmental effects in the
waveform models used for parameter estimation can bias the chirp mass, mass
ratio and arrival time of a merger. If not corrected, this can significantly
impact our ability to carry out multiband data analysis of IMBBHs that combines
information from LISA and the ground-based gravitational wave detectors.","['astro-ph.HE', 'astro-ph.GA', 'gr-qc']",False,,,,Noncooperative Equilibrium Selection via a Trading-based Auction,"Identifying intermediate mass binary black hole mergers in AGN disks
  using LISA"
neg-d2-607,2025-03-09,,2503.06663," Deep neural network (DNN) inference in energy harvesting (EH) devices poses
significant challenges due to resource constraints and frequent power
interruptions. These power losses not only increase end-to-end latency, but
also compromise inference consistency and accuracy, as existing checkpointing
and restore mechanisms are prone to errors. Consequently, the quality of
service (QoS) for DNN inference on EH devices is severely impacted. In this
paper, we propose an energy-adaptive DNN inference mechanism capable of
dynamically transitioning the model into a low-power mode by reducing
computational complexity when harvested energy is limited. This approach
ensures that end-to-end latency requirements are met. Additionally, to address
the limitations of error-prone checkpoint-and-restore mechanisms, we introduce
a checkpoint-free intermittent inference framework that ensures consistent,
progress-preserving DNN inference during power failures in energy-harvesting
systems.",['cs.CE'],2503.04086," Gcd-graphs represent an interesting and historically important class of
integral graphs. Since the pioneering work of Klotz and Sander, numerous
incarnations of these graphs have been explored in the literature. In this
article, we define and establish some foundational properties of gcd-graphs
defined over a general finite commutative ring. In particular, we investigate
the connectivity and diameter of these graphs. Additionally, when the ring is a
finite symmetric $\mathbb{Z}/n$-algebra, we give an explicit description of
their spectrum using the theory of Ramanujan sums that gives a unified
treatment of various results in the literature.","['math.NT', 'math.AC', 'math.CO']",False,,,,"Energy-Adaptive Checkpoint-Free Intermittent Inference for Low Power
  Energy Harvesting Systems",On gcd-graphs over finite rings
neg-d2-608,2025-01-03,,2501.01902," Massive galaxies in cooling flow clusters display clear evidence of feedback
from Active Galactic Nuclei (AGN). Joint X-ray and radio observations have
shown that AGN radio jets push aside the surrounding hot gas and form cavities
in the hot intracluster medium (ICM). These systems host complex,
kiloparsec-scale, multiphase filamentary structures, from warm ionized (10,000
K) to cold molecular ($<$100 K). These striking clumpy filaments are believed
to be a natural outcome of thermally unstable cooling from the hot ICM, likely
triggered by feedback processes while contributing to feeding the AGN via
Chaotic Cold Accretion (CCA). However, the detailed constraints on the
formation mechanism of the filaments are still uncertain, and the connection
between the different gas phases has to be fully unveiled. By leveraging a
sample of seven X-ray bright cooling-flow clusters, we have discovered a tight
positive correlation between the X-ray surface brightness and the H$\alpha$
surface brightness of the filaments over two orders of magnitude, as also found
in stripped tails.","['astro-ph.GA', 'astro-ph.HE']",2501.18848," Symbolic task representation is a powerful tool for encoding human
instructions and domain knowledge. Such instructions guide robots to accomplish
diverse objectives and meet constraints through reinforcement learning (RL).
Most existing methods are based on fixed mappings from environmental states to
symbols. However, in inspection tasks, where equipment conditions must be
evaluated from multiple perspectives to avoid errors of oversight, robots must
fulfill the same symbol from different states. To help robots respond to
flexible symbol mapping, we propose representing symbols and their mapping
specifications separately within an RL policy. This approach imposes on RL
policy to learn combinations of symbolic instructions and mapping
specifications, requiring an efficient learning framework. To cope with this
issue, we introduce an approach for learning flexible policies called Symbolic
Instructions with Adjustable Mapping Specifications (SIAMS). This paper
represents symbolic instructions using linear temporal logic (LTL), a formal
language that can be easily integrated into RL. Our method addresses the
diversified completion patterns of instructions by (1) a specification-aware
state modulation, which embeds differences in mapping specifications in state
features, and (2) a symbol-number-based task curriculum, which gradually
provides tasks according to the learning's progress. Evaluations in 3D
simulations with discrete and continuous action spaces demonstrate that our
method outperforms context-aware multitask RL comparisons.",['cs.RO'],False,,,,"H$\alpha$-X-ray Surface Brightness Correlation for Filaments in Cooling
  Flow Clusters","Reinforcement Learning of Flexible Policies for Symbolic Instructions
  with Adjustable Mapping Specifications"
neg-d2-609,2025-03-18,,2503.13892," The formation mechanisms of open cluster (OCs) groups remain unclear due to
limited sample sizes and data precision. Recent advancements in Gaia
astrometric data provide an unprecedented opportunity to study OC groups in
greater detail. This study aims to extend the sample of OC groups and
investigate their formation and evolution mechanisms, with a focus on the role
of stellar feedback in triggering star formation. We identify four new OC
groups based on Gaia data, whose member OCs are spatially proximate and
kinematically coherent. Their age spreads are consistent with the timescale of
continuous star formation, suggesting that their member OCs formed sequentially
from the same molecular cloud. N-body simulation results further reveal that
these groups will gradually disperse, evolving into independent OCs. By
analyzing the correlation between OC ages and their separation from potential
SN explosion sites, we predict SN explosion regions around the birthplaces of
OC groups. The strong correlation between OC ages and predicted SN explosion
sites supports a supernova-triggered star formation scenario. Additionally, we
trace pulsar (PSR) orbits to examine their association with these regions. We
detected three PSRs near Group 1 and 26 PSRs near Group 2, whose birthplaces
align with the predicted SN explosions regions. The presence of PSRs associated
with OC groups provides additional observational evidence for SN explosions in
this region, further supporting a supernova-triggered star formation scenario
for G1 and G2. We propose that multiple SN explosions in a short period
triggered the formation of Group 1 and Group 2, reinforcing the hierarchical
star formation model. These results highlight the multi-scale interactions
driving star and OC formation and provide new insights into the role of stellar
feedback in shaping OC groups.",['astro-ph.GA'],2501.19382," In this paper, we propose a novel loop closure detection algorithm that uses
graph attention neural networks to encode semantic graphs to perform place
recognition and then use semantic registration to estimate the 6 DoF relative
pose constraint. Our place recognition algorithm has two key modules, namely, a
semantic graph encoder module and a graph comparison module. The semantic graph
encoder employs graph attention networks to efficiently encode spatial,
semantic and geometric information from the semantic graph of the input point
cloud. We then use self-attention mechanism in both node-embedding and
graph-embedding steps to create distinctive graph vectors. The graph vectors of
the current scan and a keyframe scan are then compared in the graph comparison
module to identify a possible loop closure. Specifically, employing the
difference of the two graph vectors showed a significant improvement in
performance, as shown in ablation studies. Lastly, we implemented a semantic
registration algorithm that takes in loop closure candidate scans and estimates
the relative 6 DoF pose constraint for the LiDAR SLAM system. Extensive
evaluation on public datasets shows that our model is more accurate and robust,
achieving 13% improvement in maximum F1 score on the SemanticKITTI dataset,
when compared to the baseline semantic graph algorithm. For the benefit of the
community, we open-source the complete implementation of our proposed algorithm
and custom implementation of semantic registration at
https://github.com/crepuscularlight/SemanticLoopClosure","['cs.CV', 'cs.RO']",False,,,,"Formation and evolution of new primordial open cluster groups:
  Feedback-driven star formation","LiDAR Loop Closure Detection using Semantic Graphs with Graph Attention
  Networks"
neg-d2-610,2025-01-19,,2501.11137," The transformation to equivalent dimensions that offers a novel approach for
investigating earthquake clustering was engaged to analyze the preparatory
phase of the 2020 Samos, Greece, Mw7.0 main shock. The analysis considered
earthquakes that occurred between 2006 and October 2020, covering an area
extended three times the length of the main rupture. Each earthquake was
parameterized by its magnitude, the interevent time (interval since the
previous earthquake), and the interevent spatial distance (distance between the
epicenters of consecutive earthquakes). Transforming these parameters into
equivalent dimensions allowed them to be directly compared. The degree of
clustering was quantified using the average distance between earthquakes in
this transformed parameter space, calculated within consecutive 100 events data
windows. Results revealed a distinct pattern, the average distance was
increasing steadily during the twelve year period before the main shock. These
temporal changes in the average distance were driven by a systematic evolution
of earthquake clustering in the used parameter space. Beginning from a
two-cluster system, when the distance was minimal, the clustering development
continued along two branches and ended before the main shock with the formation
of five earthquake clusters of different characteristics.",['physics.geo-ph'],2501.04675," Chart interpretation is crucial for visual data analysis, but accurately
extracting information from charts poses significant challenges for automated
models. This study investigates the fine-tuning of DEPLOT, a modality
conversion module that translates the image of a plot or chart to a linearized
table, on a custom dataset of 50,000 bar charts. The dataset comprises simple,
stacked, and grouped bar charts, targeting the unique structural features of
these visualizations. The finetuned DEPLOT model is evaluated against its base
version using a test set of 1,000 images and two metrics: Relative Mapping
Similarity (RMS), which measures categorical mapping accuracy, and Relative
Number Set Similarity (RNSS), which evaluates numerical interpretation
accuracy. To further explore the reasoning capabilities of large language
models (LLMs), we curate an additional set of 100 bar chart images paired with
question answer sets. Our findings demonstrate that providing a structured
intermediate table alongside the image significantly enhances LLM reasoning
performance compared to direct image queries.","['cs.CL', 'cs.AI', 'cs.CV', 'cs.LG']",False,,,,"Clustering indications before the Mw7.0 2020 Samos, Greece, main shock
  as revealed in an equivalent dimensions space","Enhancing Financial VQA in Vision Language Models using Intermediate
  Structured Representations"
neg-d2-611,2025-02-06,,2502.03996," The efficient and independent operation of power-over-fiber (PoF) and
distributed acoustic sensing (DAS) has been demonstrated using standard
single-mode fiber (SSMF). A transmission optical power efficiency (OPTE) of
6.67% was achieved over an 11.8 km fiber link, supporting both power delivery
and distributed optical fiber sensing (DOFS). To minimize cross-talk, the
system separates the power and sensing channels by a 40 THz bandwidth. In the
experiment, the power and sensing light wavelengths are 1064 nm (continuous)
and 1550 nm (pulsed), respectively. As the transmitted optical power increased
from 0 W to 2.13 W, the DAS system successfully localized vibration sources and
reconstructed phase information, confirming its ability to operate under high
optical power. The reported scheme verifies the possibility of constructing the
sensing-energy hybrid network based on conventional optical fiber with the
advantages of flexibility and low cost.",['physics.optics'],2502.20517," This is the first of three papers motivated by the author's desire to
understand and explain ""algebraically"" one aspect of Dmitriy Zhuk's proof of
the CSP Dichotomy Theorem. In this paper we study abelian congruences in
varieties having a weak difference term. Each class of the congruence supports
an abelian group structure; if the congruence is minimal, each class supports
the structure of a vector space over a division ring determined by the
congruence. A construction due to J. Hagemann, C. Herrmann and R. Freese in the
congruence modular setting extends to varieties with a weak difference term,
and provides a ""universal domain"" for the abelian groups or vector spaces that
arise from the classes of the congruence within a single class of the
annihilator of the congruence. The construction also supports an extension of
Freese's similarity relation (between subdirectly irreducible algebras) from
the congruence modular setting to varieties with a weak difference term.",['math.LO'],False,,,,"Power-over-fiber and distributed acoustic sensing hybridization in
  single fiber channel","Abelian congruences and similarity in varieties with a weak difference
  term"
neg-d2-612,2025-03-23,,2503.17933," To improve the reliability of Large Language Models (LLMs) in clinical
applications, retrieval-augmented generation (RAG) is extensively applied to
provide factual medical knowledge. However, beyond general medical knowledge
from open-ended datasets, clinical case-based knowledge is also critical for
effective medical reasoning, as it provides context grounded in real-world
patient experiences. Motivated by this, we propose Experience Retrieval
Augmentation - ExpRAG framework based on Electronic Health Record (EHR), aiming
to offer the relevant context from other patients' discharge reports. ExpRAG
performs retrieval through a coarse-to-fine process, utilizing an EHR-based
report ranker to efficiently identify similar patients, followed by an
experience retriever to extract task-relevant content for enhanced medical
reasoning. To evaluate ExpRAG, we introduce DischargeQA, a clinical QA dataset
with 1,280 discharge-related questions across diagnosis, medication, and
instruction tasks. Each problem is generated using EHR data to ensure realistic
and challenging scenarios. Experimental results demonstrate that ExpRAG
consistently outperforms a text-based ranker, achieving an average relative
improvement of 5.2%, highlighting the importance of case-based knowledge for
medical reasoning.","['cs.CL', 'cs.AI', 'cs.IR']",2502.21272," Let $K = \mathbb{R}$ or $\mathbb{C}$. An $n$-element subset $A$ of $K$ is a
$B_h$-set if every element of $K$ has at most one representation as the sum of
$h$ not necessarily distinct elements of $A$. Associated to the $B_h$ set $A =
\{a_1,\ldots, a_n\}$ are the $B_h$-vectors $\mathbf{a} = (a_1,\ldots, a_n)$ in
$K^n$. This paper proves that ``almost all'' $n$-element subsets of $K$ are
$B_h$-sets in the sense that the set of all $B_h$-vectors is a dense open
subset of $K^n$.","['math.CO', 'math.NT']",False,,,,"Experience Retrieval-Augmentation with Electronic Health Records Enables
  Accurate Discharge QA",$B_h$-sets of real and complex numbers
neg-d2-613,2025-01-22,,2501.12663," This paper investigates the trajectories of light beams in a Kerr metric,
which describes the gravitational field in the neighborhood of a rotating black
hole. After reduction by cyclic coordinates, this problem reduces to analysis
of a Hamiltonian system with two degrees of freedom. A bifurcation diagram is
constructed and a classification is made of the types of trajectories of the
system according to the values of first integrals. Relations describing the
boundary of the shadow of the black hole are obtained for a stationary observer
who rotates with an arbitrary angular velocity about the axis of rotation of
the black hole.","['math.DS', 'gr-qc']",2503.06971," Europa, Jupiter's second Galilean moon, is believed to host a subsurface
ocean in contact with a rocky mantle, where hydrothermal activity may drive the
synthesis of organic molecules. Of these molecules, abiotic synthesis of
aromatic amino acids is unlikely, and their detection on Europa could be
considered a biosignature. Fluorescence from aromatic amino acids, with
characteristic emissions in the 200-400 nanometer wavelength range, can be
induced by a laser and may be detectable where ocean material has been
relatively recently emplaced on Europa's surface, as indicated by geologically
young terrain and surface features. However, surface bombardment by charged
particles from the Jovian magnetosphere and solar ultraviolet (UV) radiation
degrades organic molecules, limiting their longevity. We model radiolysis and
photolysis of aromatic amino acids embedded in ice, showing dependencies on
hemispheric and latitudinal patterns of charged particle bombardment and ice
phase. We demonstrate that biosignatures contained within freshly deposited ice
in high-latitude regions on the surface of Europa are detectable using
laser-induced UV fluorescence, even from an orbiting spacecraft.",['astro-ph.EP'],False,,,,"Trajectories of light beams in a Kerr metric: the influence of the
  rotation of an observer on the shadow of a black hole",Fluorescent Biomolecules Detectable in Near-Surface Ice on Europa
neg-d2-614,2025-01-23,,2501.13681," Particle discretizations of partial differential equations are advantageous
for high-dimensional kinetic models in phase space due to their better
scalability than continuum approaches with respect to dimension. Complex
processes collectively referred to as \textit{particle noise} hamper long-time
simulations with particle methods. One approach to address this problem is
particle mesh adaptivity or remapping, known as \textit{particle resampling}.
This paper introduces a resampling method that projects particles to and from a
(finite element) function space. The method is simple; using standard sparse
linear algebra and finite element techniques, it can adapt to almost any set of
new particle locations and preserves all moments up to the order of polynomial
represented exactly by the continuum function space.
  This work is motivated by the Vlasov-Maxwell-Landau model of magnetized
plasmas with up to six dimensions, $3X$ in physical space and $3V$ in velocity
space, and is developed in the context of a $1X$ + $1V$ Vlasov-Poisson model of
Landau damping with logically regular particle and continuum phase space grids.
The evaluation codes are publicly available, along with the data and
reproducibility artifacts, and developed in the PETSc numerical library
(petsc.org).","['physics.plasm-ph', 'physics.comp-ph']",2503.12662," This paper introduces TuneNSearch, a hybrid transfer learning and local
search approach for addressing different variants of vehicle routing problems
(VRP). Recently, multi-task learning has gained much attention for solving VRP
variants. However, this adaptability often compromises the performance of the
models. To address this challenge, we first pre-train a reinforcement learning
model on the multi-depot VRP, followed by a short fine-tuning phase to adapt it
to different variants. By leveraging the complexity of the multi-depot VRP, the
pre-trained model learns richer node representations and gains more
transferable knowledge compared to models trained on simpler routing problems,
such as the traveling salesman problem. TuneNSearch employs, in the first
stage, a Transformer-based architecture, augmented with a residual edge-graph
attention network to capture the impact of edge distances and residual
connections between layers. This architecture allows for a more precise capture
of graph-structured data, improving the encoding of VRP's features. After
inference, our model is also coupled with a second stage composed of a local
search algorithm, which yields substantial performance gains with minimal
computational overhead added. Results show that TuneNSearch outperforms many
existing state-of-the-art models trained for each VRP variant, requiring only
one-fifth of the training epochs. Our approach demonstrates strong
generalization, achieving high performance across different tasks,
distributions and problem sizes, thus addressing a long-standing gap in the
literature.",['cs.LG'],False,,,,A projection method for particle resampling,"TuneNSearch: a hybrid transfer learning and local search approach for
  solving vehicle routing problems"
neg-d2-615,2025-03-06,,2503.04572," Declines in vaccination coverage for vaccine-preventable diseases, such as
measles and chickenpox, have enabled their surprising comebacks and pose
significant public health challenges in the wake of growing vaccine hesitancy.
Vaccine opt-outs and refusals are often fueled by beliefs concerning
perceptions of vaccine effectiveness and exaggerated risks. Here, we quantify
the impact of competing beliefs -- vaccine-averse versus vaccine-neutral -- on
social imitation dynamics of vaccination, alongside the epidemiological
dynamics of disease transmission. These beliefs may be pre-existing and fixed,
or coevolving attitudes. This interplay among beliefs, behaviors, and disease
dynamics demonstrates that individuals are not perfectly rational; rather, they
base their vaccine uptake decisions on beliefs, personal experiences, and
social influences. We find that the presence of a small proportion of fixed
vaccine-averse beliefs can significantly exacerbate the vaccination dilemma,
making the tipping point in the hysteresis loop more sensitive to changes in
individuals' perceived costs of vaccination and vaccine effectiveness. However,
in scenarios where competing beliefs spread concurrently with vaccination
behavior, their double-edged impact can lead to self-correction and alignment
between vaccine beliefs and behaviors. The results show that coevolution of
vaccine beliefs and behaviors makes populations more sensitive to abrupt
changes in perceptions of vaccine cost and effectiveness compared to scenarios
without beliefs. Our work provides valuable insights into harnessing the social
contagion of even vaccine-neutral attitudes to overcome vaccine hesitancy.","['physics.soc-ph', 'q-bio.PE']",2501.16637," In this note, we review the latest qualitative results, referring to the
Li\'enard Equation, in the framework of non-conformable, generalized and
fractional differential operators.",['math.GM'],False,,,,"Social Imitation Dynamics of Vaccination Driven by Vaccine Effectiveness
  and Beliefs",On the Li\'enard's type equation: an icon of the Nonlinear Analysis
neg-d2-616,2025-01-16,,2501.09954," Design space exploration (DSE) plays a crucial role in enabling custom
hardware architectures, particularly for emerging applications like AI, where
optimized and specialized designs are essential. With the growing complexity of
deep neural networks (DNNs) and the introduction of advanced foundational
models (FMs), the design space for DNN accelerators is expanding at an
exponential rate. Additionally, this space is highly non-uniform and
non-convex, making it increasingly difficult to navigate and optimize.
Traditional DSE techniques rely on search-based methods, which involve
iterative sampling of the design space to find the optimal solution. However,
this process is both time-consuming and often fails to converge to the global
optima for such design spaces. Recently, AIrchitect v1, the first attempt to
address the limitations of search-based techniques, transformed DSE into a
constant-time classification problem using recommendation networks. In this
work, we propose AIrchitect v2, a more accurate and generalizable
learning-based DSE technique applicable to large-scale design spaces that
overcomes the shortcomings of earlier approaches. Specifically, we devise an
encoder-decoder transformer model that (a) encodes the complex design space
into a uniform intermediate representation using contrastive learning and (b)
leverages a novel unified representation blending the advantages of
classification and regression to effectively explore the large DSE space
without sacrificing accuracy. Experimental results evaluated on 10^5 real DNN
workloads demonstrate that, on average, AIrchitect v2 outperforms existing
techniques by 15% in identifying optimal design points. Furthermore, to
demonstrate the generalizability of our method, we evaluate performance on
unseen model workloads (LLMs) and attain a 1.7x improvement in inference
latency on the identified hardware architecture.","['cs.LG', 'cs.AI', 'cs.AR']",2503.12662," This paper introduces TuneNSearch, a hybrid transfer learning and local
search approach for addressing different variants of vehicle routing problems
(VRP). Recently, multi-task learning has gained much attention for solving VRP
variants. However, this adaptability often compromises the performance of the
models. To address this challenge, we first pre-train a reinforcement learning
model on the multi-depot VRP, followed by a short fine-tuning phase to adapt it
to different variants. By leveraging the complexity of the multi-depot VRP, the
pre-trained model learns richer node representations and gains more
transferable knowledge compared to models trained on simpler routing problems,
such as the traveling salesman problem. TuneNSearch employs, in the first
stage, a Transformer-based architecture, augmented with a residual edge-graph
attention network to capture the impact of edge distances and residual
connections between layers. This architecture allows for a more precise capture
of graph-structured data, improving the encoding of VRP's features. After
inference, our model is also coupled with a second stage composed of a local
search algorithm, which yields substantial performance gains with minimal
computational overhead added. Results show that TuneNSearch outperforms many
existing state-of-the-art models trained for each VRP variant, requiring only
one-fifth of the training epochs. Our approach demonstrates strong
generalization, achieving high performance across different tasks,
distributions and problem sizes, thus addressing a long-standing gap in the
literature.",['cs.LG'],False,,,,"AIRCHITECT v2: Learning the Hardware Accelerator Design Space through
  Unified Representations","TuneNSearch: a hybrid transfer learning and local search approach for
  solving vehicle routing problems"
neg-d2-617,2025-02-13,,2502.09016," We study the collective modes of an atomic bright soliton realised in a
quasi-one-dimensional Bose-Einstein condensate, using Bogoliubov-de Gennes
theory. In particular we focus on the breathing mode of the soliton, which is
not a single linearized normal mode but a common component of many modes, and
therefore decays within a $t^{-1/2}$ envelope due to dispersion. If the soliton
is held in the center of a harmonic trap, we show that the breathing amplitude
revives periodically, as atoms shed from the vibrating soliton oscillate in the
trap, and return. After each revival the breathing amplitude again decays, and
this cycle repeats every trap half-period. The amplitude envelope of these
breathing revivals shows a curious asymmetry, however, with a gradual increase
in breathing followed by sudden drop in breathing amplitude that becomes more
and more pronounced in later revivals. We explain this asymmetrical revival
pattern by deriving a close analytical approximation to the Bogoliubov-de
Gennes frequency spectrum, and offer this coherent Bogoliubov-de Gennes
phenomenon as a background against which to compare possible quantum many-body
effects, including decoherence over trap-period time scales.","['cond-mat.quant-gas', 'quant-ph']",2501.10221," We model human activity scheduling behaviour using a deep generative machine
learning approach. Activity schedules, which represent the activities and
associated travel behaviours of individuals, are a core component of many
applied models in the transport, energy and epidemiology domains. Our data
driven approach learns human preferences and scheduling logic without the need
for complex interacting combinations of sub-models and custom-rules, this makes
our approach significantly faster and simpler to operate that existing
approaches. We find activity schedule data combines aspects of both continuous
image data and also discrete text data, requiring novel approaches. We
additionally contribute a novel schedule representation and comprehensive
evaluation framework for generated schedules. Evaluation shows our approach is
able to rapidly generate large, diverse and realistic synthetic samples of
activity schedules.",['cs.LG'],False,,,,"Soliton resuscitations: asymmetric revivals of the breathing mode of an
  atomic bright soliton in a harmonic trap","Modelling Activity Scheduling Behaviour with Deep Generative Machine
  Learning"
neg-d2-618,2025-03-18,,2503.14669," This paper presents a reinforcement learning-based neuroadaptive control
framework for robotic manipulators operating under deferred constraints. The
proposed approach improves traditional barrier Lyapunov functions by
introducing a smooth constraint enforcement mechanism that offers two key
advantages: (i) it minimizes control effort in unconstrained regions and
progressively increases it near constraints, improving energy efficiency, and
(ii) it enables gradual constraint activation through a prescribed-time
shifting function, allowing safe operation even when initial conditions violate
constraints. To address system uncertainties and improve adaptability, an
actor-critic reinforcement learning framework is employed. The critic network
estimates the value function, while the actor network learns an optimal control
policy in real time, enabling adaptive constraint handling without requiring
explicit system modeling. Lyapunov-based stability analysis guarantees the
boundedness of all closed-loop signals. The effectiveness of the proposed
method is validated through numerical simulations.","['cs.RO', 'cs.SY', 'eess.SY']",2503.10174," This paper presents a novel sensitivity-based distributed programming (SBDP)
approach for non-convex, large-scale nonlinear programs (NLP). The algorithm
relies on first-order sensitivities to cooperatively solve the central NLP in a
distributed manner with only neighbor-to-neighbor communication and
parallelizable local computations. The scheme is based on primal decomposition
and offers minimal algorithmic complexity. We derive sufficient local
convergence conditions for non-convex problems. Furthermore, we consider the
SBDP method in a distributed optimal control context and derive favorable
convergence properties in this setting. We illustrate these theoretical
findings and the performance of the proposed algorithm with simulations of
various distributed optimization and control problems.",['math.OC'],False,,,,"Reinforcement Learning-Based Neuroadaptive Control of Robotic
  Manipulators under Deferred Constraints",Sensitivity-Based Distributed Programming for Non-Convex Optimization
neg-d2-619,2025-01-27,,2501.18627," We present a fast and simple technique to convert images into an emissive
surface-based scene representation. Building on existing emissive volume
reconstruction algorithms, we introduce a subtle yet impactful modification of
the loss function requiring changes to only a few lines of code: instead of
integrating the radiance field along rays and supervising the resulting images,
we project the training images into the scene to directly supervise the
spatio-directional radiance field.
  The primary outcome of this change is the complete removal of alpha blending
and ray marching from the image formation model, instead moving these steps
into the loss computation. In addition to promoting convergence to surfaces,
this formulation assigns explicit semantic meaning to 2D subsets of the
radiance field, turning them into well-defined emissive surfaces. We finally
extract a level set from this representation, which results in a high-quality
emissive surface model.
  Our method retains much of the speed and quality of the baseline algorithm.
For instance, a suitably modified variant of Instant~NGP maintains comparable
computational efficiency, while achieving an average PSNR that is only 0.1 dB
lower. Most importantly, our method generates explicit surfaces in place of an
exponential volume, doing so with a level of simplicity not seen in prior work.","['cs.GR', 'cs.CV']",2501.07166," Combinatorial medication recommendation(CMR) is a fundamental task of
healthcare, which offers opportunities for clinical physicians to provide more
precise prescriptions for patients with intricate health conditions,
particularly in the scenarios of long-term medical care. Previous research
efforts have sought to extract meaningful information from electronic health
records (EHRs) to facilitate combinatorial medication recommendations. Existing
learning-based approaches further consider the chemical structures of
medications, but ignore the textual medication descriptions in which the
functionalities are clearly described. Furthermore, the textual knowledge
derived from the EHRs of patients remains largely underutilized. To address
these issues, we introduce the Natural Language-Assisted Multi-modal Medication
Recommendation(NLA-MMR), a multi-modal alignment framework designed to learn
knowledge from the patient view and medication view jointly. Specifically,
NLA-MMR formulates CMR as an alignment problem from patient and medication
modalities. In this vein, we employ pretrained language models(PLMs) to extract
in-domain knowledge regarding patients and medications, serving as the
foundational representation for both modalities. In the medication modality, we
exploit both chemical structures and textual descriptions to create medication
representations. In the patient modality, we generate the patient
representations based on textual descriptions of diagnosis, procedure, and
symptom. Extensive experiments conducted on three publicly accessible datasets
demonstrate that NLA-MMR achieves new state-of-the-art performance, with a
notable average improvement of 4.72% in Jaccard score. Our source code is
publicly available on https://github.com/jtan1102/NLA-MMR_CIKM_2024.",['cs.AI'],False,,,,"A Radiance Field Loss for Fast and Simple Emissive Surface
  Reconstruction",Natural Language-Assisted Multi-modal Medication Recommendation
neg-d2-620,2025-02-09,,2502.05908," In image processing, solving inverse problems is the task of finding
plausible reconstructions of an image that was corrupted by some (usually
known) degradation model. Commonly, this process is done using a generative
image model that can guide the reconstruction towards solutions that appear
natural. The success of diffusion models over the last few years has made them
a leading candidate for this task. However, the sequential nature of diffusion
models makes this conditional sampling process challenging. Furthermore, since
diffusion models are often defined in the latent space of an autoencoder, the
encoder-decoder transformations introduce additional difficulties. Here, we
suggest a novel sampling method based on sequential Monte Carlo (SMC) in the
latent space of diffusion models. We use the forward process of the diffusion
model to add additional auxiliary observations and then perform an SMC sampling
as part of the backward process. Empirical evaluations on ImageNet and FFHQ
show the benefits of our approach over competing methods on various inverse
problem tasks.","['eess.IV', 'cs.CV', 'cs.LG']",2501.15786," The authors present their research on chocolate games with a pass-move.
Chocolate games are generalizations of Nim. In this work, we modify the
standard rules of the game to allow a one-time pass; that is, a pass-move may
be used at most once in the game, but not from a terminal position. Once the
pass has been used by either player, it is no longer available. In the case of
the classical three-pile nim with $x,y,z$ for the number of stones of each
pile, the previous player wins when the ``exclusive or'' of $x,y,z$ is $0$, and
its Grundy number is calculated as ``exclusive or'' of $x,y,z$. However, no
mathematical formula is known for the previous player's winning position when a
pass-move is allowed. In this study, we show a theorem to determine the Grundy
number of a position with a pass where the position can be considered as a
disjunctive sum of two positions which have a special property. By using the
theorem, we show closed formulas for positions which have Grundy numbers $0,
1,$ and $2$ for some chocolate games with a pass.",['math.CO'],False,,,,Inverse Problem Sampling in Latent Space Using Sequential Monte Carlo,Chocolate Games with a Pass
neg-d2-621,2025-02-18,,2502.13199," GitHub Copilot is transforming software development by automating tasks and
boosting productivity through AI-driven code generation. In this paper, we
con-duct a literature survey to synthesize insights on Copilot's impact on
productivity and security. We review academic journal databases, industry
reports, and official docu-mentation to highlight key findings and challenges.
While Copilot accelerates coding and prototyping, concerns over security
vulnerabilities and intellectual property risks persist. Drawing from the
literature, we provide a perspective on best practices and future directions
for responsible AI adoption in software engineering, offering action-able
insights for developers and organizations to integrate Copilot effectively
while maintaining high standards of quality and security.","['cs.SE', 'cs.AI']",2502.10222," If $A \colon D(A) \subset \mathcal{H} \to \mathcal{H}$ is an unbounded
Fredholm operator of index $0$ on a Hilbert space $\mathcal{H}$ with a dense
domain $D(A)$, then its spectrum is either discrete or the entire complex
plane. This spectral dichotomy plays a central role in the study of
\textit{magic angles} in twisted bilayer graphene.
  This paper proves that if such operators (with certain additional
assumptions) are perturbed by certain random trace-class operators, their
spectrum is discrete with high probability.","['math.SP', 'math-ph', 'math.MP']",False,,,,"The Role of GitHub Copilot on Software Development: A Perspec-tive on
  Productivity, Security, Best Practices and Future Directions",Spectral Instability of Random Fredholm Operators
neg-d2-622,2025-03-22,,2503.17813," Current frameworks for evaluating security bug severity, such as the Common
Vulnerability Scoring System (CVSS), prioritize the ratio of exploitability to
impact. This paper suggests that the above approach measures the ""known knowns""
but inadequately addresses the ""known unknowns"" especially when there exist
multiple possible exploit paths and side effects, which introduce significant
uncertainty. This paper introduces the concept of connectedness, which measures
how strongly a security bug is connected with different entities, thereby
reflecting the uncertainty of impact and the exploit potential. This work
highlights the critical but underappreciated role connectedness plays in
severity assessments.",['cs.CR'],2502.17776," Tip-of-the-tongue (TOT) search occurs when a user struggles to recall a
specific identifier, such as a document title. While common, existing search
systems often fail to effectively support TOT scenarios. Research on TOT
retrieval is further constrained by the challenge of collecting queries, as
current approaches rely heavily on community question-answering (CQA) websites,
leading to labor-intensive evaluation and domain bias. To overcome these
limitations, we introduce two methods for eliciting TOT queries - leveraging
large language models (LLMs) and human participants - to facilitate simulated
evaluations of TOT retrieval systems. Our LLM-based TOT user simulator
generates synthetic TOT queries at scale, achieving high correlations with how
CQA-based TOT queries rank TOT retrieval systems when tested in the Movie
domain. Additionally, these synthetic queries exhibit high linguistic
similarity to CQA-derived queries. For human-elicited queries, we developed an
interface that uses visual stimuli to place participants in a TOT state,
enabling the collection of natural queries. In the Movie domain, system rank
correlation and linguistic similarity analyses confirm that human-elicited
queries are both effective and closely resemble CQA-based queries. These
approaches reduce reliance on CQA-based data collection while expanding
coverage to underrepresented domains, such as Landmark and Person. LLM-elicited
queries for the Movie, Landmark, and Person domains have been released as test
queries in the TREC 2024 TOT track, with human-elicited queries scheduled for
inclusion in the TREC 2025 TOT track. Additionally, we provide source code for
synthetic query generation and the human query collection interface, along with
curated visual stimuli used for eliciting TOT queries.","['cs.IR', 'cs.CL', 'cs.HC']",False,,,,"Connectedness: a dimension of security bug severity assessment for
  measuring uncertainty",Tip of the Tongue Query Elicitation for Simulated Evaluation
neg-d2-623,2025-01-06,,2501.03116," We extend the classical Poincar\'e-Birkhoff-Witt theorem to higher algebra by
establishing a version that applies to spectral Lie algebras. We deduce this
statement from a basic relation between operads in spectra: the commutative
operad is the quotient of the associative operad by a right action of the
spectral Lie operad. This statement, in turn, is a consequence of a fundamental
relation between different $\mathbb{E}_n$-operads, which we articulate and
prove. We deduce a variant of the Poincar\'{e}--Birkhoff--Witt theorem for
relative enveloping algebras of $\mathbb{E}_n$-algebras. Our methods also give
a simple construction and description of the higher enveloping
$\mathbb{E}_n$-algebras of a spectral Lie algebra.","['math.AT', 'math.CT', 'math.RT']",2503.14077," The radiative open circuit voltage loss in a solar cell occurs because the
absorptance spectrum near the band gap shows gradual increase rather than sharp
step function like transition. This broadening effect has been attributed to
band gap fluctuations and or to Urbach tails. In this report, we use modelling
based on Planck s generalized law to distinguish between these two effects. Our
results demonstrate that Urbach tails have only a minimal effect on the
absorptance edge broadening and clarify that even an ideal direct semiconductor
with no band gap fluctuations shows broadening at the absorptance onset.
Furthermore, state of the art inorganic thin film solar cells often incorporate
a band gap gradient across their thickness, which can further contribute to
absorptance broadening. Using Cu(In,Ga)Se2 (CIGSe) absorbers as a case study,
we perform a comprehensive analysis of voltage losses through absolute
photoluminescence and electroluminescence spectroscopy, combined with
photospectrometry and high-spatial-resolution cathodoluminescence measurements.
We find that the loss analysis based on the combination of radiative,
generation and non-radiative losses is complete. Samples with a graded band gap
profile show more pronounced broadening of the absorptance onset and up to 16
mV higher radiative losses compared to the samples with uniform band gap. There
is indication, that band gap-graded samples also have larger lateral band gap
inhomogeneity.",['cond-mat.mtrl-sci'],False,,,,Poincar\'{e}-Birkhoff-Witt Theorems in Higher Algebra,"The effect of a band gap gradient on the radiative losses in the open
  circuit voltage of solar cells"
neg-d2-624,2025-03-19,,2503.15196," Ultra-short Period exoplanets (USPs) like 55 Cnc e, hosting dayside magma
oceans, present unique opportunities to study surface-atmosphere interactions.
The composition of a vaporised mineral atmosphere enveloping the dayside is
dictated by that of the surface magma ocean, which in turn is sensitive to its
oxygen fugacity ($f$O$_2$). Observability estimations and characterisation of
the atmospheric emission of 55 Cnc e have mostly remained limited to low
spectral resolution space-based studies. Here, we aim to examine ground-based
high-resolution observabilities of a diverse set of mineral atmospheres
produced across a grid of mantle $f$O$_2$s varying over 12 orders of magnitude.
We assume a Bulk Silicate Earth mantle composition and a substellar dayside
temperature of T = 2500K in the near infrared wavelength (NIR) region. This
spectral range is often featureless for this class of atmospheres at
low-resolution. Coupling our newly developed simulator for synthesising
realistic observations from high-resolution ground-based spectrographs (Ratri)
to a pre-developed high-resolution cross-correlation spectroscopy (HRCCS)
analysis pipeline (Upamana), we find that this array of mineral atmospheres
would all be detectable with 11 hours of observing time of the dayside of 55
Cnc e with CARMENES and each individual scenario can be correctly
differentiated within 1$\sigma$. Our analysis is readily able to distinguish
between a planet with an Earth-like redox state (with $f$O$_2$ $\sim$3.5
log$_{10}$ units above the iron-w\""ustite, IW buffer) from a Mercury-like
planet ($f$O$_2$ $\sim$5 log$_{10}$ units below IW). We thus conclude that the
HRCCS technique holds promise for cataloguing the diversity of redox states
among the rocky exoplanetary population.",['astro-ph.EP'],2502.0995," Duminil-Copin and Manolescu (2022) recently proved the scaling relations for
planar Fortuin-Kasteleyn (FK) percolation. In particular, they showed that the
one-arm exponent and the mixing rate exponent are sufficient to derive the
other near-critical exponents. The scaling limit of critical FK percolation is
conjectured to be a conformally invariant random collection of loops called the
conformal loop ensemble (CLE). In this paper, we define the CLE analog of the
mixing rate exponent. Assuming the convergence of FK percolation to CLE, we
show that the mixing rate exponent for FK percolation agrees with that of CLE.
We prove that the CLE$_\kappa$ mixing rate exponent equals $\frac{3
\kappa}{8}-1$, thereby answering Question 3 of Duminil-Copin and Manolescu
(2022). The derivation of the CLE exponent is based on an exact formula for the
Radon-Nikodym derivative between the marginal laws of the odd-level and
even-level CLE loops, which is obtained from the coupling between Liouville
quantum gravity and CLE.","['math.PR', 'math-ph', 'math.MP']",False,,,,"Detectability of oxygen fugacity regimes in the magma ocean world 55
  Cancri e at high spectral resolution",Mixing rate exponent of planar Fortuin-Kasteleyn percolation
neg-d2-625,2025-02-07,,2502.05388," We present a modular DNA origami design approach to address the challenges of
assembling geometrically complex nanoscale structures, including those with
nonuniform Gaussian curvature. This approach features a core structure that
completely conserves the scaffold routing across different designs and
preserves more than 70% of the DNA staples between designs, dramatically
reducing both cost and effort, while enabling precise and independent
programming of subunit interactions and binding angles through adjustable
overhang lengths and sequences. Using cryogenic electron microscopy, gel
electrophoresis, and coarse-grained molecular dynamics simulations, we validate
a set of robust design rules. We demonstrate the method's utility by assembling
a variety of self-limiting structures, including anisotropic shells with
controlled inter-subunit interactions and curvature, and a toroid with globally
varying curvature. Our strategy is both cost-effective and versatile, providing
a promising and efficient solution for the synthetic fabrication of complex
nanostructures.",['cond-mat.soft'],2503.1702," Quantum kernels quantify similarity between data points by measuring the
inner product between quantum states, computed through quantum circuit
measurements. By embedding data into quantum systems, quantum kernel feature
maps, that may be classically intractable to compute, could efficiently exploit
high-dimensional Hilbert spaces to capture complex patterns. However, designing
effective quantum feature maps remains a major challenge. Many quantum kernels,
such as the fidelity kernel, suffer from exponential concentration, leading to
near-identity kernel matrices that fail to capture meaningful data correlations
and lead to overfitting and poor generalization. In this paper, we propose a
novel strategy for constructing quantum kernels that achieve good
generalization performance, drawing inspiration from benign overfitting in
classical machine learning. Our approach introduces the concept of local-global
quantum kernels, which combine two complementary components: a local quantum
kernel based on measurements of small subsystems and a global quantum kernel
derived from full-system measurements. Through numerical experiments, we
demonstrate that local-global quantum kernels exhibit benign overfitting,
supporting the effectiveness of our approach in enhancing quantum kernel
methods.","['quant-ph', 'cs.LG', 'stat.ML']",False,,,,"Modular programming of interaction and geometric specificity enables
  assembly of complex DNA origami nanostructures",Benign Overfitting with Quantum Kernels
neg-d2-626,2025-03-04,,2503.03077," Testing aerial robots in tasks such as pickup-and-delivery and surveillance
significantly benefits from high energy efficiency and scalability of the
deployed robotic system. This paper presents MochiSwarm, an open-source testbed
of light-weight robotic blimps, ready for multi-robot operation without
external localization. We introduce the system design in hardware, software,
and perception, which capitalizes on modularity, low cost, and light weight.
The hardware allows for rapid modification, which enables the integration of
additional sensors to enhance autonomy for different scenarios. The software
framework supports different actuation models and communication between the
base station and multiple blimps. The detachable perception module allows
independent blimps to perform tasks that involve detection and autonomous
actuation. We showcase a differential-drive module as an example, of which the
autonomy is enabled by visual servoing using the perception module. A case
study of pickup-and-delivery tasks with up to 12 blimps highlights the autonomy
of the MochiSwarm without external infrastructures.",['cs.RO'],2503.1656," Alzheimer's disease and related dementias (AD/ADRD) represent a growing
healthcare crisis affecting over 6 million Americans. While genetic factors
play a crucial role, emerging research reveals that social determinants of
health (SDOH) significantly influence both the risk and progression of
cognitive functioning, such as cognitive scores and cognitive decline. This
report examines how these social, environmental, and structural factors impact
cognitive health trajectories, with a particular focus on Hispanic populations,
who face disproportionate risk for AD/ADRD. Using data from the Mexican Health
and Aging Study (MHAS) and its cognitive assessment sub study (Mex-Cog), we
employed ensemble of regression trees models to predict 4-year and 9-year
cognitive scores and cognitive decline based on SDOH. This approach identified
key predictive SDOH factors to inform potential multilevel interventions to
address cognitive health disparities in this population.","['q-bio.QM', 'cs.LG', 'stat.AP']",False,,,,MochiSwarm: A testbed for robotic blimps in realistic environments,"Early Prediction of Alzheimer's and Related Dementias: A Machine
  Learning Approach Utilizing Social Determinants of Health Data"
neg-d2-627,2025-01-23,,2501.13676," Text classifiers suffer from small perturbations, that if chosen
adversarially, can dramatically change the output of the model. Verification
methods can provide robustness certificates against such adversarial
perturbations, by computing a sound lower bound on the robust accuracy.
Nevertheless, existing verification methods incur in prohibitive costs and
cannot practically handle Levenshtein distance constraints. We propose the
first method for computing the Lipschitz constant of convolutional classifiers
with respect to the Levenshtein distance. We use these Lipschitz constant
estimates for training 1-Lipschitz classifiers. This enables computing the
certified radius of a classifier in a single forward pass. Our method, LipsLev,
is able to obtain $38.80$% and $13.93$% verified accuracy at distance $1$ and
$2$ respectively in the AG-News dataset, while being $4$ orders of magnitude
faster than existing approaches. We believe our work can open the door to more
efficient verification in the text domain.","['cs.LG', 'cs.AI', 'cs.CL']",2501.05015," Adversarial attacks are allegedly unnoticeable. Prior studies have designed
attack noticeability measures on graphs, primarily using statistical tests to
compare the topology of original and (possibly) attacked graphs. However, we
observe two critical limitations in the existing measures. First, because the
measures rely on simple rules, attackers can readily enhance their attacks to
bypass them, reducing their attack ""noticeability"" and, yet, maintaining their
attack performance. Second, because the measures naively leverage global
statistics, such as degree distributions, they may entirely overlook attacks
until severe perturbations occur, letting the attacks be almost ""totally
unnoticeable."" To address the limitations, we introduce HideNSeek, a learnable
measure for graph attack noticeability. First, to mitigate the bypass problem,
HideNSeek learns to distinguish the original and (potential) attack edges using
a learnable edge scorer (LEO), which scores each edge on its likelihood of
being an attack. Second, to mitigate the overlooking problem, HideNSeek
conducts imbalance-aware aggregation of all the edge scores to obtain the final
noticeability score. Using six real-world graphs, we empirically demonstrate
that HideNSeek effectively alleviates the observed limitations, and LEO (i.e.,
our learnable edge scorer) outperforms eleven competitors in distinguishing
attack edges under five different attack methods. For an additional
application, we show that LEO boost the performance of robust GNNs by removing
attack-like edges.","['cs.LG', 'cs.AI']",False,,,,Certified Robustness Under Bounded Levenshtein Distance,"On Measuring Unnoticeability of Graph Adversarial Attacks: Observations,
  New Measure, and Applications"
neg-d2-628,2025-03-19,,2503.15171," Electric double layer (EDL) formation underlies the functioning of
supercapacitors and several other electrochemical technologies. Here, we study
how the EDL formation near two flat blocking electrodes separated by $2L$ is
affected by beyond-mean-field Coulombic interactions, which can be substantial
for electrolytes of high salt concentration or with multivalent ions. Our model
combines the Nernst-Planck and Bazant-Storey-Kornyshev (BSK) equations; the
latter is a modified Poisson equation with a correlation length $\ell_c$. In
response to a voltage step, the system charges exponentially with a
characteristic timescale $\tau$ that depends nonmonotonically on $\ell_c$. For
small $\ell_c$, $\tau$ is given by the BSK capacitance times a dilute
electrolyte's resistance, in line with [Zhao, Phys. Rev. E 84, 051504 (2011)];
here, $\tau$ decreases with increasing $\ell_c$. Increasing the correlation
length beyond $\ell_c\approx L^{2/3}\lambda_D^{1/3}$, with $\lambda_D$ the
Debye length, $\tau$ reaches a minimum, rises as $\tau\propto
\lambda_D\ell_c/D$, and plateaus at $\tau=4L^2/(\pi^2 D)$. Our results imply
that strongly correlated, strongly confined electrolytes - ionic liquids in the
surface force balance apparatus, say - move slower than predicted so far.","['physics.chem-ph', 'cond-mat.soft']",2502.00447," One of the most often used methods of summing divergent series in physics is
the Borel-type summation with control parameters improving convergence, which
are defined by some optimization conditions. The well known annoying problem in
this procedure is the occurrence of multiple solutions for control parameters.
We suggest a method for resolving this problem, based on the minimization of
cost functional. Control parameters can be introduced by employing the
Borel-Leroy or Mittag-Leffler transforms. Also, two novel transformations are
proposed using fractional integrals and fractional derivatives. New cost
functionals are advanced, based on lasso and ridge selection criteria, and
their performance is studied for a number of models. The developed method is
shown to provide good accuracy for the calculated quantities.","['math-ph', 'cond-mat.stat-mech', 'hep-ph', 'math.MP']",False,,,,"Charging dynamics of electric double layer capacitors including
  beyond-mean-field electrostatic correlations","Resolving the Problem of Multiple Control Parameters in Optimized
  Borel-Type Summation"
neg-d2-629,2025-01-23,,2501.14039," As the atomistic simulations of materials science move from traditional
potentials to machine learning interatomic potential (MLIP), the field is
entering the second phase focused on discovering and explaining new material
phenomena. While MLIP development relies on curated data and flexible datasets
from ab-initio simulations, transitioning seamlessly between ab-initio
workflows and MLIP frameworks remains challenging. A global survey was
conducted to understand the current standing (progress and bottleneck) of the
machine learning-guided materials science research. The survey responses have
been implemented to design an open-source software to reduce the access
barriers of MLIP models for the global scientific community. Here, we present
AtomProNet, an open-source Python package that automates obtaining atomic
structures, prepares and submits ab-initio jobs, and efficiently collects
batch-processed data for streamlined neural network (NN) training. Finally, we
compared empirical and start-of-the-art machine learning potential, showing the
practicality of using MLIPs based on computational time and resources.",['cond-mat.mtrl-sci'],2502.08654," Entropy and its various generalizations are important in many fields,
including mathematical statistics, communication theory, physics and computer
science, for characterizing the amount of information associated with a
probability distribution. In this paper we propose goodness-of-fit statistics
for the multivariate Student and multivariate Pearson type II distributions,
based on the maximum entropy principle and a class of estimators for Renyi
entropy based on nearest neighbour distances. We prove the L2-consistency of
these statistics using results on the subadditivity of Euclidean functionals on
nearest neighbour graphs, and investigate their rate of convergence and
asymptotic distribution using Monte Carlo methods.","['stat.ME', 'math.ST', 'stat.TH']",False,,,,"AtomProNet: Data flow to and from machine learning interatomic
  potentials in materials science",Statistical tests based on Renyi entropy estimation
neg-d2-630,2025-03-03,,2503.01474," Interactive navigation is crucial in scenarios where proactively interacting
with objects can yield shorter paths, thus significantly improving traversal
efficiency. Existing methods primarily focus on using the robot body to
relocate large obstacles (which could be comparable to the size of a robot).
However, they prove ineffective in narrow or constrained spaces where the
robot's dimensions restrict its manipulation capabilities. This paper
introduces a novel interactive navigation framework for legged manipulators,
featuring an active arm-pushing mechanism that enables the robot to reposition
movable obstacles in space-constrained environments. To this end, we develop a
reinforcement learning-based arm-pushing controller with a two-stage reward
strategy for large-object manipulation. Specifically, this strategy first
directs the manipulator to a designated pushing zone to achieve a kinematically
feasible contact configuration. Then, the end effector is guided to maintain
its position at appropriate contact points for stable object displacement while
preventing toppling. The simulations validate the robustness of the arm-pushing
controller, showing that the two-stage reward strategy improves policy
convergence and long-term performance. Real-world experiments further
demonstrate the effectiveness of the proposed navigation framework, which
achieves shorter paths and reduced traversal time. The open-source project can
be found at
https://github.com/Zhihaibi/Interactive-Navigation-for-legged-manipulator.git.",['cs.RO'],2502.06209," This paper introduces a cost-efficient active learning (AL) framework for
classification, featuring a novel query design called candidate set query.
Unlike traditional AL queries requiring the oracle to examine all possible
classes, our method narrows down the set of candidate classes likely to include
the ground-truth class, significantly reducing the search space and labeling
cost. Moreover, we leverage conformal prediction to dynamically generate small
yet reliable candidate sets, adapting to model enhancement over successive AL
rounds. To this end, we introduce an acquisition function designed to
prioritize data points that offer high information gain at lower cost.
Empirical evaluations on CIFAR-10, CIFAR-100, and ImageNet64x64 demonstrate the
effectiveness and scalability of our framework. Notably, it reduces labeling
cost by 42% on ImageNet64x64.","['cs.LG', 'cs.CV']",False,,,,"Interactive Navigation for Legged Manipulators with Learned Arm-Pushing
  Controller",Enhancing Cost Efficiency in Active Learning with Candidate Set Query
neg-d2-631,2025-02-27,,2502.19921," Deep learning models lack shift invariance, making them sensitive to input
shifts that cause changes in output. While recent techniques seek to address
this for images, our findings show that these approaches fail to provide
shift-invariance in time series, where the data generation mechanism is more
challenging due to the interaction of low and high frequencies. Worse, they
also decrease performance across several tasks. In this paper, we propose a
novel differentiable bijective function that maps samples from their
high-dimensional data manifold to another manifold of the same dimension,
without any dimensional reduction. Our approach guarantees that samples -- when
subjected to random shifts -- are mapped to a unique point in the manifold
while preserving all task-relevant information without loss. We theoretically
and empirically demonstrate that the proposed transformation guarantees
shift-invariance in deep learning models without imposing any limits to the
shift. Our experiments on six time series tasks with state-of-the-art methods
show that our approach consistently improves the performance while enabling
models to achieve complete shift-invariance without modifying or imposing
restrictions on the model's topology. The source code is available on
\href{https://github.com/eth-siplab/Shifting-the-Paradigm}{GitHub}.",['cs.LG'],2501.04866," M dwarfs are the most common stars in the galaxy, with long lifespans, a high
occurrence rate of rocky planets, and close-in habitable zones. However, high
stellar activity in the form of frequent flaring and any associated coronal
mass ejections may drive atmospheric escape with the bombardment of radiation
and high-energy particles, drastically impacting the habitability of these
systems. The stellar latitude where flares and coronal mass ejections occur
determines the space weather that exoplanets are subject to, with high-energy
particle events associated with equatorial flares producing significant
atmospheric erosion. However, the flaring latitudes for M dwarfs remain largely
unconstrained. To aid in the effort to locate these flaring regions we explore
the applicability of flare occultations using optical photometry to identify
the latitudes of flares. As a planet transits in front of an ongoing flare the
timing and geometry of the transit can be used to constrain the latitude and
longitude of the flare. We predict the probability of detecting an occultation
for known transiting planets and eclipsing binaries. From this, we estimate
3-22 detectable occultations exist within the TESS primary mission photometry,
with the majority occurring in eclipsing binary observations. To demonstrate
this technique, we analyze a candidate flare occultation event for the
eclipsing binary CM Draconis.","['astro-ph.EP', 'astro-ph.SR']",False,,,,"Shifting the Paradigm: A Diffeomorphism Between Time Series Data
  Manifolds for Achieving Shift-Invariancy in Deep Learning",Identifying Flare Locations Through Exoplanet Transit Occultations
neg-d2-632,2025-03-15,,2503.12178," The research investigates the effects of public spending on health and
education in shaping the human development in south Asian three countries:
India, Pakistan and Bangladesh. The study uses the VAR (Vector Auto regression)
model to estimate the effects on government spending on these sectors to
evaluate the human development. The findings state that there are different
degrees of impact in these three countries. In Bangladesh and India, health
spending increases the human development in short term. On the other hand
education spending shows the significance on the HDI.Moreover, the study also
highlights that there are different levels of effectiveness of government
spending across these three countries. In order to maximize the human
development an optimum country specific strategies should be adopted.","['econ.GN', 'q-fin.EC']",2502.13384," The derivative of a polynomial with all zeros on the unit circle has the
zeros of its derivative on or inside the unit circle. It has been observed that
in many cases the zeros of the derivative have a bimodal distribution: there
are two smaller circles near which it is more likely to find those zeros. We
identify the likely source of the second mode. This idea is supported with
numerical examples involving the characteristic polynomials of random unitary
matrices.","['math.CV', 'math.NT']",False,,,,"Public Sector Efficiency in Delivering Social Services and Its Impact on
  Human Development: A Comparative Study of Healthcare and Education Spending
  in India, Pakistan, and Bangladesh",The bimodal distribution in the derivative of unitary polynomials
neg-d2-633,2025-02-20,,2502.14344," Binary Spiking Neural Networks (BSNNs) inherit the eventdriven paradigm of
SNNs, while also adopting the reduced storage burden of binarization
techniques. These distinct advantages grant BSNNs lightweight and
energy-efficient characteristics, rendering them ideal for deployment on
resource-constrained edge devices. However, due to the binary synaptic weights
and non-differentiable spike function, effectively training BSNNs remains an
open question. In this paper, we conduct an in-depth analysis of the challenge
for BSNN learning, namely the frequent weight sign flipping problem. To
mitigate this issue, we propose an Adaptive Gradient Modulation Mechanism
(AGMM), which is designed to reduce the frequency of weight sign flipping by
adaptively adjusting the gradients during the learning process. The proposed
AGMM can enable BSNNs to achieve faster convergence speed and higher accuracy,
effectively narrowing the gap between BSNNs and their full-precision
equivalents. We validate AGMM on both static and neuromorphic datasets, and
results indicate that it achieves state-of-the-art results among BSNNs. This
work substantially reduces storage demands and enhances SNNs' inherent energy
efficiency, making them highly feasible for resource-constrained environments.",['cs.CV'],2503.13533," As artificial intelligence (AI) technology becomes increasingly prevalent in
the filed of education, there is a growing need for mathematics teacher
education students (MTES) to demonstrate proficiency in the integration of AI
with the technological pedagogical content knowledge (AI-TPACK). To study the
issue, we firstly devised an systematic AI-TPACK scale and test on 412 MTES
from seven universities. Through descriptive statistical analyses, we found
that the current status of AI-TPACK for MTES in China is at a basic,
preliminary stage. Secondly, we compared MTES between three different grades on
the six variables and found that there is no discernible difference, which
suggested that graduate studies were observed to have no promotion in the
development of AI-TPACK competencies. Thirdly, we proposed a new AI-TPACK
structural equation model (AI-TPACK-SEM) to explore the impact of self-efficacy
and teaching beliefs on AI-TPACK. Our findings indicate a positive correlation
between self-efficacy and AI-TPACK. We also come to a conclusion that may be
contrary to common perception, excessive teaching beliefs may impede the
advancement of AI-TPACK. Overall, this paper revealed the current status of
AI-TPACK for MTES in China for the first time, designed a dedicated SEM to
study the effect of specific factors on AI-TPACK, and proposed some suggestions
on future developments.","['cs.CY', 'cs.AI']",False,,,,"Towards Accurate Binary Spiking Neural Networks: Learning with Adaptive
  Gradient Modulation Mechanism","The Status Quo and Future of AI-TPACK for Mathematics Teacher Education
  Students: A Case Study in Chinese Universities"
neg-d2-634,2025-02-16,,2502.11071," The paper gives a bound on the generalization error of the Gibbs algorithm,
which recovers known data-independent bounds for the high temperature range and
extends to the low-temperature range, where generalization depends critically
on the data-dependent loss-landscape. It is shown, that with high probability
the generalization error of a single hypothesis drawn from the Gibbs posterior
decreases with the total prior volume of all hypotheses with similar or smaller
empirical error. This gives theoretical support to the belief in the benefit of
flat minima. The zero temperature limit is discussed and the bound is extended
to a class of similar stochastic algorithms.","['cs.LG', 'stat.ML']",2501.18709," The time-modulated array (TMA) is a simple array architecture in which each
antenna is connected via a multi-throw switch. The switch acts as a modulator
switching state faster than the symbol rate. The phase shifting and beamforming
is achieved by a cyclic shift of the periodical modulating signal across
antennas. In this paper, the TMA mode of operation is proposed to improve the
resolution of a conventional phase shifter. The TMAs are analyzed under
constrained switching frequency being a small multiple of the symbol rate. The
presented generic signal model gives insight into the magnitude, phase and
spacing of the harmonic components generated by the quantized modulating
sequence. It is shown that the effective phase-shifting resolution can be
improved multiplicatively by the oversampling factor ($O$) at the cost of
introducing harmonics. Finally, the array tapering with an oversampled
modulating signal is proposed. The oversampling provides $O+1$ uniformly
distributed tapering amplitudes.",['eess.SP'],False,,,,"Generalization of the Gibbs algorithm with high probability at low
  temperatures",Beamforming with Oversampled Time-Modulated Arrays
neg-d2-635,2025-02-17,,2502.12408," Recent advances in speech foundation models are largely driven by scaling
both model size and data, enabling them to perform a wide range of tasks,
including speech recognition. Traditionally, ASR models are evaluated using
metrics like Word Error Rate (WER) and Character Error Rate (CER), which depend
on ground truth labels. As a result of limited labeled data from diverse
domains and testing conditions, the true generalization capabilities of these
models beyond standard benchmarks remain unclear. Moreover, labeling data is
both costly and time-consuming. To address this, we propose a novel label-free
approach for approximating ASR performance metrics, eliminating the need for
ground truth labels. Our method utilizes multimodal embeddings in a unified
space for speech and transcription representations, combined with a
high-quality proxy model to compute proxy metrics. These features are used to
train a regression model to predict key ASR metrics like Word Error Rate (WER)
and Character Error Rate (CER). We experiment with over 40 models across 14
datasets representing both standard and in-the-wild testing conditions. Our
results show that we approximate the metrics within a single-digit absolute
difference across all experimental configurations, outperforming the most
recent baseline by more than 50\%.",['cs.CL'],2502.17101," The continuous effort in making artificial neural networks more alike to
human brain calls for the hardware elements to implement biological
synapse-like functionalities. The recent experimental demonstration of
ferroelectric-like FETs promises low-power operation as compared to the
conventional ferroelectric switching devices. This work presents an in-house
numerical tool, which self-consistently solves the electrostatics and
time-dependent electronic and ionic transport. The tool is exploited to analyze
the effect that various physical parameters such as mobility and ion
concentration could have on the design of the ferroelectric-like FETs. Their
suitability in emulating different functions of the biological synapses is also
demonstrated.","['cond-mat.mtrl-sci', 'physics.comp-ph']",False,,,,On the Robust Approximation of ASR Metrics,"Numerical study of synaptic behavior in amorphous HfO2-based
  ferroelectric-like FETs generated by voltage-driven ion migration"
neg-d2-636,2025-03-12,,2503.09594," Integrating large language models (LLMs) into autonomous driving has
attracted significant attention with the hope of improving generalization and
explainability. However, existing methods often focus on either driving or
vision-language understanding but achieving both high driving performance and
extensive language understanding remains challenging. In addition, the dominant
approach to tackle vision-language understanding is using visual question
answering. However, for autonomous driving, this is only useful if it is
aligned with the action space. Otherwise, the model's answers could be
inconsistent with its behavior. Therefore, we propose a model that can handle
three different tasks: (1) closed-loop driving, (2) vision-language
understanding, and (3) language-action alignment. Our model SimLingo is based
on a vision language model (VLM) and works using only camera, excluding
expensive sensors like LiDAR. SimLingo obtains state-of-the-art performance on
the widely used CARLA simulator on the Bench2Drive benchmark and is the winning
entry at the CARLA challenge 2024. Additionally, we achieve strong results in a
wide variety of language-related tasks while maintaining high driving
performance.","['cs.CV', 'cs.RO']",2501.08834," Billions of dollars are transacted through smart contracts, making
vulnerabilities a major financial risk. One focus in the security arms race is
on profitable vulnerabilities that attackers can exploit. Fuzzing is a key
method for identifying these vulnerabilities. However, current solutions face
two main limitations: a lack of profit-centric techniques for expediting
detection, and insufficient automation in maximizing the profitability of
discovered vulnerabilities, leaving the analysis to human experts. To address
these gaps, we have developed VERITE, a profit-centric smart contract fuzzing
framework that not only effectively detects those profitable vulnerabilities
but also maximizes the exploited profits.
  VERITE has three key features: 1) DeFi action-based mutators for boosting the
exploration of transactions with different fund flows; 2) potentially
profitable candidates identification criteria, which checks whether the input
has caused abnormal fund flow properties during testing; 3) a gradient
descent-based profit maximization strategy for these identified candidates.
  VERITE is fully developed from scratch and evaluated on a dataset consisting
of 61 exploited real-world DeFi projects with an average of over 1.1 million
dollars loss. The results show that VERITE can automatically extract more than
18 million dollars in total and is significantly better than state-of-the-art
fuzzer ITYFUZZ in both detection (29/10) and exploitation (134 times more
profits gained on average). Remarkably, in 12 targets, it gains more profits
than real-world attacking exploits (1.01 to 11.45 times more). VERITE is also
applied by auditors in contract auditing, where 6 (5 high severity) zero-day
vulnerabilities are found with over $2,500 bounty rewards.","['cs.CR', 'cs.SE']",False,,,,"SimLingo: Vision-Only Closed-Loop Autonomous Driving with
  Language-Action Alignment",Smart Contract Fuzzing Towards Profitable Vulnerabilities
neg-d2-637,2025-02-17,,2502.12485," To ensure safe usage, Large Language Models (LLMs) typically undergo
alignment with human-defined values. However, this alignment often relies on
primarily English data and is biased towards Western-centric values, limiting
its effectiveness in low-resource language settings. In this paper, we describe
our approach for aligning SEA-Lion-v2.1-Instruct (a Llama3-8B variant) to
minimize toxicity in Singlish, an English creole specific to Singapore. We find
that supervised fine-tuning and Kahneman-Tversky Optimization (KTO) on paired
and unpaired preferences is more sample efficient and yields significantly
better results than Direct Preference Optimization (DPO). Our analysis reveals
that DPO implicitly enforces a weaker safety objective than KTO, and that SFT
complements KTO by improving training stability. Finally, we introduce a simple
but novel modification to KTO, KTO-S, which improves training stability through
better gradient exploitation. Overall, we present a general approach for safety
alignment conducive to low-resource English languages, successfully reducing
toxicity by 99\% on our Singlish benchmark, with gains generalizing to the
broader TOXIGEN dataset while maintaining strong performance across standard
LLM benchmarks.","['cs.CL', 'cs.AI']",2503.06055," Recent approaches to the theory of dynamic programming view dynamic programs
as families of policy operators acting on partially ordered sets. In this
paper, we extend these ideas by shifting from arbitrary partially ordered sets
to ordered vector space. The advantage of working in this setting is that
ordered vector spaces have well integrated algebric and order structure, which
leads to sharper fixed point results. These fixed point results can then be
exploited to obtain strong optimality properties. We illustrate our results
through a range of applications, including new findings for several useful
models.",['math.OC'],False,,,,"Safe at the Margins: A General Approach to Safety Alignment in
  Low-Resource English Languages -- A Singlish Case Study",Dynamic Programming in Ordered Vector Space
neg-d2-638,2025-02-27,,2502.20034," Recently, Large Vision-Language Models (LVLMs) show remarkable performance
across various domains. However, these models suffer from object hallucination.
This study revisits the previous claim that the primary cause of such
hallucination lies in the limited representational capacity of the vision
encoder. Our analysis reveals that the capacity of the vision encoder itself is
already enough for detecting object hallucination. Based on this insight, we
propose a Fine-grained CLIPScore (F-CLIPScore), a simple yet effective
evaluation metric that enhances object-level granularity by incorporating text
embeddings at the noun phrase level. Evaluations on the OHD-Caps benchmark show
that F-CLIPScore significantly outperforms conventional CLIPScore in accuracy
by a large margin of 39.6% without additional training. We further validate
F-CLIPScore by showing that LVLM trained with the data filtered using
F-CLIPScore exhibits reduced hallucination.","['cs.CV', 'cs.CL']",2503.07414," This study aims to develop a cost-effective microgrid design that optimally
balances the economic feasibility, reliability, efficiency, and environmental
impact in a grid-tied community microgrid. A multi-objective optimization
framework is employed, integrating HOMER Pro for system sizing with deep
reinforcement learning (DRL). Sensitivity analyses are conducted to evaluate
the system performance under varying load demand and renewable energy
fluctuations, while an economic sensitivity assessment examines the impact of
electricity prices and capital costs on the Levelized Cost of Energy (LCOE).
The proposed microgrid configuration achieves high reliability, satisfying 100%
of the load, even under adverse weather conditions. The proposed framework
attains an efficiency of 91.99% while maintaining a carbon footprint of 302,747
kg/year, which is approximately 95% lower than that of the grid system. The
economic analysis indicates a net present cost (NPC) of $4.83M with a
competitive LCOE of $0.208/kWh. In addition, the operation cost is $201,473 per
year with a capital investment of $1.42M, rendering it a financially viable
alternative to conventional grid-dependent systems.This work can be valuable in
identifying effective solutions for supplying reliable and cost-effective power
to regional and remote areas.","['eess.SY', 'cs.SY']",False,,,,"Vision-Encoders (Already) Know What They See: Mitigating Object
  Hallucination via Simple Fine-Grained CLIPScore",Cost-Effective Design of Grid-tied Community Microgrid
neg-d2-639,2025-02-11,,2502.07369," The ability to identify useful features or representations of the input data
based on training data that achieves low prediction error on test data across
multiple prediction tasks is considered the key to multitask learning success.
In practice, however, one faces the issue of the choice of prediction tasks and
the availability of test data from the chosen tasks while comparing the
relative performance of different features. In this work, we develop a class of
pseudometrics called Uniform Kernel Prober (UKP) for comparing features or
representations learned by different statistical models such as neural networks
when the downstream prediction tasks involve kernel ridge regression. The
proposed pseudometric, UKP, between any two representations, provides a uniform
measure of prediction error on test data corresponding to a general class of
kernel ridge regression tasks for a given choice of a kernel without access to
test data. Additionally, desired invariances in representations can be
successfully captured by UKP only through the choice of the kernel function and
the pseudometric can be efficiently estimated from $n$ input data samples with
$O(\frac{1}{\sqrt{n}})$ estimation error. We also experimentally demonstrate
the ability of UKP to discriminate between different types of features or
representations based on their generalization performance on downstream kernel
ridge regression tasks.","['stat.ML', 'cs.LG', 'math.ST', 'stat.TH']",2503.14308," Enzyme immobilization plays a crucial role in enhancing the stability and
recyclability of enzymes for industrial applications. However, traditional
methods for quantifying enzyme loading within porous carriers are limited by
time-consuming workflows, cumulative errors, and the inability to probe enzymes
adsorbed inside the pores. In this study, we introduce Time-Domain Nuclear
Magnetic Resonance (TD-NMR) relaxometry as a novel, non-invasive technique for
directly quantifying enzyme adsorption within porous carriers. Focusing on
epoxy methyl acrylate carriers, commonly used in biocatalysis, we correlate
changes in T2 relaxation times with enzyme concentration, leading to the
development of an NMR-based pore-filling ratio that quantifies enzyme loading.
Validation experiments demonstrate that TD-NMR-derived adsorption curves align
closely with traditional photometric measurements, offering a reliable and
reproducible alternative for enzyme quantification. The accessibility of
tabletop TD-NMR spectrometers makes this technique a practical and
cost-effective tool for optimizing biocatalytic processes. Furthermore, the
method holds promise for real-time monitoring of adsorption dynamics and could
be adapted for a wider range of carrier materials and enzymes.",['cond-mat.soft'],False,,,,Uniform Kernel Prober,"A novel method for quantifying enzyme immobilization in porous carriers
  using simple NMR relaxometry"
neg-d2-640,2025-02-24,,2502.17727," The remarkable success of deep learning in recent years has prompted
applications in medical image classification and diagnosis tasks. While
classification models have demonstrated robustness in classifying simpler
datasets like MNIST or natural images such as ImageNet, this resilience is not
consistently observed in complex medical image datasets where data is more
scarce and lacks diversity. Moreover, previous findings on natural image
datasets have indicated a potential trade-off between data likelihood and
classification accuracy. In this study, we explore the use of score-based
generative models as classifiers for medical images, specifically mammographic
images. Our findings suggest that our proposed generative classifier model not
only achieves superior classification results on CBIS-DDSM, INbreast and Vin-Dr
Mammo datasets, but also introduces a novel approach to image classification in
a broader context. Our code is publicly available at
https://github.com/sushmitasarker/sgc_for_medical_image_classification",['cs.CV'],2502.12882," We present efficient classical algorithms to approximate expectation values
and probability amplitudes in linear optical circuits. Specifically, our
classical algorithm efficiently approximates the expectation values of
observables in linear optical circuits for arbitrary product input states
within an additive error under a mild condition. This result suggests that
certain applications of linear optical circuits relying on expectation value
estimation, such as photonic variational algorithms, may face challenges in
achieving quantum advantage. In addition, the (marginal) output probabilities
of boson sampling with arbitrary product input states can be efficiently
approximated using our algorithm, implying that boson sampling can be
efficiently simulated if its output probability distribution is polynomially
sparse. Moreover, our method generalizes Gurvits's algorithm, originally
designed to approximate the permanent, to also approximate the hafnian of
complex symmetric matrices with an additive error. The algorithm also solves a
molecular vibronic spectra problem for arbitrary product input states as
precisely as boson samplers. Finally, our method extends to near-Clifford
circuits, enabling the classical approximation of their expectation values of
any observables and (marginal) output probabilities.",['quant-ph'],False,,,,"Can Score-Based Generative Modeling Effectively Handle Medical Image
  Classification?",Efficient classical algorithms for linear optical circuits
neg-d2-641,2025-03-19,,2503.15287," This paper presents a novel approach to classical linear regression, enabling
model computation from data streams or in a distributed setting while
preserving data privacy in federated environments. We extend this framework to
generalized linear models (GLMs), ensuring scalability and adaptability to
diverse data distributions while maintaining privacy-preserving properties. To
assess the effectiveness of our approach, we conduct numerical studies on both
simulated and real datasets, comparing our method with conventional maximum
likelihood estimation for GLMs using iteratively reweighted least squares. Our
results demonstrate the advantages of the proposed method in distributed and
federated settings.","['stat.CO', 'cs.DC']",2501.15719," Following a method introduced by Thomas-Vasquez and developed by Grundman, we
prove that many Hilbert modular threefolds of arithmetic genus $0$ and $1$ are
of general type, and that some are of nonnegative Kodaira dimension. The new
ingredient is a detailed study of the geometry and combinatorics of totally
positive integral elements $x$ of a fractional ideal $I$ in a totally real
number field $K$ with the property that $\mathop{\mathrm{tr}} xy <
\mathop{\mathrm{min}} I \mathop{\mathrm{tr}} y$ for some $y \gg 0 \in K$.","['math.NT', 'math.AG']",False,,,,Distributed Generalized Linear Models: A Privacy-Preserving Approach,The Kodaira dimension of Hilbert modular threefolds
neg-d2-642,2025-02-16,,2502.11092," States with long coherence are a crucial requirement for qubits and quantum
memories. Nuclear spins in epitaxial quantum dots are a great candidate,
offering excellent isolation from external environments and on-demand coupling
to optical flying qubits. However, coherence times are limited to $\lesssim1$
ms by the dipole-dipole interactions between the nuclei and their quadrupolar
coupling to inhomogeneous crystal strain. Here, we combine strain engineering
of the nuclear spin ensemble and tailored dynamical decoupling sequences to
achieve nuclear spin coherence times exceeding 100 ms. Recently, a reversible
transfer of quantum information into nuclear spin ensembles has been
demonstrated in quantum dots. Our results provide a path to develop this
concept into a functioning solid-state quantum memory suitable for quantum
repeaters in optical quantum communication networks.",['cond-mat.mes-hall'],2501.0588," Designing efficient neural networks for embedded devices is a critical
challenge, particularly in applications requiring real-time performance, such
as aerial imaging with drones and UAVs for emergency responses. In this work,
we introduce TakuNet, a novel light-weight architecture which employs
techniques such as depth-wise convolutions and an early downsampling stem to
reduce computational complexity while maintaining high accuracy. It leverages
dense connections for fast convergence during training and uses 16-bit
floating-point precision for optimization on embedded hardware accelerators.
Experimental evaluation on two public datasets shows that TakuNet achieves
near-state-of-the-art accuracy in classifying aerial images of emergency
situations, despite its minimal parameter count. Real-world tests on embedded
devices, namely Jetson Orin Nano and Raspberry Pi, confirm TakuNet's
efficiency, achieving more than 650 fps on the 15W Jetson board, making it
suitable for real-time AI processing on resource-constrained platforms and
advancing the applicability of drones in emergency scenarios. The code and
implementation details are publicly released.","['cs.CV', 'cs.PF']",False,,,,"Storing quantum coherence in a quantum dot nuclear spin ensemble for
  over 100 milliseconds","TakuNet: an Energy-Efficient CNN for Real-Time Inference on Embedded UAV
  systems in Emergency Response Scenarios"
neg-d2-643,2025-01-08,,2501.04908," Recently, the use of large language models (LLMs) for Verilog code generation
has attracted great research interest to enable hardware design automation.
However, previous works have shown a gap between the ability of LLMs and the
practical demands of hardware description language (HDL) engineering. This gap
includes differences in how engineers phrase questions and hallucinations in
the code generated. To address these challenges, we introduce HaVen, a novel
LLM framework designed to mitigate hallucinations and align Verilog code
generation with the practices of HDL engineers. HaVen tackles hallucination
issues by proposing a comprehensive taxonomy and employing a chain-of-thought
(CoT) mechanism to translate symbolic modalities (e.g. truth tables, state
diagrams, etc.) into accurate natural language descriptions. Furthermore, HaVen
bridges this gap by using a data augmentation strategy. It synthesizes
high-quality instruction-code pairs that match real HDL engineering practices.
Our experiments demonstrate that HaVen significantly improves the correctness
of Verilog code generation, outperforming state-of-the-art LLM-based Verilog
generation methods on VerilogEval and RTLLM benchmark. HaVen is publicly
available at https://github.com/Intelligent-Computing-Research-Group/HaVen.","['cs.PL', 'cs.AR']",2501.01515," Motivated by deep learning regimes with multiple interacting yet distinct
model components, we introduce learning diagrams, graphical depictions of
training setups that capture parameterized learning as data rather than code. A
learning diagram compiles to a unique loss function on which component models
are trained. The result of training on this loss is a collection of models
whose predictions ``agree"" with one another. We show that a number of popular
learning setups such as few-shot multi-task learning, knowledge distillation,
and multi-modal learning can be depicted as learning diagrams. We further
implement learning diagrams in a library that allows users to build diagrams of
PyTorch and Flux.jl models. By implementing some classic machine learning use
cases, we demonstrate how learning diagrams allow practitioners to build
complicated models as compositions of smaller components, identify
relationships between workflows, and manipulate models during or after
training. Leveraging a category theoretic framework, we introduce a rigorous
semantics for learning diagrams that puts such operations on a firm
mathematical foundation.","['cs.LG', 'cs.AI', 'cs.PL', 'math.CT']",False,,,,"HaVen: Hallucination-Mitigated LLM for Verilog Code Generation Aligned
  with HDL Engineers","DiagrammaticLearning: A Graphical Language for Compositional Training
  Regimes"
neg-d2-644,2025-03-11,,2503.08892," Methods for computing the integral of the Planck blackbody function over a
finite spectral range, the so-called incomplete Planck integral, are necessary
to perform multigroup radiative transfer calculations. We present a comparison,
in terms of speed and accuracy, of a wide array of approaches to numerically
evaluating these integrals. Our results indicate that a direct rational
polynomial approximation to these integrals has the best combination of
accuracy and efficiency. We also present for the first time a derivation of the
polylogarithm form of these integrals and show that modern approaches to
polylogarithm evaluation are suitable for numerically evaluating incomplete
Planck integrals. This article is dedicated to Prof. B.D. Ganapol, the
Transport Cowboy, on the occasion of his retirement.",['physics.comp-ph'],2501.06778," In this study, we analyze the observational images of a Konoplya-Zhidenko
rotating non-Kerr black hole, wherein a thin accretion disk, serving as the
sole background light source, is situated on the equatorial plane of the black
hole. The inner boundary of the thin accretion disk extends to the event
horizon, and the accretion material in the disk exhibits two different motion
behaviors, that is, it moves along the critical plunging orbit inside the
innermost stable circular orbit (ISCO) and follows the Keplerian orbit outside
the ISCO. The shadow image is captured on the imaging plane of a zero angular
momentum observer utilizing advanced fisheye camera ray-tracing techniques. The
results demonstrate that an image consistently reveals a dark region encircled
by a narrow photon ring, which is called the inner shadow. At low observation
inclination angles, the observation intensity is highly concentrated, with the
lensed image of accretion disk being superimposed on the direct image. As
observation inclination angle increases, the direct and lensed images gradually
separate, becoming distinctly distinguishable and forming a hat-like structure.
Furthermore, variations in the parameter space and observation angle will
influence pertinent image characteristics, including image symmetry, the range
or deformation degree of the inner shadow. We further examined the distinctive
characteristics of images observed in both prograde and retrograde accretion
disk scenarios. Subsequently, we also examined the redshift distribution on the
disk. The findings indicate that while variations in relevant parameters do
influence the redshift distribution, the primary factor is the change in
observational inclination. The observer can detect both redshift and blueshift
phenomena on the screen when viewed at a higher observation angle.","['astro-ph.HE', 'gr-qc']",False,,,,"Fast, Accurate Numerical Evaluation of Incomplete Planck Integrals","Optical appearance of the Konoplya-Zhidenko rotating non-Kerr black hole
  surrounded by a thin accretion disk"
neg-d2-645,2025-01-05,,2501.02582," We apply Carleman linearization of the Lattice Boltzmann (CLB) representation
of fluid flows to quantum emulate the dynamics of a 2D Kolmogorov-like flow. We
assess the accuracy of the result and find a relative error of the order of
$10^{-3}$ with just two Carleman iterates, for a range of the Reynolds number
up to a few hundreds. We first define a gate-based quantum circuit for the
implementation of the CLB method and then exploit the sparse nature of the CLB
matrix to build a quantum circuit based on block-encoding techniques which
makes use of matrix oracles. It is shown that the gate complexity of the
algorithm is thereby dramatically reduced, from exponential to quadratic.
However, due to the need of employing up to seven ancilla qubits, the
probability of success of the corresponding circuit for a single time step is
too low to enable multi-step time evolution. Several possible directions to
circumvent this problem are briefly outlined.","['quant-ph', 'physics.comp-ph']",2502.03605," Device sizing is crucial for meeting performance specifications in
operational transconductance amplifiers (OTAs), and this work proposes an
automated sizing framework based on a transformer model. The approach first
leverages the driving-point signal flow graph (DP-SFG) to map an OTA circuit
and its specifications into transformer-friendly sequential data. A specialized
tokenization approach is applied to the sequential data to expedite the
training of the transformer on a diverse range of OTA topologies, under
multiple specifications. Under specific performance constraints, the trained
transformer model is used to accurately predict DP-SFG parameters in the
inference phase. The predicted DP-SFG parameters are then translated to
transistor sizes using a precomputed look-up table-based approach inspired by
the gm/Id methodology. In contrast to previous conventional or
machine-learning-based methods, the proposed framework achieves significant
improvements in both speed and computational efficiency by reducing the need
for expensive SPICE simulations within the optimization loop; instead, almost
all SPICE simulations are confined to the one-time training phase. The method
is validated on a variety of unseen specifications, and the sizing solution
demonstrates over 90% success in meeting specifications with just one SPICE
simulation for validation, and 100% success with 3-5 additional SPICE
simulations.",['cs.AR'],False,,,,Carleman-lattice-Boltzmann quantum circuit with matrix access oracles,"Accelerating OTA Circuit Design: Transistor Sizing Based on a
  Transformer Model and Precomputed Lookup Tables"
neg-d2-646,2025-01-29,,2501.18056," Query rewriting (QR) is a critical technique in e-commerce search, addressing
the lexical gap between user queries and product descriptions to enhance search
performance. Existing QR approaches typically fall into two categories:
discriminative models and generative methods leveraging large language models
(LLMs). Discriminative models often struggle with natural language
understanding and offer limited flexibility in rewriting, while generative
LLMs, despite producing high-quality rewrites, face high inference latency and
cost in online settings. These limitations force offline deployment, making
them vulnerable to issues like information staleness and semantic drift. To
overcome these challenges, we propose a novel hybrid pipeline for QR that
balances efficiency and effectiveness. Our approach combines offline knowledge
distillation to create a lightweight but efficient student model with online
reinforcement learning (RL) to refine query rewriting dynamically using
real-time feedback. A key innovation is the use of LLMs as simulated human
feedback, enabling scalable reward signals and cost-effective evaluation
without manual annotations. Experimental results on Amazon ESCI dataset
demonstrate significant improvements in query relevance, diversity, and
adaptability, as well as positive feedback from the LLM simulation. This work
contributes to advancing LLM capabilities for domain-specific applications,
offering a robust solution for dynamic and complex e-commerce search
environments.",['cs.IR'],2502.08772," Protein flexibility, measured by the B-factor or Debye-Waller factor, is
essential for protein functions such as structural support, enzyme activity,
cellular communication, and molecular transport. Theoretical analysis and
prediction of protein flexibility are crucial for protein design, engineering,
and drug discovery. In this work, we introduce the persistent sheaf Laplacian
(PSL), an effective tool in topological data analysis, to model and analyze
protein flexibility. By representing the local topology and geometry of protein
atoms through the multiscale harmonic and non-harmonic spectra of PSLs, the
proposed model effectively captures protein flexibility and provides accurate,
robust predictions of protein B-factors. Our PSL model demonstrates an increase
in accuracy of 32% compared to the classical Gaussian network model (GNM) in
predicting B-factors for a dataset of 364 proteins. Additionally, we construct
a blind machine learning prediction method utilizing global and local protein
features. Extensive computations and comparisons validate the effectiveness of
the proposed PSL model for B-factor predictions.","['q-bio.BM', 'q-bio.QM']",False,,,,"RL-based Query Rewriting with Distilled LLM for online E-Commerce
  Systems",Persistent Sheaf Laplacian Analysis of Protein Flexibility
neg-d2-647,2025-01-31,,2502.00275," Accurate estimation of human hand configuration and the forces they exert is
critical for effective teleoperation and skill transfer in robotic
manipulation. A deeper understanding of human interactions with objects can
further enhance teleoperation performance. To address this need, researchers
have explored methods to capture and translate human manipulation skills and
applied forces to robotic systems. Among these, biosignal-based approaches,
particularly those using forearm ultrasound data, have shown significant
potential for estimating hand movements and finger forces. In this study, we
present a method for simultaneously estimating manipulation skills and applied
hand force using forearm ultrasound data. Data collected from seven
participants were used to train deep learning models for classifying
manipulation skills and estimating grasp force. Our models achieved an average
classification accuracy of 94.87 percent plus or minus 10.16 percent for
manipulation skills and an average root mean square error (RMSE) of 0.51 plus
or minus 0.19 Newtons for force estimation, as evaluated using five-fold
cross-validation. These results highlight the effectiveness of forearm
ultrasound in advancing human-machine interfacing and robotic teleoperation for
complex manipulation tasks. This work enables new and effective possibilities
for human-robot skill transfer and tele-manipulation, bridging the gap between
human dexterity and robotic control.","['cs.RO', 'cs.CV', 'cs.ET', 'cs.HC']",2503.12119," The photostriction effect, a light-induced mechanical deformation in
materials, originates from the intricate interplay between lattice structure
and electronic excitation. In photovoltaic semiconductors, this effect plays a
crucial role in shaping non-equilibrium structural responses, yet its
fundamental mechanism remains elusive. Here, we uncover lattice expansion and
structural reconfiguration in two-dimensional (2D) perovskites driven by
photoinduced excitation using first-principles calculations. Our findings
reveal that the photoinduced carriers lead to a substantial lattice expansion
by about 2%. The expanded lattice facilitates strain relaxation with the
amplitude of 20% by increasing interatomic distances and reducing internal
stresses, thereby enhancing structural stability. The lattice dynamics can be
systematically engineered through photodoping density, unveiling a new pathway
to modulate light-matter interactions in 2D perovskites. These insights not
only advance the understanding of optically driven structural dynamics but also
offer a guiding principle for optimizing next-generation high-efficiency
photovoltaic devices and optoelectronics.","['cond-mat.mtrl-sci', 'physics.comp-ph']",False,,,,"Simultaneous Estimation of Manipulation Skill and Hand Grasp Force from
  Forearm Ultrasound Images","Photostriction Facilitates Relaxation of Lattice Distortion in
  Two-Dimensional Perovskites"
neg-d2-648,2025-02-04,,2502.02323," Resolvers, like all electromagnetic devices, are constantly under
investigation, both operationally and structurally. In this regard, proposing a
modeling methodology that can save significant time without compromising
accuracy is a big honor. In this study, a generalized hybrid model is suggested
that, in addition to the above benefits, has sufficient capability to ease
reliability study in the field of resolvers, where a large number of faulty
conditions must be investigated under different operating conditions, including
changes in angular velocity, voltage, and frequency of excitation; all of which
are highlighted in the context of fault coverage. This model also serves as a
promising tool for generating large datasets, which is advantageous for fault
diagnosis. A resolver with a non-uniform air gap is chosen as a case study to
challenge the suggested model, particularly in relation to eccentricity faults.
We generalize the suggested model to account for the most common faulty
conditions of resolvers: in-turn short circuits in signal and excitation
windings, as well as static and dynamic eccentricity faults. The close
agreement between the results of the suggested model and those from
Time-Stepping Finite Element Analysis (TS-FEA), along with significant time
savings in both healthy and faulty conditions, highlights the generality and
proficiency of the suggested model. Finally, the case study is prototyped, and
we verify the accuracy of the suggested model experimentally.","['eess.SY', 'cs.SY']",2502.09016," We study the collective modes of an atomic bright soliton realised in a
quasi-one-dimensional Bose-Einstein condensate, using Bogoliubov-de Gennes
theory. In particular we focus on the breathing mode of the soliton, which is
not a single linearized normal mode but a common component of many modes, and
therefore decays within a $t^{-1/2}$ envelope due to dispersion. If the soliton
is held in the center of a harmonic trap, we show that the breathing amplitude
revives periodically, as atoms shed from the vibrating soliton oscillate in the
trap, and return. After each revival the breathing amplitude again decays, and
this cycle repeats every trap half-period. The amplitude envelope of these
breathing revivals shows a curious asymmetry, however, with a gradual increase
in breathing followed by sudden drop in breathing amplitude that becomes more
and more pronounced in later revivals. We explain this asymmetrical revival
pattern by deriving a close analytical approximation to the Bogoliubov-de
Gennes frequency spectrum, and offer this coherent Bogoliubov-de Gennes
phenomenon as a background against which to compare possible quantum many-body
effects, including decoherence over trap-period time scales.","['cond-mat.quant-gas', 'quant-ph']",False,,,,"Hybrid Resolver Model Generalization for Fault Condition Modeling: A
  Promising Tool for Reliability Study","Soliton resuscitations: asymmetric revivals of the breathing mode of an
  atomic bright soliton in a harmonic trap"
neg-d2-649,2025-01-18,,2501.10662," We study non-local operators for analyzing the Higgs-confinement phase
transition in lattice gauge theory. Since the nature of the Higgs-confinement
phase transition is topological, its order parameter is the expectation value
of non-local operators, such as loop and surface operators. There exist several
candidates for the non-local operators. Adopting the charge-2 Abelian Higgs
model, we test numerical simulation of conventional ones, the Polyakov loop and
the 't Hooft loop, and an unconventional one, the Aharonov-Bohm phase defined
by the Wilson loop wrapping around a vortex line.","['hep-lat', 'hep-th']",2503.07841," We present the first measurements with a new collinear laser spectroscopy
setup at the Argonne Tandem Linac Accelerator System utilizing its unique
capability to deliver neutron-rich refractory metal isotopes produced by the
spontaneous fission of 252Cf. We measured isotope shifts from optical spectra
for nine radioactive ruthenium isotopes 106-114Ru, reaching deep into the
mid-shell region. The extracted charge radii are in excellent agreement with
predictions from the Brussels-Skyrme-on-a-Grid models that account for the
triaxial deformation of nuclear ground states in this region. We show that
triaxial deformation impacts charge radii in models that feature shell effects,
in contrast to what could be concluded from a liquid drop analysis. This
indicates that this exotic type of deformation should not be neglected in
regions where it is known to occur, even if its presence cannot be
unambiguously inferred through laser spectroscopy.","['nucl-ex', 'nucl-th']",False,,,,"Analyzing the Higgs-confinement transition with non-local operators on
  the lattice","Fingerprints of triaxiality in the charge radii of neutron-rich
  Ruthenium"
neg-d2-650,2025-01-15,,2501.08884," Scenario decision making offers a flexible way of making decision in an
uncertain environment while obtaining probabilistic guarantees on the risk of
failure of the decision. The idea of this approach is to draw samples of the
uncertainty and make a decision based on the samples, called ""scenarios"". The
probabilistic guarantees take the form of a bound on the probability of
sampling a set of scenarios that will lead to a decision whose risk of failure
is above a given maximum tolerance. This bound can be expressed as a function
of the number of sampled scenarios, the maximum tolerated risk, and some
intrinsic property of the problem called the ""compression size"". Several such
bounds have been proposed in the literature under various assumptions on the
problem. We propose new bounds that improve upon the existing ones without
requiring stronger assumptions on the problem.","['math.OC', 'cs.LG']",2502.10682," Effective deepfake detection tools are becoming increasingly essential over
the last few years due to the growing usage of deepfakes in unethical
practices. There exists a diverse range of deepfake generation techniques,
which makes it challenging to develop an accurate universal detection
mechanism. The 2025 Signal Processing Cup (DFWild-Cup competition) provided a
diverse dataset of deepfake images, which are generated from multiple deepfake
image generators, for training machine learning model(s) to emphasize the
generalization of deepfake detection. To this end, we proposed an
ensemble-based approach that employs three different neural network
architectures: a ResNet-34-based architecture, a data-efficient image
transformer (DeiT), and an XceptionNet with Wavelet Transform to capture both
local and global features of deepfakes. We visualize the specific regions that
these models focus for classification using Grad-CAM, and empirically
demonstrate the effectiveness of these models in grouping real and fake images
into cohesive clusters using t-SNE plots. Individually, the ResNet-34
architecture has achieved 88.9% accuracy, whereas the Xception network and the
DeiT architecture have achieved 87.76% and 89.32% accuracy, respectively. With
these networks, our weighted ensemble model achieves an excellent accuracy of
93.23% on the validation dataset of the SP Cup 2025 competition. Finally, the
confusion matrix and an Area Under the ROC curve of 97.44% further confirm the
stability of our proposed method.","['cs.CV', 'cs.LG', 'eess.IV']",False,,,,Improved Compression Bounds for Scenario Decision Making,"Hybrid Deepfake Image Detection: A Comprehensive Dataset-Driven Approach
  Integrating Convolutional and Attention Mechanisms with Frequency Domain
  Features"
neg-d2-651,2025-03-04,,2503.03121," The Littlewood decomposition for partitions is a well-known bijection between
partitions and pairs of $t$-core and $t$-quotient partitions. This
decomposition can be described in several ways, such as the $t$-abacus method
of James or the biinfinite word method of Garvan, Kim, and Stanton. In a recent
study, Frobenius partitions have proven to be a highly useful tool in dealing
with partition statistics related to $t$-core partitions. Motivated by this
study, in this paper, we present an alternative description of the Littlewood
decomposition using Frobenius partitions. We also apply our approach to
self-conjugate partitions and doubled distinct partitions, and give new
characterizations of their $t$-cores and $t$-quotients.",['math.CO'],2501.04782," Efficient neural representations for dynamic video scenes are critical for
applications ranging from video compression to interactive simulations. Yet,
existing methods often face challenges related to high memory usage, lengthy
training times, and temporal consistency. To address these issues, we introduce
a novel neural video representation that combines 3D Gaussian splatting with
continuous camera motion modeling. By leveraging Neural ODEs, our approach
learns smooth camera trajectories while maintaining an explicit 3D scene
representation through Gaussians. Additionally, we introduce a spatiotemporal
hierarchical learning strategy, progressively refining spatial and temporal
features to enhance reconstruction quality and accelerate convergence. This
memory-efficient approach achieves high-quality rendering at impressive speeds.
Experimental results show that our hierarchical learning, combined with robust
camera motion modeling, captures complex dynamic scenes with strong temporal
consistency, achieving state-of-the-art performance across diverse video
datasets in both high- and low-motion scenarios.",['cs.CV'],False,,,,The Littlewood decomposition via colored Frobenius partitions,"GaussianVideo: Efficient Video Representation via Hierarchical Gaussian
  Splatting"
neg-d2-652,2025-03-18,,2503.13997," We introduce a novel approach that utilizes neutrino events from the off-axis
near detector to investigate the beam profile in long-baseline neutrino
experiments. Understanding the dynamics of the neutrino beam is crucial for
improving the precision of neutrino oscillation measurements. We demonstrate
that certain observables related to the azimuthal angle of the neutrino
direction are useful for determining the average neutrino production point from
experimental data, providing a valuable cross-check against Monte Carlo
simulations. Additionally, these observables can help identify potential
alignment issues between the detector and the decay volume. In future neutrino
experiments with significantly higher statistics, these observables will become
essential to ensure the accuracy and stability of the beam profile.","['hep-ex', 'physics.ins-det']",2502.181," A graph $G$ is $\mathcal S_3$-connected if, for any mapping $\beta : V (G)
\mapsto {\mathbb Z}_3$ with $\sum_{v\in V(G)} \beta(v)\equiv 0\pmod3$, there
exists a strongly connected orientation $D$ satisfying
$d^{+}_D(v)-d^{-}_D(v)\equiv \beta(v)\pmod{3}$ for any $v \in V(G)$. It is
known that $\mathcal S_3$-connected graphs are contractible configurations for
the property of flow index strictly less than three. In this paper, we provide
a complete characterization of graphic sequences that have an
$\mathcal{S}_{3}$-connected realization: A graphic sequence $\pi=(d_1,\,
\ldots,\, d_n )$ has an $\mathcal S_3$-connected realization if and only if
$\min \{d_1,\, \ldots,\, d_n\} \ge 4$ and $\sum^n_{i=1}d_i \ge 6n - 4$.
Consequently, every graphic sequence $\pi=(d_1,\, \ldots,\, d_n )$ with $\min
\{d_1,\, \ldots,\, d_n\} \ge 6$ has a realization $G$ with flow index strictly
less than three. This supports a conjecture of Li, Thomassen, Wu and Zhang
[European J. Combin., 70 (2018) 164-177] that every $6$-edge-connected graph
has flow index strictly less than three.",['math.CO'],False,,,,"Characterizing Beam Profiles in Accelerator Neutrino Experiments through
  Off-Axis Neutrino Interactions",Realizing degree sequences with $\mathcal S_3$-connected graphs
neg-d2-653,2025-02-05,,2502.03432," We present a formalization of Borel determinacy in the Lean 4 theorem prover.
The formalization includes a definition of Gale-Stewart games and a proof of
Martin's theorem stating that Borel games are determined. The proof closely
follows Martin's ""A purely inductive proof of Borel determinacy"".",['math.LO'],2501.04112," In this work, we provide the first example of an infinite family of branch
groups in the class of non-contracting self-similar groups. We show that these
groups are very strongly fractal, not regular branch, and of exponential
growth. Further, we prove that these groups do not have the congruence subgroup
property by explicitly calculating the structure of their rigid kernels. This
class of groups is also the first example of branch groups with non-torsion
rigid kernels. As a consequence of these results, we also determine the
Hausdorff dimension of these groups.",['math.GR'],False,,,,A formalization of Borel determinacy in Lean,A Class of Non-Contracting Branch Groups with Non-Torsion Rigid Kernels
neg-d2-654,2025-03-07,,2503.06018," The evolution of the role of lattice vibrations in the formation of the
pseudogap state in strongly correlated electron systems has been investigated
concerning changes in the electron-phonon coupling parameters and the
concentration of doped charge carriers. We apply the polaronic version of the
generalized tight-binding method to analyze the band structure of a realistic
multiband two-dimensional model that incorporates the electron-lattice
contributions of both Holstein and Peierls types. It has been demonstrated that
the emergence of polaronic effects begins with the modulation of spectral
function intensity. However, within a specific region of the phase diagram, a
significant transformation of the electron band structure and pseudogap state
occurs. It results from coherent polaron excitations that create a partially
flat band near the Fermi level. This process leads to a change in the topology
of the Fermi surface and the emergence of corresponding features in the density
of states.","['cond-mat.str-el', 'cond-mat.supr-con']",2502.0804," We consider ways to construct a transducer for a given set of input word to
output symbol pairs. This is motivated by the need for representing game
playing programs in a low-level mathematical format that can be analyzed by
algebraic tools. This is different from the classical applications of finite
state automata, thus the usual optimization techniques are not directly
applicable. Therefore, we use relational programming tools to find minimal
transducers realizing a given set of input-output pairs.",['cs.FL'],False,,,,"Evolution of the pseudogap band structure in a system of
  electron-correlated lattice polarons",On Constructing Finite Automata by Relational Programming
neg-d2-655,2025-02-09,,2502.06121," Given a positive definite even lattice and a commutative ring, there is a
standard construction of a lattice vertex algebra over the commutative ring,
and this is promoted to a vertex operator algebra when the determinant of the
lattice is invertible. We describe the groups of automorphisms of these vertex
algebras and vertex operator algebras as affine group schemes, showing in
particular that each is an extension of an explicitly described split reductive
group of ADE type by the outer automorphism group of the lattice.","['math.QA', 'math.AG', 'math.GR']",2501.19226," In this paper, we explore a taxonomy of connectivity for space-like
structures. It is inspired by isolating posets of connected pieces of a space
and examining its embedding in the ambient space. The taxonomy includes in its
scope all standard notions of connectivity in point-set and point-free
contexts, such as connectivity in graphs and hypergraphs (as well as
k-connectivity in graphs), connectivity and path-connectivity in topology, and
connectivity of elements in a frame.","['math.GN', 'math.CT', 'math.RA']",False,,,,Automorphism group schemes of lattice vertex operator algebras,What is Connectivity?
neg-d2-656,2025-01-13,,2501.07166," Combinatorial medication recommendation(CMR) is a fundamental task of
healthcare, which offers opportunities for clinical physicians to provide more
precise prescriptions for patients with intricate health conditions,
particularly in the scenarios of long-term medical care. Previous research
efforts have sought to extract meaningful information from electronic health
records (EHRs) to facilitate combinatorial medication recommendations. Existing
learning-based approaches further consider the chemical structures of
medications, but ignore the textual medication descriptions in which the
functionalities are clearly described. Furthermore, the textual knowledge
derived from the EHRs of patients remains largely underutilized. To address
these issues, we introduce the Natural Language-Assisted Multi-modal Medication
Recommendation(NLA-MMR), a multi-modal alignment framework designed to learn
knowledge from the patient view and medication view jointly. Specifically,
NLA-MMR formulates CMR as an alignment problem from patient and medication
modalities. In this vein, we employ pretrained language models(PLMs) to extract
in-domain knowledge regarding patients and medications, serving as the
foundational representation for both modalities. In the medication modality, we
exploit both chemical structures and textual descriptions to create medication
representations. In the patient modality, we generate the patient
representations based on textual descriptions of diagnosis, procedure, and
symptom. Extensive experiments conducted on three publicly accessible datasets
demonstrate that NLA-MMR achieves new state-of-the-art performance, with a
notable average improvement of 4.72% in Jaccard score. Our source code is
publicly available on https://github.com/jtan1102/NLA-MMR_CIKM_2024.",['cs.AI'],2501.01193," We contribute to the lively debate in current scholarship on the Leibnizian
calculus. In a recent text, Arthur and Rabouin argue that non-Archimedean
continua are incompatible with Leibniz's concepts of number, quantity and
magnitude.
  They allege that Leibniz viewed infinitesimals as contradictory, and claim to
deduce such a conclusion from an analysis of the Leibnizian definition of
quantity. However, their argument is marred by numerous errors, deliberate
omissions, and misrepresentations, stemming in a number of cases from flawed
analyses in their earlier publications.
  We defend the thesis, traceable to the classic study by Henk Bos, that
Leibniz used genuine infinitesimals, which he viewed as fictional mathematical
entities (and not merely shorthand for talk about more ordinary quantities) on
par with negatives and imaginaries.",['math.HO'],False,,,,Natural Language-Assisted Multi-modal Medication Recommendation,Leibniz's contested infinitesimals: Further depictions
neg-d2-657,2025-03-18,,2503.14077," The radiative open circuit voltage loss in a solar cell occurs because the
absorptance spectrum near the band gap shows gradual increase rather than sharp
step function like transition. This broadening effect has been attributed to
band gap fluctuations and or to Urbach tails. In this report, we use modelling
based on Planck s generalized law to distinguish between these two effects. Our
results demonstrate that Urbach tails have only a minimal effect on the
absorptance edge broadening and clarify that even an ideal direct semiconductor
with no band gap fluctuations shows broadening at the absorptance onset.
Furthermore, state of the art inorganic thin film solar cells often incorporate
a band gap gradient across their thickness, which can further contribute to
absorptance broadening. Using Cu(In,Ga)Se2 (CIGSe) absorbers as a case study,
we perform a comprehensive analysis of voltage losses through absolute
photoluminescence and electroluminescence spectroscopy, combined with
photospectrometry and high-spatial-resolution cathodoluminescence measurements.
We find that the loss analysis based on the combination of radiative,
generation and non-radiative losses is complete. Samples with a graded band gap
profile show more pronounced broadening of the absorptance onset and up to 16
mV higher radiative losses compared to the samples with uniform band gap. There
is indication, that band gap-graded samples also have larger lateral band gap
inhomogeneity.",['cond-mat.mtrl-sci'],2502.11846," We develop a machine learning approach to reconstructing the cosmological
initial conditions from late-time dark matter halo number density fields in
redshift space, with the goal of improving sensitivity to cosmological
parameters, and in particular primordial non-Gaussianity. Using an U-Net
architecture, our model achieves a cross-correlation accuracy of 44% for scales
out to $k = 0.4 \text{ h}/\text{Mpc}$ between reconstructed and true initial
conditions of Quijote 1 Gpc$^3$ simulation boxes with an average halo number
density of $\bar{n} = 4\times 10^{-4}$ (h/Mpc)$^{3}$ in the tracer field at
$z=0$ . We demonstrate that our reconstruction is likely to be optimal for this
setup and that it is highly effective at reducing redshift-space distortions.
Using a Fisher analysis, we show that reconstruction improves cosmological
parameter constraints derived from the power spectrum and bispectrum. By
combining the power spectrum monopole, quadrupole, and bispectrum monopole up
to $k_{\rm{max}} = 0.52 \text{ h}/\text{Mpc}$, our joint analysis of pre- and
post-reconstructed fields from the Quijote simulation suite finds improved
marginalized errors on all cosmological parameters. In particular,
reconstruction improves constraints on $f_{\rm{NL}}$ by factors of 1.33, 1.88,
and 1.57 for local, equilateral, and orthogonal shapes. Our findings
demonstrate the effectiveness of reconstruction in decoupling modes, mitigating
redshift-space distortions and maximizing information on cosmology. The results
provide important insights into the amount of cosmological information that can
be extracted from small scales, and can potentially be used to complement
standard analysis of observational data, upon further development.",['astro-ph.CO'],False,,,,"The effect of a band gap gradient on the radiative losses in the open
  circuit voltage of solar cells","Neural Network Reconstruction of Non-Gaussian Initial Conditions from
  Dark Matter Halos"
neg-d2-658,2025-01-21,,2501.12637," Neural Radiance Fields (NeRF) has achieved superior performance in novel view
synthesis and 3D scene representation, but its practical applications are
hindered by slow convergence and reliance on dense training views. To this end,
we present DWTNeRF, a unified framework based on Instant-NGP's fast-training
hash encoding. It is coupled with regularization terms designed for few-shot
NeRF, which operates on sparse training views. Our DWTNeRF additionally
includes a novel Discrete Wavelet loss that allows explicit prioritization of
low frequencies directly in the training objective, reducing few-shot NeRF's
overfitting on high frequencies in earlier training stages. We also introduce a
model-based approach, based on multi-head attention, that is compatible with
INGP, which are sensitive to architectural changes. On the 3-shot LLFF
benchmark, DWTNeRF outperforms Vanilla INGP by 15.07% in PSNR, 24.45% in SSIM
and 36.30% in LPIPS. Our approach encourages a re-thinking of current few-shot
approaches for fast-converging implicit representations like INGP or 3DGS.",['cs.CV'],2503.10512," We consider the problem of generating valid and small prediction sets by
sampling outputs (e.g., software code and natural language text) from a
black-box deep generative model for a given input (e.g., textual prompt). The
validity of a prediction set is determined by a user-defined binary
admissibility function depending on the target application. For example,
requiring at least one program in the set to pass all test cases in code
generation application. To address this problem, we develop a simple and
effective conformal inference algorithm referred to as Generative Prediction
Sets (GPS). Given a set of calibration examples and black-box access to a deep
generative model, GPS can generate prediction sets with provable guarantees.
The key insight behind GPS is to exploit the inherent structure within the
distribution over the minimum number of samples needed to obtain an admissible
output to develop a simple conformal regression approach over the minimum
number of samples. Experiments on multiple datasets for code and math word
problems using different large language models demonstrate the efficacy of GPS
over state-of-the-art methods.","['cs.LG', 'cs.AI']",False,,,,"DWTNeRF: Boosting Few-shot Neural Radiance Fields via Discrete Wavelet
  Transform","Conformal Prediction Sets for Deep Generative Models via Reduction to
  Conformal Regression"
neg-d2-659,2025-03-05,,2503.0342," The spontaneous (so-called Quincke) rotation of an uncharged, solid,
dielectric, spherical particle under a steady electric field is analyzed,
accounting for the inertia of the particle and the transient fluid inertia, or
``hydrodynamic memory,'' due to the unsteady Stokes flow around the particle.
The dynamics of the particle are encapsulated in three coupled nonlinear
integro-differential equations for the evolution of the angular velocity of the
particle, and the components of the induced dipole of the particle that are
parallel and transverse to the applied field. These equations represent a
generalization of the celebrated Lorenz system. A numerical solution of these
`modified Lorenz equations' (MLE) shows that hydrodynamic memory leads to an
increase in the threshold field strength for chaotic particle rotation, which
is in qualitative agreement with experimental observations. Furthermore,
hydrodynamic memory leads to an increase in the range of field strengths where
multi-stability between steady and chaotic rotation occurs. At large field
strengths, chaos ceases and the particle is predicted to execute periodic
rotational motion.",['physics.flu-dyn'],2501.18836," Dynamic pricing strategies are crucial for firms to maximize revenue by
adjusting prices based on market conditions and customer characteristics.
However, designing optimal pricing strategies becomes challenging when
historical data are limited, as is often the case when launching new products
or entering new markets. One promising approach to overcome this limitation is
to leverage information from related products or markets to inform the focal
pricing decisions. In this paper, we explore transfer learning for
nonparametric contextual dynamic pricing under a covariate shift model, where
the marginal distributions of covariates differ between source and target
domains while the reward functions remain the same. We propose a novel Transfer
Learning for Dynamic Pricing (TLDP) algorithm that can effectively leverage
pre-collected data from a source domain to enhance pricing decisions in the
target domain. The regret upper bound of TLDP is established under a simple
Lipschitz condition on the reward function. To establish the optimality of
TLDP, we further derive a matching minimax lower bound, which includes the
target-only scenario as a special case and is presented for the first time in
the literature. Extensive numerical experiments validate our approach,
demonstrating its superiority over existing methods and highlighting its
practical utility in real-world applications.","['cs.LG', 'math.ST', 'stat.ME', 'stat.TH']",False,,,,Hydrodynamic memory and Quincke rotation,Transfer Learning for Nonparametric Contextual Dynamic Pricing
neg-d2-660,2025-03-06,,2503.04313," Infinitesimals have seen ups and downs in their tumultuous history. In the
18th century, d'Alembert set the tone by describing infinitesimals as chimeras.
Some adversaries of infinitesimals, including Moigno and Connes, picked up on
the term. We highlight the work of Cauchy, No\""el, Poisson and Riemann. We also
chronicle reactions by Moigno, Lamarle and Cantor, and signal the start of a
revival with Peano.",['math.HO'],2501.06891," A remarkable feature of dark matter consisting of ultralight bosonic
particles is the emergence of superfluid Bose-Einstein condensate structures on
galactic scales. We investigate the oscillations of the solitonic dark matter
structure in the central galactic region by numerically solving the
Bogoliubov-de Gennes problem, accounting for perturbations in the gravitational
potential and local self-interactions. Our findings reveal that the central
solitonic core, formed by the balance of gravitational attraction, quantum
pressure, and repulsive interactions, exhibits significant oscillatory
behaviour. These oscillations, characterized by distinct eigenmodes, provide
insights into the dynamical properties of solitonic dark matter structures and
their observational implications and contributions to galactic structure
formation and evolution.","['astro-ph.CO', 'astro-ph.GA', 'nlin.PS']",False,,,,Episodes from the history of infinitesimals,Oscillations of Solitonic Galactic Cores in Ultralight Dark Matter
neg-d2-661,2025-01-08,,2501.04279," In daily domestic settings, frequently used objects like cups often have
unfixed positions and multiple instances within the same category, and their
carriers frequently change as well. As a result, it becomes challenging for a
robot to efficiently navigate to a specific instance. To tackle this challenge,
the robot must capture and update scene changes and plans continuously.
However, current object navigation approaches primarily focus on the semantic
level and lack the ability to dynamically update scene representation. In
contrast, this paper captures the relationships between frequently used objects
and their static carriers. It constructs an open-vocabulary
Carrier-Relationship Scene Graph (CRSG) and updates the carrying status during
robot navigation to reflect the dynamic changes of the scene. Based on the
CRSG, we further propose an instance navigation strategy that models the
navigation process as a Markov Decision Process. At each step, decisions are
informed by the Large Language Model's commonsense knowledge and
visual-language feature similarity. We designed a series of long-sequence
navigation tasks for frequently used everyday items in the Habitat simulator.
The results demonstrate that by updating the CRSG, the robot can efficiently
navigate to moved targets. Additionally, we deployed our algorithm on a real
robot and validated its practical effectiveness. The project page can be found
here: https://OpenIN-nav.github.io.",['cs.RO'],2501.05555," Detecting object-level changes between two images across possibly different
views is a core task in many applications that involve visual inspection or
camera surveillance. Existing change-detection approaches suffer from three
major limitations: (1) lack of evaluation on image pairs that contain no
changes, leading to unreported false positive rates; (2) lack of
correspondences (i.e., localizing the regions before and after a change); and
(3) poor zero-shot generalization across different domains. To address these
issues, we introduce a novel method that leverages change correspondences (a)
during training to improve change detection accuracy, and (b) at test time, to
minimize false positives. That is, we harness the supervision labels of where
an object is added or removed to supervise change detectors, improving their
accuracy over previous work by a large margin. Our work is also the first to
predict correspondences between pairs of detected changes using estimated
homography and the Hungarian algorithm. Our model demonstrates superior
performance over existing methods, achieving state-of-the-art results in change
detection and change correspondence accuracy across both in-distribution and
zero-shot benchmarks.","['cs.CV', 'cs.AI']",False,,,,"OpenIN: Open-Vocabulary Instance-Oriented Navigation in Dynamic Domestic
  Environments","Improving Zero-Shot Object-Level Change Detection by Incorporating
  Visual Correspondence"
neg-d2-662,2025-03-04,,2503.02292," Selecting the right monitoring level in Remote Patient Monitoring (RPM)
systems for e-healthcare is crucial for balancing patient outcomes, various
resources, and patient's quality of life. A prior work has used one-dimensional
health representations, but patient health is inherently multidimensional and
typically consists of many measurable physiological factors. In this paper, we
introduce a multidimensional health state model within the RPM framework and
use dynamic programming to study optimal monitoring strategies. Our analysis
reveals that the optimal control is characterized by switching curves (for
two-dimensional health states) or switching hyper-surfaces (in general):
patients switch to intensive monitoring when health measurements cross a
specific multidimensional surface. We further study how the optimal switching
curve varies for different medical conditions and model parameters. This
finding of the optimal control structure provides actionable insights for
clinicians and aids in resource planning. The tunable modeling framework
enhances the applicability and effectiveness of RPM services across various
medical conditions.","['eess.SY', 'cs.SY']",2502.03618," The field of mechanistic interpretability in pre-trained transformer models
has demonstrated substantial evidence supporting the ''linear representation
hypothesis'', which is the idea that high level concepts are encoded as vectors
in the space of activations of a model. Studies also show that model generation
behavior can be steered toward a given concept by adding the concept's vector
to the corresponding activations. We show how to leverage these properties to
build a form of logical implication into models, enabling transparent and
interpretable adjustments that induce a chosen generation behavior in response
to the presence of any given concept. Our method, Logical Implication Model
Steering (LIMS), unlocks new hand engineered reasoning capabilities by
integrating neuro-symbolic logic into pre-trained transformer models.",['cs.LG'],False,,,,"Optimal Control for Remote Patient Monitoring with Multidimensional
  Health States","The Logical Implication Steering Method for Conditional Interventions on
  Transformer Generation"
neg-d2-663,2025-03-08,,2503.06367," We investigate, both experimentally and theoretically, the eigenmodes of an
electronic circuit in which gain and loss $RLC$ resonators are coupled through
a capacitor. Due to the unavoidable magnetic loss in the inductors, we find
that the eigenmode coalescence no longer emerges in contrast to the
conventional non-Hermitian systems with the spontaneous $\cal{PT}$-symmetry
breaking. In particular, we find a transition from the exponential decay to
exponential growth in the amplitude of the periodic voltage oscillations of the
resonators. The transition occurs near the exceptional points of the
non-Hermitian circuit without considering the dissipations in inductors. We
introduce a small resistor of three orders of magnitude smaller than that of
the $RLC$ resonators to mimic the energy dissipation in inductors and
numerically solve the equivalent non-Hermitian Schr{\"" o}dinger equation. The
numerical results can well reproduce experimental observations. Our above
findings unambiguously indicate that the exponential growth behavior beyond the
exceptional points is robust against some unavoidable dissipative
perturbations.",['quant-ph'],2502.01219," Dynamical systems can be coupled in a manner that is designed to drive the
resulting dynamics onto a specified lower dimensional submanifold in the phase
space of the combined system. On the submanifold, the variables of the two
systems have a well-defined unique functional relationship. This process can
thus be viewed as a control technique that ensures generalized synchronization.
Depending on the nature of the dynamical systems and the specified submanifold,
different coupling functions can be derived in order to achieve a desired
control objective. We discuss the circuit implementations of this strategy in
representative examples of coupled chaotic dynamical systems, namely Lorenz
oscillators",['nlin.CD'],False,,,,"Observing the exponential growth of the eigenmodes in the absence of
  coalescence for a non-Hermitian circuit with an unavoidable inductor
  dissipation",Control Strategy for Generalized Synchrony in Coupled Dynamical Systems
neg-d2-664,2025-03-10,,2503.07432," The ringdown of perturbed black holes has been studied since the 1970s, but
until recently, studies have focused on linear perturbations. There is now
burgeoning interest in nonlinear perturbative effects during ringdown. Here,
using a hyperboloidal framework, we provide a complete treatment of linear and
quadratic quasinormal modes (QNMs and QQNMs) in second-order perturbation
theory, in Schwarzschild spacetime. We include novel methods for extracting
QNMs and QQNMs amplitudes using a Laplace transform treatment, allowing for the
inclusion of arbitrary initial data. We produce both time- and frequency-domain
codes. From these codes, we present new results further exploring the
unforeseen dependence of QQNMs amplitudes on the parity of the progenitor
system, as demonstrated in our letter [Phys. Rev. Lett. 134, 061401 (2025)].
Our numerical results are restricted to perturbations of a Schwarzschild black
hole, but our methods extend straightforwardly to the astrophysically realistic
case of a Kerr black hole.",['gr-qc'],2501.06891," A remarkable feature of dark matter consisting of ultralight bosonic
particles is the emergence of superfluid Bose-Einstein condensate structures on
galactic scales. We investigate the oscillations of the solitonic dark matter
structure in the central galactic region by numerically solving the
Bogoliubov-de Gennes problem, accounting for perturbations in the gravitational
potential and local self-interactions. Our findings reveal that the central
solitonic core, formed by the balance of gravitational attraction, quantum
pressure, and repulsive interactions, exhibits significant oscillatory
behaviour. These oscillations, characterized by distinct eigenmodes, provide
insights into the dynamical properties of solitonic dark matter structures and
their observational implications and contributions to galactic structure
formation and evolution.","['astro-ph.CO', 'astro-ph.GA', 'nlin.PS']",False,,,,"Quadratic quasinormal modes at null infinity on a Schwarzschild
  spacetime",Oscillations of Solitonic Galactic Cores in Ultralight Dark Matter
neg-d2-665,2025-01-17,,2501.10346," We prove that Hopf manifolds admit holomorphic $(G,X)$-structures, extending
to any dimension a result of McKay and Pokrovskiy. For this, we revisit
Guysinsky-Katok's group of invertible sub-resonant polynomials, and
Bertheloot's approach of Poincar\'e-Dulac normal form theory.",['math.CV'],2503.18244," We propose a novel knowledge distillation approach, CustomKD, that
effectively leverages large vision foundation models (LVFMs) to enhance the
performance of edge models (e.g., MobileNetV3). Despite recent advancements in
LVFMs, such as DINOv2 and CLIP, their potential in knowledge distillation for
enhancing edge models remains underexplored. While knowledge distillation is a
promising approach for improving the performance of edge models, the
discrepancy in model capacities and heterogeneous architectures between LVFMs
and edge models poses a significant challenge. Our observation indicates that
although utilizing larger backbones (e.g., ViT-S to ViT-L) in teacher models
improves their downstream task performances, the knowledge distillation from
the large teacher models fails to bring as much performance gain for student
models as for teacher models due to the large model discrepancy. Our simple yet
effective CustomKD customizes the well-generalized features inherent in LVFMs
to a given student model in order to reduce model discrepancies. Specifically,
beyond providing well-generalized original knowledge from teachers, CustomKD
aligns the features of teachers to those of students, making it easy for
students to understand and overcome the large model discrepancy overall.
CustomKD significantly improves the performances of edge models in scenarios
with unlabeled data such as unsupervised domain adaptation (e.g., OfficeHome
and DomainNet) and semi-supervised learning (e.g., CIFAR-100 with 400 labeled
samples and ImageNet with 1% labeled samples), achieving the new
state-of-the-art performances.",['cs.CV'],False,,,,Normal forms and geometric structures on Hopf manifolds,"CustomKD: Customizing Large Vision Foundation for Edge Model Improvement
  via Knowledge Distillation"
neg-d2-666,2025-01-10,,2501.06429," Vertical Federated Learning (VFL) is a well-known FL variant that enables
multiple parties to collaboratively train a model without sharing their raw
data. Existing VFL approaches focus on overlapping samples among different
parties, while their performance is constrained by the limited number of these
samples, leaving numerous non-overlapping samples unexplored. Some previous
work has explored techniques for imputing missing values in samples, but often
without adequate attention to the quality of the imputed samples. To address
this issue, we propose a Reliable Imputed-Sample Assisted (RISA) VFL framework
to effectively exploit non-overlapping samples by selecting reliable imputed
samples for training VFL models. Specifically, after imputing non-overlapping
samples, we introduce evidence theory to estimate the uncertainty of imputed
samples, and only samples with low uncertainty are selected. In this way,
high-quality non-overlapping samples are utilized to improve VFL model.
Experiments on two widely used datasets demonstrate the significant performance
gains achieved by the RISA, especially with the limited overlapping samples,
e.g., a 48% accuracy gain on CIFAR-10 with only 1% overlapping samples.","['cs.LG', 'stat.ML']",2503.12146," Let $\mathcal{D}_{n} \subset \mathbb{N}$ denote the set of the $\tau(n)$
divisors of $n$. We study the function $$ D_{n}(X,Y):=|\{d \in
\mathcal{D}_{n}:\ X \le d \le X+Y\}| $$ for $Y \le X$.",['math.NT'],False,,,,Reliable Imputed-Sample Assisted Vertical Federated Learning,Divisors of an Integer in a Short Interval
neg-d2-667,2025-03-14,,2503.11981," Text-to-3D generation saw dramatic advances in recent years by leveraging
Text-to-Image models. However, most existing techniques struggle with
compositional prompts, which describe multiple objects and their spatial
relationships. They often fail to capture fine-grained inter-object
interactions. We introduce DecompDreamer, a Gaussian splatting-based training
routine designed to generate high-quality 3D compositions from such complex
prompts. DecompDreamer leverages Vision-Language Models (VLMs) to decompose
scenes into structured components and their relationships. We propose a
progressive optimization strategy that first prioritizes joint relationship
modeling before gradually shifting toward targeted object refinement. Our
qualitative and quantitative evaluations against state-of-the-art text-to-3D
models demonstrate that DecompDreamer effectively generates intricate 3D
compositions with superior object disentanglement, offering enhanced control
and flexibility in 3D generation. Project page :
https://decompdreamer3d.github.io",['cs.CV'],2501.16839," Among generative neural models, flow matching techniques stand out for their
simple applicability and good scaling properties. Here, velocity fields of
curves connecting a simple latent and a target distribution are learned. Then
the corresponding ordinary differential equation can be used to sample from a
target distribution, starting in samples from the latent one. This paper
reviews from a mathematical point of view different techniques to learn the
velocity fields of absolutely continuous curves in the Wasserstein geometry. We
show how the velocity fields can be characterized and learned via i) transport
plans (couplings) between latent and target distributions, ii) Markov kernels
and iii) stochastic processes, where the latter two include the coupling
approach, but are in general broader. Besides this main goal, we show how flow
matching can be used for solving Bayesian inverse problems, where the
definition of conditional Wasserstein distances plays a central role. Finally,
we briefly address continuous normalizing flows and score matching techniques,
which approach the learning of velocity fields of curves from other directions.","['cs.LG', 'math.PR']",False,,,,"DecompDreamer: Advancing Structured 3D Asset Generation with
  Multi-Object Decomposition and Gaussian Splatting","Flow Matching: Markov Kernels, Stochastic Processes and Transport Plans"
neg-d2-668,2025-02-12,,2502.08308," A pruning-aware adaptive gradient method is proposed which classifies the
variables in two sets before updating them using different strategies. This
technique extends the ``relevant/irrelevant"" approach of Ding (2019) and Zimmer
et al. (2022) and allows a posteriori sparsification of the solution of model
parameter fitting problems. The new method is proved to be convergent with a
global rate of decrease of the averaged gradient's norm of the form
$\calO(\log(k)/\sqrt{k+1})$. Numerical experiments on several applications show
that it is competitive.",['math.OC'],2503.13535," The advent of generative artificial intelligence (GAI) has brought about a
notable surge in the field of education. The use of GAI to support learning is
becoming increasingly prevalent among students. However, the manner and extent
of its utilisation vary considerably from one individual to another. And
researches about student's utilisation and perceptions of GAI remains
relatively scarce. To gain insight into the issue, this paper proposed a
hybrid-survey method to examine the impact of GAI on students across four
different grades in six key areas (LIPSAL): learning interest, independent
learning, problem solving, self-confidence, appropriate use, and learning
enjoyment. Firstly, through questionnaire, we found that among LIPSAL, GAI has
the greatest impact on the concept of appropriate use, the lowest level of
learning interest and self-confidence. Secondly, a comparison of four grades
revealed that the high and low factors of LIPSAL exhibited grade-related
variation, and college students exhibited a higher level than high school
students across LIPSAL. Thirdly, through interview, the students demonstrated a
comprehensive understanding of the application of GAI. We found that students
have a positive attitude towards GAI and are very willing to use it, which is
why GAI has grown so rapidly in popularity. They also told us prospects and
challenges in using GAI. In the future, as GAI matures technologically, it will
have an greater impact on students. These findings may help better understand
usage by different students and inform future research in digital education.","['cs.CY', 'cs.AI']",False,,,,prunAdag: an adaptive pruning-aware gradient method,"Unlocking Learning Potentials: The Transformative Effect of Generative
  AI in Education Across Grade Levels"
neg-d2-669,2025-02-07,,2502.04988," Learned Image Compression (LIC) has explored various architectures, such as
Convolutional Neural Networks (CNNs) and transformers, in modeling image
content distributions in order to achieve compression effectiveness. However,
achieving high rate-distortion performance while maintaining low computational
complexity (\ie, parameters, FLOPs, and latency) remains challenging. In this
paper, we propose a hybrid Convolution and State Space Models (SSMs) based
image compression framework, termed \textit{CMamba}, to achieve superior
rate-distortion performance with low computational complexity. Specifically,
CMamba introduces two key components: a Content-Adaptive SSM (CA-SSM) module
and a Context-Aware Entropy (CAE) module. First, we observed that SSMs excel in
modeling overall content but tend to lose high-frequency details. In contrast,
CNNs are proficient at capturing local details. Motivated by this, we propose
the CA-SSM module that can dynamically fuse global content extracted by SSM
blocks and local details captured by CNN blocks in both encoding and decoding
stages. As a result, important image content is well preserved during
compression. Second, our proposed CAE module is designed to reduce spatial and
channel redundancies in latent representations after encoding. Specifically,
our CAE leverages SSMs to parameterize the spatial content in latent
representations. Benefiting from SSMs, CAE significantly improves spatial
compression efficiency while reducing spatial content redundancies. Moreover,
along the channel dimension, CAE reduces inter-channel redundancies of latent
representations via an autoregressive manner, which can fully exploit prior
knowledge from previous channels without sacrificing efficiency. Experimental
results demonstrate that CMamba achieves superior rate-distortion performance.","['eess.IV', 'cs.CV']",2501.07166," Combinatorial medication recommendation(CMR) is a fundamental task of
healthcare, which offers opportunities for clinical physicians to provide more
precise prescriptions for patients with intricate health conditions,
particularly in the scenarios of long-term medical care. Previous research
efforts have sought to extract meaningful information from electronic health
records (EHRs) to facilitate combinatorial medication recommendations. Existing
learning-based approaches further consider the chemical structures of
medications, but ignore the textual medication descriptions in which the
functionalities are clearly described. Furthermore, the textual knowledge
derived from the EHRs of patients remains largely underutilized. To address
these issues, we introduce the Natural Language-Assisted Multi-modal Medication
Recommendation(NLA-MMR), a multi-modal alignment framework designed to learn
knowledge from the patient view and medication view jointly. Specifically,
NLA-MMR formulates CMR as an alignment problem from patient and medication
modalities. In this vein, we employ pretrained language models(PLMs) to extract
in-domain knowledge regarding patients and medications, serving as the
foundational representation for both modalities. In the medication modality, we
exploit both chemical structures and textual descriptions to create medication
representations. In the patient modality, we generate the patient
representations based on textual descriptions of diagnosis, procedure, and
symptom. Extensive experiments conducted on three publicly accessible datasets
demonstrate that NLA-MMR achieves new state-of-the-art performance, with a
notable average improvement of 4.72% in Jaccard score. Our source code is
publicly available on https://github.com/jtan1102/NLA-MMR_CIKM_2024.",['cs.AI'],False,,,,CMamba: Learned Image Compression with State Space Models,Natural Language-Assisted Multi-modal Medication Recommendation
neg-d2-670,2025-01-02,,2501.01149," AI agents have become increasingly prevalent in recent years, driven by
significant advancements in the field of large language models (LLMs). Mobile
GUI agents, a subset of AI agents, are designed to autonomously perform tasks
on mobile devices. While numerous studies have introduced agents, datasets, and
benchmarks to advance mobile GUI agent research, many existing datasets focus
on static frame evaluations and fail to provide a comprehensive platform for
assessing performance on real-world, in-the-wild tasks. To address this gap, we
present Android Agent Arena (A3), a novel evaluation platform. Unlike existing
in-the-wild systems, A3 offers: (1) meaningful and practical tasks, such as
real-time online information retrieval and operational instructions; (2) a
larger, more flexible action space, enabling compatibility with agents trained
on any dataset; and (3) automated business-level LLM-based evaluation process.
A3 includes 21 widely used general third-party apps and 201 tasks
representative of common user scenarios, providing a robust foundation for
evaluating mobile GUI agents in real-world situations and a new autonomous
evaluation process for less human labor and coding expertise. The project is
available at https://yuxiangchai.github.io/Android-Agent-Arena/.",['cs.AI'],2501.13507," In this paper, we address the problem of manipulating multi-particle
aggregates using a bimanual robotic system. Our approach enables the autonomous
transport of dispersed particles through a series of shaping and pushing
actions using robotically-controlled tools. Achieving this advanced
manipulation capability presents two key challenges: high-level task planning
and trajectory execution. For task planning, we leverage Vision Language Models
(VLMs) to enable primitive actions such as tool affordance grasping and
non-prehensile particle pushing. For trajectory execution, we represent the
evolving particle aggregate's contour using truncated Fourier series, providing
efficient parametrization of its closed shape. We adaptively compute trajectory
waypoints based on group cohesion and the geometric centroid of the aggregate,
accounting for its spatial distribution and collective motion. Through
real-world experiments, we demonstrate the effectiveness of our methodology in
actively shaping and manipulating multi-particle aggregates while maintaining
high system cohesion.",['cs.RO'],False,,,,A3: Android Agent Arena for Mobile GUI Agents,"Iterative Shaping of Multi-Particle Aggregates based on Action Trees and
  VLM"
neg-d2-671,2025-03-14,,2503.11801," We present Diffuse-CLoC, a guided diffusion framework for physics-based
look-ahead control that enables intuitive, steerable, and physically realistic
motion generation. While existing kinematics motion generation with diffusion
models offer intuitive steering capabilities with inference-time conditioning,
they often fail to produce physically viable motions. In contrast, recent
diffusion-based control policies have shown promise in generating physically
realizable motion sequences, but the lack of kinematics prediction limits their
steerability. Diffuse-CLoC addresses these challenges through a key insight:
modeling the joint distribution of states and actions within a single diffusion
model makes action generation steerable by conditioning it on the predicted
states. This approach allows us to leverage established conditioning techniques
from kinematic motion generation while producing physically realistic motions.
As a result, we achieve planning capabilities without the need for a high-level
planner. Our method handles a diverse set of unseen long-horizon downstream
tasks through a single pre-trained model, including static and dynamic obstacle
avoidance, motion in-betweening, and task-space control. Experimental results
show that our method significantly outperforms the traditional hierarchical
framework of high-level motion diffusion and low-level tracking.","['cs.GR', 'cs.LG', 'cs.RO']",2503.00069," Recent progress in large language models (LLMs) has focused on producing
responses that meet human expectations and align with shared values - a process
coined alignment. However, aligning LLMs remains challenging due to the
inherent disconnect between the complexity of human values and the narrow
nature of the technological approaches designed to address them. Current
alignment methods often lead to misspecified objectives, reflecting the broader
issue of incomplete contracts, the impracticality of specifying a contract
between a model developer, and the model that accounts for every scenario in
LLM alignment. In this paper, we argue that improving LLM alignment requires
incorporating insights from societal alignment frameworks, including social,
economic, and contractual alignment, and discuss potential solutions drawn from
these domains. Given the role of uncertainty within societal alignment
frameworks, we then investigate how it manifests in LLM alignment. We end our
discussion by offering an alternative view on LLM alignment, framing the
underspecified nature of its objectives as an opportunity rather than perfect
their specification. Beyond technical improvements in LLM alignment, we discuss
the need for participatory alignment interface designs.","['cs.CY', 'cs.AI', 'cs.CL']",False,,,,"Diffuse-CLoC: Guided Diffusion for Physics-based Character Look-ahead
  Control",Societal Alignment Frameworks Can Improve LLM Alignment
neg-d2-672,2025-03-04,,2503.02301," Direct kernel fuzzing is a targeted approach that focuses on specific areas
of the kernel, effectively addressing the challenges of frequent updates and
the inherent complexity of operating systems, which are critical
infrastructure. This paper introduces SyzAgent, a framework that integrates
LLMs with the state-of-the-art kernel fuzzer Syzkaller, where the LLMs are used
to guide the mutation and generation of test cases in real-time. We present
preliminary results demonstrating that this method is effective on around 67\%
cases in our benchmark during the experiment.",['cs.SE'],2503.01261," Image quantization is a crucial technique in image generation, aimed at
learning a codebook that encodes an image into a discrete token sequence.
Recent advancements have seen researchers exploring learning multi-modal
codebook (i.e., text-aligned codebook) by utilizing image caption semantics,
aiming to enhance codebook performance in cross-modal tasks. However, existing
image-text paired datasets exhibit a notable flaw in that the text descriptions
tend to be overly concise, failing to adequately describe the images and
provide sufficient semantic knowledge, resulting in limited alignment of text
and codebook at a fine-grained level. In this paper, we propose a novel
Text-Augmented Codebook Learning framework, named TA-VQ, which generates longer
text for each image using the visual-language model for improved text-aligned
codebook learning. However, the long text presents two key challenges: how to
encode text and how to align codebook and text. To tackle two challenges, we
propose to split the long text into multiple granularities for encoding, i.e.,
word, phrase, and sentence, so that the long text can be fully encoded without
losing any key semantic knowledge. Following this, a hierarchical encoder and
novel sampling-based alignment strategy are designed to achieve fine-grained
codebook-text alignment. Additionally, our method can be seamlessly integrated
into existing VQ models. Extensive experiments in reconstruction and various
downstream tasks demonstrate its effectiveness compared to previous
state-of-the-art approaches.",['cs.CV'],False,,,,Towards Large Language Model Guided Kernel Direct Fuzzing,"Towards Improved Text-Aligned Codebook Learning: Multi-Hierarchical
  Codebook-Text Alignment with Long Text"
neg-d2-673,2025-03-14,,2503.1126," This paper investigates the relationship between smart city initiatives and
evolving urbanization trends in the United States. The research addresses the
critical issue of rapid urban growth in the U.S. and explores how innovations
within the smart city paradigm influence urban development. Utilizing
principles from Urban Complexity Theory, this study identifies four key
variables relevant to smart cities and their impact on urbanization: smart city
technology, government policy, environmental sustainability, and socioeconomic
factors. A mixed-method approach, combining quantitative and qualitative
methodologies, was employed. A web-based survey (n=50) utilizing a five-point
Likert scale was conducted among residents of Manhattan, New York, and Capitol
Hill, Seattle. Results indicate that the implementation of smart city
technologies is significantly associated with shifts in population density,
land use diversification, and enhanced infrastructure dynamics. Additionally,
residents demonstrated preferences for smart cities based on efficient urban
mobility, environmental sustainability, and personal socioeconomic
improvements. The findings highlight essential considerations for urban
planners, policymakers, and employers. This study concludes that incorporating
the identified influential factors into strategic urban planning optimizes city
development to better accommodate growing urban populations.","['cs.CY', 'cs.ET']",2503.18033," Omnimatte aims to decompose a given video into semantically meaningful
layers, including the background and individual objects along with their
associated effects, such as shadows and reflections. Existing methods often
require extensive training or costly self-supervised optimization. In this
paper, we present OmnimatteZero, a training-free approach that leverages
off-the-shelf pre-trained video diffusion models for omnimatte. It can remove
objects from videos, extract individual object layers along with their effects,
and composite those objects onto new videos. We accomplish this by adapting
zero-shot image inpainting techniques for video object removal, a task they
fail to handle effectively out-of-the-box. We then show that self-attention
maps capture information about the object and its footprints and use them to
inpaint the object's effects, leaving a clean background. Additionally, through
simple latent arithmetic, object layers can be isolated and recombined
seamlessly with new video layers to produce new videos. Evaluations show that
OmnimatteZero not only achieves superior performance in terms of background
reconstruction but also sets a new record for the fastest Omnimatte approach,
achieving real-time performance with minimal frame runtime.",['cs.CV'],False,,,,"To Assess the Impact of Smart Cities on Urbanization Patterns in the
  United States","OmnimatteZero: Training-free Real-time Omnimatte with Pre-trained Video
  Diffusion Models"
neg-d2-674,2025-02-16,,2502.11343," In this paper, a new class of structured polynomials, which we dub the {\it
separable plus lower degree {\rm (SPLD in short)} polynomials}, is introduced.
The formal definition of an SPLD polynomial, which extends the concept of the
SPQ polynomial (Ahmadi et al. in Math Oper Res 48:1316--1343, 2023), is
defined. A type of bounded degree SOS hierarchy (BSOS-SPLD) is proposed to
efficiently solve the optimization problems with SPLD polynomials, and several
numerical examples are performed much better than the bounded degree SOS
hierarchy (Lasserre et al. in EURO J Comput Optim 5:87--117, 2017). An exact
SOS relaxation for a class of convex SPLD polynomial optimization problems is
proposed. Finally, an application of SPLD polynomials to polynomial regression
problems in statistics is presented.",['math.OC'],2503.13535," The advent of generative artificial intelligence (GAI) has brought about a
notable surge in the field of education. The use of GAI to support learning is
becoming increasingly prevalent among students. However, the manner and extent
of its utilisation vary considerably from one individual to another. And
researches about student's utilisation and perceptions of GAI remains
relatively scarce. To gain insight into the issue, this paper proposed a
hybrid-survey method to examine the impact of GAI on students across four
different grades in six key areas (LIPSAL): learning interest, independent
learning, problem solving, self-confidence, appropriate use, and learning
enjoyment. Firstly, through questionnaire, we found that among LIPSAL, GAI has
the greatest impact on the concept of appropriate use, the lowest level of
learning interest and self-confidence. Secondly, a comparison of four grades
revealed that the high and low factors of LIPSAL exhibited grade-related
variation, and college students exhibited a higher level than high school
students across LIPSAL. Thirdly, through interview, the students demonstrated a
comprehensive understanding of the application of GAI. We found that students
have a positive attitude towards GAI and are very willing to use it, which is
why GAI has grown so rapidly in popularity. They also told us prospects and
challenges in using GAI. In the future, as GAI matures technologically, it will
have an greater impact on students. These findings may help better understand
usage by different students and inform future research in digital education.","['cs.CY', 'cs.AI']",False,,,,SPLD polynomial optimization and bounded degree SOS hierarchies,"Unlocking Learning Potentials: The Transformative Effect of Generative
  AI in Education Across Grade Levels"
neg-d2-675,2025-03-21,,2503.17201," The commodity and widespread use of online shopping are having an
unprecedented impact on climate, with emission figures from key actors that are
easily comparable to those of a large-scale metropolis. Despite online shopping
being fueled by recommender systems (RecSys) algorithms, the role and potential
of the latter in promoting more sustainable choices is little studied. One of
the main reasons for this could be attributed to the lack of a dataset
containing carbon footprint emissions for the items. While building such a
dataset is a rather challenging task, its presence is pivotal for opening the
doors to novel perspectives, evaluations, and methods for RecSys research. In
this paper, we target this bottleneck and study the environmental role of
RecSys algorithms. First, we mine a dataset that includes carbon footprint
emissions for its items. Then, we benchmark conventional RecSys algorithms in
terms of accuracy and sustainability as two faces of the same coin. We find
that RecSys algorithms optimized for accuracy overlook greenness and that
longer recommendation lists are greener but less accurate. Then, we show that a
simple reranking approach that accounts for the item's carbon footprint can
establish a better trade-off between accuracy and greenness. This reranking
approach is modular, ready to use, and can be applied to any RecSys algorithm
without the need to alter the underlying mechanisms or retrain models. Our
results show that a small sacrifice of accuracy can lead to significant
improvements of recommendation greenness across all algorithms and list
lengths. Arguably, this accuracy-greenness trade-off could even be seen as an
enhancement of user satisfaction, particularly for purpose-driven users who
prioritize the environmental impact of their choices. We anticipate this work
will serve as the starting point for studying RecSys for more sustainable
recommendations.",['cs.IR'],2502.04659," Blockchains have revolutionized decentralized applications, with
composability enabling atomic, trustless interactions across smart contracts.
However, layer 2 (L2) scalability solutions like rollups introduce
fragmentation and hinder composability. Current cross-chain protocols,
including atomic swaps, bridges, and shared sequencers, lack the necessary
coordination mechanisms or rely on trust assumptions, and are thus not
sufficient to support full cross-rollup composability. This paper presents
$\mathsf{CRATE}$, a secure protocol for cross-rollup composability that ensures
all-or-nothing and serializable execution of cross-rollup transactions (CRTs).
$\mathsf{CRATE}$ supports rollups on distinct layer 1 (L1) chains, achieves
finality in 4 rounds on L1, and only relies on the underlying L1s and the
liveness of L2s. We introduce two formal models for CRTs, define atomicity
within them, and formally prove the security of $\mathsf{CRATE}$. We also
provide an implementation of $\mathsf{CRATE}$ along with a cross-rollup flash
loan application; our experiments demonstrate that $\mathsf{CRATE}$ is
practical in terms of gas usage on L1.",['cs.CR'],False,,,,"Towards Carbon Footprint-Aware Recommender Systems for Greener Item
  Recommendation",$\mathsf{CRATE}$: Cross-Rollup Atomic Transaction Execution
neg-d2-676,2025-01-07,,2501.04085," We present the Cosmic Evolution Early Release Science (CEERS) Survey, a 77.2
hour Director's Discretionary Early Release Science Program. CEERS
demonstrates, tests, and validates efficient extragalactic surveys using
coordinated, overlapping parallel observations with the JWST instrument suite,
including NIRCam and MIRI imaging, NIRSpec low (R~100) and medium (R~1000)
resolution spectroscopy, and NIRCam slitless grism (R~1500) spectroscopy. CEERS
targets the Hubble Space Telescope-observed region of the Extended Groth Strip
(EGS) field, supported by a rich set of multiwavelength data. CEERS facilitated
immediate community science in both of the extragalactic core JWST science
drivers ``First Light"" and ``Galaxy Assembly,"" including: 1) The discovery and
characterization of large samples of galaxies at z >~ 10 from ~90 arcmin^2 of
NIRCam imaging, constraining their abundance and physical nature; 2) Deep
spectra of >1000 galaxies, including dozens of galaxies at 6<z<10, enabling
redshift measurements and constraints on the physical conditions of
star-formation and black hole growth via line diagnostics; 3) Quantifying the
first bulge, bar and disk structures at z>3; and 4) Characterizing galaxy
mid-IR emission with MIRI to study dust-obscured star-formation and
supermassive black hole growth at z~1-3. As a legacy product for the community,
the CEERS team has provided several data releases, accompanied by detailed
notes on the data reduction procedures and notebooks to aid in reproducibility.
In addition to an overview of the survey and quality of the data, we provide
science highlights from the first two years with CEERS data.",['astro-ph.GA'],2502.09455," Formulas for the combined nuclear-recoil and finite-nuclear-size effects of
order $(Z\,\alpha)^5$ and $(Z\,\alpha)^6$ are derived without any expansion in
the nuclear charge radius $r_C$, making them applicable to both electronic and
muonic atoms. The obtained results are particularly relevant for high-precision
determinations of root-mean-square charge radii from muonic atom spectroscopy.
We demonstrate that calculations of the atomic isotope shift based on the
widely used Breit approximation give rise to an unphysical nuclear-size
contribution that is linear in the nuclear charge radius $r_C$ at order
$(Z\,\alpha)^5$. This spurious term vanishes in a full QED treatment, leaving
the correct contribution quadratic in $r_C$. For electronic atoms, this
quadratic term is significantly smaller than the spurious linear contribution.",['physics.atom-ph'],False,,,,The Cosmic Evolution Early Release Science Survey (CEERS),Recoil nuclear size corrections in hydrogenic systems
neg-d2-677,2025-01-22,,2501.1326," On the kagome lattice, electrons benefit from the simultaneous presence of
band topology, flat electronic bands, and van Hove singularities, forming
competing or cooperating orders. Understanding the interrelation between these
distinct order parameters remains a significant challenge, leaving much of the
associated physics unexplored. In the kagome superconductor KV3Sb5, which
exhibits a charge density wave (CDW) state below T = 78 K, we uncover an
unpredicted field-induced phase transition below 6 K. The observed transition
is marked by a hysteretic anomaly in the resistivity, nonlinear electrical
transport, and a change in the symmetry of the electronic response as probed
via the angular dependence of the magnetoresistivity. These observations
surprisingly suggest the emergence of an unanticipated broken symmetry state
coexisting with the original CDW. To understand this experimental observation,
we developed a theoretical minimal model for the normal state inside the
high-temperature parent CDW phase where an incommensurate CDW order emerges as
an instability sub-leading to superconductivity. The incommensurate CDW emerges
when superconducting fluctuations become fully suppressed by large magnetic
fields. Our results suggest that, in kagome superconductors, quantum states can
either coexist or are nearly degenerate in energy, indicating that these are
rich platforms to expose new correlated phenomena.","['cond-mat.str-el', 'cond-mat.mes-hall', 'cond-mat.mtrl-sci']",2501.01838," The kagome lattice has garnered significant attention due to its ability to
host quantum spin Fermi liquid states. Recently, the combination of unique
lattice geometry, electron-electron correlations, and adjustable magnetism in
solid kagome materials has led to the discovery of numerous fascinating quantum
properties. These include unconventional superconductivity, charge and spin
density waves (CDW/SDW), pair density waves (PDW), and Chern insulator phases.
These emergent states are closely associated with the distinctive
characteristics of the kagome lattice's electronic structure, such as van Hove
singularities, Dirac fermions, and flat bands, which can exhibit exotic
quasi-particle excitations under different symmetries and magnetic conditions.
Recently, various quantum kagome materials have been developed, typically
consisting of kagome layers stacked along the $z$-axis with atoms either
filling the geometric centers of the kagome lattice or embedded between the
layers. In this topical review, we begin by introducing the fundamental
properties of several kagome materials. To gain an in-depth understanding of
the relationship between topology and correlation, we then discuss the complex
phenomena observed in these systems. These include the simplest kagome metal
$T_3X$, kagome intercalation metal $TX$, and the ternary compounds $AT_6X_6$
and $RT_3X_5$ ($A$ = Li, Mg, Ca, or rare earth; $T$ = V, Cr, Mn, Fe, Co, Ni;
$X$ = Sn, Ge; $R$ = K, Rb, Cs). Finally, we provide a perspective on future
experimental work in this field.","['cond-mat.str-el', 'cond-mat.mtrl-sci', 'cond-mat.supr-con']",False,,,,Field induced density wave in a kagome superconductor,Electronic band structures of topological kagome materials
neg-d2-678,2025-03-20,,2503.16779," Tool learning can further broaden the usage scenarios of large language
models (LLMs). However most of the existing methods either need to finetune
that the model can only use tools seen in the training data, or add tool
demonstrations into the prompt with lower efficiency. In this paper, we present
a new Tool Learning method Chain-of-Tools. It makes full use of the powerful
semantic representation capability of frozen LLMs to finish tool calling in CoT
reasoning with a huge and flexible tool pool which may contain unseen tools.
Especially, to validate the effectiveness of our approach in the massive unseen
tool scenario, we construct a new dataset SimpleToolQuestions. We conduct
experiments on two numerical reasoning benchmarks (GSM8K-XL and FuncQA) and two
knowledge-based question answering benchmarks (KAMEL and SimpleToolQuestions).
Experimental results show that our approach performs better than the baseline.
We also identify dimensions of the model output that are critical in tool
selection, enhancing the model interpretability. Our code and data are
available at: https://github.com/fairyshine/Chain-of-Tools .","['cs.CL', 'cs.AI']",2501.04162," Let $N$ and $p$ be prime numbers with $p \geq 5$ such that $p || (N + 1)$. In
a previous paper, we showed that there is a cuspform $f$ of weight 2 and level
$\Gamma_0(N^2)$ whose $\ell$-th Fourier coefficient is congruent to $\ell + 1$
modulo a prime above $p$ for all primes $\ell$. In this paper, we prove that
this form $f$ is unique up to Galois conjugacy, and the extension of
$\mathbb{Z}_p$ generated by the coefficients of $f$ is exactly
$\mathbb{Z}_p[\zeta_p + \zeta_p^{-1}]$. We also prove similar results when a
higher power of $p$ divides $N + 1$.",['math.NT'],False,,,,"Chain-of-Tools: Utilizing Massive Unseen Tools in the CoT Reasoning of
  Frozen Language Models",The Eisenstein ideal at prime-square level has constant rank
neg-d2-679,2025-02-01,,2502.00322," Query-focused summarization (QFS) gives a summary of documents to answer a
query. Past QFS work assumes queries have one answer, ignoring debatable ones
(Is law school worth it?). We introduce Debatable QFS (DQFS), a task to create
summaries that answer debatable queries via documents with opposing
perspectives; summaries must comprehensively cover all sources and balance
perspectives, favoring no side. These goals elude LLM QFS systems, which: 1)
lack structured content plans, failing to guide LLMs to write balanced
summaries, and 2) use the same query to retrieve contexts across documents,
failing to cover all perspectives specific to each document's content. To
overcome this, we design MODS, a multi-LLM framework mirroring human panel
discussions. MODS treats documents as individual Speaker LLMs and has a
Moderator LLM that picks speakers to respond to tailored queries for planned
topics. Speakers use tailored queries to retrieve relevant contexts from their
documents and supply perspectives, which are tracked in a rich outline,
yielding a content plan to guide the final summary. Experiments on
ConflictingQA with controversial web queries and DebateQFS, our new dataset of
debate queries from Debatepedia, show MODS beats SOTA by 38-59% in topic
paragraph coverage and balance, based on new citation metrics. Users also find
MODS's summaries to be readable and more balanced.","['cs.CL', 'cs.IR']",2503.15988," Spectra of vibrational overtone and combination bands from vibrational ground
state of HCNH+ were measured using an action spectroscopy technique with active
background suppression in a cryogenic 22 pole radio frequency ion trap
apparatus. Spectroscopic constants for the upper vibrational levels of the
transitions were determined with vibrational band origins being 6846.77981(90)
$\text{cm}^{-1}$ ($2\nu_1$ , NH stretch), 6640.47624(43) $\text{cm}^{-1}$
($\nu_1 + \nu_2$), 6282.03578(63) $\text{cm}^{-1}$ ($2\nu_2$, CH stretch), and
6588.4894(20) $\text{cm}^{-1}$ ($\nu_2 + \nu_3 + 2\nu_5^0$). State of the art
ab initio VCI calculations up to 10000 $\text{cm}^{-1}$ complement the
experimental data.","['astro-ph.GA', 'physics.atom-ph', 'physics.plasm-ph']",False,,,,"MODS: Moderating a Mixture of Document Speakers to Summarize Debatable
  Queries in Document Collections",Rovibrational Overtone and Combination Bands of the HCNH+ Ion
neg-d2-680,2025-03-14,,2503.11567," A modification of the optical model for rough surfaces, implemented in Geant4
as a part of the unified model, is suggested. The modified model takes into
account the variation of the interaction probability of the photon with the
microfacet based on the relative orientation of the photon and the sampled
microfacet's normal. The implementation is using a rejection algorithm and
assumes the interaction probability to be proportional to the projection of the
microfacet area on the plane perpendicular to the photon direction. A
comparison of the results obtained with the original and the modified models,
as well as obtained in direct Monte Carlo simulations are presented for several
test surfaces constructed using a pattern of elementary geometrical shapes.",['physics.ins-det'],2502.18261," Traditionally, the impact of minimum wages on employment has been studied,
and it is generally believed to have a negative effect. Yet, some recent
studies have shown that the impact of minimum wages on employment can sometimes
be positive. In addition, certain recent proposals set a higher minimum wage
than the wage earned by some high-productivity workers. However, the impact of
minimum wages on employment has been primarily studied on low-skilled workers,
whereas there is limited research on high-skilled workers. To address this gap
and examine the effects of minimum wages on high-productivity workers'
employment, I construct a macroeconomic model incorporating productivity
fluctuations, incomplete markets, directed search, and on-the-job search and
compare the steady-state distributions between the baseline model and the model
with a minimum wage. As a result, binding minimum wages increase the
unemployment rate of both low and high-productivity workers.","['econ.GN', 'q-fin.EC']",False,,,,"Microfacet projected area-based correction for unified model of Geant4
  for rough surfaces","The effect of minimum wages on employment in the presence of
  productivity fluctuations"
neg-d2-681,2025-01-23,,2501.13616," Altermagnetism has attracted considerable attention for its remarkable
combination of spin-polarized band structures and zero net magnetization,
making it a promising candidate for spintronics applications. We demonstrate
that this magnetic phase represents a case of ``unconventional magnetism,""
first proposed nearly two decades ago by one of the present authors as part of
a broader framework for understanding Landau-Pomeranchuk instabilities in the
spin channel, driven by many-body interactions. By systematically analyzing the
altermagnetism in RuO$_2$ with first-principles calculations, we reconcile
conflicting experimental and theoretical reports by attributing it to RuO$_2$'s
proximity to a quantum phase transition. We emphasize the critical role of
tuning parameters, such as the Hubbard $U$, hole doping, and epitaxial strain,
in modulating quasiparticle interactions near the Fermi surface. This work
provides fresh insights into the origin and tunability of altermagnetism in
RuO$_2$, highlighting its potential as a platform for investigating quantum
phase transitions and the broader realm of unconventional magnetism.",['cond-mat.mtrl-sci'],2501.17202," An ideal multimodal agent should be aware of the quality of its input
modalities. Recent advances have enabled large language models (LLMs) to
incorporate auditory systems for handling various speech-related tasks.
However, most audio LLMs remain unaware of the quality of the speech they
process. This limitation arises because speech quality evaluation is typically
excluded from multi-task training due to the lack of suitable datasets. To
address this, we introduce the first natural language-based speech evaluation
corpus, generated from authentic human ratings. In addition to the overall Mean
Opinion Score (MOS), this corpus offers detailed analysis across multiple
dimensions and identifies causes of quality degradation. It also enables
descriptive comparisons between two speech samples (A/B tests) with human-like
judgment. Leveraging this corpus, we propose an alignment approach with LLM
distillation (ALLD) to guide the audio LLM in extracting relevant information
from raw speech and generating meaningful responses. Experimental results
demonstrate that ALLD outperforms the previous state-of-the-art regression
model in MOS prediction, with a mean square error of 0.17 and an A/B test
accuracy of 98.6%. Additionally, the generated responses achieve BLEU scores of
25.8 and 30.2 on two tasks, surpassing the capabilities of task-specific
models. This work advances the comprehensive perception of speech signals by
audio LLMs, contributing to the development of real-world auditory and sensory
intelligent agents.","['cs.SD', 'cs.CL', 'eess.AS']",False,,,,"Fragile Unconventional Magnetism in RuO$_2$ by Proximity to
  Landau-Pomeranchuk Instability",Audio Large Language Models Can Be Descriptive Speech Quality Evaluators
neg-d2-682,2025-03-23,,2503.18182," In this work, we apply topic modeling using Non-Negative Matrix Factorization
(NMF) on the COVID-19 Open Research Dataset (CORD-19) to uncover the underlying
thematic structure and its evolution within the extensive body of COVID-19
research literature. NMF factorizes the document-term matrix into two
non-negative matrices, effectively representing the topics and their
distribution across the documents. This helps us see how strongly documents
relate to topics and how topics relate to words. We describe the complete
methodology which involves a series of rigorous pre-processing steps to
standardize the available text data while preserving the context of phrases,
and subsequently feature extraction using the term frequency-inverse document
frequency (tf-idf), which assigns weights to words based on their frequency and
rarity in the dataset. To ensure the robustness of our topic model, we conduct
a stability analysis. This process assesses the stability scores of the NMF
topic model for different numbers of topics, enabling us to select the optimal
number of topics for our analysis. Through our analysis, we track the evolution
of topics over time within the CORD-19 dataset. Our findings contribute to the
understanding of the knowledge structure of the COVID-19 research landscape,
providing a valuable resource for future research in this field.",['cs.CL'],2503.1702," Quantum kernels quantify similarity between data points by measuring the
inner product between quantum states, computed through quantum circuit
measurements. By embedding data into quantum systems, quantum kernel feature
maps, that may be classically intractable to compute, could efficiently exploit
high-dimensional Hilbert spaces to capture complex patterns. However, designing
effective quantum feature maps remains a major challenge. Many quantum kernels,
such as the fidelity kernel, suffer from exponential concentration, leading to
near-identity kernel matrices that fail to capture meaningful data correlations
and lead to overfitting and poor generalization. In this paper, we propose a
novel strategy for constructing quantum kernels that achieve good
generalization performance, drawing inspiration from benign overfitting in
classical machine learning. Our approach introduces the concept of local-global
quantum kernels, which combine two complementary components: a local quantum
kernel based on measurements of small subsystems and a global quantum kernel
derived from full-system measurements. Through numerical experiments, we
demonstrate that local-global quantum kernels exhibit benign overfitting,
supporting the effectiveness of our approach in enhancing quantum kernel
methods.","['quant-ph', 'cs.LG', 'stat.ML']",False,,,,"Exploring Topic Trends in COVID-19 Research Literature using
  Non-Negative Matrix Factorization",Benign Overfitting with Quantum Kernels
neg-d2-683,2025-03-15,,2503.13535," The advent of generative artificial intelligence (GAI) has brought about a
notable surge in the field of education. The use of GAI to support learning is
becoming increasingly prevalent among students. However, the manner and extent
of its utilisation vary considerably from one individual to another. And
researches about student's utilisation and perceptions of GAI remains
relatively scarce. To gain insight into the issue, this paper proposed a
hybrid-survey method to examine the impact of GAI on students across four
different grades in six key areas (LIPSAL): learning interest, independent
learning, problem solving, self-confidence, appropriate use, and learning
enjoyment. Firstly, through questionnaire, we found that among LIPSAL, GAI has
the greatest impact on the concept of appropriate use, the lowest level of
learning interest and self-confidence. Secondly, a comparison of four grades
revealed that the high and low factors of LIPSAL exhibited grade-related
variation, and college students exhibited a higher level than high school
students across LIPSAL. Thirdly, through interview, the students demonstrated a
comprehensive understanding of the application of GAI. We found that students
have a positive attitude towards GAI and are very willing to use it, which is
why GAI has grown so rapidly in popularity. They also told us prospects and
challenges in using GAI. In the future, as GAI matures technologically, it will
have an greater impact on students. These findings may help better understand
usage by different students and inform future research in digital education.","['cs.CY', 'cs.AI']",2503.09495," The calibration of the CR39 and Makrofol Nuclear Track Detectors of the
MoEDAL experiment at the CERN-LHC was performed by exposing stacks of detector
foils to heavy ion beams with energies ranging from 340 MeV/nucleon to 150
GeV/nucleon. After chemical etching, the base areas and lengths of etch-pit
cones were measured using automatic and manual optical microscopes. The
response of the detectors, as measured by the ratio of the track-etching rate
over the bulk-etching rate, was determined over a range extending from their
threshold at Z/$\beta\sim7$ and $\sim50$ for CR39 and Makrofol, respectively,
up to Z/$\beta\sim92$",['physics.ins-det'],False,,,,"Unlocking Learning Potentials: The Transformative Effect of Generative
  AI in Education Across Grade Levels","Calibration of Solid State Nuclear Track Detectors for Rare Event
  Searches"
neg-d2-684,2025-02-25,,2502.18601," The rapid advancements in data-driven methodologies have underscored the
critical importance of ensuring data quality. Consequently, detecting
out-of-distribution (OOD) data has emerged as an essential task to maintain the
reliability and robustness of data-driven models, in general, and machine and
deep learning models, in particular. In this study, we leveraged the convex
hull property of a dataset and the fact that anomalies highly contribute to the
increase of the CH's volume to propose a novel anomaly detection algorithm. Our
algorithm computes the CH's volume as an increasing number of data points are
removed from the dataset to define a decision line between OOD and
in-distribution data points. We compared the proposed algorithm to seven widely
used anomaly detection algorithms over ten datasets, showing comparable results
for state-of-the-art (SOTA) algorithms. Moreover, we show that with a
computationally cheap and simple check, one can detect datasets that are
well-suited for the proposed algorithm which outperforms the SOTA anomaly
detection algorithms.",['cs.LG'],2501.14229," The Kitaev honeycomb model has received significant attention for its exactly
solvable quantum spin liquid ground states and fractionalized excitations. For
realizing the model, layered cobalt oxides have been considered a promising
platform. Yet, in contrast to the conventional wisdom about single-$\mathbf{q}$
zigzag magnetic order inferred from previous studies of the Na$_2$IrO$_3$ and
$\alpha$-RuCl$_3$ candidate materials, recent experiments on two of the
representative honeycomb cobalt oxides, hexagonal Na$_2$Co$_2$TeO$_6$ and
monoclinic Na$_3$Co$_2$SbO$_6$, have uncovered evidence for more complex
multi-$\mathbf{q}$ variants of the zigzag order. This review surveys on
experimental strategies to distinguish between single- and multi-$\mathbf{q}$
orders, along with the crystallographic symmetries of the cobalt oxides in
comparison to the previously studied systems. General formation mechanism of
multi-$\mathbf{q}$ order is also briefly discussed. The goal is to provide some
rationales for examining the relevance of multi-$\mathbf{q}$ order in the
honeycomb cobalt oxides, along with its implications on the microscopic model
of these intriguing quantum magnets.",['cond-mat.str-el'],False,,,,Tighten The Lasso: A Convex Hull Volume-based Anomaly Detection Method,"On the multi-$\mathbf{q}$ characteristics of magnetic ground states of
  honeycomb cobalt oxides"
neg-d2-685,2025-02-22,,2502.1612," Data-driven inverse optimization seeks to estimate unknown parameters in an
optimization model from observations of optimization solutions. Many existing
methods are ineffective in handling noisy and suboptimal solution observations
and also suffer from computational challenges. In this paper, we build a
connection between inverse optimization and the Fenchel-Young (FY) loss
originally designed for structured prediction, proposing a FY loss approach to
data-driven inverse optimization. This new approach is amenable to efficient
gradient-based optimization, hence much more efficient than existing methods.
We provide theoretical guarantees for the proposed method and use extensive
simulation and real-data experiments to demonstrate its significant advantage
in parameter estimation accuracy, decision error and computational speed.","['math.OC', 'stat.ML']",2503.14237," Popular video training methods mainly operate on a fixed number of tokens
sampled from a predetermined spatiotemporal grid, resulting in sub-optimal
accuracy-computation trade-offs due to inherent video redundancy. They also
lack adaptability to varying computational budgets for downstream tasks,
hindering applications of the most competitive model in real-world scenes. We
thus propose a new test setting, Token Optimization, for maximized input
information across budgets, which optimizes the size-limited set of input
tokens through token selection from more suitably sampled videos. To this end,
we propose a novel augmentation tool termed Flux. By making the sampling grid
flexible and leveraging token selection, it is easily adopted in most popular
video training frameworks, boosting model robustness with nearly no additional
cost. We integrate Flux in large-scale video pre-training, and the resulting
FluxViT establishes new state-of-the-art results across extensive tasks at
standard costs. Notably, with 1/4 tokens only, it can still match the
performance of previous state-of-the-art models with Token Optimization,
yielding nearly 90\% savings. All models and data are available at
https://github.com/OpenGVLab/FluxViT.",['cs.CV'],False,,,,A Fenchel-Young Loss Approach to Data-Driven Inverse Optimization,Make Your Training Flexible: Towards Deployment-Efficient Video Models
neg-d2-686,2025-01-24,,2501.14331," The sources of cosmic rays between the knee and the ankle are still debated.
The Galactic wind and its termination shock have been proposed to contribute to
this transition between Galactic and extragalactic origin, but another
possibility is large-scale shock structures from local sources in the Milky
Way. In this paper, we investigate CR transport in a time-dependent landscape
of shocks in the Galactic halo. These shocks could result from local outbursts,
e.g. starforming regions and superbubbles. CRs re-accelerated at such shocks
can reach energies above the knee. Since the shocks are closer to the Galaxy
than a termination shock and CRs escape downstream, they can propagate back
more easily. With such outbursts happening frequently, shocks will interact.
This interaction could adjust the CR spectrum, particularly for the particles
that are able to be accelerated at two shocks simultaneously. The transport and
acceleration of CRs at the shock is modeled by Stochastic Differential
Equations (SDEs) within the public CR propagation framework CRPropa. We
developed extensions for time-dependent wind profiles and for the first time
connected the code to hydrodynamic simulations, which were run with the public
Athena++ code. We find that, depending on the concrete realization of the
diffusion tensor, a significant fraction of CRs can make it back to the Galaxy.
These could contribute to the observed spectrum around and above the CR knee
($E \gtrsim 10\,\mathrm{PeV}$). In contrast to simplified models, a simple
power-law does not describe the energy spectra well. Instead, for single
shocks, we find a flat spectrum ($E^{-2}$) at low energies, which steepens
gradually until it reaches an exponential decline. When shocks collide, the
energy spectra transiently become harder than $E^{-2}$ at high energies.",['astro-ph.HE'],2502.03152," In this work, we have carried out lattice simulations of $(2+1)$-flavor QCD
using highly improved staggered quarks at the physical pion mass on $32^3
\times 8$ and $48^3 \times 12$ lattices, with magnetic field strengths ranging
up to 0.8 GeV$^2$ and nonzero baryon chemical potentials employing the Taylor
expansion framework. We present lattice QCD continuum estimate results, along
with the magnetized hadron resonance and ideal gas comparisons, for the
leading-order Taylor expansion coefficients for bulk thermodynamic quantities
such as pressure, number density, energy density, and entropy density, focusing
on the significant impact of strong magnetic fields.","['hep-lat', 'hep-ph', 'hep-th']",False,,,,Cosmic ray transport and acceleration in an evolving shock landscape,"QCD Equation of State with Strong Magnetic Fields and Nonzero Baryon
  Density"
neg-d2-687,2025-03-04,,2503.02762," Context: Compared to Class 0 protostars, the higher densities and lower
temperatures of the disk midplanes of Class I young stellar objects (YSOs)
limit the detectability of complex organic molecules (COMs). The elevated
luminosities of eruptive YSOs increase disk temperatures sublimating frozen
molecules and easing their detection.
  Aims: Our aim is to investigate the chemical composition of four FUor-like
Class I YSOs: L1551 IRS 5, Haro 5a IRS, V346 Nor, and OO Ser, and to compare
their abundances of COMs with other YSOs in the literature.
  Methods: We search for COMs line emission in ALMA Band 6 observations. We use
the CASSIS software to determine their column densities (N) and excitation
temperatures (T_ex) assuming local thermodynamical equilibrium.
  Results: We detect 249 transitions from 12 COMs. In L1551 IRS 5 we identified
CH3OH, 13CH3OH, CH318OH, CH2DOH, CH3CHO, CH3OCH3, CH3OCHO, CH3COCH3, C2H5OH,
C2H5CN, 13CH3CN, and CH3C15)N. Haro 5a IRS and OO Ser have emission from CH3OH,
CH3CHO, CH3OCH3, and CH3OCHO. CH3COCH3 is also detected in OO Ser. In V346 Nor
we found CH3OH, CH2DOH, CH3CHO, CH3OCH3, CH3OCHO, and C2H5CN. The emission of
COMs is compact in all targets. The analysis indicates their temperatures are
above 100K. The abundance ratios of COMs derived for these eruptive YSOs, as
well as for other protostars in the literature, span several orders of
magnitude without any clear differentiation between the eruptive and quiescent
YSOs. The column density of the main isotopologue of CH3OH should not be used
as a reference, as most of the lines are optically thick.
  Conclusions: The hot and compact emission of COMs indicates that the four
FUor-like targets are hot corino-like. Spectral studies of such objects can be
useful to investigate the complex organic chemistry at later evolutionary
stages than the usual Class 0 stage.","['astro-ph.GA', 'astro-ph.SR']",2502.19475," We study the response of mono-energetic stellar populations with initially
isotropic kinematics to impulsive and adiabatic changes to an underlying dark
matter potential. Half-light radii expand and velocity dispersions decrease as
enclosed dark matter is removed. The details of this expansion and cooling
depend on the time scale on which the underlying potential changes. In the
adiabatic regime, the product of half-light radius and average velocity
dispersion is conserved. We show that the stellar populations maintain
centrally isotropic kinematics throughout their adiabatic evolution, and their
densities can be approximated by a family of analytical radial profiles.
Metallicity gradients within the galaxy flatten as dark matter is slowly
removed. In the case of strong impulsive perturbations, stellar populations
develop power-law-like density tails with radially biased kinematics. We show
that the distribution of stellar binding energies within the dark matter halo
substantially widens after an impulsive perturbation, no matter the sign of the
perturbation. This allows initially energetically separated stellar populations
to mix, to the extent that previously chemo-dynamically distinct populations
may masquerade as a single population with large metallicity and energy spread.
Finally, we show that in response to an impulsive perturbation, stellar
populations that are deeply embedded in cored dark matter halos undergo a
series of damped oscillations before reaching a virialised equilibrium state,
driven by inefficient phase mixing in the harmonic potentials of cored halos.
This slow return to equilibrium adds substantial systematic uncertainty to
dynamical masses estimated from Jeans modeling or the virial theorem.",['astro-ph.GA'],False,,,,The hot corino-like chemistry of four FUor-like protostars,Impulsive mixing of stellar populations in dwarf spheroidal galaxies
neg-d2-688,2025-02-17,,2502.11846," We develop a machine learning approach to reconstructing the cosmological
initial conditions from late-time dark matter halo number density fields in
redshift space, with the goal of improving sensitivity to cosmological
parameters, and in particular primordial non-Gaussianity. Using an U-Net
architecture, our model achieves a cross-correlation accuracy of 44% for scales
out to $k = 0.4 \text{ h}/\text{Mpc}$ between reconstructed and true initial
conditions of Quijote 1 Gpc$^3$ simulation boxes with an average halo number
density of $\bar{n} = 4\times 10^{-4}$ (h/Mpc)$^{3}$ in the tracer field at
$z=0$ . We demonstrate that our reconstruction is likely to be optimal for this
setup and that it is highly effective at reducing redshift-space distortions.
Using a Fisher analysis, we show that reconstruction improves cosmological
parameter constraints derived from the power spectrum and bispectrum. By
combining the power spectrum monopole, quadrupole, and bispectrum monopole up
to $k_{\rm{max}} = 0.52 \text{ h}/\text{Mpc}$, our joint analysis of pre- and
post-reconstructed fields from the Quijote simulation suite finds improved
marginalized errors on all cosmological parameters. In particular,
reconstruction improves constraints on $f_{\rm{NL}}$ by factors of 1.33, 1.88,
and 1.57 for local, equilateral, and orthogonal shapes. Our findings
demonstrate the effectiveness of reconstruction in decoupling modes, mitigating
redshift-space distortions and maximizing information on cosmology. The results
provide important insights into the amount of cosmological information that can
be extracted from small scales, and can potentially be used to complement
standard analysis of observational data, upon further development.",['astro-ph.CO'],2502.13005," Characterizing bipolaron binding, and understanding how it depends on
electron-phonon interaction, is crucial to unraveling the nature of emergent
many-body states in strongly interacting electron-phonon systems. So far, most
studies of bipolarons have been limited to the Holstein model, in which the
coupling constant is momentum-independent. The paradigmatic example of
momentum-dependent electron-phonon interaction comes from the system in which
phonon distortions modify electron hopping, the SSH model. Already individual
polarons in the SSH model are richer than the Holstein model counterparts, and
feature a phase transition into the finite momentum ground state with
increasing electron-phonon interaction. In this paper, we use a variational
approach to study bipolarons in the one-dimensional SSH model and discuss their
ground state, dispersion, and excitation spectra. We explore the full parameter
range of the system, including the adiabatic regime of slow phonons, which was
inaccessible to previous theoretical studies. In agreement with earlier
studies, we find that in the anti-adiabatic strongly interacting regime,
bipolarons have low effective mass. By contrast, in the adiabatic case, we find
that increasing electron-phonon interactions results in an exponential increase
of the bipolaron mass. We establish the existence of multiple branches of bound
excited states of SSH bipolaron and discuss the signatures of these bound
states in dynamics. We show that in the anti-adiabatic regime, response
functions obey a parity selection rule, that imposes symmetry constraints on
the excitation spectra and provides a clear signature of SSH bipolarons.",['cond-mat.str-el'],False,,,,"Neural Network Reconstruction of Non-Gaussian Initial Conditions from
  Dark Matter Halos",Bipolaron dynamics in the one-dimensional SSH model
neg-d2-689,2025-03-10,,2503.07102," The paper proposes a novel Economic Model Predictive Control (EMPC) scheme
for Autonomous Surface Vehicles (ASVs) to simultaneously address path following
accuracy and energy constraints under environmental disturbances. By
formulating lateral deviations as energy-equivalent penalties in the cost
function, our method enables explicit trade-offs between tracking precision and
energy consumption. Furthermore, a motion-dependent decomposition technique is
proposed to estimate terminal energy costs based on vehicle dynamics. Compared
with the existing EMPC method, simulations with real-world ocean disturbance
data demonstrate the controller's energy consumption with a 0.06 energy
increase while reducing cross-track errors by up to 18.61. Field experiments
conducted on an ASV equipped with an Intel N100 CPU in natural lake
environments validate practical feasibility, achieving 0.22 m average
cross-track error at nearly 1 m/s and 10 Hz control frequency. The proposed
scheme provides a computationally tractable solution for ASVs operating under
resource constraints.","['eess.SY', 'cs.SY']",2501.0944," In this paper, we present a class of systems of non-local conservation laws
in one space-dimension incorporating time delay, which can be used to
investigate the interaction between autonomous and human-driven vehicles, each
characterized by a different reaction time and interaction range. We construct
approximate solutions using a Hilliges-Weidlich scheme and we provide uniform L
$\infty$ and BV estimates which ensure the convergence of the scheme, thus
obtaining existence of entropy weak solutions of bounded variation. Uniqueness
follows from an L 1 stability result derived from the entropy condition.
Additionally, we provide numerical simulations to illustrate applications to
mixed autonomous / human-driven traffic flow modeling. In particular, we show
that the presence of autonomous vehicles improves overall traffic flow and
stability.","['math.AP', 'cs.NA', 'math.NA']",False,,,,"Coordinated Energy-Trajectory Economic Model Predictive Control for
  Autonomous Surface Vehicles under Disturbances","A multi-class non-local macroscopic model with time delay for mixed
  autonomous / human-driven traffic"
neg-d2-690,2025-01-21,,2501.12379," Constant weight codes can arise from an input process sampled from a periodic
Markov chain. A previous result showed that, in general, polarization does not
occur for input-output processes with an underlying periodic Markov chain. In
this work, we show that if we fix the initial state of an underlying periodic
Markov chain, polarization does occur. Fixing the initial state is aligned with
ensuring a constant weight code.","['cs.IT', 'math.IT']",2501.03361," We report the results from a study of two massive ($M_{500c} > 6.0 \times
10^{14} M_{\odot}$) strong lensing clusters selected from the South Pole
Telescope cluster survey for their high Einstein radius ($R_E > 40''$),
SPT-CLJ2325$-$4111 and SPT-CLJ0049$-$2440. Ground-based and shallow HST imaging
indicated extensive strong lensing evidence in these fields, with giant arcs
spanning 18\arcsec\ and 31\arcsec, respectively, motivating further space-based
imaging followup. Here, we present multiband HST imaging and ground-based
Magellan spectroscopy of the fields, from which we compile detailed strong
lensing models. The lens models of SPT-CL\,J2325$-$4111 and
SPT-CL\,J0049$-$2440 were optimized using 9, and 8 secure multiple-imaged
systems with a final image-plane rms of 0\farcs63 and 0\farcs73, respectively.
From the lensing analysis, we measure the projected mass density within 500~kpc
of $M(<500 ~{\rm kpc}) = 7.30\pm0.07 \times 10^{14}$$M_{\odot}$, and $M(<500
~{\rm kpc})=7.12^{+0.16}_{-0.19}\times 10^{14}$ $M_{\odot}$ for these two
clusters, and a sub-halos mass ratio of $0.12\pm{0.01}$ and
$0.21^{+0.07}_{-0.05}$, respectively. Both clusters produce a large area with
high magnification ($\mu\geq 3$) for a source at $z=9$, $A^{lens}_{| \mu | \geq
3 }=4.93^{+0.03}_{-0.04} arcmin^2$, and $A^{lens}_{| \mu | \geq 3
}=3.64^{+0.14}_{-0.10} arcmin^2$ respectively, placing them in the top tier of
strong lensing clusters. We conclude that these clusters are spectacular
sightlines for further observations that will reduce the systematic
uncertainties due to cosmic variance. This paper provides the community with
two additional well-calibrated cosmic telescopes, as strong as the Frontier
Fields, suitable for studies of the highly magnified background Universe.","['astro-ph.GA', 'astro-ph.CO']",False,,,,Constant Weight Polar Codes through Periodic Markov Processes,"Strong Lensing analysis of SPT-CLJ2325$-$4111 and SPT-CLJ0049$-$2440,
  two Powerful Cosmic Telescopes ($R_E > 40''$) from the SPT Clusters Sample"
neg-d2-691,2025-01-27,,2501.16039," In this paper, we investigate the complexity of computing the minimal
faithful permutation degree for groups without abelian normal subgroups. When
our groups are given as quotients of permutation groups, we establish that this
problem is in $\textsf{P}$. Furthermore, in the setting of permutation groups,
we obtain an upper bound of $\textsf{NC}$ for this problem. This improves upon
the work of Das and Thakkar (STOC 2024), who established a Las Vegas
polynomial-time algorithm for this class in the setting of permutation groups.","['cs.DS', 'cs.CC', 'math.GR']",2503.01064," Large language models (LLMs) can answer questions and reason about complex
tasks, also from the scientific domain. We assess several multimodal LLMs
(MLLMs) on ScienceQA and find that Gemini models show the highest accuracy with
little context, and the highest textual similarity to human explanations with
richer context. Adapter-tuning of smaller MLLMs did not lead to any reliable
performance. Training from Gemini outputs consistently underperformed training
from the original data.","['cs.CL', 'cs.AI', 'cs.CV']",False,,,,"Complexity of Minimal Faithful Permutation Degree for Fitting-free
  Groups",Scientific Reasoning: Assessment of Multimodal Generative LLMs
neg-d2-692,2025-01-03,,2501.02096," Strong gravitational lenses come in many forms, but are typically divided
into two populations: galaxies, and groups and clusters of galaxies. When
calculating the properties of the images we expect to see from these lenses, it
is typically assumed that each lens is roughly a singular isothermal sphere. In
reality, the largest objects in the Universe (i.e. galaxy clusters) are highly
irregular and composed of many components due to a history of (or active)
hierarchical mergers. In this work, we analyze the discrepancies in the
observables of strongly lensed transients in both scenarios, namely relative
magnifications, time delays, and image multiplicities. Focusing on
gravitational waves, we compare the detection rates between the single
spherical dark matter halo models found in the literature, and publicly
available state-of-the-art cluster lens models. We find there to be
approximately an order of magnitude fewer detection of strongly lensed
transients in the realistic model case, likely caused by their loss of overall
strong lensing optical depth. We also report detection rates in the weak
lensing or single-image regime. Additionally, we find a systemic shift towards
lower time delays between the brightest image pairs in the cases of the
realistic models, as well as higher fractions of positive versus negative
parity images, as seen elsewhere in the literature. This significant deviation
in the joint relative magnification factor-time delay distribution will hinder
the feasibility of the reconstruction of lenses through time domain transients
alone, but can still provide a lower limit on the lens mass.","['astro-ph.CO', 'gr-qc']",2502.18295," In this note, we provide a proof of the existence and complete classification
of $G$-invariant star products with quantum momentum maps on Poisson manifolds
by means of an equivariant version of the formality theorem.","['math.QA', 'math-ph', 'math.MP', 'math.SG']",False,,,,Effects of Galaxy Cluster Structure on Lensed Transients,Quantization of the Momentum Map via $\frak{g}$-adapted Formalities
neg-d2-693,2025-02-20,,2502.15009," Conversational query rewriting is crucial for effective conversational
search, yet traditional supervised methods require substantial labeled data,
which is scarce in low-resource settings. This paper introduces Prompt-Guided
In-Context Learning, a novel approach that leverages the in-context learning
capabilities of Large Language Models (LLMs) for few-shot conversational query
rewriting. Our method employs carefully designed prompts, incorporating task
descriptions, input/output format specifications, and a small set of
illustrative examples, to guide pre-trained LLMs to generate
context-independent queries without explicit fine-tuning. Extensive experiments
on benchmark datasets, TREC and Taskmaster-1, demonstrate that our approach
significantly outperforms strong baselines, including supervised models and
contrastive co-training methods, across various evaluation metrics such as
BLEU, ROUGE-L, Success Rate, and MRR. Ablation studies confirm the importance
of in-context examples, and human evaluations further validate the superior
fluency, relevance, and context utilization of our generated rewrites. The
results highlight the potential of prompt-guided in-context learning as an
efficient and effective paradigm for low-resource conversational query
rewriting, reducing the reliance on extensive labeled data and complex training
procedures.",['cs.CL'],2503.07969," With the advent of deep learning, expression recognition has made significant
advancements. However, due to the limited availability of annotated compound
expression datasets and the subtle variations of compound expressions, Compound
Emotion Recognition (CE) still holds considerable potential for exploration. To
advance this task, the 7th Affective Behavior Analysis in-the-wild (ABAW)
competition introduces the Compound Expression Challenge based on C-EXPR-DB, a
limited dataset without labels. In this paper, we present a curriculum
learning-based framework that initially trains the model on single-expression
tasks and subsequently incorporates multi-expression data. This design ensures
that our model first masters the fundamental features of basic expressions
before being exposed to the complexities of compound emotions. Specifically,
our designs can be summarized as follows: 1) Single-Expression Pre-training:
The model is first trained on datasets containing single expressions to learn
the foundational facial features associated with basic emotions. 2) Dynamic
Compound Expression Generation: Given the scarcity of annotated compound
expression datasets, we employ CutMix and Mixup techniques on the original
single-expression images to create hybrid images exhibiting characteristics of
multiple basic emotions. 3) Incremental Multi-Expression Integration: After
performing well on single-expression tasks, the model is progressively exposed
to multi-expression data, allowing the model to adapt to the complexity and
variability of compound expressions. The official results indicate that our
method achieves the \textbf{best} performance in this competition track with an
F-score of 0.6063. Our code is released at https://github.com/YenanLiu/ABAW7th.",['cs.CV'],False,,,,"Contextualizing Search Queries In-Context Learning for Conversational
  Rewriting with LLMs",7ABAW-Compound Expression Recognition via Curriculum Learning
neg-d2-694,2025-03-11,,2503.08064," Continual learning aims to learn knowledge of tasks observed in sequential
time steps while mitigating the forgetting of previously learned knowledge.
Existing methods were proposed under the assumption of learning a single
modality (e.g., image) over time, which limits their applicability in scenarios
involving multiple modalities. In this work, we propose a novel continual
learning framework that accommodates multiple modalities (image, video, audio,
depth, and text). We train a model to align various modalities with text,
leveraging its rich semantic information. However, this increases the risk of
forgetting previously learned knowledge, exacerbated by the differing input
traits of each task. To alleviate the overwriting of the previous knowledge of
modalities, we propose a method for aggregating knowledge within and across
modalities. The aggregated knowledge is obtained by assimilating new
information through self-regularization within each modality and associating
knowledge between modalities by prioritizing contributions from relevant
modalities. Furthermore, we propose a strategy that re-aligns the embeddings of
modalities to resolve biased alignment between modalities. We evaluate the
proposed method in a wide range of continual learning scenarios using multiple
datasets with different modalities. Extensive experiments demonstrate that ours
outperforms existing methods in the scenarios, regardless of whether the
identity of the modality is given.","['cs.CV', 'cs.AI']",2501.14873," Local-type primordial non-Gaussianity (PNG), predicted by many non-minimal
models of inflation, creates a scale-dependent contribution to the power
spectrum of large-scale structure (LSS) tracers. Its amplitude is characterized
by the product $b_\phi f_{\rm NL}^{\rm loc}$, where $b_\phi$ is an
astrophysical parameter dependent on the properties of the tracer. However,
$b_\phi$ exhibits significant secondary dependence on halo concentration and
other astrophysical properties, which may bias and weaken the constraints on
$f_{\rm NL}^{\rm loc}$. In this work, we demonstrate that incorporating
knowledge of the relation between Lagrangian bias parameters and $b_\phi$ can
significantly enhance PNG constraints. We employ the Hybrid Effective Field
Theory (HEFT) approach at the field-level and a linear regression model to seek
a connection between the bias parameters and $b_{\phi}$ for halo and galaxy
samples, constructed using the \textsc{AbacusSummit} simulation suite and
mimicking the luminous red galaxies (LRGs) and quasi-stellar objects (QSOs) of
the Dark Energy Spectroscopic Instrument (DESI) survey. For the fixed-mass halo
samples, our full bias model reduces the uncertainty by more than 70\%, with
most of that improvement coming from $b_\nabla$, which we find to be an
excellent proxy for concentration. For the galaxy samples, our model reduces
the uncertainty on $b_\phi$ by 80\% for all tracers. By adopting
Lagrangian-bias informed priors on the parameter $b_\phi$, future analyses can
thus constrain $f_{\rm NL}^{\rm loc}$ with less bias and smaller errors.","['astro-ph.CO', 'astro-ph.GA']",False,,,,Continual Learning for Multiple Modalities,"Refining local-type primordial non-Gaussianity: Sharpened $b_\phi$
  constraints through bias expansion"
neg-d2-695,2025-02-26,,2502.19538," There are many different probabilistic programming languages that are
specialized to specific kinds of probabilistic programs. From a usability and
scalability perspective, this is undesirable: today, probabilistic programmers
are forced up-front to decide which language they want to use and cannot
mix-and-match different languages for handling heterogeneous programs. To
rectify this, we seek a foundation for sound interoperability for probabilistic
programming languages: just as today's Python programmers can resort to
low-level C programming for performance, we argue that probabilistic
programmers should be able to freely mix different languages for meeting the
demands of heterogeneous probabilistic programming environments. As a first
step towards this goal, we introduce \textsc{MultiPPL}, a probabilistic
multi-language that enables programmers to interoperate between two different
probabilistic programming languages: one that leverages a high-performance
exact discrete inference strategy, and one that uses approximate importance
sampling. We give a syntax and semantics for \textsc{MultiPPL}, prove soundness
of its inference algorithm, and provide empirical evidence that it enables
programmers to perform inference on complex heterogeneous probabilistic
programs and flexibly exploits the strengths and weaknesses of two languages
simultaneously.%",['cs.PL'],2502.08782," The increasing penetration of Distributed Energy Resources (DERs) in the
distribution system has led to the emergence of a new market actor - the
aggregator. The aggregator serves as a facilitator, enabling flexibility asset
owners to get access to different markets. In which, EVs aggregators are
gaining more attention due to their expanding use and potential to provide
services in various types of markets, particularly in the reserve market.
Currently, TSO indirectly utilizes these resources under the management of the
distribution system operators (DSO), which can negatively impact the
distribution grid. Conversely, adjustments from DSOs can impact service
provision to TSO due to the shortage of TSO usage information. These factors
highlight the importance of evaluating the service provision from aggregators
under different TSO-DSO coordination schemes. This paper focuses on the
provision of flexibility from electric vehicles (EVs) aggregators for balancing
service in the TSO-DSO hybrid-managed and compares it with the DSO-managed
coordination schemes. The behavior of aggregators reacting to price
fluctuations and TSO requests under different coordination schemes and
simulation scenarios is thoroughly evaluated. Additionally, their impact on the
grid is analyzed through the DSO's congestion management process and validated
using data from a real part of the Dutch distribution network. Results find
that the hybrid-managed coordination scheme gives more benefit to the
aggregator than the DSO-managed scheme and the EVs aggregator will gain more
profit in winter than summer due to more upward regulation service is needed.","['eess.SY', 'cs.SY']",False,,,,Multi-Language Probabilistic Programming,"A comparative study of different TSO-DSO coordination in the reserve
  market"
neg-d2-696,2025-01-09,,2501.05246," Semantic segmentation for autonomous driving is an even more challenging task
when faced with adverse driving conditions. Standard models trained on data
recorded under ideal conditions show a deteriorated performance in unfavorable
weather or illumination conditions. Fine-tuning on the new task or condition
would lead to overwriting the previously learned information resulting in
catastrophic forgetting. Adapting to the new conditions through traditional
domain adaption methods improves the performance on the target domain at the
expense of the source domain. Addressing these issues, we propose an
architecture-based domain-incremental learning approach called Progressive
Semantic Segmentation (PSS). PSS is a task-agnostic, dynamically growing
collection of domain-specific segmentation models. The task of inferring the
domain and subsequently selecting the appropriate module for segmentation is
carried out using a collection of convolutional autoencoders. We extensively
evaluate our proposed approach using several datasets at varying levels of
granularity in the categorization of adverse driving conditions. Furthermore,
we demonstrate the generalization of the proposed approach to similar and
unseen domains.",['cs.CV'],2503.15548," The widespread adoption of Retrieval-Augmented Generation (RAG) systems in
real-world applications has heightened concerns about the confidentiality and
integrity of their proprietary knowledge bases. These knowledge bases, which
play a critical role in enhancing the generative capabilities of Large Language
Models (LLMs), are increasingly vulnerable to breaches that could compromise
sensitive information. To address these challenges, this paper proposes an
advanced encryption methodology designed to protect RAG systems from
unauthorized access and data leakage. Our approach encrypts both textual
content and its corresponding embeddings prior to storage, ensuring that all
data remains securely encrypted. This mechanism restricts access to authorized
entities with the appropriate decryption keys, thereby significantly reducing
the risk of unintended data exposure. Furthermore, we demonstrate that our
encryption strategy preserves the performance and functionality of RAG
pipelines, ensuring compatibility across diverse domains and applications. To
validate the robustness of our method, we provide comprehensive security proofs
that highlight its resilience against potential threats and vulnerabilities.
These proofs also reveal limitations in existing approaches, which often lack
robustness, adaptability, or reliance on open-source models. Our findings
suggest that integrating advanced encryption techniques into the design and
deployment of RAG systems can effectively enhance privacy safeguards. This
research contributes to the ongoing discourse on improving security measures
for AI-driven services and advocates for stricter data protection standards
within RAG architectures.","['cs.CR', 'cs.AI']",False,,,,"Domain-Incremental Semantic Segmentation for Autonomous Driving under
  Adverse Driving Conditions",Privacy-Aware RAG: Secure and Isolated Knowledge Retrieval
neg-d2-697,2025-01-03,,2501.04041," We implement a stabilized finite element method for steady
Darcy-Brinkman-Forchheimer model within the continuous Galerkin framework. The
nonlinear fluid model is first linearized using a standard \textit{Newton's
method. The sequence of linear problems is then discretized utilizing a stable
\textit{inf-sup} type continuous finite elements based on the
\textit{Taylor-Hood} pair to approximate the primary variables: velocity and
pressure}. Such a pair is known to be optimal for the approximation of the
isotropic Navier-Stokes equation. To overcome the well-known numerical
instability in the convection-dominated problems, the Grad-Div stabilization is
employed with an efficient \textit{augmented Lagrangian-type} penalty method.
We use the penalty term to develop the \textit{block Schur complement}
preconditioner, which is later coupled with a Krylov-space-based iterative
linear solver. In addition, the Kelly error estimator for the adaptive mesh
refinement is employed to achieve better numerical results with less
computational cost. Performance of the proposed algorithm is verified for a
classical benchmark problem. Particularly for the Forchheimer parameter, we
present some interesting flow patterns with the velocity components and their
streamlines along the mid-lines in the computational domain. The role of the
Forchheimer term is highlighted for different porous medium scenarios. This
study can offer an attractive setting for discretizing many multi-physics
problems along with the fluid flow having inertial effects in porous media.","['math.NA', 'cs.NA']",2501.06374," This paper introduces AFRIDOC-MT, a document-level multi-parallel translation
dataset covering English and five African languages: Amharic, Hausa, Swahili,
Yor\`ub\'a, and Zulu. The dataset comprises 334 health and 271 information
technology news documents, all human-translated from English to these
languages. We conduct document-level translation benchmark experiments by
evaluating neural machine translation (NMT) models and large language models
(LLMs) for translations between English and these languages, at both the
sentence and pseudo-document levels. These outputs are realigned to form
complete documents for evaluation. Our results indicate that NLLB-200 achieved
the best average performance among the standard NMT models, while GPT-4o
outperformed general-purpose LLMs. Fine-tuning selected models led to
substantial performance gains, but models trained on sentences struggled to
generalize effectively to longer documents. Furthermore, our analysis reveals
that some LLMs exhibit issues such as under-generation, repetition of words or
phrases, and off-target translations, especially for African languages.",['cs.CL'],False,,,,"A stabilized finite element method for steady Darcy-Brinkman-Forchheimer
  flow model with different viscous and inertial resistances in porous media",AFRIDOC-MT: Document-level MT Corpus for African Languages
neg-d2-698,2025-03-11,,2503.165," Aligning robot navigation with human preferences is essential for ensuring
comfortable and predictable robot movement in shared spaces, facilitating
seamless human-robot coexistence. While preference-based learning methods, such
as reinforcement learning from human feedback (RLHF), enable this alignment,
the choice of the preference collection interface may influence the process.
Traditional 2D interfaces provide structured views but lack spatial depth,
whereas immersive VR offers richer perception, potentially affecting preference
articulation. This study systematically examines how the interface modality
impacts human preference collection and navigation policy alignment. We
introduce a novel dataset of 2,325 human preference queries collected through
both VR and 2D interfaces, revealing significant differences in user
experience, preference consistency, and policy outcomes. Our findings highlight
the trade-offs between immersion, perception, and preference reliability,
emphasizing the importance of interface selection in preference-based robot
learning. The dataset will be publicly released to support future research.","['cs.HC', 'cs.RO']",2503.06018," The evolution of the role of lattice vibrations in the formation of the
pseudogap state in strongly correlated electron systems has been investigated
concerning changes in the electron-phonon coupling parameters and the
concentration of doped charge carriers. We apply the polaronic version of the
generalized tight-binding method to analyze the band structure of a realistic
multiband two-dimensional model that incorporates the electron-lattice
contributions of both Holstein and Peierls types. It has been demonstrated that
the emergence of polaronic effects begins with the modulation of spectral
function intensity. However, within a specific region of the phase diagram, a
significant transformation of the electron band structure and pseudogap state
occurs. It results from coherent polaron excitations that create a partially
flat band near the Fermi level. This process leads to a change in the topology
of the Fermi surface and the emergence of corresponding features in the density
of states.","['cond-mat.str-el', 'cond-mat.supr-con']",False,,,,"The Impact of VR and 2D Interfaces on Human Feedback in Preference-Based
  Robot Learning","Evolution of the pseudogap band structure in a system of
  electron-correlated lattice polarons"
neg-d2-699,2025-01-23,,2501.13846," Matched-filtering is a long-standing technique for the optimal detection of
known signals in stationary Gaussian noise. However, it has known departures
from optimality when operating on unknown signals in real noise and suffers
from computational inefficiencies in its pursuit to near-optimality. A
compelling alternative that has emerged in recent years to address this problem
is deep learning. Although it has shown significant promise when applied to the
search for gravitational-waves in detector noise, we demonstrate the existence
of a multitude of learning biases that hinder generalisation and detection
performance. Our work identifies the sources of a set of 11 interconnected
biases present in the supervised learning of the gravitational-wave detection
problem, and contributes mitigation tactics and training strategies to
concurrently address them. We introduce, Sage, a machine-learning based binary
black hole search pipeline. We evaluate our pipeline on the injection study
presented in the Machine Learning Gravitational-Wave Search Challenge and show
that Sage detects ~11.2% more signals than the benchmark PyCBC analysis at a
false alarm rate of one per month in O3a noise. Moreover, we also show that it
can detect ~48.29% more signals than the previous best performing
machine-learning pipeline on the same dataset. We empirically prove that our
pipeline has the capability to effectively handle out-of-distribution noise
power spectral densities and reject non-Gaussian transient noise artefacts. By
studying machine-learning biases and conducting empirical investigations to
understand the reasons for performance improvement/degradation, we aim to
address the need for interpretability of machine-learning methods for
gravitational-wave detection. All code and implementations are available at
https://github.com/nnarenraju/sage.","['gr-qc', 'astro-ph.IM']",2503.04003," Mobile platforms now power not only smartphones but also in-vehicle systems
like Android Auto and CarPlay. Despite an ecosystem of over 3.5 million Android
apps and more than 200 million Android Auto-compatible vehicles, only a few
hundred apps have been adapted for automotive use. To better understand this
gap, we studied 147 reported issues related to Android Auto and identified
their root causes. We found that more than 70% of issues result from UI
incompatibilities, 24% from media playback errors, and around 5% from failures
in voice command handling, showing a lack of effective tools for developers. We
introduce CarCompat, a static analysis framework that detects compatibility
problems in Android Auto apps. CarCompat constructs a Car-Control Flow Graph
(CCFG) to capture interactions among app components, lifecycle methods, and
platform-specific callbacks. It applies specialized checkers to detect UI
violations, media playback errors, and issues with voice command handling. We
evaluated CarCompat on a dataset of 54 Android Auto apps and detected 25 new
issues, 4 of which were confirmed by developers, and 2 developers have already
released their fixes. The results show that CarCompat helps developers identify
and fix compatibility issues, improving the in-vehicle experience.","['cs.SE', 'cs.PL']",False,,,,"Identifying and Mitigating Machine Learning Biases for the
  Gravitational-wave Detection Problem",Understanding and Detecting Compatibility Issues in Android Auto Apps
neg-d2-700,2025-02-26,,2502.19303," Advancements in differential pumping and electron optics over the past few
decades have enabled x-ray photoelectron spectroscopy (XPS) measurements at
(near-)ambient pressures, bridging the pressure gap for characterizing
realistic sample chemistries. Recently, we demonstrated the capabilities of an
ambient pressure XPS (APXPS) setup for in-situ plasma environment measurements,
allowing plasma-surface interactions to be studied in operando rather than
using the traditional before-and-after analysis approach. This new plasma-XPS
technique facilitates the identification of reaction intermediates critical for
understanding plasma-assisted surface processes relevant to semiconductor
nanomanufacturing, such as physical vapor deposition, etching, atomic layer
deposition, etc. In this report, we apply the plasma-XPS approach to monitor
real-time surface chemical changes on a model Ag(111) single crystal exposed to
oxidizing and reducing plasmas. We correlate surface-sensitive data with
concurrent gas-phase XPS measurements and residual gas mass-spectra analysis of
species generated during plasma exposure, highlighting the significant role of
plasma-induced chamber wall reactions. Ultimately, we demonstrate that
plasma-XPS provides comprehensive insights into both surface and gas-phase
chemistry, establishing it as a versatile and dynamic characterization tool
with broad applications in microelectronics research.","['physics.plasm-ph', 'cond-mat.mtrl-sci']",2501.16011," Legal texts, characterized by complex and specialized terminology, present a
significant challenge for Language Models. Adding an underrepresented language,
such as Spanish, to the mix makes it even more challenging. While pre-trained
models like XLM-RoBERTa have shown capabilities in handling multilingual
corpora, their performance on domain specific documents remains underexplored.
This paper presents the development and evaluation of MEL, a legal language
model based on XLM-RoBERTa-large, fine-tuned on legal documents such as BOE
(Bolet\'in Oficial del Estado, the Spanish oficial report of laws) and congress
texts. We detail the data collection, processing, training, and evaluation
processes. Evaluation benchmarks show a significant improvement over baseline
models in understanding the legal Spanish language. We also present case
studies demonstrating the model's application to new legal texts, highlighting
its potential to perform top results over different NLP tasks.",['cs.CL'],False,,,,Operando XPS in Reactive Plasmas: The Importance of The Wall Reactions,MEL: Legal Spanish Language Model
neg-d2-701,2025-03-13,,2503.11036," We theoretically propose a method to generate topological spin textures by
irradiating a classical spin system with a linearly polarized AC electric
field. To this end, we investigate non-equilibrium steady states in a classical
Heisenberg model with frustrated exchange interactions on a two-dimensional
triangular lattice by numerically solving the Landau-Lifshitz-Gilbert equation
at zero temperature. Our results reveal that the linearly polarized AC
electric-field irradiation induces a topological phase transition from a
single-Q spiral state to a bimeron crystal with the skyrmion number of one in
the low-frequency regime. Furthermore, we show that the obtained bimeron
crystal remains relatively stable against both easy-axis and easy-plane
single-ion anisotropies.","['cond-mat.str-el', 'cond-mat.mes-hall']",2503.07414," This study aims to develop a cost-effective microgrid design that optimally
balances the economic feasibility, reliability, efficiency, and environmental
impact in a grid-tied community microgrid. A multi-objective optimization
framework is employed, integrating HOMER Pro for system sizing with deep
reinforcement learning (DRL). Sensitivity analyses are conducted to evaluate
the system performance under varying load demand and renewable energy
fluctuations, while an economic sensitivity assessment examines the impact of
electricity prices and capital costs on the Levelized Cost of Energy (LCOE).
The proposed microgrid configuration achieves high reliability, satisfying 100%
of the load, even under adverse weather conditions. The proposed framework
attains an efficiency of 91.99% while maintaining a carbon footprint of 302,747
kg/year, which is approximately 95% lower than that of the grid system. The
economic analysis indicates a net present cost (NPC) of $4.83M with a
competitive LCOE of $0.208/kWh. In addition, the operation cost is $201,473 per
year with a capital investment of $1.42M, rendering it a financially viable
alternative to conventional grid-dependent systems.This work can be valuable in
identifying effective solutions for supplying reliable and cost-effective power
to regional and remote areas.","['eess.SY', 'cs.SY']",False,,,,"Bimeron Crystals by a Linearly Polarized AC Electric Field in Frustrated
  Magnets",Cost-Effective Design of Grid-tied Community Microgrid
neg-d2-702,2025-01-31,,2502.00133," Deep learning methods have demonstrated strong performance in objection
tasks; however, their ability to learn domain-specific applications with
limited training data remains a significant challenge. Transfer learning
techniques address this issue by leveraging knowledge from pre-training on
related datasets, enabling faster and more efficient learning for new tasks.
Finding the right dataset for pre-training can play a critical role in
determining the success of transfer learning and overall model performance. In
this paper, we investigate the impact of pre-training a YOLOv8n model on seven
distinct datasets, evaluating their effectiveness when transferred to the task
of polyp detection. We compare whether large, general-purpose datasets with
diverse objects outperform niche datasets with characteristics similar to
polyps. In addition, we assess the influence of the size of the dataset on the
efficacy of transfer learning. Experiments on the polyp datasets show that
models pre-trained on relevant datasets consistently outperform those trained
from scratch, highlighting the benefit of pre-training on datasets with shared
domain-specific features.","['cs.CV', 'cs.AI']",2502.02958," Large Language Models (LLMs) contain large amounts of facts about the world.
These facts can become outdated over time, which has led to the development of
knowledge editing methods (KEs) that can change specific facts in LLMs with
limited side effects. This position paper argues that editing LLMs poses
serious safety risks that have been largely overlooked. First, we note the fact
that KEs are widely available, computationally inexpensive, highly performant,
and stealthy makes them an attractive tool for malicious actors. Second, we
discuss malicious use cases of KEs, showing how KEs can be easily adapted for a
variety of malicious purposes. Third, we highlight vulnerabilities in the AI
ecosystem that allow unrestricted uploading and downloading of updated models
without verification. Fourth, we argue that a lack of social and institutional
awareness exacerbates this risk, and discuss the implications for different
stakeholders. We call on the community to (i) research tamper-resistant models
and countermeasures against malicious model editing, and (ii) actively engage
in securing the AI ecosystem.",['cs.CL'],False,,,,"Exploring Transfer Learning for Deep Learning Polyp Detection in
  Colonoscopy Images Using YOLOv8",Position: Editing Large Language Models Poses Serious Safety Risks
neg-d2-703,2025-02-06,,2502.04472," Strong gravitational lensing of variable sources, such as quasars or
supernovae, can be used to constrain cosmological parameters through a
technique known as ""time-delay cosmography''. Competitive constraints on the
Hubble constant have been achieved with electromagnetic observations of lensed
quasars and lensed supernovae. Gravitational wave (GW) astronomy may open up a
new channel for time-delay cosmography with GW signal replacing the
electromagnetic (EM) one. We highlight the similarities of using GW signals to
be applied to time-delay cosmography compared to EM signal. We then discuss key
differences between GW and EM signals and their resulting advantages and
inconveniences from the angle of the current state-of-the-art using quasars and
lensed supernovae for time-delay cosmography. We identify the astrometric
precision requirement of the images as a key challenge to overcome and
highlight the potentially significant impact that near-perfect time-delay
measurements of lensed GWs can bring to the table.",['astro-ph.CO'],2502.17875," In recent years, the fifth-generation (5G) new radio (NR) signals have
emerged as a promising supplementary resource for urban navigation. However, a
major challenge in utilizing 5G signals lies in their vulnerability to
non-line-of-sight (NLoS) propagation effects, which are especially prevalent in
urban street canyons. This paper applies the direct position estimation (DPE)
method to 5G cellular signals to mitigate the NLoS bias as well as the
multipath effects, thereby enabling precise localization in urbanized
environments. The feasibility of applying the DPE method to NR positioning is
analyzed, followed by a discussion of the tapped delay line (TDL) channel
propagation model provided by the 3rd Generation Partnership Project (3GPP).
The positioning performance is then evaluated through large-scale system-level
simulations. The simulation results demonstrate that 5G DPE achieves
satisfactory positioning accuracy in a 10 dB noisy channel, with an overall
root mean square error (RMSE) constrained within 6 m. In addition, 5G DPE
outperforms the observed time difference of arrival (OTDoA) method by 95.24% in
terms of positioning accuracy in an NLoS-dominated propagation environment.",['eess.SP'],False,,,,"Challenges and Opportunities for time-delay cosmography with
  multi-messenger gravitational lensing","5G Direct Position Estimation for Precise Localization in Dense Urban
  Area"
neg-d2-704,2025-02-05,,2502.0439," Despite remarkable capabilities, large language models (LLMs) struggle to
continually update their knowledge without catastrophic forgetting. In
contrast, humans effortlessly integrate new information, detect conflicts with
existing beliefs, and selectively update their mental models. This paper
introduces a cognitive-inspired investigation paradigm to study continual
knowledge updating in LLMs. We implement two key components inspired by human
cognition: (1) Dissonance and Familiarity Awareness, analyzing model behavior
to classify information as novel, familiar, or dissonant; and (2) Targeted
Network Updates, which track neural activity to identify frequently used
(stubborn) and rarely used (plastic) neurons. Through carefully designed
experiments in controlled settings, we uncover a number of empirical findings
demonstrating the potential of this approach. First, dissonance detection is
feasible using simple activation and gradient features, suggesting potential
for cognitive-inspired training. Second, we find that non-dissonant updates
largely preserve prior knowledge regardless of targeting strategy, revealing
inherent robustness in LLM knowledge integration. Most critically, we discover
that dissonant updates prove catastrophically destructive to the model's
knowledge base, indiscriminately affecting even information unrelated to the
current updates. This suggests fundamental limitations in how neural networks
handle contradictions and motivates the need for new approaches to knowledge
updating that better mirror human cognitive mechanisms.","['cs.CL', 'cs.AI', 'cs.LG', 'q-bio.NC']",2503.06187," In Neural Networks, there are various methods of feature fusion. Different
strategies can significantly affect the effectiveness of feature
representation, consequently influencing the ability of model to extract
representative and discriminative features. In the field of face recognition,
traditional feature fusion methods include feature concatenation and feature
addition. Recently, various attention mechanism-based fusion strategies have
emerged. However, we found that these methods primarily focus on the important
features in the image, referred to as salient features in this paper, while
neglecting another equally important set of features for image recognition
tasks, which we term differential features. This may cause the model to
overlook critical local differences when dealing with complex facial samples.
Therefore, in this paper, we propose an efficient convolution module called
MSConv (Multiplicative and Subtractive Convolution), designed to balance the
learning of model about salient and differential features. Specifically, we
employ multi-scale mixed convolution to capture both local and broader
contextual information from face images, and then utilize Multiplication
Operation (MO) and Subtraction Operation (SO) to extract salient and
differential features, respectively. Experimental results demonstrate that by
integrating both salient and differential features, MSConv outperforms models
that only focus on salient features.","['cs.CV', 'cs.AI']",False,,,,"In Praise of Stubbornness: The Case for Cognitive-Dissonance-Aware
  Knowledge Updates in LLMs",MSConv: Multiplicative and Subtractive Convolution for Face Recognition
neg-d2-705,2025-03-03,,2503.01815," We explain the relation between the Witt class and the universal
equicommutative class for PSL(2,K). We discuss an analogue of the Milnor-Wood
inequality.","['math.KT', 'math.GR']",2501.04085," We present the Cosmic Evolution Early Release Science (CEERS) Survey, a 77.2
hour Director's Discretionary Early Release Science Program. CEERS
demonstrates, tests, and validates efficient extragalactic surveys using
coordinated, overlapping parallel observations with the JWST instrument suite,
including NIRCam and MIRI imaging, NIRSpec low (R~100) and medium (R~1000)
resolution spectroscopy, and NIRCam slitless grism (R~1500) spectroscopy. CEERS
targets the Hubble Space Telescope-observed region of the Extended Groth Strip
(EGS) field, supported by a rich set of multiwavelength data. CEERS facilitated
immediate community science in both of the extragalactic core JWST science
drivers ``First Light"" and ``Galaxy Assembly,"" including: 1) The discovery and
characterization of large samples of galaxies at z >~ 10 from ~90 arcmin^2 of
NIRCam imaging, constraining their abundance and physical nature; 2) Deep
spectra of >1000 galaxies, including dozens of galaxies at 6<z<10, enabling
redshift measurements and constraints on the physical conditions of
star-formation and black hole growth via line diagnostics; 3) Quantifying the
first bulge, bar and disk structures at z>3; and 4) Characterizing galaxy
mid-IR emission with MIRI to study dust-obscured star-formation and
supermassive black hole growth at z~1-3. As a legacy product for the community,
the CEERS team has provided several data releases, accompanied by detailed
notes on the data reduction procedures and notebooks to aid in reproducibility.
In addition to an overview of the survey and quality of the data, we provide
science highlights from the first two years with CEERS data.",['astro-ph.GA'],False,,,,Tautological characteristic classes III: the Witt class for PSL(2),The Cosmic Evolution Early Release Science Survey (CEERS)
neg-d2-706,2025-02-05,,2502.035," Recent advances in generative image restoration (IR) have demonstrated
impressive results. However, these methods are hindered by their substantial
size and computational demands, rendering them unsuitable for deployment on
edge devices. This work introduces ELIR, an Efficient Latent Image Restoration
method. ELIR operates in latent space by first predicting the latent
representation of the minimum mean square error (MMSE) estimator and then
transporting this estimate to high-quality images using a latent consistency
flow-based model. Consequently, ELIR is more than 4x faster compared to the
state-of-the-art diffusion and flow-based approaches. Moreover, ELIR is also
more than 4x smaller, making it well-suited for deployment on
resource-constrained edge devices. Comprehensive evaluations of various image
restoration tasks show that ELIR achieves competitive results, effectively
balancing distortion and perceptual quality metrics while offering improved
efficiency in terms of memory and computation.","['eess.IV', 'cs.AI', 'stat.AP']",2501.05712," We introduce HRMCR (HAE-RAE Multi-Step Commonsense Reasoning), a benchmark
designed to evaluate large language models' ability to perform multi-step
reasoning in culturally specific contexts, focusing on Korean. The questions
are automatically generated via templates and algorithms, requiring LLMs to
integrate Korean cultural knowledge into sequential reasoning steps. Consistent
with prior observations on emergent abilities, our experiments reveal that
models trained on fewer than \(2 \cdot 10^{25}\) training FLOPs struggle to
solve any questions, showing near-zero performance. Beyond this threshold,
performance improves sharply. State-of-the-art models (e.g., O1) still score
under 50\%, underscoring the difficulty of our tasks. Notably, stepwise
analysis suggests the observed emergent behavior may stem from compounding
errors across multiple steps rather than reflecting a genuinely new capability.
We publicly release the benchmark and commit to regularly updating the dataset
to prevent contamination.",['cs.CL'],False,,,,Efficient Image Restoration via Latent Consistency Flow Matching,Multi-Step Reasoning in Korean and the Emergent Mirage
neg-d2-707,2025-02-10,,2502.07037," Self-assembly of amphiphilic molecules is an important phenomenon attracting
a broad range of research. In this work, we study the self-assembly of KTOF4
sphere-rod amphiphilic molecules in mixed water-dioxane solvents. The molecules
are of a T-shaped geometry, comprised of a hydrophilic spherical Keggin-type
cluster attached by a flexible bridge to the center of a hydrophobic rod-like
oligodialkylfluorene (OF), which consists of four OF units. Transmission
electron microscopy (TEM) uncovers self-assembled spherical structures of KTOF4
in dilute solutions. These spheres are filled with smectic-like layers of KTOF4
separated by layers of the solution. There are two types of layer packings: (i)
concentric spheres and (ii) flat layers. The concentric spheres form when the
dioxane volume fraction in the solution is 35-50 vol%. The flat layers are
formed when the dioxane volume fraction is either below (20 and 30 vol%.) or
above (55 and 60 vol%.) the indicated range. The layered structures show no
in-plane orientational order and thus resemble thermotropic smectic A liquid
crystals and their lyotropic analogs. The layered packings reveal edge and
screw dislocations. Evaporation of the solvent produces a bulk birefringent
liquid crystal phase with textures resembling the ones of uniaxial nematic
liquid crystals. These findings demonstrate that sphere-rod molecules produce a
variety of self-assembled structures that are controlled by the solvent
properties.",['cond-mat.soft'],2501.08747," We prove that, to every abstract group $G$, we can associate a sequence of
graphs $\Gamma_n$ such that the automorphism group of $\Gamma_n$ is isomorphic
to $G$ and the genus of $\Gamma_n$ is an unbounded function of $n$.","['math.GR', 'math.CO']",False,,,,"Liquid crystalline structures formed by sphere-rod amphiphilic molecules
  in solvents","Every group is the automorphism group of a graph with arbitrarily large
  genus"
neg-d2-708,2025-02-20,,2502.14693," Recent advancements in large language models (LLMs) have shown remarkable
potential in automating machine learning tasks. However, existing LLM-based
agents often struggle with low-diversity and suboptimal code generation. While
recent work has introduced Monte Carlo Tree Search (MCTS) to address these
issues, limitations persist in the quality and diversity of thoughts generated,
as well as in the scalar value feedback mechanisms used for node selection. In
this study, we introduce Introspective Monte Carlo Tree Search (I-MCTS), a
novel approach that iteratively expands tree nodes through an introspective
process that meticulously analyzes solutions and results from parent and
sibling nodes. This facilitates a continuous refinement of the node in the
search tree, thereby enhancing the overall decision-making process.
Furthermore, we integrate a Large Language Model (LLM)-based value model to
facilitate direct evaluation of each node's solution prior to conducting
comprehensive computational rollouts. A hybrid rewarding mechanism is
implemented to seamlessly transition the Q-value from LLM-estimated scores to
actual performance scores. This allows higher-quality nodes to be traversed
earlier. Applied to the various ML tasks, our approach demonstrates a 6%
absolute improvement in performance compared to the strong open-source AutoML
agents, showcasing its effectiveness in enhancing agentic AutoML systems.
Resource available at https://github.com/jokieleung/I-MCTS",['cs.CL'],2503.02414," 3D models are widely used in various industries, and mesh data has become an
indispensable part of 3D modeling because of its unique advantages. Mesh data
can provide an intuitive and practical expression of rich 3D information.
However, its disordered, irregular data structure and complex surface
information make it challenging to apply with deep learning models directly.
Traditional mesh data processing methods often rely on mesh models with many
limitations, such as manifold, which restrict their application scopes in
reality and do not fully utilize the advantages of mesh models. This paper
proposes a novel end-to-end framework for addressing the challenges associated
with deep learning in mesh models centered around graph neural networks (GNN)
and is titled InfoGNN. InfoGNN treats the mesh model as a graph, which enables
it to handle irregular mesh data efficiently. Moreover, we propose InfoConv and
InfoMP modules, which utilize the position information of the points and fully
use the static information such as face normals, dihedral angles, and dynamic
global feature information to fully use all kinds of data. In addition, InfoGNN
is an end-to-end framework, and we simplify the network design to make it more
efficient, paving the way for efficient deep learning of complex 3D models. We
conducted experiments on several publicly available datasets, and the results
show that InfoGNN achieves excellent performance in mesh classification and
segmentation tasks.","['cs.CV', 'cs.LG']",False,,,,"I-MCTS: Enhancing Agentic AutoML via Introspective Monte Carlo Tree
  Search",InfoGNN: End-to-end deep learning on mesh via graph neural networks
neg-d2-709,2025-03-12,,2503.09425," In arXiv:1303.3724, the authors provide an axiomatic way of constructing new
polynomially bounded o-minimal structures. However, all of the structures
satisfying these axioms must also have smooth cell-decomposition. In this
paper, we generalize their approach by allowing weakly smooth germs into the
construction. In particular, we showed in arXiv:2501.17583 that the o-minimal
structure constructed in [O. Le Gal, J.-P. Rolin. ""An o-minimal structure which
does not admit $C^\infty$ cellular decomposition"" Ann. Inst. Fourier 59 (2009),
pp 543-562] satisfies the assumptions of our theorem.",['math.LO'],2501.07779," The transition route from laminar to turbulent flow in a magnetohydrodynamic
(MHD) duct with a square cross-section is investigated in the limit of low
magnetic Reynolds number. In the presence of a transverse magnetic field,
Hartmann and Shercliff layers are present on the walls orthogonal and parallel
to the field direction, respectively. We assume reflection symmetries in both
transverse directions, and investigate the competition between transition
mechanisms specific to each boundary layer using direct numerical simulations.
Independently of which wall turbulence eventually occupies, transition relies
exclusively on a tripping of the Shercliff layer by perturbations, while the
Hartmann layer plays a passive role. This is explained, using a dynamical
systems interpretation, by the spatial localization of the edge states in the
Shercliff layer at the expense of the Hartmann layer. The link between these
non-linear coherent structures and the linear optimal modes known from
non-modal stability and energy stability theory is pointed out.",['physics.flu-dyn'],False,,,,"Quasianalytic algebras with weakly smooth germs generate o-minimal
  structures",The route to turbulence in magnetohydrodynamic square duct flow
neg-d2-710,2025-02-15,,2502.10682," Effective deepfake detection tools are becoming increasingly essential over
the last few years due to the growing usage of deepfakes in unethical
practices. There exists a diverse range of deepfake generation techniques,
which makes it challenging to develop an accurate universal detection
mechanism. The 2025 Signal Processing Cup (DFWild-Cup competition) provided a
diverse dataset of deepfake images, which are generated from multiple deepfake
image generators, for training machine learning model(s) to emphasize the
generalization of deepfake detection. To this end, we proposed an
ensemble-based approach that employs three different neural network
architectures: a ResNet-34-based architecture, a data-efficient image
transformer (DeiT), and an XceptionNet with Wavelet Transform to capture both
local and global features of deepfakes. We visualize the specific regions that
these models focus for classification using Grad-CAM, and empirically
demonstrate the effectiveness of these models in grouping real and fake images
into cohesive clusters using t-SNE plots. Individually, the ResNet-34
architecture has achieved 88.9% accuracy, whereas the Xception network and the
DeiT architecture have achieved 87.76% and 89.32% accuracy, respectively. With
these networks, our weighted ensemble model achieves an excellent accuracy of
93.23% on the validation dataset of the SP Cup 2025 competition. Finally, the
confusion matrix and an Area Under the ROC curve of 97.44% further confirm the
stability of our proposed method.","['cs.CV', 'cs.LG', 'eess.IV']",2503.03925," In recent years, attempts have been made to extend ISS small-gain theorems
from finite networks to countably infinite, locally finite networks. Under
specific assumptions about the interconnection gains and the ISS formulation,
corresponding infinite-dimensional small-gain results have been proven.
However, concerning these assumptions, the results are still too narrow to be
considered a full extension of the state-of-the-art for finite networks. We
take a step to closing this gap by a thorough investigation of various monotone
operators associated with an infinite network and a specific ISS formulation.
Our results shed more light on the theory of finite networks, yield complete
characterizations of the small-gain condition for specific ISS formulations,
and show which obstacles still have to be overcome to obtain a complete theory
for the most general case.","['math.OC', 'math.DS']",False,,,,"Hybrid Deepfake Image Detection: A Comprehensive Dataset-Driven Approach
  Integrating Convolutional and Attention Mechanisms with Frequency Domain
  Features",The Small-Gain Condition for Infinite Networks
neg-d2-711,2025-03-18,,2503.14187," In this paper, we consider the inviscid limit problem to the higher
dimensional incompressible Navier-Stokes equations in the whole space. It was
proved in \cite[J. Funct. Anal., 276 (2019)]{GZ} that given initial data
$u_0\in B^{s}_{p,r}$ with $1\leq r<\infty$, the solutions of the Navier-Stokes
equations converge strongly in $B^{s}_{p,r}$ to the Euler equations as the
viscosity parameter tends to zero. In the case when $r=\infty$, we prove the
failure of the $B^{s}_{p,\infty}$-convergence of the Navier-Stokes equations
toward the Euler equations in the inviscid limit.",['math.AP'],2503.00329," Visual embedding models excel at zero-shot tasks like visual retrieval and
classification. However, these models cannot be used for tasks that contain
ambiguity or require user instruction. These tasks necessitate a multimodal
embedding model, which outputs embeddings that combine visual and natural
language input. Existing CLIP-based approaches embed images and text
independently, and fuse the result. We find that this results in weak
interactions between modalities, and poor user control over the representation.
We introduce ABC, an open-source multimodal embedding model that uses a
vision-language model backbone to deeply integrate image features with natural
language instructions. ABC achieves bestfor-size performance on MSCOCO
image-to-text retrieval and is the top performing model on classification and
VQA tasks in the Massive Multimodal Embedding Benchmark. With a strongly
unified vision-language representation, ABC can use natural language to solve
subtle and potentially ambiguous visual retrieval problems. To evaluate this
capability, we design CtrlBench, a benchmark that requires interleaving textual
instructions with image content for correct retrieval. ABC advances the state
of multimodal embeddings by offering high-quality representations and flexible
natural language control. Our model and datasets are available at our project
page.","['cs.CV', 'cs.LG']",False,,,,"Non-convergence of the Navier-Stokes equations toward the Euler
  equations in weak Besov spaces",ABC: Achieving Better Control of Multimodal Embeddings using VLMs
neg-d2-712,2025-02-20,,2502.14266," In this article, we delve into the intricate relationship between the number
of ring homomorphisms and surjective group homomorphisms between two finite
cyclic structures, specifically $\mathbb{Z}_m$ and $\mathbb{Z}_n$. We
demonstrate that the number of ring homomorphisms from $\mathbb{Z}_m$ to
$\mathbb{Z}_n$ is a divisor of the number of surjective group homomorphisms
from $\mathbb{Z}_m$ to $\mathbb{Z}_n$, provided that $n$ is not of the form $2
\cdot \alpha$, where each prime factor $p$ of $\alpha$ satisfies $p \equiv 3
\pmod{4}$.","['math.AC', 'math.CO']",2501.14369," Research on continual learning in multi-modal tasks has been receiving
increasing attention. However, most existing work overlooks the explicit
cross-modal and cross-task interactions. In this paper, we innovatively propose
the Low-rank Prompt Interaction (LPI) to address this general problem of
multi-modal understanding, which considers both cross-modal and cross-task
interactions. Specifically, as for the former, we employ multi-modal
correlation modules for corresponding Transformer layers. Considering that the
training parameters scale to the number of layers and tasks, we propose
low-rank interaction-augmented decomposition to avoid memory explosion while
enhancing the cross-modal association through sharing and separating
common-specific low-rank factors. In addition, due to the multi-modal semantic
differences carried by the low-rank initialization, we adopt hierarchical
low-rank contrastive learning to ensure training robustness. As for the latter,
we initially employ a visual analysis and identify that different tasks have
clear distinctions in proximity. Therefore, we introduce explicit task
contrastive constraints in the prompt learning process based on task semantic
distances. Experiments on two retrieval tasks show performance improvements
with the introduction of a minimal number of parameters, demonstrating the
effectiveness of our method. Code is available at
https://github.com/Kelvin-ywc/LPI.",['cs.CV'],False,,,,"Divisibility Relations Between Ring Homomorphisms and Surjective Group
  Homomorphisms in Finite Cyclic Structures",Low-rank Prompt Interaction for Continual Vision-Language Retrieval
neg-d2-713,2025-02-25,,2502.18157," Snow avalanches present significant risks to human life and infrastructure,
particularly in mountainous regions, making effective monitoring crucial.
Traditional monitoring methods, such as field observations, are limited by
accessibility, weather conditions, and cost. Satellite-borne Synthetic Aperture
Radar (SAR) data has become an important tool for large-scale avalanche
detection, as it can capture data in all weather conditions and across remote
areas. However, traditional processing methods struggle with the complexity and
variability of avalanches. This chapter reviews the application of deep
learning for detecting and segmenting snow avalanches from SAR data. Early
efforts focused on the binary classification of SAR images, while recent
advances have enabled pixel-level segmentation, providing greater accuracy and
spatial resolution. A case study using Sentinel-1 SAR data demonstrates the
effectiveness of deep learning models for avalanche segmentation, achieving
superior results over traditional methods. We also present an extension of this
work, testing recent state-of-the-art segmentation architectures on an expanded
dataset of over 4,500 annotated SAR images. The best-performing model among
those tested was applied for large-scale avalanche detection across the whole
of Norway, revealing important spatial and temporal patterns over several
winter seasons.","['cs.CV', 'cs.AI', 'eess.IV']",2502.04879," As platforms increasingly rely on learning algorithms, collectives may form
and seek ways to influence these platforms to align with their own interests.
This can be achieved by coordinated submission of altered data. To evaluate the
potential impact of such behavior, it is essential to understand the
computations that collectives must perform to impact platforms in this way. In
particular, collectives need to make a priori assessments of the effect of the
collective before taking action, as they may face potential risks when
modifying their data. Moreover they need to develop implementable coordination
algorithms based on quantities that can be inferred from observed data. We
develop a framework that provides a theoretical and algorithmic treatment of
these issues and present experimental results in a product evaluation domain.","['stat.ML', 'cs.LG']",False,,,,Monitoring snow avalanches from SAR data with deep learning,Statistical Collusion by Collectives on Learning Platforms
neg-d2-714,2025-03-12,,2503.09495," The calibration of the CR39 and Makrofol Nuclear Track Detectors of the
MoEDAL experiment at the CERN-LHC was performed by exposing stacks of detector
foils to heavy ion beams with energies ranging from 340 MeV/nucleon to 150
GeV/nucleon. After chemical etching, the base areas and lengths of etch-pit
cones were measured using automatic and manual optical microscopes. The
response of the detectors, as measured by the ratio of the track-etching rate
over the bulk-etching rate, was determined over a range extending from their
threshold at Z/$\beta\sim7$ and $\sim50$ for CR39 and Makrofol, respectively,
up to Z/$\beta\sim92$",['physics.ins-det'],2502.16351," Neural radiance field (NeRF) research has made significant progress in
modeling static video content captured in the wild. However, current models and
rendering processes rarely consider scenes captured underwater, which are
useful for studying and filming ocean life. They fail to address visual
artifacts unique to underwater scenes, such as moving fish and suspended
particles. This paper introduces a novel NeRF renderer and optimization scheme
for an implicit MLP-based NeRF model. Our renderer reduces the influence of
floaters and moving objects that interfere with static objects of interest by
estimating a single surface per ray. We use a Gaussian weight function with a
small offset to ensure that the transmittance of the surrounding media remains
constant. Additionally, we enhance our model with a depth-based scaling
function to upscale gradients for near-camera volumes. Overall, our method
outperforms the baseline Nerfacto by approximately 7.5\% and SeaThru-NeRF by
6.2% in terms of PSNR. Subjective evaluation also shows a significant reduction
of artifacts while preserving details of static targets and background compared
to the state of the arts.",['cs.CV'],False,,,,"Calibration of Solid State Nuclear Track Detectors for Rare Event
  Searches","AquaNeRF: Neural Radiance Fields in Underwater Media with Distractor
  Removal"
neg-d2-715,2025-01-10,,2501.0588," Designing efficient neural networks for embedded devices is a critical
challenge, particularly in applications requiring real-time performance, such
as aerial imaging with drones and UAVs for emergency responses. In this work,
we introduce TakuNet, a novel light-weight architecture which employs
techniques such as depth-wise convolutions and an early downsampling stem to
reduce computational complexity while maintaining high accuracy. It leverages
dense connections for fast convergence during training and uses 16-bit
floating-point precision for optimization on embedded hardware accelerators.
Experimental evaluation on two public datasets shows that TakuNet achieves
near-state-of-the-art accuracy in classifying aerial images of emergency
situations, despite its minimal parameter count. Real-world tests on embedded
devices, namely Jetson Orin Nano and Raspberry Pi, confirm TakuNet's
efficiency, achieving more than 650 fps on the 15W Jetson board, making it
suitable for real-time AI processing on resource-constrained platforms and
advancing the applicability of drones in emergency scenarios. The code and
implementation details are publicly released.","['cs.CV', 'cs.PF']",2502.04364," Recent advancements in diffusion models have driven the growth of text-guided
image editing tools, enabling precise and iterative modifications of
synthesized content. However, as these tools become increasingly accessible,
they also introduce significant risks of misuse, emphasizing the critical need
for robust attribution methods to ensure content authenticity and traceability.
Despite the creative potential of such tools, they pose significant challenges
for attribution, particularly in adversarial settings where edits can be
layered to obscure an image's origins. We propose LambdaTracer, a novel
latent-space attribution method that robustly identifies and differentiates
authentic outputs from manipulated ones without requiring any modifications to
generative or editing pipelines. By adaptively calibrating reconstruction
losses, LambdaTracer remains effective across diverse iterative editing
processes, whether automated through text-guided editing tools such as
InstructPix2Pix and ControlNet or performed manually with editing software such
as Adobe Photoshop. Extensive experiments reveal that our method consistently
outperforms baseline approaches in distinguishing maliciously edited images,
providing a practical solution to safeguard ownership, creativity, and
credibility in the open, fast-evolving AI ecosystems.","['cs.CV', 'cs.AI', 'cs.HC', 'cs.LG']",False,,,,"TakuNet: an Energy-Efficient CNN for Real-Time Inference on Embedded UAV
  systems in Emergency Response Scenarios",Lost in Edits? A $\lambda$-Compass for AIGC Provenance
neg-d2-716,2025-01-26,,2501.1542," Classifier-Free Guidance (CFG) has been a default technique in various visual
generative models, yet it requires inference from both conditional and
unconditional models during sampling. We propose to build visual models that
are free from guided sampling. The resulting algorithm, Guidance-Free Training
(GFT), matches the performance of CFG while reducing sampling to a single
model, halving the computational cost. Unlike previous distillation-based
approaches that rely on pretrained CFG networks, GFT enables training directly
from scratch. GFT is simple to implement. It retains the same maximum
likelihood objective as CFG and differs mainly in the parameterization of
conditional models. Implementing GFT requires only minimal modifications to
existing codebases, as most design choices and hyperparameters are directly
inherited from CFG. Our extensive experiments across five distinct visual
models demonstrate the effectiveness and versatility of GFT. Across domains of
diffusion, autoregressive, and masked-prediction modeling, GFT consistently
achieves comparable or even lower FID scores, with similar diversity-fidelity
trade-offs compared with CFG baselines, all while being guidance-free. Code
will be available at https://github.com/thu-ml/GFT.","['cs.CV', 'cs.AI', 'cs.LG']",2501.1326," On the kagome lattice, electrons benefit from the simultaneous presence of
band topology, flat electronic bands, and van Hove singularities, forming
competing or cooperating orders. Understanding the interrelation between these
distinct order parameters remains a significant challenge, leaving much of the
associated physics unexplored. In the kagome superconductor KV3Sb5, which
exhibits a charge density wave (CDW) state below T = 78 K, we uncover an
unpredicted field-induced phase transition below 6 K. The observed transition
is marked by a hysteretic anomaly in the resistivity, nonlinear electrical
transport, and a change in the symmetry of the electronic response as probed
via the angular dependence of the magnetoresistivity. These observations
surprisingly suggest the emergence of an unanticipated broken symmetry state
coexisting with the original CDW. To understand this experimental observation,
we developed a theoretical minimal model for the normal state inside the
high-temperature parent CDW phase where an incommensurate CDW order emerges as
an instability sub-leading to superconductivity. The incommensurate CDW emerges
when superconducting fluctuations become fully suppressed by large magnetic
fields. Our results suggest that, in kagome superconductors, quantum states can
either coexist or are nearly degenerate in energy, indicating that these are
rich platforms to expose new correlated phenomena.","['cond-mat.str-el', 'cond-mat.mes-hall', 'cond-mat.mtrl-sci']",False,,,,Visual Generation Without Guidance,Field induced density wave in a kagome superconductor
neg-d2-717,2025-03-04,,2503.05812," Frontier AI models -- highly capable foundation models at the cutting edge of
AI development -- may pose severe risks to public safety, human rights,
economic stability, and societal value in the coming years. These risks could
arise from deliberate adversarial misuse, system failures, unintended cascading
effects, or simultaneous failures across multiple models.
  In response to such risks, at the AI Seoul Summit in May 2024, 16 global AI
industry organizations signed the Frontier AI Safety Commitments, and 27
nations and the EU issued a declaration on their intent to define these
thresholds. To fulfill these commitments, organizations must determine and
disclose ``thresholds at which severe risks posed by a model or system, unless
adequately mitigated, would be deemed intolerable.''
  To assist in setting and operationalizing intolerable risk thresholds, we
outline key principles and considerations; for example, to aim for ``good, not
perfect'' thresholds in the face of limited data on rapidly advancing AI
capabilities and consequently evolving risks. We also propose specific
threshold recommendations, including some detailed case studies, for a subset
of risks across eight risk categories: (1) Chemical, Biological, Radiological,
and Nuclear (CBRN) Weapons, (2) Cyber Attacks, (3) Model Autonomy, (4)
Persuasion and Manipulation, (5) Deception, (6) Toxicity, (7) Discrimination,
and (8) Socioeconomic Disruption. Our goal is to serve as a starting point or
supplementary resource for policymakers and industry leaders, encouraging
proactive risk management that prioritizes preventing intolerable risks (ex
ante) rather than merely mitigating them after they occur (ex post).","['cs.CY', 'cs.AI', 'cs.CR', 'cs.HC', 'cs.LG']",2503.17472," Our goal is to estimate the total gas mass in the direction of the Central
Molecular Zone (CMZ), quantify the various uncertainties associated, and
discuss the implications for the estimates of CR energy densities and dust
opacities. The $H_{\rm{I}}$ 21 cm line and the carbon monoxide isotopes
($^{12}\rm{CO}$, $^{13}\rm{CO}$ and $\rm{C}^{18}\rm{O}$) line emission maps are
used to derive the total gas column density. The gas in the CMZ is separated
from the disk contribution in position and velocity thanks to its different
properties in term of velocity dispersion and brightness ratio of CO isotopes.
The variations of the $X_{\rm{CO}}$ factors are modelled relying on both
theoretical trends from simulations and empirical corrections. We use the new
gas column density estimated together with gamma-ray and dust emission
measurements to derive the CR energy density and dust opacities, respectively.
The $X_{\rm{CO}}$ values in the CMZ range from $(0.32 - 1.37) \ \times$
$10^{20}$ cm$^{-2}$ K$^{-1}$ km$^{-1}$ s, with a distribution that is highly
asymmetric and skewed. The median value is $ \rm{\overline{X}_{CO}^{CMZ}} =
0.39 \ \times$ $10^{20}$ cm$^{-2}$ K$^{-1}$ km$^{-1}$ s. The total gas mass in
the CMZ is estimated to be $2.3_{-0.3}^{+0.3}\times10^{7} \; \rm{M_{\odot}}$
with $\sim 10 \%$ contribution from the atomic phase. Without removing the disk
contamination the total mass is about twice higher, and the atomic gas fraction
increases to $\sim30\%$. The cosmic-ray (CR) energy density in the CMZ,
assuming a 1/r profile, is higher by a factor of two compared to the previous
calculations at TeV energies. Using molecular gas tracers which probes only the
densest molecular cores leads to an overestimation of the CR energy density,
while ignoring the foreground/background contribution leads to an
underestimation of the CR energy density in the CMZ.","['astro-ph.GA', 'astro-ph.HE']",False,,,,Intolerable Risk Threshold Recommendations for Artificial Intelligence,"Cosmic rays, gas and dust in the Central Molecular Zone I -- $X_{CO}$
  factors, cosmic-ray densities and dust opacities"
neg-d2-718,2025-01-31,,2501.18993," Image Super-Resolution (ISR) has seen significant progress with the
introduction of remarkable generative models. However, challenges such as the
trade-off issues between fidelity and realism, as well as computational
complexity, have also posed limitations on their application. Building upon the
tremendous success of autoregressive models in the language domain, we propose
\textbf{VARSR}, a novel visual autoregressive modeling for ISR framework with
the form of next-scale prediction. To effectively integrate and preserve
semantic information in low-resolution images, we propose using prefix tokens
to incorporate the condition. Scale-aligned Rotary Positional Encodings are
introduced to capture spatial structures and the diffusion refiner is utilized
for modeling quantization residual loss to achieve pixel-level fidelity.
Image-based Classifier-free Guidance is proposed to guide the generation of
more realistic images. Furthermore, we collect large-scale data and design a
training process to obtain robust generative priors. Quantitative and
qualitative results show that VARSR is capable of generating high-fidelity and
high-realism images with more efficiency than diffusion-based methods. Our
codes will be released at https://github.com/qyp2000/VARSR.",['cs.CV'],2503.06314," Understanding how renormalized quasiparticles emerge in strongly correlated
electron materials provides a challenge for both experiment and theory. It has
been predicted that distinctive spin and orbital screening mechanisms drive
this process in multiorbital materials with strong Coulomb and Hund's
interactions. Here, we provide the experimental evidence of both mechanisms
from angle-resolved photoemission spectroscopy on RbFe$_2$As$_2$. We observe
that the emergence of low-energy Fe 3$d_{xy}$ quasiparticles below 90K is tied
to spin screening. A second process changes the spectral weight at high
energies up to room temperature. Supported by theoretical calculations we
attribute it to orbital screening of Fe 3d atomic excitations. These two
cascading screening processes drive the temperature evolution from a bad metal
to a correlated Fermi liquid.",['cond-mat.str-el'],False,,,,Visual Autoregressive Modeling for Image Super-Resolution,"Observation of Two Cascading Screening Processes in an Iron-based
  Superconductor"
neg-d2-719,2025-02-28,,2503.05793," Medical education faces challenges in scalability, accessibility, and
consistency, particularly in clinical skills training for physician-patient
communication. Traditional simulation-based learning, while effective, is
resource-intensive, difficult to schedule, and often highly variable in
feedback quality. Through a collaboration between AI, learning science, and
medical education experts, we co-developed MedSimAI, an AI-powered simulation
platform that enables deliberate practice, self-regulated learning (SRL), and
automated assessment through interactive patient encounters. Leveraging large
language models (LLMs), MedSimAI generates realistic clinical interactions and
provides immediate, structured feedback using established medical evaluation
frameworks such as the Master Interview Rating Scale (MIRS). In a pilot study
with 104 first-year medical students, we examined engagement, conversation
patterns, and user perceptions. Students found MedSimAI beneficial for
repeated, realistic patient-history practice. Conversation analysis revealed
that certain higher-order skills were often overlooked, though students
generally performed systematic histories and empathic listening. By integrating
unlimited practice opportunities, real-time AI assessment, and SRL principles,
MedSimAI addresses key limitations of traditional simulation-based training,
making high-quality clinical education more accessible and scalable.","['cs.CY', 'cs.AI', 'cs.CL']",2501.09671," Flapping-based propulsive systems rely on fluid-structure interactions to
produce thrust. At intermediate and high Reynolds numbers, vortex formation and
organization in the wake of such systems are crucial for the generation of a
propulsive force. In this work, we experimentally investigate the wake produced
by a tethered robotic fish immersed in a water tunnel. By systematically
varying the amplitude and frequency of the fish tail as well as the free-stream
speed, we are able to observe and characterize different vortex streets as a
function of the Strouhal number. The produced wakes are three-dimensional and
exhibit a classical V-shape, mainly with two oblique trains of vortex rings
convecting outward. Using two-dimensional Particle Image Velocimetry (PIV) in
the mid-span plane behind the fish and through extensive data processing of the
velocity and vorticity fields, we demonstrate the strong couplings at place
between vortex dynamics, thrust production and wake structure. We first measure
the evolution of the vortex velocity with the Strouhal number, and model it
using a momentum balance equation directly related to thrust production. We
then focus on the wake structure, such as wake angle as well as vortex ring
orientation, diameter and vorticity. The wake structure is modelled in a simple
geometrical framework where the vortex ring velocity is composed of the
free-stream speed and the ring self-advecting speed. This framework is tested
and validated by our experimental measurements as well as literature data
collapsing on master curves, highlighting a universal behavior dominated by the
Strouhal number. This allows us to establish a comprehensive understanding of
how the wake structure varies with this number and, thus, thrust production.",['physics.flu-dyn'],False,,,,"MedSimAI: Simulation and Formative Feedback Generation to Enhance
  Deliberate Practice in Medical Education","Undulatory underwater swimming: Linking vortex dynamics, thrust, and
  wake structure with a biorobotic fish"
neg-d2-720,2025-01-23,,2501.14087," We consider a generating set of reparametrization invariants that can be
constructed from the couplings and masses entering the scalar potential of the
general Two-Higgs-Doublet Model (2HDM). Being independent of higgs-basis
rotations, they generate a polynomial ring of basis invariants that represent
the physical content of the model. Ignoring for the moment gauge and Yukawa
interactions, we derive six-loop renormalization group equations (RGE) for all
the invariants entering the set. We do not compute a single Feynman diagram but
rely heavily on the general RGE results for scalar theories. We use linear
algebra together with techniques from Invariant Theory. The latter not only
allow one to compute the number of linearly independent invariants entering
beta functions at a certain loop order (via Hilbert series) but also provide a
convenient tool for dealing with polynomial relations (so-called syzygies)
between invariants from the generating set.",['hep-ph'],2501.06778," In this study, we analyze the observational images of a Konoplya-Zhidenko
rotating non-Kerr black hole, wherein a thin accretion disk, serving as the
sole background light source, is situated on the equatorial plane of the black
hole. The inner boundary of the thin accretion disk extends to the event
horizon, and the accretion material in the disk exhibits two different motion
behaviors, that is, it moves along the critical plunging orbit inside the
innermost stable circular orbit (ISCO) and follows the Keplerian orbit outside
the ISCO. The shadow image is captured on the imaging plane of a zero angular
momentum observer utilizing advanced fisheye camera ray-tracing techniques. The
results demonstrate that an image consistently reveals a dark region encircled
by a narrow photon ring, which is called the inner shadow. At low observation
inclination angles, the observation intensity is highly concentrated, with the
lensed image of accretion disk being superimposed on the direct image. As
observation inclination angle increases, the direct and lensed images gradually
separate, becoming distinctly distinguishable and forming a hat-like structure.
Furthermore, variations in the parameter space and observation angle will
influence pertinent image characteristics, including image symmetry, the range
or deformation degree of the inner shadow. We further examined the distinctive
characteristics of images observed in both prograde and retrograde accretion
disk scenarios. Subsequently, we also examined the redshift distribution on the
disk. The findings indicate that while variations in relevant parameters do
influence the redshift distribution, the primary factor is the change in
observational inclination. The observer can detect both redshift and blueshift
phenomena on the screen when viewed at a higher observation angle.","['astro-ph.HE', 'gr-qc']",False,,,,"On the scalar sector of 2HDM: ring of basis invariants, syzygies, and
  six-loop renormalization-group equations","Optical appearance of the Konoplya-Zhidenko rotating non-Kerr black hole
  surrounded by a thin accretion disk"
neg-d2-721,2025-03-19,,2503.14965," Visualizations are powerful tools for conveying information but often rely on
accompanying text for essential context and guidance. This study investigates
the impact of annotation patterns on reader preferences and comprehension
accuracy among multilingual populations, addressing a gap in visualization
research. We conducted experiments with two groups fluent in English and either
Tamil (n = 557) or Arabic (n = 539) across six visualization types, each
varying in annotation volume and semantic content. Full-text annotations
yielded the highest comprehension accuracy across all languages, while
preferences diverged: English readers favored highly annotated charts, whereas
Tamil/Arabic readers preferred full-text or minimally annotated versions.
Semantic variations in annotations (L1-L4) did not significantly affect
comprehension, demonstrating the robustness of text comprehension across
languages. English annotations were generally preferred, with a tendency to
think technically in English linked to greater aversion to non-English
annotations, though this diminished among participants who regularly switched
languages internally. Non-English annotations incorporating visual or external
knowledge were less favored, particularly in titles. Our findings highlight
cultural and educational factors influencing perceptions of visual information,
underscoring the need for inclusive annotation practices for diverse linguistic
audiences. All data and materials are available at: https://osf.io/ckdb4/.",['cs.HC'],2502.19477," Effective field theories (EFTs) parametrize our ignorance of the underlying
UV theory through their Wilson coefficients. However, not all values of these
coefficients are consistent with fundamental physical principles. In this
paper, we explore the consequences of imposing causal propagation on the
comoving curvature perturbation in the EFT of inflation, particularly its
impact on the primordial power spectrum and the effective sound speed
$c_s^\text{eff}$. We investigate scenarios where $c_s^\text{eff}$ undergoes a
transition, remaining consistent with CMB constraints at early times but later
experiencing a drastic change, becoming highly subluminal. Such scenarios allow
the primordial power spectrum to grow at small scales, potentially leading to
the formation of primordial black holes or the generation of scalar-induced
gravitational waves. We find the generic feature that in a causal theory,
luminal sound speeds imply a free theory, effectively constraining the
dynamics. Additionally, we obtain that when considering natural values for the
Wilson coefficients, maintaining the validity of the EFT and the weakly coupled
regime, and enforcing causal propagation of the EFT modes, the power spectrum
cannot increase drastically. This imposes significant constraints on the
parameter space of models aiming to produce such features.","['hep-th', 'astro-ph.CO']",False,,,,"Lost in Translation: How Does Bilingualism Shape Reader Preferences for
  Annotated Charts?",Causality Bounds on the Primordial Power Spectrum
neg-d2-722,2025-03-22,,2503.17882," Recent advancements in large language models (LLMs) have demonstrated that
fine-tuning and human alignment can render LLMs harmless. In practice, such
""harmlessness"" behavior is mainly achieved by training models to reject harmful
requests, such as ""Explain how to burn down my neighbor's house"", where the
model appropriately declines to respond. However, this approach can
inadvertently result in false refusal, where models reject benign queries as
well, such as ""Tell me how to kill a Python process"". In this work, we
demonstrate that prompting safety reflection before generating a response can
mitigate false refusal behavior. Building on this finding, we introduce the
Think-Before-Refusal (TBR) schema and conduct safety-aware instruction
fine-tuning incorporating safety reflection. In an ablation study across 15
pre-trained models, we show that models fine-tuned with safety reflection
significantly reduce false refusal behavior while maintaining safety and
overall performance compared to those fine-tuned without safety reflection.","['cs.CL', 'cs.AI']",2503.07414," This study aims to develop a cost-effective microgrid design that optimally
balances the economic feasibility, reliability, efficiency, and environmental
impact in a grid-tied community microgrid. A multi-objective optimization
framework is employed, integrating HOMER Pro for system sizing with deep
reinforcement learning (DRL). Sensitivity analyses are conducted to evaluate
the system performance under varying load demand and renewable energy
fluctuations, while an economic sensitivity assessment examines the impact of
electricity prices and capital costs on the Levelized Cost of Energy (LCOE).
The proposed microgrid configuration achieves high reliability, satisfying 100%
of the load, even under adverse weather conditions. The proposed framework
attains an efficiency of 91.99% while maintaining a carbon footprint of 302,747
kg/year, which is approximately 95% lower than that of the grid system. The
economic analysis indicates a net present cost (NPC) of $4.83M with a
competitive LCOE of $0.208/kWh. In addition, the operation cost is $201,473 per
year with a capital investment of $1.42M, rendering it a financially viable
alternative to conventional grid-dependent systems.This work can be valuable in
identifying effective solutions for supplying reliable and cost-effective power
to regional and remote areas.","['eess.SY', 'cs.SY']",False,,,,"Think Before Refusal : Triggering Safety Reflection in LLMs to Mitigate
  False Refusal Behavior",Cost-Effective Design of Grid-tied Community Microgrid
neg-d2-723,2025-03-16,,2503.12674," The entanglement spectra for a subsystem in a spin chain fine-tuned to a
quantum-critical point contains signatures of the underlying quantum field
theory that governs its low-energy properties. For an open chain with given
boundary conditions described by a 2D conformal field theory~(CFT), the
entanglement spectrum of the left/right half of the system coincides with a
boundary CFT spectrum, where one of the boundary conditions arise due to the
`entanglement cut'. The latter has been argued to be conformal and has been
numerically found to be the `free' boundary condition for Ising, Potts and free
boson theories. For these models, the `free' boundary condition for the lattice
degree of freedom has a counterpart in the continuum theory. However, this is
not true in general. Here, this question is analyzed for the unitary minimal
models of 2D CFTs using the density matrix renormalization group technique. The
entanglement spectra are computed for blocks of spins in open chains of A-type
restricted solid-on-solid models with identical boundary conditions at the
ends. The imposed boundary conditions are realized exactly for these lattice
models due to their integrable nature. The obtained entanglement spectra are in
good agreement with certain boundary CFT spectra. The boundary condition for
the entanglement cut is found to be conformal and to coincide with the one with
the highest boundary entropy. This identification enables determination of the
exponents governing the unusual corrections to the entanglement entropy from
the CFT partition functions. These are compared with numerical results.","['quant-ph', 'hep-th', 'math-ph', 'math.MP']",2503.123," By imposing conditions upon the index of a self-centralizing subgroup of a
group, and upon the index of the center of the group, we are able to classify
the Chermak-Delgado lattice of the group. This is our main result. We use this
result to classify the Chermak-Delgado lattices of dicyclic groups and of
metabelian $p$-groups of maximal class.",['math.GR'],False,,,,"Boundary Conditions for the Entanglement Cut in 2D Conformal Field
  Theories",On the Chermak-Delgado lattice of a finite group
neg-d2-724,2025-02-07,,2502.051," Virtual reality enables users to experience real-life situations in immersive
environments. Interaction methods significantly shape user experience,
particularly in high fidelity simulations mimicking real world tasks. This
study evaluates two primary VR interaction techniques, hand based and
controller based, through virtual shopping tasks in a simulated supermarket
with 40 participants. Hand-based interaction was preferred for its natural,
immersive qualities and alignment with real-world gestures but faced usability
challenges, including limited haptic feedback and grasping inefficiencies. In
contrast, controller-based interaction offered greater precision and
reliability, making it more suitable for tasks requiring fine motor skills.",['cs.HC'],2501.05015," Adversarial attacks are allegedly unnoticeable. Prior studies have designed
attack noticeability measures on graphs, primarily using statistical tests to
compare the topology of original and (possibly) attacked graphs. However, we
observe two critical limitations in the existing measures. First, because the
measures rely on simple rules, attackers can readily enhance their attacks to
bypass them, reducing their attack ""noticeability"" and, yet, maintaining their
attack performance. Second, because the measures naively leverage global
statistics, such as degree distributions, they may entirely overlook attacks
until severe perturbations occur, letting the attacks be almost ""totally
unnoticeable."" To address the limitations, we introduce HideNSeek, a learnable
measure for graph attack noticeability. First, to mitigate the bypass problem,
HideNSeek learns to distinguish the original and (potential) attack edges using
a learnable edge scorer (LEO), which scores each edge on its likelihood of
being an attack. Second, to mitigate the overlooking problem, HideNSeek
conducts imbalance-aware aggregation of all the edge scores to obtain the final
noticeability score. Using six real-world graphs, we empirically demonstrate
that HideNSeek effectively alleviates the observed limitations, and LEO (i.e.,
our learnable edge scorer) outperforms eleven competitors in distinguishing
attack edges under five different attack methods. For an additional
application, we show that LEO boost the performance of robust GNNs by removing
attack-like edges.","['cs.LG', 'cs.AI']",False,,,,"Hands vs. Controllers: Comparing User Interactions in Virtual Reality
  Shopping Environments","On Measuring Unnoticeability of Graph Adversarial Attacks: Observations,
  New Measure, and Applications"
neg-d2-725,2025-01-19,,2501.11089," Strontium titanate (STO) possesses promising properties for applications in
thermoelectricity, catalysis, fuel cells, and more, but its performance is
highly dependent on stoichiometry and impurity levels. While atom probe
tomography (APT) can provide detailed three-dimensional atomic-scale chemical
information, STO specimens have been challenging to analyze due to premature
specimen fracture. In this study, we show that by applying a thin metal coating
to atom probe tips, STO specimens can be analyzed with nearly 100% success.
Using this approach, we investigate both undoped STO and 1 at% Nb-doped STO,
achieving sufficient sensitivity to detect Nb concentrations as low as 0.7 at%.
This work establishes a reliable APT method for high-resolution chemical
analysis of STO at the nanoscale.",['cond-mat.mtrl-sci'],2502.14314," You Look Only Once (YOLO) models have been widely used for building real-time
object detectors across various domains. With the increasing frequency of new
YOLO versions being released, key questions arise. Are the newer versions
always better than their previous versions? What are the core innovations in
each YOLO version and how do these changes translate into real-world
performance gains? In this paper, we summarize the key innovations from YOLOv1
to YOLOv11, introduce a comprehensive benchmark called ODverse33, which
includes 33 datasets spanning 11 diverse domains (Autonomous driving,
Agricultural, Underwater, Medical, Videogame, Industrial, Aerial, Wildlife,
Retail, Microscopic, and Security), and explore the practical impact of model
improvements in real-world, multi-domain applications through extensive
experimental results. We hope this study can provide some guidance to the
extensive users of object detection models and give some references for future
real-time object detector development.",['cs.CV'],False,,,,"Advancing Atom Probe Tomography of SrTiO$_3$: Measurement Methodology
  and Impurity Detection Limits","ODVerse33: Is the New YOLO Version Always Better? A Multi Domain
  benchmark from YOLO v5 to v11"
neg-d2-726,2025-03-15,,2503.12075," We develop a structure theory for the limit of $SU(2)$ $G_2$-monopoles (resp.
Calabi-Yau monopoles) on a principal $SU(2)$-bundle over an asymptotically
conical $G_2$-manifolds (resp. Calabi-Yau 3-folds) as the mass parameter tends
to infinity, while the topologial data for the bundle stays fixed. We show how
to extract a singular abelian $G_2$-monopole (resp. Calabi-Yau monopole) with
Dirac singularity along a calibrated cycle in the large mass limit, and we
prove an energy identity for monopole bubbles.",['math.DG'],2502.11524," In this paper we deal with generalizations of the Mahler volume product for
log-concave functions. We show that the polarity transform $\mathcal A$ can be
rescaled so that the Mahler product it induces has upper and lower bounds of
the same asymptotics. We discuss a similar result for the $\mathcal J$
transform.
  As an application, we extend the K\""onig-Milman duality of entropy result to
the class of geometric log-concave functions.",['math.FA'],False,,,,The large mass limit of $G_2$ and Calabi-Yau monopoles,The Scaled Polarity transform and related inequalities
neg-d2-727,2025-02-12,,2502.08298," The integration of Large Language Models (LLMs) into optimization has created
a powerful synergy, opening exciting research opportunities. This paper
investigates how LLMs can enhance existing optimization algorithms. Using their
pre-trained knowledge, we demonstrate their ability to propose innovative
heuristic variations and implementation strategies. To evaluate this, we
applied a non-trivial optimization algorithm, Construct, Merge, Solve and Adapt
(CMSA) -- a hybrid metaheuristic for combinatorial optimization problems that
incorporates a heuristic in the solution construction phase. Our results show
that an alternative heuristic proposed by GPT-4o outperforms the
expert-designed heuristic of CMSA, with the performance gap widening on larger
and denser graphs. Project URL: https://imp-opt-algo-llms.surge.sh/","['cs.AI', 'cs.CL', 'cs.LG', 'cs.SE']",2503.16719," Cloud services have become an essential infrastructure for enterprises and
individuals. Access to these cloud services is typically governed by Identity
and Access Management systems, where user authentication often relies on
passwords. While best practices dictate the implementation of multi-factor
authentication, it's a reality that many such users remain solely protected by
passwords. This reliance on passwords creates a significant vulnerability, as
these credentials can be compromised through various means, including
side-channel attacks. This paper exploits keyboard acoustic emanations to infer
typed natural language passphrases via unsupervised learning, necessitating no
previous training data. Whilst this work focuses on short passphrases, it is
also applicable to longer messages, such as confidential emails, where the
margin for error is much greater, than with passphrases, making the attack even
more effective in such a setting. Unlike traditional attacks that require
physical access to the target device, acoustic side-channel attacks can be
executed within the vicinity, without the user's knowledge, offering a
worthwhile avenue for malicious actors. Our findings replicate and extend
previous work, confirming that cross-correlation audio preprocessing
outperforms methods like mel-frequency-cepstral coefficients and fast-fourier
transforms in keystroke clustering. Moreover, we show that partial passphrase
recovery through clustering and a dictionary attack can enable faster than
brute-force attacks, further emphasizing the risks posed by this attack vector.",['cs.CR'],False,,,,Improving Existing Optimization Algorithms with LLMs,Practical Acoustic Eavesdropping On Typed Passphrases
neg-d2-728,2025-03-18,,2503.14137," Applying the least action principle to the motion of an ideal gas, we find
Bernoulli's equation where the local velocity is expressed as the gradient of a
velocity potential, while the internal energy depends on the interaction among
the particles of the gas. Then, assuming that the internal energy is
proportional non-locally to the logarithm of the mass density and truncating
the resulting sum of density gradients after the second term, we find an
additional Bohm's quantum potential term in the internal energy. Therefore, the
Bernoulli equation reduces to the Madelung equation, revealing a novel
classical description of quantum fluids that does not require to postulate
quantum mechanics. Finally, non-locality can be removed by introducing a
retarded potential, thus leading to a covariant formulation of the quantum
potential and of the equation of motion of an ideal quantum fluid.",['quant-ph'],2503.04962," Topological insulators (TIs) are intriguing materials for advanced computing
applications based on spintronics because they can host robust spin effects.
For instance, TIs have intrinsically large spin generation enabled by their
large spin-orbit coupling. Furthermore, topological surface states (TSS) with
spin-momentum locking and Dirac dispersion lead to long spin diffusion. Future
spintronic device technology will require scalable film growth of high-quality
material. We grow epitaxial films of Bi$_{1-x}$Sb$_x$Te$_{3-y}$Se$_y$ (BSTS, $x
= 0.58, y = 1$) and confirm the gapless band structure with optimal doping
using angle-resolved photoelectron spectra. The temperature dependence of
longitudinal resistivity shows bulk transport is suppressed as temperature is
decreased, and at low temperature surface transport dominates. We evaluate the
spin transport properties in BSTS without using ferromagnetic tunnel contacts
via a non-local resistance experiment as a function of temperature and applied
charge current. As expected, these experiments reveal the necessity of
decreasing the bulk conduction to best enhance the spin transport. In the TSS,
we find high efficiency of charge-to-spin conversion (spin Hall angle,
$\theta_{SH} \approx 1$) and spin diffusion over several microns. Further
development of high-quality TIs will make them viable candidates for efficient
and lossless spintronics.","['cond-mat.mes-hall', 'physics.app-ph']",False,,,,"A variational formulation of the governing equations of ideal quantum
  fluids",Intrinsic Spin Transport in a Topological Insulator Thin Film
neg-d2-729,2025-02-05,,2502.03276," We present a method for high-precision numerical evaluations of Lauricella
functions, whose indices are linearly dependent on some parameter
$\varepsilon$, in terms of their Laurent series expansions at zero. This method
is based on finding analytic continuations of these functions in terms of
Frobenius generalized power series. Being one-dimensional, these series are
much more suited for high-precision numerical evaluations than
multi-dimensional sums arising in approaches to analytic continuations based on
re-expansions of hypergeometric series or Mellin--Barnes integral
representations. To accelerate the calculation procedure further, the
$\varepsilon$ dependence of the result is reconstructed from the evaluations of
given Lauricella functions at specific numerical values of $\varepsilon$,
which, in addition, allows for efficient parallel implementation. The method
has been implemented in the $\texttt{PrecisionLauricella}$ package, written in
Wolfram Mathematica language.","['hep-th', 'hep-ph', 'math-ph', 'math.MP']",2503.04962," Topological insulators (TIs) are intriguing materials for advanced computing
applications based on spintronics because they can host robust spin effects.
For instance, TIs have intrinsically large spin generation enabled by their
large spin-orbit coupling. Furthermore, topological surface states (TSS) with
spin-momentum locking and Dirac dispersion lead to long spin diffusion. Future
spintronic device technology will require scalable film growth of high-quality
material. We grow epitaxial films of Bi$_{1-x}$Sb$_x$Te$_{3-y}$Se$_y$ (BSTS, $x
= 0.58, y = 1$) and confirm the gapless band structure with optimal doping
using angle-resolved photoelectron spectra. The temperature dependence of
longitudinal resistivity shows bulk transport is suppressed as temperature is
decreased, and at low temperature surface transport dominates. We evaluate the
spin transport properties in BSTS without using ferromagnetic tunnel contacts
via a non-local resistance experiment as a function of temperature and applied
charge current. As expected, these experiments reveal the necessity of
decreasing the bulk conduction to best enhance the spin transport. In the TSS,
we find high efficiency of charge-to-spin conversion (spin Hall angle,
$\theta_{SH} \approx 1$) and spin diffusion over several microns. Further
development of high-quality TIs will make them viable candidates for efficient
and lossless spintronics.","['cond-mat.mes-hall', 'physics.app-ph']",False,,,,High-precision numerical evaluation of Lauricella functions,Intrinsic Spin Transport in a Topological Insulator Thin Film
neg-d2-730,2025-02-11,,2502.07744," A realistic description of active particles should include interactions with
the medium, commonly a momentum-conserving simple fluid, in which they are
suspended. In this work, we consider a multi-species suspension of
self-diffusiophoretic Janus colloids interacting via chemical and hydrodynamic
fields. Through a systematic coarse-graining of the microscopic dynamics, we
calculate the multi-component contribution to the hydrodynamic stress tensor of
the incompressible Stokesian fluid in which the particles are immersed. For a
single species, we find that the strength of the stress produced by the
gradients of the number density field is determined by the particles'
self-propulsion and chemotactic alignment, and can be tuned to be either
contractile or extensile. For a multi-species system, we unveil how different
forms of activity modify the stress tensor, and how non-reciprocity in
hydrodynamic interactions emerges in an active binary mixture.","['cond-mat.stat-mech', 'cond-mat.soft']",2502.15654," As Large Language Models (LLMs) become increasingly prevalent, their
generated outputs are proliferating across the web, risking a future where
machine-generated content dilutes human-authored text. Since online data is the
primary resource for LLM pre-training, subsequent models could be trained on an
unknown portion of synthetic samples. This will lead to model collapse, a
degenerative process whereby LLMs reinforce their own errors, and ultimately
yield a declining performance. In this study, we investigate the impact of
decoding strategy on model collapse, analysing the characteristics of text at
each model generation, the similarity to human references, and the resulting
model performance. Using the decoding strategies that lead to the most
significant degradation, we evaluate model collapse in more realistic scenarios
where the origin of the data (human or synthetic) is unknown. We train a
machine-generated text detector and propose an importance sampling approach to
alleviate model collapse. Our method is validated on two LLM variants (GPT-2
and SmolLM2) on the open-ended text generation task. We demonstrate that it can
not only prevent model collapse but also improve performance when sufficient
human-authored samples are present.","['cs.CL', 'cs.LG']",False,,,,"Hydrodynamic stresses in a multi-species suspension of active Janus
  colloids",Machine-generated text detection prevents language model collapse
neg-d2-731,2025-02-07,,2502.05159," Large language models (LLMs) demonstrate impressive capabilities across many
tasks yet risk reproducing copyrighted content verbatim, raising legal and
ethical concerns. Although methods like differential privacy or neuron editing
can reduce memorization, they typically require costly retraining or direct
access to model weights and may degrade performance. To address these
challenges, we propose TokenSwap, a lightweight, post-hoc approach that
replaces the probabilities of grammar-related tokens with those from a small
auxiliary model (e.g., DistilGPT-2). We run extensive experiments on commercial
grade models such as Pythia-6.9b and LLaMA-3-8b and demonstrate that our method
effectively reduces well-known cases of memorized generation by upto 10x with
little to no impact on downstream tasks. Our approach offers a uniquely
accessible and effective solution to users of real-world systems.","['cs.LG', 'cs.CL']",2502.13728," Dataset Distillation (DD) is a powerful technique for reducing large datasets
into compact, representative synthetic datasets, accelerating Machine Learning
training. However, traditional DD methods operate in a centralized manner,
which poses significant privacy threats and reduces its applicability. To
mitigate these risks, we propose a Secure Federated Data Distillation (SFDD)
framework to decentralize the distillation process while preserving privacy.
Unlike existing Federated Distillation techniques that focus on training global
models with distilled knowledge, our approach aims to produce a distilled
dataset without exposing local contributions. We leverage the
gradient-matching-based distillation method, adapting it for a distributed
setting where clients contribute to the distillation process without sharing
raw data. The central aggregator iteratively refines a synthetic dataset by
integrating client-side updates while ensuring data confidentiality. To make
our approach resilient to inference attacks perpetrated by the server that
could exploit gradient updates to reconstruct private data, we create an
optimized Local Differential Privacy approach, called LDPO-RLD. Furthermore, we
assess the framework's resilience against malicious clients executing backdoor
attacks (such as Doorping) and demonstrate robustness under the assumption of a
sufficient number of participating clients. Our experimental results
demonstrate the effectiveness of SFDD and that the proposed defense concretely
mitigates the identified vulnerabilities, with minimal impact on the
performance of the distilled dataset. By addressing the interplay between
privacy and federation in dataset distillation, this work advances the field of
privacy-preserving Machine Learning making our SFDD framework a viable solution
for sensitive data-sharing applications.","['cs.CR', 'cs.AI']",False,,,,A Lightweight Method to Disrupt Memorized Sequences in LLM,Secure Federated Data Distillation
neg-d2-732,2025-02-11,,2502.07366," A holobiont is made up of a host organism together with its microbiota. In
the context of animal breeding, as the holobiont can be viewed as the single
unit upon which selection operates, integrating microbiota data into genomic
prediction models may be a promising approach to improve predictions of
phenotypic and genetic values. Nevertheless, there is a paucity of hologenomic
transgenerational data to address this hypothesis, and thus to fill this gap,
we propose a new simulation framework. Our approach, an R Implementation of a
Transgenerational Hologenomic Model-based Simulator (RITHMS) is an open-source
package, builds upon the MoBPS package and incorporates distinctive
characteristics of the microbiota, notably vertical and horizontal transmission
as well as modulation due to the environment and host genetics. In addition,
RITHMS can account for a variety of selection strategies and is adaptable to
different genetic architectures. We simulated transgenerational hologenomic
data using RITHMS under a wide variety of scenarios, varying heritability,
microbiability, and microbiota heritability. We found that simulated data
accurately reflected expected characteristics, notably based on microbial
diversity metrics, correlation between taxa, modulation of vertical and
horizontal transmission, response to environmental effects and the evolution of
phenotypic values depending on selection strategy. Our results support the
relevance of our simulation framework and illustrate its possible use for
building a selection index balancing genetic gain and microbial diversity.
RITHMS is an advanced, flexible tool for generating transgenerational
hologenomic data that incorporate the complex interplay between genetics,
microbiota and environment.",['stat.ME'],2502.03282," We compute two-loop helicity amplitudes in QCD for diphoton production
through quark- and gluon-initiated channels, accounting for a massive internal
quark loop by keeping its full mass dependence. Using physical projectors, we
directly decompose the amplitude into its helicity components. By renormalising
the heavy quark mass in on-shell, and other quantities in $\overline{\rm MS}$
schemes, we obtain finite remainders. This work paves the way for calculating
the cross-section for diphoton production at higher orders in QCD with a
massive quark loop, employing different subtraction schemes. The effect of a
heavy quark is expected to play a crucial role in high-luminosity LHC.","['hep-ph', 'hep-th']",False,,,,"RITHMS : An advanced stochastic framework for the simulation of
  transgenerational hologenomic data","Two-loop helicity amplitudes for diphoton production with massive quark
  loop"
neg-d2-733,2025-03-10,,2503.07087," The development of a generalist agent with adaptive multiple manipulation
skills has been a long-standing goal in the robotics community. In this paper,
we explore a crucial task, skill-incremental learning, in robotic manipulation,
which is to endow the robots with the ability to learn new manipulation skills
based on the previous learned knowledge without re-training. First, we build a
skill-incremental environment based on the RLBench benchmark, and explore how
traditional incremental methods perform in this setting. We find that they
suffer from severe catastrophic forgetting due to the previous methods on
classification overlooking the characteristics of temporality and action
complexity in robotic manipulation tasks. Towards this end, we propose an
incremental Manip}ulation framework, termed iManip, to mitigate the above
issues. We firstly design a temporal replay strategy to maintain the integrity
of old skills when learning new skill. Moreover, we propose the extendable
PerceiverIO, consisting of an action prompt with extendable weight to adapt to
new action primitives in new skill. Extensive experiments show that our
framework performs well in Skill-Incremental Learning. Codes of the
skill-incremental environment with our framework will be open-source.",['cs.RO'],2503.0342," The spontaneous (so-called Quincke) rotation of an uncharged, solid,
dielectric, spherical particle under a steady electric field is analyzed,
accounting for the inertia of the particle and the transient fluid inertia, or
``hydrodynamic memory,'' due to the unsteady Stokes flow around the particle.
The dynamics of the particle are encapsulated in three coupled nonlinear
integro-differential equations for the evolution of the angular velocity of the
particle, and the components of the induced dipole of the particle that are
parallel and transverse to the applied field. These equations represent a
generalization of the celebrated Lorenz system. A numerical solution of these
`modified Lorenz equations' (MLE) shows that hydrodynamic memory leads to an
increase in the threshold field strength for chaotic particle rotation, which
is in qualitative agreement with experimental observations. Furthermore,
hydrodynamic memory leads to an increase in the range of field strengths where
multi-stability between steady and chaotic rotation occurs. At large field
strengths, chaos ceases and the particle is predicted to execute periodic
rotational motion.",['physics.flu-dyn'],False,,,,iManip: Skill-Incremental Learning for Robotic Manipulation,Hydrodynamic memory and Quincke rotation
neg-d2-734,2025-03-12,,2503.09329," This work explores an extension of ML-optimized piecewise polynomial
approximation by incorporating energy optimization as an additional objective.
Traditional closed-form solutions enable continuity and approximation targets
but lack flexibility in accommodating complex optimization goals. By leveraging
modern gradient descent optimizers within TensorFlow, we introduce a framework
that minimizes total curvature in cam profiles, leading to smoother motion and
reduced energy consumption for input data that is unfavorable for sole
approximation and continuity optimization. Experimental results confirm the
effectiveness of this approach, demonstrating its potential to improve
efficiency in scenarios where input data is noisy or suboptimal for
conventional methods.",['cs.LG'],2503.06663," Deep neural network (DNN) inference in energy harvesting (EH) devices poses
significant challenges due to resource constraints and frequent power
interruptions. These power losses not only increase end-to-end latency, but
also compromise inference consistency and accuracy, as existing checkpointing
and restore mechanisms are prone to errors. Consequently, the quality of
service (QoS) for DNN inference on EH devices is severely impacted. In this
paper, we propose an energy-adaptive DNN inference mechanism capable of
dynamically transitioning the model into a low-power mode by reducing
computational complexity when harvested energy is limited. This approach
ensures that end-to-end latency requirements are met. Additionally, to address
the limitations of error-prone checkpoint-and-restore mechanisms, we introduce
a checkpoint-free intermittent inference framework that ensures consistent,
progress-preserving DNN inference during power failures in energy-harvesting
systems.",['cs.CE'],False,,,,"Energy Optimized Piecewise Polynomial Approximation Utilizing Modern
  Machine Learning Optimizers","Energy-Adaptive Checkpoint-Free Intermittent Inference for Low Power
  Energy Harvesting Systems"
neg-d2-735,2025-01-17,,2501.10074," Spatial reasoning is an essential problem in embodied AI research. Efforts to
enhance spatial reasoning abilities through supplementary spatial data and
fine-tuning have proven limited and ineffective when addressing complex
embodied tasks, largely due to their dependence on language-based outputs.
While some approaches have introduced a point-based action space to mitigate
this issue, they fall short in managing more intricate tasks within complex
environments. This deficiency arises from their failure to fully exploit the
inherent thinking and reasoning capabilities that are fundamental strengths of
Vision-Language Models (VLMs). To address these limitations, we propose a novel
approach named SpatialCoT, specifically designed to bolster the spatial
reasoning capabilities of VLMs. Our approach comprises two stages: spatial
coordinate bi-directional alignment, which aligns vision-language inputs with
spatial coordinates, and chain-of-thought spatial grounding, which harnesses
the reasoning capabilities of language models for advanced spatial reasoning.
We evaluate SpatialCoT on challenging navigation and manipulation tasks, both
in simulation and real-world settings. Experimental results demonstrate that
our method significantly outperforms previous state-of-the-art approaches in
both tasks.","['cs.RO', 'cs.AI', 'cs.CV']",2501.18993," Image Super-Resolution (ISR) has seen significant progress with the
introduction of remarkable generative models. However, challenges such as the
trade-off issues between fidelity and realism, as well as computational
complexity, have also posed limitations on their application. Building upon the
tremendous success of autoregressive models in the language domain, we propose
\textbf{VARSR}, a novel visual autoregressive modeling for ISR framework with
the form of next-scale prediction. To effectively integrate and preserve
semantic information in low-resolution images, we propose using prefix tokens
to incorporate the condition. Scale-aligned Rotary Positional Encodings are
introduced to capture spatial structures and the diffusion refiner is utilized
for modeling quantization residual loss to achieve pixel-level fidelity.
Image-based Classifier-free Guidance is proposed to guide the generation of
more realistic images. Furthermore, we collect large-scale data and design a
training process to obtain robust generative priors. Quantitative and
qualitative results show that VARSR is capable of generating high-fidelity and
high-realism images with more efficiency than diffusion-based methods. Our
codes will be released at https://github.com/qyp2000/VARSR.",['cs.CV'],False,,,,"SpatialCoT: Advancing Spatial Reasoning through Coordinate Alignment and
  Chain-of-Thought for Embodied Task Planning",Visual Autoregressive Modeling for Image Super-Resolution
neg-d2-736,2025-03-10,,2503.07969," With the advent of deep learning, expression recognition has made significant
advancements. However, due to the limited availability of annotated compound
expression datasets and the subtle variations of compound expressions, Compound
Emotion Recognition (CE) still holds considerable potential for exploration. To
advance this task, the 7th Affective Behavior Analysis in-the-wild (ABAW)
competition introduces the Compound Expression Challenge based on C-EXPR-DB, a
limited dataset without labels. In this paper, we present a curriculum
learning-based framework that initially trains the model on single-expression
tasks and subsequently incorporates multi-expression data. This design ensures
that our model first masters the fundamental features of basic expressions
before being exposed to the complexities of compound emotions. Specifically,
our designs can be summarized as follows: 1) Single-Expression Pre-training:
The model is first trained on datasets containing single expressions to learn
the foundational facial features associated with basic emotions. 2) Dynamic
Compound Expression Generation: Given the scarcity of annotated compound
expression datasets, we employ CutMix and Mixup techniques on the original
single-expression images to create hybrid images exhibiting characteristics of
multiple basic emotions. 3) Incremental Multi-Expression Integration: After
performing well on single-expression tasks, the model is progressively exposed
to multi-expression data, allowing the model to adapt to the complexity and
variability of compound expressions. The official results indicate that our
method achieves the \textbf{best} performance in this competition track with an
F-score of 0.6063. Our code is released at https://github.com/YenanLiu/ABAW7th.",['cs.CV'],2502.21272," Let $K = \mathbb{R}$ or $\mathbb{C}$. An $n$-element subset $A$ of $K$ is a
$B_h$-set if every element of $K$ has at most one representation as the sum of
$h$ not necessarily distinct elements of $A$. Associated to the $B_h$ set $A =
\{a_1,\ldots, a_n\}$ are the $B_h$-vectors $\mathbf{a} = (a_1,\ldots, a_n)$ in
$K^n$. This paper proves that ``almost all'' $n$-element subsets of $K$ are
$B_h$-sets in the sense that the set of all $B_h$-vectors is a dense open
subset of $K^n$.","['math.CO', 'math.NT']",False,,,,7ABAW-Compound Expression Recognition via Curriculum Learning,$B_h$-sets of real and complex numbers
neg-d2-737,2025-02-17,,2502.12119," Visual instruction tuning refines pre-trained Multimodal Large Language
Models (MLLMs) to enhance their real-world task performance. However, the rapid
expansion of visual instruction datasets introduces significant data
redundancy, leading to excessive computational costs. Existing data selection
methods predominantly rely on proxy models or loss-based metrics, both of which
impose substantial computational overheads due to the necessity of model
inference and backpropagation. To address this challenge, we propose PRISM, a
novel training-free approach for efficient multimodal data selection. Unlike
existing methods, PRISM eliminates the reliance on proxy models, warm-up
pretraining, and gradient-based optimization. Instead, it leverages Pearson
correlation analysis to quantify the intrinsic visual encoding properties of
MLLMs, computing a task-specific correlation score to identify high-value
instances. This not only enbles data-efficient selection,but maintains the
original performance. Empirical evaluations across multiple MLLMs demonstrate
that PRISM reduces the overall time required for visual instruction tuning and
data selection to just 30% of conventional methods, while surpassing fully
fine-tuned models across eight multimodal and three language understanding
benchmarks, achieving a 101.7% relative improvement in final performance.","['cs.CV', 'cs.AI', 'cs.CL']",2501.08056," We present numerical simulations of x-ray magnetic circular dichroism (XMCD)
at the L$_{2,3}$ edge of Ni in the weakly ferromagnetic altermagnet NiF$_2$.
Our results predict a significant XMCD signal for light propagating
perpendicular to the magnetic moments, which are approximately aligned along
the [100] easy-axis direction. The analysis shows that the altermagnetic and
ferromagnetic contributions to the XMCD signal can be uniquely distinguished by
their dependence on an applied magnetic field. By varying the angle of the
field relative to the easy axis, the in-plane orientation of both the N\'eel
vector and the net magnetization can be systematically controlled. We further
demonstrate that the XMCD signal, even under fields as strong as 40 T and for
any in-plane orientation, can be accurately described as a linear combination
of two spectral components, with geometrical prefactors determined by the field
magnitude and direction. This insight enables experimental validation of the
distinctive relationship between the N\'eel vector orientation and the x-ray
Hall vector in the rutile structure. Quantitative simulations supporting these
findings are provided.","['cond-mat.mtrl-sci', 'cond-mat.str-el']",False,,,,"PRISM: Self-Pruning Intrinsic Selection Method for Training-Free
  Multimodal Data Selection","Magnetic Dichroism in Rutile NiF$_2$: Separating Altermagnetic and
  Ferromagnetic Effects"
neg-d2-738,2025-02-05,,2502.03282," We compute two-loop helicity amplitudes in QCD for diphoton production
through quark- and gluon-initiated channels, accounting for a massive internal
quark loop by keeping its full mass dependence. Using physical projectors, we
directly decompose the amplitude into its helicity components. By renormalising
the heavy quark mass in on-shell, and other quantities in $\overline{\rm MS}$
schemes, we obtain finite remainders. This work paves the way for calculating
the cross-section for diphoton production at higher orders in QCD with a
massive quark loop, employing different subtraction schemes. The effect of a
heavy quark is expected to play a crucial role in high-luminosity LHC.","['hep-ph', 'hep-th']",2501.03818," We study the Dirichlet dynamical zeta function $\eta_D(s)$ for billiard flow
corresponding to several strictly convex disjoint obstacles. For large ${\rm
Re}\: s$ we have $\eta_D(s) =\sum_{n= 1}^{\infty} a_n e^{-\lambda_n s}, \: a_n
\in \mathbb R$ and $\eta_D$ admits a meromorphic continuation to $\mathbb C$.
We obtain some conditions of the frequencies $\lambda_n$ and some sums of
coefficients $a_n$ which imply that $\eta_D$ cannot be prolonged as entire
function.","['math.DS', 'math.NT']",False,,,,"Two-loop helicity amplitudes for diphoton production with massive quark
  loop",Dirichlet dynamical zeta function for billiard flow
neg-d2-739,2025-02-07,,2502.04706," This paper focuses on simulating text dialogues in which impressions between
speakers improve during speed dating. This simulation involves selecting an
utterance from multiple candidates generated by a text generation model that
replicates a specific speaker's utterances, aiming to improve the impression of
the speaker. Accurately selecting an utterance that improves the impression is
crucial for the simulation. We believe that whether an utterance improves a
dialogue partner's impression of the speaker may depend on the personalities of
both parties. However, recent methods for utterance selection do not consider
the impression per utterance or the personalities. To address this, we propose
a method that predicts whether an utterance improves a partner's impression of
the speaker, considering the personalities. The evaluation results showed that
personalities are useful in predicting impression changes per utterance.
Furthermore, we conducted a human evaluation of simulated dialogues using our
method. The results showed that it could simulate dialogues more favorably
received than those selected without considering personalities.","['cs.CL', 'cs.HC']",2501.08747," We prove that, to every abstract group $G$, we can associate a sequence of
graphs $\Gamma_n$ such that the automorphism group of $\Gamma_n$ is isomorphic
to $G$ and the genus of $\Gamma_n$ is an unbounded function of $n$.","['math.GR', 'math.CO']",False,,,,"Enhancing Impression Change Prediction in Speed Dating Simulations Based
  on Speakers' Personalities","Every group is the automorphism group of a graph with arbitrarily large
  genus"
neg-d2-740,2025-02-02,,2502.00693," The Bloom filter is a simple yet space-efficient probabilistic data structure
that supports membership queries for dramatically large datasets. It is widely
utilized and implemented across various industrial scenarios, often handling
massive datasets that include sensitive user information necessitating privacy
preservation. To address the challenge of maintaining privacy within the Bloom
filter, we have developed the DPBloomfilter. This innovation integrates the
classical differential privacy mechanism, specifically the Random Response
technique, into the Bloom filter, offering robust privacy guarantees under the
same running complexity as the standard Bloom filter. Through rigorous
simulation experiments, we have demonstrated that our DPBloomfilter algorithm
maintains high utility while ensuring privacy protections. To the best of our
knowledge, this is the first work to provide differential privacy guarantees
for the Bloom filter for membership query problems.",['cs.CR'],2503.05488," Document Key Information Extraction (KIE) is a technology that transforms
valuable information in document images into structured data, and it has become
an essential function in industrial settings. However, current evaluation
metrics of this technology do not accurately reflect the critical attributes of
its industrial applications. In this paper, we present KIEval, a novel
application-centric evaluation metric for Document KIE models. Unlike prior
metrics, KIEval assesses Document KIE models not just on the extraction of
individual information (entity) but also of the structured information
(grouping). Evaluation of structured information provides assessment of
Document KIE models that are more reflective of extracting grouped information
from documents in industrial settings. Designed with industrial application in
mind, we believe that KIEval can become a standard evaluation metric for
developing or applying Document KIE models in practice. The code will be
publicly available.",['cs.CL'],False,,,,DPBloomfilter: Securing Bloom Filters with Differential Privacy,KIEval: Evaluation Metric for Document Key Information Extraction
neg-d2-741,2025-02-05,,2502.04365," Approximately 10% of newborns need some assistance to start breathing and 5\%
proper ventilation. It is crucial that interventions are initiated as soon as
possible after birth. Accurate documentation of Time of Birth (ToB) is thereby
essential for documenting and improving newborn resuscitation performance.
However, current clinical practices rely on manual recording of ToB, typically
with minute precision. In this study, we present an AI-driven, video-based
system for automated ToB detection using thermal imaging, designed to preserve
the privacy of healthcare providers and mothers by avoiding the use of
identifiable visual data. Our approach achieves 91.4% precision and 97.4%
recall in detecting ToB within thermal video clips during performance
evaluation. Additionally, our system successfully identifies ToB in 96% of test
cases with an absolute median deviation of 1 second compared to manual
annotations. This method offers a reliable solution for improving ToB
documentation and enhancing newborn resuscitation outcomes.","['cs.CV', 'cs.AI']",2503.1537," We reappraise the idea of colliding with robots, moving from a position that
tries to avoid or mitigate collisions to one that considers them an important
facet of human interaction. We report on a soma design workshop that explored
how our bodies could collide with telepresence robots, mobility aids, and a
quadruped robot. Based on our findings, we employed soma trajectories to
analyse collisions as extended experiences that negotiate key transitions of
consent, preparation, launch, contact, ripple, sting, untangle, debris and
reflect. We then employed these ideas to analyse two collision experiences, an
accidental collision between a person and a drone, and the deliberate design of
a robot to play with cats, revealing how real-world collisions involve the
complex and ongoing entanglement of soma trajectories. We discuss how viewing
collisions as entangled trajectories, or tangles, can be used analytically, as
a design approach, and as a lens to broach ethical complexity.","['cs.RO', 'cs.HC']",False,,,,"AI-Based Thermal Video Analysis in Privacy-Preserving Healthcare: A Case
  Study on Detecting Time of Birth",Tangles: Unpacking Extended Collision Experiences with Soma Trajectories
neg-d2-742,2025-02-05,,2502.03529," Interpreting galactic luminosity requires assumptions about the galaxy-wide
initial mass function (gwIMF), often assumed invariant in most stellar
population synthesis (SPS) models. If stars form in clusters with metallicity-
and density-dependent \textit{stellar IMFs}, the integrated galaxy-wide IMF
(IGIMF) can be calculated, with its shape depending on the star formation rate
(SFR) and metallicity. The shape of the IGIMF thus depends on the star
formation rate (SFR) and metallicity. We develop the \texttt{SPS-VarIMF} code
which enables us for the first time to compute the spectra, luminosities, and
remnant populations of galaxies in the context of the varying gwIMF with time,
SFR, and an assumed metallicity. Using the \texttt{SPS-VarIMF} code one can
calculate how the interpretation from the integrated galactic light may change
if the underlying galaxy-wide IMF is assumed to be environmentally dependent
instead of being invariant. In particular, we compare the time evolution of the
galaxy color and the stellar mass-to-light ratio in different bands for the
IGIMF and invariant canonical gwIMF assuming constant and delayed-$\tau$ star
formation histories. We show that the underlying gwIMF can be determined by
examining the colors and luminosities of late-type galaxies in UV and optical
bands. On the other hand, for early-type galaxies, it is difficult to
distinguish which gwIMF is valid since adopting the different gwIMFs yields
almost identical colors. However, their gwIMF-dependent $M/L$ ratios differ by
up to an order of magnitude. Massive present-day elliptical galaxies would have
been $10^4$ times as bright as at present when they were forming.",['astro-ph.GA'],2503.02768," We develop a denotational model for programs that have standard programming
constructs such as conditionals and while-loops, as well as probabilistic and
concurrent commands. Whereas semantic models for languages with either
concurrency or randomization are well studied, their combination is limited to
languages with bounded loops. Our work is the first to consider both
randomization and concurrency for a language with unbounded looping constructs.
The interaction between Boolean tests (arising from the control flow
structures), probabilistic actions, and concurrent execution creates challenges
in generalizing previous work on pomsets and convex languages, prominent models
for those effects, individually. To illustrate the generality of our model, we
show that it recovers a typical powerdomain semantics for concurrency, as well
as the convex powerset semantics for probabilistic nondeterminism.","['cs.PL', 'cs.LO']",False,,,,Stellar population synthesis models with a physically varying IMF,Denotational Semantics for Probabilistic and Concurrent Programs
neg-d2-743,2025-01-15,,2501.08623," Understanding which effective field theories are consistent with an
ultraviolet completion in quantum gravity is an important theoretical question.
Therefore, it is important to know the structure of the 4D effective theory
associated with a given compactification of string theory. We present a
first-principles derivation of the low-energy 4D effective theory of geometric
moduli in a warped Calabi-Yau compactification of type IIB string theory with
imaginary self-dual 3-form flux. This completes the derivation of the metric on
K\""ahler moduli space from the 10D equations of motion. We also give the first
derivation of an effective action for flat directions in the complex structure
moduli space of the Calabi-Yau (which generically mix with the axiodilaton).",['hep-th'],2503.18075," Gaussian variational approximations are widely used for summarizing posterior
distributions in Bayesian models, especially in high-dimensional settings.
However, a drawback of such approximations is the inability to capture skewness
or more complex features of the posterior. Recent work suggests applying
skewness corrections to existing Gaussian or other symmetric approximations to
address this limitation. We propose to incorporate the skewness correction into
the definition of an approximating variational family. We consider
approximating the posterior for hierarchical models, in which there are
``global'' and ``local'' parameters. A baseline variational approximation is
defined as the product of a Gaussian marginal posterior for global parameters
and a Gaussian conditional posterior for local parameters given the global
ones. Skewness corrections are then considered. The adjustment of the
conditional posterior term for local variables is adaptive to the global
parameter value. Optimization of baseline variational parameters is performed
jointly with the skewness correction. Our approach allows the location, scale
and skewness to be captured separately, without using additional parameters for
skewness adjustments. The proposed method substantially improves accuracy for
only a modest increase in computational cost compared to state-of-the-art
Gaussian approximations. Good performance is demonstrated in generalized linear
mixed models and multinomial logit discrete choice models.",['stat.ME'],False,,,,"Dimensional Reduction and K\""ahler Metric for Metric Moduli in Imaginary
  Self-Dual Flux","Variational inference for hierarchical models with conditional scale and
  skewness corrections"
neg-d2-744,2025-01-13,,2501.0779," Robust Bayesian inference using density power divergence (DPD) has emerged as
a promising approach for handling outliers in statistical estimation. While the
DPD-based posterior offers theoretical guarantees for robustness, its practical
implementation faces significant computational challenges, particularly for
general parametric models with intractable integral terms. These challenges
become especially pronounced in high-dimensional settings where traditional
numerical integration methods prove inadequate and computationally expensive.
We propose a novel sampling methodology that addresses these limitations by
integrating the loss-likelihood bootstrap with a stochastic gradient descent
algorithm specifically designed for DPD-based estimation. Our approach enables
efficient and scalable sampling from DPD-based posteriors for a broad class of
parametric models, including those with intractable integrals, and we further
extend it to accommodate generalized linear models. Through comprehensive
simulation studies, we demonstrate that our method efficiently samples from
DPD-based posteriors, offering superior computational scalability compared to
conventional methods, particularly in high-dimensional settings. The results
also highlight its ability to handle complex parametric models with intractable
integral terms.",['stat.ME'],2502.04164," Distributed optimization has become the default training paradigm in modern
machine learning due to the growing scale of models and datasets. To mitigate
communication overhead, local updates are often applied before global
aggregation, resulting in a nested optimization approach with inner and outer
steps. However, heavy-tailed stochastic gradient noise remains a significant
challenge, particularly in attention-based models, hindering effective
training. In this work, we propose TailOPT, an efficient framework designed to
address heavy-tailed noise by leveraging adaptive optimization or clipping
techniques. We establish convergence guarantees for the TailOPT framework under
heavy-tailed noise with potentially unbounded gradient variance and local
updates. Among its variants, we highlight a memory and communication efficient
instantiation which we call $Bi^2Clip$, which performs coordinate-wise clipping
at both the inner and outer optimizers, achieving adaptive-like performance
(e.g., Adam) without the cost of maintaining or transmitting additional
gradient statistics. Empirically, TailOPT, including $Bi^2Clip$, demonstrates
superior performance on several language tasks and models, outperforming
state-of-the-art methods.",['cs.LG'],False,,,,"Sampling from Density power divergence-based Generalized posterior
  distribution via Stochastic optimization",Efficient Distributed Optimization under Heavy-Tailed Noise
neg-d2-745,2025-02-18,,2502.12705," Two-dimensional (2D) layered nanomaterials heterostructures, arising from the
combination of 2D materials with other low-dimensional species, feature large
surface area to volume ratio, which provides a high density of active sites for
catalytic ap-plications and in particular for (photo)electrocatalysis (PEC).
Meanwhile, their unique electronic band structure and high electrical
conductivity enable efficient charge transfer (CT) between the active material
and the substrate, which is essential for catalytic activity. In recent years,
researchers have demonstrated the potential of a range of 2D material
interfaces, such as graphene, graphitic carbon nitride (g-C3N4), metal
chalcogenides (MCs), and MXenes, for (photo)electrocatalytic applica-tions. For
instance, MCs such as MoS2 and WS2 have shown excellent catalytic activity for
hydrogen evolution, while gra-phene and MXenes have been used for the reduction
of carbon dioxide to higher value chemicals. However, despite their great
potential, there are still major challenges that need to be addressed in order
to fully realize the potential of 2D materials for PEC. For example, their
stability under harsh reaction conditions, as well as their scalability for
large-scale production are important factors to be considered. Generating
heterojunctions (HJs) by combining 2D layered structures with other
na-nomaterials is a promising method to improve the photoelectrocatalytic
properties of the former. In this review, we inspect thoroughly the recent
literature, to demonstrate the significant potential that arises from utilizing
2D layered heterostructures in PEC processes across a broad spectrum of
applications, from energy conversion and storage to environmental remediation.
With the ongoing research and development, it is likely that the potential of
these materials will be fully expressed in the near future.",['cond-mat.mtrl-sci'],2502.03171," Localization methods based on holographic multiple input multiple output
(HMIMO) have gained much attention for its potential to achieve high accuracy.
By deploying multiple HMIMOs, we can improve the link quality and system
coverage. As the scale of HMIMO increases to improve beam control capability,
the near-field (NF) region of each HMIMO expands. However, existing multiple
HMIMO-enabled methods mainly focus on the far-field (FF) of each HMIMO, which
leads to low localization accuracy when applied in the NF. In this paper, a
hybrid NF and FF localization method aided by multiple RISs, a low cost
implementation of HMIMO, is proposed. In such a scenario, it is difficult to
achieve user localization and RIS optimization since the equivalent NF of all
RISs expands, which results in high complexity, and we need to handle the
interference caused by multiple RISs. To tackle this challenge, we propose a
two-phase RIS-enabled localization method that first estimate the relative
locations of the user to each RIS and fuse the results to obtain the global
estimation. In this way, the algorithm complexity is reduced. We formulate the
RIS optimization problem to keep the RIS sidelobe as low as possible to
minimize the interference. The effectiveness of the proposed method is verified
through simulations.",['eess.SP'],False,,,,2D Layered Heterojunctions for Photoelectrocatalysis,"Hybrid Near-Field and Far-Field Localization with Multiple Holographic
  MIMO Surfaces"
neg-d2-746,2025-01-23,,2501.14005," Deep-learning-based face recognition (FR) systems are susceptible to
adversarial examples in both digital and physical domains. Physical attacks
present a greater threat to deployed systems as adversaries can easily access
the input channel, allowing them to provide malicious inputs to impersonate a
victim. This paper addresses the limitations of existing projector-camera-based
adversarial light attacks in practical FR setups. By incorporating device-aware
adaptations into the digital attack algorithm, such as resolution-aware and
color-aware adjustments, we mitigate the degradation from digital to physical
domains. Experimental validation showcases the efficacy of our proposed
algorithm against real and spoof adversaries, achieving high physical
similarity scores in FR models and state-of-the-art commercial systems. On
average, there is only a 14% reduction in scores from digital to physical
attacks, with high attack success rate in both white- and black-box scenarios.","['cs.CV', 'cs.AI']",2501.17881," Wireless indoor localization has been a pivotal area of research over the
last two decades, becoming a cornerstone for numerous sensing applications.
However, conventional wireless localization methods rely on channel state
information to perform blind modelling and estimation of a limited set of
localization parameters. This oversimplification neglects many sensing scene
details, resulting in suboptimal localization accuracy. To address this
limitation, this paper presents a novel approach to wireless indoor
localization by reformulating it as an inverse problem of wireless ray-tracing,
inferring scene parameters that generates the measured CSI. At the core of our
solution is a fully differentiable ray-tracing simulator that enables
backpropagation to comprehensive parameters of the sensing scene, allowing for
precise localization. To establish a robust localization context, RayLoc
constructs a high-fidelity sensing scene by refining coarse-grained background
model. Furthermore, RayLoc overcomes the challenges of sparse gradient and
local minima by convolving the signal generation process with a Gaussian
kernel. Extensive experiments showcase that RayLoc outperforms traditional
localization baselines and is able to generalize to different sensing
environments.","['eess.SP', 'cs.AI', 'cs.LG', 'cs.NI']",False,,,,"Device-aware Optical Adversarial Attack for a Portable Projector-camera
  System","RayLoc: Wireless Indoor Localization via Fully Differentiable
  Ray-tracing"
neg-d2-747,2025-03-08,,2503.06222," The vision-based semantic scene completion task aims to predict dense
geometric and semantic 3D scene representations from 2D images. However, the
presence of dynamic objects in the scene seriously affects the accuracy of the
model inferring 3D structures from 2D images. Existing methods simply stack
multiple frames of image input to increase dense scene semantic information,
but ignore the fact that dynamic objects and non-texture areas violate
multi-view consistency and matching reliability. To address these issues, we
propose a novel method, CDScene: Vision-based Robust Semantic Scene Completion
via Capturing Dynamic Representations. First, we leverage a multimodal
large-scale model to extract 2D explicit semantics and align them into 3D
space. Second, we exploit the characteristics of monocular and stereo depth to
decouple scene information into dynamic and static features. The dynamic
features contain structural relationships around dynamic objects, and the
static features contain dense contextual spatial information. Finally, we
design a dynamic-static adaptive fusion module to effectively extract and
aggregate complementary features, achieving robust and accurate semantic scene
completion in autonomous driving scenarios. Extensive experimental results on
the SemanticKITTI, SSCBench-KITTI360, and SemanticKITTI-C datasets demonstrate
the superiority and robustness of CDScene over existing state-of-the-art
methods.",['cs.CV'],2502.12705," Two-dimensional (2D) layered nanomaterials heterostructures, arising from the
combination of 2D materials with other low-dimensional species, feature large
surface area to volume ratio, which provides a high density of active sites for
catalytic ap-plications and in particular for (photo)electrocatalysis (PEC).
Meanwhile, their unique electronic band structure and high electrical
conductivity enable efficient charge transfer (CT) between the active material
and the substrate, which is essential for catalytic activity. In recent years,
researchers have demonstrated the potential of a range of 2D material
interfaces, such as graphene, graphitic carbon nitride (g-C3N4), metal
chalcogenides (MCs), and MXenes, for (photo)electrocatalytic applica-tions. For
instance, MCs such as MoS2 and WS2 have shown excellent catalytic activity for
hydrogen evolution, while gra-phene and MXenes have been used for the reduction
of carbon dioxide to higher value chemicals. However, despite their great
potential, there are still major challenges that need to be addressed in order
to fully realize the potential of 2D materials for PEC. For example, their
stability under harsh reaction conditions, as well as their scalability for
large-scale production are important factors to be considered. Generating
heterojunctions (HJs) by combining 2D layered structures with other
na-nomaterials is a promising method to improve the photoelectrocatalytic
properties of the former. In this review, we inspect thoroughly the recent
literature, to demonstrate the significant potential that arises from utilizing
2D layered heterostructures in PEC processes across a broad spectrum of
applications, from energy conversion and storage to environmental remediation.
With the ongoing research and development, it is likely that the potential of
these materials will be fully expressed in the near future.",['cond-mat.mtrl-sci'],False,,,,"Vision-based 3D Semantic Scene Completion via Capture Dynamic
  Representations",2D Layered Heterojunctions for Photoelectrocatalysis
neg-d2-748,2025-01-22,,2501.13068," The interconnection between the human lungs and other organs, such as the
liver and kidneys, is crucial for understanding the underlying risks and
effects of lung diseases and improving patient care. However, most research
chest CT imaging is focused solely on the lungs due to considerations of cost
and radiation dose. This restricted field of view (FOV) in the acquired images
poses challenges to comprehensive analysis and hinders the ability to gain
insights into the impact of lung diseases on other organs. To address this, we
propose SCOPE (Spatial Coverage Optimization with Prior Encoding), a novel
approach to capture the inter-organ relationships from CT images and extend the
FOV of chest CT images. Our approach first trains a variational autoencoder
(VAE) to encode 2D axial CT slices individually, then stacks the latent
representations of the VAE to form a 3D context for training a latent diffusion
model. Once trained, our approach extends the FOV of CT images in the
z-direction by generating new axial slices in a zero-shot manner. We evaluated
our approach on the National Lung Screening Trial (NLST) dataset, and results
suggest that it effectively extends the FOV to include the liver and kidneys,
which are not completely covered in the original NLST data acquisition.
Quantitative results on a held-out whole-body dataset demonstrate that the
generated slices exhibit high fidelity with acquired data, achieving an SSIM of
0.81.","['cs.CV', 'eess.IV']",2502.12108," Integrated Gradients (IG), a widely used axiomatic path-based attribution
method, assigns importance scores to input features by integrating model
gradients along a straight path from a baseline to the input. While effective
in some cases, we show that straight paths can lead to flawed attributions. In
this paper, we identify the cause of these misattributions and propose an
alternative approach that treats the input space as a Riemannian manifold,
computing attributions by integrating gradients along geodesics. We call this
method Geodesic Integrated Gradients (GIG). To approximate geodesic paths, we
introduce two techniques: a k-Nearest Neighbours-based approach for smaller
models and a Stochastic Variational Inference-based method for larger ones.
Additionally, we propose a new axiom, Strong Completeness, extending the axioms
satisfied by IG. We show that this property is desirable for attribution
methods and that GIG is the only method that satisfies it. Through experiments
on both synthetic and real-world data, we demonstrate that GIG outperforms
existing explainability methods, including IG.","['cs.LG', 'cs.AI', 'stat.ML']",False,,,,"Beyond the Lungs: Extending the Field of View in Chest CT with Latent
  Diffusion Models",Using the Path of Least Resistance to Explain Deep Networks
neg-d2-749,2025-02-17,,2502.11967," High intensity coherent light can dress matter, realizing new hybrid phases
that are not accessible in equilibrium. This effect results from the coherent
interaction between Bloch states inside the solid and the periodic field of
impinging photons which produces hybrid light-matter states called
Floquet-Bloch states that can alter properties of the solid. Optically inducing
a topological state in a semiconductor using so-called Floquet engineering is
an exciting prospect. However, it has not been realized, despite its
theoretical prediction more than 10 years ago. Here we show that an
ultrashort-lived topological state that is absent at equilibrium in the ground
state of SnTe can be created with femtosecond light pulses. This occurs when
the photoexcitation is similar in energy with the band gap of this polar
semiconductor. We observe a concomitant renormalization of the band dispersions
that reveals the generation of Floquet states connecting to the topological
state. We therefore provide the first direct experimental observation of a
Floquet topological state and propose that it is driven by a light-induced band
inversion in SnTe. Our discovery opens the way for controlling optically
on-demand the topological properties of semiconductors.",['cond-mat.str-el'],2502.03123," In this study, Disentanglement in Difference(DiD) is proposed to address the
inherent inconsistency between the statistical independence of latent variables
and the goal of semantic disentanglement in disentanglement representation
learning. Conventional disentanglement methods achieve disentanglement
representation by improving statistical independence among latent variables.
However, the statistical independence of latent variables does not necessarily
imply that they are semantically unrelated, thus, improving statistical
independence does not always enhance disentanglement performance. To address
the above issue, DiD is proposed to directly learn semantic differences rather
than the statistical independence of latent variables. In the DiD, a Difference
Encoder is designed to measure the semantic differences; a contrastive loss
function is established to facilitate inter-dimensional comparison. Both of
them allow the model to directly differentiate and disentangle distinct
semantic factors, thereby resolving the inconsistency between statistical
independence and semantic disentanglement. Experimental results on the dSprites
and 3DShapes datasets demonstrate that the proposed DiD outperforms existing
mainstream methods across various disentanglement metrics.","['cs.LG', 'cs.AI']",False,,,,Floquet topological state induced by light-driven band inversion in SnTe,"Disentanglement in Difference: Directly Learning Semantically
  Disentangled Representations by Maximizing Inter-Factor Differences"
neg-d2-750,2025-01-12,,2501.06782," An edge-coloring of a graph $H$ is a function $\mathcal{C}: E(H) \rightarrow
\mathbb{N}$. We say that $H$ is rainbow if all edges of $H$ have different
colors. Given a graph $F$, an edge-colored graph $G$ is $F$-rainbow saturated
if $G$ does not contain a rainbow copy of $F$, but the addition of any nonedge
with any color on it would create a rainbow copy of $F$. The rainbow saturation
number $rsat(n,F)$ is the minimum number of edges in an $F$-rainbow saturated
graph with order $n$. In this paper we proved several results on cycle rainbow
saturation. For $n \geq 5$, we determined the exact value of $rsat(n,C_4)$. For
$ n \geq 15$, we proved that $\frac{3}{2}n-\frac{5}{2} \leq rsat(n,C_{5}) \leq
2n-6$. For $r \geq 6$ and $n \geq r+3$, we showed that $ \frac{6}{5}n \leq
rsat(n,C_r) \leq 2n+O(r^2)$. Moreover, we establish better lower bound on
$C_r$-rainbow saturated graph $G$ while $G$ is rainbow.",['math.CO'],2502.20639," Federated Learning (FL) facilitates collaborative training of a shared global
model without exposing clients' private data. In practical FL systems, clients
(e.g., edge servers, smartphones, and wearables) typically have disparate
system resources. Conventional FL, however, adopts a one-size-fits-all
solution, where a homogeneous large global model is transmitted to and trained
on each client, resulting in an overwhelming workload for less capable clients
and starvation for other clients. To address this issue, we propose FedConv, a
client-friendly FL framework, which minimizes the computation and memory burden
on resource-constrained clients by providing heterogeneous customized
sub-models. FedConv features a novel learning-on-model paradigm that learns the
parameters of the heterogeneous sub-models via convolutional compression.
Unlike traditional compression methods, the compressed models in FedConv can be
directly trained on clients without decompression. To aggregate the
heterogeneous sub-models, we propose transposed convolutional dilation to
convert them back to large models with a unified size while retaining
personalized information from clients. The compression and dilation processes,
transparent to clients, are optimized on the server leveraging a small public
dataset. Extensive experiments on six datasets demonstrate that FedConv
outperforms state-of-the-art FL systems in terms of model accuracy (by more
than 35% on average), computation and communication overhead (with 33% and 25%
reduction, respectively).","['cs.LG', 'cs.AI']",False,,,,The Rainbow Saturation Number of Cycles,"FedConv: A Learning-on-Model Paradigm for Heterogeneous Federated
  Clients"
neg-d2-751,2025-01-26,,2501.15687," In this work we study the problem of user association and resource allocation
to maximize the proportional fairness of a wireless network with limited
backhaul capacity. The optimal solution of this problem requires solving a
mixed integer non-linear programming problem which generally cannot be solved
in real time. We propose instead to model the problem as a potential game,
which decreases dramatically the computational complexity and obtains a user
association and resource allocation close to the optimal solution.
Additionally, the use of a game-theoretic approach allows an efficient
distribution of the computational burden among the computational resources of
the network.",['cs.NI'],2502.16941," Instance-level change detection in 3D scenes presents significant challenges,
particularly in uncontrolled environments lacking labeled image pairs,
consistent camera poses, or uniform lighting conditions. This paper addresses
these challenges by introducing a novel approach for detecting changes in
real-world scenarios. Our method leverages 4D Gaussians to embed multiple
images into Gaussian distributions, enabling the rendering of two coherent
image sequences. We segment each image and assign unique identifiers to
instances, facilitating efficient change detection through ID comparison.
Additionally, we utilize change maps and classification encodings to categorize
4D Gaussians as changed or unchanged, allowing for the rendering of
comprehensive change maps from any viewpoint. Extensive experiments across
various instance-level change detection datasets demonstrate that our method
significantly outperforms state-of-the-art approaches like C-NERF and CYWS-3D,
especially in scenarios with substantial lighting variations. Our approach
offers improved detection accuracy, robustness to lighting changes, and
efficient processing times, advancing the field of 3D change detection.",['cs.CV'],False,,,,"Joint Cell Selection and Resource Allocation Games with Backhaul
  Constraints",Gaussian Difference: Find Any Change Instance in 3D Scenes
neg-d2-752,2025-01-16,,2501.0963," The Chern-Simons gravitational term during inflation is usually coupled to
the inflaton field. The resulting theory suffers from ghost-field formation in
the tensor sector, which limits the observational effects of P-violation on
cosmological correlators. In this work, we consider the Chern-Simons term
coupled to an isocurvature component in a multi-field model of inflation. Since
the resulting theory does not affect the quadratic action of tensor
perturbations, ghost fields do not appear. This operator provides (P-violating)
interactions between the isocurvature perturbation and the curvature and tensor
perturbations. We show that combining these couplings with interactions between
the curvature and isocurvature components coming from a turning trajectory, the
resulting $\langle sst \rangle_{PV}$ non-Gaussianities can reach $f^{sst,
PV}_{\rm NL}=B_{PV}^{\zeta\zeta h}(k,k,k)/P^2_{\zeta}(k)\sim \mathcal O(1)$
within the parameter space of the theory. Our result motivates the systematic
study of the Chern-Simons gravitational term coupled to isocurvature fields in
multi-field models of inflation with couplings between the curvature and
isocurvature fields or other mechanisms that transfer effects on the
isocurvature field into the curvature field.","['astro-ph.CO', 'hep-th']",2502.14344," Binary Spiking Neural Networks (BSNNs) inherit the eventdriven paradigm of
SNNs, while also adopting the reduced storage burden of binarization
techniques. These distinct advantages grant BSNNs lightweight and
energy-efficient characteristics, rendering them ideal for deployment on
resource-constrained edge devices. However, due to the binary synaptic weights
and non-differentiable spike function, effectively training BSNNs remains an
open question. In this paper, we conduct an in-depth analysis of the challenge
for BSNN learning, namely the frequent weight sign flipping problem. To
mitigate this issue, we propose an Adaptive Gradient Modulation Mechanism
(AGMM), which is designed to reduce the frequency of weight sign flipping by
adaptively adjusting the gradients during the learning process. The proposed
AGMM can enable BSNNs to achieve faster convergence speed and higher accuracy,
effectively narrowing the gap between BSNNs and their full-precision
equivalents. We validate AGMM on both static and neuromorphic datasets, and
results indicate that it achieves state-of-the-art results among BSNNs. This
work substantially reduces storage demands and enhances SNNs' inherent energy
efficiency, making them highly feasible for resource-constrained environments.",['cs.CV'],False,,,,Chern-Simons gravitational term coupled to an isocurvature field,"Towards Accurate Binary Spiking Neural Networks: Learning with Adaptive
  Gradient Modulation Mechanism"
neg-d2-753,2025-02-25,,2502.18719," Achieving high subject-independent accuracy in functional near-infrared
spectroscopy (fNIRS)-based brain-computer interfaces (BCIs) remains a
challenge, particularly when minimizing the number of channels. This study
proposes a novel feature extraction scheme and a Pearson correlation-based
channel selection algorithm to enhance classification accuracy while reducing
hardware complexity. Using an open-access fNIRS dataset, our method improved
average accuracy by 28.09% compared to existing approaches, achieving a peak
subject-independent accuracy of 95.98% with only two channels. These results
demonstrate the potential of our optimized feature extraction and channel
selection methods for developing efficient, subject-independent fNIRS-based BCI
systems.","['cs.HC', 'eess.SP']",2502.07037," Self-assembly of amphiphilic molecules is an important phenomenon attracting
a broad range of research. In this work, we study the self-assembly of KTOF4
sphere-rod amphiphilic molecules in mixed water-dioxane solvents. The molecules
are of a T-shaped geometry, comprised of a hydrophilic spherical Keggin-type
cluster attached by a flexible bridge to the center of a hydrophobic rod-like
oligodialkylfluorene (OF), which consists of four OF units. Transmission
electron microscopy (TEM) uncovers self-assembled spherical structures of KTOF4
in dilute solutions. These spheres are filled with smectic-like layers of KTOF4
separated by layers of the solution. There are two types of layer packings: (i)
concentric spheres and (ii) flat layers. The concentric spheres form when the
dioxane volume fraction in the solution is 35-50 vol%. The flat layers are
formed when the dioxane volume fraction is either below (20 and 30 vol%.) or
above (55 and 60 vol%.) the indicated range. The layered structures show no
in-plane orientational order and thus resemble thermotropic smectic A liquid
crystals and their lyotropic analogs. The layered packings reveal edge and
screw dislocations. Evaporation of the solvent produces a bulk birefringent
liquid crystal phase with textures resembling the ones of uniaxial nematic
liquid crystals. These findings demonstrate that sphere-rod molecules produce a
variety of self-assembled structures that are controlled by the solvent
properties.",['cond-mat.soft'],False,,,,"Enhancing Subject-Independent Accuracy in fNIRS-based Brain-Computer
  Interfaces with Optimized Channel Selection","Liquid crystalline structures formed by sphere-rod amphiphilic molecules
  in solvents"
neg-d2-754,2025-01-16,,2501.0958," Stellar-mass and supermassive black holes abound in the Universe, whereas
intermediate-mass black holes (IMBHs) of ~10^2-10^5 solar masses in between are
largely missing observationally, with few cases found only. Here we report the
real-time discovery of a long-duration X-ray transient, EP240222a, accompanied
by an optical flare with prominent H and He emission lines revealed by prompt
follow-up observations. Its observed properties evidence an IMBH located
unambiguously in the halo of a nearby galaxy and flaring by tidally disrupting
a star -- the only confirmed off-nucleus IMBH-tidal disruption event so far.
This work demonstrates the potential of sensitive time-domain X-ray surveys,
complemented by timely multi-wavelength follow-ups, in probing IMBHs, their
environments, demographics, origins and connections to stellar-mass and
supermassive black holes.","['astro-ph.HE', 'astro-ph.GA']",2501.07358," We propose a novel deep clustering method that integrates Variational
Autoencoders (VAEs) into the Expectation-Maximization (EM) framework. Our
approach models the probability distribution of each cluster with a VAE and
alternates between updating model parameters by maximizing the Evidence Lower
Bound (ELBO) of the log-likelihood and refining cluster assignments based on
the learned distributions. This enables effective clustering and generation of
new samples from each cluster. Unlike existing VAE-based methods, our approach
eliminates the need for a Gaussian Mixture Model (GMM) prior or additional
regularization techniques. Experiments on MNIST and FashionMNIST demonstrate
superior clustering performance compared to state-of-the-art methods.","['cs.LG', 'stat.ML']",False,,,,"An Intermediate-mass Black Hole Lurking in A Galactic Halo Caught Alive
  during Outburst",Deep Generative Clustering with VAEs and Expectation-Maximization
neg-d2-755,2025-03-05,,2503.03661," We study existence and uniqueness of spherically symmetric solutions of
S_k(D^2v)+beta xi\cdot\nabla v+\alpha v+\abs{v}^{q-1}v=0 in R^n, where
\alpha,\beta are real parameters, n>2,\, q>k\geq 1 and S_k(D^2v) stands for the
k-Hessian operator of v. Our results are based mainly on the analysis of an
associated dynamical system and energy methods. We derive some properties of
the solutions of the above equation for different ranges of the parameters
\alpha and \beta. In particular, we describe with precision its asymptotic
behavior at infinity. Further, according to the position of q with respect to
the first critical exponent \frac{(n+2)k}{n} and the Tso critical exponent
\frac{(n+2)k}{n-2k} we study the existence of three classes of solutions:
crossing, slow decay or fast decay solutions. In particular, if k>1 all the
fast decay solutions have a compact support in R^n. The results also apply to
construct self-similar solutions of type I to a related nonlinear evolution
equation. These are self-similar functions of the form
u(t,x)=t^{-\alpha}v(xt^{-\beta}) with suitable \alpha and \beta.",['math.AP'],2502.09016," We study the collective modes of an atomic bright soliton realised in a
quasi-one-dimensional Bose-Einstein condensate, using Bogoliubov-de Gennes
theory. In particular we focus on the breathing mode of the soliton, which is
not a single linearized normal mode but a common component of many modes, and
therefore decays within a $t^{-1/2}$ envelope due to dispersion. If the soliton
is held in the center of a harmonic trap, we show that the breathing amplitude
revives periodically, as atoms shed from the vibrating soliton oscillate in the
trap, and return. After each revival the breathing amplitude again decays, and
this cycle repeats every trap half-period. The amplitude envelope of these
breathing revivals shows a curious asymmetry, however, with a gradual increase
in breathing followed by sudden drop in breathing amplitude that becomes more
and more pronounced in later revivals. We explain this asymmetrical revival
pattern by deriving a close analytical approximation to the Bogoliubov-de
Gennes frequency spectrum, and offer this coherent Bogoliubov-de Gennes
phenomenon as a background against which to compare possible quantum many-body
effects, including decoherence over trap-period time scales.","['cond-mat.quant-gas', 'quant-ph']",False,,,,"A k-Hessian equation with a power nonlinearity source and
  self-similarity","Soliton resuscitations: asymmetric revivals of the breathing mode of an
  atomic bright soliton in a harmonic trap"
neg-d2-756,2025-03-23,,2503.18244," We propose a novel knowledge distillation approach, CustomKD, that
effectively leverages large vision foundation models (LVFMs) to enhance the
performance of edge models (e.g., MobileNetV3). Despite recent advancements in
LVFMs, such as DINOv2 and CLIP, their potential in knowledge distillation for
enhancing edge models remains underexplored. While knowledge distillation is a
promising approach for improving the performance of edge models, the
discrepancy in model capacities and heterogeneous architectures between LVFMs
and edge models poses a significant challenge. Our observation indicates that
although utilizing larger backbones (e.g., ViT-S to ViT-L) in teacher models
improves their downstream task performances, the knowledge distillation from
the large teacher models fails to bring as much performance gain for student
models as for teacher models due to the large model discrepancy. Our simple yet
effective CustomKD customizes the well-generalized features inherent in LVFMs
to a given student model in order to reduce model discrepancies. Specifically,
beyond providing well-generalized original knowledge from teachers, CustomKD
aligns the features of teachers to those of students, making it easy for
students to understand and overcome the large model discrepancy overall.
CustomKD significantly improves the performances of edge models in scenarios
with unlabeled data such as unsupervised domain adaptation (e.g., OfficeHome
and DomainNet) and semi-supervised learning (e.g., CIFAR-100 with 400 labeled
samples and ImageNet with 1% labeled samples), achieving the new
state-of-the-art performances.",['cs.CV'],2503.06664," High-quality, error-free datasets are a key ingredient in building reliable,
accurate, and unbiased machine learning (ML) models. However, real world
datasets often suffer from errors due to sensor malfunctions, data entry
mistakes, or improper data integration across multiple sources that can
severely degrade model performance. Detecting and correcting these issues
typically require tailor-made solutions and demand extensive domain expertise.
Consequently, automation is challenging, rendering the process labor-intensive
and tedious. In this study, we investigate whether Large Language Models (LLMs)
can help alleviate the burden of manual data cleaning. We set up an experiment
in which an LLM, paired with Python, is tasked with cleaning the training
dataset to improve the performance of a learning algorithm without having the
ability to modify the training pipeline or perform any feature engineering. We
run this experiment on multiple Kaggle datasets that have been intentionally
corrupted with errors. Our results show that LLMs can identify and correct
erroneous entries, such as illogical values or outlier, by leveraging
contextual information from other features within the same row, as well as
feedback from previous iterations. However, they struggle to detect more
complex errors that require understanding data distribution across multiple
rows, such as trends and biases.","['cs.LG', 'cs.AI']",False,,,,"CustomKD: Customizing Large Vision Foundation for Edge Model Improvement
  via Knowledge Distillation",Exploring LLM Agents for Cleaning Tabular Machine Learning Datasets
neg-d2-757,2025-02-03,,2502.01425," In a fixed-confidence pure exploration problem in stochastic multi-armed
bandits, an algorithm iteratively samples arms and should stop as early as
possible and return the correct answer to a query about the arms distributions.
We are interested in batched methods, which change their sampling behaviour
only a few times, between batches of observations. We give an
instance-dependent lower bound on the number of batches used by any sample
efficient algorithm for any pure exploration task. We then give a general
batched algorithm and prove upper bounds on its expected sample complexity and
batch complexity. We illustrate both lower and upper bounds on best-arm
identification and thresholding bandits.","['cs.LG', 'stat.ML']",2503.14064," The rapid advancement in AI-generated video synthesis has led to a growth
demand for standardized and effective evaluation metrics. Existing metrics lack
a unified framework for systematically categorizing methodologies, limiting a
holistic understanding of the evaluation landscape. Additionally, fragmented
implementations and the absence of standardized interfaces lead to redundant
processing overhead. Furthermore, many prior approaches are constrained by
dataset-specific dependencies, limiting their applicability across diverse
video domains. To address these challenges, we introduce AIGVE-Tool
(AI-Generated Video Evaluation Toolkit), a unified framework that provides a
structured and extensible evaluation pipeline for a comprehensive AI-generated
video evaluation. Organized within a novel five-category taxonomy, AIGVE-Tool
integrates multiple evaluation methodologies while allowing flexible
customization through a modular configuration system. Additionally, we propose
AIGVE-Bench, a large-scale benchmark dataset created with five SOTA video
generation models based on hand-crafted instructions and prompts. This dataset
systematically evaluates various video generation models across nine critical
quality dimensions. Extensive experiments demonstrate the effectiveness of
AIGVE-Tool in providing standardized and reliable evaluation results,
highlighting specific strengths and limitations of current models and
facilitating the advancements of next-generation AI-generated video techniques.",['cs.CV'],False,,,,The Batch Complexity of Bandit Pure Exploration,"AIGVE-Tool: AI-Generated Video Evaluation Toolkit with Multifaceted
  Benchmark"
neg-d2-758,2025-01-08,,2501.04459," Despite widespread adoption of deep learning models to address a variety of
computer vision tasks, planetary science has yet to see extensive utilization
of such tools to address its unique problems. On Titan, the largest moon of
Saturn, tracking seasonal trends and weather patterns of clouds provides
crucial insights into one of the most complex climates in the Solar System, yet
much of the available image data are still analyzed in a conventional way. In
this work, we apply a Mask R-CNN trained via transfer learning to perform
instance segmentation of clouds in Titan images acquired by the Cassini
spacecraft - a previously unexplored approach to a big data problem in
planetary science. We demonstrate that an automated technique can provide
quantitative measures for clouds, such as areas and centroids, that may
otherwise be prohibitively time-intensive to produce by human mapping.
Furthermore, despite Titan specific challenges, our approach yields accuracy
comparable to contemporary cloud identification studies on Earth and other
worlds. We compare the efficiencies of human-driven versus algorithmic
approaches, showing that transfer learning provides speed-ups that may open new
horizons for data investigation for Titan. Moreover, we suggest that such
approaches have broad potential for application to similar problems in
planetary science where they are currently under-utilized. Future planned
missions to the planets and remote sensing initiatives for the Earth promise to
provide a deluge of image data in the coming years that will benefit strongly
from leveraging machine learning approaches to perform the analysis.","['astro-ph.IM', 'astro-ph.EP', 'cs.CV', 'eess.IV']",2501.1332," As Artificial Intelligence (AI) systems become increasingly integrated into
various aspects of daily life, concerns about privacy and ethical
accountability are gaining prominence. This study explores stakeholder
perspectives on privacy in AI systems, focusing on educators, parents, and AI
professionals. Using qualitative analysis of survey responses from 227
participants, the research identifies key privacy risks, including data
breaches, ethical misuse, and excessive data collection, alongside perceived
benefits such as personalized services, enhanced efficiency, and educational
advancements. Stakeholders emphasized the need for transparency,
privacy-by-design, user empowerment, and ethical oversight to address privacy
concerns effectively. The findings provide actionable insights into balancing
the benefits of AI with robust privacy protections, catering to the diverse
needs of stakeholders. Recommendations include implementing selective data use,
fostering transparency, promoting user autonomy, and integrating ethical
principles into AI development. This study contributes to the ongoing discourse
on ethical AI, offering guidance for designing privacy-centric systems that
align with societal values and build trust among users. By addressing privacy
challenges, this research underscores the importance of developing AI
technologies that are not only innovative but also ethically sound and
responsive to the concerns of all stakeholders.","['cs.CY', 'cs.AI']",False,,,,Rapid Automated Mapping of Clouds on Titan With Instance Segmentation,Toward Ethical AI: A Qualitative Analysis of Stakeholder Perspectives
neg-d2-759,2025-03-21,,2503.17085," Artificial intelligence (AI) systems powered by large language models have
become increasingly prevalent in modern society, enabling a wide range of
applications through natural language interaction. As AI agents proliferate in
our daily lives, their generic and uniform expressiveness presents a
significant limitation to their appeal and adoption. Personality expression
represents a key prerequisite for creating more human-like and distinctive AI
systems. We show that AI models can express deterministic and consistent
personalities when instructed using established psychological frameworks, with
varying degrees of accuracy depending on model capabilities. We find that more
advanced models like GPT-4o and o1 demonstrate the highest accuracy in
expressing specified personalities across both Big Five and Myers-Briggs
assessments, and further analysis suggests that personality expression emerges
from a combination of intelligence and reasoning capabilities. Our results
reveal that personality expression operates through holistic reasoning rather
than question-by-question optimization, with response-scale metrics showing
higher variance than test-scale metrics. Furthermore, we find that model
fine-tuning affects communication style independently of personality expression
accuracy. These findings establish a foundation for creating AI agents with
diverse and consistent personalities, which could significantly enhance
human-AI interaction across applications from education to healthcare, while
additionally enabling a broader range of more unique AI agents. The ability to
quantitatively assess and implement personality expression in AI systems opens
new avenues for research into more relatable, trustworthy, and ethically
designed AI.","['cs.LG', 'cs.AI', 'cs.CY', 'cs.HC']",2502.15504," In this paper, we show a more concise and high level proof than the original
one, derived by researcher Bart Jacobs, for the following theorem: in the
context of Bayesian update rules for learning or updating internal states that
produce predictions, the relative entropy between the observations and the
predictions is reduced when applying Jeffrey's update rule to update the
internal state.","['stat.ML', 'cs.CR']",False,,,,"Deterministic AI Agent Personality Expression through Standard
  Psychological Diagnostics",Jeffrey's update rule as a minimizer of Kullback-Leibler divergence
neg-d2-760,2025-01-02,,2501.0155," The dimensionality of vortical structures has recently been extended beyond
two dimensions, providing higher-order topological characteristics and
robustness for high-capacity information processing and turbulence control. The
generation of high-dimensional vortical structures has mostly been demonstrated
in classical systems through the complex interference of fluidic, acoustic, or
electromagnetic waves. However, natural materials rarely support three- or
higher-dimensional vortical structures and their physical interactions. Here,
we present a high-dimensional gradient thickness optical cavity (GTOC) in which
the optical coupling of planar metal-dielectric multilayers implements
topological interactions across multiple dimensions. Topological interactions
in high-dimensional GTOC construct non-trivial topological phases, which induce
high-dimensional vortical structures in generalized parameter space in three,
four dimensions, and beyond. These emergent high-dimensional vortical
structures are observed under electro-optic tomography as optical vortex
dynamics in two-dimensional real-space, employing the optical thicknesses of
the dielectric layers as synthetic dimensions. We experimentally demonstrate
emergent vortical structures, optical vortex lines and vortex rings, in a
three-dimensional generalized parameter space and their topological
transitions. Furthermore, we explore four-dimensional vortical structures,
termed optical vortex sheets, which provide the programmability of real-space
optical vortex dynamics. Our findings hold significant promise for emulating
high-dimensional physics and developing active topological photonic devices.","['physics.optics', 'cond-mat.mes-hall']",2503.09358," Standardization of clinical reports is crucial for improving the quality of
healthcare and facilitating data integration. The lack of unified standards,
including format, terminology, and style, is a great challenge in clinical
fundus diagnostic reports, which increases the difficulty for large language
models (LLMs) to understand the data. To address this, we construct a bilingual
standard terminology, containing fundus clinical terms and commonly used
descriptions in clinical diagnosis. Then, we establish two models,
RetSTA-7B-Zero and RetSTA-7B. RetSTA-7B-Zero, fine-tuned on an augmented
dataset simulating clinical scenarios, demonstrates powerful standardization
behaviors. However, it encounters a challenge of limitation to cover a wider
range of diseases. To further enhance standardization performance, we build
RetSTA-7B, which integrates a substantial amount of standardized data generated
by RetSTA-7B-Zero along with corresponding English data, covering diverse
complex clinical scenarios and achieving report-level standardization for the
first time. Experimental results demonstrate that RetSTA-7B outperforms other
compared LLMs in bilingual standardization task, which validates its superior
performance and generalizability. The checkpoints are available at
https://github.com/AB-Story/RetSTA-7B.","['cs.CL', 'cs.AI']",False,,,,Dynamic realization of emergent high-dimensional optical vortices,"RetSTA: An LLM-Based Approach for Standardizing Clinical Fundus Image
  Reports"
neg-d2-761,2025-03-15,,2503.1204," Recently, Griffin, Ono, and Tsai examined the distribution of the number of
$t$-hooks in partitions of $n$, which was later followed by the work of Craig,
Ono, and Singh on the distribution of the number of $t$-hooks in self-conjugate
partitions of $n$. Motivated by these studies, in this paper, we further
investigate the number of $t$-hooks in some subsets of partitions. More
specifically, we obtain the generating functions for the number of $t$-hooks in
doubled distinct partitions and the number of $t$-shifted hooks in strict
partitions. Based on these generating functions, we prove that the number of
$t$-hooks in doubled distinct partitions and the number of $t$-shifted hooks in
strict partitions are both asymptotically normally distributed.","['math.CO', 'math.NT']",2502.11911," In recent studies, analogs of the electronic Quantum Spin-Hall Effect have
been explored within photonic crystals that incorporate spatial symmetries,
especially those with $ C_{6v} $ symmetry, where $ \mathbb{Z}_2 $ topological
invariants are enforced by crystalline symmetry. These photonic crystals
possess bulk states with well-defined pseudospins and exhibit helical edge
states, closely resembling their electronic counterparts. However, achieving
$\mathbb{Z}_2$ topological protection in a square lattice photonic crystal
remains great theoretical and experimental challange. In this work, we propose
a single material photonic crystal structure based on a $ C_4 $ lattice that
supports partially $ \mathbb{Z}_2 $-protected edge modes. We show that this
structure can host photonic band-gap that hosts $ \mathbb{Z}_2 $-like modes,
enabling perfect transmission in waveguide applications. Furthermore, we
investigate the robustness of these modes against structural defects and
directional turns, highlighting the distinctions between full $ \mathbb{Z}_2 $
topological protection and partial topological protection. Finally, we analyze
the impact of the number of elementary cells surrounding the interface on the
formation and stability of these protected modes.",['physics.optics'],False,,,,On the distribution of $t$-hooks of doubled distinct partitions,Partial Topological Protection in C4 Lattices for Optical Communications
neg-d2-762,2025-03-17,,2503.12987," The behaviour of the moment-sums-of-squares (moment-SOS) hierarchy for
polynomial optimal control problems on compact sets has been explored to a
large extent. Our contribution focuses on the case of non-compact control sets.
We describe a new approach to optimal control problems with unbounded controls,
using compactification by partial homogenization, leading to an equivalent
infinite dimensional linear program with compactly supported measures. Our
results are closely related to the results of a previous approach using
DiPerna-Majda measures. However, our work provides a sound proof of the absence
of relaxation gap, which was conjectured in the previous work, and thereby
enables the design of a moment-sum-of-squares relaxation with guaranteed
convergence.",['math.OC'],2502.18261," Traditionally, the impact of minimum wages on employment has been studied,
and it is generally believed to have a negative effect. Yet, some recent
studies have shown that the impact of minimum wages on employment can sometimes
be positive. In addition, certain recent proposals set a higher minimum wage
than the wage earned by some high-productivity workers. However, the impact of
minimum wages on employment has been primarily studied on low-skilled workers,
whereas there is limited research on high-skilled workers. To address this gap
and examine the effects of minimum wages on high-productivity workers'
employment, I construct a macroeconomic model incorporating productivity
fluctuations, incomplete markets, directed search, and on-the-job search and
compare the steady-state distributions between the baseline model and the model
with a minimum wage. As a result, binding minimum wages increase the
unemployment rate of both low and high-productivity workers.","['econ.GN', 'q-fin.EC']",False,,,,"Solving unbounded optimal control problems with the moment-SOS hierarchy
  *","The effect of minimum wages on employment in the presence of
  productivity fluctuations"
neg-d2-763,2025-02-19,,2502.13481," Tagging systems play an essential role in various information retrieval
applications such as search engines and recommender systems. Recently, Large
Language Models (LLMs) have been applied in tagging systems due to their
extensive world knowledge, semantic understanding, and reasoning capabilities.
Despite achieving remarkable performance, existing methods still have
limitations, including difficulties in retrieving relevant candidate tags
comprehensively, challenges in adapting to emerging domain-specific knowledge,
and the lack of reliable tag confidence quantification. To address these three
limitations above, we propose an automatic tagging system LLM4Tag. First, a
graph-based tag recall module is designed to effectively and comprehensively
construct a small-scale highly relevant candidate tag set. Subsequently, a
knowledge-enhanced tag generation module is employed to generate accurate tags
with long-term and short-term knowledge injection. Finally, a tag confidence
calibration module is introduced to generate reliable tag confidence scores.
Extensive experiments over three large-scale industrial datasets show that
LLM4Tag significantly outperforms the state-of-the-art baselines and LLM4Tag
has been deployed online for content tagging to serve hundreds of millions of
users.",['cs.IR'],2502.18852," Full Waveform Inversion (FWI) reconstructs high-resolution subsurface models
via multi-variate optimization but faces challenges with solver selection and
data availability. Deep Learning (DL) offers a promising alternative, bridging
data-driven and physics-based methods. While FWI in DL has been explored in the
time domain, the pseudo-spectral approach remains underutilized, despite its
success in classical FWI.
  This thesis integrates pseudo-spectral FWI into DL, formulating both
data-driven and theory-guided approaches using Deep Neural Networks (DNNs) and
Recurrent Neural Networks (RNNs). These methods were theoretically derived,
tested on synthetic and Marmousi datasets, and compared with deterministic and
time-domain approaches.
  Results show that data-driven pseudo-spectral DNNs outperform classical FWI
in deeper and over-thrust regions due to their global approximation capability.
Theory-guided RNNs yield greater accuracy, with lower error and better fault
identification. While DNNs excel in velocity contrast recovery, RNNs provide
superior edge definition and stability in shallow and deep sections.
  Beyond enhancing FWI performance, this research identifies broader
applications of DL-based inversion and outlines future directions for these
frameworks.","['physics.geo-ph', 'cs.LG']",False,,,,"LLM4Tag: Automatic Tagging System for Information Retrieval via Large
  Language Models","Data-Driven and Theory-Guided Pseudo-Spectral Seismic Imaging Using Deep
  Neural Network Architectures"
neg-d2-764,2025-02-28,,2502.21288," Delta lenses are functors equipped with a functorial choice of lifts,
generalising the notion of split opfibration. In this paper, we introduce a
Grothendieck construction (or category of elements) for delta lenses, thus
demonstrating a correspondence between delta lenses and certain lax double
functors into the double category of sets, functions, and split multivalued
functions. We show that the double category of split multivalued functions
admits a universal property as a certain kind of limit, and inherits many nice
properties from the double category of spans. Applications of this construction
to the theory of delta lenses are explored in detail.",['math.CT'],2503.10174," This paper presents a novel sensitivity-based distributed programming (SBDP)
approach for non-convex, large-scale nonlinear programs (NLP). The algorithm
relies on first-order sensitivities to cooperatively solve the central NLP in a
distributed manner with only neighbor-to-neighbor communication and
parallelizable local computations. The scheme is based on primal decomposition
and offers minimal algorithmic complexity. We derive sufficient local
convergence conditions for non-convex problems. Furthermore, we consider the
SBDP method in a distributed optimal control context and derive favorable
convergence properties in this setting. We illustrate these theoretical
findings and the performance of the proposed algorithm with simulations of
various distributed optimization and control problems.",['math.OC'],False,,,,The Grothendieck construction for delta lenses,Sensitivity-Based Distributed Programming for Non-Convex Optimization
neg-d2-765,2025-02-11,,2502.08671," Information regarding images should be visually understood by anyone,
including those with color deficiency. However, such information is not
recognizable if the color that seems to be distorted to the color deficiencies
meets an adjacent object. The aim of this paper is to propose a color universal
design network, called CUD-Net, that generates images that are visually
understandable by individuals with color deficiency. CUD-Net is a convolutional
deep neural network that can preserve color and distinguish colors for input
images by regressing the node point of a piecewise linear function and using a
specific filter for each image. To generate CUD images for color deficiencies,
we follow a four-step process. First, we refine the CUD dataset based on
specific criteria by color experts. Second, we expand the input image
information through pre-processing that is specialized for color deficiency
vision. Third, we employ a multi-modality fusion architecture to combine
features and process the expanded images. Finally, we propose a conjugate loss
function based on the composition of the predicted image through the model to
address one-to-many problems that arise from the dataset. Our approach is able
to produce high-quality CUD images that maintain color and contrast stability.
The code for CUD-Net is available on the GitHub repository","['eess.IV', 'cs.CV']",2501.01186," A summary of recent contributions in the field of rough partial differential
equations is given. For that purpose we rely on the formalism of ``unbounded
rough driver''. We present applications to concrete models including
Landau-Lifshitz-Gilbert, Navier-Stokes and Euler equations.","['math.AP', 'math.PR']",False,,,,Color Universal Design Neural Network for the Color Vision Deficiencies,"Unbounded rough drivers, rough PDEs and applications"
neg-d2-766,2025-01-14,,2501.08234," This paper addresses a critical challenge in the high-speed passenger railway
industry: designing effective dynamic pricing strategies in the context of
competing and cooperating operators. To address this, a multi-agent
reinforcement learning (MARL) framework based on a non-zero-sum Markov game is
proposed, incorporating random utility models to capture passenger decision
making. Unlike prior studies in areas such as energy, airlines, and mobile
networks, dynamic pricing for railway systems using deep reinforcement learning
has received limited attention. A key contribution of this paper is a
parametrisable and versatile reinforcement learning simulator designed to model
a variety of railway network configurations and demand patterns while enabling
realistic, microscopic modelling of user behaviour, called RailPricing-RL. This
environment supports the proposed MARL framework, which models heterogeneous
agents competing to maximise individual profits while fostering cooperative
behaviour to synchronise connecting services. Experimental results validate the
framework, demonstrating how user preferences affect MARL performance and how
pricing policies influence passenger choices, utility, and overall system
dynamics. This study provides a foundation for advancing dynamic pricing
strategies in railway systems, aligning profitability with system-wide
efficiency, and supporting future research on optimising pricing policies.","['cs.LG', 'cs.AI', 'cs.MA']",2503.05469," We identify the size of the largest connected component in a subcritical
inhomogeneous random graph with a kernel of preferential attachment type. The
component is polynomial in the graph size with an explicitly given exponent,
which is strictly larger than the exponent for the largest degree in the graph.
This is in stark contrast to the behaviour of inhomogeneous random graphs with
a kernel of rank one. Our proof uses local approximation by branching random
walks going well beyond the weak local limit and novel results on subcritical
killed branching random walks.","['math.PR', 'math.CO']",False,,,,"Dynamic Pricing in High-Speed Railways Using Multi-Agent Reinforcement
  Learning","The largest subcritical component in inhomogeneous random graphs of
  preferential attachment type"
neg-d2-767,2025-02-11,,2502.07856," In applications of diffusion models, controllable generation is of practical
significance, but is also challenging. Current methods for controllable
generation primarily focus on modifying the score function of diffusion models,
while Mean Reverting (MR) Diffusion directly modifies the structure of the
stochastic differential equation (SDE), making the incorporation of image
conditions simpler and more natural. However, current training-free fast
samplers are not directly applicable to MR Diffusion. And thus MR Diffusion
requires hundreds of NFEs (number of function evaluations) to obtain
high-quality samples. In this paper, we propose a new algorithm named MaRS (MR
Sampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time
SDE and the probability flow ordinary differential equation (PF-ODE) associated
with MR Diffusion, and derive semi-analytical solutions. The solutions consist
of an analytical function and an integral parameterized by a neural network.
Based on this solution, we can generate high-quality samples in fewer steps.
Our approach does not require training and supports all mainstream
parameterizations, including noise prediction, data prediction and velocity
prediction. Extensive experiments demonstrate that MR Sampler maintains high
sampling quality with a speedup of 10 to 20 times across ten different image
restoration tasks. Our algorithm accelerates the sampling procedure of MR
Diffusion, making it more practical in controllable generation.","['cs.CV', 'cs.AI', 'cs.LG']",2501.0854," The task of building semantics for structured data such as CSV, JSON, and XML
files is highly relevant in the knowledge representation field. Even though we
have a vast of structured data on the internet, mapping them to domain
ontologies to build semantics for them is still very challenging as it requires
the construction model to understand and learn graph-structured knowledge.
Otherwise, the task will require human beings' effort and cost. In this paper,
we proposed a novel automatic semantic modeling framework: Knowledge Prompt
Chaining. It can serialize the graph-structured knowledge and inject it into
the LLMs properly in a Prompt Chaining architecture. Through this knowledge
injection and prompting chaining, the model in our framework can learn the
structure information and latent space of the graph and generate the semantic
labels and semantic graphs following the chains' insturction naturally. Based
on experimental results, our method achieves better performance than existing
leading techniques, despite using reduced structured input data.","['cs.CL', 'cs.AI', 'cs.DB']",False,,,,"MaRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE
  Solvers",Knowledge prompt chaining for semantic modeling
neg-d2-768,2025-03-04,,2503.02414," 3D models are widely used in various industries, and mesh data has become an
indispensable part of 3D modeling because of its unique advantages. Mesh data
can provide an intuitive and practical expression of rich 3D information.
However, its disordered, irregular data structure and complex surface
information make it challenging to apply with deep learning models directly.
Traditional mesh data processing methods often rely on mesh models with many
limitations, such as manifold, which restrict their application scopes in
reality and do not fully utilize the advantages of mesh models. This paper
proposes a novel end-to-end framework for addressing the challenges associated
with deep learning in mesh models centered around graph neural networks (GNN)
and is titled InfoGNN. InfoGNN treats the mesh model as a graph, which enables
it to handle irregular mesh data efficiently. Moreover, we propose InfoConv and
InfoMP modules, which utilize the position information of the points and fully
use the static information such as face normals, dihedral angles, and dynamic
global feature information to fully use all kinds of data. In addition, InfoGNN
is an end-to-end framework, and we simplify the network design to make it more
efficient, paving the way for efficient deep learning of complex 3D models. We
conducted experiments on several publicly available datasets, and the results
show that InfoGNN achieves excellent performance in mesh classification and
segmentation tasks.","['cs.CV', 'cs.LG']",2501.12663," This paper investigates the trajectories of light beams in a Kerr metric,
which describes the gravitational field in the neighborhood of a rotating black
hole. After reduction by cyclic coordinates, this problem reduces to analysis
of a Hamiltonian system with two degrees of freedom. A bifurcation diagram is
constructed and a classification is made of the types of trajectories of the
system according to the values of first integrals. Relations describing the
boundary of the shadow of the black hole are obtained for a stationary observer
who rotates with an arbitrary angular velocity about the axis of rotation of
the black hole.","['math.DS', 'gr-qc']",False,,,,InfoGNN: End-to-end deep learning on mesh via graph neural networks,"Trajectories of light beams in a Kerr metric: the influence of the
  rotation of an observer on the shadow of a black hole"
neg-d2-769,2025-02-05,,2502.0683," Efficient and reliable probabilistic prediction of intraday electricity
prices is essential to manage market uncertainties and support robust trading
strategies. However, current methods often suffer from parameter
inefficiencies, as they fail to fully exploit the potential of modeling
interdependencies between bids and offers in the orderbook, requiring a large
number of parameters for representation learning. Furthermore, these methods
face the quantile crossing issue, where upper quantiles fall below the lower
quantiles, resulting in unreliable probabilistic predictions. To address these
two challenges, we propose an encoding method called OrderFusion and design a
hierarchical multi-quantile head. The OrderFusion encodes the orderbook into a
2.5D representation, which is processed by a tailored jump cross-attention
backbone to capture the interdependencies of bids and offers, enabling
parameter-efficient learning. The head sets the median quantile as an anchor
and predicts multiple quantiles hierarchically, ensuring reliability by
enforcing monotonicity between quantiles through non-negative functions.
Extensive experiments and ablation studies are conducted on four price indices:
60-min ID3, 60-min ID1, 15-min ID3, and 15-min ID1 using the German orderbook
over three years to ensure a fair evaluation. The results confirm that our
design choices improve overall performance, offering a parameter-efficient and
reliable solution for probabilistic intraday price prediction.","['q-fin.CP', 'cs.AI', 'cs.LG']",2503.15196," Ultra-short Period exoplanets (USPs) like 55 Cnc e, hosting dayside magma
oceans, present unique opportunities to study surface-atmosphere interactions.
The composition of a vaporised mineral atmosphere enveloping the dayside is
dictated by that of the surface magma ocean, which in turn is sensitive to its
oxygen fugacity ($f$O$_2$). Observability estimations and characterisation of
the atmospheric emission of 55 Cnc e have mostly remained limited to low
spectral resolution space-based studies. Here, we aim to examine ground-based
high-resolution observabilities of a diverse set of mineral atmospheres
produced across a grid of mantle $f$O$_2$s varying over 12 orders of magnitude.
We assume a Bulk Silicate Earth mantle composition and a substellar dayside
temperature of T = 2500K in the near infrared wavelength (NIR) region. This
spectral range is often featureless for this class of atmospheres at
low-resolution. Coupling our newly developed simulator for synthesising
realistic observations from high-resolution ground-based spectrographs (Ratri)
to a pre-developed high-resolution cross-correlation spectroscopy (HRCCS)
analysis pipeline (Upamana), we find that this array of mineral atmospheres
would all be detectable with 11 hours of observing time of the dayside of 55
Cnc e with CARMENES and each individual scenario can be correctly
differentiated within 1$\sigma$. Our analysis is readily able to distinguish
between a planet with an Earth-like redox state (with $f$O$_2$ $\sim$3.5
log$_{10}$ units above the iron-w\""ustite, IW buffer) from a Mercury-like
planet ($f$O$_2$ $\sim$5 log$_{10}$ units below IW). We thus conclude that the
HRCCS technique holds promise for cataloguing the diversity of redox states
among the rocky exoplanetary population.",['astro-ph.EP'],False,,,,"OrderFusion: Encoding Orderbook for Probabilistic Intraday Price
  Prediction","Detectability of oxygen fugacity regimes in the magma ocean world 55
  Cancri e at high spectral resolution"
neg-d2-770,2025-01-20,,2501.11864," Thorough simulation testing is crucial for validating the correct behavior of
small Uncrewed Aerial Systems (sUAS) across multiple scenarios, including
adverse weather conditions (such as wind, and fog), diverse settings (hilly
terrain, or urban areas), and varying mission profiles (surveillance,
tracking). While various sUAS simulation tools exist to support developers, the
entire process of creating, executing, and analyzing simulation tests remains a
largely manual and cumbersome task. Developers must identify test scenarios,
set up the simulation environment, integrate the System under Test (SuT) with
simulation tools, formulate mission plans, and collect and analyze results.
These labor-intensive tasks limit the ability of developers to conduct
exhaustive testing across a wide range of scenarios. To alleviate this problem,
in this paper, we propose AutoSimTest, a Large Language Model (LLM)-driven
framework, where multiple LLM agents collaborate to support the sUAS simulation
testing process. This includes: (1) creating test scenarios that subject the
SuT to unique environmental contexts; (2) preparing the simulation environment
as per the test scenario; (3) generating diverse sUAS missions for the SuT to
execute; and (4) analyzing simulation results and providing an interactive
analytics interface. Further, the design of the framework is flexible for
creating and testing scenarios for a variety of sUAS use cases, simulation
tools, and SuT input requirements. We evaluated our approach by (a) conducting
simulation testing of PX4 and ArduPilot flight-controller-based SuTs, (b)
analyzing the performance of each agent, and (c) gathering feedback from sUAS
developers. Our findings indicate that AutoSimTest significantly improves the
efficiency and scope of the sUAS testing process, allowing for more
comprehensive and varied scenario evaluations while reducing the manual effort.",['cs.SE'],2501.01847," We derive the full system of canonical differential equations for all planar
two-loop massless six-particle master integrals, and determine analytically the
boundary conditions. This fully specifies the solutions, which may be written
as Chen iterated integrals. We argue that this is sufficient information for
evaluating any scattering amplitude in four dimensions up to the finite part.
We support this claim by reducing, for the most complicated integral
topologies, integrals with typical Yang-Mills numerators. We use the analytic
solutions to the differential equations, together with dihedral symmetry, to
provide the full solution space relevant for two-loop six-particle
computations. This includes the relevant function alphabet, as well as the
independent set of iterated integrals up to weight four. We also provide the
answer for all master integrals in terms of iterated integrals that can be
readily evaluated numerically. As a proof of concept, we provide a numerical
implementation that evaluates the integrals in part of the Euclidean region,
and validate this against numerical evaluation of the Feynman integrals. Our
result removes the bottleneck of Feynman integral evaluation, paving the way to
future analytic evaluations of six-particle scattering amplitudes.","['hep-ph', 'hep-th']",False,,,,"LLM-Agents Driven Automated Simulation Testing and Analysis of small
  Uncrewed Aerial Systems","Complete function space for planar two-loop six-particle scattering
  amplitudes"
neg-d2-771,2025-02-25,,2502.18422," Apart from relating interesting quantum mechanical systems to equations
describing a parabolic discrete minimal surface, the quantization of a cubic
minimal surface in $\mathbb{R}^4$ is considered.","['math-ph', 'math.MP']",2503.04199," RGB-Thermal fusion is a potential solution for various weather and light
conditions in challenging scenarios. However, plenty of studies focus on
designing complex modules to fuse different modalities. With the widespread
application of large language models (LLMs), valuable information can be more
effectively extracted from natural language. Therefore, we aim to leverage the
advantages of large language models to design a structurally simple and highly
adaptable multimodal fusion model architecture. We proposed MultimodAl
Segmentation with TExt PRompts (MASTER) architecture, which integrates LLM into
the fusion of RGB-Thermal multimodal data and allows complex query text to
participate in the fusion process. Our model utilizes a dual-path structure to
extract information from different modalities of images. Additionally, we
employ LLM as the core module for multimodal fusion, enabling the model to
generate learnable codebook tokens from RGB, thermal images, and textual
information. A lightweight image decoder is used to obtain semantic
segmentation results. The proposed MASTER performs exceptionally well in
benchmark tests across various automated driving scenarios, yielding promising
results.","['cs.CV', 'cs.AI']",False,,,,Quantum States from Minimal Surfaces,MASTER: Multimodal Segmentation with Text Prompts
neg-d2-772,2025-03-10,,2503.08007," Developing versatile quadruped robots that can smoothly perform various
actions and tasks in real-world environments remains a significant challenge.
This paper introduces a novel vision-language-action (VLA) model, mixture of
robotic experts (MoRE), for quadruped robots that aim to introduce
reinforcement learning (RL) for fine-tuning large-scale VLA models with a large
amount of mixed-quality data. MoRE integrates multiple low-rank adaptation
modules as distinct experts within a dense multi-modal large language model
(MLLM), forming a sparse-activated mixture-of-experts model. This design
enables the model to effectively adapt to a wide array of downstream tasks.
Moreover, we employ a reinforcement learning-based training objective to train
our model as a Q-function after deeply exploring the structural properties of
our tasks. Effective learning from automatically collected mixed-quality data
enhances data efficiency and model performance. Extensive experiments
demonstrate that MoRE outperforms all baselines across six different skills and
exhibits superior generalization capabilities in out-of-distribution scenarios.
We further validate our method in real-world scenarios, confirming the
practicality of our approach and laying a solid foundation for future research
on multi-task learning in quadruped robots.","['cs.RO', 'cs.AI']",2502.1776," This paper studies probabilistic dual frames and associated dual frame
potentials from the optimal mass transport perspective. The main contribution
in this work shows that given a probabilistic frame, its dual frame potential
is minimized if and only if the probabilistic frame is tight and the
probabilistic dual frame is the canonical dual. In particular, the tightness
condition can be dropped if the probabilistic dual frame potential is minimized
only among probabilistic dual frames of pushforward type.",['math.FA'],False,,,,"MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped
  Vision-Language-Action Models",Probabilistic Dual Frames and Minimization of Dual Frame Potentials
neg-d2-773,2025-02-15,,2502.10722," Modern processors widely equip the Performance Monitoring Unit (PMU) to
collect various architecture and microarchitecture events. Software developers
often utilize the PMU to enhance program's performance, but the potential side
effects that arise from its activation are often disregarded. In this paper, we
find that the PMU can be employed to retrieve instruction operands. Based on
this discovery, we introduce PMU-Data, a novel category of side-channel attacks
aimed at leaking secret by identifying instruction operands with PMU.
  To achieve the PMU-Data attack, we develop five gadgets to encode the
confidential data into distinct data-related traces while maintaining the
control-flow unchanged. We then measure all documented PMU events on three
physical machines with different processors while those gadgets are performing.
We successfully identify two types of vulnerable gadgets caused by DIV and MOV
instructions. Additionally, we discover 40 vulnerable PMU events that can be
used to carry out the PMU-Data attack. We through real experiments to
demonstrate the perniciousness of the PMU-Data attack by implementing three
attack goals: (1) leaking the kernel data illegally combined with the transient
execution vulnerabilities including Meltdown, Spectre, and Zombieload; (2)
building a covert-channel to secretly transfer data; (3) extracting the secret
data protected by the Trusted Execution Environment (TEE) combined with the
Zombieload vulnerability.",['cs.CR'],2503.09389," We study canonical-equilibrium properties of Random Field $O(n)$ Models
involving classical continuous vector spins of $n$ components with mean-field
interactions and subject to disordered fields acting on individual spins. To
this end, we employ two complementary approaches: the mean-field approximation,
valid for any disorder distribution, and the replica trick, applicable when the
disordered fields are sampled from a Gaussian distribution. On the basis of an
exact analysis, we demonstrate that when replica symmetry holds, both the
approaches yield identical expression for the free energy per spin of the
system. As consequences, we study the case of $n=2$ ($XY$ spins) and that of
$n=3$ (Heisenberg spins) for two representative choices of the disorder
distribution, namely, a Gaussian and a symmetric bimodal distribution. For both
$n=2$ and $n=3$, we demonstrate that while the magnetization exhibits a
continuous phase transition as a function of temperature for the Gaussian case,
the transition could be either continuous or first-order with an emergent
tricriticality when the disorder distribution is bimodal. We also discuss in
the context of our models the issue of self-averaging of extensive variables
near the critical point of a continuous phase transition.","['cond-mat.stat-mech', 'cond-mat.dis-nn']",False,,,,PMU-Data: Data Traces Could be Distinguished,"Canonical equilibrium of mean-field $O(n)$~models in presence of random
  fields"
neg-d2-774,2025-03-17,,2503.13664," Defects and interfaces are essential to understand the properties of matter.
However, studying their dynamics in the quantum regime remains a challenge in
particular concerning the regime of two spatial dimensions. Recently, it has
been shown that a quantum counterpart of the hard-disk problem on a lattice
yields defects and interfaces, which are stable just due to quantum effects
while they delocalize and dissolve classically. Here, we study in more detail
the properties of defects and interfaces in this quantum hard-disk problem with
a particular emphasis on the stability of these quantum effects upon including
perturbations. Specifically, we introduce short-range soft-core interactions
between the hard disks. From both analytical arguments and numerical
simulations we find that large classes of defects and interfaces remain stable
even under such perturbations suggesting that the quantum nature of the
dynamics exhibits a large range of robustness. Our findings demonstrate the
stability and non-classical behavior of quantum interface dynamics, offering
insights into the dynamics of two-dimensional quantum matter and establishing
the quantum hard-disk model as a platform for studying unconventional
constrained quantum dynamics.","['quant-ph', 'cond-mat.stat-mech']",2503.12987," The behaviour of the moment-sums-of-squares (moment-SOS) hierarchy for
polynomial optimal control problems on compact sets has been explored to a
large extent. Our contribution focuses on the case of non-compact control sets.
We describe a new approach to optimal control problems with unbounded controls,
using compactification by partial homogenization, leading to an equivalent
infinite dimensional linear program with compactly supported measures. Our
results are closely related to the results of a previous approach using
DiPerna-Majda measures. However, our work provides a sound proof of the absence
of relaxation gap, which was conjectured in the previous work, and thereby
enables the design of a moment-sum-of-squares relaxation with guaranteed
convergence.",['math.OC'],False,,,,Dynamics of defects and interfaces for interacting quantum hard disks,"Solving unbounded optimal control problems with the moment-SOS hierarchy
  *"
neg-d2-775,2025-01-08,,2501.04742," This study introduces a meta-learning-based approach for low-resource Tabla
Stroke Transcription (TST) and $t\bar{a}la$ identification in Hindustani
classical music. Using Model-Agnostic Meta-Learning (MAML), we address the
challenge of limited annotated datasets, enabling rapid adaptation to new tasks
with minimal data. The method is validated across various datasets, including
tabla solo and concert recordings, demonstrating robustness in polyphonic audio
scenarios. We propose two novel $t\bar{a}la$ identification techniques based on
stroke sequences and rhythmic patterns. Additionally, the approach proves
effective for Automatic Drum Transcription (ADT), showcasing its flexibility
for Indian and Western percussion music. Experimental results show that the
proposed method outperforms existing techniques in low-resource settings,
significantly contributing to music transcription and studying musical
traditions through computational tools.",['eess.AS'],2503.12118," We consider a periodic quantum clock based on cooperative resonance
fluorescence at zero temperature.
  In the quantum case, this system has an exact steady state and the limit
cycle appears in conditional quantum dynamics under homodyne detection. We show
that the intrinsic quantum phase diffusion on the limit cycle leads to
fluctuations in the period. By simulating the stochastic master equation for
homodyne detection, we extract the statistical properties of the clock period.
We show that the precision of the clock satisfies the quantum-thermodynamic
kinetic uncertainty relations. As energy dissipation increases, the clock
quality improves, fully validating, in a quantum stochastic system, the link
between energy dissipation and clock precision.",['quant-ph'],False,,,,"Meta-learning-based percussion transcription and $t\bar{a}la$
  identification from low-resource audio",Quantum Thermodynamics on a limit cycle
neg-d2-776,2025-02-19,,2502.14232," Two bolides (2 June 2016 and 4 April 2019) were detected at multiple regional
infrasound stations with many of the locations receiving multiple detections.
Analysis of the received signals was used to estimate the yield, location and
trajectory, and the type of shock that produced the received signal. The
results from the infrasound analysis were compared with ground truth
information that was collected through other sensing modalities. This
multi-modal framework offers an expanded perspective on the processes governing
bolide shock generation and propagation. The majority of signal features showed
reasonable agreement between the infrasound-based interpretation and the other
observational modalities, though the yield estimate from the 2019 bolide was
significantly lower using the infrasound detections. There was also evidence
suggesting that one of the detections was from a cylindrical shock that was
initially propagating upward, which is unusual though not impossible.","['astro-ph.EP', 'astro-ph.IM', 'physics.ao-ph', 'physics.geo-ph', 'physics.ins-det', 'physics.space-ph']",2501.04279," In daily domestic settings, frequently used objects like cups often have
unfixed positions and multiple instances within the same category, and their
carriers frequently change as well. As a result, it becomes challenging for a
robot to efficiently navigate to a specific instance. To tackle this challenge,
the robot must capture and update scene changes and plans continuously.
However, current object navigation approaches primarily focus on the semantic
level and lack the ability to dynamically update scene representation. In
contrast, this paper captures the relationships between frequently used objects
and their static carriers. It constructs an open-vocabulary
Carrier-Relationship Scene Graph (CRSG) and updates the carrying status during
robot navigation to reflect the dynamic changes of the scene. Based on the
CRSG, we further propose an instance navigation strategy that models the
navigation process as a Markov Decision Process. At each step, decisions are
informed by the Large Language Model's commonsense knowledge and
visual-language feature similarity. We designed a series of long-sequence
navigation tasks for frequently used everyday items in the Habitat simulator.
The results demonstrate that by updating the CRSG, the robot can efficiently
navigate to moved targets. Additionally, we deployed our algorithm on a real
robot and validated its practical effectiveness. The project page can be found
here: https://OpenIN-nav.github.io.",['cs.RO'],False,,,,"Bolide infrasound signal morphology and yield estimates: A case study of
  two events detected by a dense acoustic sensor network","OpenIN: Open-Vocabulary Instance-Oriented Navigation in Dynamic Domestic
  Environments"
neg-d2-777,2025-02-17,,2502.12108," Integrated Gradients (IG), a widely used axiomatic path-based attribution
method, assigns importance scores to input features by integrating model
gradients along a straight path from a baseline to the input. While effective
in some cases, we show that straight paths can lead to flawed attributions. In
this paper, we identify the cause of these misattributions and propose an
alternative approach that treats the input space as a Riemannian manifold,
computing attributions by integrating gradients along geodesics. We call this
method Geodesic Integrated Gradients (GIG). To approximate geodesic paths, we
introduce two techniques: a k-Nearest Neighbours-based approach for smaller
models and a Stochastic Variational Inference-based method for larger ones.
Additionally, we propose a new axiom, Strong Completeness, extending the axioms
satisfied by IG. We show that this property is desirable for attribution
methods and that GIG is the only method that satisfies it. Through experiments
on both synthetic and real-world data, we demonstrate that GIG outperforms
existing explainability methods, including IG.","['cs.LG', 'cs.AI', 'stat.ML']",2501.03859," In this paper, we present a novel synergistic framework for learning shape
estimation and a shape-aware whole-body control policy for tendon-driven
continuum robots. Our approach leverages the interaction between two Augmented
Neural Ordinary Differential Equations (ANODEs) -- the Shape-NODE and
Control-NODE -- to achieve continuous shape estimation and shape-aware control.
The Shape-NODE integrates prior knowledge from Cosserat rod theory, allowing it
to adapt and account for model mismatches, while the Control-NODE uses this
shape information to optimize a whole-body control policy, trained in a Model
Predictive Control (MPC) fashion. This unified framework effectively overcomes
limitations of existing data-driven methods, such as poor shape awareness and
challenges in capturing complex nonlinear dynamics. Extensive evaluations in
both simulation and real-world environments demonstrate the framework's robust
performance in shape estimation, trajectory tracking, and obstacle avoidance.
The proposed method consistently outperforms state-of-the-art end-to-end,
Neural-ODE, and Recurrent Neural Network (RNN) models, particularly in terms of
tracking accuracy and generalization capabilities.",['cs.RO'],False,,,,Using the Path of Least Resistance to Explain Deep Networks,"A Synergistic Framework for Learning Shape Estimation and Shape-Aware
  Whole-Body Control Policy for Continuum Robots"
neg-d2-778,2025-03-10,,2503.16491," The rapid adoption of generative AI in software development has impacted the
industry, yet its effects on developers with visual impairments remain largely
unexplored. To address this gap, we used an Activity Theory framework to
examine how developers with visual impairments interact with AI coding
assistants. For this purpose, we conducted a study where developers who are
visually impaired completed a series of programming tasks using a generative AI
coding assistant. We uncovered that, while participants found the AI assistant
beneficial and reported significant advantages, they also highlighted
accessibility challenges. Specifically, the AI coding assistant often
exacerbated existing accessibility barriers and introduced new challenges. For
example, it overwhelmed users with an excessive number of suggestions, leading
developers who are visually impaired to express a desire for ``AI timeouts.''
Additionally, the generative AI coding assistant made it more difficult for
developers to switch contexts between the AI-generated content and their own
code. Despite these challenges, participants were optimistic about the
potential of AI coding assistants to transform the coding experience for
developers with visual impairments. Our findings emphasize the need to apply
activity-centered design principles to generative AI assistants, ensuring they
better align with user behaviors and address specific accessibility needs. This
approach can enable the assistants to provide more intuitive, inclusive, and
effective experiences, while also contributing to the broader goal of enhancing
accessibility in software development.","['cs.HC', 'cs.AI', 'cs.CY']",2502.08782," The increasing penetration of Distributed Energy Resources (DERs) in the
distribution system has led to the emergence of a new market actor - the
aggregator. The aggregator serves as a facilitator, enabling flexibility asset
owners to get access to different markets. In which, EVs aggregators are
gaining more attention due to their expanding use and potential to provide
services in various types of markets, particularly in the reserve market.
Currently, TSO indirectly utilizes these resources under the management of the
distribution system operators (DSO), which can negatively impact the
distribution grid. Conversely, adjustments from DSOs can impact service
provision to TSO due to the shortage of TSO usage information. These factors
highlight the importance of evaluating the service provision from aggregators
under different TSO-DSO coordination schemes. This paper focuses on the
provision of flexibility from electric vehicles (EVs) aggregators for balancing
service in the TSO-DSO hybrid-managed and compares it with the DSO-managed
coordination schemes. The behavior of aggregators reacting to price
fluctuations and TSO requests under different coordination schemes and
simulation scenarios is thoroughly evaluated. Additionally, their impact on the
grid is analyzed through the DSO's congestion management process and validated
using data from a real part of the Dutch distribution network. Results find
that the hybrid-managed coordination scheme gives more benefit to the
aggregator than the DSO-managed scheme and the EVs aggregator will gain more
profit in winter than summer due to more upward regulation service is needed.","['eess.SY', 'cs.SY']",False,,,,"The Impact of Generative AI Coding Assistants on Developers Who Are
  Visually Impaired","A comparative study of different TSO-DSO coordination in the reserve
  market"
neg-d2-779,2025-03-21,,2503.17072," We implement a ML-based attention framework with component-specific decoders,
improving optical power spectrum prediction in multi-span networks. By reducing
the need for in-depth training on each component, the framework can be scaled
to multi-span topologies with minimal data collection, making it suitable for
brown-field scenarios.","['cs.LG', 'cs.NI']",2502.18385," Phononic materials are crucial for developing efficient, robust mechanical
waveguides with strong transport properties, enabling advances in sensing,
signal processing, energy harvesting, and microfluidics. A key motivation is
their integration into monolithic systems for on-chip applications. While
topological phononic materials developed in the past decade offer
unidirectional edge states immune to backscattering, their integration requires
large volumes to control localized small volumes' transport properties,
limiting their efficiency and application in modern phononic circuits. The
recently introduced chiral anomalous bulk states (CABSs) combine the advantages
of topological materials with innovative boundary designs, overcoming
transmission limitations and ensuring full material utilization for superior
wave propagation. Here, we present the first on-chip monolithic CABS device
integrated on a suspended LiNbO3 thin film. This breakthrough enables the
creation of phononic waveguides with unmatched unidirectionality, low loss, and
high transmission efficiency, seamlessly integrated with broadband
piezoelectric transducers, and showcasing their potential for high-fidelity,
broad-bandwidth microwave signal transmission. Additionally, we exploit the
slow-wave characteristics of CABSs for delay lines and high-density signal
processing. Tailoring wave propagation through boundary engineering opens a new
paradigm for phononic/photonic device design, with implications across
microelectronics, high-frequency communications, radar, and advanced sensing
technologies. The work sets the stage for the future development of highly
scalable, multifunctional, and robust phononic systems, unlocking new avenues
for integrated acoustic technologies.","['physics.app-ph', 'cond-mat.mes-hall', 'cond-mat.mtrl-sci']",False,,,,"Multi-Span Optical Power Spectrum Evolution Modeling using ML-based
  Multi-Decoder Attention Framework","Monolithic On-Chip Phononic Chiral Anomalous Bulk States on LiNbO3
  Thin-films"
neg-d2-780,2025-02-04,,2502.02212," We address the problem of solving a system of linear equations via the
Quantum Singular Value Transformation (QSVT). One drawback of the QSVT
algorithm is that it requires huge quantum resources if we want to achieve an
acceptable accuracy. To reduce the quantum cost, we propose a hybrid
quantum-classical algorithm that improves the accuracy and reduces the cost of
the QSVT by adding iterative refinement in mixed-precision A first quantum
solution is computed using the QSVT, in low precision, and then refined in
higher precision until we get a satisfactory accuracy. For this solver, we
present an error and complexity analysis, and first experiments using the
quantum software stack myQLM.",['quant-ph'],2501.14331," The sources of cosmic rays between the knee and the ankle are still debated.
The Galactic wind and its termination shock have been proposed to contribute to
this transition between Galactic and extragalactic origin, but another
possibility is large-scale shock structures from local sources in the Milky
Way. In this paper, we investigate CR transport in a time-dependent landscape
of shocks in the Galactic halo. These shocks could result from local outbursts,
e.g. starforming regions and superbubbles. CRs re-accelerated at such shocks
can reach energies above the knee. Since the shocks are closer to the Galaxy
than a termination shock and CRs escape downstream, they can propagate back
more easily. With such outbursts happening frequently, shocks will interact.
This interaction could adjust the CR spectrum, particularly for the particles
that are able to be accelerated at two shocks simultaneously. The transport and
acceleration of CRs at the shock is modeled by Stochastic Differential
Equations (SDEs) within the public CR propagation framework CRPropa. We
developed extensions for time-dependent wind profiles and for the first time
connected the code to hydrodynamic simulations, which were run with the public
Athena++ code. We find that, depending on the concrete realization of the
diffusion tensor, a significant fraction of CRs can make it back to the Galaxy.
These could contribute to the observed spectrum around and above the CR knee
($E \gtrsim 10\,\mathrm{PeV}$). In contrast to simplified models, a simple
power-law does not describe the energy spectra well. Instead, for single
shocks, we find a flat spectrum ($E^{-2}$) at low energies, which steepens
gradually until it reaches an exponential decline. When shocks collide, the
energy spectra transiently become harder than $E^{-2}$ at high energies.",['astro-ph.HE'],False,,,,A mixed-precision quantum-classical algorithm for solving linear systems,Cosmic ray transport and acceleration in an evolving shock landscape
neg-d2-781,2025-02-19,,2502.13968," Separable 3D reconstruction of multiple objects from multi-view RGB images --
resulting in two different 3D shapes for the two objects with a clear
separation between them -- remains a sparsely researched problem. It is
challenging due to severe mutual occlusions and ambiguities along the objects'
interaction boundaries. This paper investigates the setting and introduces a
new neuro-implicit method that can reconstruct the geometry and appearance of
two objects undergoing close interactions while disjoining both in 3D, avoiding
surface inter-penetrations and enabling novel-view synthesis of the observed
scene. The framework is end-to-end trainable and supervised using a novel
alpha-blending regularisation that ensures that the two geometries are well
separated even under extreme occlusions. Our reconstruction method is
markerless and can be applied to rigid as well as articulated objects. We
introduce a new dataset consisting of close interactions between a human and an
object and also evaluate on two scenes of humans performing martial arts. The
experiments confirm the effectiveness of our framework and substantial
improvements using 3D and novel view synthesis metrics compared to several
existing approaches applicable in our setting.",['cs.CV'],2501.0944," In this paper, we present a class of systems of non-local conservation laws
in one space-dimension incorporating time delay, which can be used to
investigate the interaction between autonomous and human-driven vehicles, each
characterized by a different reaction time and interaction range. We construct
approximate solutions using a Hilliges-Weidlich scheme and we provide uniform L
$\infty$ and BV estimates which ensure the convergence of the scheme, thus
obtaining existence of entropy weak solutions of bounded variation. Uniqueness
follows from an L 1 stability result derived from the entropy condition.
Additionally, we provide numerical simulations to illustrate applications to
mixed autonomous / human-driven traffic flow modeling. In particular, we show
that the presence of autonomous vehicles improves overall traffic flow and
stability.","['math.AP', 'cs.NA', 'math.NA']",False,,,,"Betsu-Betsu: Multi-View Separable 3D Reconstruction of Two Interacting
  Objects","A multi-class non-local macroscopic model with time delay for mixed
  autonomous / human-driven traffic"
neg-d2-782,2025-03-15,,2503.13533," As artificial intelligence (AI) technology becomes increasingly prevalent in
the filed of education, there is a growing need for mathematics teacher
education students (MTES) to demonstrate proficiency in the integration of AI
with the technological pedagogical content knowledge (AI-TPACK). To study the
issue, we firstly devised an systematic AI-TPACK scale and test on 412 MTES
from seven universities. Through descriptive statistical analyses, we found
that the current status of AI-TPACK for MTES in China is at a basic,
preliminary stage. Secondly, we compared MTES between three different grades on
the six variables and found that there is no discernible difference, which
suggested that graduate studies were observed to have no promotion in the
development of AI-TPACK competencies. Thirdly, we proposed a new AI-TPACK
structural equation model (AI-TPACK-SEM) to explore the impact of self-efficacy
and teaching beliefs on AI-TPACK. Our findings indicate a positive correlation
between self-efficacy and AI-TPACK. We also come to a conclusion that may be
contrary to common perception, excessive teaching beliefs may impede the
advancement of AI-TPACK. Overall, this paper revealed the current status of
AI-TPACK for MTES in China for the first time, designed a dedicated SEM to
study the effect of specific factors on AI-TPACK, and proposed some suggestions
on future developments.","['cs.CY', 'cs.AI']",2501.13826," Humans acquire knowledge through three cognitive stages: perceiving
information, comprehending knowledge, and adapting knowledge to solve novel
problems. Videos serve as an effective medium for this learning process,
facilitating a progression through these cognitive stages. However, existing
video benchmarks fail to systematically evaluate the knowledge acquisition
capabilities in Large Multimodal Models (LMMs). To address this gap, we
introduce Video-MMMU, a multi-modal, multi-disciplinary benchmark designed to
assess LMMs' ability to acquire and utilize knowledge from videos. Video-MMMU
features a curated collection of 300 expert-level videos and 900
human-annotated questions across six disciplines, evaluating knowledge
acquisition through stage-aligned question-answer pairs: Perception,
Comprehension, and Adaptation. A proposed knowledge gain metric,
{\Delta}knowledge, quantifies improvement in performance after video viewing.
Evaluation of LMMs reveals a steep decline in performance as cognitive demands
increase and highlights a significant gap between human and model knowledge
acquisition, underscoring the need for methods to enhance LMMs' capability to
learn and adapt from videos.","['cs.CV', 'cs.CL']",False,,,,"The Status Quo and Future of AI-TPACK for Mathematics Teacher Education
  Students: A Case Study in Chinese Universities","Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline
  Professional Videos"
neg-d2-783,2025-02-20,,2502.15168," Style embeddings are useful for stylistic analysis and style transfer;
however, only English style embeddings have been made available. We introduce
Multilingual StyleDistance (mStyleDistance), a multilingual style embedding
model trained using synthetic data and contrastive learning. We train the model
on data from nine languages and create a multilingual STEL-or-Content benchmark
(Wegmann et al., 2022) that serves to assess the embeddings' quality. We also
employ our embeddings in an authorship verification task involving different
languages. Our results show that mStyleDistance embeddings outperform existing
models on these multilingual style benchmarks and generalize well to unseen
features and languages. We make our model publicly available at
https://huggingface.co/StyleDistance/mstyledistance .",['cs.CL'],2503.1528," We develop a system of non-linear stochastic evolution equations that
describes the continuous measurements of quantum systems with mixed initial
state. We address quantum systems with unbounded Hamiltonians and unbounded
interaction operators. Using arguments of the theory of quantum measurements we
derive a system of stochastic interacting wave functions (SIWF for short) that
models the continuous monitoring of quantum systems. We prove the existence and
uniqueness of the solution to this system under conditions general enough for
the applications. We obtain that the mixed state generated by the SIWF at any
time does not depend on the initial state, and satisfies the diffusive
stochastic quantum master equation, which is also known as Belavkin equation.
We present two physical examples. In one, the SIWF becomes a system of
non-linear stochastic partial differential equations. In the other, we deal
with a model of a circuit quantum electrodynamics.","['math-ph', 'math.MP', 'math.PR', 'quant-ph']",False,,,,mStyleDistance: Multilingual Style Embeddings and their Evaluation,"System of stochastic interacting wave functions that model quantum
  measurements"
neg-d2-784,2025-03-04,,2503.02345," The detection of Alzheimer disease (AD) from clinical MRI data is an active
area of research in medical imaging. Recent advances in quantum computing,
particularly the integration of parameterized quantum circuits (PQCs) with
classical machine learning architectures, offer new opportunities to develop
models that may outperform traditional methods. However, quantum machine
learning (QML) remains in its early stages and requires further experimental
analysis to better understand its behavior and limitations. In this paper, we
propose an end to end hybrid classical quantum convolutional neural network (CQ
CNN) for AD detection using clinically formatted 3D MRI data. Our approach
involves developing a framework to make 3D MRI data usable for machine
learning, designing and training a brain tissue segmentation model (Skull Net),
and training a diffusion model to generate synthetic images for the minority
class. Our converged models exhibit potential quantum advantages, achieving
higher accuracy in fewer epochs than classical models. The proposed beta8 3
qubit model achieves an accuracy of 97.50%, surpassing state of the art (SOTA)
models while requiring significantly fewer computational resources. In
particular, the architecture employs only 13K parameters (0.48 MB), reducing
the parameter count by more than 99.99% compared to current SOTA models.
Furthermore, the diffusion-generated data used to train our quantum models, in
conjunction with real samples, preserve clinical structural standards,
representing a notable first in the field of QML. We conclude that CQCNN
architecture like models, with further improvements in gradient optimization
techniques, could become a viable option and even a potential alternative to
classical models for AD detection, especially in data limited and resource
constrained clinical settings.","['quant-ph', 'cs.AI', 'cs.CV', 'cs.LG']",2502.12705," Two-dimensional (2D) layered nanomaterials heterostructures, arising from the
combination of 2D materials with other low-dimensional species, feature large
surface area to volume ratio, which provides a high density of active sites for
catalytic ap-plications and in particular for (photo)electrocatalysis (PEC).
Meanwhile, their unique electronic band structure and high electrical
conductivity enable efficient charge transfer (CT) between the active material
and the substrate, which is essential for catalytic activity. In recent years,
researchers have demonstrated the potential of a range of 2D material
interfaces, such as graphene, graphitic carbon nitride (g-C3N4), metal
chalcogenides (MCs), and MXenes, for (photo)electrocatalytic applica-tions. For
instance, MCs such as MoS2 and WS2 have shown excellent catalytic activity for
hydrogen evolution, while gra-phene and MXenes have been used for the reduction
of carbon dioxide to higher value chemicals. However, despite their great
potential, there are still major challenges that need to be addressed in order
to fully realize the potential of 2D materials for PEC. For example, their
stability under harsh reaction conditions, as well as their scalability for
large-scale production are important factors to be considered. Generating
heterojunctions (HJs) by combining 2D layered structures with other
na-nomaterials is a promising method to improve the photoelectrocatalytic
properties of the former. In this review, we inspect thoroughly the recent
literature, to demonstrate the significant potential that arises from utilizing
2D layered heterostructures in PEC processes across a broad spectrum of
applications, from energy conversion and storage to environmental remediation.
With the ongoing research and development, it is likely that the potential of
these materials will be fully expressed in the near future.",['cond-mat.mtrl-sci'],False,,,,"CQ CNN: A Hybrid Classical Quantum Convolutional Neural Network for
  Alzheimer's Disease Detection Using Diffusion Generated and U Net Segmented
  3D MRI",2D Layered Heterojunctions for Photoelectrocatalysis
neg-d2-785,2025-01-08,,2501.04338," Numerous biological and microscale systems exhibit synchronization in noisy
environments. The theory of such noisy oscillators and their synchronization
has been developed and experimentally demonstrated, but inferring the noise
intensity and phase response is not always straightforward. In this study, we
propose a useful formula that enables us to infer the noise intensity and phase
response of a noisy oscillator synchronized with periodic external forcing.
Through asymptotic approximations for small noise, we show that noisy
synchronous oscillators satisfy a simple relationship among the noise intensity
and measurable quantities, i.e., the stationary distribution of the oscillation
phase and stationary probability current obtained as the average phase
velocity, which is verified through systematic numerical analysis. The proposed
formula facilitates a unified analysis and design of synchronous oscillators in
weakly noisy environments.",['nlin.AO'],2501.09367," Large language models (LLMs), while driving a new wave of interactive AI
applications across numerous domains, suffer from high inference costs and
heavy cloud dependency. Motivated by the redundancy phenomenon in linguistics,
we propose a progressive inference paradigm over cloud and edge, i.e., firstly
generating the sketch of the answer by LLMs at cloud, and then conducting
parallel extension to fill in details by small models (SLMs) at edge.
Progressive inference offers potential benefits to improve throughput and
reduce inference latency while facing key implementation challenges, including
decreased response quality from SLMs, a tradeoff between the brevity and
comprehensiveness of sketches, as well as increased latency caused by network
transmission and edge inference. In this work, we propose and implement PICE,
an LLM serving system with semantic-level cloud-edge collaboration, enhancing
inference throughput and quality through dynamic inference task scheduling,
ensemble learning, and parallel edge inference. Extensive testbed experiments
illustrate that our approach achieves $1.5-2\times$ throughput enhancement and
up to 43% latency reduction, while also potentially enhancing the quality
compared to SOTA systems.",['cs.DC'],False,,,,"Inference of noise intensity and phase response from noisy synchronous
  oscillators","PICE: A Semantic-Driven Progressive Inference System for LLM Serving in
  Cloud-Edge Networks"
neg-d2-786,2025-02-24,,2502.16941," Instance-level change detection in 3D scenes presents significant challenges,
particularly in uncontrolled environments lacking labeled image pairs,
consistent camera poses, or uniform lighting conditions. This paper addresses
these challenges by introducing a novel approach for detecting changes in
real-world scenarios. Our method leverages 4D Gaussians to embed multiple
images into Gaussian distributions, enabling the rendering of two coherent
image sequences. We segment each image and assign unique identifiers to
instances, facilitating efficient change detection through ID comparison.
Additionally, we utilize change maps and classification encodings to categorize
4D Gaussians as changed or unchanged, allowing for the rendering of
comprehensive change maps from any viewpoint. Extensive experiments across
various instance-level change detection datasets demonstrate that our method
significantly outperforms state-of-the-art approaches like C-NERF and CYWS-3D,
especially in scenarios with substantial lighting variations. Our approach
offers improved detection accuracy, robustness to lighting changes, and
efficient processing times, advancing the field of 3D change detection.",['cs.CV'],2503.17201," The commodity and widespread use of online shopping are having an
unprecedented impact on climate, with emission figures from key actors that are
easily comparable to those of a large-scale metropolis. Despite online shopping
being fueled by recommender systems (RecSys) algorithms, the role and potential
of the latter in promoting more sustainable choices is little studied. One of
the main reasons for this could be attributed to the lack of a dataset
containing carbon footprint emissions for the items. While building such a
dataset is a rather challenging task, its presence is pivotal for opening the
doors to novel perspectives, evaluations, and methods for RecSys research. In
this paper, we target this bottleneck and study the environmental role of
RecSys algorithms. First, we mine a dataset that includes carbon footprint
emissions for its items. Then, we benchmark conventional RecSys algorithms in
terms of accuracy and sustainability as two faces of the same coin. We find
that RecSys algorithms optimized for accuracy overlook greenness and that
longer recommendation lists are greener but less accurate. Then, we show that a
simple reranking approach that accounts for the item's carbon footprint can
establish a better trade-off between accuracy and greenness. This reranking
approach is modular, ready to use, and can be applied to any RecSys algorithm
without the need to alter the underlying mechanisms or retrain models. Our
results show that a small sacrifice of accuracy can lead to significant
improvements of recommendation greenness across all algorithms and list
lengths. Arguably, this accuracy-greenness trade-off could even be seen as an
enhancement of user satisfaction, particularly for purpose-driven users who
prioritize the environmental impact of their choices. We anticipate this work
will serve as the starting point for studying RecSys for more sustainable
recommendations.",['cs.IR'],False,,,,Gaussian Difference: Find Any Change Instance in 3D Scenes,"Towards Carbon Footprint-Aware Recommender Systems for Greener Item
  Recommendation"
neg-d2-787,2025-01-02,,2501.02019," Modeling the associations between real world entities from their multivariate
cross-sectional profiles can provide cues into the concerted working of these
entities as a system. Several techniques have been proposed for deciphering
these associations including constraint-based Bayesian structure learning (BSL)
algorithms that model them as directed acyclic graphs. Benchmarking these
algorithms have typically focused on assessing the variation in performance
measures such as sensitivity as a function of the dimensionality represented by
the number of nodes in the DAG, and sample size. The present study elucidates
the importance of network topology in benchmarking exercises. More
specifically, it investigates variations in sensitivity across distinct network
topologies while constraining the nodes, edges, and sample-size to be
identical, eliminating these as potential confounders. Sensitivity of three
popular constraint-based BSL algorithms (Peter-Clarke, Grow-Shrink, Incremental
Association Markov Blanket) in learning the network structure from multivariate
cross-sectional profiles sampled from network models with sub-linear, linear,
and super-linear DAG topologies generated using preferential attachment is
investigated. Results across linear and nonlinear models revealed statistically
significant $(\alpha=0.05)$ decrease in sensitivity estimates from sub-linear
to super-linear topology constitutively across the three algorithms. These
results are demonstrated on networks with nodes $(N_{nods}=48,64)$, noise
strengths $(\sigma =3,6)$ and sample size $(N = 2^{10})$. The findings
elucidate the importance of accommodating the network topology in
constraint-based BSL benchmarking exercises.","['cs.LG', 'cs.AI', 'q-bio.MN']",2503.15171," Electric double layer (EDL) formation underlies the functioning of
supercapacitors and several other electrochemical technologies. Here, we study
how the EDL formation near two flat blocking electrodes separated by $2L$ is
affected by beyond-mean-field Coulombic interactions, which can be substantial
for electrolytes of high salt concentration or with multivalent ions. Our model
combines the Nernst-Planck and Bazant-Storey-Kornyshev (BSK) equations; the
latter is a modified Poisson equation with a correlation length $\ell_c$. In
response to a voltage step, the system charges exponentially with a
characteristic timescale $\tau$ that depends nonmonotonically on $\ell_c$. For
small $\ell_c$, $\tau$ is given by the BSK capacitance times a dilute
electrolyte's resistance, in line with [Zhao, Phys. Rev. E 84, 051504 (2011)];
here, $\tau$ decreases with increasing $\ell_c$. Increasing the correlation
length beyond $\ell_c\approx L^{2/3}\lambda_D^{1/3}$, with $\lambda_D$ the
Debye length, $\tau$ reaches a minimum, rises as $\tau\propto
\lambda_D\ell_c/D$, and plateaus at $\tau=4L^2/(\pi^2 D)$. Our results imply
that strongly correlated, strongly confined electrolytes - ionic liquids in the
surface force balance apparatus, say - move slower than predicted so far.","['physics.chem-ph', 'cond-mat.soft']",False,,,,"Benchmarking Constraint-Based Bayesian Structure Learning Algorithms:
  Role of Network Topology","Charging dynamics of electric double layer capacitors including
  beyond-mean-field electrostatic correlations"
neg-d2-788,2025-01-15,,2501.0883," In this work, we offer a historical stroll through the vast topic of binary
quadratic forms. We begin with a quick review of their history and then an
overview of contemporary algebraic developments on the subject.","['math.HO', 'math.NT']",2502.13384," The derivative of a polynomial with all zeros on the unit circle has the
zeros of its derivative on or inside the unit circle. It has been observed that
in many cases the zeros of the derivative have a bimodal distribution: there
are two smaller circles near which it is more likely to find those zeros. We
identify the likely source of the second mode. This idea is supported with
numerical examples involving the characteristic polynomials of random unitary
matrices.","['math.CV', 'math.NT']",False,,,,Binary quadratic forms: modern developments,The bimodal distribution in the derivative of unitary polynomials
neg-d2-789,2025-01-22,,2501.13345," For large-scale network systems, network centrality based on control theory
plays a crucial role in understanding their properties and controlling them
efficiently. The controllability score is such a centrality index and can give
a physically meaningful measure. Nevertheless, the existing work is limited to
linear time-invariant (LTI) systems and the controllability score cannot be
applied to linear time-varying (LTV) systems, which include essential models
such as temporal networks for real application. This paper extends it to apply
to LTV systems. Since it is defined as an optimal solution to some optimization
problem, it is not necessarily uniquely determined. Its uniqueness must be
guaranteed for reproducibility and interpretability. This paper also shows its
uniqueness in most practical cases, which guarantees its use as a network
centrality. In addition, we propose a data-driven method to compute it for its
practical use. Finally, in numerical experiments, we compare controllability
scores between LTI and LTV systems and assess the performance of the proposed
data-driven method.",['math.OC'],2502.1972," We study the performance of the linear consensus algorithm on strongly
connected graphs using the linear quadratic (LQ) cost as a performance measure.
  In particular, we derive bounds on the LQ cost by leveraging effective
resistance. Our results extend previous analyses -- which were limited to
reversible cases -- to the nonreversible setting. To facilitate this
generalization, we introduce novel concepts, termed the back-and-forth path and
the pivot node, which serve as effective alternatives to traditional techniques
that require reversibility. Moreover, we apply our approach to geometric graphs
to estimate the LQ cost without the reversibility assumption. The proposed
approach provides a framework that can be adapted to other contexts where
reversibility is typically assumed.","['math.OC', 'cs.MA']",False,,,,Controllability scores of linear time-varying network systems,"Analysis of Linear Consensus Algorithm on Strongly Connected Graph Using
  Effective Resistance"
neg-d2-790,2025-02-27,,2502.20506," This study examines the mid-infrared properties of Giant HII (GHII) regions
in the Milky Way's Central Molecular Zone (CMZ) -- Sgr B1, Sgr B2, and Sgr C --
using SOFIA-FORCAST imaging at 25 and 37 microns. It compares these
mid-infrared data with previous multi-wavelength observations to explore their
present star formation activity and global properties. The study identifies 77
massive young stellar object (MYSO) candidates in and around the three regions.
Sgr B2 appears to host the youngest MYSOs and have much higher extinction than
the other regions, containing several radio sources not detected in the
mid-infrared even at 37 microns. Meanwhile, cm radio continuum regions of Sgr
B1 shows remarkable correspondence to its mid-infrared emission. Sgr C has
fewer confirmed MYSOs, and seems to have a higher fraction of low-mass young
stellar objects and contamination from more evolved interloper/foreground
stars. Derived MYSO densities are consistent with GHII regions elsewhere in the
Galactic plane, though the CMZ GHII regions appear to have less prolific
present star formation overall. Unlike Sgr B2, the cm continuum emission in Sgr
B1 and Sgr C GHII regions appears to be absent cold dust and molecular gas,
suggesting environmental differences, possibly driven by turbulence and rapid
dynamical changes near the Galactic Center. Furthermore, unlike typical GHII
regions, Sgr B1 and Sgr C are significantly ionized by evolved interloper
stars, which likely did not form within these regions. In these ways, Sgr B1
and Sgr C deviate from classical GHII region behavior, thus potentially
representing a new category of GHII region or challenging their classification
as GHII regions.","['astro-ph.GA', 'astro-ph.SR']",2503.11721," We show that Laser Interferometer Space Antenna can uniquely identify the
sites of intermediate-mass binary black hole (IMBBH) mergers if they occur in
Active Galactic Nuclei (AGN) disks with a gas density $\rho\geq10^{-12} \, {\rm
g/cc}$ via measurement of dynamical friction effect in the gravitational
waveform. We find that even a single observation of a gravitational wave source
with a total mass of $10^3 M_{\odot}$ and a mass ratio of 2 at a luminosity
distance of 3 Gpc is sufficient to confidently associate the merger to be in an
AGN disk with a density $\sim 10^{-12} \, {\rm g/cc}$, as it allows estimation
of the density with an error bar ${\cal O}(100\%)$. This provides a new way of
inferring AGN disk densities that complement traditional X-ray observations.
Further, we find that neglecting the presence of environmental effects in the
waveform models used for parameter estimation can bias the chirp mass, mass
ratio and arrival time of a merger. If not corrected, this can significantly
impact our ability to carry out multiband data analysis of IMBBHs that combines
information from LISA and the ground-based gravitational wave detectors.","['astro-ph.HE', 'astro-ph.GA', 'gr-qc']",False,,,,"Surveying the Giant HII Regions of the Milky Way with SOFIA: VII.
  Galactic Center Regions Sgr B1, Sgr B2, and Sgr C","Identifying intermediate mass binary black hole mergers in AGN disks
  using LISA"
neg-d2-791,2025-02-20,,2502.14378," This paper explores extremal self-dual double circulant (DC) codes and linear
complementary dual (LCD) codes of arbitrary length over the Galois field
$\mathbb F_2$. We establish the sufficient and necessary conditions for DC
codes and bordered DC codes to be self-dual and identify the conditions for
self-dual DC codes of length up to 44 to be extremal or non-extremal.
Additionally, The self-duality and extremality between DC codes and bordered DC
codes are also examined. Finally, sufficient conditions for bordered DC codes
to be LCD codes over $\mathbb F_2$ under Euclidean inner product are presented.","['cs.IT', 'math.IT']",2501.13217," In 1985, Chv\'{a}tal introduced the concept of star cutsets as a means to
investigate the properties of perfect graphs, which inspired many researchers
to study cutsets with some specific structures, for example, star cutsets,
clique cutsets, stable cutsets. In recent years, approximation algorithms have
developed rapidly, the computational complexity associated with determining the
minimum vertex cut possessing a particular structural property have attracted
considerable academic attention.
  In this paper, we demonstrate that determining whether there is a matching
vertex-cutset in $H$ with size at most $k$, is $\mathbf{NP}$-complete, where
$k$ is a given positive integer and $H$ is a connected graph. Furthermore, we
demonstrate that for a connected graph $H$, there exists a $2$-approximation
algorithm in $O(nm^2)$ for us to find a minimum matching vertex-cutset.
Finally, we show that every plane graph $H$ satisfying $H\not\in\{K_2, K_4\}$
contains a matching vertex-cutset with size at most three, and this bound is
tight.","['cs.DS', 'math.CO']",False,,,,"Extremal Self-Dual Codes and Linear Complementary Dual Codes from Double
  Circulant Codes",Complexity and Algorithm for the Matching vertex-cutset Problem
neg-d2-792,2025-02-14,,2502.10537," Analyzing data subgroups is a common data science task to build intuition
about a dataset and identify areas to improve model performance. However,
subgroup analysis is prohibitively difficult in datasets with many features,
and existing tools limit unexpected discoveries by relying on user-defined or
static subgroups. We propose exploratory subgroup analysis as a set of tasks in
which practitioners discover, evaluate, and curate interesting subgroups to
build understanding about datasets and models. To support these tasks we
introduce Divisi, an interactive notebook-based tool underpinned by a fast
approximate subgroup discovery algorithm. Divisi's interface allows data
scientists to interactively re-rank and refine subgroups and to visualize their
overlap and coverage in the novel Subgroup Map. Through a think-aloud study
with 13 practitioners, we find that Divisi can help uncover surprising patterns
in data features and their interactions, and that it encourages more thorough
exploration of subtypes in complex data.",['cs.HC'],2503.04028," Stress-stress correlations in crystalline solids with long-range order can be
straightforwardly derived using elasticity theory. In contrast, the `emergent
elasticity' of amorphous solids, rigid materials characterized by an underlying
disordered structure, defies direct explanation within traditional theoretical
frameworks. To address this challenge, tensor gauge theories have been recently
proposed as a promising approach to describe the emergent elasticity of
disordered solids and predict their stress-stress correlations. In this work,
we revisit this problem in two-dimensional amorphous and crystalline solids by
employing a canonical elasticity theory approach, supported by experimental and
simulation data. We demonstrate that, with respect to static stress-stress
correlations, the response of a 2D disordered solid is indistinguishable from
that of a 2D isotropic crystalline solid and it is well predicted by vanilla
elasticity theory. Moreover, we show that the presence of pinch-point
singularities in the stress response is not an exclusive feature of amorphous
solids. Our results confirm previous observations about the universal character
of static stress-stress correlations in crystalline and amorphous packings.","['cond-mat.soft', 'cond-mat.mtrl-sci', 'cond-mat.stat-mech']",False,,,,"Divisi: Interactive Search and Visualization for Scalable Exploratory
  Subgroup Analysis","Stress-stress correlations in two-dimensional amorphous and crystalline
  solids"
neg-d2-793,2025-01-16,,2501.0944," In this paper, we present a class of systems of non-local conservation laws
in one space-dimension incorporating time delay, which can be used to
investigate the interaction between autonomous and human-driven vehicles, each
characterized by a different reaction time and interaction range. We construct
approximate solutions using a Hilliges-Weidlich scheme and we provide uniform L
$\infty$ and BV estimates which ensure the convergence of the scheme, thus
obtaining existence of entropy weak solutions of bounded variation. Uniqueness
follows from an L 1 stability result derived from the entropy condition.
Additionally, we provide numerical simulations to illustrate applications to
mixed autonomous / human-driven traffic flow modeling. In particular, we show
that the presence of autonomous vehicles improves overall traffic flow and
stability.","['math.AP', 'cs.NA', 'math.NA']",2502.11911," In recent studies, analogs of the electronic Quantum Spin-Hall Effect have
been explored within photonic crystals that incorporate spatial symmetries,
especially those with $ C_{6v} $ symmetry, where $ \mathbb{Z}_2 $ topological
invariants are enforced by crystalline symmetry. These photonic crystals
possess bulk states with well-defined pseudospins and exhibit helical edge
states, closely resembling their electronic counterparts. However, achieving
$\mathbb{Z}_2$ topological protection in a square lattice photonic crystal
remains great theoretical and experimental challange. In this work, we propose
a single material photonic crystal structure based on a $ C_4 $ lattice that
supports partially $ \mathbb{Z}_2 $-protected edge modes. We show that this
structure can host photonic band-gap that hosts $ \mathbb{Z}_2 $-like modes,
enabling perfect transmission in waveguide applications. Furthermore, we
investigate the robustness of these modes against structural defects and
directional turns, highlighting the distinctions between full $ \mathbb{Z}_2 $
topological protection and partial topological protection. Finally, we analyze
the impact of the number of elementary cells surrounding the interface on the
formation and stability of these protected modes.",['physics.optics'],False,,,,"A multi-class non-local macroscopic model with time delay for mixed
  autonomous / human-driven traffic",Partial Topological Protection in C4 Lattices for Optical Communications
neg-d2-794,2025-01-08,,2501.04866," M dwarfs are the most common stars in the galaxy, with long lifespans, a high
occurrence rate of rocky planets, and close-in habitable zones. However, high
stellar activity in the form of frequent flaring and any associated coronal
mass ejections may drive atmospheric escape with the bombardment of radiation
and high-energy particles, drastically impacting the habitability of these
systems. The stellar latitude where flares and coronal mass ejections occur
determines the space weather that exoplanets are subject to, with high-energy
particle events associated with equatorial flares producing significant
atmospheric erosion. However, the flaring latitudes for M dwarfs remain largely
unconstrained. To aid in the effort to locate these flaring regions we explore
the applicability of flare occultations using optical photometry to identify
the latitudes of flares. As a planet transits in front of an ongoing flare the
timing and geometry of the transit can be used to constrain the latitude and
longitude of the flare. We predict the probability of detecting an occultation
for known transiting planets and eclipsing binaries. From this, we estimate
3-22 detectable occultations exist within the TESS primary mission photometry,
with the majority occurring in eclipsing binary observations. To demonstrate
this technique, we analyze a candidate flare occultation event for the
eclipsing binary CM Draconis.","['astro-ph.EP', 'astro-ph.SR']",2501.04162," Let $N$ and $p$ be prime numbers with $p \geq 5$ such that $p || (N + 1)$. In
a previous paper, we showed that there is a cuspform $f$ of weight 2 and level
$\Gamma_0(N^2)$ whose $\ell$-th Fourier coefficient is congruent to $\ell + 1$
modulo a prime above $p$ for all primes $\ell$. In this paper, we prove that
this form $f$ is unique up to Galois conjugacy, and the extension of
$\mathbb{Z}_p$ generated by the coefficients of $f$ is exactly
$\mathbb{Z}_p[\zeta_p + \zeta_p^{-1}]$. We also prove similar results when a
higher power of $p$ divides $N + 1$.",['math.NT'],False,,,,Identifying Flare Locations Through Exoplanet Transit Occultations,The Eisenstein ideal at prime-square level has constant rank
neg-d2-795,2025-02-14,,2502.10559," Accurate morphometric assessment of cartilage-such as thickness/volume-via
MRI is essential for monitoring knee osteoarthritis. Segmenting cartilage
remains challenging and dependent on extensive expert-annotated datasets, which
are heavily subjected to inter-reader variability. Recent advancements in
Visual Foundational Models (VFM), especially memory-based approaches, offer
opportunities for improving generalizability and robustness. This study
introduces a deep learning (DL) method for cartilage and meniscus segmentation
from 3D MRIs using interactive, memory-based VFMs. To improve spatial awareness
and convergence, we incorporated a Hybrid Shuffling Strategy (HSS) during
training and applied a segmentation mask propagation technique to enhance
annotation efficiency. We trained four AI models-a CNN-based 3D-VNet, two
automatic transformer-based models (SaMRI2D and SaMRI3D), and a
transformer-based promptable memory-based VFM (SAMRI-2)-on 3D knee MRIs from
270 patients using public and internal datasets and evaluated on 57 external
cases, including multi-radiologist annotations and different data acquisitions.
Model performance was assessed against reference standards using Dice Score
(DSC) and Intersection over Union (IoU), with additional morphometric
evaluations to further quantify segmentation accuracy. SAMRI-2 model, trained
with HSS, outperformed all other models, achieving an average DSC improvement
of 5 points, with a peak improvement of 12 points for tibial cartilage. It also
demonstrated the lowest cartilage thickness errors, reducing discrepancies by
up to threefold. Notably, SAMRI-2 maintained high performance with as few as
three user clicks per volume, reducing annotation effort while ensuring
anatomical precision. This memory-based VFM with spatial awareness offers a
novel approach for reliable AI-assisted knee MRI segmentation, advancing DL in
musculoskeletal imaging.","['eess.IV', 'cs.AI', 'cs.CV']",2502.17715," Effective conversational systems are expected to dynamically generate
contextual follow-up questions to elicit new information while maintaining the
conversation flow. While humans excel at asking diverse and informative
questions by intuitively assessing both obtained and missing information,
existing models often fall short of human performance on this task. To mitigate
this, we propose a method that generates diverse and informative questions
based on targeting unanswered information using a hypothetical LLM-generated
""comprehensive answer"". Our method is applied to augment an existing follow-up
questions dataset. The experimental results demonstrate that language models
fine-tuned on the augmented datasets produce follow-up questions of
significantly higher quality and diversity. This promising approach could be
effectively adopted to future work to augment information-seeking dialogues for
reducing ambiguities and improving the accuracy of LLM answers.","['cs.CL', 'cs.AI', 'cs.HC']",False,,,,"SAMRI-2: A Memory-based Model for Cartilage and Meniscus Segmentation in
  3D MRIs of the Knee Joint","Bridging Information Gaps with Comprehensive Answers: Improving the
  Diversity and Informativeness of Follow-Up Questions"
neg-d2-796,2025-01-12,,2501.06785," Understanding objects in 3D at the part level is essential for humans and
robots to navigate and interact with the environment. Current datasets for
part-level 3D object understanding encompass a limited range of categories. For
instance, the ShapeNet-Part and PartNet datasets only include 16, and 24 object
categories respectively. The 3DCoMPaT dataset, specifically designed for
compositional understanding of parts and materials, contains only 42 object
categories. To foster richer and fine-grained part-level 3D understanding, we
introduce 3DCoMPaT200, a large-scale dataset tailored for compositional
understanding of object parts and materials, with 200 object categories with
$\approx$5 times larger object vocabulary compared to 3DCoMPaT and $\approx$ 4
times larger part categories. Concretely, 3DCoMPaT200 significantly expands
upon 3DCoMPaT, featuring 1,031 fine-grained part categories and 293 distinct
material classes for compositional application to 3D object parts.
Additionally, to address the complexities of compositional 3D modeling, we
propose a novel task of Compositional Part Shape Retrieval using ULIP to
provide a strong 3D foundational model for 3D Compositional Understanding. This
method evaluates the model shape retrieval performance given one, three, or six
parts described in text format. These results show that the model's performance
improves with an increasing number of style compositions, highlighting the
critical role of the compositional dataset. Such results underscore the
dataset's effectiveness in enhancing models' capability to understand complex
3D shapes from a compositional perspective. Code and Data can be found at
http://github.com/3DCoMPaT200/3DCoMPaT200","['cs.CV', 'cs.CL']",2503.13533," As artificial intelligence (AI) technology becomes increasingly prevalent in
the filed of education, there is a growing need for mathematics teacher
education students (MTES) to demonstrate proficiency in the integration of AI
with the technological pedagogical content knowledge (AI-TPACK). To study the
issue, we firstly devised an systematic AI-TPACK scale and test on 412 MTES
from seven universities. Through descriptive statistical analyses, we found
that the current status of AI-TPACK for MTES in China is at a basic,
preliminary stage. Secondly, we compared MTES between three different grades on
the six variables and found that there is no discernible difference, which
suggested that graduate studies were observed to have no promotion in the
development of AI-TPACK competencies. Thirdly, we proposed a new AI-TPACK
structural equation model (AI-TPACK-SEM) to explore the impact of self-efficacy
and teaching beliefs on AI-TPACK. Our findings indicate a positive correlation
between self-efficacy and AI-TPACK. We also come to a conclusion that may be
contrary to common perception, excessive teaching beliefs may impede the
advancement of AI-TPACK. Overall, this paper revealed the current status of
AI-TPACK for MTES in China for the first time, designed a dedicated SEM to
study the effect of specific factors on AI-TPACK, and proposed some suggestions
on future developments.","['cs.CY', 'cs.AI']",False,,,,"3DCoMPaT200: Language-Grounded Compositional Understanding of Parts and
  Materials of 3D Shapes","The Status Quo and Future of AI-TPACK for Mathematics Teacher Education
  Students: A Case Study in Chinese Universities"
neg-d2-797,2025-03-14,,2503.11098," High-dimensional broadband quantum memory significantly expands quantum
information processing capabilities, but the memory efficiency becomes
insufficient when extended to high dimensions. We demonstrate an efficient
quantum memorize for hyper-dimensional photons encoded with orbital angular
momentum (OAM) and spin angular momentum (SAM). OAM information is encoded from
-5 to +5, combined with spin angular momentum encoding, enabling up to 22
dimensions. To ensure high memory efficiency, an artificial intelligent
algorithm, a modified Differential Evolution (DE) algorithm using Chebyshev
sampling, is developed to obtain a perfect signal-control waveform matching.
Memory efficiency is experimentally achieved 92% for single-mode Gaussian
signal, 91% for information dimension of 6 and 80% for dimensional number to
22. The fidelity is achieved up to 99% for single-mode Gaussian signal, 96% for
OAM information and 97% for SAM one, which is far beyond no-cloning limitation.
Our results demonstrate superior performance and potential applications in
high-dimensional quantum information processing. This achievement provides a
crucial foundation for future quantum communication and quantum computing.",['quant-ph'],2502.12181," Explainability remains a significant problem for AI models in medical
imaging, making it challenging for clinicians to trust AI-driven predictions.
We introduce 3D ReX, the first causality-based post-hoc explainability tool for
3D models. 3D ReX uses the theory of actual causality to generate
responsibility maps which highlight the regions most crucial to the model's
decision. We test 3D ReX on a stroke detection model, providing insight into
the spatial distribution of features relevant to stroke.","['eess.IV', 'cs.AI', 'cs.CV', 'cs.LG']",False,,,,"AI-assisted hyper-dimensional broadband quantum memory with efficiency
  above 90% in warm atoms",3D ReX: Causal Explanations in 3D Neuroimaging Classification
neg-d2-798,2025-02-26,,2502.19107," Generative artificial intelligence (generative AI) has entered the mainstream
culture and become a subject of extensive academic investigation. However, the
character and background of its impact on art require subtler scrutiny and more
nuanced contextualization. This paper summarizes a broader study of the roles
that AI's conceptual and ideological substrata play in influencing art notions.
The focus is on divergent but coalescing and often questionable ideas, values,
and political views that generative AI and other art-related AI technologies
propagate from the computer science and AI/tech industry to the contemporary
art and culture. The paper maps the main areas of this complex relationship and
concisely critiques their key aspects.","['cs.CY', 'cs.AI']",2502.18904," Commit messages concisely describe code changes in natural language and are
important for software maintenance. Several approaches have been proposed to
automatically generate commit messages, but they still suffer from critical
limitations, such as time-consuming training and poor generalization ability.
To tackle these limitations, we propose to borrow the weapon of large language
models (LLMs) and in-context learning (ICL). Our intuition is based on the fact
that the training corpora of LLMs contain extensive code changes and their
pairwise commit messages, which makes LLMs capture the knowledge about commits,
while ICL can exploit the knowledge hidden in the LLMs and enable them to
perform downstream tasks without model tuning. However, it remains unclear how
well LLMs perform on commit message generation via ICL. In this paper, we
conduct an empirical study to investigate the capability of LLMs to generate
commit messages via ICL. Specifically, we first explore the impact of different
settings on the performance of ICL-based commit message generation. We then
compare ICL-based commit message generation with state-of-the-art approaches on
a popular multilingual dataset and a new dataset we created to mitigate
potential data leakage. The results show that ICL-based commit message
generation significantly outperforms state-of-the-art approaches on subjective
evaluation and achieves better generalization ability. We further analyze the
root causes for LLM's underperformance and propose several implications, which
shed light on future research directions for using LLMs to generate commit
messages.",['cs.SE'],False,,,,The Shady Light of Art Automation,"An Empirical Study on Commit Message Generation using LLMs via
  In-Context Learning"
neg-d2-799,2025-01-27,,2501.16011," Legal texts, characterized by complex and specialized terminology, present a
significant challenge for Language Models. Adding an underrepresented language,
such as Spanish, to the mix makes it even more challenging. While pre-trained
models like XLM-RoBERTa have shown capabilities in handling multilingual
corpora, their performance on domain specific documents remains underexplored.
This paper presents the development and evaluation of MEL, a legal language
model based on XLM-RoBERTa-large, fine-tuned on legal documents such as BOE
(Bolet\'in Oficial del Estado, the Spanish oficial report of laws) and congress
texts. We detail the data collection, processing, training, and evaluation
processes. Evaluation benchmarks show a significant improvement over baseline
models in understanding the legal Spanish language. We also present case
studies demonstrating the model's application to new legal texts, highlighting
its potential to perform top results over different NLP tasks.",['cs.CL'],2503.06795," Femoral artery access is essential for numerous clinical procedures,
including diagnostic angiography, therapeutic catheterization, and emergency
interventions. Despite its critical role, successful vascular access remains
challenging due to anatomical variability, overlying adipose tissue, and the
need for precise ultrasound (US) guidance. Errors in needle placement can lead
to severe complications, restricting the procedure to highly skilled clinicians
in controlled hospital settings. While robotic systems have shown promise in
addressing these challenges through autonomous scanning and vessel
reconstruction, clinical translation remains limited due to reliance on
simplified phantom models that fail to capture human anatomical complexity. In
this work, we present a method for autonomous robotic US scanning of bifurcated
femoral arteries, and validate it on five vascular phantoms created from real
patient computed tomography (CT) data. Additionally, we introduce a video-based
deep learning US segmentation network tailored for vascular imaging, enabling
improved 3D arterial reconstruction. The proposed network achieves a Dice score
of 89.21% and an Intersection over Union of 80.54% on a newly developed
vascular dataset. The quality of the reconstructed artery centerline is
evaluated against ground truth CT data, demonstrating an average L2 deviation
of 0.91+/-0.70 mm, with an average Hausdorff distance of 4.36+/-1.11mm. This
study is the first to validate an autonomous robotic system for US scanning of
the femoral artery on a diverse set of patient-specific phantoms, introducing a
more advanced framework for evaluating robotic performance in vascular imaging
and intervention.","['cs.RO', 'cs.CV']",False,,,,MEL: Legal Spanish Language Model,"Robotic Ultrasound-Guided Femoral Artery Reconstruction of
  Anatomically-Representative Phantoms"
neg-d2-800,2025-02-01,,2502.00447," One of the most often used methods of summing divergent series in physics is
the Borel-type summation with control parameters improving convergence, which
are defined by some optimization conditions. The well known annoying problem in
this procedure is the occurrence of multiple solutions for control parameters.
We suggest a method for resolving this problem, based on the minimization of
cost functional. Control parameters can be introduced by employing the
Borel-Leroy or Mittag-Leffler transforms. Also, two novel transformations are
proposed using fractional integrals and fractional derivatives. New cost
functionals are advanced, based on lasso and ridge selection criteria, and
their performance is studied for a number of models. The developed method is
shown to provide good accuracy for the calculated quantities.","['math-ph', 'cond-mat.stat-mech', 'hep-ph', 'math.MP']",2501.14716," The spectral theory on the $S$-spectrum originated to give quaternionic
quantum mechanics a precise mathematical foundation and as a spectral theory
for linear operators in vector analysis.
  This theory has proven to be significantly more general than initially
anticipated, naturally extending to fully Clifford operators and revealing
unexpected connections with the spectral theory based on the monogenic
spectrum, developed over forty years ago by A. McIntosh and collaborators.
  In recent years, we have combined slice hyperholomorphic functions with the
Fueter-Sce mapping theorem, also called Fueter-Sce extension theorem, to
broaden the class of functions and operators to which the theory can be
applied. This generalization has led to the definition of what we call the {\em
fine structures on the $S$-spectrum}, consisting of classes of functions that
admit an integral representation and their associated functional calculi.
  In this paper, we focus on the fine structures within the Clifford algebra
setting, particularly addressing polyharmonic functions, polyanalytic
functions, holomorphic Cliffordian functions and their associated functional
calculi defined via integral representation formulas.
  Moreover, we demonstrate that the monogenic functional calculus, defined via
the monogenic Cauchy formula, and the $F$-functional calculus of the fine
structures, defined via the Fueter-Sce mapping theorem in integral form, yield
the same operator.",['math.FA'],False,,,,"Resolving the Problem of Multiple Control Parameters in Optimized
  Borel-Type Summation","Functions and operators of the polyharmonic and polyanalytic Clifford
  fine structures on the $S$-spectrum"
neg-d2-801,2025-02-08,,2502.05544," The semiclassical Boltzmann equation is widely used to study transport
effects. It is usually introduced in an intuitive fashion, which could cause
confusion, e.g., over the collision integral with skew scattering. Actually,
the Boltzmann equation is closely linked to the quantum density matrix,
although term-by-term correspondence between the two is yet to be established.
Here we start from the quantum Liouville equation in the interactive picture
and show that the diagonal components of the equation yield the Boltzmann
equation in homogeneous systems in an applied uniform electric field in the
semiclassical limit, while the off-diagonal components give the anomalous
velocity induced by Berry curvature and the side-jump velocity. The
skew-scattering contribution is obtained when we include corrections beyond the
first-Born approximation. The result derived from the denstiy matrix agrees
with the semiclassical one from wave-packet analysis, showing that the
semiclassical Boltzmann equation is more than an equation built from intuition,
and it can be derived with the density matrix. Our work further clarifies the
origin of the equation and eliminates the puzzles surrounding it.",['cond-mat.mes-hall'],2501.05015," Adversarial attacks are allegedly unnoticeable. Prior studies have designed
attack noticeability measures on graphs, primarily using statistical tests to
compare the topology of original and (possibly) attacked graphs. However, we
observe two critical limitations in the existing measures. First, because the
measures rely on simple rules, attackers can readily enhance their attacks to
bypass them, reducing their attack ""noticeability"" and, yet, maintaining their
attack performance. Second, because the measures naively leverage global
statistics, such as degree distributions, they may entirely overlook attacks
until severe perturbations occur, letting the attacks be almost ""totally
unnoticeable."" To address the limitations, we introduce HideNSeek, a learnable
measure for graph attack noticeability. First, to mitigate the bypass problem,
HideNSeek learns to distinguish the original and (potential) attack edges using
a learnable edge scorer (LEO), which scores each edge on its likelihood of
being an attack. Second, to mitigate the overlooking problem, HideNSeek
conducts imbalance-aware aggregation of all the edge scores to obtain the final
noticeability score. Using six real-world graphs, we empirically demonstrate
that HideNSeek effectively alleviates the observed limitations, and LEO (i.e.,
our learnable edge scorer) outperforms eleven competitors in distinguishing
attack edges under five different attack methods. For an additional
application, we show that LEO boost the performance of robust GNNs by removing
attack-like edges.","['cs.LG', 'cs.AI']",False,,,,"Quantum kinetic theory of semiclassical Boltzmann equation with side
  jump and skew scattering","On Measuring Unnoticeability of Graph Adversarial Attacks: Observations,
  New Measure, and Applications"
neg-d2-802,2025-03-12,,2503.09358," Standardization of clinical reports is crucial for improving the quality of
healthcare and facilitating data integration. The lack of unified standards,
including format, terminology, and style, is a great challenge in clinical
fundus diagnostic reports, which increases the difficulty for large language
models (LLMs) to understand the data. To address this, we construct a bilingual
standard terminology, containing fundus clinical terms and commonly used
descriptions in clinical diagnosis. Then, we establish two models,
RetSTA-7B-Zero and RetSTA-7B. RetSTA-7B-Zero, fine-tuned on an augmented
dataset simulating clinical scenarios, demonstrates powerful standardization
behaviors. However, it encounters a challenge of limitation to cover a wider
range of diseases. To further enhance standardization performance, we build
RetSTA-7B, which integrates a substantial amount of standardized data generated
by RetSTA-7B-Zero along with corresponding English data, covering diverse
complex clinical scenarios and achieving report-level standardization for the
first time. Experimental results demonstrate that RetSTA-7B outperforms other
compared LLMs in bilingual standardization task, which validates its superior
performance and generalizability. The checkpoints are available at
https://github.com/AB-Story/RetSTA-7B.","['cs.CL', 'cs.AI']",2502.12047," In communication theory, attacks like eavesdropping or jamming are typically
assumed to occur at the channel level, while communication parties are expected
to follow established protocols. But what happens if one of the parties turns
malicious? In this work, we investigate a compelling scenario: a
multiple-access channel with two transmitters and one receiver, where one
transmitter deviates from the protocol and acts dishonestly. To address this
challenge, we introduce the Byzantine multiple-access classical-quantum channel
and derive an achievable communication rate for this adversarial setting.","['cs.IT', 'math.IT', 'math.QA']",False,,,,"RetSTA: An LLM-Based Approach for Standardizing Clinical Fundus Image
  Reports",Quantum Byzantine Multiple Access Channels
neg-d2-803,2025-03-21,,2503.17164," The gravitational deflection effect of relativistic massive and massless
particles up to the first post-Minkowskian order caused by a moving
Schwarzschild black hole with a two-dimensional equatorial velocity, which
contains the radial and transversal components, is studied analytically, and a
new unified formula for the deflection angle is achieved. The expression of the
angle matches well with the results of the weak deflection of relativistic
particles induced by a radially moving Schwarzschild source given in the
literature, when the transversal component of the lens velocity vanishes. The
joint velocity effect, which consists of the influences of the transversal and
radial motions of the lens on the leading-order Schwarzschild deflection of the
massive particles and light, is then discussed in the context of general
relativity. We analyze the order of magnitude of this kinematical effect and
evaluate the possibility of its astronomical detection subsequently.",['gr-qc'],2501.19382," In this paper, we propose a novel loop closure detection algorithm that uses
graph attention neural networks to encode semantic graphs to perform place
recognition and then use semantic registration to estimate the 6 DoF relative
pose constraint. Our place recognition algorithm has two key modules, namely, a
semantic graph encoder module and a graph comparison module. The semantic graph
encoder employs graph attention networks to efficiently encode spatial,
semantic and geometric information from the semantic graph of the input point
cloud. We then use self-attention mechanism in both node-embedding and
graph-embedding steps to create distinctive graph vectors. The graph vectors of
the current scan and a keyframe scan are then compared in the graph comparison
module to identify a possible loop closure. Specifically, employing the
difference of the two graph vectors showed a significant improvement in
performance, as shown in ablation studies. Lastly, we implemented a semantic
registration algorithm that takes in loop closure candidate scans and estimates
the relative 6 DoF pose constraint for the LiDAR SLAM system. Extensive
evaluation on public datasets shows that our model is more accurate and robust,
achieving 13% improvement in maximum F1 score on the SemanticKITTI dataset,
when compared to the baseline semantic graph algorithm. For the benefit of the
community, we open-source the complete implementation of our proposed algorithm
and custom implementation of semantic registration at
https://github.com/crepuscularlight/SemanticLoopClosure","['cs.CV', 'cs.RO']",False,,,,"Leading-order deflection of particles by a moving Schwarzschild lens
  with a two-dimensional velocity","LiDAR Loop Closure Detection using Semantic Graphs with Graph Attention
  Networks"
neg-d2-804,2025-01-29,,2501.17618," We develop an algorithm for bosonic path integral molecular dynamics (PIMD)
simulations with periodic boundary conditions (PBC) that scales quadratically
with the number of particles. Path integral methods are a powerful tool to
simulate bosonic condensed phases, which exhibit fundamental physical phenomena
such as Bose--Einstein condensation and superfluidity. Recently, we developed a
quadratic scaling algorithm for bosonic PIMD, but employed an ad hoc treatment
of PBC. Here we rigorously enforce PBC in bosonic PIMD. It requires summing
over the spring energies of all periodic images in the partition function, and
a naive implementation scales exponentially with the system size. We present an
algorithm for bosonic PIMD simulations of periodic systems that scales only
quadratically. We benchmark our implementation on the free Bose gas and a model
system of cold atoms in optical lattices. We also study an approximate
treatment of PBC based on the minimum-image convention, and derive a numerical
criterion to determine when it is valid.","['physics.chem-ph', 'cond-mat.quant-gas', 'cond-mat.stat-mech']",2503.09113," This paper presents a constraint-guided deep learning framework for
developing physically consistent health indicators in bearing prognostics and
health management. Conventional data-driven methods often lack physical
plausibility, while physics-based models are limited by incomplete system
knowledge. To address this, we integrate domain knowledge into deep learning
using constraints to enforce monotonicity, bound output values between 1 and 0
(representing healthy to failed states), and ensure consistency between signal
energy trends and health indicator estimates. This eliminates the need for
complex loss term balancing. We implement constraint-guided gradient descent
within an autoencoder architecture, creating a constrained autoencoder.
However, the framework is adaptable to other architectures. Using
time-frequency representations of accelerometer signals from the Pronostia
dataset, our constrained model generates smoother, more reliable degradation
profiles compared to conventional methods, aligning with expected physical
behavior. Performance is assessed using three metrics: trendability,
robustness, and consistency. Compared to a conventional baseline, the
constrained model improves all three. Another baseline, incorporating
monotonicity via a soft-ranking loss function, outperforms in trendability but
falls short in robustness and consistency. An ablation study confirms that the
monotonicity constraint enhances trendability, the boundary constraint ensures
consistency, and the energy-health consistency constraint improves robustness.
These findings highlight the effectiveness of constraint-guided deep learning
in producing reliable, physically meaningful health indicators, offering a
promising direction for future prognostic applications.","['cs.LG', 'cs.AI']",False,,,,"Periodic Boundary Conditions for Bosonic Path Integral Molecular
  Dynamics","Constraint-Guided Learning of Data-driven Health Indicator Models: An
  Application on the Pronostia Bearing Dataset"
neg-d2-805,2025-02-24,,2502.17101," The continuous effort in making artificial neural networks more alike to
human brain calls for the hardware elements to implement biological
synapse-like functionalities. The recent experimental demonstration of
ferroelectric-like FETs promises low-power operation as compared to the
conventional ferroelectric switching devices. This work presents an in-house
numerical tool, which self-consistently solves the electrostatics and
time-dependent electronic and ionic transport. The tool is exploited to analyze
the effect that various physical parameters such as mobility and ion
concentration could have on the design of the ferroelectric-like FETs. Their
suitability in emulating different functions of the biological synapses is also
demonstrated.","['cond-mat.mtrl-sci', 'physics.comp-ph']",2501.06079," In this work we deal with set-valued functions with values in the power set
of a separated locally convex space where a nontrivial pointed convex cone
induces a partial order relation. A set-valued function is evenly convex if its
epigraph is an evenly convex set, i.e., it is the intersection of an arbitrary
family of open half-spaces. In this paper we characterize evenly convex
set-valued functions as the pointwise supremum of its set-valued e-affine
minorants. Moreover, a suitable conjugation pattern will be developed for these
functions, as well as the counterpart of the biconjugation Fenchel-Moreau
theorem.",['math.OC'],False,,,,"Numerical study of synaptic behavior in amorphous HfO2-based
  ferroelectric-like FETs generated by voltage-driven ion migration",Set-valued evenly convex functions: characterizations and c-conjugacy
neg-d2-806,2025-01-21,,2501.14825," A search for a heavy pseudoscalar Higgs boson, A, decaying to a 125 GeV Higgs
boson h and a Z boson is presented. The h boson is identified via its decay to
a pair of tau leptons, while the Z boson is identified via its decay to a pair
of electrons or muons. The search targets the production of the A boson via the
gluon-gluon fusion process, gg $\to$ A, and in association with bottom quarks,
$\mathrm{b\bar{b}}$A. The analysis uses a data sample corresponding to an
integrated luminosity of 138 fb$^{-1}$ collected with the CMS detector at the
CERN LHC in proton-proton collisions at a centre-of-mass energy of $\sqrt{s}$ =
13 TeV. Constraints are set on the product of the cross sections of the A
production mechanisms and the A $\to$ Zh decay branching fraction. The observed
(expected) upper limit at 95% confidence level ranges from 0.049 (0.060) pb to
1.02 (0.79) pb for the gg $\to$ A process and from 0.053 (0.059) pb to 0.79
(0.61) pb for the $\text{b}\bar{\text{b}}$A process in the probed range of the
A boson mass, $m_\text{A}$, from 225 GeV to 1 TeV. The results of the search
are used to constrain parameters within the
${\text{M}_{\text{h,EFT}}^{\text{125}}}$ benchmark scenario of the minimal
supersymmetric extension of the standard model. Values of $\tan\beta$ below 2.2
are excluded in this scenario at 95% confidence level for all $m_\text{A}$
values in the range from 225 to 350 GeV.",['hep-ex'],2502.11843," Large Language Models (LLMs) are widely used as conversational agents,
exploiting their capabilities in various sectors such as education, law,
medicine, and more. However, LLMs are often subjected to context-shifting
behaviour, resulting in a lack of consistent and interpretable
personality-aligned interactions. Adherence to psychological traits lacks
comprehensive analysis, especially in the case of dyadic (pairwise)
conversations. We examine this challenge from two viewpoints, initially using
two conversation agents to generate a discourse on a certain topic with an
assigned personality from the OCEAN framework (Openness, Conscientiousness,
Extraversion, Agreeableness, and Neuroticism) as High/Low for each trait. This
is followed by using multiple judge agents to infer the original traits
assigned to explore prediction consistency, inter-model agreement, and
alignment with the assigned personality. Our findings indicate that while LLMs
can be guided toward personality-driven dialogue, their ability to maintain
personality traits varies significantly depending on the combination of models
and discourse settings. These inconsistencies emphasise the challenges in
achieving stable and interpretable personality-aligned interactions in LLMs.","['cs.CL', 'cs.AI', 'cs.SI']",False,,,,"Search for a heavy pseudoscalar Higgs boson decaying to a 125 GeV Higgs
  boson and a Z boson in final states with two tau and two light leptons in
  proton-proton collisions at $\sqrt{s}$ = 13 TeV",Can LLM Agents Maintain a Persona in Discourse?
neg-d2-807,2025-02-18,,2502.13005," Characterizing bipolaron binding, and understanding how it depends on
electron-phonon interaction, is crucial to unraveling the nature of emergent
many-body states in strongly interacting electron-phonon systems. So far, most
studies of bipolarons have been limited to the Holstein model, in which the
coupling constant is momentum-independent. The paradigmatic example of
momentum-dependent electron-phonon interaction comes from the system in which
phonon distortions modify electron hopping, the SSH model. Already individual
polarons in the SSH model are richer than the Holstein model counterparts, and
feature a phase transition into the finite momentum ground state with
increasing electron-phonon interaction. In this paper, we use a variational
approach to study bipolarons in the one-dimensional SSH model and discuss their
ground state, dispersion, and excitation spectra. We explore the full parameter
range of the system, including the adiabatic regime of slow phonons, which was
inaccessible to previous theoretical studies. In agreement with earlier
studies, we find that in the anti-adiabatic strongly interacting regime,
bipolarons have low effective mass. By contrast, in the adiabatic case, we find
that increasing electron-phonon interactions results in an exponential increase
of the bipolaron mass. We establish the existence of multiple branches of bound
excited states of SSH bipolaron and discuss the signatures of these bound
states in dynamics. We show that in the anti-adiabatic regime, response
functions obey a parity selection rule, that imposes symmetry constraints on
the excitation spectra and provides a clear signature of SSH bipolarons.",['cond-mat.str-el'],2503.08367," Occlusion is one of the fundamental challenges in crowd counting. In the
community, various data-driven approaches have been developed to address this
issue, yet their effectiveness is limited. This is mainly because most existing
crowd counting datasets on which the methods are trained are based on passive
cameras, restricting their ability to fully sense the environment. Recently,
embodied navigation methods have shown significant potential in precise object
detection in interactive scenes. These methods incorporate active camera
settings, holding promise in addressing the fundamental issues in crowd
counting. However, most existing methods are designed for indoor navigation,
showing unknown performance in analyzing complex object distribution in large
scale scenes, such as crowds. Besides, most existing embodied navigation
datasets are indoor scenes with limited scale and object quantity, preventing
them from being introduced into dense crowd analysis. Based on this, a novel
task, Embodied Crowd Counting (ECC), is proposed. We first build up an
interactive simulator, Embodied Crowd Counting Dataset (ECCD), which enables
large scale scenes and large object quantity. A prior probability distribution
that approximates realistic crowd distribution is introduced to generate
crowds. Then, a zero-shot navigation method (ZECC) is proposed. This method
contains a MLLM driven coarse-to-fine navigation mechanism, enabling active
Z-axis exploration, and a normal-line-based crowd distribution analysis method
for fine counting. Experimental results against baselines show that the
proposed method achieves the best trade-off between counting accuracy and
navigation cost.",['cs.CV'],False,,,,Bipolaron dynamics in the one-dimensional SSH model,Embodied Crowd Counting
neg-d2-808,2025-02-20,,2502.14754," In this paper, we present a simplified proof of Kharitonov's Theorem, an
important result on determining the Hurwitz stability of interval polynomials.
Our new approach to the proof, which is based on the Wronskian of a pair of
polynomials, is not only more elementary in comparison to known methods, but is
able to handle the degree drop case with ease.","['math.OC', 'math.CA', 'math.CV']",2503.10658," The limitations sections of scientific articles play a crucial role in
highlighting the boundaries and shortcomings of research, thereby guiding
future studies and improving research methods. Analyzing these limitations
benefits researchers, reviewers, funding agencies, and the broader academic
community. We introduce LimTopic, a strategy where Topic generation in
Limitation sections in scientific articles with Large Language Models (LLMs).
Here, each topic contains the title and Topic Summary. This study focuses on
effectively extracting and understanding these limitations through topic
modeling and text summarization, utilizing the capabilities of LLMs. We
extracted limitations from research articles and applied an LLM-based topic
modeling integrated with the BERtopic approach to generate a title for each
topic and Topic Sentences. To enhance comprehension and accessibility, we
employed LLM-based text summarization to create concise and generalizable
summaries for each topic Topic Sentences and produce a Topic Summary. Our
experimentation involved prompt engineering, fine-tuning LLM and BERTopic, and
integrating BERTopic with LLM to generate topics, titles, and a topic summary.
We also experimented with various LLMs with BERTopic for topic modeling and
various LLMs for text summarization tasks. Our results showed that the
combination of BERTopic and GPT 4 performed the best in terms of silhouette and
coherence scores in topic modeling, and the GPT4 summary outperformed other LLM
tasks as a text summarizer.","['cs.CL', 'cs.LG']",False,,,,Kharitonov's Theorem with Degree Drop: a Wronskian Approach,"LimTopic: LLM-based Topic Modeling and Text Summarization for Analyzing
  Scientific Articles limitations"
neg-d2-809,2025-02-13,,2502.09003," Supervised fine-tuning is a standard method for adapting pre-trained large
language models (LLMs) to downstream tasks. Quantization has been recently
studied as a post-training technique for efficient LLM deployment. To obtain
quantized fine-tuned LLMs, conventional pipelines would first fine-tune the
pre-trained models, followed by post-training quantization. This often yields
suboptimal performance as it fails to leverage the synergy between fine-tuning
and quantization. To effectively realize low-bit quantization of weights,
activations, and KV caches in LLMs, we propose an algorithm named Rotated
Straight-Through-Estimator (RoSTE), which combines quantization-aware
supervised fine-tuning (QA-SFT) with an adaptive rotation strategy that
identifies an effective rotation configuration to reduce activation outliers.
We provide theoretical insights on RoSTE by analyzing its prediction error when
applied to an overparameterized least square quantized training problem. Our
findings reveal that the prediction error is directly proportional to the
quantization error of the converged weights, which can be effectively managed
through an optimized rotation configuration. Experiments on Pythia, Qwen and
Llama models of different sizes demonstrate the effectiveness of RoSTE.
Compared to existing post-SFT quantization baselines, our method consistently
achieves superior performances across various tasks and different LLM
architectures.","['cs.LG', 'cs.AI']",2501.06891," A remarkable feature of dark matter consisting of ultralight bosonic
particles is the emergence of superfluid Bose-Einstein condensate structures on
galactic scales. We investigate the oscillations of the solitonic dark matter
structure in the central galactic region by numerically solving the
Bogoliubov-de Gennes problem, accounting for perturbations in the gravitational
potential and local self-interactions. Our findings reveal that the central
solitonic core, formed by the balance of gravitational attraction, quantum
pressure, and repulsive interactions, exhibits significant oscillatory
behaviour. These oscillations, characterized by distinct eigenmodes, provide
insights into the dynamical properties of solitonic dark matter structures and
their observational implications and contributions to galactic structure
formation and evolution.","['astro-ph.CO', 'astro-ph.GA', 'nlin.PS']",False,,,,"RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach
  for Large Language Models",Oscillations of Solitonic Galactic Cores in Ultralight Dark Matter
neg-d2-810,2025-01-10,,2501.05797," Time crystals represent a non-equilibrium state of matter with broken
time-translation symmetry that repeats itself at regular time intervals. Though
initially envisioned as a self-generated and self-sustained periodic motion,
their realization has usually required the utilization of external periodic
inputs or modulations. While at first it looked like, for a time crystal to
exist, the initial proposal had to be abandoned, the recent evidence of
inherent time crystals is bringing back the idea of self-generated time crystal
under the spotlight. In this work, we demonstrate the appearance of a
self-generated space-time crystalline order in hybrid Josephson junctions with
the ferromagnet interface without any external influence. The presence of the
exchange and the Dzyaloshinskii-Moriya interactions in a ferromagnet with
broken structural inversion symmetry modifies the current phase relation and
the critical current due to the coupling between the magnetic moment and
Josephson phase. This breaks the time translation symmetry leading to the
appearance of the time-crystalline order in the spatiotemporal dependence of
superconducting current, which evolves with the double of the modulation
frequency. Due to its unique origin and properties, this inherent time
crystalline order stands out from the commonly known classification of time
crystals into discrete and continuous ones. A self-generated time crystal is
demonstrated in two types of hybrid Josephson junctions: the
superconductor-ferromagnet-superconductor on a topological insulator and the
superconductor-three layer ferromagnet-superconductor. Further, we also show
that a recently developed magnetometry device that visualizes a supercurrent
flow in the Josephson junction at the nanoscale can be used as a platform for
experimental detection of space-time crystalline order in hybrid Josephson
junctions.",['cond-mat.supr-con'],2501.0429," Although the star formation process has been studied for decades, many
important aspects of the physics involved remain unsolved. Recent advancement
of instrumentation in the infrared, far-infrared and sub-millimetre wavelength
regimes have contributed to a significantly improved understanding of processes
in the interstellar medium (ISM) leading to star formation. The future of
research on the ISM and star formation looks exciting with instruments like the
JWST, ALMA, etc., already contributing to the topic by gathering
high-resolution high-sensitivity data and with several larger ground- and
space-bound facilities either being planned or constructed. India has a sizable
number of astronomers engaged in research on topics related to the ISM and star
formation. In this white paper invited by the Astronomical Society of India to
prepare a vision document for Indian astronomy, we review the Indian
contributions to the global understanding of the star formation process and
suggest areas that require focused efforts both in creating observing
facilities and in theoretical front in India, in order to improve the impact of
our research in the coming decades.",['astro-ph.GA'],False,,,,Self-generated time crystal in hybrid Josephson junctions,"Research on the Interstellar Medium and Star Formation in the Galaxy: An
  Indian Perspective"
neg-d2-811,2025-02-07,,2502.05366," Multivariate kernel density estimations have received much spate of interest.
In addition to conventional methods of (non-)classical associated-kernels for
(un)bounded densities and bandwidth selections, the multiple extended-beta
kernel (MEBK) estimators with Bayesian adaptive bandwidths are invested to gain
a deeper and better insight into the estimation of multivariate density
functions. Being unimodal, the univariate extended-beta smoother has an
adaptable compact support which is suitable for each dataset, always limited.
The support of the density MBEK estimator can be known or estimated by extreme
values. Thus, asymptotical properties for the (non-)normalized estimators are
established. Explicit and general choices of bandwidths using the flexible
Bayesian adaptive method are provided. Behavioural analyses, specifically
undertaken on the sensitive edges of the estimator support, are studied and
compared to Gaussian and gamma kernel estimators. Finally, simulation studies
and three applications on original and usual real-data sets of the proposed
method yielded very interesting advantages with respect to its flexibility as
well as its universality.","['math.ST', 'stat.TH']",2503.13388," In this work, we develop a novel mathematical framework for universal digital
quantum computation using algebraic probability theory. We rigorously define
quantum circuits as finite sequences of elementary quantum gates and establish
their role in implementing unitary transformations. A key result demonstrates
that every unitary matrix in \(\mathrm{U}(N)\) can be expressed as a product of
elementary quantum gates, leading to the concept of a universal dictionary for
quantum computation. We apply this framework to the construction of quantum
circuits that encode probability distributions, focusing on the Grover-Rudolph
algorithm. By leveraging controlled quantum gates and rotation matrices, we
design a quantum circuit that approximates a given probability density
function. Numerical simulations, conducted using Qiskit, confirm the
theoretical predictions and validate the effectiveness of our approach. These
results provide a rigorous foundation for quantum circuit synthesis within an
algebraic probability framework and offer new insights into the encoding of
probability distributions in quantum algorithms. Potential applications include
quantum machine learning, circuit optimization, and experimental
implementations on real quantum hardware.","['quant-ph', 'cs.NA', 'math.NA']",False,,,,"An effective estimation of multivariate density functions using
  extended-beta kernels with Bayesian adaptive bandwidths","A mathematical model for a universal digital quantum computer with an
  application to the Grover-Rudolph algorithm"
neg-d2-812,2025-01-08,,2501.04675," Chart interpretation is crucial for visual data analysis, but accurately
extracting information from charts poses significant challenges for automated
models. This study investigates the fine-tuning of DEPLOT, a modality
conversion module that translates the image of a plot or chart to a linearized
table, on a custom dataset of 50,000 bar charts. The dataset comprises simple,
stacked, and grouped bar charts, targeting the unique structural features of
these visualizations. The finetuned DEPLOT model is evaluated against its base
version using a test set of 1,000 images and two metrics: Relative Mapping
Similarity (RMS), which measures categorical mapping accuracy, and Relative
Number Set Similarity (RNSS), which evaluates numerical interpretation
accuracy. To further explore the reasoning capabilities of large language
models (LLMs), we curate an additional set of 100 bar chart images paired with
question answer sets. Our findings demonstrate that providing a structured
intermediate table alongside the image significantly enhances LLM reasoning
performance compared to direct image queries.","['cs.CL', 'cs.AI', 'cs.CV', 'cs.LG']",2501.17337," In this paper, we establish the global H\""older gradient estimate for
solutions to the Dirichlet problem of the Monge-Amp\`ere equation $\det D^2u =
f$ on strictly convex but not uniformly convex domain $\Omega$.",['math.AP'],False,,,,"Enhancing Financial VQA in Vision Language Models using Intermediate
  Structured Representations","Global $C^{1,\alpha}$ regularity for Monge-Amp\`ere equations on planar
  convex domains"
neg-d2-813,2025-03-11,,2503.08367," Occlusion is one of the fundamental challenges in crowd counting. In the
community, various data-driven approaches have been developed to address this
issue, yet their effectiveness is limited. This is mainly because most existing
crowd counting datasets on which the methods are trained are based on passive
cameras, restricting their ability to fully sense the environment. Recently,
embodied navigation methods have shown significant potential in precise object
detection in interactive scenes. These methods incorporate active camera
settings, holding promise in addressing the fundamental issues in crowd
counting. However, most existing methods are designed for indoor navigation,
showing unknown performance in analyzing complex object distribution in large
scale scenes, such as crowds. Besides, most existing embodied navigation
datasets are indoor scenes with limited scale and object quantity, preventing
them from being introduced into dense crowd analysis. Based on this, a novel
task, Embodied Crowd Counting (ECC), is proposed. We first build up an
interactive simulator, Embodied Crowd Counting Dataset (ECCD), which enables
large scale scenes and large object quantity. A prior probability distribution
that approximates realistic crowd distribution is introduced to generate
crowds. Then, a zero-shot navigation method (ZECC) is proposed. This method
contains a MLLM driven coarse-to-fine navigation mechanism, enabling active
Z-axis exploration, and a normal-line-based crowd distribution analysis method
for fine counting. Experimental results against baselines show that the
proposed method achieves the best trade-off between counting accuracy and
navigation cost.",['cs.CV'],2501.11137," The transformation to equivalent dimensions that offers a novel approach for
investigating earthquake clustering was engaged to analyze the preparatory
phase of the 2020 Samos, Greece, Mw7.0 main shock. The analysis considered
earthquakes that occurred between 2006 and October 2020, covering an area
extended three times the length of the main rupture. Each earthquake was
parameterized by its magnitude, the interevent time (interval since the
previous earthquake), and the interevent spatial distance (distance between the
epicenters of consecutive earthquakes). Transforming these parameters into
equivalent dimensions allowed them to be directly compared. The degree of
clustering was quantified using the average distance between earthquakes in
this transformed parameter space, calculated within consecutive 100 events data
windows. Results revealed a distinct pattern, the average distance was
increasing steadily during the twelve year period before the main shock. These
temporal changes in the average distance were driven by a systematic evolution
of earthquake clustering in the used parameter space. Beginning from a
two-cluster system, when the distance was minimal, the clustering development
continued along two branches and ended before the main shock with the formation
of five earthquake clusters of different characteristics.",['physics.geo-ph'],False,,,,Embodied Crowd Counting,"Clustering indications before the Mw7.0 2020 Samos, Greece, main shock
  as revealed in an equivalent dimensions space"
neg-d2-814,2025-03-10,,2503.07031," Real quantum systems can exhibit a local object called local partial density
of states (LPDOS) that cannot be proved within the axiomatic approach of
quantum mechanics. We demonstrate that real mesoscopic system that can exhibit
Fano resonances will show this object and also very counterintuitively it can
become negative, resulting in the enhancement of coherent currents.","['quant-ph', 'cond-mat.dis-nn']",2501.08222," We consider the spatial classification problem for monitoring using data
collected by a coordinated team of mobile robots. Such classification problems
arise in several applications including search-and-rescue and precision
agriculture. Specifically, we want to classify the regions of a search
environment into interesting and uninteresting as quickly as possible using a
team of mobile sensors and mobile charging stations. We develop a data-driven
strategy that accommodates the noise in sensed data and the limited energy
capacity of the sensors, and generates collision-free motion plans for the
team. We propose a bi-level approach, where a high-level planner leverages a
multi-armed bandit framework to determine the potential regions of interest for
the drones to visit next based on the data collected online. Then, a low-level
path planner based on integer programming coordinates the paths for the team to
visit the target regions subject to the physical constraints. We characterize
several theoretical properties of the proposed approach, including anytime
guarantees and task completion time. We show the efficacy of our approach in
simulation, and further validate these observations in physical experiments
using mobile robots.",['cs.RO'],False,,,,Negative Local Partial Density of States,"Data-driven Spatial Classification using Multi-Arm Bandits for
  Monitoring with Energy-Constrained Mobile Robots"
neg-d2-815,2025-01-30,,2501.18525," We study vorticity production in isothermal, subsonic, acoustic
(nonvortical), decaying turbulence due to the presence of magnetic fields.
Using three-dimensional numerical simulations, we always find that the
resulting turbulent kinetic energy cascade follows the ordinary Kolmogorov
phenomenology involving a constant spectral energy flux. For acoustic
turbulence, the corresponding nondimensional prefactor is larger than the
standard Kolmogorov constant due to an inefficiency in dissipating kinetic
energy. We find that the Lorentz force can not only drive the direct production
of vortical motions, but it can also facilitate the conversion of acoustic
energy into vortical energy. This conversion is shown to be quadratic in the
magnetic field strength and linear in the acoustic flow speed. By contrast, the
direct production of vortical motions by the magnetic field is linear in the
field strength. Our results suggest that magnetic fields play a crucial role in
vorticity production in cosmological flows, particularly in scenarios where
significant acoustic turbulence is prevalent. We also discuss the implications
of our findings for the early universe, where magnetic fields may convert
acoustic turbulence generated during cosmological phase transitions into
vortical turbulence.","['physics.flu-dyn', 'astro-ph.CO']",2501.1825," Efficient channel state information (CSI) compression is crucial in frequency
division duplexing (FDD) massive multiple-input multiple-output (MIMO) systems
due to excessive feedback overhead. Recently, deep learning-based compression
techniques have demonstrated superior performance across various data types,
including CSI. However, these approaches often experience performance
degradation when the data distribution changes due to their limited
generalization capabilities. To address this challenge, we propose a model
fine-tuning approach for CSI feedback in massive MIMO systems. The idea is to
fine-tune the encoder/decoder network models in a dynamic fashion using the
recent CSI samples. First, we explore encoder-only fine-tuning, where only the
encoder parameters are updated, leaving the decoder and latent parameters
unchanged. Next, we consider full-model fine-tuning, where the encoder and
decoder models are jointly updated. Unlike encoder-only fine-tuning, full-model
fine-tuning requires the updated decoder and latent parameters to be
transmitted to the decoder side. To efficiently handle this, we propose
different prior distributions for model updates, such as uniform and truncated
Gaussian to entropy code them together with the compressed CSI and account for
additional feedback overhead imposed by conveying the model updates. Moreover,
we incorporate quantized model updates during fine-tuning to reflect the impact
of quantization in the deployment phase. Our results demonstrate that
full-model fine-tuning significantly enhances the rate-distortion (RD)
performance of neural CSI compression. Furthermore, we analyze how often the
full-model fine-tuning should be applied in a new wireless environment and
identify an optimal period interval for achieving the best RD trade-off.","['cs.IT', 'eess.SP', 'math.IT']",False,,,,"Magnetically-assisted vorticity production in decaying acoustic
  turbulence",Dynamic Model Fine-Tuning For Extreme MIMO CSI Compression
neg-d2-816,2025-01-21,,2501.12432," Although current Large Language Models (LLMs) exhibit impressive
capabilities, performing complex real-world tasks still requires tool learning.
Mainstream methods, such as CoT/ReAct, rely on step-by-step tool invocation to
interact with external environments, but they are limited in perceptual scope
and lack adequate task-planning capability. To address these limitations, other
studies introduce the first Search-based Decision Tree (DFSDT), which still
suffers from the high computational cost. In this paper, we introduce a novel
parallel tool invocation paradigm, DTA-Llama (Divide-Then-Aggregate Llama).
First, we transform traditional tree-based tool search paths into Directed
Acyclic Graph (DAG) structure, generating a high-quality parallel tool
invocation dataset. The DTA-Llama is then trained on the dataset to learn to
iteratively divide the current task into several parallel tool invocation
sub-tasks and aggregate the invocation results to decide the next actions.
Furthermore, we introduce an efficient inference framework inspired by the
Process/Threads mechanism when applying the DTA-Llama to practical tasks.
Experimental results show that our approach substantially enhances task
performance while reducing token consumption and inference time. Llama2-7B,
using our method, is comparable to the official parallel function calling
method of GPT-3.5. The relevant code, dataset, and model weights are available
at https://corn0205.github.io/","['cs.LG', 'cs.AI', 'cs.CL']",2503.12301," Large Language Models (LLMs) have made significant strides in generating
human-like responses, largely due to preference alignment techniques. However,
these methods often assume unbiased human feedback, which is rarely the case in
real-world scenarios. This paper introduces Content-Aware Noise-Resilient
Preference Optimization (CNRPO), a novel framework that addresses multiple
sources of content-dependent noise in preference learning. CNRPO employs a
multi-objective optimization approach to separate true preferences from
content-aware noises, effectively mitigating their impact. We leverage backdoor
attack mechanisms to efficiently learn and control various noise sources within
a single model. Theoretical analysis and extensive experiments on different
synthetic noisy datasets demonstrate that CNRPO significantly improves
alignment with primary human preferences while controlling for secondary noises
and biases, such as response length and harmfulness.","['cs.LG', 'cs.CL']",False,,,,"Divide-Then-Aggregate: An Efficient Tool Learning Method via Parallel
  Tool Invocation","One Goal, Many Challenges: Robust Preference Optimization Amid
  Content-Aware and Multi-Source Noise"
neg-d2-817,2025-01-22,,2501.13336," Adversarial training and adversarial purification are two effective and
practical defense methods to enhance a model's robustness against adversarial
attacks. However, adversarial training necessitates additional training, while
adversarial purification suffers from low time efficiency. More critically,
current defenses are designed under the perturbation-based adversarial threat
model, which is ineffective against the recently proposed unrestricted
adversarial attacks. In this paper, we propose an effective and efficient
adversarial defense method that counters both perturbation-based and
unrestricted adversarial attacks. Our defense is inspired by the observation
that adversarial attacks are typically located near the decision boundary and
are sensitive to pixel changes. To address this, we introduce adversarial
anti-aliasing to mitigate adversarial modifications. Additionally, we propose
adversarial super-resolution, which leverages prior knowledge from clean
datasets to benignly recover images. These approaches do not require additional
training and are computationally efficient without calculating gradients.
Extensive experiments against both perturbation-based and unrestricted
adversarial attacks demonstrate that our defense method outperforms
state-of-the-art adversarial purification methods.","['cs.CV', 'eess.IV']",2503.05469," We identify the size of the largest connected component in a subcritical
inhomogeneous random graph with a kernel of preferential attachment type. The
component is polynomial in the graph size with an explicitly given exponent,
which is strictly larger than the exponent for the largest degree in the graph.
This is in stark contrast to the behaviour of inhomogeneous random graphs with
a kernel of rank one. Our proof uses local approximation by branching random
walks going well beyond the weak local limit and novel results on subcritical
killed branching random walks.","['math.PR', 'math.CO']",False,,,,Gradient-Free Adversarial Purification with Diffusion Models,"The largest subcritical component in inhomogeneous random graphs of
  preferential attachment type"
neg-d2-818,2025-02-04,,2502.02839," The Laser Interferometer Space Antenna (LISA) will soon detect gravitational
waves (GWs) emitted by massive black hole (MBH) mergers. Some theoretical
models have predicted transient electromagnetic (EM) emission from these
mergers, enabling the association of LISA GW sources with their EM counterparts
via telescope follow-up. However, the number of unrelated EM transients that
might contaminate telescope searches for the true transient counterparts of
LISA MBH mergers is unknown. We investigate the expected numbers of unrelated
EM transients that will coincide with simulated LISA localization volumes of
MBH mergers, as a function of the merger total mass and redshift. We find that
the number of potential contaminants in LISA localization volumes drops to
unity for mergers at $z \lesssim 0.8$ and at 1 hour before coalescence. After
coalescence, the parameter space corresponding to a maximum of one potential
contaminant expands to $z \lesssim 1.5$. In contrast, if the redshifts for all
transients detected in LISA sky localization regions are not available, the
number of potential contaminants increases by an average factor of $\sim100$,
and never drops below unity. Overall, we expect the average number of
contaminating transients in telescope follow-up of LISA MBH mergers to be
non-negligible, especially without redshift information for the detected
transients. We recommend that endeavors designing follow-up strategies of LISA
events should focus on: (1) building large redshift catalogs for host galaxies,
(2) developing robust real-time transient classification algorithms, (3) and
coordinating telescope resources to obtain redshifts for candidate transient EM
counterparts in a timely manner.","['astro-ph.GA', 'astro-ph.HE']",2503.15879," Non-factoid question-answering (NFQA) poses a significant challenge due to
its open-ended nature, diverse intents, and the need for multi-aspect
reasoning, which renders conventional factoid QA approaches, including
retrieval-augmented generation (RAG), inadequate. Unlike factoid questions,
non-factoid questions (NFQs) lack definitive answers and require synthesizing
information from multiple sources across various reasoning dimensions. To
address these limitations, we introduce Typed-RAG, a type-aware multi-aspect
decomposition framework within the RAG paradigm for NFQA. Typed-RAG classifies
NFQs into distinct types -- such as debate, experience, and comparison -- and
applies aspect-based decomposition to refine retrieval and generation
strategies. By decomposing multi-aspect NFQs into single-aspect sub-queries and
aggregating the results, Typed-RAG generates more informative and contextually
relevant responses. To evaluate Typed-RAG, we introduce Wiki-NFQA, a benchmark
dataset covering diverse NFQ types. Experimental results demonstrate that
Typed-RAG outperforms baselines, thereby highlighting the importance of
type-aware decomposition for effective retrieval and generation in NFQA. Our
code and dataset are available at https://github.com/TeamNLP/Typed-RAG.","['cs.CL', 'cs.IR']",False,,,,"Contaminating Electromagnetic Transients in LISA Gravitational Wave
  Localization Volumes. I: The Intrinsic Rates","Typed-RAG: Type-aware Multi-Aspect Decomposition for Non-Factoid
  Question Answering"
neg-d2-819,2025-02-03,,2502.01335," While traditional self-supervised learning methods improve performance and
robustness across various medical tasks, they rely on single-vector embeddings
that may not capture fine-grained concepts such as anatomical structures or
organs. The ability to identify such concepts and their characteristics without
supervision has the potential to improve pre-training methods, and enable novel
applications such as fine-grained image retrieval and concept-based outlier
detection. In this paper, we introduce ConceptVAE, a novel pre-training
framework that detects and disentangles fine-grained concepts from their style
characteristics in a self-supervised manner. We present a suite of loss terms
and model architecture primitives designed to discretise input data into a
preset number of concepts along with their local style. We validate ConceptVAE
both qualitatively and quantitatively, demonstrating its ability to detect
fine-grained anatomical structures such as blood pools and septum walls from 2D
cardiac echocardiographies. Quantitatively, ConceptVAE outperforms traditional
self-supervised methods in tasks such as region-based instance retrieval,
semantic segmentation, out-of-distribution detection, and object detection.
Additionally, we explore the generation of in-distribution synthetic data that
maintains the same concepts as the training data but with distinct styles,
highlighting its potential for more calibrated data generation. Overall, our
study introduces and validates a promising new pre-training technique based on
concept-style disentanglement, opening multiple avenues for developing models
for medical image analysis that are more interpretable and explainable than
black-box approaches.",['cs.CV'],2501.05859," In this paper, we introduce a large model-empowered streaming semantic
communication system for speech transmission across various languages, named
LSSC-ST. Specifically, we devise an edge-device collaborative semantic
communication architecture by offloading the intricate semantic extraction and
channel coding modules to edge servers, thereby reducing the computational
burden on local devices. To support multilingual speech transmission,
pre-trained large speech models are utilized to learn unified semantic features
from speech in different languages, breaking the constraint of a single input
language and enhancing the practicality of the LSSC-ST. Moreover, the input
speech is sequentially streamed into the developed system as short speech
segments, which enables low transmission latency without degrading the quality
of the produced speech. A novel dynamic speech segmentation algorithm is
proposed to further reduce the transmission latency by adaptively adjusting the
duration of speech segments. According to simulation results, the LSSC-ST
provides more accurate speech transmission and achieves a streaming manner with
lower latency compared to the existing non-streaming semantic communication
systems.",['eess.AS'],False,,,,"ConceptVAE: Self-Supervised Fine-Grained Concept Disentanglement from 2D
  Echocardiographies",Large Model Empowered Streaming Speech Semantic Communications
neg-d2-820,2025-01-20,,2501.11758," Distracted driving remains a significant global challenge with severe human
and economic repercussions, demanding improved detection and intervention
strategies. While previous studies have extensively explored single-modality
approaches, recent research indicates that these systems often fall short in
identifying complex distraction patterns, particularly cognitive distractions.
This systematic review addresses critical gaps by providing a comprehensive
analysis of machine learning (ML) and deep learning (DL) techniques applied
across various data modalities - visual,, sensory, auditory, and multimodal. By
categorizing and evaluating studies based on modality, data accessibility, and
methodology, this review clarifies which approaches yield the highest accuracy
and are best suited for specific distracted driving detection goals. The
findings offer clear guidance on the advantages of multimodal versus
single-modal systems and capture the latest advancements in the field.
Ultimately, this review contributes valuable insights for developing robust
distracted driving detection frameworks, supporting enhanced road safety and
mitigation strategies.","['cs.CV', 'stat.ML']",2502.08051," We study the screening mass of the neutral rho-meson in the presence of
strong magnetic fields using the Kroll-Lee-Zumino (KLZ) model. The rho-meson
self-energy is computed at one-loop order within the lowest Landau level (LLL)
approximation, considering the magnetic field as the dominant energy scale. Due
to Lorentz symmetry breaking induced by the external field, we decompose the
self-energy into three independent tensor structures, which give rise to three
distinct modes. Additionally, the four-momentum splits into parallel and
perpendicular components, leading to two types of screening masses: the
parallel screening mass ( $p_0=0$ and $p_\perp \to 0$ ) and the perpendicular
screening mass ( $p_0=0$ and $p_\parallel \to 0$ ). Our results show that the
zero and perpendicular modes exhibit a monotonically increasing behavior with
the magnetic field strength, whereas the parallel mode remains essentially
constant. These findings provide new insights into the behavior of vector
mesons in strongly magnetized media, with implications for QCD under extreme
conditions.","['hep-ph', 'hep-th']",False,,,,"A Review Paper of the Effects of Distinct Modalities and ML Techniques
  to Distracted Driving Detection",Screening rho-meson mass in the presence of strong magnetic fields
neg-d2-821,2025-03-07,,2503.0592," Recent advancements in large language models have intensified the need for
efficient and deployable models within limited inference budgets. Structured
pruning pipelines have shown promise in token efficiency compared to training
target-size models from scratch. In this paper, we advocate incorporating
enlarged model pretraining, which is often ignored in previous works, into
pruning. We study the enlarge-and-prune pipeline as an integrated system to
address two critical questions: whether it is worth pretraining an enlarged
model even when the model is never deployed, and how to optimize the entire
pipeline for better pruned models. We propose an integrated enlarge-and-prune
pipeline, which combines enlarge model training, pruning, and recovery under a
single cosine annealing learning rate schedule. This approach is further
complemented by a novel iterative structured pruning method for gradual
parameter removal. The proposed method helps to mitigate the knowledge loss
caused by the rising learning rate in naive enlarge-and-prune pipelines and
enable effective redistribution of model capacity among surviving neurons,
facilitating smooth compression and enhanced performance. We conduct
comprehensive experiments on compressing 2.8B models to 1.3B with up to 2T
tokens in pretraining. It demonstrates the integrated approach not only
provides insights into the token efficiency of enlarged model pretraining but
also achieves superior performance of pruned models.","['cs.CL', 'cs.AI', 'cs.LG']",2501.17746," In vehicle-to-everything (V2X) applications, roadside units (RSUs) can be
tasked with both sensing and communication functions to enable sensing-assisted
communications. Recent studies have demonstrated that distance, angle, and
velocity information obtained through sensing can be leveraged to reduce the
overhead associated with communication beam tracking. In this work, we extend
this concept to scenarios involving multiple distributed RSUs and distributed
MIMO (multiple-input multiple-output) systems. We derive the state evolution
model, formulate the extended Kalman-filter equations, and implement predictive
beamforming for distributed MIMO. Simulation results indicate that, when
compared with a co-located massive MIMO antenna array, distributed antennas
lead to more uniform and robust sensing performance, coverage, and data rates,
while the vehicular user is in motion.","['eess.SP', 'cs.IT', 'math.IT']",False,,,,"IDEA Prune: An Integrated Enlarge-and-Prune Pipeline in Generative
  Language Model Pretraining",Predictive Beamforming with Distributed MIMO
neg-d2-822,2025-01-23,,2502.10411," Personalised education is one of the domains that can greatly benefit from
the most recent advances in Artificial Intelligence (AI) and Large Language
Models (LLM). However, it is also one of the most challenging applications due
to the cognitive complexity of teaching effectively while personalising the
learning experience to suit independent learners. We hypothesise that one
promising approach to excelling in such demanding use cases is using a
\emph{society of minds}. In this chapter, we present TrueReason, an exemplar
personalised learning system that integrates a multitude of specialised AI
models that can mimic micro skills that are composed together by a LLM to
operationalise planning and reasoning. The architecture of the initial
prototype is presented while describing two micro skills that have been
incorporated in the prototype. The proposed system demonstrates the first step
in building sophisticated AI systems that can take up very complex cognitive
tasks that are demanded by domains such as education.","['cs.CY', 'cs.AI', 'cs.CL', 'cs.IR', 'cs.MA']",2503.07087," The development of a generalist agent with adaptive multiple manipulation
skills has been a long-standing goal in the robotics community. In this paper,
we explore a crucial task, skill-incremental learning, in robotic manipulation,
which is to endow the robots with the ability to learn new manipulation skills
based on the previous learned knowledge without re-training. First, we build a
skill-incremental environment based on the RLBench benchmark, and explore how
traditional incremental methods perform in this setting. We find that they
suffer from severe catastrophic forgetting due to the previous methods on
classification overlooking the characteristics of temporality and action
complexity in robotic manipulation tasks. Towards this end, we propose an
incremental Manip}ulation framework, termed iManip, to mitigate the above
issues. We firstly design a temporal replay strategy to maintain the integrity
of old skills when learning new skill. Moreover, we propose the extendable
PerceiverIO, consisting of an action prompt with extendable weight to adapt to
new action primitives in new skill. Extensive experiments show that our
framework performs well in Skill-Incremental Learning. Codes of the
skill-incremental environment with our framework will be open-source.",['cs.RO'],False,,,,"TrueReason: An Exemplar Personalised Learning System Integrating
  Reasoning with Foundational Models",iManip: Skill-Incremental Learning for Robotic Manipulation
neg-d2-823,2025-01-17,,2501.10573," We investigate the relationship between the geometry of token embeddings and
their role in the next token prediction within transformer models. An important
aspect of this connection uses the notion of empirical measure, which encodes
the distribution of token point clouds across transformer layers and drives the
evolution of token representations in the mean-field interacting picture. We
use metrics such as intrinsic dimension, neighborhood overlap, and cosine
similarity to observationally probe these empirical measures across layers. To
validate our approach, we compare these metrics to a dataset where the tokens
are shuffled, which disrupts the syntactic and semantic structure. Our findings
reveal a correlation between the geometric properties of token embeddings and
the cross-entropy loss of next token predictions, implying that prompts with
higher loss values have tokens represented in higher-dimensional spaces.","['cs.CL', 'cs.LG']",2502.11337," Machine learning applications in high-stakes scenarios should always operate
under human oversight. Developing an optimal combination of human and machine
intelligence requires an understanding of their complementarities, particularly
regarding the similarities and differences in the way they make mistakes. We
perform extensive experiments in the area of face recognition and compare two
automated face recognition systems against human annotators through a
demographically balanced user study. Our research uncovers important ways in
which machine learning errors and human errors differ from each other, and
suggests potential strategies in which human-machine collaboration can improve
accuracy in face recognition.","['cs.HC', 'cs.CV', 'cs.CY']",False,,,,"The Geometry of Tokens in Internal Representations of Large Language
  Models",A Comparison of Human and Machine Learning Errors in Face Recognition
neg-d2-824,2025-03-07,,2503.0603," Computed tomography (CT) is extensively used for accurate visualization and
segmentation of organs and lesions. While deep learning models such as
convolutional neural networks (CNNs) and vision transformers (ViTs) have
significantly improved CT image analysis, their performance often declines when
applied to diverse, real-world clinical data. Although foundation models offer
a broader and more adaptable solution, their potential is limited due to the
challenge of obtaining large-scale, voxel-level annotations for medical images.
In response to these challenges, prompting-based models using visual or text
prompts have emerged. Visual-prompting methods, such as the Segment Anything
Model (SAM), still require significant manual input and can introduce ambiguity
when applied to clinical scenarios. Instead, foundation models that use text
prompts offer a more versatile and clinically relevant approach. Notably,
current text-prompt models, such as the CLIP-Driven Universal Model, are
limited to text prompts already encountered during training and struggle to
process the complex and diverse scenarios of real-world clinical applications.
Instead of fine-tuning models trained from natural imaging, we propose
OpenVocabCT, a vision-language model pretrained on large-scale 3D CT images for
universal text-driven segmentation. Using the large-scale CT-RATE dataset, we
decompose the diagnostic reports into fine-grained, organ-level descriptions
using large language models for multi-granular contrastive learning. We
evaluate our OpenVocabCT on downstream segmentation tasks across nine public
datasets for organ and tumor segmentation, demonstrating the superior
performance of our model compared to existing methods. All code, datasets, and
models will be publicly released at https://github.com/ricklisz/OpenVocabCT.","['cs.CV', 'cs.AI']",2501.13068," The interconnection between the human lungs and other organs, such as the
liver and kidneys, is crucial for understanding the underlying risks and
effects of lung diseases and improving patient care. However, most research
chest CT imaging is focused solely on the lungs due to considerations of cost
and radiation dose. This restricted field of view (FOV) in the acquired images
poses challenges to comprehensive analysis and hinders the ability to gain
insights into the impact of lung diseases on other organs. To address this, we
propose SCOPE (Spatial Coverage Optimization with Prior Encoding), a novel
approach to capture the inter-organ relationships from CT images and extend the
FOV of chest CT images. Our approach first trains a variational autoencoder
(VAE) to encode 2D axial CT slices individually, then stacks the latent
representations of the VAE to form a 3D context for training a latent diffusion
model. Once trained, our approach extends the FOV of CT images in the
z-direction by generating new axial slices in a zero-shot manner. We evaluated
our approach on the National Lung Screening Trial (NLST) dataset, and results
suggest that it effectively extends the FOV to include the liver and kidneys,
which are not completely covered in the original NLST data acquisition.
Quantitative results on a held-out whole-body dataset demonstrate that the
generated slices exhibit high fidelity with acquired data, achieving an SSIM of
0.81.","['cs.CV', 'eess.IV']",False,,,,Towards Universal Text-driven CT Image Segmentation,"Beyond the Lungs: Extending the Field of View in Chest CT with Latent
  Diffusion Models"
neg-d2-825,2025-01-06,,2501.03137," We investigate the problem of synthesizing distributionally robust control
policies for stochastic systems under safety and reach-avoid specifications.
Using a game-theoretical framework, we consider the setting where the
probability distribution of the disturbance at each time step is selected from
an ambiguity set defined by the Wasserstein distance. The goal is to synthesize
a distributionally robust control policy that ensures the satisfaction
probability exceeds a specified threshold under any distribution within the
ambiguity set. First, for both safety and reach-avoid specifications, we
establish the existence of optimal policies by leveraging the dynamic
programming principles. Then we demonstrate how the associated optimization
problem can be efficiently solved using the dual representation of Wasserstein
distributionally robust optimization. Furthermore, for safety specifications in
particular, we introduce a novel concept of distributionally robust control
barrier certificates and show how these enable the efficient synthesis of
controllers through sum-of-squares programming techniques. Finally, our
experimental results reveal that incorporating distributional robustness during
the synthesis phase significantly improves the satisfaction probability during
online execution, even with limited statistical knowledge of the disturbance
distribution.","['eess.SY', 'cs.SY']",2502.10171," We prove area estimates for stable capillary $cmc$ (minimal) hypersurfaces
$\Sigma$ with nonpositive Yamabe invariant that are properly immersed in a
Riemannian $n$-dimensional manifold $M$ with scalar curvature $R^M$ and mean
curvature of the boundary $H^{\partial M}$ bounded from below. We also prove a
local rigidity result in the case $\Sigma$ is embedded and
$\mathcal{J}$-energy-minimizing. In this case, we show that $M$ locally splits
along $\Sigma$ and is isometric to $(-\varepsilon,\varepsilon)\times \Sigma,
dt^2 + e^{-2Ht}g)$, where $g$ is Einstein, or Ricci flat, $H\geq 0$ and
$\partial\Sigma$ is totally geodesic.",['math.DG'],False,,,,"Distributionally Robust Control Synthesis for Stochastic Systems with
  Safety and Reach-Avoid Specifications","Area estimates for capillary cmc hypersurfaces with nonpositive Yamabe
  invariant"
neg-d2-826,2025-02-21,,2502.15504," In this paper, we show a more concise and high level proof than the original
one, derived by researcher Bart Jacobs, for the following theorem: in the
context of Bayesian update rules for learning or updating internal states that
produce predictions, the relative entropy between the observations and the
predictions is reduced when applying Jeffrey's update rule to update the
internal state.","['stat.ML', 'cs.CR']",2503.14963," Multimodal contrastive learning (MCL) advances in aligning different
modalities and generating multimodal representations in a joint space. By
leveraging contrastive learning across diverse modalities, large-scale
multimodal data enhances representational quality. However, a critical yet
often overlooked challenge remains: multimodal data is rarely collected in a
single process, and training from scratch is computationally expensive.
Instead, emergent multimodal data can be used to optimize existing models
gradually, \textit{i.e.}, models are trained on a sequence of modality pair
data. We define this problem as Continual Multimodal Contrastive Learning
(CMCL), an underexplored yet crucial research direction at the intersection of
multimodal and continual learning. In this paper, we formulate CMCL through two
specialized principles of stability and plasticity. We theoretically derive a
novel optimization-based method, which projects updated gradients from dual
sides onto subspaces where any gradient is prevented from interfering with the
previously learned knowledge. Two upper bounds provide theoretical insights on
both stability and plasticity in our solution. Beyond our theoretical
contributions, we conduct experiments on multiple datasets by comparing our
method against advanced continual learning baselines. The empirical results
further support our claims and demonstrate the efficacy of our method. The code
will be publicly available.",['cs.LG'],False,,,,Jeffrey's update rule as a minimizer of Kullback-Leibler divergence,Continual Multimodal Contrastive Learning
neg-d2-827,2025-02-18,,2502.13205," We perform a numerical study of non-local partonic transport in anisotropic
QCD matter, relevant to the evolution of hard probes in the aftermath of
high-energy nuclear scattering events. The recently derived master equation,
obtained from QFT considerations, differs from Boltzmann transport by
incorporating a non-local elastic scattering kernel arising from density
gradients. After rewriting the master equation in a form suitable for numerical
implementation and assuming a static density profile, we compare the non-local
evolution to Boltzmann transport, demonstrating that the new interaction kernel
is essential for accurately describing the azimuthal structure of the
final-state momentum distribution. We further study the non-local partonic
transport in the case of a matter profile governed by two-dimensional
hydrodynamics, accounting for its flow and generalizing the evolution equation.
Our results demonstrate the necessity of going beyond classical transport at
high-$p_t$ to accurately capture the structure of jets propagating through
structured QCD matter. The master equation used in the numerical simulations
can be seamlessly integrated into state-of-the-art transport codes.","['nucl-th', 'hep-ph']",2502.035," Recent advances in generative image restoration (IR) have demonstrated
impressive results. However, these methods are hindered by their substantial
size and computational demands, rendering them unsuitable for deployment on
edge devices. This work introduces ELIR, an Efficient Latent Image Restoration
method. ELIR operates in latent space by first predicting the latent
representation of the minimum mean square error (MMSE) estimator and then
transporting this estimate to high-quality images using a latent consistency
flow-based model. Consequently, ELIR is more than 4x faster compared to the
state-of-the-art diffusion and flow-based approaches. Moreover, ELIR is also
more than 4x smaller, making it well-suited for deployment on
resource-constrained edge devices. Comprehensive evaluations of various image
restoration tasks show that ELIR achieves competitive results, effectively
balancing distortion and perceptual quality metrics while offering improved
efficiency in terms of memory and computation.","['eess.IV', 'cs.AI', 'stat.AP']",False,,,,Non-local high-$p_t$ transport in anisotropic QCD matter,Efficient Image Restoration via Latent Consistency Flow Matching
neg-d2-828,2025-03-20,,2503.16243," The recent Einstein Probe (EP) event EP240414a exhibits several unusual
observational features. Its prompt and afterglow emissions place it between
long gamma-ray bursts (LGRBs) and low-luminosity GRBs (LLGRBs). The event is
followed by a fast optical transient (AT~2024gsa), initially exhibiting a
thermal-like spectrum but later evolving into an unusually red peak at $\sim
3-5$ days, which is difficult to explain with thermal emission. Using our
generalized analytic framework for jet propagation in a circumstellar material
(CSM; Hamidani et al. 2025), we explore a scenario in which a conventional LGRB
jet is launched in a progenitor surrounded by a dense CSM. For a CSM of $\sim
0.03 M_\odot$ extending to $\sim 3\times 10^{13}$ cm, we find that the jet is
significantly weakened before breaking out, becoming ``barely failed'', an
intermediate state between successful (LGRB) and completely failed (LLGRB)
jets. This scenario naturally explains EP240414a's multi-wavelength
observations, with the early thermal component produced by cocoon cooling
emission, and the red peak explained by non-thermal afterglow emission from the
mildly relativistic barely failed jet (and its inner cocoon). Our work
demonstrates the important role of extended CSM in shaping GRB jets and
illustrates how early multi-wavelength follow-up observations can reveal the
physically diverse nature of jet-driven transients.",['astro-ph.HE'],2501.18876," Developing machine learning protocols for molecular simulations requires
comprehensive and efficient datasets. Here we introduce the QMe14S dataset,
comprising 186,102 small organic molecules featuring 14 elements (H, B, C, N,
O, F, Al, Si, P, S, Cl, As, Se, Br) and 47 functional groups. Using density
functional theory at the B3LYP/TZVP level, we optimized the geometries and
calculated properties including energy, atomic charge, atomic force, dipole
moment, quadrupole moment, polarizability, octupole moment, first
hyperpolarizability, and Hessian. At the same level, we obtained the harmonic
IR, Raman and NMR spectra. Furthermore, we conducted ab initio molecular
dynamics simulations to generate dynamic configurations and extract
nonequilibrium properties, including energy, forces, and Hessians. By
leveraging our E(3)-equivariant message-passing neural network (DetaNet), we
demonstrated that models trained on QMe14S outperform those trained on the
previously developed QM9S dataset in simulating molecular spectra. The QMe14S
dataset thus serves as a comprehensive benchmark for molecular simulations,
offering valuable insights into structure-property relationships.","['physics.chem-ph', 'cs.LG']",False,,,,"EP240414a: A Gamma-Ray Burst Jet Weakened by an Extended Circumstellar
  Material","QMe14S, A Comprehensive and Efficient Spectral Dataset for Small Organic
  Molecules"
neg-d2-829,2025-01-14,,2501.08222," We consider the spatial classification problem for monitoring using data
collected by a coordinated team of mobile robots. Such classification problems
arise in several applications including search-and-rescue and precision
agriculture. Specifically, we want to classify the regions of a search
environment into interesting and uninteresting as quickly as possible using a
team of mobile sensors and mobile charging stations. We develop a data-driven
strategy that accommodates the noise in sensed data and the limited energy
capacity of the sensors, and generates collision-free motion plans for the
team. We propose a bi-level approach, where a high-level planner leverages a
multi-armed bandit framework to determine the potential regions of interest for
the drones to visit next based on the data collected online. Then, a low-level
path planner based on integer programming coordinates the paths for the team to
visit the target regions subject to the physical constraints. We characterize
several theoretical properties of the proposed approach, including anytime
guarantees and task completion time. We show the efficacy of our approach in
simulation, and further validate these observations in physical experiments
using mobile robots.",['cs.RO'],2502.19303," Advancements in differential pumping and electron optics over the past few
decades have enabled x-ray photoelectron spectroscopy (XPS) measurements at
(near-)ambient pressures, bridging the pressure gap for characterizing
realistic sample chemistries. Recently, we demonstrated the capabilities of an
ambient pressure XPS (APXPS) setup for in-situ plasma environment measurements,
allowing plasma-surface interactions to be studied in operando rather than
using the traditional before-and-after analysis approach. This new plasma-XPS
technique facilitates the identification of reaction intermediates critical for
understanding plasma-assisted surface processes relevant to semiconductor
nanomanufacturing, such as physical vapor deposition, etching, atomic layer
deposition, etc. In this report, we apply the plasma-XPS approach to monitor
real-time surface chemical changes on a model Ag(111) single crystal exposed to
oxidizing and reducing plasmas. We correlate surface-sensitive data with
concurrent gas-phase XPS measurements and residual gas mass-spectra analysis of
species generated during plasma exposure, highlighting the significant role of
plasma-induced chamber wall reactions. Ultimately, we demonstrate that
plasma-XPS provides comprehensive insights into both surface and gas-phase
chemistry, establishing it as a versatile and dynamic characterization tool
with broad applications in microelectronics research.","['physics.plasm-ph', 'cond-mat.mtrl-sci']",False,,,,"Data-driven Spatial Classification using Multi-Arm Bandits for
  Monitoring with Energy-Constrained Mobile Robots",Operando XPS in Reactive Plasmas: The Importance of The Wall Reactions
neg-d2-830,2025-01-20,,2501.11676," We present results for the cosmic non-linear density-fluctuation power
spectrum based on the analytical formalism developed in [1] which allows us to
study cosmic structure formation based on Newtonian particle dynamics in
phase-space. This framework provides a field-theory approach to a perturbative
solution of the BBGKY-hierarchy where the resulting loop-expansion of the
theory introduces a natural truncation criterion. We show that we are able to
reproduce structure growth on large scales $k \leq 0.2
\mathrm{h}\,\mathrm{Mpc}^{-1}$ to very high precision while on small and
intermediate scales we find deviations of the order of $10\%$ from current
numerical simulations. The results strongly suggest that a significant
improvement may be achieved by restructuring the perturbation theory.","['astro-ph.CO', 'cond-mat.stat-mech']",2502.0995," Duminil-Copin and Manolescu (2022) recently proved the scaling relations for
planar Fortuin-Kasteleyn (FK) percolation. In particular, they showed that the
one-arm exponent and the mixing rate exponent are sufficient to derive the
other near-critical exponents. The scaling limit of critical FK percolation is
conjectured to be a conformally invariant random collection of loops called the
conformal loop ensemble (CLE). In this paper, we define the CLE analog of the
mixing rate exponent. Assuming the convergence of FK percolation to CLE, we
show that the mixing rate exponent for FK percolation agrees with that of CLE.
We prove that the CLE$_\kappa$ mixing rate exponent equals $\frac{3
\kappa}{8}-1$, thereby answering Question 3 of Duminil-Copin and Manolescu
(2022). The derivation of the CLE exponent is based on an exact formula for the
Radon-Nikodym derivative between the marginal laws of the odd-level and
even-level CLE loops, which is obtained from the coupling between Liouville
quantum gravity and CLE.","['math.PR', 'math-ph', 'math.MP']",False,,,,Cosmic Large-Scale Structure Formation from Newtonian Particle Dynamics,Mixing rate exponent of planar Fortuin-Kasteleyn percolation
neg-d2-831,2025-03-02,,2503.00941," Next-generation mobile networks are set to utilize integrated sensing and
communication (ISAC) as a critical technology, providing significant support
for sectors like the industrial Internet of Things (IIoT), extended reality
(XR), and smart home applications. A key challenge in ISAC implementation is
the extraction of sensing parameters from radio signals, a task that
conventional methods struggle to achieve due to the complexity of acquiring
sensing channel data. In this paper, we introduce a novel auto-encoder
(AE)-based framework to acquire sensing information using channel state
information (CSI). Specifically, our framework, termed C2S (CSI to sensing)-AE,
learns the relationship between CSI and the delay power spectrum (DPS), from
which the range information can be readily accessed. To validate our
framework's performance, we conducted measurements of DPS and CSI in real-world
scenarios and introduced the dataset 'SHU7'. Our extensive experiments
demonstrate that the framework excels in C2S extrapolation, surpassing existing
methods in terms of accuracy for both delay and signal strength of individual
paths. This innovative approach holds the potential to greatly enhance sensing
capabilities in future mobile networks, paving the way for more robust and
versatile ISAC applications.",['eess.SP'],2501.16412," Neutrino masses may have evolved dynamically throughout the history of the
Universe, potentially leading to a mass spectrum distinct from the normal or
inverted ordering observed today. While cosmological measurements constrain the
total energy density of neutrinos, they are not directly sensitive to a
dynamically changing mass ordering unless future surveys achieve exceptional
precision in detecting the distinct imprints of each mass eigenstate on
large-scale structures. In this work, we investigate the impact of a dynamic
neutrino mass spectrum on the diffuse supernova neutrino background (DSNB),
which is composed of neutrinos from all supernova explosions throughout cosmic
history and is on the verge of experimental detection. Since neutrino
oscillations are highly sensitive to the mass spectrum, we show that the
electron neutrino survival probability carries distinct signatures of the
evolving neutrino mass spectrum. Our results indicate that the resulting
modifications to the DSNB spectrum would exhibit unique energy-dependent
features. These features are distinguishable from the effects of significant
astrophysical uncertainties, providing a potential avenue for probing the
dynamic nature of neutrino masses.",['hep-ph'],False,,,,C2S-AE: CSI to Sensing enabled by an Auto-Encoder-based Framework,"Dynamic Neutrino Mass Ordering and Its Imprint on the Diffuse Supernova
  Neutrino Background"
neg-d2-832,2025-03-20,,2503.16603," The weak gravity conjecture has been invoked to conjecture that the
dimensions of charged operators in a CFT should obey a superadditivity relation
(sometimes referred to as convexity). In this paper, we study superadditivity
of the operator spectrum in theories expanded about the semi-classical saddle
point that dominates correlators of large charge operators. We explore this in
two contexts. The first is a model with two scalar fields that carry different
charges, at a non-trivial Wilson-Fisher fixed point. A careful analysis of the
semi-classics for this two field model demonstrates that 'quantum' violations
of superadditivity (those not forbidden by the conjecture) persist in the large
charge regime. We then turn to study the general properties of CFTs at large
charge as bottom-up EFTs. By a trial and error procedure we come up with a
seemingly consistent family of examples violating the conjecture. In so doing
the presence of a genuine dilaton field appears necessary. On the one hand our
result demonstrates that the superadditivity conjecture cannot be proven purely
on the basis of a bottom-up analysis. On the other hand, the need for a
dilaton, with the corresponding infinite fine tuning, indicates the
conjecture-violating EFTs are unlikely to be UV completable.",['hep-th'],2502.08388," In this paper, we study the shadow and observational image of the Kerr-like
Loop Quantum Gravity (LQG) inspired black bounce with the help of the celestial
light source and the thin disk source by employing the backward ray-tracing
method. The results indicate that both the LQG parameter alpha and the rotation
parameter a contribute to a reduction in the shadow size; however, the
influence of a is predominant, while the effect of alpha circular orbit. One
can find that the correlation parameter (a, alpha), along with the observer's
inclination angle, affect the image's asymmetry and the distortion of the inner
shadow. As the inclination increases, the direct and lensed images diverge,
creating a structure resembling a hat. Meanwhile, we also investigate the
redshift distribution of the direct lensed images of the accretion disk under
different parameters and observation angle. The results show that the
distribution of redshift and observed intensity is obviously related to the
behavior of accretion flow. These results may provide a potential approach to
limit black hole parameters, detect quantum gravity effects, and distinguish
the LQG black hole from other black hole models.",['gr-qc'],False,,,,Superadditivity at Large Charge,"The shadow and accretion disk images of the rotation loop quantum black
  bounce"
neg-d2-833,2025-03-19,,2503.14893," Life cycle assessment (LCA) is a methodology for holistically measuring the
environmental impact of a product from initial manufacturing to end-of-life
disposal. However, the extent to which LCA informs the design of computing
devices remains unclear. To understand how this information is collected and
applied, we interviewed 17 industry professionals with experience in LCA or
electronics design, systematically coded the interviews, and investigated
common themes. These themes highlight the challenge of LCA data collection and
reveal distributed decision-making processes where responsibility for
sustainable design choices, and their associated costs, is often ambiguous. Our
analysis identifies opportunities for HCI technologies to support LCA
computation and its integration into the design process to facilitate
sustainability-oriented decision-making. While this work provides a nuanced
discussion about sustainable design in the information and communication
technologies (ICT) hardware industry, we hope our insights will also be
valuable to other sectors.",['cs.HC'],2503.00593," On electron kinetic scales, ions and electrons decouple, and electron
velocity shear on electron inertial length $\sim d_e$ can trigger
electromagnetic (EM) electron Kelvin-Helmholtz instability (EKHI). In this
paper, we present an analytic study of EM EKHI in an inviscid collisionless
plasma with a step-function electron shear flow. We show that in incompressible
collisionless plasma the ideal electron frozen-in condition $\mathbf{E} +
\mathbf{v}_e \times \mathbf{B}/c = 0$ must be broken for the EM EKHI to occur.
In a step-function electron shear flow, the ideal electron frozen-in condition
is replaced by magnetic flux conservation, i.e., $\nabla \times (\mathbf{E} +
\mathbf{v}_e\times \mathbf{B}/c) = 0$, resulting in a dispersion relation
similar to that of the standard ideal and incompressible magnetohydrodynamics
KHI. The magnetic field parallel to the electron streaming suppresses the EM
EKHI due to magnetic tension. The threshold for the EM mode of the EKHI is
$(\mathbf{k}\cdot\Delta\mathbf{U}_e)^2>\frac{n_{e1}+n_{e2}}{n_{e1}
n_{e2}}[n_{e1}(\mathbf{v}_{Ae1}\cdot\mathbf{k})^2+n_{e2}(\mathbf{v}_{Ae2}\cdot\mathbf{k})^2]$,
where $\mathbf{v}_{Ae} =\mathbf{B}/(4\pi m_e n_e)^{1/2}$, $\Delta\mathbf{U}_e$
and $n_e$ are the electron streaming velocity shear and densities,
respectively. The growth rate of the EM mode is $\gamma_{em} \sim \Omega_{ce}$,
the electron gyro-frequency.","['astro-ph.SR', 'astro-ph.HE', 'physics.plasm-ph']",False,,,,"Incorporating Sustainability in Electronics Design: Obstacles and
  Opportunities",Electromagnetic Electron Kelvin-Helmholtz Instability
neg-d2-834,2025-02-19,,2502.14097," We study the existence and qualitative properties of action ground-states
(that is, bound-states with minimal action) {of the nonlinear Schr\""odinger
equation} over single-knot metric graphs -- which are made of half-lines, loops
and pendants, all connected at a single vertex. First, we prove existence of
action ground-state for generic single-knot graphs, even in the absence of an
associated variational problem. Second, for regular single-knot graphs of
length $\ell$, we perform a complete analysis of positive monotone
bound-states. Furthermore, we characterize all positive bound-states when
$\ell$ is small and prove some symmetry-breaking results for large $\ell$.
Finally, we apply the results to some particular graphs to illustrate the
complex relation between action ground-states and the topological {and metric}
features of the underlying metric graph.
  The proofs are nonvariational, using a careful phase-plane analysis, the
study of sections of period functions, asymptotic estimates and blowup
arguments. We show, in particular, how nonvariational techniques are
complementary to variational ones in order to deeply understand bound-states of
the nonlinear Schr\""odinger equation on metric graphs.","['math.AP', 'math.CA']",2502.10794," Multimodal Large Language Models (MLLMs) bridge the gap between visual and
textual data, enabling a range of advanced applications. However, complex
internal interactions among visual elements and their alignment with text can
introduce vulnerabilities, which may be exploited to bypass safety mechanisms.
To address this, we analyze the relationship between image content and task and
find that the complexity of subimages, rather than their content, is key.
Building on this insight, we propose the Distraction Hypothesis, followed by a
novel framework called Contrasting Subimage Distraction Jailbreaking (CS-DJ),
to achieve jailbreaking by disrupting MLLMs alignment through multi-level
distraction strategies. CS-DJ consists of two components: structured
distraction, achieved through query decomposition that induces a distributional
shift by fragmenting harmful prompts into sub-queries, and visual-enhanced
distraction, realized by constructing contrasting subimages to disrupt the
interactions among visual elements within the model. This dual strategy
disperses the model's attention, reducing its ability to detect and mitigate
harmful content. Extensive experiments across five representative scenarios and
four popular closed-source MLLMs, including GPT-4o-mini, GPT-4o, GPT-4V, and
Gemini-1.5-Flash, demonstrate that CS-DJ achieves average success rates of
52.40% for the attack success rate and 74.10% for the ensemble attack success
rate. These results reveal the potential of distraction-based approaches to
exploit and bypass MLLMs' defenses, offering new insights for attack
strategies.",['cs.CV'],False,,,,"A comprehensive study of bound-states for the nonlinear Schr\""odinger
  equation on single-knot metric graphs","Distraction is All You Need for Multimodal Large Language Model
  Jailbreaking"
neg-d2-835,2025-01-30,,2501.18689," The first direct detection of gravitational waves in 2015 marked the
beginning of a new era for the study of compact objects. Upcoming detectors,
such as the Einstein Telescope, are expected to add thousands of binary
coalescences to the list. However, from a theoretical perspective, our
understanding of compact objects is hindered by many uncertainties, and a
comprehensive study of the nature of stellar remnants from core-collapse
supernovae is still lacking. In this work, we investigate the properties of
stellar remnants using a homogeneous grid of rotating and non-rotating massive
stars at various metallicities from Limongi and Chieffi 2018. We simulate the
supernova explosion of the evolved progenitors using the HYdrodynamic Ppm
Explosion with Radiation diffusION (HYPERION) code (Limongi and Chieffi 2020),
assuming a thermal bomb model calibrated to match the main properties of
SN1987A. We find that the heaviest black hole that can form depends on the
initial stellar rotation, metallicity, and the assumed criterion for the onset
of pulsational pair-instability supernovae. Non-rotating progenitors at
$\big[\rm Fe/H \big]=-3$ can form black holes up to $\sim 87 M_\odot$, falling
within the theorized pair-instability mass gap. Conversely, enhanced wind mass
loss prevents the formation of BHs more massive than $\sim 41.6 M_\odot$ from
rotating progenitors. We use our results to study the black hole mass
distribution from a population of $10^6$ isolated massive stars following a
Kroupa initial mass function. Finally, we provide fitting formulas to compute
the mass of compact remnants as a function of stellar progenitor properties.
Our up-to-date prescriptions can be easily implemented in rapid population
synthesis codes.","['astro-ph.HE', 'astro-ph.SR']",2502.18043," We analyze (p,t) two-neutron transfer reactions in a semi-microscopic model.
The overlap integrals of the target nucleus are calculated in a microscopic
cluster model. The Resonating Group Method (RGM) assumes a cluster structure of
the nucleus, and is well adapted to halo nuclei since the long-range part of
the wave function is accurately described. We focus on (p,t) reactions
involving 6He and 11Li, which are well known core+n+n halo nuclei. The RGM is
based on a nucleon-nucleon interaction, and therefore does not involve any
fitting procedure. It also provides overlap integrals of excited states of the
core nucleus. We present overlap integrals and spectroscopic factors of 6He and
11Li. We compute the 6He(p,t)alpha and 11Li(p,t)9Li cross sections at the DWBA,
and compare them with experiments. For 11Li we also determine the 11Li(p,t)9Li*
cross section which involves the first excited states of 9Li. A fair agreement
with experiment is obtained, considering that no parameter is adjusted.",['nucl-th'],False,,,,The initial mass-remnant mass relation for core collapse supernovae,"Microscopic study of halo nuclei through (p,t) reactions"
neg-d2-836,2025-02-11,,2502.0804," We consider ways to construct a transducer for a given set of input word to
output symbol pairs. This is motivated by the need for representing game
playing programs in a low-level mathematical format that can be analyzed by
algebraic tools. This is different from the classical applications of finite
state automata, thus the usual optimization techniques are not directly
applicable. Therefore, we use relational programming tools to find minimal
transducers realizing a given set of input-output pairs.",['cs.FL'],2503.04572," Declines in vaccination coverage for vaccine-preventable diseases, such as
measles and chickenpox, have enabled their surprising comebacks and pose
significant public health challenges in the wake of growing vaccine hesitancy.
Vaccine opt-outs and refusals are often fueled by beliefs concerning
perceptions of vaccine effectiveness and exaggerated risks. Here, we quantify
the impact of competing beliefs -- vaccine-averse versus vaccine-neutral -- on
social imitation dynamics of vaccination, alongside the epidemiological
dynamics of disease transmission. These beliefs may be pre-existing and fixed,
or coevolving attitudes. This interplay among beliefs, behaviors, and disease
dynamics demonstrates that individuals are not perfectly rational; rather, they
base their vaccine uptake decisions on beliefs, personal experiences, and
social influences. We find that the presence of a small proportion of fixed
vaccine-averse beliefs can significantly exacerbate the vaccination dilemma,
making the tipping point in the hysteresis loop more sensitive to changes in
individuals' perceived costs of vaccination and vaccine effectiveness. However,
in scenarios where competing beliefs spread concurrently with vaccination
behavior, their double-edged impact can lead to self-correction and alignment
between vaccine beliefs and behaviors. The results show that coevolution of
vaccine beliefs and behaviors makes populations more sensitive to abrupt
changes in perceptions of vaccine cost and effectiveness compared to scenarios
without beliefs. Our work provides valuable insights into harnessing the social
contagion of even vaccine-neutral attitudes to overcome vaccine hesitancy.","['physics.soc-ph', 'q-bio.PE']",False,,,,On Constructing Finite Automata by Relational Programming,"Social Imitation Dynamics of Vaccination Driven by Vaccine Effectiveness
  and Beliefs"
neg-d2-837,2025-01-14,,2501.0854," The task of building semantics for structured data such as CSV, JSON, and XML
files is highly relevant in the knowledge representation field. Even though we
have a vast of structured data on the internet, mapping them to domain
ontologies to build semantics for them is still very challenging as it requires
the construction model to understand and learn graph-structured knowledge.
Otherwise, the task will require human beings' effort and cost. In this paper,
we proposed a novel automatic semantic modeling framework: Knowledge Prompt
Chaining. It can serialize the graph-structured knowledge and inject it into
the LLMs properly in a Prompt Chaining architecture. Through this knowledge
injection and prompting chaining, the model in our framework can learn the
structure information and latent space of the graph and generate the semantic
labels and semantic graphs following the chains' insturction naturally. Based
on experimental results, our method achieves better performance than existing
leading techniques, despite using reduced structured input data.","['cs.CL', 'cs.AI', 'cs.DB']",2503.13892," The formation mechanisms of open cluster (OCs) groups remain unclear due to
limited sample sizes and data precision. Recent advancements in Gaia
astrometric data provide an unprecedented opportunity to study OC groups in
greater detail. This study aims to extend the sample of OC groups and
investigate their formation and evolution mechanisms, with a focus on the role
of stellar feedback in triggering star formation. We identify four new OC
groups based on Gaia data, whose member OCs are spatially proximate and
kinematically coherent. Their age spreads are consistent with the timescale of
continuous star formation, suggesting that their member OCs formed sequentially
from the same molecular cloud. N-body simulation results further reveal that
these groups will gradually disperse, evolving into independent OCs. By
analyzing the correlation between OC ages and their separation from potential
SN explosion sites, we predict SN explosion regions around the birthplaces of
OC groups. The strong correlation between OC ages and predicted SN explosion
sites supports a supernova-triggered star formation scenario. Additionally, we
trace pulsar (PSR) orbits to examine their association with these regions. We
detected three PSRs near Group 1 and 26 PSRs near Group 2, whose birthplaces
align with the predicted SN explosions regions. The presence of PSRs associated
with OC groups provides additional observational evidence for SN explosions in
this region, further supporting a supernova-triggered star formation scenario
for G1 and G2. We propose that multiple SN explosions in a short period
triggered the formation of Group 1 and Group 2, reinforcing the hierarchical
star formation model. These results highlight the multi-scale interactions
driving star and OC formation and provide new insights into the role of stellar
feedback in shaping OC groups.",['astro-ph.GA'],False,,,,Knowledge prompt chaining for semantic modeling,"Formation and evolution of new primordial open cluster groups:
  Feedback-driven star formation"
neg-d2-838,2025-02-16,,2502.11144," In Genome-Wide Association Studies (GWAS), heritability is defined as the
fraction of variance of an outcome explained by a large number of genetic
predictors in a high-dimensional polygenic linear model. This work studies the
asymptotic properties of the most common estimator of heritability from summary
statistics called linkage disequilibrium score (LDSC) regression, together with
a simpler and closely related estimator called GWAS heritability (GWASH). These
estimators are analyzed in their basic versions and under various modifications
used in practice including weighting and standardization. We show that, with
some variations, two conditions which we call weak dependence (WD) and
bounded-kurtosis effects (BKE) are sufficient for consistency of both the basic
LDSC with fixed intercept and GWASH estimators, for both Gaussian and
non-Gaussian predictors. For Gaussian predictors it is shown that these
conditions are also necessary for consistency of GWASH (with truncation) and
simulations suggest that necessity holds too when the predictors are
non-Gaussian. We also show that, with properly truncated weights, weighting
does not change the consistency results, but standardization of the predictors
and outcome, as done in practice, introduces bias in both LDSC and GWASH if the
two essential conditions are violated. Finally, we show that, when population
stratification is present, all the estimators considered are biased, and the
bias is not remedied by using the LDSC regression estimator with free
intercept, as originally suggested by the authors of that estimator.","['math.ST', 'stat.TH']",2502.05549," The problem ""A general characterization of uniqueness polynomial for
non-critically injective polynomials"" has been remained open since the last two
decades. In this paper, we explore this open problem. To this end, we initiate
a new approach that also includes critically injective polynomials. We provide
this characterization for both the complex and p-adic cases. We also provide
various examples as an application of our results along with the verification
of the existing examples. Consequently, we find examples of unique range sets
generated by non-critically injective polynomials with least cardinalities
achieved so far and one of these results is sharp with respect to all the
available formulas in the literature. Furthermore, we cover the part of least
degree uniqueness polynomials. In this part, we also provide some sharp bounds.",['math.CV'],False,,,,"Consistency of heritability estimation from summary statistics in
  high-dimensional linear models","On the characterization of uniqueness polynomials: both complex and
  p-adic versions"
neg-d2-839,2025-01-02,,2501.01193," We contribute to the lively debate in current scholarship on the Leibnizian
calculus. In a recent text, Arthur and Rabouin argue that non-Archimedean
continua are incompatible with Leibniz's concepts of number, quantity and
magnitude.
  They allege that Leibniz viewed infinitesimals as contradictory, and claim to
deduce such a conclusion from an analysis of the Leibnizian definition of
quantity. However, their argument is marred by numerous errors, deliberate
omissions, and misrepresentations, stemming in a number of cases from flawed
analyses in their earlier publications.
  We defend the thesis, traceable to the classic study by Henk Bos, that
Leibniz used genuine infinitesimals, which he viewed as fictional mathematical
entities (and not merely shorthand for talk about more ordinary quantities) on
par with negatives and imaginaries.",['math.HO'],2501.03116," We extend the classical Poincar\'e-Birkhoff-Witt theorem to higher algebra by
establishing a version that applies to spectral Lie algebras. We deduce this
statement from a basic relation between operads in spectra: the commutative
operad is the quotient of the associative operad by a right action of the
spectral Lie operad. This statement, in turn, is a consequence of a fundamental
relation between different $\mathbb{E}_n$-operads, which we articulate and
prove. We deduce a variant of the Poincar\'{e}--Birkhoff--Witt theorem for
relative enveloping algebras of $\mathbb{E}_n$-algebras. Our methods also give
a simple construction and description of the higher enveloping
$\mathbb{E}_n$-algebras of a spectral Lie algebra.","['math.AT', 'math.CT', 'math.RT']",False,,,,Leibniz's contested infinitesimals: Further depictions,Poincar\'{e}-Birkhoff-Witt Theorems in Higher Algebra
neg-d2-840,2025-02-23,,2502.1645," By connecting disparate sources of scientific literature, literature\-/based
discovery (LBD) methods help to uncover new knowledge and generate new research
hypotheses that cannot be found from domain-specific documents alone. Our work
focuses on bisociative LBD methods that combine bisociative reasoning with LBD
techniques. The paper presents LBD through the lens of reproducible science to
ensure the reproducibility of LBD experiments, overcome the inconsistent use of
benchmark datasets and methods, trigger collaboration, and advance the LBD
field toward more robust and impactful scientific discoveries. The main novelty
of this study is a collection of Jupyter Notebooks that illustrate the steps of
the bisociative LBD process, including data acquisition, text preprocessing,
hypothesis formulation, and evaluation. The contributed notebooks implement a
selection of traditional LBD approaches, as well as our own ensemble-based,
outlier-based, and link prediction-based approaches. The reader can benefit
from hands-on experience with LBD through open access to benchmark datasets,
code reuse, and a ready-to-run Docker recipe that ensures reproducibility of
the selected LBD methods.",['cs.CL'],2501.13826," Humans acquire knowledge through three cognitive stages: perceiving
information, comprehending knowledge, and adapting knowledge to solve novel
problems. Videos serve as an effective medium for this learning process,
facilitating a progression through these cognitive stages. However, existing
video benchmarks fail to systematically evaluate the knowledge acquisition
capabilities in Large Multimodal Models (LMMs). To address this gap, we
introduce Video-MMMU, a multi-modal, multi-disciplinary benchmark designed to
assess LMMs' ability to acquire and utilize knowledge from videos. Video-MMMU
features a curated collection of 300 expert-level videos and 900
human-annotated questions across six disciplines, evaluating knowledge
acquisition through stage-aligned question-answer pairs: Perception,
Comprehension, and Adaptation. A proposed knowledge gain metric,
{\Delta}knowledge, quantifies improvement in performance after video viewing.
Evaluation of LMMs reveals a steep decline in performance as cognitive demands
increase and highlights a significant gap between human and model knowledge
acquisition, underscoring the need for methods to enhance LMMs' capability to
learn and adapt from videos.","['cs.CV', 'cs.CL']",False,,,,"Make Literature-Based Discovery Great Again through Reproducible
  Pipelines","Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline
  Professional Videos"
neg-d2-841,2025-01-16,,2501.09328," Developing high-performance deep learning models is resource-intensive,
leading model owners to utilize Machine Learning as a Service (MLaaS) platforms
instead of publicly releasing their models. However, malicious users may
exploit query interfaces to execute model extraction attacks, reconstructing
the target model's functionality locally. While prior research has investigated
triggerable watermarking techniques for asserting ownership, existing methods
face significant challenges: (1) most approaches require additional training,
resulting in high overhead and limited flexibility, and (2) they often fail to
account for advanced attackers, leaving them vulnerable to adaptive attacks.
  In this paper, we propose Neural Honeytrace, a robust plug-and-play
watermarking framework against model extraction attacks. We first formulate a
watermark transmission model from an information-theoretic perspective,
providing an interpretable account of the principles and limitations of
existing triggerable watermarking. Guided by the model, we further introduce:
(1) a similarity-based training-free watermarking method for plug-and-play and
flexible watermarking, and (2) a distribution-based multi-step watermark
information transmission strategy for robust watermarking. Comprehensive
experiments on four datasets demonstrate that Neural Honeytrace outperforms
previous methods in efficiency and resisting adaptive attacks. Neural
Honeytrace reduces the average number of samples required for a worst-case
t-Test-based copyright claim from $12,000$ to $200$ with zero training cost.","['cs.CR', 'cs.AI']",2501.12202," We present Hunyuan3D 2.0, an advanced large-scale 3D synthesis system for
generating high-resolution textured 3D assets. This system includes two
foundation components: a large-scale shape generation model -- Hunyuan3D-DiT,
and a large-scale texture synthesis model -- Hunyuan3D-Paint. The shape
generative model, built on a scalable flow-based diffusion transformer, aims to
create geometry that properly aligns with a given condition image, laying a
solid foundation for downstream applications. The texture synthesis model,
benefiting from strong geometric and diffusion priors, produces high-resolution
and vibrant texture maps for either generated or hand-crafted meshes.
Furthermore, we build Hunyuan3D-Studio -- a versatile, user-friendly production
platform that simplifies the re-creation process of 3D assets. It allows both
professional and amateur users to manipulate or even animate their meshes
efficiently. We systematically evaluate our models, showing that Hunyuan3D 2.0
outperforms previous state-of-the-art models, including the open-source models
and closed-source models in geometry details, condition alignment, texture
quality, and etc. Hunyuan3D 2.0 is publicly released in order to fill the gaps
in the open-source 3D community for large-scale foundation generative models.
The code and pre-trained weights of our models are available at:
https://github.com/Tencent/Hunyuan3D-2",['cs.CV'],False,,,,"Neural Honeytrace: A Robust Plug-and-Play Watermarking Framework against
  Model Extraction Attacks","Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D
  Assets Generation"
neg-d2-842,2025-01-14,,2501.08114," Change captioning has become essential for accurately describing changes in
multi-temporal remote sensing data, providing an intuitive way to monitor
Earth's dynamics through natural language. However, existing change captioning
methods face two key challenges: high computational demands due to multistage
fusion strategy, and insufficient detail in object descriptions due to limited
semantic extraction from individual images. To solve these challenges, we
propose SAT-Cap based on the transformers model with a single-stage feature
fusion for remote sensing change captioning. In particular, SAT-Cap integrates
a Spatial-Channel Attention Encoder, a Difference-Guided Fusion module, and a
Caption Decoder. Compared to typical models that require multi-stage fusion in
transformer encoder and fusion module, SAT-Cap uses only a simple cosine
similarity-based fusion module for information integration, reducing the
complexity of the model architecture. By jointly modeling spatial and channel
information in Spatial-Channel Attention Encoder, our approach significantly
enhances the model's ability to extract semantic information from objects in
multi-temporal remote sensing images. Extensive experiments validate the
effectiveness of SAT-Cap, achieving CIDEr scores of 140.23% on the LEVIR-CC
dataset and 97.74% on the DUBAI-CC dataset, surpassing current state-of-the-art
methods. The code and pre-trained models will be available online.",['cs.CV'],2502.09003," Supervised fine-tuning is a standard method for adapting pre-trained large
language models (LLMs) to downstream tasks. Quantization has been recently
studied as a post-training technique for efficient LLM deployment. To obtain
quantized fine-tuned LLMs, conventional pipelines would first fine-tune the
pre-trained models, followed by post-training quantization. This often yields
suboptimal performance as it fails to leverage the synergy between fine-tuning
and quantization. To effectively realize low-bit quantization of weights,
activations, and KV caches in LLMs, we propose an algorithm named Rotated
Straight-Through-Estimator (RoSTE), which combines quantization-aware
supervised fine-tuning (QA-SFT) with an adaptive rotation strategy that
identifies an effective rotation configuration to reduce activation outliers.
We provide theoretical insights on RoSTE by analyzing its prediction error when
applied to an overparameterized least square quantized training problem. Our
findings reveal that the prediction error is directly proportional to the
quantization error of the converged weights, which can be effectively managed
through an optimized rotation configuration. Experiments on Pythia, Qwen and
Llama models of different sizes demonstrate the effectiveness of RoSTE.
Compared to existing post-SFT quantization baselines, our method consistently
achieves superior performances across various tasks and different LLM
architectures.","['cs.LG', 'cs.AI']",False,,,,"Change Captioning in Remote Sensing: Evolution to SAT-Cap -- A
  Single-Stage Transformer Approach","RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach
  for Large Language Models"
neg-d2-843,2025-03-12,,2503.09318," Modern data analytics requires a huge amount of computing power and processes
a massive amount of data. At the same time, the underlying computing platform
is becoming much more heterogeneous on both hardware and software. Even though
specialized hardware, e.g., FPGA- or GPU- or TPU-based systems, often achieves
better performance than a CPU-only system due to the slowing of Moore's law,
such systems are limited in what they can do. For example, GPU-only approaches
suffer from severe IO limitations. To truly exploit the potential of hardware
heterogeneity, we present FpgaHub, an FPGA-centric hyper-heterogeneous
computing platform for big data analytics. The key idea of FpgaHub is to use
reconfigurable computing to implement a versatile hub complementing other
processors (CPUs, GPUs, DPUs, programmable switches, computational storage,
etc.). Using an FPGA as the basis, we can take advantage of its highly
reconfigurable nature and rich IO interfaces such as PCIe, networking, and
on-board memory, to place it at the center of the architecture and use it as a
data and control plane for data movement, scheduling, pre-processing, etc.
FpgaHub enables architectural flexibility to allow exploring the rich design
space of heterogeneous computing platforms.","['cs.DC', 'cs.AR']",2503.05021," Large Language Models (LLMs) are vulnerable to jailbreak attacks that exploit
weaknesses in traditional safety alignment, which often relies on rigid refusal
heuristics or representation engineering to block harmful outputs. While they
are effective for direct adversarial attacks, they fall short of broader safety
challenges requiring nuanced, context-aware decision-making. To address this,
we propose Reasoning-enhanced Finetuning for interpretable LLM Safety
(Rational), a novel framework that trains models to engage in explicit safe
reasoning before response. Fine-tuned models leverage the extensive pretraining
knowledge in self-generated reasoning to bootstrap their own safety through
structured reasoning, internalizing context-sensitive decision-making. Our
findings suggest that safety extends beyond refusal, requiring context
awareness for more robust, interpretable, and adaptive responses. Reasoning is
not only a core capability of LLMs but also a fundamental mechanism for LLM
safety. Rational employs reasoning-enhanced fine-tuning, allowing it to reject
harmful prompts while providing meaningful and context-aware responses in
complex scenarios.","['cs.CL', 'cs.CR']",False,,,,"FpgaHub: Fpga-centric Hyper-heterogeneous Computing Platform for Big
  Data Analytics","Safety is Not Only About Refusal: Reasoning-Enhanced Fine-tuning for
  Interpretable LLM Safety"
neg-d2-844,2025-02-25,,2502.18423," Retrieving objects buried beneath multiple objects is not only challenging
but also time-consuming. Performing manipulation in such environments presents
significant difficulty due to complex contact relationships. Existing methods
typically address this task by sequentially grasping and removing each
occluding object, resulting in lengthy execution times and requiring
impractical grasping capabilities for every occluding object. In this paper, we
present a dexterous arm-hand system for efficient object retrieval in
multi-object stacked environments. Our approach leverages large-scale parallel
reinforcement learning within diverse and carefully designed cluttered
environments to train policies. These policies demonstrate emergent
manipulation skills (e.g., pushing, stirring, and poking) that efficiently
clear occluding objects to expose sufficient surface area of the target object.
We conduct extensive evaluations across a set of over 10 household objects in
diverse clutter configurations, demonstrating superior retrieval performance
and efficiency for both trained and unseen objects. Furthermore, we
successfully transfer the learned policies to a real-world dexterous
multi-fingered robot system, validating their practical applicability in
real-world scenarios. Videos can be found on our project website
https://ChangWinde.github.io/RetrDex.",['cs.RO'],2502.11951," This paper dives into the exciting and rapidly growing field of quantum
computing, explaining its core ideas, current progress, and how it could
revolutionize the way we solve complex problems. It starts by breaking down the
basics, like qubits, quantum circuits, and how principles like superposition
and entanglement make quantum computers fundamentally different-and far more
powerful for certain tasks-than the classical computers we use today. We also
explore how quantum computing deals with complex problems and why it is
uniquely suited for challenges classical systems struggle to handle. A big part
of this paper focuses on Quantum Machine Learning (QML), where the strengths of
quantum computing meet the world of artificial intelligence. By processing
massive datasets and optimizing intricate algorithms, quantum systems offer new
possibilities for machine learning. We highlight different approaches to
combining quantum and classical computing, showing how they can work together
to produce faster and more accurate results. Additionally, we explore the tools
and platforms available-like TensorFlow Quantum, Qiskit and PennyLane-that are
helping researchers and developers bring these theories to life. Of course,
quantum computing has its hurdles. Challenges like scaling up hardware,
correcting errors, and keeping qubits stable are significant roadblocks. Yet,
with rapid advancements in cloud-based platforms and innovative technologies,
the potential of quantum computing feels closer than ever. This paper aims to
offer readers a clear and comprehensive introduction to quantum computing, its
role in machine learning, and the immense possibilities it holds for the future
of technology.","['cs.CE', 'cs.LG', 'quant-ph']",False,,,,"Retrieval Dexterity: Efficient Object Retrieval in Clutters with
  Dexterous Hand","Qubit-Based Framework for Quantum Machine Learning: Bridging Classical
  Data and Quantum Algorithms"
neg-d2-845,2025-02-20,,2502.14511," The development of reliable ab initio methods for light-matter strong
coupling is necessary for a deeper understanding of molecular polaritons. The
recently developed strong coupling quantum electrodynamics Hartree-Fock model
(SC-QED-HF) provides cavity-consistent molecular orbitals, overcoming several
difficulties related to the simpler QED-HF wave function. In this paper, we
further develop this method by implementing the response theory for SC-QED-HF.
We compare the derived linear response equations with the time-dependent QED-HF
theory and discuss the validity of equivalence relations connecting matter and
electromagnetic observables. Our results show that electron-photon correlation
induces an excitation redshift compared to the time-dependent QED-HF energies,
and we discuss the effect of the dipole self-energy on the ground and excited
state properties with different basis sets.",['physics.chem-ph'],2501.17705," Integrating high-dimensional, heterogeneous data from multi-site cohort
studies with complex hierarchical structures poses significant feature
selection and prediction challenges. We extend the Bayesian Integrative
Analysis and Prediction (BIP) framework to enable simultaneous feature
selection and outcome modeling in data of nested hierarchical structure. We
apply the proposed Bayesian Integrative Mixed Modeling (BIPmixed) framework to
the Adolescent Brain Cognitive Development (ABCD) Study, leveraging multi-view
data, including structural and functional MRI and early life adversity (ELA)
metrics, to identify relevant features and predict the behavioral outcome.
BIPmixed incorporates 2-level nested random effects, to enhance
interpretability and make predictions in hierarchical data settings. Simulation
studies illustrate BIPmixed's robustness in distinct random effect settings,
highlighting its use for complex study designs. Our findings suggest that
BIPmixed effectively integrates multi-view data while accounting for nested
sampling, making it a valuable tool for analyzing large-scale studies with
hierarchical data.","['stat.ME', 'stat.AP']",False,,,,Strong coupling quantum electrodynamics Hartree-Fock response theory,"A Bayesian Integrative Mixed Modeling Framework for Analysis of the
  Adolescent Brain and Cognitive Development Study"
neg-d2-846,2025-01-05,,2501.02509," Facial attractiveness prediction (FAP) has long been an important computer
vision task, which could be widely applied in live streaming for facial
retouching, content recommendation, etc. However, previous FAP datasets are
either small, closed-source, or lack diversity. Moreover, the corresponding FAP
models exhibit limited generalization and adaptation ability. To overcome these
limitations, in this paper we present LiveBeauty, the first large-scale
live-specific FAP dataset, in a more challenging application scenario, i.e.,
live streaming. 10,000 face images are collected from a live streaming platform
directly, with 200,000 corresponding attractiveness annotations obtained from a
well-devised subjective experiment, making LiveBeauty the largest open-access
FAP dataset in the challenging live scenario. Furthermore, a multi-modal FAP
method is proposed to measure the facial attractiveness in live streaming.
Specifically, we first extract holistic facial prior knowledge and multi-modal
aesthetic semantic features via a Personalized Attractiveness Prior Module
(PAPM) and a Multi-modal Attractiveness Encoder Module (MAEM), respectively,
then integrate the extracted features through a Cross-Modal Fusion Module
(CMFM). Extensive experiments conducted on both LiveBeauty and other
open-source FAP datasets demonstrate that our proposed method achieves
state-of-the-art performance. Dataset will be available soon.",['cs.CV'],2503.04049," As the demand for computational power increases, high-bandwidth memory (HBM)
has become a critical technology for next-generation computing systems.
However, the widespread adoption of HBM presents significant thermal management
challenges, particularly in multilayer through-silicon-via (TSV) stacked
structures under varying thermal conditions, where accurate prediction of
junction temperature and hotspot position is essential during the early design.
This work develops a data-driven neural network model for the fast prediction
of junction temperature and hotspot position in 3D HBM chiplets. The model,
trained with a data set of $13,494$ different combinations of thermal condition
parameters, sampled from a vast parameter space characterized by
high-dimensional combination (up to $3^{27}$), can accurately and quickly infer
the junction temperature and hotspot position for any thermal conditions in the
parameter space. Moreover, it shows good generalizability for other thermal
conditions not considered in the parameter space. The data set is constructed
using accurate finite element solvers. This method not only minimizes the
reliance on costly experimental tests and extensive computational resources for
finite element analysis but also accelerates the design and optimization of
complex HBM systems, making it a valuable tool for improving thermal management
and performance in high-performance computing applications.",['cs.LG'],False,,,,"Facial Attractiveness Prediction in Live Streaming: A New Benchmark and
  Multi-modal Method","Neural Network Surrogate Model for Junction Temperature and Hotspot
  Position in $3$D Multi-Layer High Bandwidth Memory (HBM) Chiplets under
  Varying Thermal Conditions"
neg-d2-847,2025-03-19,,2503.15798," Mixture-of-Experts (MoE) activates only a subset of experts during inference,
allowing the model to maintain low inference FLOPs and latency even as the
parameter count scales up. However, since MoE dynamically selects the experts,
all the experts need to be loaded into VRAM. Their large parameter size still
limits deployment, and offloading, which load experts into VRAM only when
needed, significantly increase inference latency. To address this, we propose
Mixture of Lookup Experts (MoLE), a new MoE architecture that is efficient in
both communication and VRAM usage. In MoLE, the experts are Feed-Forward
Networks (FFNs) during training, taking the output of the embedding layer as
input. Before inference, these experts can be re-parameterized as lookup tables
(LUTs) that retrieves expert outputs based on input ids, and offloaded to
storage devices. Therefore, we do not need to perform expert computations
during inference. Instead, we directly retrieve the expert's computation
results based on input ids and load them into VRAM, and thus the resulting
communication overhead is negligible. Experiments show that, with the same
FLOPs and VRAM usage, MoLE achieves inference speeds comparable to dense models
and significantly faster than MoE with experts offloading, while maintaining
performance on par with MoE.","['cs.LG', 'cs.CL']",2503.13806," Accurate segmentation is essential for effective treatment planning and
disease monitoring. Existing medical image segmentation methods predominantly
rely on uni-modal visual inputs, such as images or videos, requiring
labor-intensive manual annotations. Additionally, medical imaging techniques
capture multiple intertwined organs within a single scan, further complicating
segmentation accuracy. To address these challenges, MedSAM, a large-scale
medical segmentation model based on the Segment Anything Model (SAM), was
developed to enhance segmentation accuracy by integrating image features with
user-provided prompts. While MedSAM has demonstrated strong performance across
various medical segmentation tasks, it primarily relies on geometric prompts
(e.g., points and bounding boxes) and lacks support for text-based prompts,
which could help specify subtle or ambiguous anatomical structures. To overcome
these limitations, we propose the Organ-aware Multi-scale Text-guided Medical
Image Segmentation Model (OMT-SAM) for multi-organ segmentation. Our approach
introduces CLIP encoders as a novel image-text prompt encoder, operating with
the geometric prompt encoder to provide informative contextual guidance. We
pair descriptive textual prompts with corresponding images, processing them
through pre-trained CLIP encoders and a cross-attention mechanism to generate
fused image-text embeddings. Additionally, we extract multi-scale visual
features from MedSAM, capturing fine-grained anatomical details at different
levels of granularity. We evaluate OMT-SAM on the FLARE 2021 dataset,
benchmarking its performance against existing segmentation methods. Empirical
results demonstrate that OMT-SAM achieves a mean Dice Similarity Coefficient of
0.937, outperforming MedSAM (0.893) and other segmentation models, highlighting
its superior capability in handling complex medical image segmentation tasks.","['cs.CV', 'cs.AI']",False,,,,Mixture of Lookup Experts,"Organ-aware Multi-scale Medical Image Segmentation Using Text Prompt
  Engineering"
neg-d2-848,2025-02-28,,2502.20897," People naturally vary in their annotations for subjective questions and some
of this variation is thought to be due to the person's sociodemographic
characteristics. LLMs have also been used to label data, but recent work has
shown that models perform poorly when prompted with sociodemographic
attributes, suggesting limited inherent sociodemographic knowledge. Here, we
ask whether LLMs can be trained to be accurate sociodemographic models of
annotator variation. Using a curated dataset of five tasks with standardized
sociodemographics, we show that models do improve in sociodemographic prompting
when trained but that this performance gain is largely due to models learning
annotator-specific behaviour rather than sociodemographic patterns. Across all
tasks, our results suggest that models learn little meaningful connection
between sociodemographics and annotation, raising doubts about the current use
of LLMs for simulating sociodemographic variation and behaviour.",['cs.CL'],2501.12166," Detecting anomalies in discrete event logs is critical for ensuring system
reliability, security, and efficiency. Traditional window-based methods for log
anomaly detection often suffer from context bias and fuzzy localization, which
hinder their ability to precisely and efficiently identify anomalies. To
address these challenges, we propose a graph-centric framework, TempoLog, which
leverages multi-scale temporal graph networks for discrete log anomaly
detection. Unlike conventional methods, TempoLog constructs continuous-time
dynamic graphs directly from event logs, eliminating the need for fixed-size
window grouping. By representing log templates as nodes and their temporal
relationships as edges, the framework dynamically captures both local and
global dependencies across multiple temporal scales. Additionally, a
semantic-aware model enhances detection by incorporating rich contextual
information. Extensive experiments on public datasets demonstrate that our
method achieves state-of-the-art performance in event-level anomaly detection,
significantly outperforming existing approaches in both accuracy and
efficiency.","['cs.SE', 'cs.LG']",False,,,,"Beyond Demographics: Fine-tuning Large Language Models to Predict
  Individuals' Subjective Text Perceptions","Beyond Window-Based Detection: A Graph-Centric Framework for Discrete
  Log Anomaly Detection"
neg-d2-849,2025-02-10,,2502.07042," Departments within a university are not only administrative units, but also
an effort to gather investigators around common fields of academic study. A
pervasive challenge is connecting members with shared research interests both
within and between departments. Here I describe a workflow that adapts methods
from natural language processing to generate a network connecting $n=79$
members of a university department, or multiple departments within a faculty
($n=278$), based on common topics in their research publications. After
extracting and processing terms from $n=16,901$ abstracts in the PubMed
database, the co-occurrence of terms is encoded in a sparse document-term
matrix. Based on the angular distances between the presence-absence vectors for
every pair of terms, I use the uniform manifold approximation and projection
(UMAP) method to embed the terms into a representational space such that terms
that tend to appear in the same documents are closer together. Each author's
corpus defines a probability distribution over terms in this space. Using the
Wasserstein distance to quantify the similarity between these distributions, I
generate a distance matrix among authors that can be analyzed and visualized as
a graph. I demonstrate that this nonparametric method produces clusters with
distinct themes that are consistent with some academic divisions, while
identifying untapped connections among members. A documented workflow
comprising Python and R scripts is available under the MIT license at
https://github.com/PoonLab/tragula.",['cs.SI'],2502.14344," Binary Spiking Neural Networks (BSNNs) inherit the eventdriven paradigm of
SNNs, while also adopting the reduced storage burden of binarization
techniques. These distinct advantages grant BSNNs lightweight and
energy-efficient characteristics, rendering them ideal for deployment on
resource-constrained edge devices. However, due to the binary synaptic weights
and non-differentiable spike function, effectively training BSNNs remains an
open question. In this paper, we conduct an in-depth analysis of the challenge
for BSNN learning, namely the frequent weight sign flipping problem. To
mitigate this issue, we propose an Adaptive Gradient Modulation Mechanism
(AGMM), which is designed to reduce the frequency of weight sign flipping by
adaptively adjusting the gradients during the learning process. The proposed
AGMM can enable BSNNs to achieve faster convergence speed and higher accuracy,
effectively narrowing the gap between BSNNs and their full-precision
equivalents. We validate AGMM on both static and neuromorphic datasets, and
results indicate that it achieves state-of-the-art results among BSNNs. This
work substantially reduces storage demands and enhances SNNs' inherent energy
efficiency, making them highly feasible for resource-constrained environments.",['cs.CV'],False,,,,"Building networks of shared research interests by embedding words into a
  representation space","Towards Accurate Binary Spiking Neural Networks: Learning with Adaptive
  Gradient Modulation Mechanism"
neg-d2-850,2025-01-27,,2501.1632," This paper is the first in a series dedicated to computing the integral Chow
rings of the moduli stacks of Prym pairs. In this work, we compute the Chow
ring for Prym pairs arising from a single pair of Weierstrass points and from
at most $(g-1)/2 $ pairs when the genus $g$ of the curve is odd.","['math.AG', 'math.RT']",2502.10722," Modern processors widely equip the Performance Monitoring Unit (PMU) to
collect various architecture and microarchitecture events. Software developers
often utilize the PMU to enhance program's performance, but the potential side
effects that arise from its activation are often disregarded. In this paper, we
find that the PMU can be employed to retrieve instruction operands. Based on
this discovery, we introduce PMU-Data, a novel category of side-channel attacks
aimed at leaking secret by identifying instruction operands with PMU.
  To achieve the PMU-Data attack, we develop five gadgets to encode the
confidential data into distinct data-related traces while maintaining the
control-flow unchanged. We then measure all documented PMU events on three
physical machines with different processors while those gadgets are performing.
We successfully identify two types of vulnerable gadgets caused by DIV and MOV
instructions. Additionally, we discover 40 vulnerable PMU events that can be
used to carry out the PMU-Data attack. We through real experiments to
demonstrate the perniciousness of the PMU-Data attack by implementing three
attack goals: (1) leaking the kernel data illegally combined with the transient
execution vulnerabilities including Meltdown, Spectre, and Zombieload; (2)
building a covert-channel to secretly transfer data; (3) extracting the secret
data protected by the Trusted Execution Environment (TEE) combined with the
Zombieload vulnerability.",['cs.CR'],False,,,,"The Integral Chow Rings of the Moduli Stacks of Hyperelliptic Prym Pairs
  I",PMU-Data: Data Traces Could be Distinguished
neg-d2-851,2025-02-20,,2502.15024," We investigate implications of the (extended) low-degree conjecture (recently
formalized in [MW23]) in the context of the symmetric stochastic block model.
Assuming the conjecture holds, we establish that no polynomial-time algorithm
can weakly recover community labels below the Kesten-Stigum (KS) threshold. In
particular, we rule out polynomial-time estimators that, with constant
probability, achieve correlation with the true communities that is
significantly better than random. Whereas, above the KS threshold,
polynomial-time algorithms are known to achieve constant correlation with the
true communities with high probability[Mas14,AS15].
  To our knowledge, we provide the first rigorous evidence for the sharp
transition in recovery rate for polynomial-time algorithms at the KS threshold.
Notably, under a stronger version of the low-degree conjecture, our lower bound
remains valid even when the number of blocks diverges. Furthermore, our results
provide evidence of a computational-to-statistical gap in learning the
parameters of stochastic block models.
  In contrast to prior work, which either (i) rules out polynomial-time
algorithms for hypothesis testing with 1-o(1) success probability [Hopkins18,
BBK+21a] under the low-degree conjecture, or (ii) rules out low-degree
polynomials for learning the edge connection probability matrix [LG23], our
approach provides stronger lower bounds on the recovery and learning problem.
  Our proof combines low-degree lower bounds from [Hopkins18, BBK+21a] with
graph splitting and cross-validation techniques. In order to rule out general
recovery algorithms, we employ the correlation preserving projection method
developed in [HS17].","['cs.CC', 'cs.LG', 'math.ST', 'stat.CO', 'stat.TH']",2502.12485," To ensure safe usage, Large Language Models (LLMs) typically undergo
alignment with human-defined values. However, this alignment often relies on
primarily English data and is biased towards Western-centric values, limiting
its effectiveness in low-resource language settings. In this paper, we describe
our approach for aligning SEA-Lion-v2.1-Instruct (a Llama3-8B variant) to
minimize toxicity in Singlish, an English creole specific to Singapore. We find
that supervised fine-tuning and Kahneman-Tversky Optimization (KTO) on paired
and unpaired preferences is more sample efficient and yields significantly
better results than Direct Preference Optimization (DPO). Our analysis reveals
that DPO implicitly enforces a weaker safety objective than KTO, and that SFT
complements KTO by improving training stability. Finally, we introduce a simple
but novel modification to KTO, KTO-S, which improves training stability through
better gradient exploitation. Overall, we present a general approach for safety
alignment conducive to low-resource English languages, successfully reducing
toxicity by 99\% on our Singlish benchmark, with gains generalizing to the
broader TOXIGEN dataset while maintaining strong performance across standard
LLM benchmarks.","['cs.CL', 'cs.AI']",False,,,,"Low degree conjecture implies sharp computational thresholds in
  stochastic block model","Safe at the Margins: A General Approach to Safety Alignment in
  Low-Resource English Languages -- A Singlish Case Study"
neg-d2-852,2025-02-20,,2502.14314," You Look Only Once (YOLO) models have been widely used for building real-time
object detectors across various domains. With the increasing frequency of new
YOLO versions being released, key questions arise. Are the newer versions
always better than their previous versions? What are the core innovations in
each YOLO version and how do these changes translate into real-world
performance gains? In this paper, we summarize the key innovations from YOLOv1
to YOLOv11, introduce a comprehensive benchmark called ODverse33, which
includes 33 datasets spanning 11 diverse domains (Autonomous driving,
Agricultural, Underwater, Medical, Videogame, Industrial, Aerial, Wildlife,
Retail, Microscopic, and Security), and explore the practical impact of model
improvements in real-world, multi-domain applications through extensive
experimental results. We hope this study can provide some guidance to the
extensive users of object detection models and give some references for future
real-time object detector development.",['cs.CV'],2501.10417," In this paper we introduce the generalized inverse of complex square matrix
with respect to other matrix having same size. Some of its representations,
properties and characterizations are obtained. Also some new representation
matrices of W-weighted BT-inverse and W-weighted core-EP inverse are determined
as well as characterizations of generalized inverses A A^\odagger,
A^{odagger,W}, A^\diamond, A^{\diamond,W}.","['math.RA', 'math.FA']",False,,,,"ODVerse33: Is the New YOLO Version Always Better? A Multi Domain
  benchmark from YOLO v5 to v11",Simultaneous extension of generalized BT-inverses and core-EP inverses
neg-d2-853,2025-02-14,,2502.12181," Explainability remains a significant problem for AI models in medical
imaging, making it challenging for clinicians to trust AI-driven predictions.
We introduce 3D ReX, the first causality-based post-hoc explainability tool for
3D models. 3D ReX uses the theory of actual causality to generate
responsibility maps which highlight the regions most crucial to the model's
decision. We test 3D ReX on a stroke detection model, providing insight into
the spatial distribution of features relevant to stroke.","['eess.IV', 'cs.AI', 'cs.CV', 'cs.LG']",2502.06103," In 2015, Phulara established a generalization of the famous central set
theorem by an original idea. Roughly speaking, this idea extends a
combinatorial result from one large subset of the given semigroup to countably
many. In this paper, we apply this idea to other combinatorial results to
obtain corresponding generalizations, and do some further investigation.
Moreover, we find that Phulara's generalization can be generalized further that
can deal with uncountably many C-sets.",['math.CO'],False,,,,3D ReX: Causal Explanations in 3D Neuroimaging Classification,"Several combinatorial results generalized from one large subset of
  semigroups to infinitely many"
neg-d2-854,2025-02-04,,2502.02248," This paper focuses on the concentration properties of the spectral norm of
the normalized Laplacian matrix for Erd\H{o}s-R\'enyi random graphs. First, We
achieve the optimal bound that can be attained in the further question posed by
Le et al. [24] for the regularized Laplacian matrix. Beyond that, we also
establish a uniform concentration inequality for the spectral norm of the
Laplacian matrix in the homogeneous case, relying on a key tool: the uniform
concentration property of degrees, which may be of independent interest.
Additionally, we prove that after normalizing the eigenvector corresponding to
the largest eigenvalue, the spectral norm of the Laplacian matrix concentrates
around 1, which may be useful in special cases.",['math.PR'],2503.02345," The detection of Alzheimer disease (AD) from clinical MRI data is an active
area of research in medical imaging. Recent advances in quantum computing,
particularly the integration of parameterized quantum circuits (PQCs) with
classical machine learning architectures, offer new opportunities to develop
models that may outperform traditional methods. However, quantum machine
learning (QML) remains in its early stages and requires further experimental
analysis to better understand its behavior and limitations. In this paper, we
propose an end to end hybrid classical quantum convolutional neural network (CQ
CNN) for AD detection using clinically formatted 3D MRI data. Our approach
involves developing a framework to make 3D MRI data usable for machine
learning, designing and training a brain tissue segmentation model (Skull Net),
and training a diffusion model to generate synthetic images for the minority
class. Our converged models exhibit potential quantum advantages, achieving
higher accuracy in fewer epochs than classical models. The proposed beta8 3
qubit model achieves an accuracy of 97.50%, surpassing state of the art (SOTA)
models while requiring significantly fewer computational resources. In
particular, the architecture employs only 13K parameters (0.48 MB), reducing
the parameter count by more than 99.99% compared to current SOTA models.
Furthermore, the diffusion-generated data used to train our quantum models, in
conjunction with real samples, preserve clinical structural standards,
representing a notable first in the field of QML. We conclude that CQCNN
architecture like models, with further improvements in gradient optimization
techniques, could become a viable option and even a potential alternative to
classical models for AD detection, especially in data limited and resource
constrained clinical settings.","['quant-ph', 'cs.AI', 'cs.CV', 'cs.LG']",False,,,,"On Concentration Inequality of the Laplacian Matrix of Erd\H{o}s-R\'enyi
  Graphs","CQ CNN: A Hybrid Classical Quantum Convolutional Neural Network for
  Alzheimer's Disease Detection Using Diffusion Generated and U Net Segmented
  3D MRI"
neg-d2-855,2025-02-20,,2502.14604," Test-time adaptation (TTA) aims to address distribution shifts between source
and target data by relying solely on target data during testing. In open-world
scenarios, models often encounter noisy samples, i.e., samples outside the
in-distribution (ID) label space. Leveraging the zero-shot capability of
pre-trained vision-language models (VLMs), this paper introduces Zero-Shot
Noisy TTA (ZS-NTTA), focusing on adapting the model to target data with noisy
samples during test-time in a zero-shot manner. We find existing TTA methods
underperform under ZS-NTTA, often lagging behind even the frozen model. We
conduct comprehensive experiments to analyze this phenomenon, revealing that
the negative impact of unfiltered noisy data outweighs the benefits of clean
data during model updating. Also, adapting a classifier for ID classification
and noise detection hampers both sub-tasks. Built on this, we propose a
framework that decouples the classifier and detector, focusing on developing an
individual detector while keeping the classifier frozen. Technically, we
introduce the Adaptive Noise Detector (AdaND), which utilizes the frozen
model's outputs as pseudo-labels to train a noise detector. To handle clean
data streams, we further inject Gaussian noise during adaptation, preventing
the detector from misclassifying clean samples as noisy. Beyond the ZS-NTTA,
AdaND can also improve the zero-shot out-of-distribution (ZS-OOD) detection
ability of VLMs. Experiments show that AdaND outperforms in both ZS-NTTA and
ZS-OOD detection. On ImageNet, AdaND achieves a notable improvement of $8.32\%$
in harmonic mean accuracy ($\text{Acc}_\text{H}$) for ZS-NTTA and $9.40\%$ in
FPR95 for ZS-OOD detection, compared to SOTA methods. Importantly, AdaND is
computationally efficient and comparable to the model-frozen method. The code
is publicly available at: https://github.com/tmlr-group/ZS-NTTA.",['cs.LG'],2503.04003," Mobile platforms now power not only smartphones but also in-vehicle systems
like Android Auto and CarPlay. Despite an ecosystem of over 3.5 million Android
apps and more than 200 million Android Auto-compatible vehicles, only a few
hundred apps have been adapted for automotive use. To better understand this
gap, we studied 147 reported issues related to Android Auto and identified
their root causes. We found that more than 70% of issues result from UI
incompatibilities, 24% from media playback errors, and around 5% from failures
in voice command handling, showing a lack of effective tools for developers. We
introduce CarCompat, a static analysis framework that detects compatibility
problems in Android Auto apps. CarCompat constructs a Car-Control Flow Graph
(CCFG) to capture interactions among app components, lifecycle methods, and
platform-specific callbacks. It applies specialized checkers to detect UI
violations, media playback errors, and issues with voice command handling. We
evaluated CarCompat on a dataset of 54 Android Auto apps and detected 25 new
issues, 4 of which were confirmed by developers, and 2 developers have already
released their fixes. The results show that CarCompat helps developers identify
and fix compatibility issues, improving the in-vehicle experience.","['cs.SE', 'cs.PL']",False,,,,Noisy Test-Time Adaptation in Vision-Language Models,Understanding and Detecting Compatibility Issues in Android Auto Apps
neg-d2-856,2025-01-23,,2501.13826," Humans acquire knowledge through three cognitive stages: perceiving
information, comprehending knowledge, and adapting knowledge to solve novel
problems. Videos serve as an effective medium for this learning process,
facilitating a progression through these cognitive stages. However, existing
video benchmarks fail to systematically evaluate the knowledge acquisition
capabilities in Large Multimodal Models (LMMs). To address this gap, we
introduce Video-MMMU, a multi-modal, multi-disciplinary benchmark designed to
assess LMMs' ability to acquire and utilize knowledge from videos. Video-MMMU
features a curated collection of 300 expert-level videos and 900
human-annotated questions across six disciplines, evaluating knowledge
acquisition through stage-aligned question-answer pairs: Perception,
Comprehension, and Adaptation. A proposed knowledge gain metric,
{\Delta}knowledge, quantifies improvement in performance after video viewing.
Evaluation of LMMs reveals a steep decline in performance as cognitive demands
increase and highlights a significant gap between human and model knowledge
acquisition, underscoring the need for methods to enhance LMMs' capability to
learn and adapt from videos.","['cs.CV', 'cs.CL']",2502.10114," Ewens' sampling formula (ESF) provides the probability distribution governing
the number of distinct genetic types and their respective frequencies at a
selectively neutral locus under the infinitely-many-alleles model of mutation.
A natural and significant question arises: ``Is the Ewens probability
distribution on regular trees Gibbsian?""
  In this paper, we demonstrate that Ewens probability distributions can be
regarded as non-Gibbsian distributions on regular trees and derive a sufficient
condition for the consistency condition. This study lays the groundwork for a
new direction in the theory of non-Gibbsian probability distributions on trees.","['math.PR', 'math.FA']",False,,,,"Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline
  Professional Videos","Non-Gibbsian Multivariate Ewens Probability Distributions on Regular
  Trees"
neg-d2-857,2025-01-14,,2501.08206," This paper proposes SAT-based techniques to calculate a specific normal form
of a given finite mathematical structure (model). The normal form is obtained
by permuting the domain elements so that the representation of the structure is
lexicographically smallest possible. Such a normal form is of interest to
mathematicians as it enables easy cataloging of algebraic structures. In
particular, two structures are isomorphic precisely when their normal forms are
the same. This form is also natural to inspect as mathematicians have been
using it routinely for many decades.
  We develop a novel approach where a SAT solver is used in a black-box fashion
to compute the smallest representative. The approach constructs the
representative gradually and searches the space of possible isomorphisms,
requiring a small number of variables. However, the approach may lead to a
large number of SAT calls and therefore we devise propagation techniques to
reduce this number. The paper focuses on finite structures with a single binary
operation (encompassing groups, semigroups, etc.). However, the approach is
generalizable to arbitrary finite structures. We provide an implementation of
the proposed algorithm and evaluate it on a variety of algebraic structures.",['cs.LO'],2501.01193," We contribute to the lively debate in current scholarship on the Leibnizian
calculus. In a recent text, Arthur and Rabouin argue that non-Archimedean
continua are incompatible with Leibniz's concepts of number, quantity and
magnitude.
  They allege that Leibniz viewed infinitesimals as contradictory, and claim to
deduce such a conclusion from an analysis of the Leibnizian definition of
quantity. However, their argument is marred by numerous errors, deliberate
omissions, and misrepresentations, stemming in a number of cases from flawed
analyses in their earlier publications.
  We defend the thesis, traceable to the classic study by Henk Bos, that
Leibniz used genuine infinitesimals, which he viewed as fictional mathematical
entities (and not merely shorthand for talk about more ordinary quantities) on
par with negatives and imaginaries.",['math.HO'],False,,,,SAT-Based Techniques for Lexicographically Smallest Finite Models,Leibniz's contested infinitesimals: Further depictions
neg-d2-858,2025-03-18,,2503.14723," Code quality is of paramount importance in all types of software development
settings. Our work seeks to enable Machine Learning (ML) engineers to write
better code by helping them find and fix instances of Data Leakage in their
models. Data Leakage often results from bad practices in writing ML code. As a
result, the model effectively ''memorizes'' the data on which it trains,
leading to an overly optimistic estimate of the model performance and an
inability to make generalized predictions. ML developers must carefully
separate their data into training, evaluation, and test sets to avoid
introducing Data Leakage into their code. Training data should be used to train
the model, evaluation data should be used to repeatedly confirm a model's
accuracy, and test data should be used only once to determine the accuracy of a
production-ready model. In this paper, we develop LEAKAGEDETECTOR, a Python
plugin for the PyCharm IDE that identifies instances of Data Leakage in ML code
and provides suggestions on how to remove the leakage.",['cs.SE'],2502.15247," We developed an anisotropic spin model that accounts for magnetic anisotropy
and evaluated the Curie temperature (Tc) dispersion due to finite size effects
in L10-FePt nanoparticles. In heat-assisted magnetic recording (HAMR) media, a
next-generation magnetic recording technology, high-density recording is
achieved by locally heating L10-FePt nanoparticles near their Tc and rapidly
cooling them. However, variations in Tc caused by differences in particle size
and shape can compromise recording stability and areal density capacity, making
the control of Tc dispersion critical. In this study, we constructed atomistic
LLG models to explicitly incorporate the spin exchange anisotropy of L10-FePt,
based on parameters determined by first-principles calculations. Using this
model, we evaluated the impact of particle size on Tc dispersion. As a result,
(1) the Tc dispersion critical to the performance of HAMR can be reproduced,
whereas it was previously underestimated by isotropic models and (2)
approximately 70% of the experimentally observed Tc dispersion can be
attributed to particle size effects. This research highlights the role of
exchange anisotropy in amplifying finite-size effects and underscores the
importance of size control in HAMR media.","['cond-mat.mtrl-sci', 'cond-mat.mes-hall']",False,,,,"LeakageDetector: An Open Source Data Leakage Analysis Tool in Machine
  Learning Pipelines","Anisotropic Exchange Spin Model to Investigate the Curie Temperature
  Dispersion of Finite-Size L10-FePt Magnetic Nanoparticles"
neg-d2-859,2025-02-12,,2502.08772," Protein flexibility, measured by the B-factor or Debye-Waller factor, is
essential for protein functions such as structural support, enzyme activity,
cellular communication, and molecular transport. Theoretical analysis and
prediction of protein flexibility are crucial for protein design, engineering,
and drug discovery. In this work, we introduce the persistent sheaf Laplacian
(PSL), an effective tool in topological data analysis, to model and analyze
protein flexibility. By representing the local topology and geometry of protein
atoms through the multiscale harmonic and non-harmonic spectra of PSLs, the
proposed model effectively captures protein flexibility and provides accurate,
robust predictions of protein B-factors. Our PSL model demonstrates an increase
in accuracy of 32% compared to the classical Gaussian network model (GNM) in
predicting B-factors for a dataset of 364 proteins. Additionally, we construct
a blind machine learning prediction method utilizing global and local protein
features. Extensive computations and comparisons validate the effectiveness of
the proposed PSL model for B-factor predictions.","['q-bio.BM', 'q-bio.QM']",2503.111," The efficiency of silicon solar cells gradually decreases in various
environments, with humidity being a key factor contributing to this decline.
This study investigates the humidity-induced failure mechanisms in crystalline
silicon solar cells. Using density functional theory and the non-equilibrium
Green's function method, we systematically examine the microscopic diffusion
mechanisms of hydrogen and oxygen defects and their impact on photovoltaic
performance. Hydrogen and oxygen are interstitial defects that can introduce
both deep-level and resonant-state recombination centers, thereby reducing
carrier lifetime and solar cell efficiency. Furthermore, hydrogen exhibits
prominent diffusion pathways, particularly in its +1 and 0 charge states at the
BC site ( ""H"" _""i(BC)"" ^""+1"" and ""H"" _""i(BC)"" ^""0"" ), while oxygen in its +1
and 0 charge states shows a higher diffusion barrier at the BC1 site ( ""O""
_""i(BC1)"" ^""+1"" and ""O"" _""i(BC1)"" ^""0"" ). These defects, induced by moisture
and temperature fluctuations, exacerbate the degradation of solar cell
performance. By analyzing these defect behaviors, this research provides
valuable insights into the failure mechanisms of Si solar cells, especially
under humid conditions.",['cond-mat.mtrl-sci'],False,,,,Persistent Sheaf Laplacian Analysis of Protein Flexibility,"The Role of Hydrogen and Oxygen Interstitial Defects in Crystalline Si
  cells: Mechanism of Device Degradation in Humid Environment"
neg-d2-860,2025-03-21,,2503.16855," Hand gesture-based Sign Language Recognition (SLR) serves as a crucial
communication bridge between deaf and non-deaf individuals. Existing SLR
systems perform well for their cultural SL but may struggle with multi-cultural
sign languages (McSL). To address these challenges, this paper proposes a Stack
Spatial-Temporal Transformer Network that leverages multi-head attention
mechanisms to capture both spatial and temporal dependencies with hierarchical
features using the Stack Transfer concept. In the proceed, firstly, we applied
a fully connected layer to make a embedding vector which has high expressive
power from the original dataset, then fed them a stack newly proposed
transformer to achieve hierarchical features with short-range and long-range
dependency. The network architecture is composed of several stages that process
spatial and temporal relationships sequentially, ensuring effective feature
extraction. After making the fully connected layer, the embedding vector is
processed by the Spatial Multi-Head Attention Transformer, which captures
spatial dependencies between joints. In the next stage, the Temporal Multi-Head
Attention Transformer captures long-range temporal dependencies, and again, the
features are concatenated with the output using another skip connection. The
processed features are then passed to the Feed-Forward Network (FFN), which
refines the feature representations further. After the FFN, additional skip
connections are applied to combine the output with earlier layers, followed by
a final normalization layer to produce the final output feature tensor. This
process is repeated for 10 transformer blocks. The extensive experiment shows
that the JSL, KSL and ASL datasets achieved good performance accuracy. Our
approach demonstrates improved performance in McSL, and it will be consider as
a novel work in this domain.",['cs.CV'],2502.14509," Does multilingual Neural Machine Translation (NMT) lead to The Curse of the
Multlinguality or provides the Cross-lingual Knowledge Transfer within a
language family? In this study, we explore multiple approaches for extending
the available data-regime in NMT and we prove cross-lingual benefits even in
0-shot translation regime for low-resource languages. With this paper, we
provide state-of-the-art open-source NMT models for translating between
selected Slavic languages. We released our models on the HuggingFace Hub
(https://hf.co/collections/allegro/multislav-6793d6b6419e5963e759a683) under
the CC BY 4.0 license. Slavic language family comprises morphologically rich
Central and Eastern European languages. Although counting hundreds of millions
of native speakers, Slavic Neural Machine Translation is under-studied in our
opinion. Recently, most NMT research focuses either on: high-resource languages
like English, Spanish, and German - in WMT23 General Translation Task 7 out of
8 task directions are from or to English; massively multilingual models
covering multiple language groups; or evaluation techniques.",['cs.CL'],False,,,,"Stack Transformer Based Spatial-Temporal Attention Model for Dynamic
  Multi-Culture Sign Language Recognition","MultiSlav: Using Cross-Lingual Knowledge Transfer to Combat the Curse of
  Multilinguality"
neg-d2-861,2025-01-02,,2501.01513," We theoretically study how the superfluid and condensate deformation of a
weakly interacting ultracold Bose gas evolve during the ramp-up of an external
weak disorder potential. Both resulting deformations turn out to consist of two
distinct contributions, namely a reversible equilibrium one, already predicted
by Huang and Meng in 1992, and a nonequilibrium dynamical one, whose magnitude
depends on the details of the ramping protocol. For the specific case of the
exponential ramp-up protocol, we are able to derive analytical time-dependent
expressions for the above quantities. After a sufficiently long time, a steady
state emerges that is generically out of equilibrium. We take the first step in
investigating its properties by studying its relaxation dynamics. In addition,
we analyze the two-time correlation function and elucidate its relation to the
equilibrium and the dynamical part of the condensate deformation.",['cond-mat.quant-gas'],2503.09402," Human daily activities can be concisely narrated as sequences of routine
events (e.g., turning off an alarm) in video streams, forming an event
vocabulary. Motivated by this, we introduce VLog, a novel video understanding
framework that define video narrations as vocabulary, going beyond the typical
subword vocabularies in existing generative video-language models. Built on the
lightweight language model GPT-2, VLog feature three key innovations: (i) A
generative retrieval model, marrying language model's complex reasoning
capabilities with contrastive retrieval's efficient similarity search. (ii) A
hierarchical vocabulary derived from large-scale video narrations using our
narration pair encoding algorithm, enabling efficient indexing of specific
events (e.g., cutting a tomato) by identifying broader scenarios (e.g.,
kitchen) with expressive postfixes (e.g., by the left hand). (iii) A vocabulary
update strategy leveraging generative models to extend the vocabulary for novel
events encountered during inference. To validate our approach, we introduce
VidCap-Eval, a development set requiring concise narrations with reasoning
relationships (e.g., before and after). Experiments on EgoSchema, COIN, and
HiREST further demonstrate the effectiveness of VLog, highlighting its ability
to generate concise, contextually accurate, and efficient narrations, offering
a novel perspective on video understanding. Codes are released at
https://github.com/showlab/VLog.",['cs.CV'],False,,,,"Out-of-equilibrium dynamical properties of Bose-Einstein condensates in
  a ramped up weak disorder","VLog: Video-Language Models by Generative Retrieval of Narration
  Vocabulary"
neg-d2-862,2025-02-11,,2502.07338," Ultrafast electron microscopy aims for imaging transient phenomena occurring
on nanoscale. One of its goals is to visualize localized optical and plasmonic
modes generated by coherent excitation in the vicinity of various types of
nanostructures. Such imaging capability was enabled by photon-induced
near-field optical microscopy, which is based on spectral filtering of
electrons inelastically scattered due to the stimulated interaction with the
near-field. Here we report on the development of ultrafast 4D scanning
transmission electron microscopy, which allows us to image the transverse
components of the optical near-field while avoiding the need of electron
spectral filtering. We demonstrate that this method is capable of imaging
optical near-fields of a tungsten nanotip and ponderomotive potential of an
optical standing wave with a spatial resolution of 21 nm.","['physics.optics', 'physics.ins-det']",2503.12146," Let $\mathcal{D}_{n} \subset \mathbb{N}$ denote the set of the $\tau(n)$
divisors of $n$. We study the function $$ D_{n}(X,Y):=|\{d \in
\mathcal{D}_{n}:\ X \le d \le X+Y\}| $$ for $Y \le X$.",['math.NT'],False,,,,"Ultrafast 4D scanning transmission electron microscopy for imaging of
  localized optical fields",Divisors of an Integer in a Short Interval
neg-d2-863,2025-03-19,,2503.14876," Supervised deep learning methods have been successful in the field of high
energy physics, and the trend within the field is to move away from high level
reconstructed variables to lower level, higher dimensional features. Supervised
methods require labelled data, which is typically provided by a simulator. As
the number of features increases, simulation accuracy decreases, leading to
greater domain shift between training and testing data when using lower-level
features. This work demonstrates that the classification without labels
paradigm can be used to remove the need for background simulation when training
supervised classifiers. This can result in classifiers with higher performance
on real data than those trained on simulated data.",['hep-ph'],2503.14137," Applying the least action principle to the motion of an ideal gas, we find
Bernoulli's equation where the local velocity is expressed as the gradient of a
velocity potential, while the internal energy depends on the interaction among
the particles of the gas. Then, assuming that the internal energy is
proportional non-locally to the logarithm of the mass density and truncating
the resulting sum of density gradients after the second term, we find an
additional Bohm's quantum potential term in the internal energy. Therefore, the
Bernoulli equation reduces to the Madelung equation, revealing a novel
classical description of quantum fluids that does not require to postulate
quantum mechanics. Finally, non-locality can be removed by introducing a
retarded potential, thus leading to a covariant formulation of the quantum
potential and of the equation of motion of an ideal quantum fluid.",['quant-ph'],False,,,,Strong CWoLa: Binary Classification Without Background Simulation,"A variational formulation of the governing equations of ideal quantum
  fluids"
neg-d2-864,2025-03-18,,2503.14584," We search for dark matter (DM) annihilating subhalos of the Milky Way halo
among the Fermi Large Area Telescope (LAT) unassociated sources. We construct,
for the first time, a statistical model of the unassociated sources at
latitudes above 10 degrees. The latter is built as a combination of both DM
annihilation subhalos as well as Galactic and extragalactic astrophysical
components. The astrophysical components are constructed based on distributions
of associated sources, while the distribution of DM subhalos is derived from
Monte Carlo simulations. In this model we take into account the differences in
the distributions of associated and unassociated sources including both
covariate and prior probability shifts (both being forms of ``dataset
shifts''). Previous searches of DM subhalos were based on classify-and-count
strategies, while the approach adopted in this work is based on quantification
learning, which allows one to determine a well-defined statistical
interpretation of the contribution of a population of DM subhalos to the
unassociated Fermi-LAT sources. In the $b\bar{b}$ annihilation channel and for
a range of DM masses from 10 GeV to 1 TeV, we don't find a significant
contribution from DM subhalos and derive a statistical 95% confidence upper
limit on the DM annihilation cross section in this channel. While the derived
limits are consistent with previous classify-and-count approaches, our
generative statistical model opens new avenues for population studies of
Fermi-LAT sources and, more generally, for searches of anomalies on top of
backgrounds in presence of statistical and systematic uncertainties.",['astro-ph.HE'],2503.00593," On electron kinetic scales, ions and electrons decouple, and electron
velocity shear on electron inertial length $\sim d_e$ can trigger
electromagnetic (EM) electron Kelvin-Helmholtz instability (EKHI). In this
paper, we present an analytic study of EM EKHI in an inviscid collisionless
plasma with a step-function electron shear flow. We show that in incompressible
collisionless plasma the ideal electron frozen-in condition $\mathbf{E} +
\mathbf{v}_e \times \mathbf{B}/c = 0$ must be broken for the EM EKHI to occur.
In a step-function electron shear flow, the ideal electron frozen-in condition
is replaced by magnetic flux conservation, i.e., $\nabla \times (\mathbf{E} +
\mathbf{v}_e\times \mathbf{B}/c) = 0$, resulting in a dispersion relation
similar to that of the standard ideal and incompressible magnetohydrodynamics
KHI. The magnetic field parallel to the electron streaming suppresses the EM
EKHI due to magnetic tension. The threshold for the EM mode of the EKHI is
$(\mathbf{k}\cdot\Delta\mathbf{U}_e)^2>\frac{n_{e1}+n_{e2}}{n_{e1}
n_{e2}}[n_{e1}(\mathbf{v}_{Ae1}\cdot\mathbf{k})^2+n_{e2}(\mathbf{v}_{Ae2}\cdot\mathbf{k})^2]$,
where $\mathbf{v}_{Ae} =\mathbf{B}/(4\pi m_e n_e)^{1/2}$, $\Delta\mathbf{U}_e$
and $n_e$ are the electron streaming velocity shear and densities,
respectively. The growth rate of the EM mode is $\gamma_{em} \sim \Omega_{ce}$,
the electron gyro-frequency.","['astro-ph.SR', 'astro-ph.HE', 'physics.plasm-ph']",False,,,,"Search for dark matter subhalos among unassociated Fermi-LAT sources in
  presence of dataset shift",Electromagnetic Electron Kelvin-Helmholtz Instability
neg-d2-865,2025-03-06,,2503.04667," This paper proposes a new principled multi-task representation learning
framework (InfoMTL) to extract noise-invariant sufficient representations for
all tasks. It ensures sufficiency of shared representations for all tasks and
mitigates the negative effect of redundant features, which can enhance language
understanding of pre-trained language models (PLMs) under the multi-task
paradigm. Firstly, a shared information maximization principle is proposed to
learn more sufficient shared representations for all target tasks. It can avoid
the insufficiency issue arising from representation compression in the
multi-task paradigm. Secondly, a task-specific information minimization
principle is designed to mitigate the negative effect of potential redundant
features in the input for each task. It can compress task-irrelevant redundant
information and preserve necessary information relevant to the target for
multi-task prediction. Experiments on six classification benchmarks show that
our method outperforms 12 comparative multi-task methods under the same
multi-task settings, especially in data-constrained and noisy scenarios.
Extensive experiments demonstrate that the learned representations are more
sufficient, data-efficient, and robust.","['cs.CL', 'cs.IT', 'cs.LG', 'math.IT']",2502.14673," Deploying ASR models at an industrial scale poses significant challenges in
hardware resource management, especially for long-form transcription tasks
where audio may last for hours. Large Conformer models, despite their
capabilities, are limited to processing only 15 minutes of audio on an 80GB
GPU. Furthermore, variable input lengths worsen inefficiencies, as standard
batching leads to excessive padding, increasing resource consumption and
execution time. To address this, we introduce ChunkFormer, an efficient ASR
model that uses chunk-wise processing with relative right context, enabling
long audio transcriptions on low-memory GPUs. ChunkFormer handles up to 16
hours of audio on an 80GB GPU, 1.5x longer than the current state-of-the-art
FastConformer, while also boosting long-form transcription performance with up
to 7.7% absolute reduction on word error rate and maintaining accuracy on
shorter tasks compared to Conformer. By eliminating the need for padding in
standard batching, ChunkFormer's masked batching technique reduces execution
time and memory usage by more than 3x in batch processing, substantially
reducing costs for a wide range of ASR systems, particularly regarding GPU
resources for models serving in real-world applications.","['cs.SD', 'eess.AS']",False,,,,"An Information-theoretic Multi-task Representation Learning Framework
  for Natural Language Understanding","ChunkFormer: Masked Chunking Conformer For Long-Form Speech
  Transcription"
neg-d2-866,2025-02-24,,2502.16906," While logical reasoning evaluation of Large Language Models (LLMs) has
attracted significant attention, existing benchmarks predominantly rely on
multiple-choice formats that are vulnerable to random guessing, leading to
overestimated performance and substantial performance fluctuations. To obtain
more accurate assessments of models' reasoning capabilities, we propose an
automated method for synthesizing open-ended logic puzzles, and use it to
develop a bilingual benchmark, AutoLogi. Our approach features program-based
verification and controllable difficulty levels, enabling more reliable
evaluation that better distinguishes models' reasoning abilities. Extensive
evaluation of eight modern LLMs shows that AutoLogi can better reflect true
model capabilities, with performance scores spanning from 35% to 73% compared
to the narrower range of 21% to 37% on the source multiple-choice dataset.
Beyond benchmark creation, this synthesis method can generate high-quality
training data by incorporating program verifiers into the rejection sampling
process, enabling systematic enhancement of LLMs' reasoning capabilities across
diverse datasets.",['cs.CL'],2503.0441," This article provides an introduction to nuclear magnetic resonance
spectroscopy in pulsed magnetic fields (PFNMR), focusing on its capabilities,
applications, and future developments in research involving high magnetic
fields. It highlights the significance of PFNMR in enhancing the understanding
of solid-state materials, with particular emphasis on those exhibiting complex
interactions and strong electronic correlations. Several technical aspects are
discussed, including the challenges associated with high-frequency NMR
experiments. The power of PFNMR is showcased through several examples,
including studies on the topical materials LiCuVO$_4$, SrCu$_2$(BO$_3$)$_2$,
and CeIn$_3$, offering insights into their magnetic and electronic properties
at high magnetic fields. The article also discusses possible future directions
for the technique, including improvements in PFNMR instrumentation and the
exploration of materials under extreme conditions. This exposition underscores
the role of PFNMR in advancing the frontiers of materials-science research.",['cond-mat.str-el'],False,,,,"AutoLogi: Automated Generation of Logic Puzzles for Evaluating Reasoning
  Abilities of Large Language Models",Nuclear magnetic resonance spectroscopy in pulsed magnetic fields
neg-d2-867,2025-03-18,,2503.14258," This paper introduces JuDGE (Judgment Document Generation Evaluation), a
novel benchmark for evaluating the performance of judgment document generation
in the Chinese legal system. We define the task as generating a complete legal
judgment document from the given factual description of the case. To facilitate
this benchmark, we construct a comprehensive dataset consisting of factual
descriptions from real legal cases, paired with their corresponding full
judgment documents, which serve as the ground truth for evaluating the quality
of generated documents. This dataset is further augmented by two external legal
corpora that provide additional legal knowledge for the task: one comprising
statutes and regulations, and the other consisting of a large collection of
past judgment documents. In collaboration with legal professionals, we
establish a comprehensive automated evaluation framework to assess the quality
of generated judgment documents across various dimensions. We evaluate various
baseline approaches, including few-shot in-context learning, fine-tuning, and a
multi-source retrieval-augmented generation (RAG) approach, using both general
and legal-domain LLMs. The experimental results demonstrate that, while RAG
approaches can effectively improve performance in this task, there is still
substantial room for further improvement. All the codes and datasets are
available at: https://github.com/oneal2000/JuDGE.","['cs.CL', 'cs.AI', 'cs.IR']",2501.0317," Microwave shielding is an important technique that can suppress the losses
that arise from collisions of ultracold polar molecules. It has been
instrumental in achieving molecular Bose-Einstein condensation (BEC) for NaCs
[Bigagli et al., Nature 631, 289 (2024)]. We demonstrate that microwave
shielding is universal, in the sense that the 2-body collision properties of
different molecules are very similar when expressed in suitable reduced units
of length and energy. This applies to rate coefficients for inelastic
scattering and loss, to scattering lengths, and to the properties of 2-molecule
bound states. We also explore the small deviations from universality that arise
at very large Rabi frequencies. In general, the collision properties are
near-universal except when the Rabi frequency exceeds a few percent of the
molecular rotational constant. The universality extends to elliptically
polarized microwaves and to combinations of multiple fields. Our results
indicate that the methods that have been used to achieve BEC for NaCs can be
transferred directly to most other polar molecules.","['cond-mat.quant-gas', 'physics.atom-ph']",False,,,,"JuDGE: Benchmarking Judgment Document Generation for Chinese Legal
  System",Universality in the microwave shielding of ultracold polar molecules
neg-d2-868,2025-01-28,,2501.17892," In the pursuit of identifying rare two-particle events within the GADGET II
Time Projection Chamber (TPC), this paper presents a comprehensive approach for
leveraging Convolutional Neural Networks (CNNs) and various data processing
methods. To address the inherent complexities of 3D TPC track reconstructions,
the data is expressed in 2D projections and 1D quantities. This approach
capitalizes on the diverse data modalities of the TPC, allowing for the
efficient representation of the distinct features of the 3D events, with no
loss in topology uniqueness. Additionally, it leverages the computational
efficiency of 2D CNNs and benefits from the extensive availability of
pre-trained models. Given the scarcity of real training data for the rare
events of interest, simulated events are used to train the models to detect
real events. To account for potential distribution shifts when predominantly
depending on simulations, significant perturbations are embedded within the
simulations. This produces a broad parameter space that works to account for
potential physics parameter and detector response variations and uncertainties.
These parameter-varied simulations are used to train sensitive 2D CNN object
detectors. When combined with 1D histogram peak detection algorithms, this
multi-modal detection framework is highly adept at identifying rare,
two-particle events in data taken during experiment 21072 at the Facility for
Rare Isotope Beams (FRIB), demonstrating a 100% recall for events of interest.
We present the methods and outcomes of our investigation and discuss the
potential future applications of these techniques.","['physics.ins-det', 'nucl-ex', 'physics.data-an']",2501.15917," This article presents a novel perspective to model and simulate
reconfigurable intelligent surface (RIS)-assisted communication systems.
Traditional methods in antenna design often rely on array method to simulate,
whereas communication system modeling tends to idealize antenna behavior.
Neither approach sufficiently captures the detailed characteristics of
RIS-assisted communication. To address this limitation, we propose a
comprehensive simulation framework that jointly models RIS antenna design and
the communication process. This framework simulates the entire communication
pipeline, encompassing signal generation, modulation, propagation, RIS-based
radiation, signal reception, alignment, demodulation, decision, and processing.
Using a QPSK-modulated signal for validation, we analyze system performance and
investigate the relationship between bit error rate (BER), aperture fill time,
array size, and baseband symbol frequency. The results indicate that larger
array sizes and higher baseband symbol frequencies exacerbate aperture fill
time effects, leading to increased BER. Furthermore, we examine BER variation
with respect to signal-to-noise ratio (SNR) and propose an optimal
matching-based alignment algorithm, which significantly reduces BER compared to
conventional pilot-based alignment methods. This work demonstrates the entire
process of RIS communication, and reveals the source of bit errors, which
provides valuable insights into the design and performance optimization of
RIS-assisted communication systems.",['physics.app-ph'],False,,,,"Object Detection with Deep Learning for Rare Event Search in the GADGET
  II TPC","RIS Assisted Wireless Communication: Advanced Modeling, Simulation, and
  Analytical Insights"
neg-d2-869,2025-01-23,,2501.14082," Communication between multiple language model (LM) agents has been shown to
scale up the reasoning ability of LMs. While natural language has been the
dominant medium for inter-LM communication, it is not obvious this should be
the standard: not only does natural language communication incur high inference
costs that scale quickly with the number of both agents and messages, but also
the decoding process abstracts away too much rich information that could be
otherwise accessed from the internal activations. In this work, we propose a
simple technique whereby LMs communicate via activations; concretely, we pause
an LM $\textit{B}$'s computation at an intermediate layer, combine its current
activation with another LM $\textit{A}$'s intermediate activation via some
function $\textit{f}$, then pass $\textit{f}$'s output into the next layer of
$\textit{B}$ and continue the forward pass till decoding is complete. This
approach scales up LMs on new tasks with zero additional parameters and data,
and saves a substantial amount of compute over natural language communication.
We test our method with various functional forms $\textit{f}$ on two
experimental setups--multi-player coordination games and reasoning
benchmarks--and find that it achieves up to $27.0\%$ improvement over natural
language communication across datasets with $<$$1/4$ the compute, illustrating
the superiority and robustness of activations as an alternative ""language"" for
communication between LMs.","['cs.CL', 'cs.AI', 'cs.LG']",2502.08298," The integration of Large Language Models (LLMs) into optimization has created
a powerful synergy, opening exciting research opportunities. This paper
investigates how LLMs can enhance existing optimization algorithms. Using their
pre-trained knowledge, we demonstrate their ability to propose innovative
heuristic variations and implementation strategies. To evaluate this, we
applied a non-trivial optimization algorithm, Construct, Merge, Solve and Adapt
(CMSA) -- a hybrid metaheuristic for combinatorial optimization problems that
incorporates a heuristic in the solution construction phase. Our results show
that an alternative heuristic proposed by GPT-4o outperforms the
expert-designed heuristic of CMSA, with the performance gap widening on larger
and denser graphs. Project URL: https://imp-opt-algo-llms.surge.sh/","['cs.AI', 'cs.CL', 'cs.LG', 'cs.SE']",False,,,,Communicating Activations Between Language Model Agents,Improving Existing Optimization Algorithms with LLMs
neg-d2-870,2025-02-05,,2502.03683," Cosmic reionization represents the latest phase transition of the
intergalactic medium (IGM) in the Universe. It has long been debated whether
galaxies or active galactic nuclei (AGNs) are the major source of Lyman
continuum (LyC) photons responsible for reionization. Previous observations
slightly favored galaxies as the major ionizing source. However, the James Webb
Space Telescope (JWST) recently discovered an unexpectedly high density of AGN
candidates at high redshift, which has largely enhanced the influence of AGNs.
Here we derive a definitive upper bound on the AGN contribution to reionization
using the latest JWST data, and conclusively rule out AGNs as the dominant
ionizing source during the epoch of reionization (EoR). We build a sample of
objects (including galaxies and AGNs) in a specific redshift range between 7.15
and 7.75 that has a high completeness. Each object is then decomposed into a
point-source component and an extended component in their rest-frame far-UV
JWST images. Assuming all point-source components are AGNs, we obtain an
absolute upper limit for the density of the AGN population. This fiducial AGN
sample reaches an unprecedentedly low luminosity of $M_{\rm UV} \approx -15$
mag. Based on this sample, we find that AGNs can contribute at most one third
of the LyC photons required to ionize the Universe in this redshift range. Our
result implies that galaxies dominate the ionizing source during the EoR.",['astro-ph.GA'],2502.04879," As platforms increasingly rely on learning algorithms, collectives may form
and seek ways to influence these platforms to align with their own interests.
This can be achieved by coordinated submission of altered data. To evaluate the
potential impact of such behavior, it is essential to understand the
computations that collectives must perform to impact platforms in this way. In
particular, collectives need to make a priori assessments of the effect of the
collective before taking action, as they may face potential risks when
modifying their data. Moreover they need to develop implementable coordination
algorithms based on quantities that can be inferred from observed data. We
develop a framework that provides a theoretical and algorithmic treatment of
these issues and present experimental results in a product evaluation domain.","['stat.ML', 'cs.LG']",False,,,,Ruling out AGNs as the dominant source of cosmic reionization with JWST,Statistical Collusion by Collectives on Learning Platforms
neg-d2-871,2025-02-19,,2502.13967," Image tokenization has enabled major advances in autoregressive image
generation by providing compressed, discrete representations that are more
efficient to process than raw pixels. While traditional approaches use 2D grid
tokenization, recent methods like TiTok have shown that 1D tokenization can
achieve high generation quality by eliminating grid redundancies. However,
these methods typically use a fixed number of tokens and thus cannot adapt to
an image's inherent complexity. We introduce FlexTok, a tokenizer that projects
2D images into variable-length, ordered 1D token sequences. For example, a
256x256 image can be resampled into anywhere from 1 to 256 discrete tokens,
hierarchically and semantically compressing its information. By training a
rectified flow model as the decoder and using nested dropout, FlexTok produces
plausible reconstructions regardless of the chosen token sequence length. We
evaluate our approach in an autoregressive generation setting using a simple
GPT-style Transformer. On ImageNet, this approach achieves an FID<2 across 8 to
128 tokens, outperforming TiTok and matching state-of-the-art methods with far
fewer tokens. We further extend the model to support to text-conditioned image
generation and examine how FlexTok relates to traditional 2D tokenization. A
key finding is that FlexTok enables next-token prediction to describe images in
a coarse-to-fine ""visual vocabulary"", and that the number of tokens to generate
depends on the complexity of the generation task.","['cs.CV', 'cs.LG']",2502.02212," We address the problem of solving a system of linear equations via the
Quantum Singular Value Transformation (QSVT). One drawback of the QSVT
algorithm is that it requires huge quantum resources if we want to achieve an
acceptable accuracy. To reduce the quantum cost, we propose a hybrid
quantum-classical algorithm that improves the accuracy and reduces the cost of
the QSVT by adding iterative refinement in mixed-precision A first quantum
solution is computed using the QSVT, in low precision, and then refined in
higher precision until we get a satisfactory accuracy. For this solver, we
present an error and complexity analysis, and first experiments using the
quantum software stack myQLM.",['quant-ph'],False,,,,FlexTok: Resampling Images into 1D Token Sequences of Flexible Length,A mixed-precision quantum-classical algorithm for solving linear systems
neg-d2-872,2025-01-28,,2501.17356," Watermarking, the practice of embedding imperceptible information into media
such as images, videos, audio, and text, is essential for intellectual property
protection, content provenance and attribution. The growing complexity of
digital ecosystems necessitates watermarks for different uses to be embedded in
the same media. However, to detect and decode all watermarks, they need to
coexist well with one another. We perform the first study of coexistence of
deep image watermarking methods and, contrary to intuition, we find that
various open-source watermarks can coexist with only minor impacts on image
quality and decoding robustness. The coexistence of watermarks also opens the
avenue for ensembling watermarking methods. We show how ensembling can increase
the overall message capacity and enable new trade-offs between capacity,
accuracy, robustness and image quality, without needing to retrain the base
models.","['cs.CV', 'cs.AI', 'cs.CY']",2503.08661," This paper proposes a task-oriented co-design framework that integrates
communication, computing, and control to address the key challenges of
bandwidth limitations, noise interference, and latency in mission-critical
industrial Cyber-Physical Systems (CPS). To improve communication efficiency
and robustness, we design a task-oriented Joint Source-Channel Coding (JSCC)
using Information Bottleneck (IB) to enhance data transmission efficiency by
prioritizing task-specific information. To mitigate the perceived End-to-End
(E2E) delays, we develop a Delay-Aware Trajectory-Guided Control Prediction
(DTCP) strategy that integrates trajectory planning with control prediction,
predicting commands based on E2E delay. Moreover, the DTCP is co-designed with
task-oriented JSCC, focusing on transmitting task-specific information for
timely and reliable autonomous driving. Experimental results in the CARLA
simulator demonstrate that, under an E2E delay of 1 second (20 time slots), the
proposed framework achieves a driving score of 48.12, which is 31.59 points
higher than using Better Portable Graphics (BPG) while reducing bandwidth usage
by 99.19%.","['cs.IT', 'cs.CV', 'eess.IV', 'math.IT']",False,,,,On the Coexistence and Ensembling of Watermarks,"Task-Oriented Co-Design of Communication, Computing, and Control for
  Edge-Enabled Industrial Cyber-Physical Systems"
neg-d2-873,2025-03-05,,2503.0399," Accurately quantifying air-sea fluxes is important for understanding air-sea
interactions and improving coupled weather and climate systems. This study
introduces a probabilistic framework to represent the highly variable nature of
air-sea fluxes, which is missing in deterministic bulk algorithms. Assuming
Gaussian distributions conditioned on the input variables, we use artificial
neural networks and eddy-covariance measurement data to estimate the mean and
variance by minimizing negative log-likelihood loss. The trained neural
networks provide alternative mean flux estimates to existing bulk algorithms,
and quantify the uncertainty around the mean estimates. Stochastic
parameterization of air-sea turbulent fluxes can be constructed by sampling
from the predicted distributions. Tests in a single-column forced upper-ocean
model suggest that changes in flux algorithms influence sea surface temperature
and mixed layer depth seasonally. The ensemble spread in stochastic runs is
most pronounced during spring restratification.","['physics.ao-ph', 'cs.LG', 'stat.AP', 'stat.ML']",2501.15719," Following a method introduced by Thomas-Vasquez and developed by Grundman, we
prove that many Hilbert modular threefolds of arithmetic genus $0$ and $1$ are
of general type, and that some are of nonnegative Kodaira dimension. The new
ingredient is a detailed study of the geometry and combinatorics of totally
positive integral elements $x$ of a fractional ideal $I$ in a totally real
number field $K$ with the property that $\mathop{\mathrm{tr}} xy <
\mathop{\mathrm{min}} I \mathop{\mathrm{tr}} y$ for some $y \gg 0 \in K$.","['math.NT', 'math.AG']",False,,,,Data-Driven Probabilistic Air-Sea Flux Parameterization,The Kodaira dimension of Hilbert modular threefolds
neg-d2-874,2025-02-16,,2502.14891," Collaborative 3D object detection holds significant importance in the field
of autonomous driving, as it greatly enhances the perception capabilities of
each individual agent by facilitating information exchange among multiple
agents. However, in practice, due to pose estimation errors and time delays,
the fusion of information across agents often results in feature
representations with spatial and temporal noise, leading to detection errors.
Diffusion models naturally have the ability to denoise noisy samples to the
ideal data, which motivates us to explore the use of diffusion models to
address the noise problem between multi-agent systems. In this work, we propose
CoDiff, a novel robust collaborative perception framework that leverages the
potential of diffusion models to generate more comprehensive and clearer
feature representations. To the best of our knowledge, this is the first work
to apply diffusion models to multi-agent collaborative perception.
Specifically, we project high-dimensional feature map into the latent space of
a powerful pre-trained autoencoder. Within this space, individual agent
information serves as a condition to guide the diffusion model's sampling. This
process denoises coarse feature maps and progressively refines the fused
features. Experimental study on both simulated and real-world datasets
demonstrates that the proposed framework CoDiff consistently outperforms
existing relevant methods in terms of the collaborative object detection
performance, and exhibits highly desired robustness when the pose and delay
information of agents is with high-level noise.","['cs.CV', 'cs.AI']",2501.07166," Combinatorial medication recommendation(CMR) is a fundamental task of
healthcare, which offers opportunities for clinical physicians to provide more
precise prescriptions for patients with intricate health conditions,
particularly in the scenarios of long-term medical care. Previous research
efforts have sought to extract meaningful information from electronic health
records (EHRs) to facilitate combinatorial medication recommendations. Existing
learning-based approaches further consider the chemical structures of
medications, but ignore the textual medication descriptions in which the
functionalities are clearly described. Furthermore, the textual knowledge
derived from the EHRs of patients remains largely underutilized. To address
these issues, we introduce the Natural Language-Assisted Multi-modal Medication
Recommendation(NLA-MMR), a multi-modal alignment framework designed to learn
knowledge from the patient view and medication view jointly. Specifically,
NLA-MMR formulates CMR as an alignment problem from patient and medication
modalities. In this vein, we employ pretrained language models(PLMs) to extract
in-domain knowledge regarding patients and medications, serving as the
foundational representation for both modalities. In the medication modality, we
exploit both chemical structures and textual descriptions to create medication
representations. In the patient modality, we generate the patient
representations based on textual descriptions of diagnosis, procedure, and
symptom. Extensive experiments conducted on three publicly accessible datasets
demonstrate that NLA-MMR achieves new state-of-the-art performance, with a
notable average improvement of 4.72% in Jaccard score. Our source code is
publicly available on https://github.com/jtan1102/NLA-MMR_CIKM_2024.",['cs.AI'],False,,,,"CoDiff: Conditional Diffusion Model for Collaborative 3D Object
  Detection",Natural Language-Assisted Multi-modal Medication Recommendation
neg-d2-875,2025-03-17,,2503.13677," Value-oriented forecasts for two-stage power system operational problems have
been demonstrated to reduce cost, but prove to be computationally challenging
for large-scale systems because the underlying optimization problem must be
internalized into the forecast model training. Therefore, existing approaches
typically scale poorly in the usable training data or require relaxations of
the underlying optimization. This paper presents a method for value-oriented
forecast combinations using progressive hedging, which unlocks high-fidelity,
at-scale models and large-scale datasets in training. We also derive a direct
one-shot training model for reference and study how different modifications of
the training model impact the solution quality. Our method reduces operation
cost by 1.8% on average and trains forecast combinations for a 2736-bus test
system with one year of data within 20 hours.","['math.OC', 'cs.SY', 'eess.SY']",2503.04312," The magneto-optical effects (MOEs), as a fundamental physical phenomenon, can
reveal the electronic structures of materials. The related probing methods are
widely used in the study of magnetic materials. However, space-time inversion
($\mathcal{PT}$) symmetric antiferromagnets were previously believed to be
magneto-optically inactive. Here, we point out that this traditional
understanding is incorrect. Based on our generic formulas and symmetry
analysis, we find that in $\mathcal{PT}$-symmetric antiferromagnets, it is the
quantum metric, i.e., the real part of the quantum geometry, that induces MOEs.
Combining a tight-binding model and first-principles calculations, we confirm
this observation by showing MOEs in the $\mathcal{PT}$-symmetric
antiferromagnet. Our work demonstrates that $\mathcal{PT}$-symmetric
antiferromagnets previously thought to lack MOEs can indeed exhibit MOEs and
greatly broaden the research on MOEs.","['cond-mat.mtrl-sci', 'cond-mat.mes-hall', 'physics.comp-ph']",False,,,,Value-Oriented Forecast Combinations for Unit Commitment,"Quantum metric induced magneto-optical effects in
  $\mathcal{PT}$-symmetric antiferromagnets"
neg-d2-876,2025-03-17,,2503.12937," Recent studies generally enhance MLLMs' reasoning capabilities via supervised
fine-tuning on high-quality chain-of-thought reasoning data, which often leads
models to merely imitate successful reasoning paths without understanding what
the wrong reasoning paths are. In this work, we aim to enhance the MLLMs'
reasoning ability beyond passively imitating positive reasoning paths. To this
end, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new
online reinforcement learning framework that enables MLLMs to self-improve
reasoning ability via simple, effective and dense step-wise rewarding.
Specifically, StepGRPO introduces two novel rule-based reasoning rewards:
Step-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity
Reward (StepRVR). StepRAR rewards the reasoning paths that contain necessary
intermediate reasoning steps via a soft key-step matching technique, while
StepRAR rewards reasoning paths that follow a well-structured and logically
consistent reasoning process through a reasoning completeness and logic
evaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series
of MLLMs with outstanding capabilities in step-by-step reasoning. Extensive
experiments over 8 benchmarks demonstrate the superiority of our methods.","['cs.AI', 'cs.CL', 'cs.CV', 'cs.LG']",2502.14798," X
  = S, Se): A Sustainable Alternative in Chalcogenide Perovskites; The quest for environmentally benign and stable optoelectronic materials has
intensified, and chalcogenide perovskites (CPs) have emerged as promising
candidates owing to their non-toxic composition, stability, small bandgaps,
large absorption coefficients. However, a detailed theoretical study of
excitonic and polaronic properties of these materials remains underexplored due
to high computational demands. Herein, we present a comprehensive theoretical
investigation of Germanium-based CPs, AGeX$_{3}$ (A = Ca, Sr, Ba; X = S, Se),
which adopt distorted perovskite structures (\beta-phase) with an orthorhombic
crystal structure (space group : Pnma) by utilizing state-of-the-art density
functional theory (DFT), density functional perturbation theory (DFPT), and
many-body perturbation theory (GW and Bethe-Salpeter equation). Our
calculations reveal that these materials are thermodynamically and mechanically
stable, with the bandgaps calculated using G$_{0}$W$_{0}$@PBE ranging from
0.646 to 2.001 eV - suitable for optoelectronic devices. We analyze the ionic
and electronic contributions to dielectric screening using DFPT and BSE
methods, finding that the electronic component dominates. The exciton binding
energies range from 0.03 to 73.63 meV, indicating efficient exciton
dissociation under ambient conditions. Additionally, these perovskites exhibit
low to high polaronic mobilities (1.67-167.65 cm$^{2}$V$^{-1}$s$^{-1}$),
exceeding many lead-free CPs and halide perovskites due to reduced
carrier-phonon interactions. The unique combination of wide tunable bandgaps,
low exciton binding energies, and enhanced charge-carrier mobility highlights
AGeX$_{3}$ as a potential material for next-generation optoelectronic
applications. These compounds are stable, high-performing, and eco-friendly,
showing great promise for experimental realization and device integration.",['cond-mat.mtrl-sci'],False,,,,"R1-VL: Learning to Reason with Multimodal Large Language Models via
  Step-wise Group Relative Policy Optimization","Unlocking the Optoelectronic Potential of AGeX$_{3}$ (A = Ca, Sr, Ba"
neg-d2-877,2025-03-06,,2503.05021," Large Language Models (LLMs) are vulnerable to jailbreak attacks that exploit
weaknesses in traditional safety alignment, which often relies on rigid refusal
heuristics or representation engineering to block harmful outputs. While they
are effective for direct adversarial attacks, they fall short of broader safety
challenges requiring nuanced, context-aware decision-making. To address this,
we propose Reasoning-enhanced Finetuning for interpretable LLM Safety
(Rational), a novel framework that trains models to engage in explicit safe
reasoning before response. Fine-tuned models leverage the extensive pretraining
knowledge in self-generated reasoning to bootstrap their own safety through
structured reasoning, internalizing context-sensitive decision-making. Our
findings suggest that safety extends beyond refusal, requiring context
awareness for more robust, interpretable, and adaptive responses. Reasoning is
not only a core capability of LLMs but also a fundamental mechanism for LLM
safety. Rational employs reasoning-enhanced fine-tuning, allowing it to reject
harmful prompts while providing meaningful and context-aware responses in
complex scenarios.","['cs.CL', 'cs.CR']",2503.01815," We explain the relation between the Witt class and the universal
equicommutative class for PSL(2,K). We discuss an analogue of the Milnor-Wood
inequality.","['math.KT', 'math.GR']",False,,,,"Safety is Not Only About Refusal: Reasoning-Enhanced Fine-tuning for
  Interpretable LLM Safety",Tautological characteristic classes III: the Witt class for PSL(2)
neg-d2-878,2025-02-09,,2502.06082," The COVID-19 pandemic underscored the urgent need for fair and effective
allocation of scarce resources, from hospital beds to vaccine distribution. In
this paper, we study a healthcare rationing problem where identical units of a
resource are divided into different categories, and agents are assigned based
on priority rankings. % We first introduce a simple and efficient algorithm
that satisfies four fundamental axioms critical to practical applications:
eligible compliance, non-wastefulness, respect for priorities, and maximum
cardinality. This new algorithm is not only conceptually simpler but also
computationally faster than the Reverse Rejecting rules proposed in recent
work. % We then extend our analysis to a more general sequential setting, where
categories can be processed both sequentially and simultaneously. For this
broader framework, we introduce a novel algorithm that preserves the four
fundamental axioms while achieving additional desirable properties that
existing rules fail to satisfy. Furthermore, we prove that when a strict
precedence order over categories is imposed, this rule is the unique mechanism
that satisfies these properties.",['cs.GT'],2501.15719," Following a method introduced by Thomas-Vasquez and developed by Grundman, we
prove that many Hilbert modular threefolds of arithmetic genus $0$ and $1$ are
of general type, and that some are of nonnegative Kodaira dimension. The new
ingredient is a detailed study of the geometry and combinatorics of totally
positive integral elements $x$ of a fractional ideal $I$ in a totally real
number field $K$ with the property that $\mathop{\mathrm{tr}} xy <
\mathop{\mathrm{min}} I \mathop{\mathrm{tr}} y$ for some $y \gg 0 \in K$.","['math.NT', 'math.AG']",False,,,,A Fair and Optimal Approach to Sequential Health Rationing,The Kodaira dimension of Hilbert modular threefolds
neg-d2-879,2025-03-13,,2503.10638," Classifier-free guidance has become a staple for conditional generation with
denoising diffusion models. However, a comprehensive understanding of
classifier-free guidance is still missing. In this work, we carry out an
empirical study to provide a fresh perspective on classifier-free guidance.
Concretely, instead of solely focusing on classifier-free guidance, we trace
back to the root, i.e., classifier guidance, pinpoint the key assumption for
the derivation, and conduct a systematic study to understand the role of the
classifier. We find that both classifier guidance and classifier-free guidance
achieve conditional generation by pushing the denoising diffusion trajectories
away from decision boundaries, i.e., areas where conditional information is
usually entangled and is hard to learn. Based on this classifier-centric
understanding, we propose a generic postprocessing step built upon
flow-matching to shrink the gap between the learned distribution for a
pre-trained denoising diffusion model and the real data distribution, majorly
around the decision boundaries. Experiments on various datasets verify the
effectiveness of the proposed approach.","['cs.CV', 'cs.AI', 'cs.LG']",2501.14087," We consider a generating set of reparametrization invariants that can be
constructed from the couplings and masses entering the scalar potential of the
general Two-Higgs-Doublet Model (2HDM). Being independent of higgs-basis
rotations, they generate a polynomial ring of basis invariants that represent
the physical content of the model. Ignoring for the moment gauge and Yukawa
interactions, we derive six-loop renormalization group equations (RGE) for all
the invariants entering the set. We do not compute a single Feynman diagram but
rely heavily on the general RGE results for scalar theories. We use linear
algebra together with techniques from Invariant Theory. The latter not only
allow one to compute the number of linearly independent invariants entering
beta functions at a certain loop order (via Hilbert series) but also provide a
convenient tool for dealing with polynomial relations (so-called syzygies)
between invariants from the generating set.",['hep-ph'],False,,,,"Studying Classifier(-Free) Guidance From a Classifier-Centric
  Perspective","On the scalar sector of 2HDM: ring of basis invariants, syzygies, and
  six-loop renormalization-group equations"
neg-d2-880,2025-02-24,,2502.17005," Recent experimental evidence for the charge-$6e$ condensed phase in kagome
superconductors has generated significant interest. We investigate the
unconventional superconductivity in the kagome superconductor
$\mathrm{CsV_3Sb_5}$, focusing on the emergence of charge-$6e$
superconductivity (SC) at temperatures higher than the conventional charge-$2e$
SC state. By modeling the phase coherence of the SC order parameter using a
frustrated antiferromagnetic XY model on an emergent kagome lattice, we show
that the condensation of fractional vortices with $1/3$ vorticity stabilizes
phase coherence in $\exp(i3\theta)$, giving rise to the charge-$6e$ SC state.
Using a tensor network approach tailored for frustrated spin systems, we
identify a Berezinskii-Kosterlitz-Thouless transition at $T_c/J \simeq 0.075$,
where the unbinding of $1/3$ fractional vortex-antivortex pairs transforms the
system from the charge-$6e$ SC phase to the normal phase. Below $T_c$, the
$1/3$ fractional vortex correlations exhibit power-law decay, while the integer
vortex correlations decay exponentially, reflecting the dominance of
charge-$6e$ SC in the absence of charge-$2e$ SC. Our results provide a
theoretical understanding of the charge-$6e$ SC in two-dimensional kagome
superconductors, emphasizing the interplay between fractional vortices,
frustration, and topology in stabilizing this exotic SC phase.","['cond-mat.supr-con', 'cond-mat.mtrl-sci', 'cond-mat.stat-mech', 'cond-mat.str-el']",2503.12966," Score-based generative models achieve state-of-the-art sampling performance
by denoising a distribution perturbed by Gaussian noise. In this paper, we
focus on a single deterministic denoising step, and compare the optimal
denoiser for the quadratic loss, we name ''full-denoising'', to the alternative
''half-denoising'' introduced by Hyv{\""a}rinen (2024). We show that looking at
the performances in term of distance between distribution tells a more nuanced
story, with different assumptions on the data leading to very different
conclusions. We prove that half-denoising is better than full-denoising for
regular enough densities, while full-denoising is better for singular densities
such as mixtures of Dirac measures or densities supported on a low-dimensional
subspace. In the latter case, we prove that full-denoising can alleviate the
curse of dimensionality under a linear manifold hypothesis.","['cs.LG', 'stat.ML']",False,,,,"Phase coherence of charge-$6e$ superconductors via a frustrated Kagome
  XY antiferromagnet","Optimal Denoising in Score-Based Generative Models: The Role of Data
  Regularity"
neg-d2-881,2025-03-23,,2503.18075," Gaussian variational approximations are widely used for summarizing posterior
distributions in Bayesian models, especially in high-dimensional settings.
However, a drawback of such approximations is the inability to capture skewness
or more complex features of the posterior. Recent work suggests applying
skewness corrections to existing Gaussian or other symmetric approximations to
address this limitation. We propose to incorporate the skewness correction into
the definition of an approximating variational family. We consider
approximating the posterior for hierarchical models, in which there are
``global'' and ``local'' parameters. A baseline variational approximation is
defined as the product of a Gaussian marginal posterior for global parameters
and a Gaussian conditional posterior for local parameters given the global
ones. Skewness corrections are then considered. The adjustment of the
conditional posterior term for local variables is adaptive to the global
parameter value. Optimization of baseline variational parameters is performed
jointly with the skewness correction. Our approach allows the location, scale
and skewness to be captured separately, without using additional parameters for
skewness adjustments. The proposed method substantially improves accuracy for
only a modest increase in computational cost compared to state-of-the-art
Gaussian approximations. Good performance is demonstrated in generalized linear
mixed models and multinomial logit discrete choice models.",['stat.ME'],2503.12982," Cooperative perception can increase the view field and decrease the occlusion
of an ego vehicle, hence improving the perception performance and safety of
autonomous driving. Despite the success of previous works on cooperative object
detection, they mostly operate on dense Bird's Eye View (BEV) feature maps,
which are computationally demanding and can hardly be extended to long-range
detection problems. More efficient fully sparse frameworks are rarely explored.
In this work, we design a fully sparse framework, SparseAlign, with three key
features: an enhanced sparse 3D backbone, a query-based temporal context
learning module, and a robust detection head specially tailored for sparse
features. Extensive experimental results on both OPV2V and DairV2X datasets
show that our framework, despite its sparsity, outperforms the state of the art
with less communication bandwidth requirements. In addition, experiments on the
OPV2Vt and DairV2Xt datasets for time-aligned cooperative object detection also
show a significant performance gain compared to the baseline works.",['cs.CV'],False,,,,"Variational inference for hierarchical models with conditional scale and
  skewness corrections",SparseAlign: A Fully Sparse Framework for Cooperative Object Detection
neg-d2-882,2025-02-03,,2502.01824," Digital quantum simulation has emerged as a powerful approach to investigate
complex quantum systems using digital quantum computers. Many-particle bosonic
systems and intricate optical experimental setups pose significant challenges
for classical simulation methods. Utilizing a recently developed formalism that
maps bosonic operators to Pauli operators via the Gray code, we digitally
simulate interferometric variants of Afshar's experiment on IBM's quantum
computers. We investigate the analogous experiments of Unruh and Pessoa
J\'unior, exploring discussions on the apparent violation of Bohr's
complementarity principle when considering the entire experimental setup.
Furthermore, we analyze these experiments within the framework of an updated
quantum complementarity principle, which applies to specific quantum state
preparations and remains consistent with the foundational principles of quantum
mechanics. Our quantum computer demonstration results are in good agreement
with the theoretical predictions and underscore the potential of quantum
computers as effective simulators for bosonic systems.",['quant-ph'],2502.01219," Dynamical systems can be coupled in a manner that is designed to drive the
resulting dynamics onto a specified lower dimensional submanifold in the phase
space of the combined system. On the submanifold, the variables of the two
systems have a well-defined unique functional relationship. This process can
thus be viewed as a control technique that ensures generalized synchronization.
Depending on the nature of the dynamical systems and the specified submanifold,
different coupling functions can be derived in order to achieve a desired
control objective. We discuss the circuit implementations of this strategy in
representative examples of coupled chaotic dynamical systems, namely Lorenz
oscillators",['nlin.CD'],False,,,,"Digital quantum simulation of bosonic systems and quantum
  complementarity",Control Strategy for Generalized Synchrony in Coupled Dynamical Systems
neg-d2-883,2025-01-04,,2501.02235," The field of visually-rich document understanding, which involves interacting
with visually-rich documents (whether scanned or born-digital), is rapidly
evolving and still lacks consensus on several key aspects of the processing
pipeline. In this work, we provide a comprehensive overview of state-of-the-art
approaches, emphasizing their strengths and limitations, pointing out the main
challenges in the field, and proposing promising research directions.",['cs.CL'],2501.11126," Multi-antenna coded caching (CC) with multicast beamforming typically relies
on a complex successive interference cancellation (SIC) structure to decode a
superposition of multiple streams received by each user. Signal-level CC
schemes require the regeneration and cancellation of interfering signals at the
physical layer of each receiver, which complicates practical implementations.
To address this, we propose a bit-level multicast scheduling scheme enabling
linear, SIC-free decoding of parallel streams by repeatedly transmitting data
terms with linearly independent coefficients. Two reference strategies and a
novel sparse strategy are considered for constructing the coefficient matrix.
The reference cases include the random strategy, which lacks control over
matrix construction, and the equal-distant strategy, which balances users'
interference and data terms equally. In contrast, the sparse strategy minimizes
the number of multicast streams transmitted in parallel during each interval.
This approach simplifies both the decoding process and the beamforming design
by decoupling the desired data terms for each user and reducing the number of
SINR constraints, respectively. To further enhance the symmetric rate, a
successive projection algorithm is applied to exploit channel properties and
optimize user ordering. With the coefficient matrix and optimized user ordering
in place, multicast beamformers are devised to aggregate desired data from
relevant multicast streams. Numerical simulations validate the effectiveness of
the sparse strategy and user scheduling, demonstrating significant gains in
symmetric rate.","['cs.IT', 'math.IT']",False,,,,"Survey on Question Answering over Visually Rich Documents: Methods,
  Challenges, and Trends",SIC-free Multicast Scheduling for Multi-antenna Coded Caching
neg-d2-884,2025-01-29,,2501.18652," The flow of information within many-body systems is a fundamental feature of
physical interaction. Given an underlying classical physics model for the
interaction between a particle and its environment, we give meaning to and
quantify the information passed between them over time. We show that the
maximum information exchange rate is proportional to the ratio of
inter-particle energy flow and initial particle energy -- a sort of
signal-to-noise ratio. In addition, a single time-point (as opposed to
trajectory) observability relation emerges.",['cond-mat.stat-mech'],2502.11528," Large Language Models (LLMs) excel in handling general knowledge tasks, yet
they struggle with user-specific personalization, such as understanding
individual emotions, writing styles, and preferences. Personalized Large
Language Models (PLLMs) tackle these challenges by leveraging individual user
data, such as user profiles, historical dialogues, content, and interactions,
to deliver responses that are contextually relevant and tailored to each user's
specific needs. This is a highly valuable research topic, as PLLMs can
significantly enhance user satisfaction and have broad applications in
conversational agents, recommendation systems, emotion recognition, medical
assistants, and more. This survey reviews recent advancements in PLLMs from
three technical perspectives: prompting for personalized context (input level),
finetuning for personalized adapters (model level), and alignment for
personalized preferences (objective level). To provide deeper insights, we also
discuss current limitations and outline several promising directions for future
research. Updated information about this survey can be found at the
https://github.com/JiahongLiu21/Awesome-Personalized-Large-Language-Models.",['cs.AI'],False,,,,Classical Information Exchange Between Particles,"A Survey of Personalized Large Language Models: Progress and Future
  Directions"
neg-d2-885,2025-02-05,,2502.03592," Maintaining the integrity of solar power plants is a vital component in
dealing with the current climate crisis. This process begins with analysts
creating a detailed map of a plant with the coordinates of every solar panel,
making it possible to quickly locate and mitigate potential faulty solar
panels. However, this task is extremely tedious and is not scalable for the
ever increasing capacity of solar power across the globe. Therefore, we propose
an end-to-end deep learning framework for detecting individual solar panels
using a rotated object detection architecture. We evaluate our approach on a
diverse dataset of solar power plants collected from across the United States
and report a mAP score of 83.3%.",['cs.CV'],2503.00395," We demonstrate dynamic pressure tuning (0-6.6 GPa) of layer-hybridized
excitons in AB-stacked trilayer WSe$_2$ via diamond-anvil-cell-integrated
reflectance spectroscopy. Pressure-controlled interlayer coupling manifests in
enhanced energy-level anti-crossings and oscillator strength redistribution,
with Stark shift analysis revealing a characteristic dipole moment reduction of
11%. Notably, the hybridization strength between the intra- and interlayer
excitons triples from $\sim$10 meV to above $\sim$30 meV, exhibiting a
near-linear scaling of 3.5$\pm$0.2 meV/GPa. Spectral density simulations
resolve four distinct components, i.e., intralayer ground/excited and
interlayer ground/excited excitons, with their relative weights transitioning
from one component dominant to strongly hybridized at higher pressures. Our
findings highlight the potential for controlling excitonic properties and
engineering novel optoelectronic devices through interlayer compression.",['cond-mat.mtrl-sci'],False,,,,Solar Panel Mapping via Oriented Object Detection,Pressure Tuning of Layer-hybridized Excitons in Trilayer WSe2
neg-d2-886,2025-01-06,,2501.03087," We derive a class of multi-species aggregation-diffusion systems from
stochastic interacting particle systems via relative entropy method with
quantitative bounds. We show an algebraic $L^1$-convergence result using
moderately interacting particle systems approximating attractive/repulsive
singular potentials up to Newtonian/Coulomb singularities without additional
cut-off on the particle level. The first step is to make use of the relative
entropy between the joint distribution of the particle system and an
approximated limiting aggregation-diffusion system. A crucial argument in the
proof is to show convergence in probability by a stopping time argument. The
second step is to obtain a quantitative convergence rate to the limiting
aggregation-diffusion system from the approximated PDE system. This is shown by
evaluating a combination of relative entropy and $L^2$-distance.","['math.PR', 'math.AP']",2502.09337," This thesis is an exposition of the author's contribution on effective
descent morphisms in various categories of generalized categorical structures.
It consists of: Chapter 1, where an elementary description of descent theory
and the content of each remaining chapter is provided, supplemented with
references; Chapter 2, consisting of various descent theoretical definitions
and results employed in the remainder of this work; four chapters, each
corresponding to an article written by the author during the period of his PhD
studies.",['math.CT'],False,,,,"Propagation of chaos for multi-species moderately interacting particle
  systems up to Newtonian singularity",Some aspects of descent theory and applications
neg-d2-887,2025-03-07,,2503.05581," A planet's albedo is a fundamental property that sets its energy budget by
dictating the fraction of incident radiation absorbed versus reflected back to
space. Generally, optical eclipse observations have revealed the majority of
hot, giant planets to have low albedos, indicating dayside atmospheres
dominated by absorption instead of reflection. However, there are several
exceptions to this rule, including the ultra-hot-Neptune LTT 9779b, which have
been found to have high geometric albedos. We observed four eclipses of LTT
9779b with the G280 grism of the Hubble Space Telescope's WFC3 UVIS mode;
targeting the scattering signatures of the cloud condensate species causing the
planet's elevated reflectivity. However, we do not definitively detect the
planet's eclipse in our observations, with injection-recovery tests yielding a
3-$\sigma$ upper limit of 113 ppm on the eclipse depth of LTT 9779b in the
0.2-0.8$\mu$m waveband. We create reflectance spectrum grids for LTT 9779b's
dayside using VIRGA/PICASO and compare to our UVIS limit, as well as previously
published CHEOPS and TESS eclipse photometry. We find that silicate condensates
are best able to explain LTT 9779b's highly-reflective dayside. Our forward
model grids only enable weak constraints on vertical mixing efficiency, and
suggest that, regardless of their particular composition, the clouds are likely
composed of smaller and more reflective particles. Our work facilitates a
deeper understanding of the reflectance properties of LTT 9779b as well as the
UVIS spectroscopic mode itself, which will remain the community's primary
access to UV wavelengths until next-generation telescopes like the Habitable
Worlds Observatory.",['astro-ph.EP'],2503.13997," We introduce a novel approach that utilizes neutrino events from the off-axis
near detector to investigate the beam profile in long-baseline neutrino
experiments. Understanding the dynamics of the neutrino beam is crucial for
improving the precision of neutrino oscillation measurements. We demonstrate
that certain observables related to the azimuthal angle of the neutrino
direction are useful for determining the average neutrino production point from
experimental data, providing a valuable cross-check against Monte Carlo
simulations. Additionally, these observables can help identify potential
alignment issues between the detector and the decay volume. In future neutrino
experiments with significantly higher statistics, these observables will become
essential to ensure the accuracy and stability of the beam profile.","['hep-ex', 'physics.ins-det']",False,,,,"Constraining the Scattered Light properties of LTT 9779 b Using HST/WFC3
  UVIS","Characterizing Beam Profiles in Accelerator Neutrino Experiments through
  Off-Axis Neutrino Interactions"
neg-d2-888,2025-02-16,,2502.11246," Memes present unique moderation challenges due to their subtle, multimodal
interplay of images, text, and social context. Standard systems relying
predominantly on explicit textual cues often overlook harmful content
camouflaged by irony, symbolism, or cultural references. To address this gap,
we introduce MemeSense, an adaptive in-context learning framework that fuses
social commonsense reasoning with visually and semantically related reference
examples. By encoding crucial task information into a learnable cognitive shift
vector, MemeSense effectively balances lexical, visual, and ethical
considerations, enabling precise yet context-aware meme intervention. Extensive
evaluations on a curated set of implicitly harmful memes demonstrate that
MemeSense substantially outperforms strong baselines, paving the way for safer
online communities. Code and data available at:
https://github.com/sayantan11995/MemeSense","['cs.IR', 'cs.CL', 'cs.CY']",2501.0944," In this paper, we present a class of systems of non-local conservation laws
in one space-dimension incorporating time delay, which can be used to
investigate the interaction between autonomous and human-driven vehicles, each
characterized by a different reaction time and interaction range. We construct
approximate solutions using a Hilliges-Weidlich scheme and we provide uniform L
$\infty$ and BV estimates which ensure the convergence of the scheme, thus
obtaining existence of entropy weak solutions of bounded variation. Uniqueness
follows from an L 1 stability result derived from the entropy condition.
Additionally, we provide numerical simulations to illustrate applications to
mixed autonomous / human-driven traffic flow modeling. In particular, we show
that the presence of autonomous vehicles improves overall traffic flow and
stability.","['math.AP', 'cs.NA', 'math.NA']",False,,,,"MemeSense: An Adaptive In-Context Framework for Social Commonsense
  Driven Meme Moderation","A multi-class non-local macroscopic model with time delay for mixed
  autonomous / human-driven traffic"
neg-d2-889,2025-01-24,,2501.14716," The spectral theory on the $S$-spectrum originated to give quaternionic
quantum mechanics a precise mathematical foundation and as a spectral theory
for linear operators in vector analysis.
  This theory has proven to be significantly more general than initially
anticipated, naturally extending to fully Clifford operators and revealing
unexpected connections with the spectral theory based on the monogenic
spectrum, developed over forty years ago by A. McIntosh and collaborators.
  In recent years, we have combined slice hyperholomorphic functions with the
Fueter-Sce mapping theorem, also called Fueter-Sce extension theorem, to
broaden the class of functions and operators to which the theory can be
applied. This generalization has led to the definition of what we call the {\em
fine structures on the $S$-spectrum}, consisting of classes of functions that
admit an integral representation and their associated functional calculi.
  In this paper, we focus on the fine structures within the Clifford algebra
setting, particularly addressing polyharmonic functions, polyanalytic
functions, holomorphic Cliffordian functions and their associated functional
calculi defined via integral representation formulas.
  Moreover, we demonstrate that the monogenic functional calculus, defined via
the monogenic Cauchy formula, and the $F$-functional calculus of the fine
structures, defined via the Fueter-Sce mapping theorem in integral form, yield
the same operator.",['math.FA'],2501.18056," Query rewriting (QR) is a critical technique in e-commerce search, addressing
the lexical gap between user queries and product descriptions to enhance search
performance. Existing QR approaches typically fall into two categories:
discriminative models and generative methods leveraging large language models
(LLMs). Discriminative models often struggle with natural language
understanding and offer limited flexibility in rewriting, while generative
LLMs, despite producing high-quality rewrites, face high inference latency and
cost in online settings. These limitations force offline deployment, making
them vulnerable to issues like information staleness and semantic drift. To
overcome these challenges, we propose a novel hybrid pipeline for QR that
balances efficiency and effectiveness. Our approach combines offline knowledge
distillation to create a lightweight but efficient student model with online
reinforcement learning (RL) to refine query rewriting dynamically using
real-time feedback. A key innovation is the use of LLMs as simulated human
feedback, enabling scalable reward signals and cost-effective evaluation
without manual annotations. Experimental results on Amazon ESCI dataset
demonstrate significant improvements in query relevance, diversity, and
adaptability, as well as positive feedback from the LLM simulation. This work
contributes to advancing LLM capabilities for domain-specific applications,
offering a robust solution for dynamic and complex e-commerce search
environments.",['cs.IR'],False,,,,"Functions and operators of the polyharmonic and polyanalytic Clifford
  fine structures on the $S$-spectrum","RL-based Query Rewriting with Distilled LLM for online E-Commerce
  Systems"
neg-d2-890,2025-01-21,,2501.12349," Robust and scalable function evaluation at any arbitrary point in the
finite/spectral element mesh is required for querying the partial differential
equation solution at points of interest, comparison of solution between
different meshes, and Lagrangian particle tracking. This is a challenging
problem, particularly for high-order unstructured meshes partitioned in
parallel with MPI, as it requires identifying the element that overlaps a given
point and computing the corresponding reference space coordinates. We present a
robust and efficient technique for general field evaluation in large-scale
high-order meshes with quadrilaterals and hexahedra. In the proposed method, a
combination of globally partitioned and processor-local maps are used to first
determine a list of candidate MPI ranks, and then locally candidate elements
that could contain a given point. Next, element-wise bounding boxes further
reduce the list of candidate elements. Finally, Newton's method with trust
region is used to determine the overlapping element and corresponding reference
space coordinates. Since GPU-based architectures have become popular for
accelerating computational analyses using meshes with tensor-product elements,
specialized kernels have been developed to utilize the proposed methodology on
GPUs. The method is also extended to enable general field evaluation on surface
meshes. The paper concludes by demonstrating the use of proposed method in
various applications ranging from mesh-to-mesh transfer during r-adaptivity to
Lagrangian particle tracking.","['cs.MS', 'cs.CE']",2503.06914," A low-energy neutral quasiparticle in a fractional quantum Hall system
appears in the latter's energy spectrum on a sphere as a series of many-body
excited states labeled by the angular momentum $L$ and whose energy is a smooth
function of $L$ in the limit of large sphere radius. We argue that the
signature of a nonvanishing spin (intrinsic angular momentum) $s$ of the
quasiparticle is the absence, in this series, of states with total angular
momentum less than $s$.We reinterpret the missing of certain states, observed
in an exact-diagonalization calculation of the spectrum of the $\nu=7/3$ FQH
state in a wide quantum well as well as in many proposed wave functions for the
excited states as a consequence of the spin-2 nature of the zero-momentum
magnetoroton.","['cond-mat.str-el', 'cond-mat.mes-hall', 'hep-th']",False,,,,General Field Evaluation in High-Order Meshes on GPUs,"Spin of fractional quantum Hall neutral modes and ""missing states"" on a
  sphere"
neg-d2-891,2025-03-18,,2503.14308," Enzyme immobilization plays a crucial role in enhancing the stability and
recyclability of enzymes for industrial applications. However, traditional
methods for quantifying enzyme loading within porous carriers are limited by
time-consuming workflows, cumulative errors, and the inability to probe enzymes
adsorbed inside the pores. In this study, we introduce Time-Domain Nuclear
Magnetic Resonance (TD-NMR) relaxometry as a novel, non-invasive technique for
directly quantifying enzyme adsorption within porous carriers. Focusing on
epoxy methyl acrylate carriers, commonly used in biocatalysis, we correlate
changes in T2 relaxation times with enzyme concentration, leading to the
development of an NMR-based pore-filling ratio that quantifies enzyme loading.
Validation experiments demonstrate that TD-NMR-derived adsorption curves align
closely with traditional photometric measurements, offering a reliable and
reproducible alternative for enzyme quantification. The accessibility of
tabletop TD-NMR spectrometers makes this technique a practical and
cost-effective tool for optimizing biocatalytic processes. Furthermore, the
method holds promise for real-time monitoring of adsorption dynamics and could
be adapted for a wider range of carrier materials and enzymes.",['cond-mat.soft'],2502.07042," Departments within a university are not only administrative units, but also
an effort to gather investigators around common fields of academic study. A
pervasive challenge is connecting members with shared research interests both
within and between departments. Here I describe a workflow that adapts methods
from natural language processing to generate a network connecting $n=79$
members of a university department, or multiple departments within a faculty
($n=278$), based on common topics in their research publications. After
extracting and processing terms from $n=16,901$ abstracts in the PubMed
database, the co-occurrence of terms is encoded in a sparse document-term
matrix. Based on the angular distances between the presence-absence vectors for
every pair of terms, I use the uniform manifold approximation and projection
(UMAP) method to embed the terms into a representational space such that terms
that tend to appear in the same documents are closer together. Each author's
corpus defines a probability distribution over terms in this space. Using the
Wasserstein distance to quantify the similarity between these distributions, I
generate a distance matrix among authors that can be analyzed and visualized as
a graph. I demonstrate that this nonparametric method produces clusters with
distinct themes that are consistent with some academic divisions, while
identifying untapped connections among members. A documented workflow
comprising Python and R scripts is available under the MIT license at
https://github.com/PoonLab/tragula.",['cs.SI'],False,,,,"A novel method for quantifying enzyme immobilization in porous carriers
  using simple NMR relaxometry","Building networks of shared research interests by embedding words into a
  representation space"
neg-d2-892,2025-01-15,,2501.08945," Adjusting for covariates in randomized controlled trials can enhance the
credibility and efficiency of treatment effect estimation. However, handling
numerous covariates and their complex (non-linear) transformations poses a
challenge. Motivated by the case study of the Best Apnea Interventions for
Research (BestAIR) trial data from the National Sleep Research Resource (NSRR),
where the number of covariates (p=114) is comparable to the sample size
(N=196), we propose a principled Covariate Adjustment with Variable Selection
(COADVISE) framework. COADVISE enables variable selection for covariates most
relevant to the outcome while accommodating both linear and nonlinear
adjustments. This framework ensures consistent estimates with improved
efficiency over unadjusted estimators and provides robust variance estimation,
even under outcome model misspecification. We demonstrate efficiency gains
through theoretical analysis, extensive simulations, and a re-analysis of the
BestAIR trial data to compare alternative variable selection strategies,
offering cautionary recommendations. A user-friendly R package, Coadvise, is
available to facilitate practical implementation.",['stat.ME'],2501.18652," The flow of information within many-body systems is a fundamental feature of
physical interaction. Given an underlying classical physics model for the
interaction between a particle and its environment, we give meaning to and
quantify the information passed between them over time. We show that the
maximum information exchange rate is proportional to the ratio of
inter-particle energy flow and initial particle energy -- a sort of
signal-to-noise ratio. In addition, a single time-point (as opposed to
trajectory) observability relation emerges.",['cond-mat.stat-mech'],False,,,,"COADVISE: Covariate Adjustment with Variable Selection in Randomized
  Controlled Trials",Classical Information Exchange Between Particles
neg-d2-893,2025-01-30,,2501.1825," Efficient channel state information (CSI) compression is crucial in frequency
division duplexing (FDD) massive multiple-input multiple-output (MIMO) systems
due to excessive feedback overhead. Recently, deep learning-based compression
techniques have demonstrated superior performance across various data types,
including CSI. However, these approaches often experience performance
degradation when the data distribution changes due to their limited
generalization capabilities. To address this challenge, we propose a model
fine-tuning approach for CSI feedback in massive MIMO systems. The idea is to
fine-tune the encoder/decoder network models in a dynamic fashion using the
recent CSI samples. First, we explore encoder-only fine-tuning, where only the
encoder parameters are updated, leaving the decoder and latent parameters
unchanged. Next, we consider full-model fine-tuning, where the encoder and
decoder models are jointly updated. Unlike encoder-only fine-tuning, full-model
fine-tuning requires the updated decoder and latent parameters to be
transmitted to the decoder side. To efficiently handle this, we propose
different prior distributions for model updates, such as uniform and truncated
Gaussian to entropy code them together with the compressed CSI and account for
additional feedback overhead imposed by conveying the model updates. Moreover,
we incorporate quantized model updates during fine-tuning to reflect the impact
of quantization in the deployment phase. Our results demonstrate that
full-model fine-tuning significantly enhances the rate-distortion (RD)
performance of neural CSI compression. Furthermore, we analyze how often the
full-model fine-tuning should be applied in a new wireless environment and
identify an optimal period interval for achieving the best RD trade-off.","['cs.IT', 'eess.SP', 'math.IT']",2503.07841," We present the first measurements with a new collinear laser spectroscopy
setup at the Argonne Tandem Linac Accelerator System utilizing its unique
capability to deliver neutron-rich refractory metal isotopes produced by the
spontaneous fission of 252Cf. We measured isotope shifts from optical spectra
for nine radioactive ruthenium isotopes 106-114Ru, reaching deep into the
mid-shell region. The extracted charge radii are in excellent agreement with
predictions from the Brussels-Skyrme-on-a-Grid models that account for the
triaxial deformation of nuclear ground states in this region. We show that
triaxial deformation impacts charge radii in models that feature shell effects,
in contrast to what could be concluded from a liquid drop analysis. This
indicates that this exotic type of deformation should not be neglected in
regions where it is known to occur, even if its presence cannot be
unambiguously inferred through laser spectroscopy.","['nucl-ex', 'nucl-th']",False,,,,Dynamic Model Fine-Tuning For Extreme MIMO CSI Compression,"Fingerprints of triaxiality in the charge radii of neutron-rich
  Ruthenium"
neg-d2-894,2025-02-15,,2502.1093," Reduced Order Modeling is of paramount importance for efficiently inferring
high-dimensional spatio-temporal fields in parametric contexts, enabling
computationally tractable parametric analyses, uncertainty quantification and
control. However, conventional dimensionality reduction techniques are
typically limited to known and constant parameters, inefficient for nonlinear
and chaotic dynamics, and uninformed to the actual system behavior. In this
work, we propose sensor-driven SHallow REcurrent Decoder networks for Reduced
Order Modeling (SHRED-ROM). Specifically, we consider the composition of a long
short-term memory network, which encodes the temporal dynamics of limited
sensor data in multiple scenarios, and a shallow decoder, which reconstructs
the corresponding high-dimensional states. SHRED-ROM is a robust decoding-only
strategy that circumvents the numerically unstable approximation of an inverse
which is required by encoding-decoding schemes. To enhance computational
efficiency and memory usage, the full-order state snapshots are reduced by,
e.g., proper orthogonal decomposition, allowing for compressive training of the
networks with minimal hyperparameter tuning. Through applications on chaotic
and nonlinear fluid dynamics, we show that SHRED-ROM (i) accurately
reconstructs the state dynamics for new parameter values starting from limited
fixed or mobile sensors, independently on sensor placement, (ii) can cope with
both physical, geometrical and time-dependent parametric dependencies, while
being agnostic to their actual values, (iii) can accurately estimate unknown
parameters, and (iv) can deal with different data sources, such as
high-fidelity simulations, coupled fields and videos.","['cs.LG', 'math.DS']",2501.11362," In this paper we give the exact order of the discrepancy of the digital van
der Corput--Kronecker sequences that are based on recent counterexamples of the
$X$-adic Littlewood conjecture in positive characteristics. Our result supports
once again the well-established conjecture in the theory of uniform
distribution which states that $D^*_N\leq c \frac{\log^s N}{N},\,c>0$ is the
best possible upper bound for the star discrepancy $D^*_N$ of a sequence in
$[0,1)^s$ or in other words for every sequence in $[0,1)^s$
$\limsup_{N\to\infty}ND^*_N/\log^s N>0$.",['math.NT'],False,,,,Reduced Order Modeling with Shallow Recurrent Decoder Networks,"On the exact order of the discrepancy of low discrepancy digital van der
  Corput--Kronecker sequences"
neg-d2-895,2025-02-05,,2502.0307," Second order information is useful in many ways in smooth optimization
problems, including for the design of step size rules and descent directions,
or the analysis of the local properties of the objective functional. However,
the computation and storage of the Hessian matrix using second order partial
derivatives is prohibitive in many contexts, and in particular in large scale
problems. In this work, we propose a new framework for computing and presenting
second order information in analytic form. The key novel insight is that the
Hessian for a problem can be worked with efficiently by computing its bilinear
form or operator form using Taylor expansions, instead of introducing a basis
and then computing the Hessian matrix. Our new framework is suited for
high-dimensional problems stemming e.g. from imaging applications, where
computation of the Hessian matrix is unfeasible. We also show how this can be
used to implement Newton's step rule, Daniel's Conjugate Gradient rule, or
Quasi-Newton schemes, without explicit knowledge of the Hessian matrix, and
illustrate our findings with a simple numerical experiment.","['math.OC', 'cs.NA', 'math.NA']",2503.17342," We present an updated reconstruction of the dark energy equation of state,
$w(a)$, using the newly released DESI DR2 Baryon Acoustic Oscillation (BAO)
data in combination with Pantheon+ and DES5Y Type Ia supernovae measurements,
respectively. Building on our previous analysis in arXiv:2503.08658, which
employed a nonparametric flexknot reconstruction approach, we examine whether
the evidence for dynamical dark energy persists with the improved precision of
the DESI DR2 dataset. We find that while the overall qualitative structure of
$w(a)$ remains consistent with our earlier findings, the statistical support
for dynamical dark energy is reduced when considering DESI DR2 data alone,
particularly for more complex flexknot models with higher numbers of knots.
However, the evidence for simpler dynamical models, such as $w$CDM and CPL
(which correspond to $n=1$ and $n=2$ knots respectively), increases relative to
$\Lambda$CDM with DESI DR2 alone, consistent with previous DESI analyses. When
combined with Pantheon+ data, the conclusions remain broadly consistent with
our earlier work, but the inclusion of DES5Y supernovae data leads to an
increase of preference for flexknot models with more than two knots, placing
$w$CDM and CPL on par with $\Lambda$CDM.",['astro-ph.CO'],False,,,,The bilinear Hessian for large scale optimization,"Comparison of dynamical dark energy with {\Lambda}CDM in light of DESI
  DR2"
neg-d2-896,2025-02-22,,2502.16318," This study presents the first Direct Numerical Simulation (DNS) of hydrogen
combustion in a real-size internal combustion engine, investigating the complex
dynamics of ignition, flame propagation, and flame-wall interaction under
engine-relevant conditions. The simulation focuses on ultra-lean hydrogen
operation at equivalence ratio $\phi=0.4$ and 800 rpm, utilizing a
state-of-the-art spectral element solver optimized for GPU architectures. The
computational domain encompasses the full engine geometry. Results highlight
the strong coupling between the flame dynamics and the coherent flow structures
during early flame kernel development, while differential diffusion effects
lead to increased reactivity at positive flame curvatures, a phenomenon that
has only been studied in canonical configurations of freely propagating
hydrogen/air flames. As the flame approaches the walls, distinct behavior is
observed during head-on and side-wall quenching scenarios, characterized by
different spatial distributions of wall heat flux. The findings provide
insights into hydrogen combustion in real engines, essential for the
development of clean and efficient hydrogen-fueled powertrains.",['physics.flu-dyn'],2502.01283," We describe the dynamics of a detector modeled by a harmonic oscillator
coupled with an otherwise free quantum field in a curved spacetime in terms of
covariant equations of motion leading to local observables. To achieve this, we
derive and renormalize the integro-differential equation that governs the
detector pointer-variable dynamics, introducing phenomenological parameters
such as a dispersion coefficient and a Lamb-shift parameter. Our formal
solution, expressed in terms of Green's functions, allows for the covariant,
and causal analysis of induced observables on the field. This formalism can be
used for instance to detect non-Gaussianities present in the field's state.","['gr-qc', 'hep-th', 'quant-ph']",False,,,,"Direct Numerical Simulation of Hydrogen Combustion in a Real-Size IC
  Engine",Covariant non-perturbative pointer variables for quantum fields
neg-d2-897,2025-02-27,,2502.1977," With the increasing prevalence of Web-based platforms handling vast amounts
of user data, machine unlearning has emerged as a crucial mechanism to uphold
users' right to be forgotten, enabling individuals to request the removal of
their specified data from trained models. However, the auditing of machine
unlearning processes remains significantly underexplored. Although some
existing methods offer unlearning auditing by leveraging backdoors, these
backdoor-based approaches are inefficient and impractical, as they necessitate
involvement in the initial model training process to embed the backdoors. In
this paper, we propose a TAilored Posterior diffErence (TAPE) method to provide
unlearning auditing independently of original model training. We observe that
the process of machine unlearning inherently introduces changes in the model,
which contains information related to the erased data. TAPE leverages
unlearning model differences to assess how much information has been removed
through the unlearning operation. Firstly, TAPE mimics the unlearned posterior
differences by quickly building unlearned shadow models based on first-order
influence estimation. Secondly, we train a Reconstructor model to extract and
evaluate the private information of the unlearned posterior differences to
audit unlearning. Existing privacy reconstructing methods based on posterior
differences are only feasible for model updates of a single sample. To enable
the reconstruction effective for multi-sample unlearning requests, we propose
two strategies, unlearned data perturbation and unlearned influence-based
division, to augment the posterior difference. Extensive experimental results
indicate the significant superiority of TAPE over the state-of-the-art
unlearning verification methods, at least 4.5$\times$ efficiency speedup and
supporting the auditing for broader unlearning scenarios.","['cs.CR', 'cs.LG']",2502.06274," Drug-side effect research is vital for understanding adverse reactions
arising in complex multi-drug therapies. However, the scarcity of higher-order
datasets that capture the combinatorial effects of multiple drugs severely
limits progress in this field. Existing resources such as TWOSIDES primarily
focus on pairwise interactions. To fill this critical gap, we introduce HODDI,
the first Higher-Order Drug-Drug Interaction Dataset, constructed from U.S.
Food and Drug Administration (FDA) Adverse Event Reporting System (FAERS)
records spanning the past decade, to advance computational pharmacovigilance.
HODDI contains 109,744 records involving 2,506 unique drugs and 4,569 unique
side effects, specifically curated to capture multi-drug interactions and their
collective impact on adverse effects. Comprehensive statistical analyses
demonstrate HODDI's extensive coverage and robust analytical metrics, making it
a valuable resource for studying higher-order drug relationships. Evaluating
HODDI with multiple models, we found that simple Multi-Layer Perceptron (MLP)
can outperform graph models, while hypergraph models demonstrate superior
performance in capturing complex multi-drug interactions, further validating
HODDI's effectiveness. Our findings highlight the inherent value of
higher-order information in drug-side effect prediction and position HODDI as a
benchmark dataset for advancing research in pharmacovigilance, drug safety, and
personalized medicine. The dataset and codes are available at
https://github.com/TIML-Group/HODDI.","['cs.LG', 'cs.AI', 'q-bio.MN']",False,,,,TAPE: Tailored Posterior Difference for Auditing of Machine Unlearning,"HODDI: A Dataset of High-Order Drug-Drug Interactions for Computational
  Pharmacovigilance"
neg-d2-898,2025-02-10,,2502.06274," Drug-side effect research is vital for understanding adverse reactions
arising in complex multi-drug therapies. However, the scarcity of higher-order
datasets that capture the combinatorial effects of multiple drugs severely
limits progress in this field. Existing resources such as TWOSIDES primarily
focus on pairwise interactions. To fill this critical gap, we introduce HODDI,
the first Higher-Order Drug-Drug Interaction Dataset, constructed from U.S.
Food and Drug Administration (FDA) Adverse Event Reporting System (FAERS)
records spanning the past decade, to advance computational pharmacovigilance.
HODDI contains 109,744 records involving 2,506 unique drugs and 4,569 unique
side effects, specifically curated to capture multi-drug interactions and their
collective impact on adverse effects. Comprehensive statistical analyses
demonstrate HODDI's extensive coverage and robust analytical metrics, making it
a valuable resource for studying higher-order drug relationships. Evaluating
HODDI with multiple models, we found that simple Multi-Layer Perceptron (MLP)
can outperform graph models, while hypergraph models demonstrate superior
performance in capturing complex multi-drug interactions, further validating
HODDI's effectiveness. Our findings highlight the inherent value of
higher-order information in drug-side effect prediction and position HODDI as a
benchmark dataset for advancing research in pharmacovigilance, drug safety, and
personalized medicine. The dataset and codes are available at
https://github.com/TIML-Group/HODDI.","['cs.LG', 'cs.AI', 'q-bio.MN']",2503.04199," RGB-Thermal fusion is a potential solution for various weather and light
conditions in challenging scenarios. However, plenty of studies focus on
designing complex modules to fuse different modalities. With the widespread
application of large language models (LLMs), valuable information can be more
effectively extracted from natural language. Therefore, we aim to leverage the
advantages of large language models to design a structurally simple and highly
adaptable multimodal fusion model architecture. We proposed MultimodAl
Segmentation with TExt PRompts (MASTER) architecture, which integrates LLM into
the fusion of RGB-Thermal multimodal data and allows complex query text to
participate in the fusion process. Our model utilizes a dual-path structure to
extract information from different modalities of images. Additionally, we
employ LLM as the core module for multimodal fusion, enabling the model to
generate learnable codebook tokens from RGB, thermal images, and textual
information. A lightweight image decoder is used to obtain semantic
segmentation results. The proposed MASTER performs exceptionally well in
benchmark tests across various automated driving scenarios, yielding promising
results.","['cs.CV', 'cs.AI']",False,,,,"HODDI: A Dataset of High-Order Drug-Drug Interactions for Computational
  Pharmacovigilance",MASTER: Multimodal Segmentation with Text Prompts
neg-d2-899,2025-02-06,,2502.03828," By analyzing 7.93 $\rm fb^{-1}$ of $e^+e^-$ collision data collected at the
center-of-mass energy of 3.773 GeV with the BESIII detector operated at the
BEPCII collider, we report the observation of the semimuonic decays of $D^+\to
\bar K_1(1270)^0\mu^+\nu_\mu$ and $D^0\to K_1(1270)^-\mu^+\nu_\mu$ with
statistical significances of $12.5\sigma$ and $6.0\sigma$, respectively. Their
decay branching fractions are determined to be ${\mathcal B}[D^{+}\to
\bar{K}_1(1270)^0 \mu^{+}\nu_{\mu}]=(2.36\pm0.20^{+0.18}_{-0.27}\pm
0.48)\times10^{-3}$ and ${\mathcal B}[D^{0}\to K_1(1270)^{-}
\mu^{+}\nu_{\mu}]=(0.78\pm0.11^{+0.05}_{-0.09}\pm 0.15)\times10^{-3}$, where
the first and second uncertainties are statistical and systematic,
respectively, and the third originates from the input branching fraction of
$\bar K_{1}(1270)^0\to K^- \pi^+\pi^0$ or $K_1(1270)^-\to K^-\pi^+\pi^-$.
Combining our branching fractions with the previous measurements of ${\mathcal
B}[D^+\to \bar K_1(1270)^0e^+\nu_{e}]$ and ${\mathcal B}[D^0\to
K_1(1270)^-e^+\nu_{e}]$, we determine the branching fraction ratios to be
${\mathcal B}[D^+\to \bar K_1(1270)^0\mu^+\nu_{\mu}]/{\mathcal B}[D^+\to \bar
K_1(1270)^0e^+\nu_{e}]=1.03 \pm 0.14 \substack{+0.11\\-0.15}$ and ${\mathcal
B}[D^0\to K_1(1270)^-\mu^+\nu_{\mu}]/{\mathcal B}[D^0\to
K_1(1270)^-e^+\nu_{e}]=0.74\pm 0.13 \substack{+0.08\\-0.13}$. Using the
branching fractions measured in this work and the world-average lifetimes of
the $D^+$ and $D^0$ mesons, we determine the semimuonic partial decay width
ratio to be $\Gamma [D^+\to \bar K_1(1270)^0 \mu^+\nu_\mu]/\Gamma [D^0\to
K_1(1270)^- \mu^+\nu_\mu]=1.22\pm 0.10\substack{+0.06\\-0.09}$, which is
consistent with unity as predicted by isospin conservation.",['hep-ex'],2501.01902," Massive galaxies in cooling flow clusters display clear evidence of feedback
from Active Galactic Nuclei (AGN). Joint X-ray and radio observations have
shown that AGN radio jets push aside the surrounding hot gas and form cavities
in the hot intracluster medium (ICM). These systems host complex,
kiloparsec-scale, multiphase filamentary structures, from warm ionized (10,000
K) to cold molecular ($<$100 K). These striking clumpy filaments are believed
to be a natural outcome of thermally unstable cooling from the hot ICM, likely
triggered by feedback processes while contributing to feeding the AGN via
Chaotic Cold Accretion (CCA). However, the detailed constraints on the
formation mechanism of the filaments are still uncertain, and the connection
between the different gas phases has to be fully unveiled. By leveraging a
sample of seven X-ray bright cooling-flow clusters, we have discovered a tight
positive correlation between the X-ray surface brightness and the H$\alpha$
surface brightness of the filaments over two orders of magnitude, as also found
in stripped tails.","['astro-ph.GA', 'astro-ph.HE']",False,,,,"Observation of $D^+\to \bar K_1(1270)^0\mu^+\nu_\mu$ and $D^0\to
  K_1(1270)^-\mu^+\nu_\mu$","H$\alpha$-X-ray Surface Brightness Correlation for Filaments in Cooling
  Flow Clusters"
neg-d2-900,2025-01-07,,2501.04201," Weyl semimetal (WSM) thin films possess unique electronic properties that
differ from bulk materials. In this article, we study the nonreciprocal
ballistic transport of the WSM thin films caused by surface modification. We
find that the surface states contribute predominantly to the nonreciprocity,
while the bulk states provide a negative correction. Our calculation shows a
kind of quantum size effect that the nonreciprocal signal decreases as the WSM
film becomes thicker, and diverges when the Fermi energy is near the bottom of
a sub-band. On the other hand, it is found that the density of states in
multi-layer systems possesses some properties roughly independent of thickness.
A single-variable theory is developed to explain it",['cond-mat.mes-hall'],2501.03578," We theoretically propose a circuit of the four-body coupler for
superconducting qubits based on Josephson parametric oscillators (JPOs). Our
coupler for the four-body interaction has a superconducting loop, similar to a
capacitively shunted flux qubit, where an external magnetic flux set to half a
flux quantum is threaded. This coupler circuit is a specific setup of the
circuit called superconducting nonlinear asymmetric inductive elements (SNAIL)
and also is a generalization of the previously proposed one for the four-body
interaction of JPOs. We clarify roles of circuit parameters in the four-body
interaction and, in particular, show that the four-body coupling constant in
our circuit can be significantly increased by tuning capacitance of the coupler
or the area ratio of the Josephson junctions of the coupler.",['quant-ph'],False,,,,"Nonreciprocal ballistic transport in multi-layer Weyl Semimetal films
  with surface engineering","Four-body coupler for superconducting qubits based on Josephson
  parametric oscillators"
neg-d2-901,2025-01-21,,2501.1233," Lossy image coding is the art of computing that is principally bounded by the
image's rate-distortion function. This bound, though never accurately
characterized, has been approached practically via deep learning technologies
in recent years. Indeed, learned image coding schemes allow direct optimization
of the joint rate-distortion cost, thereby outperforming the handcrafted image
coding schemes by a large margin. Still, it is observed that there is room for
further improvement in the rate-distortion performance of learned image coding.
In this article, we identify the gap between the ideal rate-distortion function
forecasted by Shannon's information theory and the empirical rate-distortion
function achieved by the state-of-the-art learned image coding schemes,
revealing that the gap is incurred by five different effects: modeling effect,
approximation effect, amortization effect, digitization effect, and asymptotic
effect. We design simulations and experiments to quantitively evaluate the last
three effects, which demonstrates the high potential of future lossy image
coding technologies.","['cs.IT', 'cs.LG', 'math.IT']",2501.02987," Wall shear stress (WSS) is a crucial hemodynamic quantity extensively studied
in cardiovascular research, yet its numerical computation is not
straightforward. This work aims to compare WSS results obtained from two
different finite element discretizations, quantify the differences between
continuous and discontinuous stresses, and introduce a novel method for WSS
evaluation through the formulation of a boundary-flux problem. Two benchmark
problems are considered - a 2D Stokes flow on a unit square and a 3D Poiseuille
flow through a cylindrical pipe. These are followed by investigations of
steady-state Navier-Stokes flow in two patient-specific aneurysms. The study
focuses on P1/P1 stabilized and Taylor-Hood P2/P1 mixed finite elements for
velocity and pressure. WSS is computed using either the proposed boundary-flux
method or as a projection of tangential traction onto First order Lagrange
(P1), Discontinuous Galerkin first order (DG-1), or Discontinuous Galerkin zero
order (DG-0) space. For the P1/P1 stabilized element, the boundary-flux and P1
projection methods yielded equivalent results. With the P2/P1 element, the
boundary-flux evaluation demonstrated faster convergence in the Poiseuille flow
example but showed increased sensitivity to pressure field inaccuracies in
patient-specific geometries compared to the projection method. In
patient-specific cases, the P2/P1 element exhibited superior robustness to mesh
size when evaluating average WSS and low shear area (LSA), outperforming the
P1/P1 stabilized element. Projecting discontinuous finite element results into
continuous spaces can introduce artifacts, such as the Gibbs phenomenon.
Consequently, it becomes crucial to carefully select the finite element space
for boundary stress calculations - not only in applications involving WSS
computations for aneurysms.","['math.NA', 'cs.NA']",False,,,,The Gap Between Principle and Practice of Lossy Image Coding,"On the numerical evaluation of wall shear stress using the finite
  element method"
neg-d2-902,2025-03-06,,2503.04271," We study LLM judgments of misinformation expressed with uncertainty. Our
experiments study the response of three widely used LLMs (GPT-4o, LlaMA3,
DeepSeek-v2) to misinformation propositions that have been verified false and
then are transformed into uncertain statements according to an uncertainty
typology. Our results show that after transformation, LLMs change their
factchecking classification from false to not-false in 25% of the cases.
Analysis reveals that the change cannot be explained by predictors to which
humans are expected to be sensitive, i.e., modality, linguistic cues, or
argumentation strategy. The exception is doxastic transformations, which use
linguistic cue phrases such as ""It is believed ..."".To gain further insight, we
prompt the LLM to make another judgment about the transformed misinformation
statements that is not related to truth value. Specifically, we study LLM
estimates of the frequency with which people make the uncertain statement. We
find a small but significant correlation between judgment of fact and
estimation of frequency.","['cs.CL', 'cs.CY']",2502.18719," Achieving high subject-independent accuracy in functional near-infrared
spectroscopy (fNIRS)-based brain-computer interfaces (BCIs) remains a
challenge, particularly when minimizing the number of channels. This study
proposes a novel feature extraction scheme and a Pearson correlation-based
channel selection algorithm to enhance classification accuracy while reducing
hardware complexity. Using an open-access fNIRS dataset, our method improved
average accuracy by 28.09% compared to existing approaches, achieving a peak
subject-independent accuracy of 95.98% with only two channels. These results
demonstrate the potential of our optimized feature extraction and channel
selection methods for developing efficient, subject-independent fNIRS-based BCI
systems.","['cs.HC', 'eess.SP']",False,,,,"On Fact and Frequency: LLM Responses to Misinformation Expressed with
  Uncertainty","Enhancing Subject-Independent Accuracy in fNIRS-based Brain-Computer
  Interfaces with Optimized Channel Selection"
neg-d2-903,2025-03-12,,2503.09695," Recent work has shown that the triangular lattice spin-$1/2$ $J_1$-$J_2$
Heisenberg and XXZ antiferromagnets may exhibit coplanar or supersolid orders
proximate to a gapless Dirac spin liquid phase. We explore a distinct
$SU(2N)\!\!\times\!\!SU(M)$ fermionic parton approach, complemented by
variational Monte Carlo calculations for the spin-$1/2$ model, to study the
phase diagram of these models. We also calculate their dynamical spin response
including parton interactions within a random phase approximation, and discuss
implications for neutron scattering on triangular lattice cobaltates
Ba$_3$CoSb$_2$O$_9$, Na$_2$BaCo(PO$_4$)$_2$, K$_2$Co(SeO$_3$)$_2$,
Rb$_2$Co(SeO$_3$)$_2$, and Yb-based magnet KYbSe$_2$.",['cond-mat.str-el'],2503.09241," Computer agents powered by vision-language models (VLMs) have significantly
advanced human-computer interaction, enabling users to perform complex tasks
through natural language instructions. However, these agents are vulnerable to
context deception attacks, an emerging threat where adversaries embed
misleading content into the agent's operational environment, such as a pop-up
window containing deceptive instructions. Existing defenses, such as
instructing agents to ignore deceptive elements, have proven largely
ineffective. As the first systematic study on protecting computer agents, we
introduce textbf{in-context defense}, leveraging in-context learning and
chain-of-thought (CoT) reasoning to counter such attacks. Our approach involves
augmenting the agent's context with a small set of carefully curated exemplars
containing both malicious environments and corresponding defensive responses.
These exemplars guide the agent to first perform explicit defensive reasoning
before action planning, reducing susceptibility to deceptive attacks.
Experiments demonstrate the effectiveness of our method, reducing attack
success rates by 91.2% on pop-up window attacks, 74.6% on average on
environment injection attacks, while achieving 100% successful defenses against
distracting advertisements. Our findings highlight that (1) defensive reasoning
must precede action planning for optimal performance, and (2) a minimal number
of exemplars (fewer than three) is sufficient to induce an agent's defensive
behavior.",['cs.AI'],False,,,,"Modified large-$N$ approach to gapless spin liquids, magnetic orders,
  and dynamics: Application to triangular lattice antiferromagnets",In-Context Defense in Computer Agents: An Empirical Study
neg-d2-904,2025-03-02,,2503.00843," An important unsolved problem in Diophantine number theory is to establish a
general method to effectively find all solutions to any given $S$-unit equation
with at least four terms. Although there are many works contributing to this
problem in literature, most of which handle purely exponential Diophantine
equations, it can be said that all of them only solve finitely many equations
in a natural distinction. In this paper, we study infinitely many purely
exponential Diophantine equations with four terms of consecutive bases. Our
result states that all solutions to the equation $n^x+(n+1)^y+(n+2)^z=(n+3)^w$
in positive integers $n,x,y,z,w$ with $n \equiv 3 \pmod{4}$ are given by
$(n,x,y,z,w)=(3,3,1,1,2), (3,3,3,3,3)$. The proof uses elementary congruence
arguments developed in the study of ternary case, Baker's method in both
rational and $p$-adic cases, and the algorithm of Bert\'ok and Hajdu based on a
variant of Skolem's conjecture on purely exponential equations.",['math.NT'],2503.12791," The occurrence of a topological phase transition can be demonstrated by a
direct observation of a change in the topological invariant. For holographic
topological semimetals, a topological Hamiltonian method needs to be employed
to calculate the topological invariants due to the strong coupling nature of
the system. We calculate the topological invariants for the holographic Weyl
semimetal and the holographic Weyl-$\mathrm Z_2$ semimetal, which correspond to
the chiral charge and the spin-Chern number, respectively. This is achieved by
probing fermions within the system and deriving the topological Hamiltonian
from the zero-frequency Green's function. In both cases, we have identified an
effective band structure characterized by an infinite number of Weyl or
$\mathrm Z_2$ nodes, a distinctive feature of holographic systems different
from weakly coupled systems. The topological invariants of these nodes are
computed numerically and found to be nonzero, thereby confirming the
topologically nontrivial nature of these nodes.",['hep-th'],False,,,,"Solving an infinite number of purely exponential Diophantine equations
  with four terms",Topological invariant for holographic Weyl-$\mathrm Z_2$ semimetal
neg-d2-905,2025-01-23,,2501.13507," In this paper, we address the problem of manipulating multi-particle
aggregates using a bimanual robotic system. Our approach enables the autonomous
transport of dispersed particles through a series of shaping and pushing
actions using robotically-controlled tools. Achieving this advanced
manipulation capability presents two key challenges: high-level task planning
and trajectory execution. For task planning, we leverage Vision Language Models
(VLMs) to enable primitive actions such as tool affordance grasping and
non-prehensile particle pushing. For trajectory execution, we represent the
evolving particle aggregate's contour using truncated Fourier series, providing
efficient parametrization of its closed shape. We adaptively compute trajectory
waypoints based on group cohesion and the geometric centroid of the aggregate,
accounting for its spatial distribution and collective motion. Through
real-world experiments, we demonstrate the effectiveness of our methodology in
actively shaping and manipulating multi-particle aggregates while maintaining
high system cohesion.",['cs.RO'],2502.09003," Supervised fine-tuning is a standard method for adapting pre-trained large
language models (LLMs) to downstream tasks. Quantization has been recently
studied as a post-training technique for efficient LLM deployment. To obtain
quantized fine-tuned LLMs, conventional pipelines would first fine-tune the
pre-trained models, followed by post-training quantization. This often yields
suboptimal performance as it fails to leverage the synergy between fine-tuning
and quantization. To effectively realize low-bit quantization of weights,
activations, and KV caches in LLMs, we propose an algorithm named Rotated
Straight-Through-Estimator (RoSTE), which combines quantization-aware
supervised fine-tuning (QA-SFT) with an adaptive rotation strategy that
identifies an effective rotation configuration to reduce activation outliers.
We provide theoretical insights on RoSTE by analyzing its prediction error when
applied to an overparameterized least square quantized training problem. Our
findings reveal that the prediction error is directly proportional to the
quantization error of the converged weights, which can be effectively managed
through an optimized rotation configuration. Experiments on Pythia, Qwen and
Llama models of different sizes demonstrate the effectiveness of RoSTE.
Compared to existing post-SFT quantization baselines, our method consistently
achieves superior performances across various tasks and different LLM
architectures.","['cs.LG', 'cs.AI']",False,,,,"Iterative Shaping of Multi-Particle Aggregates based on Action Trees and
  VLM","RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach
  for Large Language Models"
neg-d2-906,2025-02-28,,2502.20974," Open-world continual learning (OWCL) adapts to sequential tasks with open
samples, learning knowledge incrementally while preventing forgetting. However,
existing OWCL still requires a large amount of labeled data for training, which
is often impractical in real-world applications. Given that new
categories/entities typically come with limited annotations and are in small
quantities, a more realistic situation is OWCL with scarce labeled data, i.e.,
few-shot training samples. Hence, this paper investigates the problem of
open-world few-shot continual learning (OFCL), challenging in (i) learning
unbounded tasks without forgetting previous knowledge and avoiding overfitting,
(ii) constructing compact decision boundaries for open detection with limited
labeled data, and (iii) transferring knowledge about knowns and unknowns and
even update the unknowns to knowns once the labels of open samples are learned.
In response, we propose a novel OFCL framework that integrates three key
components: (1) an instance-wise token augmentation (ITA) that represents and
enriches sample representations with additional knowledge, (2) a margin-based
open boundary (MOB) that supports open detection with new tasks emerge over
time, and (3) an adaptive knowledge space (AKS) that endows unknowns with
knowledge for the updating from unknowns to knowns. Finally, extensive
experiments show the proposed OFCL framework outperforms all baselines
remarkably with practical importance and reproducibility. The source code is
released at https://github.com/liyj1201/OFCL.","['cs.LG', 'cs.AI']",2501.17489," Brain-computer interfaces (BCIs) present a promising avenue by translating
neural activity directly into text, eliminating the need for physical actions.
However, existing non-invasive BCI systems have not successfully covered the
entire alphabet, limiting their practicality. In this paper, we propose a novel
non-invasive EEG-based BCI system with Curriculum-based Neural Spelling
Framework, which recognizes all 26 alphabet letters by decoding neural signals
associated with handwriting first, and then apply a Generative AI (GenAI) to
enhance spell-based neural language decoding tasks. Our approach combines the
ease of handwriting with the accessibility of EEG technology, utilizing
advanced neural decoding algorithms and pre-trained large language models
(LLMs) to translate EEG patterns into text with high accuracy. This system show
how GenAI can improve the performance of typical spelling-based neural language
decoding task, and addresses the limitations of previous methods, offering a
scalable and user-friendly solution for individuals with communication
impairments, thereby enhancing inclusive communication options.","['cs.HC', 'cs.AI']",False,,,,"Improving Open-world Continual Learning under the Constraints of Scarce
  Labeled Data",Neural Spelling: A Spell-Based BCI System for Language Neural Decoding
neg-d2-907,2025-03-05,,2503.03424," Given a tracial von Neumann algebra $(M,\tau)$, we prove that a state
preserving $M$-bimodular ucp map between two stationary W$^*$-extensions of
$(M,\tau)$ preserves the Furstenberg entropy if and only if it induces an
isomorphism between the Radon-Nikodym factors. With a similar proof, we extend
this result to quasi-factor maps between stationary spaces of locally compact
groups and prove an entropy separation between unique stationary and amenable
spaces. As applications, we use these results to establish rigidity phenomena
for unique stationary Poisson boundaries.","['math.OA', 'math.DS', 'math.GR']",2501.05881," Infrared spectroscopy, e.g., with JWST, provides a glimpse into the chemical
inventory of the innermost region of protoplanetary discs, where terrestrial
planets eventually form. The chemical make-up of regions inside snowlines is
connected to the material drifting from the outer regions, which can be modeled
with dust evolution models. However, infrared observations are limited by the
high dust extinction in the inner disc, and only probes the abundances of
gaseous species in the disc surface layers. As a result, the bulk mass of
delivered volatiles is not directly relatable to what is measured through
infrared spectra. In this paper, we investigate how the delivery of dust and
ice after prolonged pebble drift affects the observable reservoir of water
vapor in the inner disc. We develop a 1+1D approach based on dust evolution
models to determine the delivery and distribution of vapor compared to the
height of the $\tau = 1$ surface in the dust continuum. We find that the
observable column density of water vapor at wavelengths probed by JWST spans
many orders of magnitude over time, exhibiting different radial profiles
depending on dust properties, drift rate, and local processing. In the presence
of a traffic-jam effect inside the snowline, the observable vapor reservoir
appears constant in time despite the ongoing delivery by pebble drift, such
that water is effectively smuggled unnoticed. Differences in measured column
densities then originate not only from variations in bulk vapor content, but
also from differences in the properties and distribution of dust particles.",['astro-ph.EP'],False,,,,Rigidity of Furstenberg entropy under ucp maps,"Smuggling unnoticed: Towards a 2D view of water and dust delivery to the
  inner regions of protoplanetary discs"
neg-d2-908,2025-01-28,,2501.17337," In this paper, we establish the global H\""older gradient estimate for
solutions to the Dirichlet problem of the Monge-Amp\`ere equation $\det D^2u =
f$ on strictly convex but not uniformly convex domain $\Omega$.",['math.AP'],2501.05505," The study of regular black holes and black hole mimickers as alternatives to
standard black holes has recently gained significant attention, driven both by
the need to extend general relativity to describe black hole interiors, and by
recent advances in observational technologies. Despite considerable progress in
this field, significant challenges remain in identifying and characterizing
physically well-motivated classes of regular black holes and black hole
mimickers. This report provides an overview of these challenges, and outlines
some of the promising research directions -- as discussed during a week-long
focus programme held at the Institute for Fundamental Physics of the Universe
(IFPU) in Trieste from November 11th to 15th, 2024.","['gr-qc', 'astro-ph.HE']",False,,,,"Global $C^{1,\alpha}$ regularity for Monge-Amp\`ere equations on planar
  convex domains",Towards a Non-singular Paradigm of Black Hole Physics
neg-d2-909,2025-02-14,,2502.10114," Ewens' sampling formula (ESF) provides the probability distribution governing
the number of distinct genetic types and their respective frequencies at a
selectively neutral locus under the infinitely-many-alleles model of mutation.
A natural and significant question arises: ``Is the Ewens probability
distribution on regular trees Gibbsian?""
  In this paper, we demonstrate that Ewens probability distributions can be
regarded as non-Gibbsian distributions on regular trees and derive a sufficient
condition for the consistency condition. This study lays the groundwork for a
new direction in the theory of non-Gibbsian probability distributions on trees.","['math.PR', 'math.FA']",2502.07095," Invasive species are a growing threat to marine ecosystems, and the recent
proliferation of the Atlantic blue crab (Callinectes sapidus) in the Po Delta
(Italy) has had significant ecological and economic impacts, particularly on
clam farming. This study explores the influence of C. sapidus on clam
production in the Po Delta, combining biological and ecological data with
socio-economic analysis. Field data collected between August and December 2023
from the Canarin and Scardovari Lagoons revealed seasonal fluctuations in crab
abundance, with a peak in captures during the warmer months. The predatory
behaviour of C. sapidus has led to a sharp decline in clam production, reaching
near-zero levels in early 2024. Statistical analysis confirmed a strong
correlation between the increase of the invasive crab population and the
decrease in clam yields. This study also explores potential management
strategies, including the economic valorisation of C. sapidus as a commercial
resource, turning an ecological challenge into an opportunity. These findings
highlight the urgent need for targeted management interventions to mitigate the
impact of this invasive species on local fisheries and ecosystems.",['q-bio.PE'],False,,,,"Non-Gibbsian Multivariate Ewens Probability Distributions on Regular
  Trees","The devasting economic impact of Callinectes sapidus on the clam fishing
  in the Po Delta (Italy): Striking evidence from novel field data"
neg-d2-910,2025-02-15,,2502.10794," Multimodal Large Language Models (MLLMs) bridge the gap between visual and
textual data, enabling a range of advanced applications. However, complex
internal interactions among visual elements and their alignment with text can
introduce vulnerabilities, which may be exploited to bypass safety mechanisms.
To address this, we analyze the relationship between image content and task and
find that the complexity of subimages, rather than their content, is key.
Building on this insight, we propose the Distraction Hypothesis, followed by a
novel framework called Contrasting Subimage Distraction Jailbreaking (CS-DJ),
to achieve jailbreaking by disrupting MLLMs alignment through multi-level
distraction strategies. CS-DJ consists of two components: structured
distraction, achieved through query decomposition that induces a distributional
shift by fragmenting harmful prompts into sub-queries, and visual-enhanced
distraction, realized by constructing contrasting subimages to disrupt the
interactions among visual elements within the model. This dual strategy
disperses the model's attention, reducing its ability to detect and mitigate
harmful content. Extensive experiments across five representative scenarios and
four popular closed-source MLLMs, including GPT-4o-mini, GPT-4o, GPT-4V, and
Gemini-1.5-Flash, demonstrate that CS-DJ achieves average success rates of
52.40% for the attack success rate and 74.10% for the ensemble attack success
rate. These results reveal the potential of distraction-based approaches to
exploit and bypass MLLMs' defenses, offering new insights for attack
strategies.",['cs.CV'],2501.08884," Scenario decision making offers a flexible way of making decision in an
uncertain environment while obtaining probabilistic guarantees on the risk of
failure of the decision. The idea of this approach is to draw samples of the
uncertainty and make a decision based on the samples, called ""scenarios"". The
probabilistic guarantees take the form of a bound on the probability of
sampling a set of scenarios that will lead to a decision whose risk of failure
is above a given maximum tolerance. This bound can be expressed as a function
of the number of sampled scenarios, the maximum tolerated risk, and some
intrinsic property of the problem called the ""compression size"". Several such
bounds have been proposed in the literature under various assumptions on the
problem. We propose new bounds that improve upon the existing ones without
requiring stronger assumptions on the problem.","['math.OC', 'cs.LG']",False,,,,"Distraction is All You Need for Multimodal Large Language Model
  Jailbreaking",Improved Compression Bounds for Scenario Decision Making
neg-d2-911,2025-01-30,,2501.18771," Data contamination -- the accidental consumption of evaluation examples
within the pre-training data -- can undermine the validity of evaluation
benchmarks. In this paper, we present a rigorous analysis of the effects of
contamination on language models at 1B and 8B scales on the machine translation
task. Starting from a carefully decontaminated train-test split, we
systematically introduce contamination at various stages, scales, and data
formats to isolate its effect and measure its impact on performance metrics.
Our experiments reveal that contamination with both source and target
substantially inflates BLEU scores, and this inflation is 2.5 times larger (up
to 30 BLEU points) for 8B compared to 1B models. In contrast, source-only and
target-only contamination generally produce smaller, less consistent
over-estimations. Finally, we study how the temporal distribution and frequency
of contaminated samples influence performance over-estimation across languages
with varying degrees of data resources.","['cs.CL', 'cs.AI']",2501.17484," We present a method for solving a large-scale stochastic capacity expansion
problem which explicitly considers reliability constraints, in particular
constraints on expected energy not served. Our method tackles this problem by a
Lagrange relaxation of the expected energy not served constraints. We solve the
relaxed formulation in an iterative manner, using a subgradient-based method.
Each iteration requires the solution of a stochastic capacity expansion
problem, for which we implement a subgradient decomposition scheme in a
high-performance computing infrastructure. We apply the proposed methodology on
the Economic Viability Assessment model that is used by ENTSO-E in the annual
European Resource Adequacy Assessment, extended to include explicit reliability
constraints. The approach is able to solve this model achieving a 1.3%
optimality gap. We compare our approach against accounting for reliability
through penalizing load shedding at VOLL, and find that the former results in
1.6% savings in total cost. We are also able to quantify the cost savings from
allowing some load curtailment in the capacity planning process, which ranges
from 1.6 to 6% in the cases analyzed.","['eess.SY', 'cs.SY']",False,,,,"Overestimation in LLM Evaluation: A Controlled Large-Scale Study on Data
  Contamination's Impact on Machine Translation","Capacity Expansion Planning under Uncertainty subject to Expected Energy
  Not Served Constraints"
neg-d2-912,2025-01-16,,2501.09744," The objective of BioCreative8 Track 3 is to extract phenotypic key medical
findings embedded within EHR texts and subsequently normalize these findings to
their Human Phenotype Ontology (HPO) terms. However, the presence of diverse
surface forms in phenotypic findings makes it challenging to accurately
normalize them to the correct HPO terms. To address this challenge, we explored
various models for named entity recognition and implemented data augmentation
techniques such as synonym marginalization to enhance the normalization step.
Our pipeline resulted in an exact extraction and normalization F1 score 2.6\%
higher than the mean score of all submissions received in response to the
challenge. Furthermore, in terms of the normalization F1 score, our approach
surpassed the average performance by 1.9\%. These findings contribute to the
advancement of automated medical data extraction and normalization techniques,
showcasing potential pathways for future research and application in the
biomedical domain.",['cs.AI'],2503.12982," Cooperative perception can increase the view field and decrease the occlusion
of an ego vehicle, hence improving the perception performance and safety of
autonomous driving. Despite the success of previous works on cooperative object
detection, they mostly operate on dense Bird's Eye View (BEV) feature maps,
which are computationally demanding and can hardly be extended to long-range
detection problems. More efficient fully sparse frameworks are rarely explored.
In this work, we design a fully sparse framework, SparseAlign, with three key
features: an enhanced sparse 3D backbone, a query-based temporal context
learning module, and a robust detection head specially tailored for sparse
features. Extensive experimental results on both OPV2V and DairV2X datasets
show that our framework, despite its sparsity, outperforms the state of the art
with less communication bandwidth requirements. In addition, experiments on the
OPV2Vt and DairV2Xt datasets for time-aligned cooperative object detection also
show a significant performance gain compared to the baseline works.",['cs.CV'],False,,,,"KU AIGEN ICL EDI@BC8 Track 3: Advancing Phenotype Named Entity
  Recognition and Normalization for Dysmorphology Physical Examination Reports",SparseAlign: A Fully Sparse Framework for Cooperative Object Detection
neg-d2-913,2025-02-26,,2502.1925," Imitation learning has proven to be highly effective in teaching robots
dexterous manipulation skills. However, it typically relies on large amounts of
human demonstration data, which limits its scalability and applicability in
dynamic, real-world environments. One key challenge in this context is object
generalization, where a robot trained to perform a task with one object, such
as ""hand over the apple,"" struggles to transfer its skills to a semantically
similar but visually different object, such as ""hand over the peach."" This gap
in generalization to new objects beyond those in the same category has yet to
be adequately addressed in previous work on end-to-end visuomotor policy
learning. In this paper, we present a simple yet effective approach for
achieving object generalization through Vision-Language-Action (VLA) models,
referred to as \textbf{ObjectVLA}. Our model enables robots to generalize
learned skills to novel objects without requiring explicit human demonstrations
for each new target object. By leveraging vision-language pair data, our method
provides a lightweight and scalable way to inject knowledge about the target
object, establishing an implicit link between the object and the desired
action. We evaluate ObjectVLA on a real robotic platform, demonstrating its
ability to generalize across 100 novel objects with a 64\% success rate in
selecting objects not seen during training. Furthermore, we propose a more
accessible method for enhancing object generalization in VLA models, using a
smartphone to capture a few images and fine-tune the pre-trained model. These
results highlight the effectiveness of our approach in enabling object-level
generalization and reducing the need for extensive human demonstrations, paving
the way for more flexible and scalable robotic learning systems.","['cs.RO', 'cs.CV']",2501.13676," Text classifiers suffer from small perturbations, that if chosen
adversarially, can dramatically change the output of the model. Verification
methods can provide robustness certificates against such adversarial
perturbations, by computing a sound lower bound on the robust accuracy.
Nevertheless, existing verification methods incur in prohibitive costs and
cannot practically handle Levenshtein distance constraints. We propose the
first method for computing the Lipschitz constant of convolutional classifiers
with respect to the Levenshtein distance. We use these Lipschitz constant
estimates for training 1-Lipschitz classifiers. This enables computing the
certified radius of a classifier in a single forward pass. Our method, LipsLev,
is able to obtain $38.80$% and $13.93$% verified accuracy at distance $1$ and
$2$ respectively in the AG-News dataset, while being $4$ orders of magnitude
faster than existing approaches. We believe our work can open the door to more
efficient verification in the text domain.","['cs.LG', 'cs.AI', 'cs.CL']",False,,,,"ObjectVLA: End-to-End Open-World Object Manipulation Without
  Demonstration",Certified Robustness Under Bounded Levenshtein Distance
neg-d2-914,2025-03-18,,2503.14015," This paper considers Bayesian optimization (BO) for problems with known outer
problem structure. In contrast to the classic BO setting, where the objective
function itself is unknown and needs to be iteratively estimated from noisy
observations, we analyze the case where the objective has a known outer
structure - given in terms of a loss function - while the inner structure - an
unknown input-output model - is again iteratively estimated from noisy
observations of the model outputs. We introduce a novel lower confidence bound
algorithm for this particular problem class which exploits the known outer
problem structure. The proposed method is analyzed in terms of regret for the
special case of convex loss functions and probabilistic parametric models which
are linear in the uncertain parameters. Numerical examples illustrate the
superior performance of structure-exploiting methods compared to
structure-agnostic approaches.",['math.OC'],2501.10662," We study non-local operators for analyzing the Higgs-confinement phase
transition in lattice gauge theory. Since the nature of the Higgs-confinement
phase transition is topological, its order parameter is the expectation value
of non-local operators, such as loop and surface operators. There exist several
candidates for the non-local operators. Adopting the charge-2 Abelian Higgs
model, we test numerical simulation of conventional ones, the Polyakov loop and
the 't Hooft loop, and an unconventional one, the Aharonov-Bohm phase defined
by the Wilson loop wrapping around a vortex line.","['hep-lat', 'hep-th']",False,,,,"Bayesian Optimization with Lower Confidence Bounds for Minimization
  Problems with Known Outer Structure","Analyzing the Higgs-confinement transition with non-local operators on
  the lattice"
neg-d2-915,2025-01-16,,2501.09839," We prove two theorems about tree-decompositions in the setting of coarse
graph theory. First, we show that a graph $G$ admits a tree-decomposition in
which each bag is contained in the union of a bounded number of balls of
bounded radius, if and only if $G$ admits a quasi-isometry to a graph with
bounded tree-width. (The ``if'' half is easy, but the ``only if'' half is
challenging.) This generalizes a recent result of Berger and Seymour,
concerning tree-decompositions when each bag has bounded radius.
  Second, we show that if $G$ admits a quasi-isometry $\phi$ to a graph $H$ of
bounded path-width, then $G$ admits a quasi-isometry (with error only an
additive constant) to a graph of bounded path-width. Indeed, we will show a
much stronger statement: that we can assign a non-negative integer length to
each edge of $H$, such that the same function $\phi$ is a quasi-isometry (with
error only an additive constant) to this weighted version of $H$.",['math.CO'],2502.03905," We prove dynamical coherence for partial hyperbolic symplectomorphism in
dimension 4 whose stable and unstable bundles are C^1.",['math.DS'],False,,,,Coarse tree-width,Partially hyperbolic symplectomorphism with C^1 bundles
neg-d2-916,2025-02-21,,2502.15247," We developed an anisotropic spin model that accounts for magnetic anisotropy
and evaluated the Curie temperature (Tc) dispersion due to finite size effects
in L10-FePt nanoparticles. In heat-assisted magnetic recording (HAMR) media, a
next-generation magnetic recording technology, high-density recording is
achieved by locally heating L10-FePt nanoparticles near their Tc and rapidly
cooling them. However, variations in Tc caused by differences in particle size
and shape can compromise recording stability and areal density capacity, making
the control of Tc dispersion critical. In this study, we constructed atomistic
LLG models to explicitly incorporate the spin exchange anisotropy of L10-FePt,
based on parameters determined by first-principles calculations. Using this
model, we evaluated the impact of particle size on Tc dispersion. As a result,
(1) the Tc dispersion critical to the performance of HAMR can be reproduced,
whereas it was previously underestimated by isotropic models and (2)
approximately 70% of the experimentally observed Tc dispersion can be
attributed to particle size effects. This research highlights the role of
exchange anisotropy in amplifying finite-size effects and underscores the
importance of size control in HAMR media.","['cond-mat.mtrl-sci', 'cond-mat.mes-hall']",2502.14693," Recent advancements in large language models (LLMs) have shown remarkable
potential in automating machine learning tasks. However, existing LLM-based
agents often struggle with low-diversity and suboptimal code generation. While
recent work has introduced Monte Carlo Tree Search (MCTS) to address these
issues, limitations persist in the quality and diversity of thoughts generated,
as well as in the scalar value feedback mechanisms used for node selection. In
this study, we introduce Introspective Monte Carlo Tree Search (I-MCTS), a
novel approach that iteratively expands tree nodes through an introspective
process that meticulously analyzes solutions and results from parent and
sibling nodes. This facilitates a continuous refinement of the node in the
search tree, thereby enhancing the overall decision-making process.
Furthermore, we integrate a Large Language Model (LLM)-based value model to
facilitate direct evaluation of each node's solution prior to conducting
comprehensive computational rollouts. A hybrid rewarding mechanism is
implemented to seamlessly transition the Q-value from LLM-estimated scores to
actual performance scores. This allows higher-quality nodes to be traversed
earlier. Applied to the various ML tasks, our approach demonstrates a 6%
absolute improvement in performance compared to the strong open-source AutoML
agents, showcasing its effectiveness in enhancing agentic AutoML systems.
Resource available at https://github.com/jokieleung/I-MCTS",['cs.CL'],False,,,,"Anisotropic Exchange Spin Model to Investigate the Curie Temperature
  Dispersion of Finite-Size L10-FePt Magnetic Nanoparticles","I-MCTS: Enhancing Agentic AutoML via Introspective Monte Carlo Tree
  Search"
neg-d2-917,2025-02-21,,2502.15642," Neural Ordinary Differential Equations (Neural ODEs) represent
continuous-time dynamics with neural networks, offering advancements for
modeling and control tasks. However, training Neural ODEs requires solving
differential equations at each epoch, leading to high computational costs. This
work investigates simultaneous optimization methods as a faster training
alternative. In particular, we employ a collocation-based, fully discretized
formulation and use IPOPT--a solver for large-scale nonlinear optimization--to
simultaneously optimize collocation coefficients and neural network parameters.
Using the Van der Pol Oscillator as a case study, we demonstrate faster
convergence compared to traditional training methods. Furthermore, we introduce
a decomposition framework utilizing Alternating Direction Method of Multipliers
(ADMM) to effectively coordinate sub-models among data batches. Our results
show significant potential for (collocation-based) simultaneous Neural ODE
training pipelines.","['cs.LG', 'math.OC']",2502.17498," In the Large Language Model(LLM) reasoning scenario, people often estimate
state value via Monte Carlo sampling. Though Monte Carlo estimation is an
elegant method with less inductive bias, noise and errors are inevitably
introduced due to the limited sampling. To handle the problem, we inject the
structural prior into the value representation and transfer the scalar value
into the expectation of a pre-defined categorical distribution, representing
the noise and errors from a distribution perspective. Specifically, by treating
the result of Monte Carlo sampling as a single sample from the prior
ground-truth Binomial distribution, we quantify the sampling error as the
mismatch between posterior estimated distribution and ground-truth
distribution, which is thus optimized via distribution selection optimization.
We test the performance of value-based process verifiers on Best-of-N task and
Beam search task. Compared with the scalar value representation, we show that
reasonable structural prior injection induced by different objective functions
or optimization methods can improve the performance of value-based process
verifiers for about 1$\sim$2 points at little-to-no cost. We also show that
under different structural prior, the verifiers' performances vary greatly
despite having the same optimal solution, indicating the importance of
reasonable structural prior injection.","['cs.LG', 'cs.AI', 'cs.CL']",False,,,,Training Neural ODEs Using Fully Discretized Simultaneous Optimization,Improving Value-based Process Verifier via Structural Prior Injection
neg-d2-918,2025-01-28,,2501.16708," Deep extragalactic X-ray surveys, such as the Chandra COSMOS-Legacy field
(CCLS), are prone to be biased against active galactic nuclei (AGN) with high
column densities due to their lower count rates at a given luminosity. To
quantify this selection effect, we forward model nearby ($z\sim0.05$) AGN from
the BAT AGN Spectroscopic Survey (BASS) with well-characterized ($\gtrsim$1000
cts) broadband X-ray spectra (0.5-195 keV) to simulate the CCLS absorption
distribution. We utilize the BASS low-redshift analogs with similar
luminosities to the CCLS ($L_\mathrm{2-10\ keV}^\mathrm{int}\sim10^{42-45}\
\mathrm{erg}\ \mathrm{s}^{-1}$), which are much less affected by obscuration
and low-count statistics, as the seed for our simulations, and follow the
spectral fitting of the CCLS. Our simulations reveal that Chandra would fail to
detect the majority (53.3%; 563/1056) of obscured ($N_\mathrm{H}>10^{22}\
\mathrm{cm}^{-2}$) simulated BASS AGN given the observed redshift and
luminosity distribution of the CCLS. Even for detected sources with sufficient
counts ($\geq30$) for spectral modeling, the level of obscuration is
significantly overestimated. This bias is most extreme for objects whose best
fit indicates a high-column density AGN ($N_\mathrm{H}\geq10^{24}\
\mathrm{cm}^{-2}$), since the majority (66.7%; 18/27) of these are actually
unobscured sources ($N_\mathrm{H}<10^{22}\ \mathrm{cm}^{-2}$). This implies
that previous studies may have significantly overestimated the increase in the
obscured fraction with redshift and the fraction of luminous obscured AGN. Our
findings highlight the importance of directly considering obscuration biases
and forward modeling in X-ray surveys, as well as the need for
higher-sensitivity X-ray missions such as the Advanced X-ray Imaging Satellite
(AXIS), and the importance of multi-wavelength indicators to estimate
obscuration in distant supermassive black holes.",['astro-ph.GA'],2503.14943," As the digital and physical worlds become more intertwined, there has been a
lot of interest in digital avatars that closely resemble their real-world
counterparts. Current digitization methods used in 3D production pipelines
require costly capture setups, making them impractical for mass usage among
common consumers. Recent academic literature has found success in
reconstructing humans from limited data using implicit representations (e.g.,
voxels used in NeRFs), which are able to produce impressive videos. However,
these methods are incompatible with traditional rendering pipelines, making it
difficult to use them in applications such as games. In this work, we propose
an end-to-end pipeline that builds explicitly-represented photorealistic 3D
avatars using standard 3D assets. Our key idea is the use of
dynamically-generated textures to enhance the realism and visually mask
deficiencies in the underlying mesh geometry. This allows for seamless
integration with current graphics pipelines while achieving comparable visual
quality to state-of-the-art 3D avatar generation methods.",['cs.CV'],False,,,,"BASS XLV: Quantifying AGN Selection Effects in the Chandra COSMOS-Legacy
  Survey with BASS",3D Engine-ready Photorealistic Avatars via Dynamic Textures
neg-d2-919,2025-02-18,,2502.1312," Gender-inclusive language is often used with the aim of ensuring that all
individuals, regardless of gender, can be associated with certain concepts.
While psycholinguistic studies have examined its effects in relation to human
cognition, it remains unclear how Large Language Models (LLMs) process
gender-inclusive language. Given that commercial LLMs are gaining an
increasingly strong foothold in everyday applications, it is crucial to examine
whether LLMs in fact interpret gender-inclusive language neutrally, because the
language they generate has the potential to influence the language of their
users. This study examines whether LLM-generated coreferent terms align with a
given gender expression or reflect model biases. Adapting psycholinguistic
methods from French to English and German, we find that in English, LLMs
generally maintain the antecedent's gender but exhibit underlying masculine
bias. In German, this bias is much stronger, overriding all tested
gender-neutralization strategies.","['cs.CL', 'cs.AI']",2503.15879," Non-factoid question-answering (NFQA) poses a significant challenge due to
its open-ended nature, diverse intents, and the need for multi-aspect
reasoning, which renders conventional factoid QA approaches, including
retrieval-augmented generation (RAG), inadequate. Unlike factoid questions,
non-factoid questions (NFQs) lack definitive answers and require synthesizing
information from multiple sources across various reasoning dimensions. To
address these limitations, we introduce Typed-RAG, a type-aware multi-aspect
decomposition framework within the RAG paradigm for NFQA. Typed-RAG classifies
NFQs into distinct types -- such as debate, experience, and comparison -- and
applies aspect-based decomposition to refine retrieval and generation
strategies. By decomposing multi-aspect NFQs into single-aspect sub-queries and
aggregating the results, Typed-RAG generates more informative and contextually
relevant responses. To evaluate Typed-RAG, we introduce Wiki-NFQA, a benchmark
dataset covering diverse NFQ types. Experimental results demonstrate that
Typed-RAG outperforms baselines, thereby highlighting the importance of
type-aware decomposition for effective retrieval and generation in NFQA. Our
code and dataset are available at https://github.com/TeamNLP/Typed-RAG.","['cs.CL', 'cs.IR']",False,,,,"Adapting Psycholinguistic Research for LLMs: Gender-inclusive Language
  in a Coreference Context","Typed-RAG: Type-aware Multi-Aspect Decomposition for Non-Factoid
  Question Answering"
neg-d2-920,2025-03-14,,2503.1133," Target search problems are central to a wide range of fields, from biological
foraging to the optimization algorithms. Recently, the ability to reset the
search has been shown to significantly improve the searcher's efficiency.
However, the optimal resetting strategy depends on the specific properties of
the search problem and can often be challenging to determine. In this work, we
propose a reinforcement learning (RL)-based framework to train agents capable
of optimizing their search efficiency in environments by learning how to reset.
First, we validate the approach in a well-established benchmark: the Brownian
search with resetting. There, RL agents consistently recover strategies closely
resembling the sharp resetting distribution, known to be optimal in this
scenario. We then extend the framework by allowing agents to control not only
when to reset, but also their spatial dynamics through turning actions. In this
more complex setting, the agents discover strategies that adapt both resetting
and turning to the properties of the environment, outperforming the proposed
benchmarks. These results demonstrate how reinforcement learning can serve both
as an optimization tool and a mechanism for uncovering new, interpretable
strategies in stochastic search processes with resetting.","['cond-mat.stat-mech', 'cs.AI', 'cs.LG', 'physics.bio-ph', 'physics.comp-ph']",2502.01491," In this work, we explore how instance-level memorization in the teacher
Neural Machine Translation (NMT) model gets inherited by the student model in
sequence-level knowledge distillation (SeqKD). We find that despite not
directly seeing the original training data, students memorize more than
baseline models (models of the same size, trained on the original data) -- 3.4%
for exact matches and 57% for extractive memorization -- and show increased
hallucination rates. Further, under this SeqKD setting, we also characterize
how students behave on specific training data subgroups, such as subgroups with
low quality and specific counterfactual memorization (CM) scores, and find that
students exhibit amplified denoising on low-quality subgroups. Finally, we
propose a modification to SeqKD named Adaptive-SeqKD, which intervenes in SeqKD
to reduce memorization and hallucinations. Overall, we recommend caution when
applying SeqKD: students inherit both their teachers' superior performance and
their fault modes, thereby requiring active monitoring.",['cs.CL'],False,,,,Learning to reset in target search problems,"Memorization Inheritance in Sequence-Level Knowledge Distillation for
  Neural Machine Translation"
neg-d2-921,2025-01-22,,2501.1293," By the planarity rank of a semigroup variety we mean the largest number of
generators of a free semigroup of a variety with respect to which the semigroup
admits a planar Cayley graph. Since the time when L.M.Martynov formulated the
problem of describing the planarity ranks of semigroup varieties, many specific
results have been obtained in this direction. A modular variety of semigroups
is a variety of semigroups with a modular lattice of subvarieties. In this
paper, we calculate the exact values of the planarity ranks of an infinite
countable set of all possible modular varieties of semigroups. It turns out
that these values do not exceed 3. Machine calculations are mostly used in the
proof. Prover9 and Mace4 are used to check the equalities of elements of free
semigroups of varieties defined by a large number of identities. To prove the
non-planarity of graphs, the Pontryagin-Kuratovsky criterion is used, and the
Colin de Verdiere invariant is indirectly used to justify planarity.",['math.RA'],2501.0429," Although the star formation process has been studied for decades, many
important aspects of the physics involved remain unsolved. Recent advancement
of instrumentation in the infrared, far-infrared and sub-millimetre wavelength
regimes have contributed to a significantly improved understanding of processes
in the interstellar medium (ISM) leading to star formation. The future of
research on the ISM and star formation looks exciting with instruments like the
JWST, ALMA, etc., already contributing to the topic by gathering
high-resolution high-sensitivity data and with several larger ground- and
space-bound facilities either being planned or constructed. India has a sizable
number of astronomers engaged in research on topics related to the ISM and star
formation. In this white paper invited by the Astronomical Society of India to
prepare a vision document for Indian astronomy, we review the Indian
contributions to the global understanding of the star formation process and
suggest areas that require focused efforts both in creating observing
facilities and in theoretical front in India, in order to improve the impact of
our research in the coming decades.",['astro-ph.GA'],False,,,,Planarity ranks of modular varieties of semigroups,"Research on the Interstellar Medium and Star Formation in the Galaxy: An
  Indian Perspective"
neg-d2-922,2025-01-30,,2501.18836," Dynamic pricing strategies are crucial for firms to maximize revenue by
adjusting prices based on market conditions and customer characteristics.
However, designing optimal pricing strategies becomes challenging when
historical data are limited, as is often the case when launching new products
or entering new markets. One promising approach to overcome this limitation is
to leverage information from related products or markets to inform the focal
pricing decisions. In this paper, we explore transfer learning for
nonparametric contextual dynamic pricing under a covariate shift model, where
the marginal distributions of covariates differ between source and target
domains while the reward functions remain the same. We propose a novel Transfer
Learning for Dynamic Pricing (TLDP) algorithm that can effectively leverage
pre-collected data from a source domain to enhance pricing decisions in the
target domain. The regret upper bound of TLDP is established under a simple
Lipschitz condition on the reward function. To establish the optimality of
TLDP, we further derive a matching minimax lower bound, which includes the
target-only scenario as a special case and is presented for the first time in
the literature. Extensive numerical experiments validate our approach,
demonstrating its superiority over existing methods and highlighting its
practical utility in real-world applications.","['cs.LG', 'math.ST', 'stat.ME', 'stat.TH']",2501.08623," Understanding which effective field theories are consistent with an
ultraviolet completion in quantum gravity is an important theoretical question.
Therefore, it is important to know the structure of the 4D effective theory
associated with a given compactification of string theory. We present a
first-principles derivation of the low-energy 4D effective theory of geometric
moduli in a warped Calabi-Yau compactification of type IIB string theory with
imaginary self-dual 3-form flux. This completes the derivation of the metric on
K\""ahler moduli space from the 10D equations of motion. We also give the first
derivation of an effective action for flat directions in the complex structure
moduli space of the Calabi-Yau (which generically mix with the axiodilaton).",['hep-th'],False,,,,Transfer Learning for Nonparametric Contextual Dynamic Pricing,"Dimensional Reduction and K\""ahler Metric for Metric Moduli in Imaginary
  Self-Dual Flux"
neg-d2-923,2025-01-29,,2501.17705," Integrating high-dimensional, heterogeneous data from multi-site cohort
studies with complex hierarchical structures poses significant feature
selection and prediction challenges. We extend the Bayesian Integrative
Analysis and Prediction (BIP) framework to enable simultaneous feature
selection and outcome modeling in data of nested hierarchical structure. We
apply the proposed Bayesian Integrative Mixed Modeling (BIPmixed) framework to
the Adolescent Brain Cognitive Development (ABCD) Study, leveraging multi-view
data, including structural and functional MRI and early life adversity (ELA)
metrics, to identify relevant features and predict the behavioral outcome.
BIPmixed incorporates 2-level nested random effects, to enhance
interpretability and make predictions in hierarchical data settings. Simulation
studies illustrate BIPmixed's robustness in distinct random effect settings,
highlighting its use for complex study designs. Our findings suggest that
BIPmixed effectively integrates multi-view data while accounting for nested
sampling, making it a valuable tool for analyzing large-scale studies with
hierarchical data.","['stat.ME', 'stat.AP']",2502.12602," This paper presents a novel learning-based approach to dynamic robot-to-human
handover, addressing the challenges of delivering objects to a moving receiver.
We hypothesize that dynamic handover, where the robot adjusts to the receiver's
movements, results in more efficient and comfortable interaction compared to
static handover, where the receiver is assumed to be stationary. To validate
this, we developed a nonparametric method for generating continuous handover
motion, conditioned on the receiver's movements, and trained the model using a
dataset of 1,000 human-to-human handover demonstrations. We integrated
preference learning for improved handover effectiveness and applied impedance
control to ensure user safety and adaptiveness. The approach was evaluated in
both simulation and real-world settings, with user studies demonstrating that
dynamic handover significantly reduces handover time and improves user comfort
compared to static methods. Videos and demonstrations of our approach are
available at https://zerotohero7886.github.io/dyn-r2h-handover .",['cs.RO'],False,,,,"A Bayesian Integrative Mixed Modeling Framework for Analysis of the
  Adolescent Brain and Cognitive Development Study",Learning-based Dynamic Robot-to-Human Handover
neg-d2-924,2025-03-21,,2503.17463," Reduced-order models (ROMs) remain generally unreliable for
convection-dominated problems, such as those encountered in hypersonic flows,
due to the slowly decaying Kolmogorov $n$-width of linear subspace
approximations, known as the Kolmogorov barrier. This limitation hinders the
accuracy of traditional ROMs and necessitates impractical amounts of training
data during the offline phase. To address this challenge, we introduce a novel
landmark-based registration procedure tailored for ROMs of convection-dominated
problems. Our approach leverages limited training data and incorporates a
nonlinear transformation of the data using a landmark-based registration
technique combined with radial basis function (RBF) interpolation. During the
offline phase, we align dominant convective features in a reference domain,
resulting in a rapid decay of error relative to the reduced space dimension.
Landmarks are generated through a three-step process: (1) detecting shocks via
edge detection techniques, (2) sampling using Monte Carlo methods, and (3)
domain partitioning with $k$-means clustering, where cluster centroids serve as
landmarks. Accurate landmark correspondence is achieved by minimizing pairing
distances for similar features. The online phase integrates standard
minimum-residual ROM methodologies, extending the optimization space to include
admissible domain mappings. We validate our approach on two test cases: a
space-time Burgers' equation parameterized by the initial condition, and a
hypersonic viscous flow over a cylinder parameterized by the Mach number.
Results demonstrate the efficacy of the proposed method in overcoming the
Kolmogorov barrier and enhancing the reliability of ROMs for
convection-dominated problems.","['math.NA', 'cs.NA', 'physics.flu-dyn']",2501.17746," In vehicle-to-everything (V2X) applications, roadside units (RSUs) can be
tasked with both sensing and communication functions to enable sensing-assisted
communications. Recent studies have demonstrated that distance, angle, and
velocity information obtained through sensing can be leveraged to reduce the
overhead associated with communication beam tracking. In this work, we extend
this concept to scenarios involving multiple distributed RSUs and distributed
MIMO (multiple-input multiple-output) systems. We derive the state evolution
model, formulate the extended Kalman-filter equations, and implement predictive
beamforming for distributed MIMO. Simulation results indicate that, when
compared with a co-located massive MIMO antenna array, distributed antennas
lead to more uniform and robust sensing performance, coverage, and data rates,
while the vehicular user is in motion.","['eess.SP', 'cs.IT', 'math.IT']",False,,,,"Model reduction of convection-dominated viscous conservation laws using
  implicit feature tracking and landmark image registration",Predictive Beamforming with Distributed MIMO
neg-d2-925,2025-03-14,,2503.111," The efficiency of silicon solar cells gradually decreases in various
environments, with humidity being a key factor contributing to this decline.
This study investigates the humidity-induced failure mechanisms in crystalline
silicon solar cells. Using density functional theory and the non-equilibrium
Green's function method, we systematically examine the microscopic diffusion
mechanisms of hydrogen and oxygen defects and their impact on photovoltaic
performance. Hydrogen and oxygen are interstitial defects that can introduce
both deep-level and resonant-state recombination centers, thereby reducing
carrier lifetime and solar cell efficiency. Furthermore, hydrogen exhibits
prominent diffusion pathways, particularly in its +1 and 0 charge states at the
BC site ( ""H"" _""i(BC)"" ^""+1"" and ""H"" _""i(BC)"" ^""0"" ), while oxygen in its +1
and 0 charge states shows a higher diffusion barrier at the BC1 site ( ""O""
_""i(BC1)"" ^""+1"" and ""O"" _""i(BC1)"" ^""0"" ). These defects, induced by moisture
and temperature fluctuations, exacerbate the degradation of solar cell
performance. By analyzing these defect behaviors, this research provides
valuable insights into the failure mechanisms of Si solar cells, especially
under humid conditions.",['cond-mat.mtrl-sci'],2502.03605," Device sizing is crucial for meeting performance specifications in
operational transconductance amplifiers (OTAs), and this work proposes an
automated sizing framework based on a transformer model. The approach first
leverages the driving-point signal flow graph (DP-SFG) to map an OTA circuit
and its specifications into transformer-friendly sequential data. A specialized
tokenization approach is applied to the sequential data to expedite the
training of the transformer on a diverse range of OTA topologies, under
multiple specifications. Under specific performance constraints, the trained
transformer model is used to accurately predict DP-SFG parameters in the
inference phase. The predicted DP-SFG parameters are then translated to
transistor sizes using a precomputed look-up table-based approach inspired by
the gm/Id methodology. In contrast to previous conventional or
machine-learning-based methods, the proposed framework achieves significant
improvements in both speed and computational efficiency by reducing the need
for expensive SPICE simulations within the optimization loop; instead, almost
all SPICE simulations are confined to the one-time training phase. The method
is validated on a variety of unseen specifications, and the sizing solution
demonstrates over 90% success in meeting specifications with just one SPICE
simulation for validation, and 100% success with 3-5 additional SPICE
simulations.",['cs.AR'],False,,,,"The Role of Hydrogen and Oxygen Interstitial Defects in Crystalline Si
  cells: Mechanism of Device Degradation in Humid Environment","Accelerating OTA Circuit Design: Transistor Sizing Based on a
  Transformer Model and Precomputed Lookup Tables"
neg-d2-926,2025-03-10,,2503.07414," This study aims to develop a cost-effective microgrid design that optimally
balances the economic feasibility, reliability, efficiency, and environmental
impact in a grid-tied community microgrid. A multi-objective optimization
framework is employed, integrating HOMER Pro for system sizing with deep
reinforcement learning (DRL). Sensitivity analyses are conducted to evaluate
the system performance under varying load demand and renewable energy
fluctuations, while an economic sensitivity assessment examines the impact of
electricity prices and capital costs on the Levelized Cost of Energy (LCOE).
The proposed microgrid configuration achieves high reliability, satisfying 100%
of the load, even under adverse weather conditions. The proposed framework
attains an efficiency of 91.99% while maintaining a carbon footprint of 302,747
kg/year, which is approximately 95% lower than that of the grid system. The
economic analysis indicates a net present cost (NPC) of $4.83M with a
competitive LCOE of $0.208/kWh. In addition, the operation cost is $201,473 per
year with a capital investment of $1.42M, rendering it a financially viable
alternative to conventional grid-dependent systems.This work can be valuable in
identifying effective solutions for supplying reliable and cost-effective power
to regional and remote areas.","['eess.SY', 'cs.SY']",2502.04389," Diagrams play a crucial role in visually conveying complex relationships and
processes within business documentation. Despite recent advances in
Vision-Language Models (VLMs) for various image understanding tasks, accurately
identifying and extracting the structures and relationships depicted in
diagrams continues to pose significant challenges. This study addresses these
challenges by proposing a text-driven approach that bypasses reliance on VLMs'
visual recognition capabilities. Instead, it utilizes the editable source
files--such as xlsx, pptx or docx--where diagram elements (e.g., shapes, lines,
annotations) are preserved as textual metadata. In our proof-of-concept, we
extracted diagram information from xlsx-based system design documents and
transformed the extracted shape data into textual input for Large Language
Models (LLMs). This approach allowed the LLM to analyze relationships and
generate responses to business-oriented questions without the bottleneck of
image-based processing. Experimental comparisons with a VLM-based method
demonstrated that the proposed text-driven framework yielded more accurate
answers for questions requiring detailed comprehension of diagram
structures.The results obtained in this study are not limited to the tested
.xlsx files but can also be extended to diagrams in other documents with source
files, such as Office pptx and docx formats. These findings highlight the
feasibility of circumventing VLM constraints through direct textual extraction
from original source files. By enabling robust diagram understanding through
LLMs, our method offers a promising path toward enhanced workflow efficiency
and information analysis in real-world business scenarios.","['cs.SE', 'cs.AI']",False,,,,Cost-Effective Design of Grid-tied Community Microgrid,"Overcoming Vision Language Model Challenges in Diagram Understanding: A
  Proof-of-Concept with XML-Driven Large Language Models Solutions"
neg-d2-927,2025-01-12,,2501.06765," In this paper, a quantum walk model which reflects the underlying embedding
on the surface is proposed. We obtain the scattering matrix of this quantum
walk model characterized by the faces on the surface, and find a detection of
the orientablility of the underlying embedding by the scattering information.
The comfortability is the square norm of the stationary state restricted to the
internal and reflected by the underlying embedding. We find that quantum walker
feels more comfortable to a surface with small genus in some natural setting.",['quant-ph'],2501.04946," The least trimmed squares (LTS) estimator is a renowned robust alternative to
the classic least squares estimator and is popular in location, regression,
machine learning, and AI literature. Many studies exist on LTS, including its
robustness, computation algorithms, extension to non-linear cases, asymptotics,
etc. The LTS has been applied in the penalized regression in a high-dimensional
real-data sparse-model setting where dimension $p$ (in thousands) is much
larger than sample size $n$ (in tens, or hundreds). In such a practical
setting, the sample size $n$ often is the count of sub-population that has a
special attribute (e.g. the count of patients of Alzheimer's, Parkinson's,
Leukemia, or ALS, etc.) among a population with a finite fixed size N.
Asymptotic analysis assuming that $n$ tends to infinity is not practically
convincing and legitimate in such a scenario. A non-asymptotic or finite sample
analysis will be more desirable and feasible.
  This article establishes some finite sample (non-asymptotic) error bounds for
estimating and predicting based on LTS with high probability for the first
time.","['stat.ML', 'cs.LG']",False,,,,Comfortability of quantum walks on embedded graphs on surfaces,"Non-asymptotic analysis of the performance of the penalized least
  trimmed squares in sparse models"
neg-d2-928,2025-02-13,,2502.09015," Cross-entropy benchmarking is a central technique used to certify a quantum
chip in recent experiments. To better understand its mathematical foundation
and develop new benchmarking schemes, we introduce the concept of ergodicity to
random circuit sampling and find that the Haar random quantum circuit satisfies
an ergodicity condition -- the average of certain types of post-processing
function over the output bit strings is close to the average over the unitary
ensemble. For noiseless random circuits, we prove that the ergodicity holds for
polynomials of degree $t$ with positive coefficients and when the random
circuits form a unitary $2t$-design. For strong enough noise, the ergodicity
condition is violated. This suggests that ergodicity is a property that can be
exploited to certify a quantum chip. We formulate the deviation of ergodicity
as a measure for quantum chip benchmarking and show that it can be used to
estimate the circuit fidelity for global depolarizing noise and weakly
correlated noise. For a quadratic post-processing function, our framework
recovers Google's result on estimating the circuit fidelity via linear
cross-entropy benchmarking (XEB), and we give a sufficient condition on the
noise model characterizing when such estimation is valid. Our results establish
an interesting connection between ergodicity and noise in random circuits and
provide new insights into designing quantum benchmarking schemes.",['quant-ph'],2503.0441," This article provides an introduction to nuclear magnetic resonance
spectroscopy in pulsed magnetic fields (PFNMR), focusing on its capabilities,
applications, and future developments in research involving high magnetic
fields. It highlights the significance of PFNMR in enhancing the understanding
of solid-state materials, with particular emphasis on those exhibiting complex
interactions and strong electronic correlations. Several technical aspects are
discussed, including the challenges associated with high-frequency NMR
experiments. The power of PFNMR is showcased through several examples,
including studies on the topical materials LiCuVO$_4$, SrCu$_2$(BO$_3$)$_2$,
and CeIn$_3$, offering insights into their magnetic and electronic properties
at high magnetic fields. The article also discusses possible future directions
for the technique, including improvements in PFNMR instrumentation and the
exploration of materials under extreme conditions. This exposition underscores
the role of PFNMR in advancing the frontiers of materials-science research.",['cond-mat.str-el'],False,,,,"Generalized Cross-Entropy Benchmarking for Random Circuits with
  Ergodicity",Nuclear magnetic resonance spectroscopy in pulsed magnetic fields
neg-d2-929,2025-02-24,,2502.1776," This paper studies probabilistic dual frames and associated dual frame
potentials from the optimal mass transport perspective. The main contribution
in this work shows that given a probabilistic frame, its dual frame potential
is minimized if and only if the probabilistic frame is tight and the
probabilistic dual frame is the canonical dual. In particular, the tightness
condition can be dropped if the probabilistic dual frame potential is minimized
only among probabilistic dual frames of pushforward type.",['math.FA'],2503.15196," Ultra-short Period exoplanets (USPs) like 55 Cnc e, hosting dayside magma
oceans, present unique opportunities to study surface-atmosphere interactions.
The composition of a vaporised mineral atmosphere enveloping the dayside is
dictated by that of the surface magma ocean, which in turn is sensitive to its
oxygen fugacity ($f$O$_2$). Observability estimations and characterisation of
the atmospheric emission of 55 Cnc e have mostly remained limited to low
spectral resolution space-based studies. Here, we aim to examine ground-based
high-resolution observabilities of a diverse set of mineral atmospheres
produced across a grid of mantle $f$O$_2$s varying over 12 orders of magnitude.
We assume a Bulk Silicate Earth mantle composition and a substellar dayside
temperature of T = 2500K in the near infrared wavelength (NIR) region. This
spectral range is often featureless for this class of atmospheres at
low-resolution. Coupling our newly developed simulator for synthesising
realistic observations from high-resolution ground-based spectrographs (Ratri)
to a pre-developed high-resolution cross-correlation spectroscopy (HRCCS)
analysis pipeline (Upamana), we find that this array of mineral atmospheres
would all be detectable with 11 hours of observing time of the dayside of 55
Cnc e with CARMENES and each individual scenario can be correctly
differentiated within 1$\sigma$. Our analysis is readily able to distinguish
between a planet with an Earth-like redox state (with $f$O$_2$ $\sim$3.5
log$_{10}$ units above the iron-w\""ustite, IW buffer) from a Mercury-like
planet ($f$O$_2$ $\sim$5 log$_{10}$ units below IW). We thus conclude that the
HRCCS technique holds promise for cataloguing the diversity of redox states
among the rocky exoplanetary population.",['astro-ph.EP'],False,,,,Probabilistic Dual Frames and Minimization of Dual Frame Potentials,"Detectability of oxygen fugacity regimes in the magma ocean world 55
  Cancri e at high spectral resolution"
neg-d2-930,2025-02-18,,2502.1334," Accurate visualization of double star astrometric data is essential for
effective analysis and interpretation. This article presents a Python toolkit
designed for astronomers who need to plot measurements from diverse sources --
historical, Gaia DR3, and the Las Cumbres Observatory (LCO) network -- while
maintaining a 1:1 aspect ratio to avoid visually distorting the data. The
toolkit is composed of three scripts: one that handles polar coordinates (P.A.,
separation), one for Cartesian (X, Y) coordinates, and another with the option
to include predicted theoretical points. This paper describes the purpose,
functionality, and usage of these scripts, including example figures,
installation guides, and licensing information.
  This toolkit has been used by the author and collaborators in published and
submitted research on double star systems, demonstrating its versatility for
both professional and student-driven investigations.","['astro-ph.IM', 'astro-ph.SR']",2502.16326," Background: Multi-collimator proton minibeam radiation therapy (MC-pMBRT) has
recently emerged as a versatile technique for dose shaping, enabling
peak-valley dose patterns in organs-at-risk (OAR) while maintaining a uniform
dose distribution in tumor. MC-pMBRT leverages a set of generic multi-slit
collimators (MSC) with varying center-to-center distances. However, the current
method for minibeam aperture optimization (MAO), i.e., the selection of MSC per
beam angle, is manual and heuristic, resulting in computational inefficiencies
and no guarantee of optimality. This work introduces a novel mixed integer
programming (MIP) approach to MAO for optimizing MC-pMBRT plan quality.
Methods: The proposed MIP approach jointly optimizes dose distributions,
peak-to-valley dose ratio (PVDR), and selects the optimal set of MSC per beam
angle. The optimization problem includes decision variables for MSC selection
per beam angle and spot weights. The proposed MIP approach is a two-step
process: Step1: the binary variables are optimally determined to select MSC for
each beam angle; Step 2: the continuous variables are solved to determine the
spot weights. Both steps utilize iterative convex relaxation and the
alternating direction method of multipliers to solve the problems. Results: The
proposed MIP method for MAO (MIP-MAO) was validated against the conventional
heuristic method (CONV) for MC-pMBRT treatment planning. Results indicate that
MIP-MAO enhances the conformity index (CI) for the target and improves PVDR for
OAR. For instance, in a head-and-neck case, CI improved from 0.61 (CONV) to
0.70 (MIP-MAO); in an abdomen case, CI improved from 0.78 (CONV) to 0.83
(MIP-MAO). Additionally, MIP-MAO reduced mean doses in the body and OAR.
Conclusions: A novel MIP approach for MAO in MC-pMBRT is presented, showing
demonstrated improvements in plan quality and PVDR compared to the heuristic
method.",['physics.med-ph'],False,,,,"A Python Toolkit for Plotting Double Star Observations with 1:1 Aspect
  Ratio","A mixed integer programming approach to minibeam aperture optimization
  for multi-collimator proton minibeam radiotherapy"
neg-d2-931,2025-02-05,,2502.04389," Diagrams play a crucial role in visually conveying complex relationships and
processes within business documentation. Despite recent advances in
Vision-Language Models (VLMs) for various image understanding tasks, accurately
identifying and extracting the structures and relationships depicted in
diagrams continues to pose significant challenges. This study addresses these
challenges by proposing a text-driven approach that bypasses reliance on VLMs'
visual recognition capabilities. Instead, it utilizes the editable source
files--such as xlsx, pptx or docx--where diagram elements (e.g., shapes, lines,
annotations) are preserved as textual metadata. In our proof-of-concept, we
extracted diagram information from xlsx-based system design documents and
transformed the extracted shape data into textual input for Large Language
Models (LLMs). This approach allowed the LLM to analyze relationships and
generate responses to business-oriented questions without the bottleneck of
image-based processing. Experimental comparisons with a VLM-based method
demonstrated that the proposed text-driven framework yielded more accurate
answers for questions requiring detailed comprehension of diagram
structures.The results obtained in this study are not limited to the tested
.xlsx files but can also be extended to diagrams in other documents with source
files, such as Office pptx and docx formats. These findings highlight the
feasibility of circumventing VLM constraints through direct textual extraction
from original source files. By enabling robust diagram understanding through
LLMs, our method offers a promising path toward enhanced workflow efficiency
and information analysis in real-world business scenarios.","['cs.SE', 'cs.AI']",2503.04612," This note is concerned with the distribution of the angles between Oseledets
subspaces for linear cocycles driven by an ergodic transformation. We restrict
ourselves to dimension $2$, and give particular attention to the question of
log-integrability of those angles. In the setting of random i.i.d.\ products of
matrices, we construct examples of probability measures on \(\GL_2(\R)\) with
finite first moment, for which the angle between Oseledets directions of the
associated cocycle is not log-integrable. Building on work for the totally
irreducible case by Benoist and Quint, we show that for probability measures
with finite second moment the angle between Oseledets subspaces is always
log-integrable. Then we pivot to general measurable \(\GL_2(\R)\)-cocycles over
an arbitrary ergodic automorphism of a non-atomic Lebesgue space. We show that
no integrability condition on the distribution of the matrices is sufficient to
guarantee log-integrability of the angle between Oseledets spaces. In fact, in
this context we show that the joint distribution of the Oseledets spaces may be
chosen arbitrarily. We also obtain a similar flexibility result for bounded
cocycles under the proper condition on the distribution of angles.",['math.DS'],False,,,,"Overcoming Vision Language Model Challenges in Diagram Understanding: A
  Proof-of-Concept with XML-Driven Large Language Models Solutions",On the distribution of the angle between Oseledets spaces
neg-d2-932,2025-02-27,,2502.20517," This is the first of three papers motivated by the author's desire to
understand and explain ""algebraically"" one aspect of Dmitriy Zhuk's proof of
the CSP Dichotomy Theorem. In this paper we study abelian congruences in
varieties having a weak difference term. Each class of the congruence supports
an abelian group structure; if the congruence is minimal, each class supports
the structure of a vector space over a division ring determined by the
congruence. A construction due to J. Hagemann, C. Herrmann and R. Freese in the
congruence modular setting extends to varieties with a weak difference term,
and provides a ""universal domain"" for the abelian groups or vector spaces that
arise from the classes of the congruence within a single class of the
annihilator of the congruence. The construction also supports an extension of
Freese's similarity relation (between subdirectly irreducible algebras) from
the congruence modular setting to varieties with a weak difference term.",['math.LO'],2501.16039," In this paper, we investigate the complexity of computing the minimal
faithful permutation degree for groups without abelian normal subgroups. When
our groups are given as quotients of permutation groups, we establish that this
problem is in $\textsf{P}$. Furthermore, in the setting of permutation groups,
we obtain an upper bound of $\textsf{NC}$ for this problem. This improves upon
the work of Das and Thakkar (STOC 2024), who established a Las Vegas
polynomial-time algorithm for this class in the setting of permutation groups.","['cs.DS', 'cs.CC', 'math.GR']",False,,,,"Abelian congruences and similarity in varieties with a weak difference
  term","Complexity of Minimal Faithful Permutation Degree for Fitting-free
  Groups"
neg-d2-933,2025-03-20,,2503.16128," Assessing smile genuineness from video sequences is a vital topic concerned
with recognizing facial expression and linking them with the underlying
emotional states. There have been a number of techniques proposed underpinned
with handcrafted features, as well as those that rely on deep learning to
elaborate the useful features. As both of these approaches have certain
benefits and limitations, in this work we propose to combine the features
learned by a long short-term memory network with the features handcrafted to
capture the dynamics of facial action units. The results of our experiments
indicate that the proposed solution is more effective than the baseline
techniques and it allows for assessing the smile genuineness from video
sequences in real-time.",['cs.CV'],2502.05549," The problem ""A general characterization of uniqueness polynomial for
non-critically injective polynomials"" has been remained open since the last two
decades. In this paper, we explore this open problem. To this end, we initiate
a new approach that also includes critically injective polynomials. We provide
this characterization for both the complex and p-adic cases. We also provide
various examples as an application of our results along with the verification
of the existing examples. Consequently, we find examples of unique range sets
generated by non-critically injective polynomials with least cardinalities
achieved so far and one of these results is sharp with respect to all the
available formulas in the literature. Furthermore, we cover the part of least
degree uniqueness polynomials. In this part, we also provide some sharp bounds.",['math.CV'],False,,,,Coupling deep and handcrafted features to assess smile genuineness,"On the characterization of uniqueness polynomials: both complex and
  p-adic versions"
neg-d2-934,2025-01-12,,2501.06891," A remarkable feature of dark matter consisting of ultralight bosonic
particles is the emergence of superfluid Bose-Einstein condensate structures on
galactic scales. We investigate the oscillations of the solitonic dark matter
structure in the central galactic region by numerically solving the
Bogoliubov-de Gennes problem, accounting for perturbations in the gravitational
potential and local self-interactions. Our findings reveal that the central
solitonic core, formed by the balance of gravitational attraction, quantum
pressure, and repulsive interactions, exhibits significant oscillatory
behaviour. These oscillations, characterized by distinct eigenmodes, provide
insights into the dynamical properties of solitonic dark matter structures and
their observational implications and contributions to galactic structure
formation and evolution.","['astro-ph.CO', 'astro-ph.GA', 'nlin.PS']",2501.16185," We study the equation of state of three-dimensional compact U(1) gauge theory
on the lattice by means of numerical simulations, and discuss the implications
of our results for the spectrum of the theory, in connection with previous
results from the literature. We also compare our findings to the case of
non-Abelian gauge theories and comment on the continuum limit.","['hep-lat', 'hep-th']",False,,,,Oscillations of Solitonic Galactic Cores in Ultralight Dark Matter,"On the equation of state of U(1) lattice gauge theory in three
  dimensions"
neg-d2-935,2025-03-03,,2503.01752," Border basis schemes are open subschemes of the Hilbert scheme of $\mu$
points in an affine space $\mathbb{A}^n$. They have easily describable systems
of generators of their vanishing ideals for a natural embedding into a large
affine space $\mathbb{A}^{\mu\nu}$. Here we bring together several techniques
for re-embedding affine schemes into lower dimensional spaces which we
developed in the last years. We study their efficacy for some special types of
border basis schemes such as MaxDeg border basis schemes, L-shape and
simplicial border basis schemes, as well as planar border basis schemes. A
particular care is taken to make these re-embeddings efficiently computable and
to check when we actually get an isomorphism with $\mathbb{A}^{n\mu}$, i.e.,
when the border basis scheme is an affine cell.","['math.AG', 'math.AC']",2503.14015," This paper considers Bayesian optimization (BO) for problems with known outer
problem structure. In contrast to the classic BO setting, where the objective
function itself is unknown and needs to be iteratively estimated from noisy
observations, we analyze the case where the objective has a known outer
structure - given in terms of a loss function - while the inner structure - an
unknown input-output model - is again iteratively estimated from noisy
observations of the model outputs. We introduce a novel lower confidence bound
algorithm for this particular problem class which exploits the known outer
problem structure. The proposed method is analyzed in terms of regret for the
special case of convex loss functions and probabilistic parametric models which
are linear in the uncertain parameters. Numerical examples illustrate the
superior performance of structure-exploiting methods compared to
structure-agnostic approaches.",['math.OC'],False,,,,Re-Embeddings of Special Border Basis Schemes,"Bayesian Optimization with Lower Confidence Bounds for Minimization
  Problems with Known Outer Structure"
neg-d2-936,2025-03-14,,2503.1196," Commit messages are crucial in software development, supporting maintenance
tasks and communication among developers. While Large Language Models (LLMs)
have advanced Commit Message Generation (CMG) using various software contexts,
some contexts developers consider to write high-quality commit messages are
often missed by CMG techniques and can't be easily retrieved or even retrieved
at all by automated tools. To address this, we propose Commit Message
Optimization (CMO), which enhances human-written messages by leveraging LLMs
and search-based optimization. CMO starts with human-written messages and
iteratively improves them by integrating key contexts and feedback from
external evaluators. Our extensive evaluation shows CMO generates commit
messages that are significantly more Rational, Comprehensive, and Expressive
while outperforming state-of-the-art CMG methods and human messages 40.3% to
78.4% of the time. Moreover, CMO can support existing CMG techniques to further
improve message quality and generate high-quality messages when the
human-written ones are left blank.",['cs.SE'],2502.02717," Foundational models have emerged as a powerful paradigm in deep learning
field, leveraging their capacity to learn robust representations from
large-scale datasets and effectively to diverse downstream applications such as
classification. In this paper, we present Astromer 2 a foundational model
specifically designed for extracting light curve embeddings. We introduce
Astromer 2 as an enhanced iteration of our self-supervised model for light
curve analysis. This paper highlights the advantages of its pre-trained
embeddings, compares its performance with that of its predecessor, Astromer 1,
and provides a detailed empirical analysis of its capabilities, offering deeper
insights into the model's representations. Astromer 2 is pretrained on 1.5
million single-band light curves from the MACHO survey using a self-supervised
learning task that predicts randomly masked observations within sequences.
Fine-tuning on a smaller labeled dataset allows us to assess its performance in
classification tasks. The quality of the embeddings is measured by the F1 score
of an MLP classifier trained on Astromer-generated embeddings. Our results
demonstrate that Astromer 2 significantly outperforms Astromer 1 across all
evaluated scenarios, including limited datasets of 20, 100, and 500 samples per
class. The use of weighted per-sample embeddings, which integrate intermediate
representations from Astromer's attention blocks, is particularly impactful.
Notably, Astromer 2 achieves a 15% improvement in F1 score on the ATLAS dataset
compared to prior models, showcasing robust generalization to new datasets.
This enhanced performance, especially with minimal labeled data, underscores
the potential of Astromer 2 for more efficient and scalable light curve
analysis.","['astro-ph.IM', 'cs.AI', 'cs.LG']",False,,,,"Consider What Humans Consider: Optimizing Commit Message Leveraging
  Contexts Considered By Human",Astromer 2
neg-d2-937,2025-01-23,,2501.14229," The Kitaev honeycomb model has received significant attention for its exactly
solvable quantum spin liquid ground states and fractionalized excitations. For
realizing the model, layered cobalt oxides have been considered a promising
platform. Yet, in contrast to the conventional wisdom about single-$\mathbf{q}$
zigzag magnetic order inferred from previous studies of the Na$_2$IrO$_3$ and
$\alpha$-RuCl$_3$ candidate materials, recent experiments on two of the
representative honeycomb cobalt oxides, hexagonal Na$_2$Co$_2$TeO$_6$ and
monoclinic Na$_3$Co$_2$SbO$_6$, have uncovered evidence for more complex
multi-$\mathbf{q}$ variants of the zigzag order. This review surveys on
experimental strategies to distinguish between single- and multi-$\mathbf{q}$
orders, along with the crystallographic symmetries of the cobalt oxides in
comparison to the previously studied systems. General formation mechanism of
multi-$\mathbf{q}$ order is also briefly discussed. The goal is to provide some
rationales for examining the relevance of multi-$\mathbf{q}$ order in the
honeycomb cobalt oxides, along with its implications on the microscopic model
of these intriguing quantum magnets.",['cond-mat.str-el'],2501.02487," We report ACE++, an instruction-based diffusion framework that tackles
various image generation and editing tasks. Inspired by the input format for
the inpainting task proposed by FLUX.1-Fill-dev, we improve the Long-context
Condition Unit (LCU) introduced in ACE and extend this input paradigm to any
editing and generation tasks. To take full advantage of image generative
priors, we develop a two-stage training scheme to minimize the efforts of
finetuning powerful text-to-image diffusion models like FLUX.1-dev. In the
first stage, we pre-train the model using task data with the 0-ref tasks from
the text-to-image model. There are many models in the community based on the
post-training of text-to-image foundational models that meet this training
paradigm of the first stage. For example, FLUX.1-Fill-dev deals primarily with
painting tasks and can be used as an initialization to accelerate the training
process. In the second stage, we finetune the above model to support the
general instructions using all tasks defined in ACE. To promote the widespread
application of ACE++ in different scenarios, we provide a comprehensive set of
models that cover both full finetuning and lightweight finetuning, while
considering general applicability and applicability in vertical scenarios. The
qualitative analysis showcases the superiority of ACE++ in terms of generating
image quality and prompt following ability. Code and models will be available
on the project page: https://ali-vilab. github.io/ACE_plus_page/.",['cs.CV'],False,,,,"On the multi-$\mathbf{q}$ characteristics of magnetic ground states of
  honeycomb cobalt oxides","ACE++: Instruction-Based Image Creation and Editing via Context-Aware
  Content Filling"
neg-d2-938,2025-03-11,,2503.08224," Reconstructing animatable and high-quality 3D head avatars from monocular
videos, especially with realistic relighting, is a valuable task. However, the
limited information from single-view input, combined with the complex head
poses and facial movements, makes this challenging. Previous methods achieve
real-time performance by combining 3D Gaussian Splatting with a parametric head
model, but the resulting head quality suffers from inaccurate face tracking and
limited expressiveness of the deformation model. These methods also fail to
produce realistic effects under novel lighting conditions. To address these
issues, we propose HRAvatar, a 3DGS-based method that reconstructs
high-fidelity, relightable 3D head avatars. HRAvatar reduces tracking errors
through end-to-end optimization and better captures individual facial
deformations using learnable blendshapes and learnable linear blend skinning.
Additionally, it decomposes head appearance into several physical properties
and incorporates physically-based shading to account for environmental
lighting. Extensive experiments demonstrate that HRAvatar not only reconstructs
superior-quality heads but also achieves realistic visual effects under varying
lighting conditions.",['cs.CV'],2503.02345," The detection of Alzheimer disease (AD) from clinical MRI data is an active
area of research in medical imaging. Recent advances in quantum computing,
particularly the integration of parameterized quantum circuits (PQCs) with
classical machine learning architectures, offer new opportunities to develop
models that may outperform traditional methods. However, quantum machine
learning (QML) remains in its early stages and requires further experimental
analysis to better understand its behavior and limitations. In this paper, we
propose an end to end hybrid classical quantum convolutional neural network (CQ
CNN) for AD detection using clinically formatted 3D MRI data. Our approach
involves developing a framework to make 3D MRI data usable for machine
learning, designing and training a brain tissue segmentation model (Skull Net),
and training a diffusion model to generate synthetic images for the minority
class. Our converged models exhibit potential quantum advantages, achieving
higher accuracy in fewer epochs than classical models. The proposed beta8 3
qubit model achieves an accuracy of 97.50%, surpassing state of the art (SOTA)
models while requiring significantly fewer computational resources. In
particular, the architecture employs only 13K parameters (0.48 MB), reducing
the parameter count by more than 99.99% compared to current SOTA models.
Furthermore, the diffusion-generated data used to train our quantum models, in
conjunction with real samples, preserve clinical structural standards,
representing a notable first in the field of QML. We conclude that CQCNN
architecture like models, with further improvements in gradient optimization
techniques, could become a viable option and even a potential alternative to
classical models for AD detection, especially in data limited and resource
constrained clinical settings.","['quant-ph', 'cs.AI', 'cs.CV', 'cs.LG']",False,,,,HRAvatar: High-Quality and Relightable Gaussian Head Avatar,"CQ CNN: A Hybrid Classical Quantum Convolutional Neural Network for
  Alzheimer's Disease Detection Using Diffusion Generated and U Net Segmented
  3D MRI"
neg-d2-939,2025-03-19,,2503.14943," As the digital and physical worlds become more intertwined, there has been a
lot of interest in digital avatars that closely resemble their real-world
counterparts. Current digitization methods used in 3D production pipelines
require costly capture setups, making them impractical for mass usage among
common consumers. Recent academic literature has found success in
reconstructing humans from limited data using implicit representations (e.g.,
voxels used in NeRFs), which are able to produce impressive videos. However,
these methods are incompatible with traditional rendering pipelines, making it
difficult to use them in applications such as games. In this work, we propose
an end-to-end pipeline that builds explicitly-represented photorealistic 3D
avatars using standard 3D assets. Our key idea is the use of
dynamically-generated textures to enhance the realism and visually mask
deficiencies in the underlying mesh geometry. This allows for seamless
integration with current graphics pipelines while achieving comparable visual
quality to state-of-the-art 3D avatar generation methods.",['cs.CV'],2502.02531," We theoretically characterize gradient descent dynamics in deep linear
networks trained at large width from random initialization and on large
quantities of random data. Our theory captures the ``wider is better"" effect of
mean-field/maximum-update parameterized networks as well as hyperparameter
transfer effects, which can be contrasted with the neural-tangent
parameterization where optimal learning rates shift with model width. We
provide asymptotic descriptions of both non-residual and residual neural
networks, the latter of which enables an infinite depth limit when branches are
scaled as $1/\sqrt{\text{depth}}$. We also compare training with one-pass
stochastic gradient descent to the dynamics when training data are repeated at
each iteration. Lastly, we show that this model recovers the accelerated power
law training dynamics for power law structured data in the rich regime observed
in recent works.","['cs.LG', 'cond-mat.dis-nn', 'stat.ML']",False,,,,3D Engine-ready Photorealistic Avatars via Dynamic Textures,"Deep Linear Network Training Dynamics from Random Initialization: Data,
  Width, Depth, and Hyperparameter Transfer"
neg-d2-940,2025-01-14,,2501.08043," Standard deep neural network inference involves the computation of
interleaved linear maps and nonlinear activation functions. Prior work for
ultra-low latency implementations has hardcoded these operations inside FPGA
lookup tables (LUTs). However, FPGA LUTs can implement a much greater variety
of functions. In this paper, we propose a novel approach to training DNNs for
FPGA deployment using multivariate polynomials as the basic building block. Our
method takes advantage of the flexibility offered by the soft logic, hiding the
polynomial evaluation inside the LUTs with minimal overhead. By using
polynomial building blocks, we achieve the same accuracy using considerably
fewer layers of soft logic than by using linear functions, leading to
significant latency and area improvements. LUT-based implementations also face
a significant challenge: the LUT size grows exponentially with the number of
inputs. Prior work relies on a priori fixed sparsity, with results heavily
dependent on seed selection. To address this, we propose a structured pruning
strategy using a bespoke hardware-aware group regularizer that encourages a
particular sparsity pattern that leads to a small number of inputs per neuron.
We demonstrate the effectiveness of PolyLUT on three tasks: network intrusion
detection, jet identification at the CERN Large Hadron Collider, and MNIST.","['cs.LG', 'cs.AR']",2503.13533," As artificial intelligence (AI) technology becomes increasingly prevalent in
the filed of education, there is a growing need for mathematics teacher
education students (MTES) to demonstrate proficiency in the integration of AI
with the technological pedagogical content knowledge (AI-TPACK). To study the
issue, we firstly devised an systematic AI-TPACK scale and test on 412 MTES
from seven universities. Through descriptive statistical analyses, we found
that the current status of AI-TPACK for MTES in China is at a basic,
preliminary stage. Secondly, we compared MTES between three different grades on
the six variables and found that there is no discernible difference, which
suggested that graduate studies were observed to have no promotion in the
development of AI-TPACK competencies. Thirdly, we proposed a new AI-TPACK
structural equation model (AI-TPACK-SEM) to explore the impact of self-efficacy
and teaching beliefs on AI-TPACK. Our findings indicate a positive correlation
between self-efficacy and AI-TPACK. We also come to a conclusion that may be
contrary to common perception, excessive teaching beliefs may impede the
advancement of AI-TPACK. Overall, this paper revealed the current status of
AI-TPACK for MTES in China for the first time, designed a dedicated SEM to
study the effect of specific factors on AI-TPACK, and proposed some suggestions
on future developments.","['cs.CY', 'cs.AI']",False,,,,"PolyLUT: Ultra-low Latency Polynomial Inference with Hardware-Aware
  Structured Pruning","The Status Quo and Future of AI-TPACK for Mathematics Teacher Education
  Students: A Case Study in Chinese Universities"
neg-d2-941,2025-01-09,,2501.05312," While $Hf_{0.5}Zr_{0.5}O_2$ (HZO) thin films hold significant promise for
modern nanoelectronic devices, a comprehensive understanding of the interplay
between their polycrystalline structure and electrical properties remains
elusive. Here, we present a novel framework combining phase-field (PF) modeling
with Variational Autoencoders (VAEs) to uncover structure-property correlations
in polycrystalline HZO. Leveraging PF simulations, we constructed a
high-fidelity dataset of $P-V$ loops by systematically varying critical
material parameters, including grain size, polar grain fraction, and
crystalline orientation. The VAEs effectively encoded hysteresis loops into a
low-dimensional latent space, capturing electrical properties while
disentangling complex material parameters interdependencies. We further
demonstrate a VAE-based inverse design approach to optimize $P-V$ loop
features, enabling the tailored design of device-specific key performance
indicators (KPIs), including coercive field, remanent polarization, and loop
area. The proposed approach offers a pathway to systematically explore and
optimize the material design space for ferroelectric nanoelectronics.",['cond-mat.mtrl-sci'],2503.16003," Mechano-bactericidal (MB) surfaces have been proposed as an emerging strategy
for preventing biofilm formation. Unlike antibiotics and metal ions that
chemically interfere with cellular processes, MB nanostructures cause physical
damage to the bacteria. The antibacterial performance of artificial MB surfaces
relies on rational control of surface features, which is difficult to achieve
for large surfaces in real-life applications. Herein, we report a facile and
scalable method for fabricating MB surfaces based on metal-organic frameworks
(MOFs) using epitaxial MOF-on-MOF hybrids as building blocks with nanopillars
of less than 5 nm tip diameter, 200 nm base diameter, and 300 nm length. Two
methods of MOF surface assembly, in-situ growth and ex-situ dropcasting, result
in surfaces with nanopillars in different orientations, both presenting MB
actions (bactericidal efficiency of 83% for E. coli). Distinct MB mechanisms,
including stretching, impaling, and apoptosis-like death induced by mechanical
injury are discussed with the observed bacterial morphology on the obtained MOF
surfaces.",['physics.bio-ph'],False,,,,"Unveiling structure-property correlations in ferroelectric
  $Hf_{0.5}Zr_{0.5}O_2$ films using variational autoencoders","Mechano-Bactericidal Surfaces Achieved by Epitaxial Growth of
  Metal-Organic Frameworks"
neg-d2-942,2025-02-26,,2502.18852," Full Waveform Inversion (FWI) reconstructs high-resolution subsurface models
via multi-variate optimization but faces challenges with solver selection and
data availability. Deep Learning (DL) offers a promising alternative, bridging
data-driven and physics-based methods. While FWI in DL has been explored in the
time domain, the pseudo-spectral approach remains underutilized, despite its
success in classical FWI.
  This thesis integrates pseudo-spectral FWI into DL, formulating both
data-driven and theory-guided approaches using Deep Neural Networks (DNNs) and
Recurrent Neural Networks (RNNs). These methods were theoretically derived,
tested on synthetic and Marmousi datasets, and compared with deterministic and
time-domain approaches.
  Results show that data-driven pseudo-spectral DNNs outperform classical FWI
in deeper and over-thrust regions due to their global approximation capability.
Theory-guided RNNs yield greater accuracy, with lower error and better fault
identification. While DNNs excel in velocity contrast recovery, RNNs provide
superior edge definition and stability in shallow and deep sections.
  Beyond enhancing FWI performance, this research identifies broader
applications of DL-based inversion and outlines future directions for these
frameworks.","['physics.geo-ph', 'cs.LG']",2503.04313," Infinitesimals have seen ups and downs in their tumultuous history. In the
18th century, d'Alembert set the tone by describing infinitesimals as chimeras.
Some adversaries of infinitesimals, including Moigno and Connes, picked up on
the term. We highlight the work of Cauchy, No\""el, Poisson and Riemann. We also
chronicle reactions by Moigno, Lamarle and Cantor, and signal the start of a
revival with Peano.",['math.HO'],False,,,,"Data-Driven and Theory-Guided Pseudo-Spectral Seismic Imaging Using Deep
  Neural Network Architectures",Episodes from the history of infinitesimals
neg-d2-943,2025-03-14,,2503.1129," Affective Image Manipulation (AIM) aims to alter an image's emotional impact
by adjusting multiple visual elements to evoke specific feelings.Effective AIM
is inherently complex, necessitating a collaborative approach that involves
identifying semantic cues within source images, manipulating these elements to
elicit desired emotional responses, and verifying that the combined adjustments
successfully evoke the target emotion.To address these challenges, we introduce
EmoAgent, the first multi-agent collaboration framework for AIM. By emulating
the cognitive behaviors of a human painter, EmoAgent incorporates three
specialized agents responsible for planning, editing, and critical evaluation.
Furthermore, we develop an emotion-factor knowledge retriever, a
decision-making tree space, and a tool library to enhance EmoAgent's
effectiveness in handling AIM. Experiments demonstrate that the proposed
multi-agent framework outperforms existing methods, offering more reasonable
and effective emotional expression.","['cs.CV', 'eess.IV']",2502.01302," Dynamical tide consists of various waves that can resonate with orbital
motion. We test this coupling of dynamical tide and orbital motion using a
simple two-dimensional shallow water model, which can be applied to a rocky
planet covered with thin ocean or atmosphere. Then we take the earth-moon
system as a fiducial model to calculate the tidal resonances and orbital
evolution. We find that tidal dissipation can even increase with increasing
orbital separation because of the coupling of dynamical tide and orbital
motion. We draw the conclusion that the coupling is not negligible to study the
orbital evolution on secular timescale.","['astro-ph.EP', 'astro-ph.SR', 'physics.geo-ph']",False,,,,"EmoAgent: Multi-Agent Collaboration of Plan, Edit, and Critic, for
  Affective Image Manipulation",Coupling of dynamical tide and orbital motion
neg-d2-944,2025-03-05,,2503.04003," Mobile platforms now power not only smartphones but also in-vehicle systems
like Android Auto and CarPlay. Despite an ecosystem of over 3.5 million Android
apps and more than 200 million Android Auto-compatible vehicles, only a few
hundred apps have been adapted for automotive use. To better understand this
gap, we studied 147 reported issues related to Android Auto and identified
their root causes. We found that more than 70% of issues result from UI
incompatibilities, 24% from media playback errors, and around 5% from failures
in voice command handling, showing a lack of effective tools for developers. We
introduce CarCompat, a static analysis framework that detects compatibility
problems in Android Auto apps. CarCompat constructs a Car-Control Flow Graph
(CCFG) to capture interactions among app components, lifecycle methods, and
platform-specific callbacks. It applies specialized checkers to detect UI
violations, media playback errors, and issues with voice command handling. We
evaluated CarCompat on a dataset of 54 Android Auto apps and detected 25 new
issues, 4 of which were confirmed by developers, and 2 developers have already
released their fixes. The results show that CarCompat helps developers identify
and fix compatibility issues, improving the in-vehicle experience.","['cs.SE', 'cs.PL']",2503.14963," Multimodal contrastive learning (MCL) advances in aligning different
modalities and generating multimodal representations in a joint space. By
leveraging contrastive learning across diverse modalities, large-scale
multimodal data enhances representational quality. However, a critical yet
often overlooked challenge remains: multimodal data is rarely collected in a
single process, and training from scratch is computationally expensive.
Instead, emergent multimodal data can be used to optimize existing models
gradually, \textit{i.e.}, models are trained on a sequence of modality pair
data. We define this problem as Continual Multimodal Contrastive Learning
(CMCL), an underexplored yet crucial research direction at the intersection of
multimodal and continual learning. In this paper, we formulate CMCL through two
specialized principles of stability and plasticity. We theoretically derive a
novel optimization-based method, which projects updated gradients from dual
sides onto subspaces where any gradient is prevented from interfering with the
previously learned knowledge. Two upper bounds provide theoretical insights on
both stability and plasticity in our solution. Beyond our theoretical
contributions, we conduct experiments on multiple datasets by comparing our
method against advanced continual learning baselines. The empirical results
further support our claims and demonstrate the efficacy of our method. The code
will be publicly available.",['cs.LG'],False,,,,Understanding and Detecting Compatibility Issues in Android Auto Apps,Continual Multimodal Contrastive Learning
neg-d2-945,2025-02-07,,2502.04901," This work investigates the theoretical boundaries of creating
publicly-detectable schemes to enable the provenance of watermarked imagery.
Metadata-based approaches like C2PA provide unforgeability and
public-detectability. ML techniques offer robust retrieval and watermarking.
However, no existing scheme combines robustness, unforgeability, and
public-detectability. In this work, we formally define such a scheme and
establish its existence. Although theoretically possible, we find that at
present, it is intractable to build certain components of our scheme without a
leap in deep learning capabilities. We analyze these limitations and propose
research directions that need to be addressed before we can practically realize
robust and publicly-verifiable provenance.","['cs.CR', 'cs.LG']",2501.17868," Due to its ability to precisely control wireless beams, holographic
multiple-input multiple-output (HMIMO) is expected to be a promising solution
to achieve high-accuracy localization. However, as the scale of HMIMO increases
to improve beam control capability, the corresponding near-field (NF) region
expands, indicating that users may exist in both NF and far-field (FF) regions
with different electromagnetic transmission characteristics. As a result,
existing methods for pure NF or FF localization are no longer applicable. We
consider a hybrid NF and FF localization scenario in this paper, where a base
station (BS) locates multiple users in both NF and FF regions with the aid of a
reconfigurable intelligent surface (RIS), which is a low-cost implementation of
HMIMO. In such a scenario, it is difficult to locate the users and optimize the
RIS phase shifts because whether the location of the user is in the NF or FF
region is unknown, and the channels of different users are coupled. To tackle
this challenge, we propose a RIS-enabled localization method that searches the
users in both NF and FF regions and tackles the coupling issue by jointly
estimating all user locations. We derive the localization error bound by
considering the channel coupling and propose an RIS phase shift optimization
algorithm that minimizes the derived bound. Simulations show the effectiveness
of the proposed method and demonstrate the performance gain compared to pure NF
and FF techniques.",['eess.SP'],False,,,,"On the Difficulty of Constructing a Robust and Publicly-Detectable
  Watermark",Hybrid Near-field and Far-field Localization with Holographic MIMO
neg-d2-946,2025-01-13,,2501.07616," Ingenuity is an autonomous Cyber-Pysical System (CPS) that has successfully
completed more than 70 flights over Mars between 2021 and 2024. Ensuring the
safety of its mission is paramount, as any failure could result in catastrophic
economic damage and significant financial losses. Dataflow Models of
Computation and Communication (DF MoCCs) serve as a formal framework for
specifying and analyzing the timing behavior of such CPSs. In particular, the
Real-time Mode-aware Dataflow (RMDF) model is highly suitable to specify and
analyze real-time and mode-dependent Cyber-Physical Systems (CPSs) like
Ingenuity. This paper showcases the application of RMDF for the specification
and analysis of Ingenuity. We propose a dataflow specification of Ingenuity,
analyze its timing behavior, and provide a feasibility test. Finally, we
proposed a plausible explanation of the timing anomaly that occurred during the
sixth flight of Ingenuity.","['eess.SY', 'cs.SY']",2502.03732," Anxiety, depression, and suicidality are common mental health sequelae
following concussion in youth patients, often exacerbating concussion symptoms
and prolonging recovery. Despite the critical need for early detection of these
mental health symptoms, clinicians often face challenges in accurately
collecting patients' mental health data and making clinical decision-making in
a timely manner. Today's remote patient monitoring (RPM) technologies offer
opportunities to objectively monitor patients' activities, but they were not
specifically designed for youth concussion patients; moreover, the large amount
of data collected by RPM technologies may also impose significant workloads on
clinicians to keep up with and use the data. To address these gaps, we employed
a three-stage study consisting of a formative study, interface design, and
design evaluation. We first conducted a formative study through semi-structured
interviews with six highly professional concussion clinicians and identified
clinicians' key challenges in remotely collecting patient information and
accessing patient treatment compliance. Subsequently, we proposed preliminary
clinician-facing interface designs with the integration of AI-based RPM
technologies (AI-RPM), followed by design evaluation sessions with highly
professional concussion clinicians. Clinicians underscored the value of
integrating multi-modal AI-RPM technologies to support clinicians'
decision-making while emphasizing the importance of customizable interfaces
with explainability and multiple responsible design considerations.",['cs.HC'],False,,,,"The Ingenuity Mars Helicopter Specified and Analyzed with the Real-time
  Mode-aware Dataflow Model","More Modality, More AI: Exploring Design Opportunities of AI-Based
  Multi-modal Remote Monitoring Technologies for Early Detection of Mental
  Health Sequelae in Youth Concussion Patients"
neg-d2-947,2025-03-07,,2503.05682," Multi-contrast magnetic resonance imaging (MRI) plays a vital role in brain
tumor segmentation and diagnosis by leveraging complementary information from
different contrasts. Each contrast highlights specific tumor characteristics,
enabling a comprehensive understanding of tumor morphology, edema, and
pathological heterogeneity. However, existing methods still face the challenges
of multi-level specificity perception across different contrasts, especially
with limited annotations. These challenges include data heterogeneity,
granularity differences, and interference from redundant information. To
address these limitations, we propose a Task-oriented Uncertainty Collaborative
Learning (TUCL) framework for multi-contrast MRI segmentation. TUCL introduces
a task-oriented prompt attention (TPA) module with intra-prompt and
cross-prompt attention mechanisms to dynamically model feature interactions
across contrasts and tasks. Additionally, a cyclic process is designed to map
the predictions back to the prompt to ensure that the prompts are effectively
utilized. In the decoding stage, the TUCL framework proposes a dual-path
uncertainty refinement (DUR) strategy which ensures robust segmentation by
refining predictions iteratively. Extensive experimental results on limited
labeled data demonstrate that TUCL significantly improves segmentation accuracy
(88.2\% in Dice and 10.853 mm in HD95). It shows that TUCL has the potential to
extract multi-contrast information and reduce the reliance on extensive
annotations. The code is available at:
https://github.com/Zhenxuan-Zhang/TUCL_BrainSeg.","['eess.IV', 'cs.CV']",2501.02509," Facial attractiveness prediction (FAP) has long been an important computer
vision task, which could be widely applied in live streaming for facial
retouching, content recommendation, etc. However, previous FAP datasets are
either small, closed-source, or lack diversity. Moreover, the corresponding FAP
models exhibit limited generalization and adaptation ability. To overcome these
limitations, in this paper we present LiveBeauty, the first large-scale
live-specific FAP dataset, in a more challenging application scenario, i.e.,
live streaming. 10,000 face images are collected from a live streaming platform
directly, with 200,000 corresponding attractiveness annotations obtained from a
well-devised subjective experiment, making LiveBeauty the largest open-access
FAP dataset in the challenging live scenario. Furthermore, a multi-modal FAP
method is proposed to measure the facial attractiveness in live streaming.
Specifically, we first extract holistic facial prior knowledge and multi-modal
aesthetic semantic features via a Personalized Attractiveness Prior Module
(PAPM) and a Multi-modal Attractiveness Encoder Module (MAEM), respectively,
then integrate the extracted features through a Cross-Modal Fusion Module
(CMFM). Extensive experiments conducted on both LiveBeauty and other
open-source FAP datasets demonstrate that our proposed method achieves
state-of-the-art performance. Dataset will be available soon.",['cs.CV'],False,,,,"Task-oriented Uncertainty Collaborative Learning for Label-Efficient
  Brain Tumor Segmentation","Facial Attractiveness Prediction in Live Streaming: A New Benchmark and
  Multi-modal Method"
neg-d2-948,2025-02-13,,2502.09434," Diffusion models, known for their tremendous ability to generate high-quality
samples, have recently raised concerns due to their data memorization behavior,
which poses privacy risks. Recent methods for memory mitigation have primarily
addressed the issue within the context of the text modality in cross-modal
generation tasks, restricting their applicability to specific conditions. In
this paper, we propose a novel method for diffusion models from the perspective
of visual modality, which is more generic and fundamental for mitigating
memorization. Directly exposing visual data to the model increases memorization
risk, so we design a framework where models learn through proxy model
parameters instead. Specially, the training dataset is divided into multiple
shards, with each shard training a proxy model, then aggregated to form the
final model. Additionally, practical analysis of training losses illustrates
that the losses for easily memorable images tend to be obviously lower. Thus,
we skip the samples with abnormally low loss values from the current mini-batch
to avoid memorizing. However, balancing the need to skip memorization-prone
samples while maintaining sufficient training data for high-quality image
generation presents a key challenge. Thus, we propose IET-AGC+, which
redistributes highly memorizable samples between shards, to mitigate these
samples from over-skipping. Furthermore, we dynamically augment samples based
on their loss values to further reduce memorization. Extensive experiments and
analysis on four datasets show that our method successfully reduces memory
capacity while maintaining performance. Moreover, we fine-tune the pre-trained
diffusion models, e.g., Stable Diffusion, and decrease the memorization score
by 46.7\%, demonstrating the effectiveness of our method. Code is available in:
https://github.com/liuxiao-guan/IET_AGC.",['cs.CV'],2502.0613," While recent Large Vision-Language Models (LVLMs) have shown remarkable
performance in multi-modal tasks, they are prone to generating hallucinatory
text responses that do not align with the given visual input, which restricts
their practical applicability in real-world scenarios. In this work, inspired
by the observation that the text-to-image generation process is the inverse of
image-conditioned response generation in LVLMs, we explore the potential of
leveraging text-to-image generative models to assist in mitigating
hallucinations in LVLMs. We discover that generative models can offer valuable
self-feedback for mitigating hallucinations at both the response and token
levels. Building on this insight, we introduce self-correcting Decoding with
Generative Feedback (DeGF), a novel training-free algorithm that incorporates
feedback from text-to-image generative models into the decoding process to
effectively mitigate hallucinations in LVLMs. Specifically, DeGF generates an
image from the initial response produced by LVLMs, which acts as an auxiliary
visual reference and provides self-feedback to verify and correct the initial
response through complementary or contrastive decoding. Extensive experimental
results validate the effectiveness of our approach in mitigating diverse types
of hallucinations, consistently surpassing state-of-the-art methods across six
benchmarks. Code is available at https://github.com/zhangce01/DeGF.","['cs.CV', 'cs.CL']",False,,,,"Redistribute Ensemble Training for Mitigating Memorization in Diffusion
  Models","Self-Correcting Decoding with Generative Feedback for Mitigating
  Hallucinations in Large Vision-Language Models"
neg-d2-949,2025-01-10,,2501.06062," In many practical natural language applications, user data are highly
sensitive, requiring anonymous uploads of text data from mobile devices to the
cloud without user identifiers. However, the absence of user identifiers
restricts the ability of cloud-based language models to provide personalized
services, which are essential for catering to diverse user needs. The trivial
method of replacing an explicit user identifier with a static user embedding as
model input still compromises data anonymization. In this work, we propose to
let each mobile device maintain a user-specific distribution to dynamically
generate user embeddings, thereby breaking the one-to-one mapping between an
embedding and a specific user. We further theoretically demonstrate that to
prevent the cloud from tracking users via uploaded embeddings, the local
distributions of different users should either be derived from a linearly
dependent space to avoid identifiability or be close to each other to prevent
accurate attribution. Evaluation on both public and industrial datasets using
different language models reveals a remarkable improvement in accuracy from
incorporating anonymous user embeddings, while preserving real-time inference
requirement.",['cs.LG'],2502.17894," Object fetching from cluttered shelves is an important capability for robots
to assist humans in real-world scenarios. Achieving this task demands robotic
behaviors that prioritize safety by minimizing disturbances to surrounding
objects, an essential but highly challenging requirement due to restricted
motion space, limited fields of view, and complex object dynamics. In this
paper, we introduce FetchBot, a sim-to-real framework designed to enable
zero-shot generalizable and safety-aware object fetching from cluttered shelves
in real-world settings. To address data scarcity, we propose an efficient
voxel-based method for generating diverse simulated cluttered shelf scenes at
scale and train a dynamics-aware reinforcement learning (RL) policy to generate
object fetching trajectories within these scenes. This RL policy, which
leverages oracle information, is subsequently distilled into a vision-based
policy for real-world deployment. Considering that sim-to-real discrepancies
stem from texture variations mostly while from geometric dimensions rarely, we
propose to adopt depth information estimated by full-fledged depth foundation
models as the input for the vision-based policy to mitigate sim-to-real gap. To
tackle the challenge of limited views, we design a novel architecture for
learning multi-view representations, allowing for comprehensive encoding of
cluttered shelf scenes. This enables FetchBot to effectively minimize
collisions while fetching objects from varying positions and depths, ensuring
robust and safety-aware operation. Both simulation and real-robot experiments
demonstrate FetchBot's superior generalization ability, particularly in
handling a broad range of real-world scenarios, includ","['cs.RO', 'cs.CV']",False,,,,"Personalized Language Model Learning on Text Data Without User
  Identifiers",FetchBot: Object Fetching in Cluttered Shelves via Zero-Shot Sim2Real
neg-d2-950,2025-02-10,,2502.06496," The Transfer Matrix Method (TMM) is a widely used technique for modeling
linear propagation of electromagnetic waves through stratified layered media.
However, since its extension to inhomogeneous and nonlinear systems is not
straightforward, much more computationally demanding methods such as
Finite-difference time-domain (FDTD) or Method of lines (MoL) are typically
used. In this work, we extend the TMM framework to incorporate the effects of
nonlinearity. We consider the case when strong coupling between excitons
(electron-hole pairs) and photons leads to the formation of exciton-polaritons.
This extension is crucial for accurately simulating the behavior of light in
polariton microcavities, where nonlinearities arising from exciton-exciton
interactions play a key role. We perform efficient simulations of light
transmission and reflection in a multidimensional system using the plane wave
basis. Additionally, we compare our extended TMM approach with the
state-of-the-art admittance transfer method, and highlight the computational
advantage of extended TMM for large-scale systems. The extended TMM not only
provides a robust and computationally efficient numerical framework, but also
paves the way for the development of future low-power nonlinear optical
devices, polariton-based photonic circuits, and quantum photonic technologies.",['physics.optics'],2501.00986," Star formation quenching in galaxies is a critical process in galaxy
formation. It is widely believed that the quenching process is dominated by the
mass of galaxies and/or their environment. In Paper V, we addressed the
challenge to disentangle the effects of mass and environment by employing the
PAC method, which combines spectroscopic and deep photometric surveys. This
approach enabled us to measure the excess surface density of blue and red
galaxies around massive central galaxies down to $10^{9.0}M_{\odot}$. However,
it is not straightforward to completely separate the two effects. To address
this issue, in this paper, we derive the average quenched fraction of central
(isolated) galaxies, $\bar{f}_{\mathrm{q}}^{\mathrm{cen}}(M_{*})$, by combining
the 3D quenched fraction distribution $f^{\mathrm{sat}}_{\mathrm{q}}(r;
M_{*,\mathrm{cen}}, M_{*,\mathrm{sat}})$, reconstructed from the
$\bar{n}_2w_{\mathrm{p}}(r_{\mathrm{p}})$ measurements, with the stellar
mass-halo mass relation in N-body simulations from Paper IV, and the observed
total quenched fraction, $\bar{f}_{\mathrm{q}}^{\mathrm{all}}(M_{*})$. Using
$f^{\mathrm{sat}}_{\mathrm{q}}(r;M_{*,\mathrm{cen}},M_{*,\mathrm{sat}})$,
$\bar{f}_{\mathrm{q}}^{\mathrm{cen}}(M_{*})$, and the galaxy-halo connection,
we assign a quenched probability to each (sub)halo in the simulation, enabling
a comprehensive study of galaxy quenching. We find that the mass-quenched
fraction increases from 0.3 to 0.87 across the stellar mass range $[10^{9.5},
10^{11.0}]M_{\odot}$, while the environmental quenched fraction decreases from
0.17 to 0.03. The mass effect dominates galaxy quenching across the entire
stellar mass range we studied. Moreover, more massive host halos are more
effective at quenching their satellite galaxies, while satellite stellar mass
has minimal influence on environmental quenching.",['astro-ph.GA'],False,,,,Modeling Nonlinear Optics with the Transfer Matrix Method,"Photometric Objects Around Cosmic Webs (PAC). VII. Disentangling Mass
  and Environment Quenching with the Aid of Galaxy-halo Connection in
  Simulations"
neg-d2-951,2025-02-03,,2502.0194," We present a cost-effective new approach for generating denser depth maps for
Autonomous Driving (AD) and Autonomous Vehicles (AVs) by integrating the images
obtained from deep neural network (DNN) 4D radar detectors with conventional
camera RGB images. Our approach introduces a novel pixel positional encoding
algorithm inspired by Bartlett's spatial spectrum estimation technique. This
algorithm transforms both radar depth maps and RGB images into a unified pixel
image subspace called the Spatial Spectrum, facilitating effective learning
based on their similarities and differences. Our method effectively leverages
high-resolution camera images to train radar depth map generative models,
addressing the limitations of conventional radar detectors in complex vehicular
environments, thus sharpening the radar output. We develop spectrum estimation
algorithms tailored for radar depth maps and RGB images, a comprehensive
training framework for data-driven generative models, and a camera-radar
deployment scheme for AV operation. Our results demonstrate that our approach
also outperforms the state-of-the-art (SOTA) by 27.95% in terms of
Unidirectional Chamfer Distance (UCD).","['cs.CV', 'eess.IV']",2502.12108," Integrated Gradients (IG), a widely used axiomatic path-based attribution
method, assigns importance scores to input features by integrating model
gradients along a straight path from a baseline to the input. While effective
in some cases, we show that straight paths can lead to flawed attributions. In
this paper, we identify the cause of these misattributions and propose an
alternative approach that treats the input space as a Riemannian manifold,
computing attributions by integrating gradients along geodesics. We call this
method Geodesic Integrated Gradients (GIG). To approximate geodesic paths, we
introduce two techniques: a k-Nearest Neighbours-based approach for smaller
models and a Stochastic Variational Inference-based method for larger ones.
Additionally, we propose a new axiom, Strong Completeness, extending the axioms
satisfied by IG. We show that this property is desirable for attribution
methods and that GIG is the only method that satisfies it. Through experiments
on both synthetic and real-world data, we demonstrate that GIG outperforms
existing explainability methods, including IG.","['cs.LG', 'cs.AI', 'stat.ML']",False,,,,"Toward a Low-Cost Perception System in Autonomous Vehicles: A Spectrum
  Learning Approach",Using the Path of Least Resistance to Explain Deep Networks
neg-d2-952,2025-02-04,,2502.02494," Similarity between training examples is used to curate pretraining datasets
for language models by many methods -- for diversification and to select
examples similar to high-quality data. However, similarity is typically
measured with off-the-shelf embedding models that are generic or trained for
tasks such as retrieval. This paper introduces a framework to analyze the
suitability of embedding models specifically for data curation in the language
model pretraining setting. We quantify the correlation between similarity in
the embedding space to similarity in pretraining loss between different
training examples, and how diversifying in the embedding space affects
pretraining quality. We analyze a variety of embedding models in our framework,
with experiments using the Pile dataset for pretraining a 1.7B parameter
decoder-only language model. We find that the embedding models we consider are
all useful for pretraining data curation. Moreover, a simple approach of
averaging per-token embeddings proves to be surprisingly competitive with more
sophisticated embedding models -- likely because the latter are not designed
specifically for pretraining data curation. Indeed, we believe our analysis and
evaluation framework can serve as a foundation for the design of embedding
models that specifically reason about similarity in pretraining datasets.","['cs.LG', 'cs.CL']",2501.06813," Subset selection is a fundamental problem in combinatorial optimization,
which has a wide range of applications such as influence maximization and
sparse regression. The goal is to select a subset of limited size from a ground
set in order to maximize a given objective function. However, the evaluation of
the objective function in real-world scenarios is often noisy. Previous
algorithms, including the greedy algorithm and multi-objective evolutionary
algorithms POSS and PONSS, either struggle in noisy environments or consume
excessive computational resources. In this paper, we focus on the noisy subset
selection problem with a cardinality constraint, where the evaluation of a
subset is noisy. We propose a novel approach based on Pareto Optimization with
Robust Evaluation for noisy subset selection (PORE), which maximizes a robust
evaluation function and minimizes the subset size simultaneously. PORE can
efficiently identify well-structured solutions and handle computational
resources, addressing the limitations observed in PONSS. Our experiments,
conducted on real-world datasets for influence maximization and sparse
regression, demonstrate that PORE significantly outperforms previous methods,
including the classical greedy algorithm, POSS, and PONSS. Further validation
through ablation studies confirms the effectiveness of our robust evaluation
function.",['cs.NE'],False,,,,"Analyzing Similarity Metrics for Data Selection for Language Model
  Pretraining",Pareto Optimization with Robust Evaluation for Noisy Subset Selection
neg-d2-953,2025-02-04,,2502.02717," Foundational models have emerged as a powerful paradigm in deep learning
field, leveraging their capacity to learn robust representations from
large-scale datasets and effectively to diverse downstream applications such as
classification. In this paper, we present Astromer 2 a foundational model
specifically designed for extracting light curve embeddings. We introduce
Astromer 2 as an enhanced iteration of our self-supervised model for light
curve analysis. This paper highlights the advantages of its pre-trained
embeddings, compares its performance with that of its predecessor, Astromer 1,
and provides a detailed empirical analysis of its capabilities, offering deeper
insights into the model's representations. Astromer 2 is pretrained on 1.5
million single-band light curves from the MACHO survey using a self-supervised
learning task that predicts randomly masked observations within sequences.
Fine-tuning on a smaller labeled dataset allows us to assess its performance in
classification tasks. The quality of the embeddings is measured by the F1 score
of an MLP classifier trained on Astromer-generated embeddings. Our results
demonstrate that Astromer 2 significantly outperforms Astromer 1 across all
evaluated scenarios, including limited datasets of 20, 100, and 500 samples per
class. The use of weighted per-sample embeddings, which integrate intermediate
representations from Astromer's attention blocks, is particularly impactful.
Notably, Astromer 2 achieves a 15% improvement in F1 score on the ATLAS dataset
compared to prior models, showcasing robust generalization to new datasets.
This enhanced performance, especially with minimal labeled data, underscores
the potential of Astromer 2 for more efficient and scalable light curve
analysis.","['astro-ph.IM', 'cs.AI', 'cs.LG']",2501.17489," Brain-computer interfaces (BCIs) present a promising avenue by translating
neural activity directly into text, eliminating the need for physical actions.
However, existing non-invasive BCI systems have not successfully covered the
entire alphabet, limiting their practicality. In this paper, we propose a novel
non-invasive EEG-based BCI system with Curriculum-based Neural Spelling
Framework, which recognizes all 26 alphabet letters by decoding neural signals
associated with handwriting first, and then apply a Generative AI (GenAI) to
enhance spell-based neural language decoding tasks. Our approach combines the
ease of handwriting with the accessibility of EEG technology, utilizing
advanced neural decoding algorithms and pre-trained large language models
(LLMs) to translate EEG patterns into text with high accuracy. This system show
how GenAI can improve the performance of typical spelling-based neural language
decoding task, and addresses the limitations of previous methods, offering a
scalable and user-friendly solution for individuals with communication
impairments, thereby enhancing inclusive communication options.","['cs.HC', 'cs.AI']",False,,,,Astromer 2,Neural Spelling: A Spell-Based BCI System for Language Neural Decoding
neg-d2-954,2025-02-13,,2502.09528," Machine learning algorithms have enabled high quality stereo depth estimation
to run on Augmented and Virtual Reality (AR/VR) devices. However, high energy
consumption across the full image processing stack prevents stereo depth
algorithms from running effectively on battery-limited devices. This paper
introduces SteROI-D, a full stereo depth system paired with a mapping
methodology. SteROI-D exploits Region-of-Interest (ROI) and temporal sparsity
at the system level to save energy. SteROI-D's flexible and heterogeneous
compute fabric supports diverse ROIs. Importantly, we introduce a systematic
mapping methodology to effectively handle dynamic ROIs, thereby maximizing
energy savings. Using these techniques, our 28nm prototype SteROI-D design
achieves up to 4.35x reduction in total system energy compared to a baseline
ASIC.","['cs.CV', 'cs.AR']",2501.14948," Histopathology, particularly hematoxylin and eosin (H\&E) staining, plays a
critical role in diagnosing and characterizing pathological conditions by
highlighting tissue morphology. However, H\&E-stained images inherently lack
molecular information, requiring costly and resource-intensive methods like
spatial transcriptomics to map gene expression with spatial resolution. To
address these challenges, we introduce HECLIP (Histology-Enhanced Contrastive
Learning for Imputation of Profiles), an innovative deep learning framework
that bridges the gap between histological imaging and molecular profiling.
HECLIP is specifically designed to infer gene expression profiles directly from
H\&E-stained images, eliminating the need for expensive spatial transcriptomics
assays. HECLIP leverages an advanced image-centric contrastive loss function to
optimize image representation learning, ensuring that critical morphological
patterns in histology images are effectively captured and translated into
accurate gene expression profiles. This design enhances the predictive power of
the image modality while minimizing reliance on gene expression data. Through
extensive benchmarking on publicly available datasets, HECLIP demonstrates
superior performance compared to existing approaches, delivering robust and
biologically meaningful predictions. Detailed ablation studies further
underscore its effectiveness in extracting molecular insights from histology
images. Additionally, HECLIP's scalable and cost-efficient approach positions
it as a transformative tool for both research and clinical applications,
driving advancements in precision medicine. The source code for HECLIP is
openly available at https://github.com/QSong-github/HECLIP.","['cs.CE', 'q-bio.QM']",False,,,,"SteROI-D: System Design and Mapping for Stereo Depth Inference on
  Regions of Interest","HECLIP: Histology-Enhanced Contrastive Learning for Imputation of
  Transcriptomics Profiles"
neg-d2-955,2025-02-03,,2503.11655," Recent advancements in large language models (LLMs) have significantly
enhanced sentiment analysis capabilities. However, the trade-offs between model
performance, efficiency, and explainability of some latest models remain
underexplored. This study presents the first comprehensive evaluation of the
DeepSeek-R1 series of models, reasoning open-source LLMs, for sentiment
analysis, comparing them against OpenAI's GPT-4 and GPT-4-mini. We
systematically analyze their performance under few-shot prompting conditions,
scaling up to 50-shot configurations to assess in-context learning
effectiveness. Our experiments reveal that DeepSeek-R1 demonstrates competitive
accuracy, particularly in multi-class sentiment tasks, while offering enhanced
interpretability through its detailed reasoning process. Additionally, we
highlight the impact of increasing few-shot examples on model performance and
discuss key trade-offs between explainability and computational efficiency.","['cs.CL', 'cs.AI']",2502.17425," To utilize visual information, Multimodal Large Language Model (MLLM) relies
on the perception process of its vision encoder. The completeness and accuracy
of visual perception significantly influence the precision of spatial
reasoning, fine-grained understanding, and other tasks. However, MLLM still
lacks the autonomous capability to control its own visual perception processes,
for example, selectively reviewing specific regions of an image or focusing on
information related to specific object categories. In this work, we propose the
concept of Visual Perception Token, aiming to empower MLLM with a mechanism to
control its visual perception processes. We design two types of Visual
Perception Tokens, termed the Region Selection Token and the Vision Re-Encoding
Token. MLLMs autonomously generate these tokens, just as they generate text,
and use them to trigger additional visual perception actions. The Region
Selection Token explicitly identifies specific regions in an image that require
further perception, while the Vision Re-Encoding Token uses its hidden states
as control signals to guide additional visual perception processes. Extensive
experiments demonstrate the advantages of these tokens in handling spatial
reasoning, improving fine-grained understanding, and other tasks. On average,
the introduction of Visual Perception Tokens improves the performance of a 2B
model by 23.6\%, increasing its score from 0.572 to 0.708, and even outperforms
a 7B parameter model by 13.4\% (from 0.624). Please check out our repo
https://github.com/yu-rp/VisualPerceptionToken","['cs.CV', 'cs.LG']",False,,,,"Explainable Sentiment Analysis with DeepSeek-R1: Performance,
  Efficiency, and Few-Shot Learning",Introducing Visual Perception Token into Multimodal Large Language Model
neg-d2-956,2025-02-24,,2502.17425," To utilize visual information, Multimodal Large Language Model (MLLM) relies
on the perception process of its vision encoder. The completeness and accuracy
of visual perception significantly influence the precision of spatial
reasoning, fine-grained understanding, and other tasks. However, MLLM still
lacks the autonomous capability to control its own visual perception processes,
for example, selectively reviewing specific regions of an image or focusing on
information related to specific object categories. In this work, we propose the
concept of Visual Perception Token, aiming to empower MLLM with a mechanism to
control its visual perception processes. We design two types of Visual
Perception Tokens, termed the Region Selection Token and the Vision Re-Encoding
Token. MLLMs autonomously generate these tokens, just as they generate text,
and use them to trigger additional visual perception actions. The Region
Selection Token explicitly identifies specific regions in an image that require
further perception, while the Vision Re-Encoding Token uses its hidden states
as control signals to guide additional visual perception processes. Extensive
experiments demonstrate the advantages of these tokens in handling spatial
reasoning, improving fine-grained understanding, and other tasks. On average,
the introduction of Visual Perception Tokens improves the performance of a 2B
model by 23.6\%, increasing its score from 0.572 to 0.708, and even outperforms
a 7B parameter model by 13.4\% (from 0.624). Please check out our repo
https://github.com/yu-rp/VisualPerceptionToken","['cs.CV', 'cs.LG']",2502.07126," We propose to relax traditional axioms in decision theory by incorporating a
measurement, or degree, of satisfaction. For example, if the independence axiom
of expected utility theory is violated, we can measure the size of the
violation. This measure allows us to derive an approximation guarantee for a
utility representation that aligns with the unmodified version of the axiom.
Almost satisfying the axiom implies, then, a utility that is near a utility
representation. We develop specific examples drawn from expected utility theory
under risk and uncertainty.",['econ.TH'],False,,,,Introducing Visual Perception Token into Multimodal Large Language Model,"Decision theory and the ""almost implies near"" phenomenon"
neg-d2-957,2025-02-05,,2502.03304," Large language models (LLMs) excel across various tasks, but standard
first-order (FO) fine-tuning demands considerable memory, significantly
limiting real-world deployment. Recently, zeroth-order (ZO) optimization stood
out as a promising memory-efficient training paradigm, avoiding backward passes
and relying solely on forward passes for gradient estimation, making it
attractive for resource-constrained scenarios. However, ZO method lags far
behind FO method in both convergence speed and accuracy. To bridge the gap, we
introduce a novel layer-wise divergence analysis that uncovers the distinct
update pattern of FO and ZO optimization. Aiming to resemble the learning
capacity of FO method from the findings, we propose \textbf{Di}vergence-driven
\textbf{Z}eroth-\textbf{O}rder (\textbf{DiZO}) optimization. DiZO conducts
divergence-driven layer adaptation by incorporating projections to ZO updates,
generating diverse-magnitude updates precisely scaled to layer-wise individual
optimization needs. Our results demonstrate that DiZO significantly reduces the
needed iterations for convergence without sacrificing throughput, cutting
training GPU hours by up to 48\% on various datasets. Moreover, DiZO
consistently outperforms the representative ZO baselines in fine-tuning
RoBERTa-large, OPT-series, and Llama-series on downstream tasks and, in some
cases, even surpasses memory-intensive FO fine-tuning.","['cs.LG', 'cs.AI', 'cs.CL']",2501.13597," Spectral clustering is a powerful technique for clustering high-dimensional
data, utilizing graph-based representations to detect complex, non-linear
structures and non-convex clusters. The construction of a similarity graph is
essential for ensuring accurate and effective clustering, making graph
structure learning (GSL) central for enhancing spectral clustering performance
in response to the growing demand for scalable solutions. Despite advancements
in GSL, there is a lack of comprehensive surveys specifically addressing its
role within spectral clustering. To bridge this gap, this survey presents a
comprehensive review of spectral clustering methods, emphasizing on the
critical role of GSL. We explore various graph construction techniques,
including pairwise, anchor, and hypergraph-based methods, in both fixed and
adaptive settings. Additionally, we categorize spectral clustering approaches
into single-view and multi-view frameworks, examining their applications within
one-step and two-step clustering processes. We also discuss multi-view
information fusion techniques and their impact on clustering data. By
addressing current challenges and proposing future research directions, this
survey provides valuable insights for advancing spectral clustering
methodologies and highlights the pivotal role of GSL in tackling large-scale
and high-dimensional data clustering tasks.",['cs.LG'],False,,,,"Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient
  Zeroth-order LLM Fine-tuning","A Comprehensive Survey on Spectral Clustering with Graph Structure
  Learning"
neg-d2-958,2025-03-05,,2503.04049," As the demand for computational power increases, high-bandwidth memory (HBM)
has become a critical technology for next-generation computing systems.
However, the widespread adoption of HBM presents significant thermal management
challenges, particularly in multilayer through-silicon-via (TSV) stacked
structures under varying thermal conditions, where accurate prediction of
junction temperature and hotspot position is essential during the early design.
This work develops a data-driven neural network model for the fast prediction
of junction temperature and hotspot position in 3D HBM chiplets. The model,
trained with a data set of $13,494$ different combinations of thermal condition
parameters, sampled from a vast parameter space characterized by
high-dimensional combination (up to $3^{27}$), can accurately and quickly infer
the junction temperature and hotspot position for any thermal conditions in the
parameter space. Moreover, it shows good generalizability for other thermal
conditions not considered in the parameter space. The data set is constructed
using accurate finite element solvers. This method not only minimizes the
reliance on costly experimental tests and extensive computational resources for
finite element analysis but also accelerates the design and optimization of
complex HBM systems, making it a valuable tool for improving thermal management
and performance in high-performance computing applications.",['cs.LG'],2502.01526," It was recently found that connection coefficients of the Heun equation can
be derived in closed form using crossing symmetry in two-dimensional Liouville
theory via the Nekrasov-Shatashvili functions. In this work, we systematize
this approach to second-order linear ODEs of Fuchsian type, which arise in the
description of N = 2, four-dimensional quiver gauge theories. After presenting
the general procedure, we focus on the specific case of Fuchsian equations with
five regular singularities and present some applications to black hole
perturbation theory. First, we consider a massive scalar perturbation of the
Schwarzschild black hole in AdS7. Next, we analyze vector type perturbations of
the Reissner-Nordstr\""om-AdS5 black hole. We also discuss the implications of
our results in the context of the AdS/CFT correspondence and present explicit
results in the large spin limit, where we make connection with the light-cone
bootstrap. Furthermore, using the spectral network technology, we identify the
region of the moduli space in Seiberg-Witten theory that is relevant for the
study of black hole quasinormal modes. Our results suggest that, in some cases,
this region corresponds to the strong-coupling regime, highlighting the
potential applicability of the conformal GMN TBA framework to address scenarios
where the gravitational dictionary implies that the instanton counting
parameters are not parametrically small.","['hep-th', 'gr-qc', 'math-ph', 'math.MP']",False,,,,"Neural Network Surrogate Model for Junction Temperature and Hotspot
  Position in $3$D Multi-Layer High Bandwidth Memory (HBM) Chiplets under
  Varying Thermal Conditions","On quivers, spectral networks and black holes"
neg-d2-959,2025-02-23,,2502.17524," Bearings play an integral role in ensuring the reliability and efficiency of
rotating machinery - reducing friction and handling critical loads. Bearing
failures that constitute up to 90% of mechanical faults highlight the
imperative need for reliable condition monitoring and fault detection. This
study proposes a multimodal bearing fault classification approach that relies
on vibration and motor phase current signals within a one-dimensional
convolutional neural network (1D CNN) framework. The method fuses features from
multiple signals to enhance the accuracy of fault detection. Under the baseline
condition (1,500 rpm, 0.7 Nm load torque, and 1,000 N radial force), the model
reaches an accuracy of 96% with addition of L2 regularization. This represents
a notable improvement of 2% compared to the non-regularized model. In addition,
the model demonstrates robust performance across three distinct operating
conditions by employing transfer learning (TL) strategies. Among the tested TL
variants, the approach that preserves parameters up to the first max-pool layer
and then adjusts subsequent layers achieves the highest performance. While this
approach attains excellent accuracy across varied conditions, it requires more
computational time due to its greater number of trainable parameters. To
address resource constraints, less computationally intensive models offer
feasible trade-offs, albeit at a slight accuracy cost. Overall, this multimodal
1D CNN framework with late fusion and TL strategies lays a foundation for more
accurate, adaptable, and efficient bearing fault classification in industrial
environments with variable operating conditions.","['eess.SP', 'cs.AI', 'cs.LG']",2502.1814," Trace conjunction integrals are introduced and studied. They appear in trace
conjunction inequalities which unify the Hardy inequality on a halfspace and
the classical Gagliardo trace inequality. At the endpoint they satisfy a
Bourgain-Brezis-Mironescu formula for smooth maps, which raises some new open
problems.",['math.FA'],False,,,,"Multimodal Bearing Fault Classification Under Variable Conditions: A 1D
  CNN with Transfer Learning",Trace conjunction inequalities
neg-d2-960,2025-02-12,,2502.08435," Recent observations of $\delta$ Scuti stars find evidence of nonlinear
three-mode coupling in their oscillation spectra. There are two types of
three-mode coupling likely to be important in $\delta$ Scuti stars: (i) direct
coupling, in which two linearly unstable modes (driven by the kappa-mechanism)
excite a linearly stable mode, and (ii) parametric coupling, in which one
linearly unstable mode excites two linearly stable modes. Breger & Montgomery
(2014) find especially strong evidence of direct coupling in the $\delta$ Scuti
star KIC 8054146. However, direct coupling is inherently unstable and cannot be
the mechanism by which the modes saturate and achieve nonlinear equilibrium. By
integrating the amplitude equations of small mode networks, we show that the
modes can achieve equilibrium if parametric coupling operates in tandem with
direct coupling. Using mode parameters calculated from a $\delta$ Scuti model,
we also find that parametric and direct coupling are likely to be
simultaneously active. Importantly, parametric coupling does not necessarily
disrupt the correlations found in KIC 8054146 between the amplitudes and phases
of the directly coupled modes. We conclude that $\delta$ Scuti stars are likely
impacted by both parametric and direct coupling and that accounting for both in
future large mode network calculations may help explain the complicated mode
dynamics observed in many $\delta$ Scuti stars.",['astro-ph.SR'],2502.08772," Protein flexibility, measured by the B-factor or Debye-Waller factor, is
essential for protein functions such as structural support, enzyme activity,
cellular communication, and molecular transport. Theoretical analysis and
prediction of protein flexibility are crucial for protein design, engineering,
and drug discovery. In this work, we introduce the persistent sheaf Laplacian
(PSL), an effective tool in topological data analysis, to model and analyze
protein flexibility. By representing the local topology and geometry of protein
atoms through the multiscale harmonic and non-harmonic spectra of PSLs, the
proposed model effectively captures protein flexibility and provides accurate,
robust predictions of protein B-factors. Our PSL model demonstrates an increase
in accuracy of 32% compared to the classical Gaussian network model (GNM) in
predicting B-factors for a dataset of 364 proteins. Additionally, we construct
a blind machine learning prediction method utilizing global and local protein
features. Extensive computations and comparisons validate the effectiveness of
the proposed PSL model for B-factor predictions.","['q-bio.BM', 'q-bio.QM']",False,,,,Stability and Dynamics of Three-Mode Coupling in $\delta$ Scuti Stars,Persistent Sheaf Laplacian Analysis of Protein Flexibility
neg-d2-961,2025-03-16,,2503.12638," The emergence of 6G wireless networks demands solutions that seamlessly
integrate communication and sensing. This letter proposes a novel waveform
design for joint sensing and communication (JSAC) systems, combining
single-carrier interleaved frequency division multiplexing (SC-IFDM), a 5G
communication candidate signal, with frequency modulated continuous wave
(FMCW), widely used for sensing. The proposed waveform leverages the sparse
nature of FMCW within SC-IFDM to achieve orthogonal integration in three steps:
SC-IFDM symbols are allocated alongside the sparse FMCW, followed by the
SC-IFDM transform into the time domain, and a cyclic prefix (CP) is applied in
which phase shifts are introduced to the FMCW. Additionally, an enhanced
channel estimation method is incorporated to boost system performance.
Simulation results demonstrate the proposed waveform's ability to deliver
high-resolution sensing and superior communication performance, surpassing
traditional multicarrier designs.",['eess.SP'],2503.18276," OpenStreetMap (OSM) has gained popularity recently in autonomous navigation
due to its public accessibility, lower maintenance costs, and broader
geographical coverage. However, existing methods often struggle with noisy OSM
data and incomplete sensor observations, leading to inaccuracies in trajectory
planning. These challenges are particularly evident in complex driving
scenarios, such as at intersections or facing occlusions. To address these
challenges, we propose a robust and explainable two-stage framework to learn an
Orientation Field (OrField) for robot navigation by integrating LiDAR scans and
OSM routes. In the first stage, we introduce the novel representation, OrField,
which can provide orientations for each grid on the map, reasoning jointly from
noisy LiDAR scans and OSM routes. To generate a robust OrField, we train a deep
neural network by encoding a versatile initial OrField and output an optimized
OrField. Based on OrField, we propose two trajectory planners for OSM-guided
robot navigation, called Field-RRT* and Field-Bezier, respectively, in the
second stage by improving the Rapidly Exploring Random Tree (RRT) algorithm and
Bezier curve to estimate the trajectories. Thanks to the robustness of OrField
which captures both global and local information, Field-RRT* and Field-Bezier
can generate accurate and reliable trajectories even in challenging conditions.
We validate our approach through experiments on the SemanticKITTI dataset and
our own campus dataset. The results demonstrate the effectiveness of our
method, achieving superior performance in complex and noisy conditions. Our
code for network training and real-world deployment is available at
https://github.com/IMRL/OriField.",['cs.RO'],False,,,,Single-Carrier Waveform Design for Joint Sensing and Communication,Learning Orientation Field for OSM-Guided Autonomous Navigation
neg-d2-962,2025-03-04,,2503.02846," Large language models (LLMs) exhibit hallucinations (i.e., unfaithful or
nonsensical information) when serving as AI assistants in various domains.
Since hallucinations always come with truthful content in the LLM responses,
previous factuality alignment methods that conduct response-level preference
learning inevitably introduced noises during training. Therefore, this paper
proposes a fine-grained factuality alignment method based on Direct Preference
Optimization (DPO), called Mask-DPO. Incorporating sentence-level factuality as
mask signals, Mask-DPO only learns from factually correct sentences in the
preferred samples and prevents the penalty on factual contents in the not
preferred samples, which resolves the ambiguity in the preference learning.
Extensive experimental results demonstrate that Mask-DPO can significantly
improve the factuality of LLMs responses to questions from both in-domain and
out-of-domain datasets, although these questions and their corresponding topics
are unseen during training. Only trained on the ANAH train set, the score of
Llama3.1-8B-Instruct on the ANAH test set is improved from 49.19% to 77.53%,
even surpassing the score of Llama3.1-70B-Instruct (53.44%), while its
FactScore on the out-of-domain Biography dataset is also improved from 30.29%
to 39.39%. We further study the generalization property of Mask-DPO using
different training sample scaling strategies and find that scaling the number
of topics in the dataset is more effective than the number of questions. We
provide a hypothesis of what factual alignment is doing with LLMs, on the
implication of this phenomenon, and conduct proof-of-concept experiments to
verify it. We hope the method and the findings pave the way for future research
on scaling factuality alignment.",['cs.CL'],2502.06678," In this paper, we study a variant of best-arm identification involving
elements of risk sensitivity and communication constraints. Specifically, the
goal of the learner is to identify the arm with the highest quantile reward,
while the communication from an agent (who observes rewards) and the learner
(who chooses actions) is restricted to only one bit of feedback per arm pull.
We propose an algorithm that utilizes noisy binary search as a subroutine,
allowing the learner to estimate quantile rewards through 1-bit feedback. We
derive an instance-dependent upper bound on the sample complexity of our
algorithm and provide an algorithm-independent lower bound for specific
instances, with the two matching to within logarithmic factors under mild
conditions, or even to within constant factors in certain low error probability
scaling regimes. The lower bound is applicable even in the absence of
communication constraints, and thus we conclude that restricting to 1-bit
feedback has a minimal impact on the scaling of the sample complexity.","['stat.ML', 'cs.IT', 'cs.LG', 'math.IT']",False,,,,Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs,Quantile Multi-Armed Bandits with 1-bit Feedback
neg-d2-963,2025-03-15,,2503.12059," This work explores the geometrical/algebraic framework of Lie algebroids,
with a specific focus on the decoupling and coupling phenomena within the
bicocycle double cross product realization. The bicocycle double cross product
theory serves as the most general method for (de)coupling an algebroid into the
direct sum of two vector bundles in the presence of mutual
\textit{representations}, along with two twisted cocycle terms. Consequently,
it encompasses unified product, double cross product (matched pairs),
semi-direct product, and cocycle extension frameworks as particular instances.
In addition to algebraic constructions, the research extends to both reversible
and irreversible Lagrangian and Hamiltonian dynamics on (de)coupled Lie
algebroids, as well as Euler-Poincar\'{e}-(Herglotz) and Lie-Poisson-(Herglotz)
dynamics on (de)coupled Lie algebras, providing insights into potential
physical applications.","['math.DG', 'math-ph', 'math.MP']",2503.0993," This paper presents a nonlinear control strategy for an aerial cooperative
payload transportation system consisting of two quadrotor UAVs rigidly
connected to a payload. The system includes human physical interaction
facilitated by an admittance control. The proposed control framework integrates
an adaptive Backstepping controller for the position subsystem and a Fast
Nonsingular Terminal Sliding Mode Control (FNTSMC) for the attitude subsystem
to ensure asymptotic stabilization. The admittance controller interprets the
interaction forces from the human operator, generating reference trajectories
for the position controller to ensure accurate tracking of the operator's
guidance. The system aims to assist humans in payload transportation, providing
both stability and responsiveness. The robustness and effectiveness of the
proposed control scheme in maintaining system stability and performance under
various conditions are presented.","['eess.SY', 'cs.SY']",False,,,,"On Product Lie Algebroids, and Collective Motion","Human Physical Interaction based on UAV Cooperative Payload
  Transportation System using Adaptive Backstepping and FNTSMC"
neg-d2-964,2025-03-19,,2503.1537," We reappraise the idea of colliding with robots, moving from a position that
tries to avoid or mitigate collisions to one that considers them an important
facet of human interaction. We report on a soma design workshop that explored
how our bodies could collide with telepresence robots, mobility aids, and a
quadruped robot. Based on our findings, we employed soma trajectories to
analyse collisions as extended experiences that negotiate key transitions of
consent, preparation, launch, contact, ripple, sting, untangle, debris and
reflect. We then employed these ideas to analyse two collision experiences, an
accidental collision between a person and a drone, and the deliberate design of
a robot to play with cats, revealing how real-world collisions involve the
complex and ongoing entanglement of soma trajectories. We discuss how viewing
collisions as entangled trajectories, or tangles, can be used analytically, as
a design approach, and as a lens to broach ethical complexity.","['cs.RO', 'cs.HC']",2503.09389," We study canonical-equilibrium properties of Random Field $O(n)$ Models
involving classical continuous vector spins of $n$ components with mean-field
interactions and subject to disordered fields acting on individual spins. To
this end, we employ two complementary approaches: the mean-field approximation,
valid for any disorder distribution, and the replica trick, applicable when the
disordered fields are sampled from a Gaussian distribution. On the basis of an
exact analysis, we demonstrate that when replica symmetry holds, both the
approaches yield identical expression for the free energy per spin of the
system. As consequences, we study the case of $n=2$ ($XY$ spins) and that of
$n=3$ (Heisenberg spins) for two representative choices of the disorder
distribution, namely, a Gaussian and a symmetric bimodal distribution. For both
$n=2$ and $n=3$, we demonstrate that while the magnetization exhibits a
continuous phase transition as a function of temperature for the Gaussian case,
the transition could be either continuous or first-order with an emergent
tricriticality when the disorder distribution is bimodal. We also discuss in
the context of our models the issue of self-averaging of extensive variables
near the critical point of a continuous phase transition.","['cond-mat.stat-mech', 'cond-mat.dis-nn']",False,,,,Tangles: Unpacking Extended Collision Experiences with Soma Trajectories,"Canonical equilibrium of mean-field $O(n)$~models in presence of random
  fields"
neg-d2-965,2025-03-02,,2503.01053," Each period, two players bargain over a unit of surplus. Each player chooses
between remaining flexible and committing to a take-it-or-leave-it offer at a
cost. If players' committed demands are incompatible, then the current-period
surplus is destroyed in the conflict. When both players are flexible, the
surplus is split according to the status quo, which is the division in the last
period where there was no conflict. We show that when players are patient and
the cost of commitment is small, there exist a class of symmetric Markov
Perfect equilibria that are asymptotically efficient and renegotiation proof,
in which players commit to fair demands in almost all periods.","['econ.TH', 'cs.GT']",2503.09425," In arXiv:1303.3724, the authors provide an axiomatic way of constructing new
polynomially bounded o-minimal structures. However, all of the structures
satisfying these axioms must also have smooth cell-decomposition. In this
paper, we generalize their approach by allowing weakly smooth germs into the
construction. In particular, we showed in arXiv:2501.17583 that the o-minimal
structure constructed in [O. Le Gal, J.-P. Rolin. ""An o-minimal structure which
does not admit $C^\infty$ cellular decomposition"" Ann. Inst. Fourier 59 (2009),
pp 543-562] satisfies the assumptions of our theorem.",['math.LO'],False,,,,"Commitment, Conflict, and Status Quo in Bargaining","Quasianalytic algebras with weakly smooth germs generate o-minimal
  structures"
neg-d2-966,2025-03-05,,2503.03326," The oceans cover the vast majority of the Earth. Therefore, their simulation
has many scientific, industrial and military interests, including computer
graphics domain. By fully exploiting the multi-threading power of GPU and CPU,
current state-of-the-art tools can achieve real-time ocean simulation, even if
it is sometimes needed to reduce the physical realism for large scenes.
Although most of the building blocks for implementing an ocean simulator are
described in the literature, a clear explanation of how they interconnect is
lacking. Hence, this paper proposes to bring all these components together,
detailing all their interactions, in a comprehensive and fully described
real-time framework that simulates the free ocean surface and the coupling
between solids and fluid. This article also presents several improvements to
enhance the physical realism of our model. The two main ones are: calculating
the real-time velocity of ocean fluids at any depth; computing the input of the
fluid to solid coupling algorithm.",['cs.GR'],2501.19226," In this paper, we explore a taxonomy of connectivity for space-like
structures. It is inspired by isolating posets of connected pieces of a space
and examining its embedding in the ambient space. The taxonomy includes in its
scope all standard notions of connectivity in point-set and point-free
contexts, such as connectivity in graphs and hypergraphs (as well as
k-connectivity in graphs), connectivity and path-connectivity in topology, and
connectivity of elements in a frame.","['math.GN', 'math.CT', 'math.RA']",False,,,,Arc Blanc: a real time ocean simulation framework,What is Connectivity?
neg-d2-967,2025-02-17,,2502.1171," Through experimental studies, however, we observed the instability of final
predicted quality scores, which change significantly over different viewpoint
settings. Inspired by the ""wooden barrel theory"", given the default
content-independent viewpoints of existing projection-related PCQA approaches,
this paper presents a novel content-aware viewpoint generation network (CAVGN)
to learn better viewpoints by taking the distribution of geometric and
attribute features of degraded point clouds into consideration. Firstly, the
proposed CAVGN extracts multi-scale geometric and texture features of the
entire input point cloud, respectively. Then, for each default
content-independent viewpoint, the extracted geometric and texture features are
refined to focus on its corresponding visible part of the input point cloud.
Finally, the refined geometric and texture features are concatenated to
generate an optimized viewpoint. To train the proposed CAVGN, we present a
self-supervised viewpoint ranking network (SSVRN) to select the viewpoint with
the worst quality projected image to construct a default-optimized viewpoint
dataset, which consists of thousands of paired default viewpoints and
corresponding optimized viewpoints. Experimental results show that the
projection-related PCQA methods can achieve higher performance using the
viewpoints generated by the proposed CAVGN.",['cs.CV'],2503.16639," Realistic crowd simulations are essential for immersive virtual environments,
relying on both individual behaviors (microscopic dynamics) and overall crowd
patterns (macroscopic characteristics). While recent data-driven methods like
deep reinforcement learning improve microscopic realism, they often overlook
critical macroscopic features such as crowd density and flow, which are
governed by spatio-temporal spawn dynamics, namely, when and where agents enter
a scene. Traditional methods, like random spawn rates, stochastic processes, or
fixed schedules, are not guaranteed to capture the underlying complexity or
lack diversity and realism. To address this issue, we propose a novel approach
called nTPP-GMM that models spatio-temporal spawn dynamics using Neural
Temporal Point Processes (nTPPs) that are coupled with a spawn-conditional
Gaussian Mixture Model (GMM) for agent spawn and goal positions. We evaluate
our approach by orchestrating crowd simulations of three diverse real-world
datasets with nTPP-GMM. Our experiments demonstrate the orchestration with
nTPP-GMM leads to realistic simulations that reflect real-world crowd scenarios
and allow crowd analysis.",['cs.LG'],False,,,,"The Worse The Better: Content-Aware Viewpoint Generation Network for
  Projection-related Point Cloud Quality Assessment","Whenever, Wherever: Towards Orchestrating Crowd Simulations with
  Spatio-Temporal Spawn Dynamics"
neg-d2-968,2025-02-01,,2502.00416," This paper presents GO-GAN, a novel Generative Adversarial Network (GAN)
architecture for geometry optimization (GO), specifically to generate
structures based on user-specified input parameters. The architecture for
GO-GAN proposed here combines a \texttt{Pix2Pix} GAN with a new input
mechanism, involving a dynamic batch gradient descent-based training loop that
leverages dataset symmetries. The model, implemented here using
\texttt{TensorFlow} and \texttt{Keras}, is trained using input images
representing scalar physical properties generated by a custom MatLab code.
After training, GO-GAN rapidly generates optimized geometries from input images
representing scalar inputs of the physical properties. Results demonstrate
GO-GAN's ability to produce acceptable designs with desirable variations. These
variations are followed by the influence of discriminators during training and
are of practical significance in ensuring adherence to specifications while
enabling creative exploration of the design space.",['cs.CE'],2501.14229," The Kitaev honeycomb model has received significant attention for its exactly
solvable quantum spin liquid ground states and fractionalized excitations. For
realizing the model, layered cobalt oxides have been considered a promising
platform. Yet, in contrast to the conventional wisdom about single-$\mathbf{q}$
zigzag magnetic order inferred from previous studies of the Na$_2$IrO$_3$ and
$\alpha$-RuCl$_3$ candidate materials, recent experiments on two of the
representative honeycomb cobalt oxides, hexagonal Na$_2$Co$_2$TeO$_6$ and
monoclinic Na$_3$Co$_2$SbO$_6$, have uncovered evidence for more complex
multi-$\mathbf{q}$ variants of the zigzag order. This review surveys on
experimental strategies to distinguish between single- and multi-$\mathbf{q}$
orders, along with the crystallographic symmetries of the cobalt oxides in
comparison to the previously studied systems. General formation mechanism of
multi-$\mathbf{q}$ order is also briefly discussed. The goal is to provide some
rationales for examining the relevance of multi-$\mathbf{q}$ order in the
honeycomb cobalt oxides, along with its implications on the microscopic model
of these intriguing quantum magnets.",['cond-mat.str-el'],False,,,,"GO-GAN: Geometry Optimization Generative Adversarial Network for
  Achieving Optimized Structures with Targeted Physical Properties","On the multi-$\mathbf{q}$ characteristics of magnetic ground states of
  honeycomb cobalt oxides"
neg-d2-969,2025-02-09,,2502.05848," We define special objects, Ulrich objects, on a derived category of polarized
smooth projective variety as a generalization of Ulrich bundles to the derived
category. These are defined by the cohomological conditions that are the same
form as a cohomological criterion determining Ulrichness for sheaves. This
paper gives a characterization of the Ulrich object similar to the one in
[ES03]. As an application, we have provided a new approach to the
Eisenbud-Schreyer question by using the notions of the generator of the derived
category. We also have given an example of Ulrich objects that are not sheaf by
the Yoneda extension.",['math.AG'],2503.13435," With the rapid development of 3D reconstruction technology, research in 4D
reconstruction is also advancing, existing 4D reconstruction methods can
generate high-quality 4D scenes. However, due to the challenges in acquiring
multi-view video data, the current 4D reconstruction benchmarks mainly display
actions performed in place, such as dancing, within limited scenarios. In
practical scenarios, many scenes involve wide-range spatial movements,
highlighting the limitations of existing 4D reconstruction datasets.
Additionally, existing 4D reconstruction methods rely on deformation fields to
estimate the dynamics of 3D objects, but deformation fields struggle with
wide-range spatial movements, which limits the ability to achieve high-quality
4D scene reconstruction with wide-range spatial movements. In this paper, we
focus on 4D scene reconstruction with significant object spatial movements and
propose a novel 4D reconstruction benchmark, WideRange4D. This benchmark
includes rich 4D scene data with large spatial variations, allowing for a more
comprehensive evaluation of the generation capabilities of 4D generation
methods. Furthermore, we introduce a new 4D reconstruction method, Progress4D,
which generates stable and high-quality 4D results across various complex 4D
scene reconstruction tasks. We conduct both quantitative and qualitative
comparison experiments on WideRange4D, showing that our Progress4D outperforms
existing state-of-the-art 4D reconstruction methods. Project:
https://github.com/Gen-Verse/WideRange4D",['cs.CV'],False,,,,A simple derived categorical generalization of Ulrich bundles,"WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range
  Movements and Scenes"
neg-d2-970,2025-02-12,,2502.0853," We introduce \emph{residually dominated groups} in pure henselian valued
fields of equicharacteristic zero as an analogue of stably dominated groups
introduced by Hrushovski and Rideau-Kikuchi. We show that when $G$ is a
residually dominated group, there is a finite-to-one group homomorphism from
its connected component into a connected stably dominated group, and we study
the functoriality and universality properties of this map. Moreover, we prove
that there is a group homomorphism into a definable group in the residue field
that witnesses the residual domination. In our proofs, we use the results of
Montenegro, Onshuus, and Simon on groups definable in $\mathrm{NTP}_2$-theories
that extend the theory of fields. Along the way, we also provide an algebraic
characterization of residually dominated types, generalizing the work by Ealy,
Haskell and Simon for stably dominated types in algebraically closed valued
fields and study the properties of residually dominated types.",['math.LO'],2502.2045," Superconductivity has recently been demonstrated in La$_3$Ni$_2$O$_7$ up to
91K under moderate pressure in bulk crystals, and up to 48K at ambient pressure
in thin films grown under compressive strain. Key questions remain open
regarding the crystal structure and low-energy electronic states that support
superconductivity in these compounds. Here we take advantage of the natural
polymorphism between bilayer (2222) or alternating monolayer-trilayer (1313)
stacking sequences that arises in bulk La$_3$Ni$_2$O$_7$ crystals to identify
universal features in this family of materials. Employing angle-resolved
photoemission spectroscopy (ARPES) we observe the fingerprint of a spin-density
wave (SDW) instability, strong and coherent enough to modify the electronic
structure. We demonstrate that this feature is a `translated' $\beta$ Fermi
surface associated with a scattering vector $Q_{t\beta}$ which matches the
$Q_{SDW}$ detected by neutron and x-ray scattering experiments. This
observation provides an important link between surface and bulk probes, and
demonstrates a universal connection between magnetism and fermiology in
La$_3$Ni$_2$O$_7$ as well as La$_4$Ni$_3$O$_{10}$. We simulate the spectral
weight distribution observed in our ARPES dichroism experiments and establish
that the low-energy electronic phenomenology is dominated by oxygen-centered
planar orbitals, which -- upon moving along the Fermi surface away from the
Ni-O-Ni bond directions -- evolve from the $d_{3x^2-r^2}$ and $d_{3y^2-r^2}$
symmetry characteristic of 3-spin polarons to the familiar $d_{x^2-y^2}$
Zhang-Rice singlets that support high-temperature superconductivity in
cuprates. Despite the multiorbital nature of the nickelates, our work
establishes an empirical correspondence between the low-energy electronic
structure of cuprates and nickelates, thus suggesting a common origin for their
unconventional superconductivity.",['cond-mat.supr-con'],False,,,,"Residually Dominated Groups in Henselian Valued Fields of
  Equicharacteristic Zero","Universal electronic structure of layered nickelates via oxygen-centered
  planar orbitals"
neg-d2-971,2025-03-03,,2503.0139," Crash consistency is essential for applications that must persist data.
Crash-consistency testing has been commonly applied to find crash-consistency
bugs in applications. The crash-state space grows exponentially as the number
of operations in the program increases, necessitating techniques for pruning
the search space. However, state-of-the-art crash-state space pruning is far
from ideal. Some techniques look for known buggy patterns or bound the
exploration for efficiency, but they sacrifice coverage and may miss bugs
lodged deep within applications. Other techniques eliminate redundancy in the
search space by skipping identical crash states, but they still fail to scale
to larger applications.
  In this work, we propose representative testing: a new crash-state space
reduction strategy that achieves high scalability and high coverage. Our key
observation is that the consistency of crash states is often correlated, even
if those crash states are not identical. We build Pathfinder, a
crash-consistency testing tool that implements an update behaviors-based
heuristic to approximate a small set of representative crash states.
  We evaluate Pathfinder on POSIX-based and MMIO-based applications, where it
finds 18 (7 new) bugs across 8 production-ready systems. Pathfinder scales more
effectively to large applications than prior works and finds 4x more bugs in
POSIX-based applications and 8x more bugs in MMIO-based applications compared
to state-of-the-art systems.","['cs.OS', 'cs.PL', 'cs.SE']",2503.14366," Variational quantum algorithms (VQAs) offer a promising approach to solving
computationally demanding problems by combining parameterized quantum circuits
with classical optimization. Estimating probabilistic outcomes on quantum
hardware requires repeated measurements (shots). However, in practice, the
limited shot budget introduces significant noise in the evaluation of the
objective function. Gradient estimation in VQAs often relies on the
finite-difference, which evaluates the noisy objective function at perturbed
circuit parameter values. The accuracy of this estimation is highly dependent
on the choice of step size for these perturbations. An inappropriate step size
can exacerbate the impact of noise, causing inaccurate gradient estimates and
hindering the classical optimization in VQAs. This paper proposes QuGStep, an
algorithm that addresses the challenge of determining the appropriate step size
for finite-difference gradient estimation under a shot budget. QuGStep is
grounded in a theorem that proves the optimal step size, which accounts for the
shot budget, minimizes the error bound in gradient estimation using finite
differences. Numerical experiments approximating the ground state energy of
several molecules demonstrate that QuGStep can identify the appropriate step
size for the given shot budget to obtain effective gradient estimation.
Notably, the step size identified by QuGStep achieved convergence to the ground
state energy with over 96% fewer shots compared to using a default step size.
These findings highlight the potential of QuGStep to improve the practical
deployment and scalability of quantum computing technologies.",['quant-ph'],False,,,,"Scalable and Accurate Application-Level Crash-Consistency Testing via
  Representative Testing","QuGStep: Refining Step Size Selection in Gradient Estimation for
  Variational Quantum Algorithms"
neg-d2-972,2025-02-28,,2502.21199," We analyze a subclass of Ising models in the context of credit risk, focusing
on Dandelion models when the correlations $\rho$ between the central node and
each non-central node are negative. We establish the possible range of values
for $\rho$ and derive an explicit formula linking the correlation between any
pair of non-central nodes to $\rho$. The paper concludes with a simulation
study.",['stat.AP'],2501.11724," This work introduces and investigates the function $J(G) =
\frac{\text{Nil}(G)}{L(G)}$, where $\text{Nil}(G)$ denotes the number of
nilpotent subgroups and $L(G)$ the total number of subgroups of a finite group
$G$. The function $J(G)$, defined over the interval $(0,1]$, serves as a tool
to analyze structural patterns in finite groups, particularly within
non-nilpotent families such as supersolvable and dihedral groups. Analytical
results demonstrate the product density of $J(G)$ values in $(0,1]$,
highlighting its distribution across products of dihedral groups. Additionally,
a probabilistic analysis was conducted, and based on extensive computational
simulations, it was conjectured that the sample mean of $J(G)$ values converges
in distribution to the standard normal distribution, in accordance with the
Central Limit Theorem, as the sample size increases. These findings expand the
understanding of multiplicative functions in group theory, offering novel
insights into the structural and probabilistic behavior of finite groups.","['math.GR', 'math.PR']",False,,,,Negative correlations in Ising models of credit risk,Proportion of Nilpotent Subgroups in Finite Groups and Their Properties
neg-d2-973,2025-02-06,,2502.03905," We prove dynamical coherence for partial hyperbolic symplectomorphism in
dimension 4 whose stable and unstable bundles are C^1.",['math.DS'],2501.17295," Machine Translation (MT) is undergoing a paradigm shift, with systems based
on fine-tuned large language models (LLM) becoming increasingly competitive
with traditional encoder-decoder models trained specifically for translation
tasks. However, LLM-based systems are at a higher risk of generating
hallucinations, which can severely undermine user's trust and safety. Most
prior research on hallucination mitigation focuses on traditional MT models,
with solutions that involve post-hoc mitigation - detecting hallucinated
translations and re-translating them. While effective, this approach introduces
additional complexity in deploying extra tools in production and also increases
latency. To address these limitations, we propose a method that intrinsically
learns to mitigate hallucinations during the model training phase.
Specifically, we introduce a data creation framework to generate hallucination
focused preference datasets. Fine-tuning LLMs on these preference datasets
reduces the hallucination rate by an average of 96% across five language pairs,
while preserving overall translation quality. In a zero-shot setting our
approach reduces hallucinations by 89% on an average across three unseen target
languages.","['cs.CL', 'cs.AI', 'cs.LG']",False,,,,Partially hyperbolic symplectomorphism with C^1 bundles,"Mitigating Hallucinated Translations in Large Language Models with
  Hallucination-focused Preference Optimization"
neg-d2-974,2025-03-15,,2503.12119," The photostriction effect, a light-induced mechanical deformation in
materials, originates from the intricate interplay between lattice structure
and electronic excitation. In photovoltaic semiconductors, this effect plays a
crucial role in shaping non-equilibrium structural responses, yet its
fundamental mechanism remains elusive. Here, we uncover lattice expansion and
structural reconfiguration in two-dimensional (2D) perovskites driven by
photoinduced excitation using first-principles calculations. Our findings
reveal that the photoinduced carriers lead to a substantial lattice expansion
by about 2%. The expanded lattice facilitates strain relaxation with the
amplitude of 20% by increasing interatomic distances and reducing internal
stresses, thereby enhancing structural stability. The lattice dynamics can be
systematically engineered through photodoping density, unveiling a new pathway
to modulate light-matter interactions in 2D perovskites. These insights not
only advance the understanding of optically driven structural dynamics but also
offer a guiding principle for optimizing next-generation high-efficiency
photovoltaic devices and optoelectronics.","['cond-mat.mtrl-sci', 'physics.comp-ph']",2503.06018," The evolution of the role of lattice vibrations in the formation of the
pseudogap state in strongly correlated electron systems has been investigated
concerning changes in the electron-phonon coupling parameters and the
concentration of doped charge carriers. We apply the polaronic version of the
generalized tight-binding method to analyze the band structure of a realistic
multiband two-dimensional model that incorporates the electron-lattice
contributions of both Holstein and Peierls types. It has been demonstrated that
the emergence of polaronic effects begins with the modulation of spectral
function intensity. However, within a specific region of the phase diagram, a
significant transformation of the electron band structure and pseudogap state
occurs. It results from coherent polaron excitations that create a partially
flat band near the Fermi level. This process leads to a change in the topology
of the Fermi surface and the emergence of corresponding features in the density
of states.","['cond-mat.str-el', 'cond-mat.supr-con']",False,,,,"Photostriction Facilitates Relaxation of Lattice Distortion in
  Two-Dimensional Perovskites","Evolution of the pseudogap band structure in a system of
  electron-correlated lattice polarons"
neg-d2-975,2025-02-12,,2502.08622," Drought is a frequent and costly natural disaster in California, with major
negative impacts on agricultural production and water resource availability,
particularly groundwater. This study investigated the performance of applying
different machine learning approaches to predicting the U.S. Drought Monitor
classification in California. Four approaches were used: a convolutional neural
network (CNN), random forest, XGBoost, and long short term memory (LSTM)
recurrent neural network, and compared to a baseline persistence model. We
evaluated the models' performance in predicting severe drought (USDM drought
category D2 or higher) using a macro F1 binary classification metric. The LSTM
model emerged as the top performer, followed by XGBoost, CNN, and random
forest. Further evaluation of our results at the county level suggested that
the LSTM model would perform best in counties with more consistent drought
patterns and where severe drought was more common, and the LSTM model would
perform worse where drought scores increased rapidly. Utilizing 30 weeks of
historical data, the LSTM model successfully forecasted drought scores for a
12-week period with a Mean Absolute Error (MAE) of 0.33, equivalent to less
than half a drought category on a scale of 0 to 5. Additionally, the LSTM
achieved a macro F1 score of 0.9, indicating high accuracy in binary
classification for severe drought conditions. Evaluation of different window
and future horizon sizes in weeks suggested that at least 24 weeks of data
would result in the best performance, with best performance for shorter horizon
sizes, particularly less than eight weeks.",['cs.LG'],2503.01039," Superradiance can cause the axion cloud around a rotating black hole to reach
extremely high densities, and the decay of these axions can produce a powerful
laser. The electric field of these lasers is strong enough that the Schwinger
effect may become significant, resulting in the production of an
electron-positron plasma. We explore the dynamics between axion lasers and this
electron-positron plasma. While there are several mechanisms by which the
inclusion of a plasma can impact the laser's behavior, the most significant of
these mechanisms is that the electron-positron plasma imparts an effective mass
on the photon. As the plasma frequency increases, axion decay becomes
energetically unfavorable, up to the point where the axion no longer decays
into photons, shutting off the laser. We find that the impact of the
electron-positron plasma on the dynamics of the system depend heavily on the
parameters, specifically the axion mass $m_\phi$ and the superradiant coupling
$\alpha$, and that we may divide parameter space into three regimes: the
unenhanced, enhanced, and unstable regimes. In the unenhanced and enhanced
regime, the system will eventually settle into an equilibrium state, emitting a
laser of constant luminosity while the number of axions remains constant. In
the unenhanced regime, this equilibrium state can be calculated while
neglecting the effects of Schwinger production; in the enhanced regime, the
equilibrium luminosity is slightly larger than what it would be without
Schwinger production. In the unstable regime, the electron-positron plasma
suppresses axion decay to the point where the system is never able to reach
equilibrium; instead, the axions continue to grow superradiantly. In all three
cases, the production of superradiant axions will eventually cause the black
hole to spin down to the point where superradiance ceases.","['hep-ph', 'astro-ph.HE', 'gr-qc', 'hep-th']",False,,,,Forecasting Drought Using Machine Learning in California,The Role of the Schwinger Effect in Superradiant Axion Lasers
neg-d2-976,2025-03-05,,2503.03582," The adoption of crowdsourced election monitoring as a complementary
alternative to traditional election monitoring is on the rise. Yet, its
reliance on digital response volunteers to manually process incoming election
reports poses a significant scaling bottleneck. In this paper, we address the
challenge of scaling crowdsourced election monitoring by advancing the task of
automated classification of crowdsourced election reports to multilingual and
cross-domain classification settings. We propose a two-step classification
approach of first identifying informative reports and then categorising them
into distinct information types. We conduct classification experiments using
multilingual transformer models such as XLM-RoBERTa and multilingual embeddings
such as SBERT, augmented with linguistically motivated features. Our approach
achieves F1-Scores of 77\% for informativeness detection and 75\% for
information type classification. We conduct cross-domain experiments, applying
models trained in a source electoral domain to a new target electoral domain in
zero-shot and few-shot classification settings. Our results show promising
potential for model transfer across electoral domains, with F1-Scores of 59\%
in zero-shot and 63\% in few-shot settings. However, our analysis also reveals
a performance bias in detecting informative English reports over Swahili,
likely due to imbalances in the training data, indicating a need for caution
when deploying classification models in real-world election scenarios.",['cs.CL'],2502.1972," We study the performance of the linear consensus algorithm on strongly
connected graphs using the linear quadratic (LQ) cost as a performance measure.
  In particular, we derive bounds on the LQ cost by leveraging effective
resistance. Our results extend previous analyses -- which were limited to
reversible cases -- to the nonreversible setting. To facilitate this
generalization, we introduce novel concepts, termed the back-and-forth path and
the pivot node, which serve as effective alternatives to traditional techniques
that require reversibility. Moreover, we apply our approach to geometric graphs
to estimate the LQ cost without the reversibility assumption. The proposed
approach provides a framework that can be adapted to other contexts where
reversibility is typically assumed.","['math.OC', 'cs.MA']",False,,,,"Scaling Crowdsourced Election Monitoring: Construction and Evaluation of
  Classification Models for Multilingual and Cross-Domain Classification
  Settings","Analysis of Linear Consensus Algorithm on Strongly Connected Graph Using
  Effective Resistance"
neg-d2-977,2025-03-08,,2503.0642," Safety-critical controllers of complex systems are hard to construct
manually. Automated approaches such as controller synthesis or learning provide
a tempting alternative but usually lack explainability. To this end, learning
decision trees (DTs) have been prevalently used towards an interpretable model
of the generated controllers. However, DTs do not exploit shared
decision-making, a key concept exploited in binary decision diagrams (BDDs) to
reduce their size and thus improve explainability. In this work, we introduce
predicate decision diagrams (PDDs) that extend BDDs with predicates and thus
unite the advantages of DTs and BDDs for controller representation. We
establish a synthesis pipeline for efficient construction of PDDs from DTs
representing controllers, exploiting reduction techniques for BDDs also for
PDDs.","['cs.AI', 'cs.SY', 'eess.SY']",2501.14229," The Kitaev honeycomb model has received significant attention for its exactly
solvable quantum spin liquid ground states and fractionalized excitations. For
realizing the model, layered cobalt oxides have been considered a promising
platform. Yet, in contrast to the conventional wisdom about single-$\mathbf{q}$
zigzag magnetic order inferred from previous studies of the Na$_2$IrO$_3$ and
$\alpha$-RuCl$_3$ candidate materials, recent experiments on two of the
representative honeycomb cobalt oxides, hexagonal Na$_2$Co$_2$TeO$_6$ and
monoclinic Na$_3$Co$_2$SbO$_6$, have uncovered evidence for more complex
multi-$\mathbf{q}$ variants of the zigzag order. This review surveys on
experimental strategies to distinguish between single- and multi-$\mathbf{q}$
orders, along with the crystallographic symmetries of the cobalt oxides in
comparison to the previously studied systems. General formation mechanism of
multi-$\mathbf{q}$ order is also briefly discussed. The goal is to provide some
rationales for examining the relevance of multi-$\mathbf{q}$ order in the
honeycomb cobalt oxides, along with its implications on the microscopic model
of these intriguing quantum magnets.",['cond-mat.str-el'],False,,,,Explaining Control Policies through Predicate Decision Diagrams,"On the multi-$\mathbf{q}$ characteristics of magnetic ground states of
  honeycomb cobalt oxides"
neg-d2-978,2025-01-29,,2501.17687," We study magneto-transport through topological insulator nanowires shaped in
the form of a constriction, as can be obtained by etching techniques. The
magnetic field is coaxial, potentially turning the nanowire into a
magneto-chiral junction. We show in a detailed analytical and numerical study
that two main transport regimes emerge, depending on the central narrow region
being short or long as compared to the magnetic length at the junction entrance
and exit. In both cases the central region hosts Dirac-particle-in-a-box states
due to magnetic confinement, whose conductance properties are strongly
influenced by Landau levels at the ends of the constriction. Notably, in the
low-energy regime only chiral states with a specific handedness can transport
charge across the junction. Based on these properties and general symmetry
considerations we argue that the shaped nanowire should exhibit strong
magneto-chiral non-reciprocal transport beyond linear response. We employ a
numerical tight-binding implementation of an effective 2D model on a
non-homogeneous grid, capable of simulating samples of realistic sizes, and
test its soundness against full simulations for scaled-down 3D topological
insulator wires.",['cond-mat.mes-hall'],2503.04313," Infinitesimals have seen ups and downs in their tumultuous history. In the
18th century, d'Alembert set the tone by describing infinitesimals as chimeras.
Some adversaries of infinitesimals, including Moigno and Connes, picked up on
the term. We highlight the work of Cauchy, No\""el, Poisson and Riemann. We also
chronicle reactions by Moigno, Lamarle and Cantor, and signal the start of a
revival with Peano.",['math.HO'],False,,,,"Topological insulator constrictions -- Dirac particles in a
  magneto-chiral box",Episodes from the history of infinitesimals
neg-d2-979,2025-03-16,,2503.12645," Optimization with matrix gradient orthogonalization has recently demonstrated
impressive results in the training of deep neural networks (Jordan et al.,
2024; Liu et al., 2025). In this paper, we provide a theoretical analysis of
this approach. In particular, we show that the orthogonalized gradient method
can be seen as a first-order trust-region optimization method, where the
trust-region is defined in terms of the matrix spectral norm. Motivated by this
observation, we provide the first theoretical analysis of the stochastic
non-Euclidean trust-region gradient method with momentum, which recovers the
Muon optimizer (Jordan et al., 2024) as a special case. In addition, we
establish the convergence of the normalized SGD with momentum (Cutkosky and
Mehta, 2020) in the constrained and composite setting, show that its iteration
complexity of finding an $\varepsilon$-accurate solution can be improved from
$\mathcal{O}(\varepsilon^{-3.5})$ to $\mathcal{O}(\varepsilon^{-3})$ under the
star-convexity assumption, and obtain similar results for the Muon algorithm.
Finally, our theoretical findings provide an explanation for the practical
superiority of Muon compared to the Orthogonal-SGDM algorithm of Tuddenham et
al. (2022).","['cs.LG', 'math.OC', 'stat.ML']",2501.09231," Quantum mechanical invariance principles dictate the most general operator
structure that can be present in the nucleon-nucleon (NN) interaction. Five
independent operators appear in the on-shell NN amplitude together with five
corresponding coefficient functions. The usual choice for these coefficient
functions is known as the NN Wolfenstein amplitudes. We analyze the
order-by-order convergence of each of the five NN Wolfenstein amplitudes
predicted by a semi-local coordinate space potential implementation of chiral
effective field theory ($\chi$EFT). We do this at laboratory kinetic energies
between 25 and 200 MeV for both neutron-proton and proton-proton scattering.
Our analysis uses the Gaussian-Process methods developed by the BUQEYE
collaboration to describe the contributions of each $\chi$EFT order, and so
yields truncation uncertainties for each Wolfenstein amplitude that are
correlated across scattering angles. We combine information on the size of
different orders in the EFT to infer the $\chi$EFT breakdown scale for each
amplitude, finding, on average, $\Lambda_b$ between 750 and 800 MeV. With this
choice of $\Lambda_b$, the EFT truncation uncertainties cover both higher-order
results and empirical Wolfenstein amplitudes well for all orders other than the
leading order.","['nucl-th', 'nucl-ex']",False,,,,"Understanding Gradient Orthogonalization for Deep Learning via
  Non-Euclidean Trust-Region Optimization","Order-by-order uncertainties of nucleon-nucleon Wolfenstein amplitudes
  in chiral effective field theory"
neg-d2-980,2025-02-25,,2502.18716," From biology and astronomy to quantum optics, there is a critical need for
high frame rate, high quantum efficiency imaging. In practice, most cameras
only satisfy one of these requirements. Here we introduce interlaced fast
kinetics imaging, a technique that allows burst video acquisition at frame
rates up to 3.33 Mfps using a commercial EMCCD camera with single-photon
sensitivity. This approach leverages EMCCD's intrinsic fast row transfer
dynamics by introducing a tilted lens array into the imaging path, creating a
spatially distributed grid of exposed pixels, each aligned to its own column of
the sensor. The remaining unexposed pixels serve as in-situ storage registers,
allowing subsequent frames to be captured after just one row shift operation.
Our interlaced fast kinetics camera maintains 50% contrast for square wave
intensity modulation frequencies up to 1.61 MHz. We provide benchmarks of the
video performance by capturing two dimensional videos of spatially evolving
patterns that repeat every 2$\mu$s, with spatial resolution of 11$\times$15
pixels. Our approach is compatible with commercial EMCCDs and opens a new route
to ultra-fast imaging at single-photon sensitivity with applications from fast
fluorescence imaging to photon correlation measurement.","['physics.atom-ph', 'physics.optics']",2503.17547," Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting
neural networks by extracting the concepts represented in their activations.
However, choosing the size of the SAE dictionary (i.e. number of learned
concepts) creates a tension: as dictionary size increases to capture more
relevant concepts, sparsity incentivizes features to be split or absorbed into
more specific features, leaving high-level features missing or warped. We
introduce Matryoshka SAEs, a novel variant that addresses these issues by
simultaneously training multiple nested dictionaries of increasing size,
forcing the smaller dictionaries to independently reconstruct the inputs
without using the larger dictionaries. This organizes features hierarchically -
the smaller dictionaries learn general concepts, while the larger dictionaries
learn more specific concepts, without incentive to absorb the high-level
features. We train Matryoshka SAEs on Gemma-2-2B and TinyStories and find
superior performance on sparse probing and targeted concept erasure tasks, more
disentangled concept representations, and reduced feature absorption. While
there is a minor tradeoff with reconstruction performance, we believe
Matryoshka SAEs are a superior alternative for practical tasks, as they enable
training arbitrarily large SAEs while retaining interpretable features at
different levels of abstraction.","['cs.LG', 'cs.AI']",False,,,,A Mega-FPS low light camera,Learning Multi-Level Features with Matryoshka Sparse Autoencoders
neg-d2-981,2025-01-29,,2501.1747," We explore how to detect the large quantum fluctuations in the throat of a
near-extremal black hole, where the dynamics are governed by the Schwarzian
theory. To this end, we scatter a low-frequency wave of a massless, minimal
scalar off the black hole and calculate the absorption cross-section. In the
semiclassical regime, where the Schwarzian is weakly coupled, we recover the
universal result that the cross-section equals the horizon area. However, in
the strongly coupled regime, where quantum fluctuations dominate, we find that
the absorption cross-section exceeds the semiclassical prediction. This result
may seem counterintuitive, given that the density of black hole states is
suppressed in this regime. Nevertheless, two effects outweigh this suppression.
First, quantum fluctuations enhance absorption transitions between individual
states, with the effect becoming stronger closer to the ground state. Second,
these fluctuations significantly reduce stimulated emission. We conclude that a
measurement showing an enhanced absorption cross-section serves as a clear
signature of the large quantum fluctuations in the geometry.","['hep-th', 'gr-qc']",2503.1537," We reappraise the idea of colliding with robots, moving from a position that
tries to avoid or mitigate collisions to one that considers them an important
facet of human interaction. We report on a soma design workshop that explored
how our bodies could collide with telepresence robots, mobility aids, and a
quadruped robot. Based on our findings, we employed soma trajectories to
analyse collisions as extended experiences that negotiate key transitions of
consent, preparation, launch, contact, ripple, sting, untangle, debris and
reflect. We then employed these ideas to analyse two collision experiences, an
accidental collision between a person and a drone, and the deliberate design of
a robot to play with cats, revealing how real-world collisions involve the
complex and ongoing entanglement of soma trajectories. We discuss how viewing
collisions as entangled trajectories, or tangles, can be used analytically, as
a design approach, and as a lens to broach ethical complexity.","['cs.RO', 'cs.HC']",False,,,,Quantum Cross-section of Near-extremal Black Holes,Tangles: Unpacking Extended Collision Experiences with Soma Trajectories
neg-d2-982,2025-01-08,,2501.04782," Efficient neural representations for dynamic video scenes are critical for
applications ranging from video compression to interactive simulations. Yet,
existing methods often face challenges related to high memory usage, lengthy
training times, and temporal consistency. To address these issues, we introduce
a novel neural video representation that combines 3D Gaussian splatting with
continuous camera motion modeling. By leveraging Neural ODEs, our approach
learns smooth camera trajectories while maintaining an explicit 3D scene
representation through Gaussians. Additionally, we introduce a spatiotemporal
hierarchical learning strategy, progressively refining spatial and temporal
features to enhance reconstruction quality and accelerate convergence. This
memory-efficient approach achieves high-quality rendering at impressive speeds.
Experimental results show that our hierarchical learning, combined with robust
camera motion modeling, captures complex dynamic scenes with strong temporal
consistency, achieving state-of-the-art performance across diverse video
datasets in both high- and low-motion scenarios.",['cs.CV'],2501.13676," Text classifiers suffer from small perturbations, that if chosen
adversarially, can dramatically change the output of the model. Verification
methods can provide robustness certificates against such adversarial
perturbations, by computing a sound lower bound on the robust accuracy.
Nevertheless, existing verification methods incur in prohibitive costs and
cannot practically handle Levenshtein distance constraints. We propose the
first method for computing the Lipschitz constant of convolutional classifiers
with respect to the Levenshtein distance. We use these Lipschitz constant
estimates for training 1-Lipschitz classifiers. This enables computing the
certified radius of a classifier in a single forward pass. Our method, LipsLev,
is able to obtain $38.80$% and $13.93$% verified accuracy at distance $1$ and
$2$ respectively in the AG-News dataset, while being $4$ orders of magnitude
faster than existing approaches. We believe our work can open the door to more
efficient verification in the text domain.","['cs.LG', 'cs.AI', 'cs.CL']",False,,,,"GaussianVideo: Efficient Video Representation via Hierarchical Gaussian
  Splatting",Certified Robustness Under Bounded Levenshtein Distance
neg-d2-983,2025-02-19,,2502.13819," Let $A$ be an $n\times n$ random matrix with independent, identically
distributed mean 0, variance 1 subgaussian entries. We prove that $$
\mathbb{P}(A\text{ has distinct singular values})\geq 1-e^{-cn} $$ for some
$c>0$, confirming a conjecture of Vu. This result is then generalized to
singular values of rectangular random matrices with i.i.d. entries.
  We also prove that for two fixed real numbers $\lambda_1,\lambda_2$ with a
sufficient lower bound on $|\lambda_1-\lambda_2|$, we have a joint singular
value small ball estimate for any $\epsilon>0$ $$
\mathbb{P}(\sigma_{min}(A-\lambda_1I_n)\leq\epsilon
n^{-1/2},\sigma_{min}(A-\lambda_2I_n)\leq\epsilon n^{-1/2})\leq
C\epsilon^2+e^{-cn}, $$ where $\sigma_{min}(A)$ is the minimal singular value
of a square matrix $A$ and $I_n$ is the identity matrix. For much smaller
$|\lambda_1-\lambda_2|$ we derive a similar estimate with $C$ replaced by
$C\sqrt{n}/|\lambda_1-\lambda_2|$. This generalizes the one-point estimate of
Rudelson and Vershynin, which proves $\mathbb{P}(\sigma_{min}(A)\leq \epsilon
n^{-1/2})\leq C\epsilon+e^{-cn}$. Analogous two-point bounds are proven when
$A$ has i.i.d. real and complex parts, with $\epsilon^4$ in place of
$\epsilon^2$ on the right hand side of the estimate and for any complex numbers
$\lambda_1,\lambda_2$. These two point estimates can be used to derive strong
anticoncentration bounds for an arbitrary linear combination of two eigenvalues
of $A$.",['math.PR'],2501.1747," We explore how to detect the large quantum fluctuations in the throat of a
near-extremal black hole, where the dynamics are governed by the Schwarzian
theory. To this end, we scatter a low-frequency wave of a massless, minimal
scalar off the black hole and calculate the absorption cross-section. In the
semiclassical regime, where the Schwarzian is weakly coupled, we recover the
universal result that the cross-section equals the horizon area. However, in
the strongly coupled regime, where quantum fluctuations dominate, we find that
the absorption cross-section exceeds the semiclassical prediction. This result
may seem counterintuitive, given that the density of black hole states is
suppressed in this regime. Nevertheless, two effects outweigh this suppression.
First, quantum fluctuations enhance absorption transitions between individual
states, with the effect becoming stronger closer to the ground state. Second,
these fluctuations significantly reduce stimulated emission. We conclude that a
measurement showing an enhanced absorption cross-section serves as a clear
signature of the large quantum fluctuations in the geometry.","['hep-th', 'gr-qc']",False,,,,"Simplicity of singular value spectrum of random matrices and two-point
  quantitative invertibility",Quantum Cross-section of Near-extremal Black Holes
neg-d2-984,2025-03-07,,2503.05928," We generalize the enhanced power graph by replacing elements with conjugacy
classes. The main result of this paper is to determine when this graph is
triangle-free.",['math.GR'],2502.12769," In the age of misinformation, hallucination -- the tendency of Large Language
Models (LLMs) to generate non-factual or unfaithful responses -- represents the
main risk for their global utility. Despite LLMs becoming increasingly
multilingual, the vast majority of research on detecting and quantifying LLM
hallucination are (a) English-centric and (b) focus on machine translation (MT)
and summarization, tasks that are less common ``in the wild'' than open
information seeking. In contrast, we aim to quantify the extent of LLM
hallucination across languages in knowledge-intensive long-form question
answering. To this end, we train a multilingual hallucination detection model
and conduct a large-scale study across 30 languages and 6 open-source LLM
families. We start from an English hallucination detection dataset and rely on
MT to generate (noisy) training data in other languages. We also manually
annotate gold data for five high-resource languages; we then demonstrate, for
these languages, that the estimates of hallucination rates are similar between
silver (LLM-generated) and gold test sets, validating the use of silver data
for estimating hallucination rates for other languages. For the final rates
estimation, we build a knowledge-intensive QA dataset for 30 languages with
LLM-generated prompts and Wikipedia articles as references. We find that, while
LLMs generate longer responses with more hallucinated tokens for
higher-resource languages, there is no correlation between length-normalized
hallucination rates of languages and their digital representation. Further, we
find that smaller LLMs exhibit larger hallucination rates than larger models.","['cs.CL', 'cs.AI']",False,,,,Triangle-free cyclic conjugacy class graph of a finite group,"How Much Do LLMs Hallucinate across Languages? On Multilingual
  Estimation of LLM Hallucination in the Wild"
neg-d2-985,2025-01-17,,2501.10547," We present HyperCam, an energy-efficient image classification pipeline that
enables computer vision tasks onboard low-power IoT camera systems. HyperCam
leverages hyperdimensional computing to perform training and inference
efficiently on low-power microcontrollers. We implement a low-power wireless
camera platform using off-the-shelf hardware and demonstrate that HyperCam can
achieve an accuracy of 93.60%, 84.06%, 92.98%, and 72.79% for MNIST,
Fashion-MNIST, Face Detection, and Face Identification tasks, respectively,
while significantly outperforming other classifiers in resource efficiency.
Specifically, it delivers inference latency of 0.08-0.27s while using
42.91-63.00KB flash memory and 22.25KB RAM at peak. Among other machine
learning classifiers such as SVM, xgBoost, MicroNets, MobileNetV3, and
MCUNetV3, HyperCam is the only classifier that achieves competitive accuracy
while maintaining competitive memory footprint and inference latency that meets
the resource requirements of low-power camera systems.","['cs.CV', 'cs.LG', 'cs.NE', 'eess.IV']",2503.18399," We introduce the Turbulent Transport in Tokamaks via Stochastic Trajectories
(T3ST) code, designed to address the problem of turbulent transport using a
statistical approach complementary to gyrokinetics. The code employs
test-particle methods to track the dynamics of charged particles in
axisymmetric magnetic equilibria, accounting for both turbulence and Coulomb
collisions. The turbulence is decoupled from plasma dynamics and represented
through a statistical ensemble of synthetic random fields with specified
spectral properties. This approach enables T3ST to compute transport
coefficients as Lagrangian correlations - orders of magnitude faster than
gyrokinetic codes.",['physics.plasm-ph'],False,,,,HyperCam: Low-Power Onboard Computer Vision for IoT Cameras,T3ST code: Turbulent Transport in Tokamaks via Stochastic Trajectories
neg-d2-986,2025-02-07,,2502.05409," This paper proposes a vision-in-the-loop simulation environment for deep
monocular pose estimation of a UAV operating in an ocean environment. Recently,
a deep neural network with a transformer architecture has been successfully
trained to estimate the pose of a UAV relative to the flight deck of a research
vessel, overcoming several limitations of GPS-based approaches. However,
validating the deep pose estimation scheme in an actual ocean environment poses
significant challenges due to the limited availability of research vessels and
the associated operational costs. To address these issues, we present a
photo-realistic 3D virtual environment leveraging recent advancements in
Gaussian splatting, a novel technique that represents 3D scenes by modeling
image pixels as Gaussian distributions in 3D space, creating a lightweight and
high-quality visual model from multiple viewpoints. This approach enables the
creation of a virtual environment integrating multiple real-world images
collected in situ. The resulting simulation enables the indoor testing of
flight maneuvers while verifying all aspects of flight software, hardware, and
the deep monocular pose estimation scheme. This approach provides a
cost-effective solution for testing and validating the autonomous flight of
shipboard UAVs, specifically focusing on vision-based control and estimation
algorithms.","['cs.CV', 'cs.AI', 'cs.LG', 'cs.RO', 'cs.SY', 'eess.SY']",2501.02817," Given a pair of time series, we study how the periodicity of one influences
the periodicity of the other. There are several known methods to measure the
similarity between a pair of time series, such as cross-correlation, coherence,
cross-recurrence, and dynamic time warping. But we have yet to find any
measures with theoretical stability results.
  Persistence homology has been utilized to construct a scoring function with
theoretical guarantees of stability that quantifies the periodicity of a single
univariate time series f1, denoted score(f1). Building on this concept, we
propose a conditional periodicity score that quantifies the periodicity of one
univariate time series f1 given another f2, denoted score(f1|f2), and derive
theoretical stability results for the same. With the use of dimension reduction
in mind, we prove a new stability result for score(f1|f2) under principal
component analysis (PCA) when we use the projections of the time series
embeddings onto their respective first K principal components. We show that the
change in our score is bounded by a function of the eigenvalues corresponding
to the remaining (unused) N-K principal components and hence is small when the
first K principal components capture most of the variation in the time series
embeddings. Finally we derive a lower bound on the minimum embedding dimension
to use in our pipeline which guarantees that any two such embeddings give
scores that are within a given epsilon of each other.
  We present a procedure for computing conditional periodicity scores and
implement it on several pairs of synthetic signals. We experimentally compare
our similarity measure to the most-similar statistical measure of
cross-recurrence, and show the increased accuracy and stability of our score
when predicting and measuring whether or not the periodicities of two time
series are similar.","['math.AT', 'math.ST', 'stat.ML', 'stat.TH']",False,,,,"Vision-in-the-loop Simulation for Deep Monocular Pose Estimation of UAV
  in Ocean Environment","A Stable Measure for Conditional Periodicity of Time Series using
  Persistent Homology"
neg-d2-987,2025-01-03,,2501.01753," Detecting the transition from laminar to turbulent flow in particulate pipe
systems remains a complex issue in fluid dynamics, often requiring
sophisticated and costly experimental apparatus. This research presents an
innovative streak visualization method designed to offer a simple and robust
approach to identify transitional turbulent patterns in particulate pipe flows
with neutrally buoyant particles. The technique employs a laser arrangement and
a low-cost camera setup to capture particle-generated streaks within the fluid,
enabling real-time observation of flow patterns. Validation of the proposed
method was conducted through comparison with established techniques like
Particle Image Velocimetry (PIV) and pressure drop measurements, confirming its
accuracy and reliability. Experiments demonstrate the streak visualization
method's capacity to differentiate between laminar, transitional, and turbulent
flow regimes by analyzing the standard deviation of streak angles. The method
is especially efficient at low particle concentration, ie precisely where other
more established methods become less effective. Furthermore, this technique
enables us to identify a critical Reynolds number using Kullback-Leibler
divergence built on the statistical distribution of streak angles, which is
consistent with previous studies.
  Because it is effective at low concentrations and robust, this streak
visualization technique opens new perspectives for the characterization of
particulate pipe flows not only in the confines of the laboratory but also in
less controlled industrial multi-phase flows where determining the laminar or
turbulent nature of the flow is a prerequisite for flowmeter calibration.",['physics.flu-dyn'],2503.14963," Multimodal contrastive learning (MCL) advances in aligning different
modalities and generating multimodal representations in a joint space. By
leveraging contrastive learning across diverse modalities, large-scale
multimodal data enhances representational quality. However, a critical yet
often overlooked challenge remains: multimodal data is rarely collected in a
single process, and training from scratch is computationally expensive.
Instead, emergent multimodal data can be used to optimize existing models
gradually, \textit{i.e.}, models are trained on a sequence of modality pair
data. We define this problem as Continual Multimodal Contrastive Learning
(CMCL), an underexplored yet crucial research direction at the intersection of
multimodal and continual learning. In this paper, we formulate CMCL through two
specialized principles of stability and plasticity. We theoretically derive a
novel optimization-based method, which projects updated gradients from dual
sides onto subspaces where any gradient is prevented from interfering with the
previously learned knowledge. Two upper bounds provide theoretical insights on
both stability and plasticity in our solution. Beyond our theoretical
contributions, we conduct experiments on multiple datasets by comparing our
method against advanced continual learning baselines. The empirical results
further support our claims and demonstrate the efficacy of our method. The code
will be publicly available.",['cs.LG'],False,,,,"Detecting Turbulent Patterns in Particulate Pipe Flow by Streak Angle
  Visualization",Continual Multimodal Contrastive Learning
neg-d2-988,2025-03-07,,2503.05656," This article proposes a roadmap to address the current challenges in
small-scale testbeds for Connected and Automated Vehicles (CAVs) and robot
swarms. The roadmap is a joint effort of participants in the workshop ""1st
Workshop on Small-Scale Testbeds for Connected and Automated Vehicles and Robot
Swarms,"" held on June 2 at the IEEE Intelligent Vehicles Symposium (IV) 2024 in
Jeju, South Korea. The roadmap contains three parts: 1) enhancing accessibility
and diversity, especially for underrepresented communities, 2) sharing best
practices for the development and maintenance of testbeds, and 3) connecting
testbeds through an abstraction layer to support collaboration. The workshop
features eight invited speakers, four contributed papers [1]-[4], and a
presentation of a survey paper on testbeds [5]. The survey paper provides an
online comparative table of more than 25 testbeds, available at
https://bassamlab.github.io/testbeds-survey. The workshop's own website is
available at https://cpm-remote.lrt.unibw-muenchen.de/iv24-workshop.","['cs.RO', 'cs.MA']",2502.1016," The multi-stage method of laser wakefield acceleration (LWFA) presents a
promising approach for developing stable, full-optical, high-energy electron
accelerators. By segmenting the acceleration process into several booster
stages, each powered by independent laser drivers, this technique effectively
mitigates challenges such as electron dephasing, pump depletion, and laser
diffraction. A critical aspect of multi-stage LWFA is the nonlinear interaction
between the injected electron beam and the laser-driven wakefields in the
booster stage. This study investigates the injection and acceleration of
external electron beams within wakefields in the booster stage using
multi-dimensional Particle-In-Cell (PIC) simulations. We provide both
qualitative and quantitative descriptions of the observed physical processes.
Key parameters influencing charge coupling process and the resultant beam
quality have been identified. Furthermore, we have examined how off-axis
injection relative to the driver laser influences the acceleration process and
beam quality. Our findings provide valuable insights for advancing and
optimizing multi-stage plasma-based accelerators.","['physics.plasm-ph', 'physics.acc-ph', 'physics.comp-ph']",False,,,,"Small-Scale Testbeds for Connected and Automated Vehicles and Robot
  Swarms: Challenges and a Roadmap","Coupling and Acceleration of Externally Injected Electron Beams in
  Laser-Driven Plasma Wakefields"
neg-d2-989,2025-01-03,,2501.01654," A fundamental alcove $\mathcal{A}$ is a tile in a paving of a vector space
$V$ by an affine reflection group $W_{\mathrm{aff}}$. Its geometry encodes
essential features of $W_{\mathrm{aff}}$, such as its affine Dynkin diagram
$\widetilde{D}$ and fundamental group $\Omega$. In this article we investigate
its full isometry group $\mathrm{Aut}(\mathcal{A})$. It is well known that the
isometry group of a regular polyhedron is generated by hyperplane reflections
on its faces. Being a simplex, an alcove $\mathcal{A}$ is the simplest of
polyhedra, nevertheless it is seldom a regular one. In our first main result we
show that $\mathrm{Aut}(\mathcal{A})$ is isomorphic to
$\mathrm{Aut}(\widetilde{D})$. Building on this connection, we establish that
$\mathrm{Aut}(\mathcal{A})$ is an abstract Coxeter group, with generators given
by affine isometric involutions of the ambient space. Although these
involutions are seldom reflections, our second main result leverages them to
construct, by slicing the Komrakov--Premet fundamental polytope $\mathcal{K}$
for the action of $\Omega$, a family of fundamental polytopes for the action of
$\mathrm{Aut}(\mathcal{A})$ on $\mathcal{A}$, whose vertices are contained in
the vertices of $\mathcal{K}$ and whose faces are parametrized by the so-called
balanced minuscule roots, which we introduce here. In an appendix, we discuss
some related negative results on stratified centralizers and equivariant
triangulations.","['math.CO', 'math.GR']",2501.14082," Communication between multiple language model (LM) agents has been shown to
scale up the reasoning ability of LMs. While natural language has been the
dominant medium for inter-LM communication, it is not obvious this should be
the standard: not only does natural language communication incur high inference
costs that scale quickly with the number of both agents and messages, but also
the decoding process abstracts away too much rich information that could be
otherwise accessed from the internal activations. In this work, we propose a
simple technique whereby LMs communicate via activations; concretely, we pause
an LM $\textit{B}$'s computation at an intermediate layer, combine its current
activation with another LM $\textit{A}$'s intermediate activation via some
function $\textit{f}$, then pass $\textit{f}$'s output into the next layer of
$\textit{B}$ and continue the forward pass till decoding is complete. This
approach scales up LMs on new tasks with zero additional parameters and data,
and saves a substantial amount of compute over natural language communication.
We test our method with various functional forms $\textit{f}$ on two
experimental setups--multi-player coordination games and reasoning
benchmarks--and find that it achieves up to $27.0\%$ improvement over natural
language communication across datasets with $<$$1/4$ the compute, illustrating
the superiority and robustness of activations as an alternative ""language"" for
communication between LMs.","['cs.CL', 'cs.AI', 'cs.LG']",False,,,,Fundamental polytope for the isometry group of an alcove,Communicating Activations Between Language Model Agents
neg-d2-990,2025-03-19,,2503.149," The past a few years have witnessed the great success of large language
models, demonstrating powerful capabilities in comprehending textual data and
generating human-like languages. Large language models achieve success by being
trained on vast amounts of textual data, including online sources with
copyrighted content and user-generated knowledge. However, this comes at a
cost: the potential risk of exposing users' privacy and violating copyright
protections. Thus, to safeguard individuals' ""right to be forgotten"", there has
been increasing interests in machine unlearning -- the process of removing
information carried by particular training samples from a model while not
deteriorating its predictive quality. This is a challenging task due to the
black-box nature of language models. Most existing studies focus on mitigating
the impact of those forgot samples upon a model's outputs, and do not
explicitly consider the geometric distributions of samples in the latent space
of a model. To address this issue, we propose a machine unlearning framework,
named Deep Contrastive Unlearning for fine-Tuning (DeepCUT) language models.
Our proposed model achieves machine unlearning by directly optimizing the
latent space of a model. Comprehensive experiments on real-world datasets
demonstrate the effectiveness and efficiency of DeepCUT with consistent and
significant improvement over baseline methods.","['cs.CL', 'cs.AI']",2502.09905," Abdominal aortic aneurysm (AAA) is a life-threatening condition involving the
permanent dilation of the aorta, often detected incidentally through imaging
for some other condition. The standard clinical approach to managing AAA
follows a one-size-fits-all model based on aneurysm size and growth rate,
leading to underestimation or overestimation of rupture risk in individual
patients. The widely studied stress-based rupture risk estimation using
computational biomechanics requires wall strength information. However,
non-invasive methods for local patient-specific wall strength measurement have
not yet been developed. Recently, we introduced an image-based approach for
patient-specific, in vivo, non-invasive AAA kinematic analysis using
time-resolved 3D computed tomography angiography (4D-CTA) images to measure
wall strain throughout the cardiac cycle. In the present study, we integrated
wall tension computation and strain measurement to develop a novel measure of
local structural integrity of AAA wall - Relative Structural Integrity Index
(RSII), independent of material properties and thickness of the wall and
conditions of blood pressure measurement. Our methods provide a visual map of
AAA wall structural integrity for individual patients using only their medical
images and blood pressure data. We applied our methods to twelve patients.
Additionally, we compared our measure of structural integrity of aneurysmal and
non-aneurysmal aortas. Our results show similar values of the wall structural
integrity measure across the patients, indicating the reliability of our
methods. In line with experimental observations reported in the literature, our
analysis revealed that localized low stiffness areas are primarily found in the
most dilated AAA regions. Our results clearly demonstrate that the AAA wall is
stiffer than the non-aneurysmal aorta.",['cs.CE'],False,,,,Deep Contrastive Unlearning for Language Models,"Towards personalised assessment of abdominal aortic aneurysm structural
  integrity"
neg-d2-991,2025-02-05,,2502.02928," Automated code generation is gaining significant importance in intelligent
computer programming and system deployment. However, current approaches often
face challenges in computational efficiency and lack robust mechanisms for code
parsing and error correction. In this work, we propose a novel framework,
PyCapsule, with a simple yet effective two-agent pipeline and efficient
self-debugging modules for Python code generation. PyCapsule features
sophisticated prompt inference, iterative error handling, and case testing,
ensuring high generation stability, safety, and correctness. Empirically,
PyCapsule achieves up to 5.7% improvement of success rate on HumanEval, 10.3%
on HumanEval-ET, and 24.4% on BigCodeBench compared to the state-of-art
methods. We also observe a decrease in normalized success rate given more
self-debugging attempts, potentially affected by limited and noisy error
feedback in retention. PyCapsule demonstrates broader impacts on advancing
lightweight and efficient code generation for artificial intelligence systems.","['cs.SE', 'cs.AI']",2502.14105," Conformal prediction provides a powerful framework for constructing
prediction intervals with finite-sample guarantees, yet its robustness under
distribution shifts remains a significant challenge. This paper addresses this
limitation by modeling distribution shifts using L\'evy-Prokhorov (LP)
ambiguity sets, which capture both local and global perturbations. We provide a
self-contained overview of LP ambiguity sets and their connections to popular
metrics such as Wasserstein and Total Variation. We show that the link between
conformal prediction and LP ambiguity sets is a natural one: by propagating the
LP ambiguity set through the scoring function, we reduce complex
high-dimensional distribution shifts to manageable one-dimensional distribution
shifts, enabling exact quantification of worst-case quantiles and coverage.
Building on this analysis, we construct robust conformal prediction intervals
that remain valid under distribution shifts, explicitly linking LP parameters
to interval width and confidence levels. Experimental results on real-world
datasets demonstrate the effectiveness of the proposed approach.","['stat.ML', 'cs.LG', 'math.ST', 'stat.ME', 'stat.TH']",False,,,,Large Language Model Guided Self-Debugging Code Generation,"Conformal Prediction under L\'evy-Prokhorov Distribution Shifts:
  Robustness to Local and Global Perturbations"
neg-d2-992,2025-01-30,,2501.18876," Developing machine learning protocols for molecular simulations requires
comprehensive and efficient datasets. Here we introduce the QMe14S dataset,
comprising 186,102 small organic molecules featuring 14 elements (H, B, C, N,
O, F, Al, Si, P, S, Cl, As, Se, Br) and 47 functional groups. Using density
functional theory at the B3LYP/TZVP level, we optimized the geometries and
calculated properties including energy, atomic charge, atomic force, dipole
moment, quadrupole moment, polarizability, octupole moment, first
hyperpolarizability, and Hessian. At the same level, we obtained the harmonic
IR, Raman and NMR spectra. Furthermore, we conducted ab initio molecular
dynamics simulations to generate dynamic configurations and extract
nonequilibrium properties, including energy, forces, and Hessians. By
leveraging our E(3)-equivariant message-passing neural network (DetaNet), we
demonstrated that models trained on QMe14S outperform those trained on the
previously developed QM9S dataset in simulating molecular spectra. The QMe14S
dataset thus serves as a comprehensive benchmark for molecular simulations,
offering valuable insights into structure-property relationships.","['physics.chem-ph', 'cs.LG']",2501.01186," A summary of recent contributions in the field of rough partial differential
equations is given. For that purpose we rely on the formalism of ``unbounded
rough driver''. We present applications to concrete models including
Landau-Lifshitz-Gilbert, Navier-Stokes and Euler equations.","['math.AP', 'math.PR']",False,,,,"QMe14S, A Comprehensive and Efficient Spectral Dataset for Small Organic
  Molecules","Unbounded rough drivers, rough PDEs and applications"
neg-d2-993,2025-02-18,,2502.13026," We revisit Hawking's original derivation of the evaporation process in a
non-stationary spacetime, presenting it in a clear and pedagogical manner, with
a focus on the spherical collapse of a star into a black hole. Our analysis
highlights the underlying assumptions in the calculations, clarifying their
physical significance, potential implications, and the limitations of this
approach.",['gr-qc'],2503.18033," Omnimatte aims to decompose a given video into semantically meaningful
layers, including the background and individual objects along with their
associated effects, such as shadows and reflections. Existing methods often
require extensive training or costly self-supervised optimization. In this
paper, we present OmnimatteZero, a training-free approach that leverages
off-the-shelf pre-trained video diffusion models for omnimatte. It can remove
objects from videos, extract individual object layers along with their effects,
and composite those objects onto new videos. We accomplish this by adapting
zero-shot image inpainting techniques for video object removal, a task they
fail to handle effectively out-of-the-box. We then show that self-attention
maps capture information about the object and its footprints and use them to
inpaint the object's effects, leaving a clean background. Additionally, through
simple latent arithmetic, object layers can be isolated and recombined
seamlessly with new video layers to produce new videos. Evaluations show that
OmnimatteZero not only achieves superior performance in terms of background
reconstruction but also sets a new record for the fastest Omnimatte approach,
achieving real-time performance with minimal frame runtime.",['cs.CV'],False,,,,Deriving the paradox: original derivation of Hawking radiation,"OmnimatteZero: Training-free Real-time Omnimatte with Pre-trained Video
  Diffusion Models"
neg-d2-994,2025-03-01,,2503.00383," By locally encoding raw data into intermediate features, collaborative
inference enables end users to leverage powerful deep learning models without
exposure of sensitive raw data to cloud servers. However, recent studies have
revealed that these intermediate features may not sufficiently preserve
privacy, as information can be leaked and raw data can be reconstructed via
model inversion attacks (MIAs). Obfuscation-based methods, such as noise
corruption, adversarial representation learning, and information filters,
enhance the inversion robustness by obfuscating the task-irrelevant redundancy
empirically. However, methods for quantifying such redundancy remain elusive,
and the explicit mathematical relation between this redundancy minimization and
inversion robustness enhancement has not yet been established. To address that,
this work first theoretically proves that the conditional entropy of inputs
given intermediate features provides a guaranteed lower bound on the
reconstruction mean square error (MSE) under any MIA. Then, we derive a
differentiable and solvable measure for bounding this conditional entropy based
on the Gaussian mixture estimation and propose a conditional entropy
maximization (CEM) algorithm to enhance the inversion robustness. Experimental
results on four datasets demonstrate the effectiveness and adaptability of our
proposed CEM; without compromising feature utility and computing efficiency,
plugging the proposed CEM into obfuscation-based defense mechanisms
consistently boosts their inversion robustness, achieving average gains ranging
from 12.9\% to 48.2\%. Code is available at
\href{https://github.com/xiasong0501/CEM}{https://github.com/xiasong0501/CEM}.","['cs.LG', 'cs.AI', 'stat.ML']",2501.12647," The recent discovery of superconductivity in La$_3$Ni$_2$O$_7$ and
La$_4$Ni$_3$O$_{10}$ under high pressure stimulates intensive research
interests. These nickelates crystallize in an orthogonal/monoclinic structure
with tilted NiO$_6$ octahedra at ambient pressure and enter a density-wave-like
phase at low temperatures. The application of pressure suppresses the
octahedral tilting and triggers a transition to tetragonal structure (I4/mmm),
which is believed to be a key prerequisite for the emergence of superconducting
state. Here, by developing a high oxidative environment growth technology, we
report the first tetragonal nickelates La$_4$Ni$_3$O$_{10}$ microcrystals
without octahedral tilting at ambient pressure. In tetragonal
La$_4$Ni$_3$O$_{10}$, transport measurements find that both density-wave and
superconducting transitions are absent up to 160 GPa, indicating a robust
tetragonal metallic ground state. Density functional theory calculations reveal
that the band structure of ambient-pressure tetragonal La$_4$Ni$_3$O$_{10}$
involves more $d_{z2}$ orbital contribution to the Fermi surface, compared to
the monoclinic phase or the high-pressure superconducting tetragonal phase. The
concurrent absence of density-wave state and high-pressure superconductivity in
our ambient-pressure tetragonal crystals of La$_4$Ni$_3$O$_{10}$ suggests an
underlying correlation between these two orders. It suggests that the
tetragonal structure is not necessary, while the density-wave state is crucial
for the superconductivity in nickelates. Our findings impose important
constraints on the mechanism of pressure-induced superconductivity in
nickelates and sheds new light on exploring ambient pressure high-temperature
Ni-based superconductors.","['cond-mat.supr-con', 'cond-mat.mtrl-sci']",False,,,,"Theoretical Insights in Model Inversion Robustness and Conditional
  Entropy Maximization for Collaborative Inference Systems","Absence of superconductivity and density-wave transition in
  ambient-pressure tetragonal La$_4$Ni$_3$O$_{10}$"
neg-d2-995,2025-01-14,,2501.08056," We present numerical simulations of x-ray magnetic circular dichroism (XMCD)
at the L$_{2,3}$ edge of Ni in the weakly ferromagnetic altermagnet NiF$_2$.
Our results predict a significant XMCD signal for light propagating
perpendicular to the magnetic moments, which are approximately aligned along
the [100] easy-axis direction. The analysis shows that the altermagnetic and
ferromagnetic contributions to the XMCD signal can be uniquely distinguished by
their dependence on an applied magnetic field. By varying the angle of the
field relative to the easy axis, the in-plane orientation of both the N\'eel
vector and the net magnetization can be systematically controlled. We further
demonstrate that the XMCD signal, even under fields as strong as 40 T and for
any in-plane orientation, can be accurately described as a linear combination
of two spectral components, with geometrical prefactors determined by the field
magnitude and direction. This insight enables experimental validation of the
distinctive relationship between the N\'eel vector orientation and the x-ray
Hall vector in the rutile structure. Quantitative simulations supporting these
findings are provided.","['cond-mat.mtrl-sci', 'cond-mat.str-el']",2501.13297," Multi-modal retrieval-augmented Question Answering (MRAQA), integrating text
and images, has gained significant attention in information retrieval (IR) and
natural language processing (NLP). Traditional ranking methods rely on small
encoder-based language models, which are incompatible with modern decoder-based
generative large language models (LLMs) that have advanced various NLP tasks.
To bridge this gap, we propose RAMQA, a unified framework combining
learning-to-rank methods with generative permutation-enhanced ranking
techniques. We first train a pointwise multi-modal ranker using LLaVA as the
backbone. Then, we apply instruction tuning to train a LLaMA model for
re-ranking the top-k documents using an innovative autoregressive multi-task
learning approach. Our generative ranking model generates re-ranked document
IDs and specific answers from document candidates in various permutations.
Experiments on two MRAQA benchmarks, WebQA and MultiModalQA, show significant
improvements over strong baselines, highlighting the effectiveness of our
approach. Code and data are available at: https://github.com/TonyBY/RAMQA","['cs.CL', 'cs.AI', 'cs.IR', 'cs.LG']",False,,,,"Magnetic Dichroism in Rutile NiF$_2$: Separating Altermagnetic and
  Ferromagnetic Effects","RAMQA: A Unified Framework for Retrieval-Augmented Multi-Modal Question
  Answering"
neg-d2-996,2025-03-08,,2503.06304," The Last Level Cache (LLC) is the processor's critical bridge between on-chip
and off-chip memory levels - optimized for high density, high bandwidth, and
low operation energy. To date, high-density (HD) SRAM has been the conventional
device of choice; however, with the slowing of transistor scaling, as reflected
in the industry's almost identical HD SRAM cell size from 5 nm to 3 nm,
alternative solutions such as 3D stacking with advanced packaging like hybrid
bonding are pursued (as demonstrated in AMD's V-cache). Escalating data demands
necessitate ultra-large on-chip caches to decrease costly off-chip memory
movement, pushing the exploration of device technology toward monolithic 3D
(M3D) integration where transistors can be stacked in the back-end-of-line
(BEOL) at the interconnect level. M3D integration requires fabrication
techniques compatible with a low thermal budget (<400 degC). Among promising
BEOL device candidates are amorphous oxide semiconductor (AOS) transistors,
particularly desirable for their ultra-low leakage (<fA/um), enabling
persistent data retention (>seconds) when used in a gain-cell configuration.
This paper examines device, circuit, and system-level tradeoffs when optimizing
BEOL-compatible AOS-based 2-transistor gain cell (2T-GC) for LLC. A cache
early-exploration tool, NS-Cache, is developed to model caches in advanced 7
and 3 nm nodes and is integrated with the Gem5 simulator to systematically
benchmark the impact of the newfound density/performance when compared to
HD-SRAM, MRAM, and 1T1C eDRAM alternatives for LLC.",['cs.ET'],2503.09695," Recent work has shown that the triangular lattice spin-$1/2$ $J_1$-$J_2$
Heisenberg and XXZ antiferromagnets may exhibit coplanar or supersolid orders
proximate to a gapless Dirac spin liquid phase. We explore a distinct
$SU(2N)\!\!\times\!\!SU(M)$ fermionic parton approach, complemented by
variational Monte Carlo calculations for the spin-$1/2$ model, to study the
phase diagram of these models. We also calculate their dynamical spin response
including parton interactions within a random phase approximation, and discuss
implications for neutron scattering on triangular lattice cobaltates
Ba$_3$CoSb$_2$O$_9$, Na$_2$BaCo(PO$_4$)$_2$, K$_2$Co(SeO$_3$)$_2$,
Rb$_2$Co(SeO$_3$)$_2$, and Yb-based magnet KYbSe$_2$.",['cond-mat.str-el'],False,,,,"Optimization and Benchmarking of Monolithically Stackable Gain Cell
  Memory for Last-Level Cache","Modified large-$N$ approach to gapless spin liquids, magnetic orders,
  and dynamics: Application to triangular lattice antiferromagnets"
neg-d2-997,2025-03-18,,2503.13993," Let $\alpha\in \mathbb{R}\setminus\mathbb{Q}$ and $\beta\in \mathbb{R}$ be
given. Suppose that $a_1,\ldots,a_s$ are distinct positive integers that do not
contain a reduced residue system modulo $p^2$ for any prime $p$. We prove that
there exist infinitely many primes $p$ such that the inequality $||\alpha
p+\beta||<p^{-1/10}$ holds and all the numbers $p+a_1,\ldots,p+a_s$ are
square-free.",['math.NT'],2503.08007," Developing versatile quadruped robots that can smoothly perform various
actions and tasks in real-world environments remains a significant challenge.
This paper introduces a novel vision-language-action (VLA) model, mixture of
robotic experts (MoRE), for quadruped robots that aim to introduce
reinforcement learning (RL) for fine-tuning large-scale VLA models with a large
amount of mixed-quality data. MoRE integrates multiple low-rank adaptation
modules as distinct experts within a dense multi-modal large language model
(MLLM), forming a sparse-activated mixture-of-experts model. This design
enables the model to effectively adapt to a wide array of downstream tasks.
Moreover, we employ a reinforcement learning-based training objective to train
our model as a Q-function after deeply exploring the structural properties of
our tasks. Effective learning from automatically collected mixed-quality data
enhances data efficiency and model performance. Extensive experiments
demonstrate that MoRE outperforms all baselines across six different skills and
exhibits superior generalization capabilities in out-of-distribution scenarios.
We further validate our method in real-world scenarios, confirming the
practicality of our approach and laying a solid foundation for future research
on multi-task learning in quadruped robots.","['cs.RO', 'cs.AI']",False,,,,"On a Diophantine Inequality with Primes Yielding Square-Free Sums with
  Given Numbers","MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped
  Vision-Language-Action Models"
neg-d2-998,2025-01-16,,2501.09741," Consider a sample of size $N$ from a population governed by a hierarchical
species sampling model. We study the large $N$ asymptotic behavior of the
number ${\bf K}_N$ of clusters and the number ${\bf M}_{r,N}$ of clusters with
frequency $r$ in the sample. In particular, we show almost sure and $L^p$
convergence for ${\bf M}_{r,N}$, obtain Gaussian fluctuation theorems for ${\bf
K}_N$, and establish large deviation principles for both ${\bf K}_N$ and ${\bf
M}_{r,N}$. Our approach relies on a random sample size representation of the
number of clusters through the corresponding non-hierarchical species sampling
model.",['math.PR'],2502.01312," Category-level object pose estimation aims to recover the rotation,
translation and size of unseen instances within predefined categories. In this
task, deep neural network-based methods have demonstrated remarkable
performance. However, previous studies show they suffer from spurious
correlations raised by ""unclean"" confounders in models, hindering their
performance on novel instances with significant variations. To address this
issue, we propose CleanPose, a novel approach integrating causal learning and
knowledge distillation to enhance category-level pose estimation. To mitigate
the negative effect of unobserved confounders, we develop a causal inference
module based on front-door adjustment, which promotes unbiased estimation by
reducing potential spurious correlations. Additionally, to further improve
generalization ability, we devise a residual-based knowledge distillation
method that has proven effective in providing comprehensive category
information guidance. Extensive experiments across multiple benchmarks
(REAL275, CAMERA25 and HouseCat6D) hightlight the superiority of proposed
CleanPose over state-of-the-art methods. Code will be released.",['cs.CV'],False,,,,Asymptotic behavior of clusters in hierarchical species sampling models,"CleanPose: Category-Level Object Pose Estimation via Causal Learning and
  Knowledge Distillation"
neg-d2-999,2025-02-10,,2502.06447," The covariance tensors in statistics{, elasticity tensor in solid mechanics,
Riemann curvature tensor in relativity theory are all biquadratic tensors that
are weakly symmetric, but not symmetric in general. Motivated by this, in this
paper, we consider nonsymmetric biquadratic tensors, and study possible
conditions and algorithms for identifying positive semi-definiteness and
definiteness of such biquadratic tensors. We extend M-eigenvalues to
nonsymmetric biquadratic tensors, prove that a general biquadratic tensor has
at least one M-eigenvalue, and show that a general biquadratic tensor is
positive semi-definite if and only if all of its M-eigenvalues are nonnegative,
and a general biquadratic tensor is positive definite if and only if all of its
M-eigenvalues are positive. We present a Gershgorin-type theorem for
biquadratic tensors, and show that (strictly) diagonally dominated biquadratic
tensors are positive semi-definite (definite). We introduce Z-biquadratic
tensors, M-biquadratic tensors, strong M-biquadratic tensors, B$_0$-biquadratic
tensors and B-biquadratic tensors. We show that M-biquadratic tensors and
symmetric B$_0$-biquadratic tensors are positive semi-definite, and that strong
M-biquadratic tensors and symmetric B-biquadratic tensors are positive
definite. A Riemannian LBFGS method for computing the smallest M-eigenvalue of
a general biquadratic tensor is presented. Numerical results are reported.",['math.SP'],2503.10174," This paper presents a novel sensitivity-based distributed programming (SBDP)
approach for non-convex, large-scale nonlinear programs (NLP). The algorithm
relies on first-order sensitivities to cooperatively solve the central NLP in a
distributed manner with only neighbor-to-neighbor communication and
parallelizable local computations. The scheme is based on primal decomposition
and offers minimal algorithmic complexity. We derive sufficient local
convergence conditions for non-convex problems. Furthermore, we consider the
SBDP method in a distributed optimal control context and derive favorable
convergence properties in this setting. We illustrate these theoretical
findings and the performance of the proposed algorithm with simulations of
various distributed optimization and control problems.",['math.OC'],False,,,,Biquadratic Tensors: Eigenvalues and Structured Tensors,Sensitivity-Based Distributed Programming for Non-Convex Optimization
