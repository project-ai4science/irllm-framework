[
  {
    "id":2411.0675,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"3-D ultrasound imaging: a review",
    "start_abstract":"The development of 3-D ultrasound imaging is a way to address the disadvantages conventional imaging. In this article authors review approaches that have been attempted in such as B-mode, color Doppler, and power Doppler systems. Acquisition, reconstruction, rendering techniques for are discussed, well applications limitations.",
    "start_categories":[
      "cs.CV",
      "eess.IV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Symmetric diffeomorphic image registration with crosscorrelation: evaluating automated labeling of elderly and neurodegenerative brain"
      ],
      "abstract":[
        "One of the most challenging problems in modern neuroimaging is detailed characterization of neurodegeneration. Quantifying spatial and longitudinal atrophy patterns is an important component of this process. These spatiotemporal signals will aid in discriminating between related diseases, such as frontotemporal dementia (FTD) and Alzheimer's disease (AD), which manifest themselves in the same at-risk population. Here, we develop a novel symmetric image normalization method (SyN) for maximizing the cross-correlation within the space of diffeomorphic maps and provide the Euler-Lagrange equations necessary for this optimization. We then turn to a careful evaluation of our method. Our evaluation uses gold standard, human cortical segmentation to contrast SyN's performance with a related elastic method and with the standard ITK implementation of Thirion's Demons algorithm. The new method compares favorably with both approaches, in particular when the distance between the template brain and the target brain is large. We then report the correlation of volumes gained by algorithmic cortical labelings of FTD and control subjects with those gained by the manual rater. This comparison shows that, of the three methods tested, SyN's volume measurements are the most strongly correlated with volume measurements gained by expert labeling. This study indicates that SyN, with cross-correlation, is a reliable method for normalizing and making anatomical measurements in volumetric MRI of patients and at-risk elderly individuals."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "Neural encoding with affine feature response transforms",
        "Evolution of diverse (and advanced) cognitive abilities through adaptive\n  fine-tuning of learning and chunking mechanisms",
        "Fluctuation-response relations and response-response relations for\n  membrane voltage and spike train of stochastic integrate-and-fire neurons",
        "Using economic value signals from primate prefrontal cortex in\n  neuro-engineering applications",
        "Automatic target validation based on neuroscientific literature mining\n  for tractography",
        "The Role of Affective States in Computational Psychiatry",
        "Vagus nerve stimulation as a modulator of feedforward and feedback\n  neural transmission",
        "Multi-Site rs-fMRI Domain Alignment for Autism Spectrum Disorder\n  Auxiliary Diagnosis Based on Hyperbolic Space",
        "From Thought to Action: How a Hierarchy of Neural Dynamics Supports\n  Language Production",
        "Structuring the Environment Nudges Participants Toward Hierarchical Over\n  Shortest Path Planning",
        "The Neural Basis of Groove Sensations: Implications for Music-Based\n  Interventions and Dance Therapy in Parkinson's Disease",
        "Role of connectivity anisotropies in the dynamics of cultured neuronal\n  networks",
        "On Questions of Predictability and Control of an Intelligent System\n  Using Probabilistic State-Transitions",
        "Super-resolution Live-cell Fluorescence Lifetime Imaging",
        "A New Circle Theorem for Two Dimensional Ising Spin Glasses",
        "Learning Conditional Average Treatment Effects in Regression\n  Discontinuity Designs using Bayesian Additive Regression Trees",
        "Reducing Frequency Bias of Fourier Neural Operators in 3D Seismic\n  Wavefield Simulations Through Multi-Stage Training",
        "Left Jacobson Rings",
        "Safe exploration in reproducing kernel Hilbert spaces",
        "Orthogonal Delay-Doppler Division Multiplexing Modulation with\n  Hierarchical Mode-Based Index Modulation",
        "Periodic Response Solutions to Multi-Dimensional Nonlinear Schr\\\"odinger\n  equation with unbounded perturbation",
        "DESI DR2 Results II: Measurements of Baryon Acoustic Oscillations and\n  Cosmological Constraints",
        "Spectral modelling of Cygnus A between 110 and 250 MHz. Impact on the\n  LOFAR 21-cm signal power spectrum",
        "Calculating the Hawking Temperature of Black Holes in $f(Q)$ Gravity\n  Using the RVB Method: A Residue-Based Approach",
        "Slow Light Waveguides based on Bound States in the Continuum",
        "Multivariable $p$-adic Hodge theory for products of Galois groups",
        "Convex Split Lemma without Inequalities",
        "FGM Modeling of Thermo-Diffusive Unstable Lean Premixed Hydrogen-Air\n  Flames"
      ],
      "abstract":[
        "Current linearizing encoding models that predict neural responses to sensory\ninput typically neglect neuroscience-inspired constraints that could enhance\nmodel efficiency and interpretability. To address this, we propose a new method\ncalled affine feature response transform (AFRT), which exploits the brain's\nretinotopic organization. Applying AFRT to encode multi-unit activity in areas\nV1, V4, and IT of the macaque brain, we demonstrate that AFRT reduces redundant\ncomputations and enhances the performance of current linearizing encoding\nmodels by segmenting each neuron's receptive field into an affine retinal\ntransform, followed by a localized feature response. Remarkably, by factorizing\nreceptive fields into a sequential affine component with three interpretable\nparameters (for shifting and scaling) and response components with a small\nnumber of feature weights per response, AFRT achieves encoding with orders of\nmagnitude fewer parameters compared to unstructured models. We show that the\nretinal transform of each neuron's encoding agrees well with the brain's\nreceptive field. Together, these findings suggest that this new subset within\nspatial transformer network can be instrumental in neural encoding models of\nnaturalistic stimuli.",
        "The evolution of cognition is frequently discussed as the evolution of\ncognitive abilities or the evolution of some neuronal structures in the brain.\nHowever, since such traits or abilities are often highly complex, understanding\ntheir evolution requires explaining how they could have gradually evolved\nthrough selection acting on heritable variations in simpler cognitive\nmechanisms. With this in mind, making use of a previously proposed theory, here\nwe show how the evolution of cognitive abilities can be captured by the\nfine-tuning of basic learning mechanisms and, in particular, chunking\nmechanisms. We use the term chunking broadly for all types of non-elemental\nlearning, claiming that the process by which elements are combined into chunks\nand associated with other chunks, or elements, is critical for what the brain\ncan do, and that it must be fine-tuned to ecological conditions. We discuss the\nrelevance of this approach to studies in animal cognition, using examples from\nanimal foraging and decision-making, problem solving, and cognitive\nflexibility. Finally, we explain how even the apparent human-animal gap in\nsequence learning ability can be explained in terms of different fine-tunings\nof a similar chunking process.",
        "Neurons display spontaneous spiking (in the absence of stimulus signals) as\nwell as a characteristic response to time-dependent external stimuli. In a\nsimple but important class of stochastic neuron models, the integrate-and-fire\nmodel with Gaussian current noise, both aspects can mathematically be related\nvia fluctuation-response relations (FRRs) as has been shown recently. Here we\nextend the class of FRRs to include the susceptibilities of the membrane\nvoltage and subthreshold voltage nonlinearity as well as the power spectrum of\nthe membrane voltage. For a simple but often considered IF model, the leaky IF\nmodel with white Gaussian noise, we exploit the FRRs and derive explicit\nexpressions for the power spectrum and susceptibility of the subthreshold\nmembrane voltage. We also put forward a relation between the response functions\nof the spike train and membrane voltage, a response-response relation (RRR)\nthat holds true for a more general setting than considered in most parts of the\npaper. For the generalized IF model with an adaptation current and colored\nGaussian noise we derive an FRR and an RRR. We briefly discuss useful\napplications of the derived FRRs and RRRs.",
        "Neural signals related to movement can be measured from intracranial\nrecordings and used in brain-machine interface devices (BMI) to restore\nphysical function in impaired patients. In this study, we explore the use of\nmore abstract neural signals related to economic value in a BMI context. Using\ndata collected from the orbitofrontal cortex in non-human primates, we develop\ndeep learning-based neural decoders that can predict the monkey's choice in a\nvalue-based decision-making task. Out-of-sample performance was improved by\naugmenting the training set with synthesized data, showing the feasibility of\nusing limited training data. We further demonstrate that we can predict the\nmonkey's choice sooner using a neural forecasting module that is equipped with\ntask-related information. These findings support the feasibility of user\npreference-informed neuroengineering devices that leverage abstract cognitive\nsignals.",
        "Target identification for tractography studies requires solid anatomical\nknowledge validated by an extensive literature review across species for each\nseed structure to be studied. Manual literature review to identify targets for\na given seed region is tedious and potentially subjective. Therefore,\ncomplementary approaches would be useful. We propose to use text-mining models\nto automatically suggest potential targets from the neuroscientific literature,\nfull-text articles and abstracts, so that they can be used for anatomical\nconnection studies and more specifically for tractography. We applied\ntext-mining models to three structures: two well-studied structures, since\nvalidated deep brain stimulation targets, the internal globus pallidus and the\nsubthalamic nucleus and, the nucleus accumbens, an exploratory target for\ntreating psychiatric disorders. We performed a systematic review of the\nliterature to document the projections of the three selected structures and\ncompared it with the targets proposed by text-mining models, both in rat and\nprimate (including human). We ran probabilistic tractography on the nucleus\naccumbens and compared the output with the results of the text-mining models\nand literature review. Overall, text-mining the literature could find three\ntimes as many targets as two man-weeks of curation could. The overall\nefficiency of the text-mining against literature review in our study was 98%\nrecall (at 36% precision), meaning that over all the targets for the three\nselected seeds, only one target has been missed by text-mining. We demonstrate\nthat connectivity for a structure of interest can be extracted from a very\nlarge amount of publications and abstracts. We believe this tool will be useful\nin helping the neuroscience community to facilitate connectivity studies of\nparticular brain regions. The text mining tools used for the study are part of\nthe HBP Neuroinformatics Platform, publicly available at\nhttp:\/\/connectivity-brainer.rhcloud.com",
        "Studying psychiatric illness has often been limited by difficulties in\nconnecting symptoms and behavior to neurobiology. Computational psychiatry\napproaches promise to bridge this gap by providing formal accounts of the\nlatent information processing changes that underlie the development and\nmaintenance of psychiatric phenomena. Models based on these theories generate\nindividual-level parameter estimates which can then be tested for relationships\nto neurobiology. In this review, we explore computational modelling approaches\nto one key aspect of health and illness: affect. We discuss strengths and\nlimitations of key approaches to modelling affect, with a focus on\nreinforcement learning, active inference, the hierarchical gaussian filter, and\ndrift-diffusion models. We find that, in this literature, affect is an\nimportant source of modulation in decision making, and has a bidirectional\ninfluence on how individuals infer both internal and external states.\nHighlighting the potential role of affect in information processing changes\nunderlying symptom development, we extend an existing model of psychosis, where\naffective changes are influenced by increasing cortical noise and consequent\nincreases in either perceived environmental instability or expected noise in\nsensory input, becoming part of a self-reinforcing process generating\nnegatively valenced, over-weighted priors underlying positive symptom\ndevelopment. We then provide testable predictions from this model at\ncomputational, neurobiological, and phenomenological levels of description.",
        "Vagus nerve stimulation (VNS) has emerged as a promising therapeutic\nintervention across various neurological and psychiatric conditions, including\nepilepsy, depression, and stroke rehabilitation; however, its mechanisms of\naction on neural circuits remain incompletely understood. Here, we present a\nnovel theoretical framework based on predictive coding that conceptualizes VNS\neffects through differential modulation of feedforward and feedback neural\ncircuits. Based on recent evidence, we propose that VNS shifts the balance\nbetween feedforward and feedback processing through multiple neuromodulatory\nsystems, resulting in enhanced feedforward signal transmission. This framework\nintegrates anatomical pathways, receptor distributions, and physiological\nresponses to explain the influence of the VNS on neural dynamics across\ndifferent spatial and temporal scales. VNS may facilitate neural plasticity and\nadaptive behavior through acetylcholine and noradrenaline (norepinephrine),\nwhich differentially modulate feedforward and feedback signaling. This\nmechanistic understanding serves as a basis for interpreting the cognitive and\ntherapeutic outcomes across different clinical conditions. Our perspective\nprovides a unified theoretical framework for understanding circuit-specific VNS\neffects and suggests new directions for investigating their therapeutic\nmechanisms.",
        "In the medical field, most resting-state fMRI (rs-fMRI) data are collected\nfrom multiple hospital sites. Multi-site rs-fMRI data can increase the volume\nof training data, enabling auxiliary diagnostic algorithms for brain diseases\nto learn more accurate and stable models. However, due to the significant\nheterogeneity and domain shift in rs-fMRI data across different sites, the\naccuracy of auxiliary diagnosis remains unsatisfactory. Moreover, there has\nbeen limited exploration of multi-source domain adaptation algorithms, and the\ninterpretability of models is often poor. To address these challenges, we\nproposed a domain-adaptive algorithm based on hyperbolic space embedding.\nHyperbolic space is naturally suited for representing the topology of complex\nnetworks such as brain functional networks. Therefore, we embedded the brain\nfunctional network into hyperbolic space and constructed the corresponding\nhyperbolic space community network to effectively extract brain network\nrepresentations. To address the heterogeneity of data across different sites\nand the issue of domain shift, we introduce a constraint loss function, HMMD\n(Hyperbolic Maximum Mean Discrepancy), to align the marginal distributions in\nthe hyperbolic space. Additionally, we employ class prototype alignment to\nalign the conditional distributions. This significantly improves the quality of\nbrain representations and enhances diagnostic classification accuracy for\nAutism Spectrum Disorder (ASD). Experimental results demonstrated that the\nproposed algorithm is robust to multi-site heterogeneity and shows promising\npotential for brain network mechanism analysis.",
        "Humans effortlessly communicate their thoughts through intricate sequences of\nmotor actions. Yet, the neural processes that coordinate language production\nremain largely unknown, in part because speech artifacts limit the use of\nneuroimaging. To elucidate the unfolding of language production in the brain,\nwe investigate with magnetoencephalography (MEG) and electroencephalography\n(EEG) the neurophysiological activity of 35 skilled typists, while they typed\nsentences on a keyboard. This approach confirms the hierarchical predictions of\nlinguistic theories: the neural activity preceding the production of each word\nis marked by the sequential rise and fall of context-, word-, syllable-, and\nletter-level representations. Remarkably, each of these neural representations\nis maintained over long time periods within each level of the language\nhierarchy. This phenomenon results in a superposition of successive\nrepresentations that is supported by a hierarchy of dynamic neural codes.\nOverall, these findings provide a precise computational breakdown of the neural\ndynamics that coordinate the production of language in the human brain.",
        "Effective planning is crucial for navigating complex environments and\nachieving goals efficiently. In this study, we investigated how environmental\nstructure influences the selection of planning strategies. Participants\nnavigated a space station to collect colored spheres, with environments either\nstructured (spheres grouped by color) or unstructured (spheres scattered\nrandomly). We tested three types of plans: hierarchical (grouping spheres by\ncolor), shortest path (minimizing travel distance), and neutral (none of the\nabove). By manipulating environmental structure, we were able to nudge\nparticipants toward a preference for hierarchical planning in structured\nenvironments, while shortest path plans were favored in unstructured\nenvironments. A mismatch between self-reported preferences and actual choices\nindicated that participants often adopted implicit strategies, unaware of their\ndecision-making processes. These findings highlight the powerful effect of\nenvironmental cues on planning and suggest that even subtle changes in\nstructure can guide the selection of planning strategies.",
        "Groove sensations arise from rhythmic structures that evoke an urge to move\nin response to music. While syncopation has been extensively studied in groove\nperception, the neural mechanisms underlying low-frequency groove remain\nunderexplored. This fMRI study examines the role of the mirror neuron system\nand associated brain regions in processing low-frequency groove.\nRegion-of-interest analysis revealed that amplifying drum and bass components\nin K-pop songs significantly increased activity in the right posterior inferior\nfrontal gyrus, right inferior\/superior parietal lobules, left dorsolateral\nprefrontal cortex, and bilateral posterior middle\/inferior temporal gyrus.\nThese findings suggest that low-frequency grooves engage sensorimotor,\nexecutive, and rhythm semantics networks, reinforcing their role in\naction-related processing. Building on these insights, we propose an enhanced\nrhythmic auditory stimulation paradigm for Parkinson's disease, incorporating\namplified low-frequency rhythmic cues to improve gait synchronization.",
        "Laboratory-grown, engineered living neuronal networks in vitro have emerged\nin the last years as an experimental technique to understand the collective\nbehavior of neuronal assemblies in relation to their underlying connectivity.\nAn inherent obstacle in the design of such engineered systems is the difficulty\nto predict the dynamic repertoire of the emerging network and its dependence on\nexperimental variables. To fill this gap, and inspired on recent experimental\nstudies, here we present a numerical model that aims at, first, replicating the\nanisotropies in connectivity imprinted through engineering, to next realize the\ncollective behavior of the neuronal network and make predictions. We use\nexperimentally measured, biologically-realistic data combined with the\nIzhikevich model to quantify the dynamics of the neuronal network in relation\nto tunable structural and dynamical parameters. These parameters include the\nsynaptic noise, strength of the imprinted anisotropies, and average axon\nlengths. The latter are involved in the simulation of the development of\nneurons in vitro. We show that the model captures the behavior of engineered\nneuronal cultures, in which a rich repertoire of activity patterns emerge but\nwhose details are strongly dependent on connectivity details and noise. Results\nalso show that the presence of connectivity anisotropies substantially improves\nthe capacity of reconstructing structural connectivity from activity data, an\naspect that is important in the quest for understanding the\nstructure-to-function relationship in neuronal networks. Our work provides the\nin silico basis to assist experimentalists in the design of laboratory in vitro\nnetworks and anticipate their outcome, an aspect that is particularly important\nin the effort to conceive reliable brain-on-a-chip circuits and explore key\naspects such as input-output relationships or information coding.",
        "One of the central aims of neuroscience is to reliably predict the behavioral\nresponse of an organism using its neural activity. If possible, this implies we\ncan causally manipulate the neural response and design brain-computer-interface\nsystems to alter behavior, and vice-versa. Hence, predictions play an important\nrole in both fundamental neuroscience and its applications. Can we predict the\nneural and behavioral states of an organism at any given time? Can we predict\nbehavioral states using neural states, and vice-versa, and is there a\nmemory-component required to reliably predict such states? Are the predictions\ncomputable within a given timescale to meaningfully stimulate and make the\nsystem reach the desired states? Through a series of mathematical treatments,\nsuch conjectures and questions are discussed. Answering them might be key for\nfuture developments in understanding intelligence and designing\nbrain-computer-interfaces.",
        "Super-resolution Structured Illumination Microscopy (SR-SIM) enables\nfluorescence microscopy beyond the diffraction limit at high frame rates.\nCompared to other super-resolution microscopy techniques, the low photon\nfluence used in SR-SIM makes it readily compatible with live-cell imaging.\nHere, we combine SR-SIM with electro-optic fluorescence lifetime imaging\n(EOFLIM), adding the capability of monitoring physicochemical parameters with\n156 nm spatial resolution at high frame rate for live-cell imaging. We\ndemonstrate that our new SIMFLIM technique enables super-resolved multiplexed\nimaging of spectrally overlapping fluorophores, environmental sensing, and\nlive-cell imaging.",
        "The Lee-Yang circle theorem revolutionized our understanding of phase\ntransitions in ferromagnetic systems by showing that the complex zeros of\npartition functions lie on the unit circle, with criticality arising as these\nzeros approach the real axis in the thermodynamic limit. However, in frustrated\nsystems such as antiferromagnets and spin glasses, the zeros deviate from this\nstructure, making it challenging to extend the Lee-Yang theory to disordered\nsystems. In this work, we establish a new circle theorem for two-dimensional\nIsing spin glasses, proving that the square of the partition function exhibits\nzeros densely packed along the unit circle. Numerical simulations on the square\nlattice confirm our theoretical predictions, demonstrating the validity of the\ncircle law for quenched disorder. Furthermore, our results uncover a\nfinite-temperature crossover in $\\pm J$ spin glasses, characterized by the\nemergence of a spectral gap in the angular distribution of zeros. This result\nextends the Lee-Yang framework to disordered systems, offering new insights\ninto spin-glass criticality.",
        "BART (Bayesian additive regression trees) has been established as a leading\nsupervised learning method, particularly in the field of causal inference. This\npaper explores the use of BART models for learning conditional average\ntreatment effects (CATE) from regression discontinuity designs, where treatment\nassignment is based on whether an observed covariate (called the running\nvariable) exceeds a pre-specified threshold. A purpose-built version of BART\nthat uses linear regression leaf models (of the running variable and treatment\nassignment dummy) is shown to out-perform off-the-shelf BART implementations as\nwell as a local polynomial regression approach and a CART-based approach. The\nnew method is evaluated in thorough simulation studies as well as an empirical\napplication looking at the effect of academic probation on student performance.",
        "The recent development of Neural Operator (NeurOp) learning for solutions to\nthe elastic wave equation shows promising results and provides the basis for\nfast large-scale simulations for different seismological applications. In this\npaper, we use the Fourier Neural Operator (FNO) model to directly solve the 3D\nHelmholtz wave equation for fast seismic ground motion simulations on different\nfrequencies, and show the frequency bias of the FNO model, i.e. it learns the\nlower frequencies better comparing to the higher frequencies. To reduce the\nfrequency bias, we adopt the multi-stage FNO training, i.e., after training a\n1st stage FNO model for estimating the ground motion, we use a second FNO model\nas the 2nd stage to learn from the residual, which greatly reduced the errors\non the higher frequencies. By adopting this multi-stage training, the FNO\nmodels show reduced biases on higher frequencies, which enhanced the overall\nresults of the ground motion simulations. Thus the multi-stage training FNO\nimproves the accuracy and realism of the ground motion simulations.",
        "We say that a ring is strongly (resp. weakly) left Jacobson if every\nsemiprime (resp. prime) left ideal is an intersection of maximal left ideals.\nThere exist Jacobson rings that are not weakly left Jacobson, e.g. the Weyl\nalgebra. Our main result is the following one-sided noncommutative\nNullstellensatz: For any finite-dimensional F-algebra A the ring\nA[$x_1$,...,$x_n$] of polynomials with coefficients in A is strongly left\nJacobson and every maximal left ideal of A[$x_1$,...,$x_n$] has a special form.\nWe also prove that an algebra that is a finitely generated module over its\ncenter is weakly left Jacobson iff it is Jacobson, and that an Azumaya algebra\nis strongly left Jacobson iff its center is Jacobson.",
        "Popular safe Bayesian optimization (BO) algorithms learn control policies for\nsafety-critical systems in unknown environments. However, most algorithms make\na smoothness assumption, which is encoded by a known bounded norm in a\nreproducing kernel Hilbert space (RKHS). The RKHS is a potentially\ninfinite-dimensional space, and it remains unclear how to reliably obtain the\nRKHS norm of an unknown function. In this work, we propose a safe BO algorithm\ncapable of estimating the RKHS norm from data. We provide statistical\nguarantees on the RKHS norm estimation, integrate the estimated RKHS norm into\nexisting confidence intervals and show that we retain theoretical guarantees,\nand prove safety of the resulting safe BO algorithm. We apply our algorithm to\nsafely optimize reinforcement learning policies on physics simulators and on a\nreal inverted pendulum, demonstrating improved performance, safety, and\nscalability compared to the state-of-the-art.",
        "The orthogonal time frequency space with index modulation (OTFS-IM) offers\nflexible tradeoffs between spectral efficiency (SE) and bit error rate (BER) in\ndoubly selective fading channels. While OTFS-IM schemes demonstrated such\npotential, a persistent challenge lies in the detection complexity. To address\nthis problem, we propose the hierarchical mode-based index modulation (HMIM).\nHMIM introduces a novel approach to modulate information bits by IM patterns,\nsignificantly simplifying the complexity of maximum a posteriori (MAP)\nestimation with Gaussian noise. Further, we incorporate HMIM with the recently\nproposed orthogonal delay-Doppler division multiplexing (ODDM) modulation,\nnamely ODDM-HMIM, to exploit the full diversity of the delay-Doppler (DD)\nchannel. The BER performance of ODDM-HMIM is analyzed considering a maximum\nlikelihood (ML) detector. Our numerical results reveal that, with the same SE,\nHMIM can outperform conventional IM in terms of both BER and computational\ncomplexity. In addition, we propose a successive interference\ncancellation-based minimum mean square error (SIC-MMSE) detector for ODDM-HMIM,\nwhich enables low-complexity detection with large frame sizes.",
        "By applying the Craig-Wayne-Bourgain (CWB) method, we establish the existence\nof periodic response solutions to multi-dimensional nonlinear Schr\\\"{o}dinger\nequations (NLS) with unbounded perturbation.",
        "We present baryon acoustic oscillation (BAO) measurements from more than 14\nmillion galaxies and quasars drawn from the Dark Energy Spectroscopic\nInstrument (DESI) Data Release 2 (DR2), based on three years of operation. For\ncosmology inference, these galaxy measurements are combined with DESI\nLyman-$\\alpha$ forest BAO results presented in a companion paper. The DR2 BAO\nresults are consistent with DESI DR1 and SDSS, and their distance-redshift\nrelationship matches those from recent compilations of supernovae (SNe) over\nthe same redshift range. The results are well described by a flat $\\Lambda$CDM\nmodel, but the parameters preferred by BAO are in mild, $2.3\\sigma$ tension\nwith those determined from the cosmic microwave background (CMB), although the\nDESI results are consistent with the acoustic angular scale $\\theta_*$ that is\nwell-measured by Planck. This tension is alleviated by dark energy with a\ntime-evolving equation of state parametrized by $w_0$ and $w_a$, which provides\na better fit to the data, with a favored solution in the quadrant with $w_0>-1$\nand $w_a<0$. This solution is preferred over $\\Lambda$CDM at $3.1\\sigma$ for\nthe combination of DESI BAO and CMB data. When also including SNe, the\npreference for a dynamical dark energy model over $\\Lambda$CDM ranges from\n$2.8-4.2\\sigma$ depending on which SNe sample is used. We present evidence from\nother data combinations which also favor the same behavior at high\nsignificance. From the combination of DESI and CMB we derive 95% upper limits\non the sum of neutrino masses, finding $\\sum m_\\nu<0.064$ eV assuming\n$\\Lambda$CDM and $\\sum m_\\nu<0.16$ eV in the $w_0w_a$ model. Unless there is an\nunknown systematic error associated with one or more datasets, it is clear that\n$\\Lambda$CDM is being challenged by the combination of DESI BAO with other\nmeasurements and that dynamical dark energy offers a possible solution.",
        "Studying the redshifted 21-cm signal from the the neutral hydrogen during the\nEpoch of Reionization and Cosmic Dawn is fundamental for understanding the\nphysics of the early universe. One of the challenges that 21-cm experiments\nface is the contamination by bright foreground sources, such as Cygnus A, for\nwhich accurate spatial and spectral models are needed to minimise the residual\ncontamination after their removal. In this work, we develop a new,\nhigh-resolution model of Cygnus A using Low Frequency Array (LOFAR)\nobservations in the $110{-}250$ MHz range, improving upon previous models by\nincorporating physical spectral information through the forced-spectrum method\nduring multi-frequency deconvolution. This approach addresses the limitations\nof earlier models by providing a more accurate representation of the complex\nstructure and spectral behaviour of Cygnus A, including the spectral turnover\nin its brightest hotspots. The impact of this new model on the LOFAR 21-cm\nsignal power spectrum is assessed by comparing it with both simulated and\nobserved North Celestial Pole data sets. Significant improvements are observed\nin the cylindrical power spectrum along the Cygnus A direction, highlighting\nthe importance of having spectrally accurate models of the brightest foreground\nsources. However, this improvement is washed out in the spherical power\nspectrum, where we measure differences of a few hundred mK at\n$k<0.63\\,h\\,\\text{cMpc}^{-1}$, but not statistically significant. The results\nsuggest that other systematic effects must be mitigated before a substantial\nimpact on 21-cm power spectrum can be achieved.",
        "This paper investigates the computation of Hawking temperatures for black\nholes within various $f(Q)$ gravity models using the RVB method. This\ntopological approach uncovers an additional term in the temperature\ncalculation, which we propose originates from the residue of a specific contour\nintegral related to the metric or curvature. By examining several specific\n$f(Q)$ models including quadratic, logarithmic, square root, and power law\nmodifications as well as well known black hole solutions such as RN, Kerr, and\nKN black holes, we demonstrate that the correction term can consistently be\ninterpreted as this residue. Our findings provide new insights into black hole\nthermodynamics within modified gravity frameworks.",
        "The concept of bound states in the continuum (BIC) has been advancing light\nconfinement technology in leaky environments. In this letter, we propose and\nnumerically demonstrate a slow light waveguide based on a BIC mode. We\nconsidered a waveguide with a polymer core loaded on a plane slab, which\nsupports a leaky guided mode coupled to the radiation continuum in the slab. We\nfound that periodic modulation of the polymer core along the propagation\ndirection can result in a high group index mode with a low propagation loss due\nto BIC confinement. The introduction of one-dimensional photonic crystals into\nthe BIC waveguides will largely expand its functionality and applications in\nintegrated photonics.",
        "In this paper we explain how to attach to a family of $p$-adic\nrepresentations of a product of Galois groups an overconvergent family of\nmultivariable $(\\varphi,\\Gamma)$-modules, generalizing results from Pal-Zabradi\nand Carter-Kedlaya-Zabradi, using Colmez-Sen-Tate descent. We also define rings\nof multivariable crystalline and semistable periods, and explain how to recover\nthis multivariable $p$-adic theory attached to a family of representations from\nits multivariable $(\\varphi,\\Gamma)$-module. We also explain how our framework\nallows us to recover the main results of Brinon-Chiarellotto-Mazzari on\nmultivariable $p$-adic Galois representations.",
        "We introduce a refinement to the convex split lemma by replacing the max\nmutual information with the collision mutual information, transforming the\ninequality into an equality. This refinement yields tighter achievability\nbounds for quantum source coding tasks, including state merging and state\nsplitting. Furthermore, we derive a universal upper bound on the smoothed max\nmutual information, where \"universal\" signifies that the bound depends\nexclusively on R\\'enyi entropies and is independent of the system's dimensions.\nThis result has significant implications for quantum information processing,\nparticularly in applications such as the reverse quantum Shannon theorem.",
        "Ultra-lean premixed hydrogen combustion is a possible solution to decarbonize\nindustry, while limiting flame temperatures and thus nitrous oxide emissions.\nThese lean hydrogen\/air flames experience strong preferential diffusion\neffects, which result in thermo-diffusive (TD) instabilities. To efficiently\nand accurately model lean premixed hydrogen flames, it is crucial to\nincorporate these preferential diffusion effects into flamelet tabulated\nchemistry frameworks, such as the Flamelet-Generated Manifold (FGM) method.\nThis is challenging because the preferential diffusion terms in the control\nvariable transport equations contain diffusion fluxes of all species in the\nmechanism. In this work, a new implementation is presented; the full term is\nreduced by only considering the most contributing species. When carefully\nselecting this set of major species, preferential diffusion fluxes along the\nflame front, i.e., cross-diffusion, can be captured. This is particularly\nimportant for manifolds that include heat loss effects, where enthalpy is one\nof the control variables. The diffusion of the H-radical has a significant\ncontribution to the enthalpy transport equation, and cross-diffusion of the\nH-radical is non-negligible. Two manifolds, without and with heat loss effects,\nand the set of major species are analyzed in an a-priori and a-posteriori\nmanner. Simulations of TD unstable hydrogen-air flames with detailed chemistry\nand several FGM models show that accurately capturing cross-diffusion of\nenthalpy is important for correctly predicting the flame shape and dynamics."
      ]
    }
  },
  {
    "id":2411.0675,
    "research_type":"applied",
    "start_id":"b21",
    "start_title":"Generative AI for Medical Imaging: extending the MONAI Framework",
    "start_abstract":"Recent advances in generative AI have brought incredible breakthroughs several areas, including medical imaging. These models tremendous potential not only to help safely share data via synthetic datasets but also perform an array of diverse applications, such as anomaly detection, image-to-image translation, denoising, and MRI reconstruction. However, due the complexity these models, their implementation reproducibility can be difficult. This hinder progress, act a use barrier, dissuade comparison new methods with existing works. In this study, we present MONAI Generative Models, freely available open-source platform that allows researchers developers easily train, evaluate, deploy related applications. Our reproduces state-of-art studies standardised way involving different architectures (such diffusion autoregressive transformers, GANs), provides pre-trained for community. We implemented generalisable fashion, illustrating results extended 2D or 3D scenarios, images modalities (like CT, MRI, X-Ray data) from anatomical areas. Finally, adopt modular extensible approach, ensuring long-term maintainability extension current applications future features.",
    "start_categories":[
      "cs.CV",
      "eess.IV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Symmetric diffeomorphic image registration with crosscorrelation: evaluating automated labeling of elderly and neurodegenerative brain"
      ],
      "abstract":[
        "One of the most challenging problems in modern neuroimaging is detailed characterization of neurodegeneration. Quantifying spatial and longitudinal atrophy patterns is an important component of this process. These spatiotemporal signals will aid in discriminating between related diseases, such as frontotemporal dementia (FTD) and Alzheimer's disease (AD), which manifest themselves in the same at-risk population. Here, we develop a novel symmetric image normalization method (SyN) for maximizing the cross-correlation within the space of diffeomorphic maps and provide the Euler-Lagrange equations necessary for this optimization. We then turn to a careful evaluation of our method. Our evaluation uses gold standard, human cortical segmentation to contrast SyN's performance with a related elastic method and with the standard ITK implementation of Thirion's Demons algorithm. The new method compares favorably with both approaches, in particular when the distance between the template brain and the target brain is large. We then report the correlation of volumes gained by algorithmic cortical labelings of FTD and control subjects with those gained by the manual rater. This comparison shows that, of the three methods tested, SyN's volume measurements are the most strongly correlated with volume measurements gained by expert labeling. This study indicates that SyN, with cross-correlation, is a reliable method for normalizing and making anatomical measurements in volumetric MRI of patients and at-risk elderly individuals."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "Neural encoding with affine feature response transforms",
        "Evolution of diverse (and advanced) cognitive abilities through adaptive\n  fine-tuning of learning and chunking mechanisms",
        "Fluctuation-response relations and response-response relations for\n  membrane voltage and spike train of stochastic integrate-and-fire neurons",
        "Using economic value signals from primate prefrontal cortex in\n  neuro-engineering applications",
        "Automatic target validation based on neuroscientific literature mining\n  for tractography",
        "The Role of Affective States in Computational Psychiatry",
        "Vagus nerve stimulation as a modulator of feedforward and feedback\n  neural transmission",
        "Multi-Site rs-fMRI Domain Alignment for Autism Spectrum Disorder\n  Auxiliary Diagnosis Based on Hyperbolic Space",
        "From Thought to Action: How a Hierarchy of Neural Dynamics Supports\n  Language Production",
        "Structuring the Environment Nudges Participants Toward Hierarchical Over\n  Shortest Path Planning",
        "The Neural Basis of Groove Sensations: Implications for Music-Based\n  Interventions and Dance Therapy in Parkinson's Disease",
        "Role of connectivity anisotropies in the dynamics of cultured neuronal\n  networks",
        "On Questions of Predictability and Control of an Intelligent System\n  Using Probabilistic State-Transitions",
        "Super-resolution Live-cell Fluorescence Lifetime Imaging",
        "A New Circle Theorem for Two Dimensional Ising Spin Glasses",
        "Learning Conditional Average Treatment Effects in Regression\n  Discontinuity Designs using Bayesian Additive Regression Trees",
        "Reducing Frequency Bias of Fourier Neural Operators in 3D Seismic\n  Wavefield Simulations Through Multi-Stage Training",
        "Left Jacobson Rings",
        "Safe exploration in reproducing kernel Hilbert spaces",
        "Orthogonal Delay-Doppler Division Multiplexing Modulation with\n  Hierarchical Mode-Based Index Modulation",
        "Periodic Response Solutions to Multi-Dimensional Nonlinear Schr\\\"odinger\n  equation with unbounded perturbation",
        "DESI DR2 Results II: Measurements of Baryon Acoustic Oscillations and\n  Cosmological Constraints",
        "Spectral modelling of Cygnus A between 110 and 250 MHz. Impact on the\n  LOFAR 21-cm signal power spectrum",
        "Calculating the Hawking Temperature of Black Holes in $f(Q)$ Gravity\n  Using the RVB Method: A Residue-Based Approach",
        "Slow Light Waveguides based on Bound States in the Continuum",
        "Multivariable $p$-adic Hodge theory for products of Galois groups",
        "Convex Split Lemma without Inequalities",
        "FGM Modeling of Thermo-Diffusive Unstable Lean Premixed Hydrogen-Air\n  Flames"
      ],
      "abstract":[
        "Current linearizing encoding models that predict neural responses to sensory\ninput typically neglect neuroscience-inspired constraints that could enhance\nmodel efficiency and interpretability. To address this, we propose a new method\ncalled affine feature response transform (AFRT), which exploits the brain's\nretinotopic organization. Applying AFRT to encode multi-unit activity in areas\nV1, V4, and IT of the macaque brain, we demonstrate that AFRT reduces redundant\ncomputations and enhances the performance of current linearizing encoding\nmodels by segmenting each neuron's receptive field into an affine retinal\ntransform, followed by a localized feature response. Remarkably, by factorizing\nreceptive fields into a sequential affine component with three interpretable\nparameters (for shifting and scaling) and response components with a small\nnumber of feature weights per response, AFRT achieves encoding with orders of\nmagnitude fewer parameters compared to unstructured models. We show that the\nretinal transform of each neuron's encoding agrees well with the brain's\nreceptive field. Together, these findings suggest that this new subset within\nspatial transformer network can be instrumental in neural encoding models of\nnaturalistic stimuli.",
        "The evolution of cognition is frequently discussed as the evolution of\ncognitive abilities or the evolution of some neuronal structures in the brain.\nHowever, since such traits or abilities are often highly complex, understanding\ntheir evolution requires explaining how they could have gradually evolved\nthrough selection acting on heritable variations in simpler cognitive\nmechanisms. With this in mind, making use of a previously proposed theory, here\nwe show how the evolution of cognitive abilities can be captured by the\nfine-tuning of basic learning mechanisms and, in particular, chunking\nmechanisms. We use the term chunking broadly for all types of non-elemental\nlearning, claiming that the process by which elements are combined into chunks\nand associated with other chunks, or elements, is critical for what the brain\ncan do, and that it must be fine-tuned to ecological conditions. We discuss the\nrelevance of this approach to studies in animal cognition, using examples from\nanimal foraging and decision-making, problem solving, and cognitive\nflexibility. Finally, we explain how even the apparent human-animal gap in\nsequence learning ability can be explained in terms of different fine-tunings\nof a similar chunking process.",
        "Neurons display spontaneous spiking (in the absence of stimulus signals) as\nwell as a characteristic response to time-dependent external stimuli. In a\nsimple but important class of stochastic neuron models, the integrate-and-fire\nmodel with Gaussian current noise, both aspects can mathematically be related\nvia fluctuation-response relations (FRRs) as has been shown recently. Here we\nextend the class of FRRs to include the susceptibilities of the membrane\nvoltage and subthreshold voltage nonlinearity as well as the power spectrum of\nthe membrane voltage. For a simple but often considered IF model, the leaky IF\nmodel with white Gaussian noise, we exploit the FRRs and derive explicit\nexpressions for the power spectrum and susceptibility of the subthreshold\nmembrane voltage. We also put forward a relation between the response functions\nof the spike train and membrane voltage, a response-response relation (RRR)\nthat holds true for a more general setting than considered in most parts of the\npaper. For the generalized IF model with an adaptation current and colored\nGaussian noise we derive an FRR and an RRR. We briefly discuss useful\napplications of the derived FRRs and RRRs.",
        "Neural signals related to movement can be measured from intracranial\nrecordings and used in brain-machine interface devices (BMI) to restore\nphysical function in impaired patients. In this study, we explore the use of\nmore abstract neural signals related to economic value in a BMI context. Using\ndata collected from the orbitofrontal cortex in non-human primates, we develop\ndeep learning-based neural decoders that can predict the monkey's choice in a\nvalue-based decision-making task. Out-of-sample performance was improved by\naugmenting the training set with synthesized data, showing the feasibility of\nusing limited training data. We further demonstrate that we can predict the\nmonkey's choice sooner using a neural forecasting module that is equipped with\ntask-related information. These findings support the feasibility of user\npreference-informed neuroengineering devices that leverage abstract cognitive\nsignals.",
        "Target identification for tractography studies requires solid anatomical\nknowledge validated by an extensive literature review across species for each\nseed structure to be studied. Manual literature review to identify targets for\na given seed region is tedious and potentially subjective. Therefore,\ncomplementary approaches would be useful. We propose to use text-mining models\nto automatically suggest potential targets from the neuroscientific literature,\nfull-text articles and abstracts, so that they can be used for anatomical\nconnection studies and more specifically for tractography. We applied\ntext-mining models to three structures: two well-studied structures, since\nvalidated deep brain stimulation targets, the internal globus pallidus and the\nsubthalamic nucleus and, the nucleus accumbens, an exploratory target for\ntreating psychiatric disorders. We performed a systematic review of the\nliterature to document the projections of the three selected structures and\ncompared it with the targets proposed by text-mining models, both in rat and\nprimate (including human). We ran probabilistic tractography on the nucleus\naccumbens and compared the output with the results of the text-mining models\nand literature review. Overall, text-mining the literature could find three\ntimes as many targets as two man-weeks of curation could. The overall\nefficiency of the text-mining against literature review in our study was 98%\nrecall (at 36% precision), meaning that over all the targets for the three\nselected seeds, only one target has been missed by text-mining. We demonstrate\nthat connectivity for a structure of interest can be extracted from a very\nlarge amount of publications and abstracts. We believe this tool will be useful\nin helping the neuroscience community to facilitate connectivity studies of\nparticular brain regions. The text mining tools used for the study are part of\nthe HBP Neuroinformatics Platform, publicly available at\nhttp:\/\/connectivity-brainer.rhcloud.com",
        "Studying psychiatric illness has often been limited by difficulties in\nconnecting symptoms and behavior to neurobiology. Computational psychiatry\napproaches promise to bridge this gap by providing formal accounts of the\nlatent information processing changes that underlie the development and\nmaintenance of psychiatric phenomena. Models based on these theories generate\nindividual-level parameter estimates which can then be tested for relationships\nto neurobiology. In this review, we explore computational modelling approaches\nto one key aspect of health and illness: affect. We discuss strengths and\nlimitations of key approaches to modelling affect, with a focus on\nreinforcement learning, active inference, the hierarchical gaussian filter, and\ndrift-diffusion models. We find that, in this literature, affect is an\nimportant source of modulation in decision making, and has a bidirectional\ninfluence on how individuals infer both internal and external states.\nHighlighting the potential role of affect in information processing changes\nunderlying symptom development, we extend an existing model of psychosis, where\naffective changes are influenced by increasing cortical noise and consequent\nincreases in either perceived environmental instability or expected noise in\nsensory input, becoming part of a self-reinforcing process generating\nnegatively valenced, over-weighted priors underlying positive symptom\ndevelopment. We then provide testable predictions from this model at\ncomputational, neurobiological, and phenomenological levels of description.",
        "Vagus nerve stimulation (VNS) has emerged as a promising therapeutic\nintervention across various neurological and psychiatric conditions, including\nepilepsy, depression, and stroke rehabilitation; however, its mechanisms of\naction on neural circuits remain incompletely understood. Here, we present a\nnovel theoretical framework based on predictive coding that conceptualizes VNS\neffects through differential modulation of feedforward and feedback neural\ncircuits. Based on recent evidence, we propose that VNS shifts the balance\nbetween feedforward and feedback processing through multiple neuromodulatory\nsystems, resulting in enhanced feedforward signal transmission. This framework\nintegrates anatomical pathways, receptor distributions, and physiological\nresponses to explain the influence of the VNS on neural dynamics across\ndifferent spatial and temporal scales. VNS may facilitate neural plasticity and\nadaptive behavior through acetylcholine and noradrenaline (norepinephrine),\nwhich differentially modulate feedforward and feedback signaling. This\nmechanistic understanding serves as a basis for interpreting the cognitive and\ntherapeutic outcomes across different clinical conditions. Our perspective\nprovides a unified theoretical framework for understanding circuit-specific VNS\neffects and suggests new directions for investigating their therapeutic\nmechanisms.",
        "In the medical field, most resting-state fMRI (rs-fMRI) data are collected\nfrom multiple hospital sites. Multi-site rs-fMRI data can increase the volume\nof training data, enabling auxiliary diagnostic algorithms for brain diseases\nto learn more accurate and stable models. However, due to the significant\nheterogeneity and domain shift in rs-fMRI data across different sites, the\naccuracy of auxiliary diagnosis remains unsatisfactory. Moreover, there has\nbeen limited exploration of multi-source domain adaptation algorithms, and the\ninterpretability of models is often poor. To address these challenges, we\nproposed a domain-adaptive algorithm based on hyperbolic space embedding.\nHyperbolic space is naturally suited for representing the topology of complex\nnetworks such as brain functional networks. Therefore, we embedded the brain\nfunctional network into hyperbolic space and constructed the corresponding\nhyperbolic space community network to effectively extract brain network\nrepresentations. To address the heterogeneity of data across different sites\nand the issue of domain shift, we introduce a constraint loss function, HMMD\n(Hyperbolic Maximum Mean Discrepancy), to align the marginal distributions in\nthe hyperbolic space. Additionally, we employ class prototype alignment to\nalign the conditional distributions. This significantly improves the quality of\nbrain representations and enhances diagnostic classification accuracy for\nAutism Spectrum Disorder (ASD). Experimental results demonstrated that the\nproposed algorithm is robust to multi-site heterogeneity and shows promising\npotential for brain network mechanism analysis.",
        "Humans effortlessly communicate their thoughts through intricate sequences of\nmotor actions. Yet, the neural processes that coordinate language production\nremain largely unknown, in part because speech artifacts limit the use of\nneuroimaging. To elucidate the unfolding of language production in the brain,\nwe investigate with magnetoencephalography (MEG) and electroencephalography\n(EEG) the neurophysiological activity of 35 skilled typists, while they typed\nsentences on a keyboard. This approach confirms the hierarchical predictions of\nlinguistic theories: the neural activity preceding the production of each word\nis marked by the sequential rise and fall of context-, word-, syllable-, and\nletter-level representations. Remarkably, each of these neural representations\nis maintained over long time periods within each level of the language\nhierarchy. This phenomenon results in a superposition of successive\nrepresentations that is supported by a hierarchy of dynamic neural codes.\nOverall, these findings provide a precise computational breakdown of the neural\ndynamics that coordinate the production of language in the human brain.",
        "Effective planning is crucial for navigating complex environments and\nachieving goals efficiently. In this study, we investigated how environmental\nstructure influences the selection of planning strategies. Participants\nnavigated a space station to collect colored spheres, with environments either\nstructured (spheres grouped by color) or unstructured (spheres scattered\nrandomly). We tested three types of plans: hierarchical (grouping spheres by\ncolor), shortest path (minimizing travel distance), and neutral (none of the\nabove). By manipulating environmental structure, we were able to nudge\nparticipants toward a preference for hierarchical planning in structured\nenvironments, while shortest path plans were favored in unstructured\nenvironments. A mismatch between self-reported preferences and actual choices\nindicated that participants often adopted implicit strategies, unaware of their\ndecision-making processes. These findings highlight the powerful effect of\nenvironmental cues on planning and suggest that even subtle changes in\nstructure can guide the selection of planning strategies.",
        "Groove sensations arise from rhythmic structures that evoke an urge to move\nin response to music. While syncopation has been extensively studied in groove\nperception, the neural mechanisms underlying low-frequency groove remain\nunderexplored. This fMRI study examines the role of the mirror neuron system\nand associated brain regions in processing low-frequency groove.\nRegion-of-interest analysis revealed that amplifying drum and bass components\nin K-pop songs significantly increased activity in the right posterior inferior\nfrontal gyrus, right inferior\/superior parietal lobules, left dorsolateral\nprefrontal cortex, and bilateral posterior middle\/inferior temporal gyrus.\nThese findings suggest that low-frequency grooves engage sensorimotor,\nexecutive, and rhythm semantics networks, reinforcing their role in\naction-related processing. Building on these insights, we propose an enhanced\nrhythmic auditory stimulation paradigm for Parkinson's disease, incorporating\namplified low-frequency rhythmic cues to improve gait synchronization.",
        "Laboratory-grown, engineered living neuronal networks in vitro have emerged\nin the last years as an experimental technique to understand the collective\nbehavior of neuronal assemblies in relation to their underlying connectivity.\nAn inherent obstacle in the design of such engineered systems is the difficulty\nto predict the dynamic repertoire of the emerging network and its dependence on\nexperimental variables. To fill this gap, and inspired on recent experimental\nstudies, here we present a numerical model that aims at, first, replicating the\nanisotropies in connectivity imprinted through engineering, to next realize the\ncollective behavior of the neuronal network and make predictions. We use\nexperimentally measured, biologically-realistic data combined with the\nIzhikevich model to quantify the dynamics of the neuronal network in relation\nto tunable structural and dynamical parameters. These parameters include the\nsynaptic noise, strength of the imprinted anisotropies, and average axon\nlengths. The latter are involved in the simulation of the development of\nneurons in vitro. We show that the model captures the behavior of engineered\nneuronal cultures, in which a rich repertoire of activity patterns emerge but\nwhose details are strongly dependent on connectivity details and noise. Results\nalso show that the presence of connectivity anisotropies substantially improves\nthe capacity of reconstructing structural connectivity from activity data, an\naspect that is important in the quest for understanding the\nstructure-to-function relationship in neuronal networks. Our work provides the\nin silico basis to assist experimentalists in the design of laboratory in vitro\nnetworks and anticipate their outcome, an aspect that is particularly important\nin the effort to conceive reliable brain-on-a-chip circuits and explore key\naspects such as input-output relationships or information coding.",
        "One of the central aims of neuroscience is to reliably predict the behavioral\nresponse of an organism using its neural activity. If possible, this implies we\ncan causally manipulate the neural response and design brain-computer-interface\nsystems to alter behavior, and vice-versa. Hence, predictions play an important\nrole in both fundamental neuroscience and its applications. Can we predict the\nneural and behavioral states of an organism at any given time? Can we predict\nbehavioral states using neural states, and vice-versa, and is there a\nmemory-component required to reliably predict such states? Are the predictions\ncomputable within a given timescale to meaningfully stimulate and make the\nsystem reach the desired states? Through a series of mathematical treatments,\nsuch conjectures and questions are discussed. Answering them might be key for\nfuture developments in understanding intelligence and designing\nbrain-computer-interfaces.",
        "Super-resolution Structured Illumination Microscopy (SR-SIM) enables\nfluorescence microscopy beyond the diffraction limit at high frame rates.\nCompared to other super-resolution microscopy techniques, the low photon\nfluence used in SR-SIM makes it readily compatible with live-cell imaging.\nHere, we combine SR-SIM with electro-optic fluorescence lifetime imaging\n(EOFLIM), adding the capability of monitoring physicochemical parameters with\n156 nm spatial resolution at high frame rate for live-cell imaging. We\ndemonstrate that our new SIMFLIM technique enables super-resolved multiplexed\nimaging of spectrally overlapping fluorophores, environmental sensing, and\nlive-cell imaging.",
        "The Lee-Yang circle theorem revolutionized our understanding of phase\ntransitions in ferromagnetic systems by showing that the complex zeros of\npartition functions lie on the unit circle, with criticality arising as these\nzeros approach the real axis in the thermodynamic limit. However, in frustrated\nsystems such as antiferromagnets and spin glasses, the zeros deviate from this\nstructure, making it challenging to extend the Lee-Yang theory to disordered\nsystems. In this work, we establish a new circle theorem for two-dimensional\nIsing spin glasses, proving that the square of the partition function exhibits\nzeros densely packed along the unit circle. Numerical simulations on the square\nlattice confirm our theoretical predictions, demonstrating the validity of the\ncircle law for quenched disorder. Furthermore, our results uncover a\nfinite-temperature crossover in $\\pm J$ spin glasses, characterized by the\nemergence of a spectral gap in the angular distribution of zeros. This result\nextends the Lee-Yang framework to disordered systems, offering new insights\ninto spin-glass criticality.",
        "BART (Bayesian additive regression trees) has been established as a leading\nsupervised learning method, particularly in the field of causal inference. This\npaper explores the use of BART models for learning conditional average\ntreatment effects (CATE) from regression discontinuity designs, where treatment\nassignment is based on whether an observed covariate (called the running\nvariable) exceeds a pre-specified threshold. A purpose-built version of BART\nthat uses linear regression leaf models (of the running variable and treatment\nassignment dummy) is shown to out-perform off-the-shelf BART implementations as\nwell as a local polynomial regression approach and a CART-based approach. The\nnew method is evaluated in thorough simulation studies as well as an empirical\napplication looking at the effect of academic probation on student performance.",
        "The recent development of Neural Operator (NeurOp) learning for solutions to\nthe elastic wave equation shows promising results and provides the basis for\nfast large-scale simulations for different seismological applications. In this\npaper, we use the Fourier Neural Operator (FNO) model to directly solve the 3D\nHelmholtz wave equation for fast seismic ground motion simulations on different\nfrequencies, and show the frequency bias of the FNO model, i.e. it learns the\nlower frequencies better comparing to the higher frequencies. To reduce the\nfrequency bias, we adopt the multi-stage FNO training, i.e., after training a\n1st stage FNO model for estimating the ground motion, we use a second FNO model\nas the 2nd stage to learn from the residual, which greatly reduced the errors\non the higher frequencies. By adopting this multi-stage training, the FNO\nmodels show reduced biases on higher frequencies, which enhanced the overall\nresults of the ground motion simulations. Thus the multi-stage training FNO\nimproves the accuracy and realism of the ground motion simulations.",
        "We say that a ring is strongly (resp. weakly) left Jacobson if every\nsemiprime (resp. prime) left ideal is an intersection of maximal left ideals.\nThere exist Jacobson rings that are not weakly left Jacobson, e.g. the Weyl\nalgebra. Our main result is the following one-sided noncommutative\nNullstellensatz: For any finite-dimensional F-algebra A the ring\nA[$x_1$,...,$x_n$] of polynomials with coefficients in A is strongly left\nJacobson and every maximal left ideal of A[$x_1$,...,$x_n$] has a special form.\nWe also prove that an algebra that is a finitely generated module over its\ncenter is weakly left Jacobson iff it is Jacobson, and that an Azumaya algebra\nis strongly left Jacobson iff its center is Jacobson.",
        "Popular safe Bayesian optimization (BO) algorithms learn control policies for\nsafety-critical systems in unknown environments. However, most algorithms make\na smoothness assumption, which is encoded by a known bounded norm in a\nreproducing kernel Hilbert space (RKHS). The RKHS is a potentially\ninfinite-dimensional space, and it remains unclear how to reliably obtain the\nRKHS norm of an unknown function. In this work, we propose a safe BO algorithm\ncapable of estimating the RKHS norm from data. We provide statistical\nguarantees on the RKHS norm estimation, integrate the estimated RKHS norm into\nexisting confidence intervals and show that we retain theoretical guarantees,\nand prove safety of the resulting safe BO algorithm. We apply our algorithm to\nsafely optimize reinforcement learning policies on physics simulators and on a\nreal inverted pendulum, demonstrating improved performance, safety, and\nscalability compared to the state-of-the-art.",
        "The orthogonal time frequency space with index modulation (OTFS-IM) offers\nflexible tradeoffs between spectral efficiency (SE) and bit error rate (BER) in\ndoubly selective fading channels. While OTFS-IM schemes demonstrated such\npotential, a persistent challenge lies in the detection complexity. To address\nthis problem, we propose the hierarchical mode-based index modulation (HMIM).\nHMIM introduces a novel approach to modulate information bits by IM patterns,\nsignificantly simplifying the complexity of maximum a posteriori (MAP)\nestimation with Gaussian noise. Further, we incorporate HMIM with the recently\nproposed orthogonal delay-Doppler division multiplexing (ODDM) modulation,\nnamely ODDM-HMIM, to exploit the full diversity of the delay-Doppler (DD)\nchannel. The BER performance of ODDM-HMIM is analyzed considering a maximum\nlikelihood (ML) detector. Our numerical results reveal that, with the same SE,\nHMIM can outperform conventional IM in terms of both BER and computational\ncomplexity. In addition, we propose a successive interference\ncancellation-based minimum mean square error (SIC-MMSE) detector for ODDM-HMIM,\nwhich enables low-complexity detection with large frame sizes.",
        "By applying the Craig-Wayne-Bourgain (CWB) method, we establish the existence\nof periodic response solutions to multi-dimensional nonlinear Schr\\\"{o}dinger\nequations (NLS) with unbounded perturbation.",
        "We present baryon acoustic oscillation (BAO) measurements from more than 14\nmillion galaxies and quasars drawn from the Dark Energy Spectroscopic\nInstrument (DESI) Data Release 2 (DR2), based on three years of operation. For\ncosmology inference, these galaxy measurements are combined with DESI\nLyman-$\\alpha$ forest BAO results presented in a companion paper. The DR2 BAO\nresults are consistent with DESI DR1 and SDSS, and their distance-redshift\nrelationship matches those from recent compilations of supernovae (SNe) over\nthe same redshift range. The results are well described by a flat $\\Lambda$CDM\nmodel, but the parameters preferred by BAO are in mild, $2.3\\sigma$ tension\nwith those determined from the cosmic microwave background (CMB), although the\nDESI results are consistent with the acoustic angular scale $\\theta_*$ that is\nwell-measured by Planck. This tension is alleviated by dark energy with a\ntime-evolving equation of state parametrized by $w_0$ and $w_a$, which provides\na better fit to the data, with a favored solution in the quadrant with $w_0>-1$\nand $w_a<0$. This solution is preferred over $\\Lambda$CDM at $3.1\\sigma$ for\nthe combination of DESI BAO and CMB data. When also including SNe, the\npreference for a dynamical dark energy model over $\\Lambda$CDM ranges from\n$2.8-4.2\\sigma$ depending on which SNe sample is used. We present evidence from\nother data combinations which also favor the same behavior at high\nsignificance. From the combination of DESI and CMB we derive 95% upper limits\non the sum of neutrino masses, finding $\\sum m_\\nu<0.064$ eV assuming\n$\\Lambda$CDM and $\\sum m_\\nu<0.16$ eV in the $w_0w_a$ model. Unless there is an\nunknown systematic error associated with one or more datasets, it is clear that\n$\\Lambda$CDM is being challenged by the combination of DESI BAO with other\nmeasurements and that dynamical dark energy offers a possible solution.",
        "Studying the redshifted 21-cm signal from the the neutral hydrogen during the\nEpoch of Reionization and Cosmic Dawn is fundamental for understanding the\nphysics of the early universe. One of the challenges that 21-cm experiments\nface is the contamination by bright foreground sources, such as Cygnus A, for\nwhich accurate spatial and spectral models are needed to minimise the residual\ncontamination after their removal. In this work, we develop a new,\nhigh-resolution model of Cygnus A using Low Frequency Array (LOFAR)\nobservations in the $110{-}250$ MHz range, improving upon previous models by\nincorporating physical spectral information through the forced-spectrum method\nduring multi-frequency deconvolution. This approach addresses the limitations\nof earlier models by providing a more accurate representation of the complex\nstructure and spectral behaviour of Cygnus A, including the spectral turnover\nin its brightest hotspots. The impact of this new model on the LOFAR 21-cm\nsignal power spectrum is assessed by comparing it with both simulated and\nobserved North Celestial Pole data sets. Significant improvements are observed\nin the cylindrical power spectrum along the Cygnus A direction, highlighting\nthe importance of having spectrally accurate models of the brightest foreground\nsources. However, this improvement is washed out in the spherical power\nspectrum, where we measure differences of a few hundred mK at\n$k<0.63\\,h\\,\\text{cMpc}^{-1}$, but not statistically significant. The results\nsuggest that other systematic effects must be mitigated before a substantial\nimpact on 21-cm power spectrum can be achieved.",
        "This paper investigates the computation of Hawking temperatures for black\nholes within various $f(Q)$ gravity models using the RVB method. This\ntopological approach uncovers an additional term in the temperature\ncalculation, which we propose originates from the residue of a specific contour\nintegral related to the metric or curvature. By examining several specific\n$f(Q)$ models including quadratic, logarithmic, square root, and power law\nmodifications as well as well known black hole solutions such as RN, Kerr, and\nKN black holes, we demonstrate that the correction term can consistently be\ninterpreted as this residue. Our findings provide new insights into black hole\nthermodynamics within modified gravity frameworks.",
        "The concept of bound states in the continuum (BIC) has been advancing light\nconfinement technology in leaky environments. In this letter, we propose and\nnumerically demonstrate a slow light waveguide based on a BIC mode. We\nconsidered a waveguide with a polymer core loaded on a plane slab, which\nsupports a leaky guided mode coupled to the radiation continuum in the slab. We\nfound that periodic modulation of the polymer core along the propagation\ndirection can result in a high group index mode with a low propagation loss due\nto BIC confinement. The introduction of one-dimensional photonic crystals into\nthe BIC waveguides will largely expand its functionality and applications in\nintegrated photonics.",
        "In this paper we explain how to attach to a family of $p$-adic\nrepresentations of a product of Galois groups an overconvergent family of\nmultivariable $(\\varphi,\\Gamma)$-modules, generalizing results from Pal-Zabradi\nand Carter-Kedlaya-Zabradi, using Colmez-Sen-Tate descent. We also define rings\nof multivariable crystalline and semistable periods, and explain how to recover\nthis multivariable $p$-adic theory attached to a family of representations from\nits multivariable $(\\varphi,\\Gamma)$-module. We also explain how our framework\nallows us to recover the main results of Brinon-Chiarellotto-Mazzari on\nmultivariable $p$-adic Galois representations.",
        "We introduce a refinement to the convex split lemma by replacing the max\nmutual information with the collision mutual information, transforming the\ninequality into an equality. This refinement yields tighter achievability\nbounds for quantum source coding tasks, including state merging and state\nsplitting. Furthermore, we derive a universal upper bound on the smoothed max\nmutual information, where \"universal\" signifies that the bound depends\nexclusively on R\\'enyi entropies and is independent of the system's dimensions.\nThis result has significant implications for quantum information processing,\nparticularly in applications such as the reverse quantum Shannon theorem.",
        "Ultra-lean premixed hydrogen combustion is a possible solution to decarbonize\nindustry, while limiting flame temperatures and thus nitrous oxide emissions.\nThese lean hydrogen\/air flames experience strong preferential diffusion\neffects, which result in thermo-diffusive (TD) instabilities. To efficiently\nand accurately model lean premixed hydrogen flames, it is crucial to\nincorporate these preferential diffusion effects into flamelet tabulated\nchemistry frameworks, such as the Flamelet-Generated Manifold (FGM) method.\nThis is challenging because the preferential diffusion terms in the control\nvariable transport equations contain diffusion fluxes of all species in the\nmechanism. In this work, a new implementation is presented; the full term is\nreduced by only considering the most contributing species. When carefully\nselecting this set of major species, preferential diffusion fluxes along the\nflame front, i.e., cross-diffusion, can be captured. This is particularly\nimportant for manifolds that include heat loss effects, where enthalpy is one\nof the control variables. The diffusion of the H-radical has a significant\ncontribution to the enthalpy transport equation, and cross-diffusion of the\nH-radical is non-negligible. Two manifolds, without and with heat loss effects,\nand the set of major species are analyzed in an a-priori and a-posteriori\nmanner. Simulations of TD unstable hydrogen-air flames with detailed chemistry\nand several FGM models show that accurately capturing cross-diffusion of\nenthalpy is important for correctly predicting the flame shape and dynamics."
      ]
    }
  },
  {
    "id":2411.0675,
    "research_type":"applied",
    "start_id":"b13",
    "start_title":"Symmetric diffeomorphic image registration with crosscorrelation: evaluating automated labeling of elderly and neurodegenerative brain",
    "start_abstract":"One of the most challenging problems in modern neuroimaging is detailed characterization of neurodegeneration. Quantifying spatial and longitudinal atrophy patterns is an important component of this process. These spatiotemporal signals will aid in discriminating between related diseases, such as frontotemporal dementia (FTD) and Alzheimer's disease (AD), which manifest themselves in the same at-risk population. Here, we develop a novel symmetric image normalization method (SyN) for maximizing the cross-correlation within the space of diffeomorphic maps and provide the Euler-Lagrange equations necessary for this optimization. We then turn to a careful evaluation of our method. Our evaluation uses gold standard, human cortical segmentation to contrast SyN's performance with a related elastic method and with the standard ITK implementation of Thirion's Demons algorithm. The new method compares favorably with both approaches, in particular when the distance between the template brain and the target brain is large. We then report the correlation of volumes gained by algorithmic cortical labelings of FTD and control subjects with those gained by the manual rater. This comparison shows that, of the three methods tested, SyN's volume measurements are the most strongly correlated with volume measurements gained by expert labeling. This study indicates that SyN, with cross-correlation, is a reliable method for normalizing and making anatomical measurements in volumetric MRI of patients and at-risk elderly individuals.",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b3",
        "b21"
      ],
      "title":[
        "3-D ultrasound imaging: a review",
        "Generative AI for Medical Imaging: extending the MONAI Framework"
      ],
      "abstract":[
        "The development of 3-D ultrasound imaging is a way to address the disadvantages conventional imaging. In this article authors review approaches that have been attempted in such as B-mode, color Doppler, and power Doppler systems. Acquisition, reconstruction, rendering techniques for are discussed, well applications limitations.",
        "Recent advances in generative AI have brought incredible breakthroughs several areas, including medical imaging. These models tremendous potential not only to help safely share data via synthetic datasets but also perform an array of diverse applications, such as anomaly detection, image-to-image translation, denoising, and MRI reconstruction. However, due the complexity these models, their implementation reproducibility can be difficult. This hinder progress, act a use barrier, dissuade comparison new methods with existing works. In this study, we present MONAI Generative Models, freely available open-source platform that allows researchers developers easily train, evaluate, deploy related applications. Our reproduces state-of-art studies standardised way involving different architectures (such diffusion autoregressive transformers, GANs), provides pre-trained for community. We implemented generalisable fashion, illustrating results extended 2D or 3D scenarios, images modalities (like CT, MRI, X-Ray data) from anatomical areas. Finally, adopt modular extensible approach, ensuring long-term maintainability extension current applications future features."
      ],
      "categories":[
        "cs.CV",
        "eess.IV"
      ]
    },
    "list":{
      "title":[
        "Revealing Microscopic Objects in Fluorescence Live Imaging by\n  Video-to-video Translation Based on A Spatial-temporal Generative Adversarial\n  Network",
        "GC-ConsFlow: Leveraging Optical Flow Residuals and Global Context for\n  Robust Deepfake Detection",
        "Training-Free Motion-Guided Video Generation with Enhanced Temporal\n  Consistency Using Motion Consistency Loss",
        "Task-Agnostic Guided Feature Expansion for Class-Incremental Learning",
        "$\\mathbf{\\Phi}$-GAN: Physics-Inspired GAN for Generating SAR Images\n  Under Limited Data",
        "DocTTT: Test-Time Training for Handwritten Document Recognition Using\n  Meta-Auxiliary Learning",
        "GCE-Pose: Global Context Enhancement for Category-level Object Pose\n  Estimation",
        "Light4GS: Lightweight Compact 4D Gaussian Splatting Generation via\n  Context Model",
        "Multi-Layer Visual Feature Fusion in Multimodal LLMs: Methods, Analysis,\n  and Best Practices",
        "Multi-dataset synergistic in supervised learning to pre-label structural\n  components in point clouds from shell construction scenes",
        "Adversarial Robustness of Discriminative Self-Supervised Learning in\n  Vision",
        "Probabilistic Prompt Distribution Learning for Animal Pose Estimation",
        "General mean-field stochastic linear quadratic control problem driven by\n  L\\'evy processes with random coefficients",
        "Slowly decaying strain solitons in nonlinear viscoelastic waveguides",
        "AI-Invented Tonal Languages: Preventing a Machine Lingua Franca Beyond\n  Human Understanding",
        "Bounded Synthesis of Synchronized Distributed Models from Lightweight\n  Specifications",
        "EEG-CLIP : Learning EEG representations from natural language\n  descriptions",
        "Please, do tell",
        "Hot-carrier thermal breakdown and S-type current-voltage characteristics\n  in perforated graphene structures",
        "Goal-oriented Transmission Scheduling: Structure-guided DRL with a\n  Unified Dual On-policy and Off-policy Approach",
        "Optically Detected Magnetic Resonance Imaging and Sensing Within\n  Functionalized Additively Manufactured Microporous Structures",
        "Craw4LLM: Efficient Web Crawling for LLM Pretraining",
        "High-Accuracy ECG Image Interpretation using Parameter-Efficient LoRA\n  Fine-Tuning with Multimodal LLaMA 3.2",
        "Modeling of Rumor Propagation in Large Populations with Network via\n  Graphon Games",
        "On the Chermak-Delgado lattice of a finite group",
        "OfficeMate: Pilot Evaluation of an Office Assistant Robot",
        "MetaOpenFOAM 2.0: Large Language Model Driven Chain of Thought for\n  Automating CFD Simulation and Post-Processing"
      ],
      "abstract":[
        "In spite of being a valuable tool to simultaneously visualize multiple types\nof subcellular structures using spectrally distinct fluorescent labels, a\nstandard fluoresce microscope is only able to identify a few microscopic\nobjects; such a limit is largely imposed by the number of fluorescent labels\navailable to the sample. In order to simultaneously visualize more objects, in\nthis paper, we propose to use video-to-video translation that mimics the\ndevelopment process of microscopic objects. In essence, we use a microscopy\nvideo-to-video translation framework namely Spatial-temporal Generative\nAdversarial Network (STGAN) to reveal the spatial and temporal relationships\nbetween the microscopic objects, after which a microscopy video of one object\ncan be translated to another object in a different domain. The experimental\nresults confirm that the proposed STGAN is effective in microscopy\nvideo-to-video translation that mitigates the spectral conflicts caused by the\nlimited fluorescent labels, allowing multiple microscopic objects be\nsimultaneously visualized.",
        "The rapid development of Deepfake technology has enabled the generation of\nhighly realistic manipulated videos, posing severe social and ethical\nchallenges. Existing Deepfake detection methods primarily focused on either\nspatial or temporal inconsistencies, often neglecting the interplay between the\ntwo or suffering from interference caused by natural facial motions. To address\nthese challenges, we propose the global context consistency flow (GC-ConsFlow),\na novel dual-stream framework that effectively integrates spatial and temporal\nfeatures for robust Deepfake detection. The global grouped context aggregation\nmodule (GGCA), integrated into the global context-aware frame flow stream\n(GCAF), enhances spatial feature extraction by aggregating grouped global\ncontext information, enabling the detection of subtle, spatial artifacts within\nframes. The flow-gradient temporal consistency stream (FGTC), rather than\ndirectly modeling the residuals, it is used to improve the robustness of\ntemporal feature extraction against the inconsistency introduced by unnatural\nfacial motion using optical flow residuals and gradient-based features. By\ncombining these two streams, GC-ConsFlow demonstrates the effectiveness and\nrobustness in capturing complementary spatiotemporal forgery traces. Extensive\nexperiments show that GC-ConsFlow outperforms existing state-of-the-art methods\nin detecting Deepfake videos under various compression scenarios.",
        "In this paper, we address the challenge of generating temporally consistent\nvideos with motion guidance. While many existing methods depend on additional\ncontrol modules or inference-time fine-tuning, recent studies suggest that\neffective motion guidance is achievable without altering the model architecture\nor requiring extra training. Such approaches offer promising compatibility with\nvarious video generation foundation models. However, existing training-free\nmethods often struggle to maintain consistent temporal coherence across frames\nor to follow guided motion accurately. In this work, we propose a simple yet\neffective solution that combines an initial-noise-based approach with a novel\nmotion consistency loss, the latter being our key innovation. Specifically, we\ncapture the inter-frame feature correlation patterns of intermediate features\nfrom a video diffusion model to represent the motion pattern of the reference\nvideo. We then design a motion consistency loss to maintain similar feature\ncorrelation patterns in the generated video, using the gradient of this loss in\nthe latent space to guide the generation process for precise motion control.\nThis approach improves temporal consistency across various motion control tasks\nwhile preserving the benefits of a training-free setup. Extensive experiments\nshow that our method sets a new standard for efficient, temporally coherent\nvideo generation.",
        "The ability to learn new concepts while preserve the learned knowledge is\ndesirable for learning systems in Class-Incremental Learning (CIL). Recently,\nfeature expansion of the model become a prevalent solution for CIL, where the\nold features are fixed during the training of the new task while new features\nare expanded for the new tasks. However, such task-specific features learned\nfrom the new task may collide with the old features, leading to\nmisclassification between tasks. Therefore, the expanded model is often\nencouraged to capture diverse features from the new task, aiming to avoid such\ncollision. However, the existing solution is largely restricted to the samples\nfrom the current task, because of the poor accessibility to previous samples.\nTo promote the learning and transferring of diverse features across tasks, we\npropose a framework called Task-Agnostic Guided Feature Expansion (TagFex).\nFirstly, it captures task-agnostic features continually with a separate model,\nproviding extra task-agnostic features for subsequent tasks. Secondly, to\nobtain useful features from the task-agnostic model for the current task, it\naggregates the task-agnostic features with the task-specific feature using a\nmerge attention. Then the aggregated feature is transferred back into the\ntask-specific feature for inference, helping the task-specific model capture\ndiverse features. Extensive experiments show the effectiveness and superiority\nof TagFex on various CIL settings. Code is available at\nhttps:\/\/github.com\/bwnzheng\/TagFex_CVPR2025.",
        "Approaches for improving generative adversarial networks (GANs) training\nunder a few samples have been explored for natural images. However, these\nmethods have limited effectiveness for synthetic aperture radar (SAR) images,\nas they do not account for the unique electromagnetic scattering properties of\nSAR. To remedy this, we propose a physics-inspired regularization method dubbed\n$\\Phi$-GAN, which incorporates the ideal point scattering center (PSC) model of\nSAR with two physical consistency losses. The PSC model approximates SAR\ntargets using physical parameters, ensuring that $\\Phi$-GAN generates SAR\nimages consistent with real physical properties while preventing discriminator\noverfitting by focusing on PSC-based decision cues. To embed the PSC model into\nGANs for end-to-end training, we introduce a physics-inspired neural module\ncapable of estimating the physical parameters of SAR targets efficiently. This\nmodule retains the interpretability of the physical model and can be trained\nwith limited data. We propose two physical loss functions: one for the\ngenerator, guiding it to produce SAR images with physical parameters consistent\nwith real ones, and one for the discriminator, enhancing its robustness by\nbasing decisions on PSC attributes. We evaluate $\\Phi$-GAN across several\nconditional GAN (cGAN) models, demonstrating state-of-the-art performance in\ndata-scarce scenarios on three SAR image datasets.",
        "Despite recent significant advancements in Handwritten Document Recognition\n(HDR), the efficient and accurate recognition of text against complex\nbackgrounds, diverse handwriting styles, and varying document layouts remains a\npractical challenge. Moreover, this issue is seldom addressed in academic\nresearch, particularly in scenarios with minimal annotated data available. In\nthis paper, we introduce the DocTTT framework to address these challenges. The\nkey innovation of our approach is that it uses test-time training to adapt the\nmodel to each specific input during testing. We propose a novel Meta-Auxiliary\nlearning approach that combines Meta-learning and self-supervised Masked\nAutoencoder~(MAE). During testing, we adapt the visual representation\nparameters using a self-supervised MAE loss. During training, we learn the\nmodel parameters using a meta-learning framework, so that the model parameters\nare learned to adapt to a new input effectively. Experimental results show that\nour proposed method significantly outperforms existing state-of-the-art\napproaches on benchmark datasets.",
        "A key challenge in model-free category-level pose estimation is the\nextraction of contextual object features that generalize across varying\ninstances within a specific category. Recent approaches leverage foundational\nfeatures to capture semantic and geometry cues from data. However, these\napproaches fail under partial visibility. We overcome this with a\nfirst-complete-then-aggregate strategy for feature extraction utilizing class\npriors. In this paper, we present GCE-Pose, a method that enhances pose\nestimation for novel instances by integrating category-level global context\nprior. GCE-Pose performs semantic shape reconstruction with a proposed Semantic\nShape Reconstruction (SSR) module. Given an unseen partial RGB-D object\ninstance, our SSR module reconstructs the instance's global geometry and\nsemantics by deforming category-specific 3D semantic prototypes through a\nlearned deep Linear Shape Model. We further introduce a Global Context Enhanced\n(GCE) feature fusion module that effectively fuses features from partial RGB-D\nobservations and the reconstructed global context. Extensive experiments\nvalidate the impact of our global context prior and the effectiveness of the\nGCE fusion module, demonstrating that GCE-Pose significantly outperforms\nexisting methods on challenging real-world datasets HouseCat6D and\nNOCS-REAL275. Our project page is available at\nhttps:\/\/colin-de.github.io\/GCE-Pose\/.",
        "3D Gaussian Splatting (3DGS) has emerged as an efficient and high-fidelity\nparadigm for novel view synthesis. To adapt 3DGS for dynamic content,\ndeformable 3DGS incorporates temporally deformable primitives with learnable\nlatent embeddings to capture complex motions. Despite its impressive\nperformance, the high-dimensional embeddings and vast number of primitives lead\nto substantial storage requirements. In this paper, we introduce a\n\\textbf{Light}weight \\textbf{4}D\\textbf{GS} framework, called Light4GS, that\nemploys significance pruning with a deep context model to provide a lightweight\nstorage-efficient dynamic 3DGS representation. The proposed Light4GS is based\non 4DGS that is a typical representation of deformable 3DGS. Specifically, our\nframework is built upon two core components: (1) a spatio-temporal significance\npruning strategy that eliminates over 64\\% of the deformable primitives,\nfollowed by an entropy-constrained spherical harmonics compression applied to\nthe remainder; and (2) a deep context model that integrates intra- and\ninter-prediction with hyperprior into a coarse-to-fine context structure to\nenable efficient multiscale latent embedding compression. Our approach achieves\nover 120x compression and increases rendering FPS up to 20\\% compared to the\nbaseline 4DGS, and also superior to frame-wise state-of-the-art 3DGS\ncompression methods, revealing the effectiveness of our Light4GS in terms of\nboth intra- and inter-prediction methods without sacrificing rendering quality.",
        "Multimodal Large Language Models (MLLMs) have made significant advancements\nin recent years, with visual features playing an increasingly critical role in\nenhancing model performance. However, the integration of multi-layer visual\nfeatures in MLLMs remains underexplored, particularly with regard to optimal\nlayer selection and fusion strategies. Existing methods often rely on arbitrary\ndesign choices, leading to suboptimal outcomes. In this paper, we\nsystematically investigate two core aspects of multi-layer visual feature\nfusion: (1) selecting the most effective visual layers and (2) identifying the\nbest fusion approach with the language model. Our experiments reveal that while\ncombining visual features from multiple stages improves generalization,\nincorporating additional features from the same stage typically leads to\ndiminished performance. Furthermore, we find that direct fusion of multi-layer\nvisual features at the input stage consistently yields superior and more stable\nperformance across various configurations. We make all our code publicly\navailable: https:\/\/github.com\/EIT-NLP\/Layer_Select_Fuse_for_MLLM.",
        "The significant effort required to annotate data for new training datasets\nhinders computer vision research and machine learning in the construction\nindustry. This work explores adapting standard datasets and the latest\ntransformer model architectures for point cloud semantic segmentation in the\ncontext of shell construction sites. Unlike common approaches focused on object\nsegmentation of building interiors and furniture, this study addressed the\nchallenges of segmenting complex structural components in Architecture,\nEngineering, and Construction (AEC). We establish a baseline through supervised\ntraining and a custom validation dataset, evaluate the cross-domain inference\nwith large-scale indoor datasets, and utilize transfer learning to maximize\nsegmentation performance with minimal new data. The findings indicate that with\nminimal fine-tuning, pre-trained transformer architectures offer an effective\nstrategy for building component segmentation. Our results are promising for\nautomating the annotation of new, previously unseen data when creating larger\ntraining resources and for the segmentation of frequently recurring objects.",
        "Self-supervised learning (SSL) has advanced significantly in visual\nrepresentation learning, yet comprehensive evaluations of its adversarial\nrobustness remain limited. In this study, we evaluate the adversarial\nrobustness of seven discriminative self-supervised models and one supervised\nmodel across diverse tasks, including ImageNet classification, transfer\nlearning, segmentation, and detection. Our findings suggest that discriminative\nSSL models generally exhibit better robustness to adversarial attacks compared\nto their supervised counterpart on ImageNet, with this advantage extending to\ntransfer learning when using linear evaluation. However, when fine-tuning is\napplied, the robustness gap between SSL and supervised models narrows\nconsiderably. Similarly, this robustness advantage diminishes in segmentation\nand detection tasks. We also investigate how various factors might influence\nadversarial robustness, including architectural choices, training duration,\ndata augmentations, and batch sizes. Our analysis contributes to the ongoing\nexploration of adversarial robustness in visual self-supervised representation\nsystems.",
        "Multi-species animal pose estimation has emerged as a challenging yet\ncritical task, hindered by substantial visual diversity and uncertainty. This\npaper challenges the problem by efficient prompt learning for Vision-Language\nPretrained (VLP) models, \\textit{e.g.} CLIP, aiming to resolve the\ncross-species generalization problem. At the core of the solution lies in the\nprompt designing, probabilistic prompt modeling and cross-modal adaptation,\nthereby enabling prompts to compensate for cross-modal information and\neffectively overcome large data variances under unbalanced data distribution.\nTo this end, we propose a novel probabilistic prompting approach to fully\nexplore textual descriptions, which could alleviate the diversity issues caused\nby long-tail property and increase the adaptability of prompts on unseen\ncategory instance. Specifically, we first introduce a set of learnable prompts\nand propose a diversity loss to maintain distinctiveness among prompts, thus\nrepresenting diverse image attributes. Diverse textual probabilistic\nrepresentations are sampled and used as the guidance for the pose estimation.\nSubsequently, we explore three different cross-modal fusion strategies at\nspatial level to alleviate the adverse impacts of visual uncertainty. Extensive\nexperiments on multi-species animal pose benchmarks show that our method\nachieves the state-of-the-art performance under both supervised and zero-shot\nsettings. The code is available at https:\/\/github.com\/Raojiyong\/PPAP.",
        "This paper studies a stochastic mean-field linear-quadratic optimal control\nproblem with random coefficients. The state equation is a general linear\nstochastic differential equation with mean-field terms $\\EE X(t)$ and $\\EE\nu(t)$ of the state and the control processes and is driven by a Brownian motion\nand a Poisson random measure. By the coupled system of Riccati equations, an\nexplicit expressions for the optimal state feedback control is obtained. As a\nby-product, the non-homogeneous stochastic linear-quadratic control problem\nwith random coefficients and L\\'evy driving noises is also studied.",
        "This paper is devoted to the modeling of longitudinal strain waves in a rod\ncomposed of a nonlinear viscoelastic material characterized by\nfrequency-dependent second- and third-order elastic constants. We demonstrate\nthat long waves in such a material can be effectively described by a damped\nBoussinesq-type equation for the longitudinal strain, incorporating dissipation\nthrough retarded operators. Using the existing theory of solitary wave\nsolutions in nearly integrable systems, we derive a slowly-decaying strain\nsoliton solution to this equation. The derived soliton characteristics are\nshown to be in a good agreement with results from full 3D simulations. We\ndemonstrate the importance of taking into account the frequency dependence of\nthird-order elastic constants for the description of strain solitons.",
        "This paper investigates the potential for large language models (LLMs) to\ndevelop private tonal languages for machine-to-machine (M2M) communication.\nInspired by cryptophasia in human twins (affecting up to 50% of twin births)\nand natural tonal languages like Mandarin and Vietnamese, we implement a\nprecise character-to-frequency mapping system that encodes the full ASCII\ncharacter set (32-126) using musical semitones. Each character is assigned a\nunique frequency, creating a logarithmic progression beginning with space (220\nHz) and ending with tilde (50,175.42 Hz). This spans approximately 7.9 octaves,\nwith higher characters deliberately mapped to ultrasonic frequencies beyond\nhuman perception (>20 kHz). Our implemented software prototype demonstrates\nthis encoding through visualization, auditory playback, and ABC musical\nnotation, allowing for analysis of information density and transmission speed.\nTesting reveals that tonal encoding can achieve information rates exceeding\nhuman speech while operating partially outside human perceptual boundaries.\nThis work responds directly to concerns about AI systems catastrophically\ndeveloping private languages within the next five years, providing a concrete\nprototype software example of how such communication might function and the\ntechnical foundation required for its emergence, detection, and governance.",
        "We present an approach to automatically synthesize synchronized models from\nlightweight formal specifications. Our approach takes as input a specification\nof a distributed system along with a global linear time constraint, which must\nbe fulfilled by the interaction of the system's components. It produces\nexecutable models for the component specifications (in the style of Promela\nlanguage) whose concurrent execution satisfies the global constraint. The\ncomponent specifications consist of a collection of actions described by means\nof pre and post conditions together with first-order relational formulas\nprescribing their behavior. We use the Alloy Analyzer to encode the component\nspecifications and enumerate their potential implementations up to some bound,\nwhose concurrent composition is model checked against the global property. Even\nthough this approach is sound and complete up to the selected bound, it is\nimpractical as the number of candidate implementations grows exponentially. To\naddress this, we propose an algorithm that uses batches of counterexamples to\nprune the solution space, it has two main phases: exploration, the algorithm\ncollects a batch of counterexamples, and exploitation, where this knowledge is\nused to speed up the search. The approach is sound, while its completeness\ndepends on the batches used. We present a prototype tool, describe some\nexperiments, and compare it with related approaches.",
        "Deep networks for electroencephalogram (EEG) decoding are currently often\ntrained to only solve a specific task like pathology or gender decoding. A more\ngeneral approach leveraging the medical reports of clinical EEG recordings is\nto learn mappings between medical reports and EEG recordings. This approach was\npioneered in the computer vision domain matching images and their text captions\nand subsequently allowed to do successful zero-shot decoding using textual\nclass prompts. In this work, we follow this approach and develop a contrastive\nlearning framework EEG-CLIP that aligns EEG time series and their corresponding\nclinical text descriptions in a shared embedding space. We investigate its\npotential for versatile EEG decoding, assessing performance on a range of\nfew-shot and zero-shot settings. Overall, results show that EEG-CLIP manages to\nnontrivially align text and EEG representations. Our work presents a promising\napproach to learn general EEG representations, which could enable easier\nanalyses of diverse decoding questions through zero shot decoding or training\ntask-specific models from fewer training examples. The code for reproducing our\nresults is available at https:\/\/github.com\/tidiane-camaret\/EEGClip.",
        "\"Math is not a spectator sport.\" \"Lecturing is educational malpractice.\"\nSlogans like these rally some mathematicians to teach classes that feature\n\"active learning\", where lecturing is eschewed for student participation. Yet\nas much as I believe that students must do math to learn math, I also find\nblanket statements to be more about bandwagons than considered reflection on\nteaching. In this column, published in the Fall 2021 AWM Newsletter, I urge us\nto think through the math we offer students and how we set up students to\nlearn. Although I draw primarily from my experiences teaching proofs in\nabstract algebra and real analysis, the scenarios extend to other topics in\nfirst year undergraduate education and beyond.",
        "We investigate the carrier transport characteristics of perforated graphene\nlayer (PGL) composed of arrays of interdigital coplanar graphene microribbons\n(GMRs) connected by graphene nanoribbon (GNR) bridges. We analyze their\noperation at room-temperature. Under an applied bias voltage, two-dimensional\nelectron and hole systems (2DES and 2DHS) form in adjacent GMRs. The terminal\ncurrent in these PGL structures is primarily governed by thermionic transport\nacross the GNR bridges. As electrons and holes traverse the GNRs, they induce\nheating in the 2DES and 2DHS, creating a positive feedback loop between carrier\nheating and thermionic emission. This phenomenon, characterized as hot-carrier\nthermal breakdown, can give rise to S-shaped inter-GMR current-voltage\ncharacteristics. These unique transport properties make PGLs promising\ncandidates for fast, voltage-controlled room-temperature switches and\nelectromagnetic radiation detectors.",
        "Goal-oriented communications prioritize application-driven objectives over\ndata accuracy, enabling intelligent next-generation wireless systems. Efficient\nscheduling in multi-device, multi-channel systems poses significant challenges\ndue to high-dimensional state and action spaces. We address these challenges by\nderiving key structural properties of the optimal solution to the goal-oriented\nscheduling problem, incorporating Age of Information (AoI) and channel states.\nSpecifically, we establish the monotonicity of the optimal state value function\n(a measure of long-term system performance) w.r.t. channel states and prove its\nasymptotic convexity w.r.t. AoI states. Additionally, we derive the\nmonotonicity of the optimal policy w.r.t. channel states, advancing the\ntheoretical framework for optimal scheduling. Leveraging these insights, we\npropose the structure-guided unified dual on-off policy DRL (SUDO-DRL), a\nhybrid algorithm that combines the stability of on-policy training with the\nsample efficiency of off-policy methods. Through a novel structural property\nevaluation framework, SUDO-DRL enables effective and scalable training,\naddressing the complexities of large-scale systems. Numerical results show\nSUDO-DRL improves system performance by up to 45% and reduces convergence time\nby 40% compared to state-of-the-art methods. It also effectively handles\nscheduling in much larger systems, where off-policy DRL fails and on-policy\nbenchmarks exhibit significant performance loss, demonstrating its scalability\nand efficacy in goal-oriented communications.",
        "Quantum sensing with nitrogen-vacancy centers in diamond has emerged as a\npowerful tool for measuring diverse physical parameters, yet the versatility of\nthese measurement approaches is often limited by the achievable layout and\ndimensionality of bulk-crystal platforms. Here, we demonstrate a versatile\napproach to creating designer quantum sensors by surface-functionalizing\nmultiphoton lithography microstructures with NV-containing nanodiamonds. We\nshowcase this capability by fabricating a 150 $\\mu$m x 150 $\\mu$m x 150 $\\mu$m\ntriply periodic minimal surface gyroid structure with millions of attached\nnanodiamonds. We demonstrate a means to volumetrically image these structures\nusing a refractive index matching confocal imaging technique, and extract ODMR\nspectra from 1.86 $\\mu$m x 1.86 $\\mu$m areas of highly concentrated\nnanodiamonds across a cross section of the gyroid. Furthermore, the high\ndensity of sensing elements enables ensemble temperature measurements with\nsensitivity of 0.548 {\\deg}K\/$\\sqrt{Hz}$ at 5 mW excitation power. This\napproach to creating quantum-enabled microarchitectures opens new possibilities\nfor multimodal sensing in complex three-dimensional environments.",
        "Web crawl is a main source of large language models' (LLMs) pretraining data,\nbut the majority of crawled web pages are discarded in pretraining due to low\ndata quality. This paper presents Craw4LLM, an efficient web crawling method\nthat explores the web graph based on the preference of LLM pretraining.\nSpecifically, it leverages the influence of a webpage in LLM pretraining as the\npriority score of the web crawler's scheduler, replacing the standard graph\nconnectivity based priority. Our experiments on a web graph containing 900\nmillion webpages from a commercial search engine's index demonstrate the\nefficiency of Craw4LLM in obtaining high-quality pretraining data. With just\n21% URLs crawled, LLMs pretrained on Craw4LLM data reach the same downstream\nperformances of previous crawls, significantly reducing the crawling waste and\nalleviating the burdens on websites. Our code is publicly available at\nhttps:\/\/github.com\/cxcscmu\/Craw4LLM.",
        "Electrocardiogram (ECG) interpretation is a cornerstone of cardiac\ndiagnostics. This paper explores a practical approach to enhance ECG image\ninterpretation using the multimodal LLaMA 3.2 model. We used a\nparameter-efficient fine-tuning strategy, Low-Rank Adaptation (LoRA),\nspecifically designed to boost the model's ability to understand ECG images and\nachieve better outcomes across a wide range of cardiac conditions. Our method\nis tailored for ECG analysis and leverages ECGInstruct, a large-scale\ninstruction dataset with 1 Million samples. This dataset is a rich collection\nof synthesized ECG images, generated from raw ECG data from trusted open-source\nrepositories like MIMIC-IV ECG and PTB-XL. Each ECG image in ECGInstruct comes\nwith expert-written questions and detailed answers, covering diverse ECG\ninterpretation scenarios, including complex cardiac conditions like Myocardial\nInfarction and Conduction Disturbances. Our fine-tuning approach efficiently\nadapts the LLaMA 3.2 model (built upon LLaMA 3) by integrating low-rank\nadaptation techniques, focusing on efficiency by updating only a small set of\nparameters, specifically ignoring the `lm_head` and `embed_tokens` layers. This\npaper details the model setup, our efficient fine-tuning method, and\nimplementation specifics. We provide a thorough evaluation through extensive\nexperiments, demonstrating the effectiveness of our method across various ECG\ninterpretation tasks. The results convincingly show that our\nparameter-efficient LoRA fine-tuning achieves excellent performance in ECG\nimage interpretation, significantly outperforming baseline models and reaching\naccuracy comparable to or exceeding traditional CNN-based methods in\nidentifying a wide range of cardiac abnormalities, including over 70 conditions\nfrom the PTB-XL dataset.",
        "In this paper, we propose a graphon game model to understand how rumor (such\nas fake news) propagates in large populations that are interacting on a network\nand how different policies affect the spread. We extend the SKIR model that is\nused to model rumor propagation and implement individual controls and weighted\ninteractions with other agents to have controlled dynamics. The agents aim to\nminimize their own expected costs non-cooperatively. We give the finite player\ngame model and the limiting graphon game model to approximate the Nash\nequilibrium in the population. We give the graphon game Nash equilibrium as a\nsolution to a continuum of ordinary differential equations (ODEs) and give\nexistence results. Finally, we give a numerical approach and analyze examples\nwhere we use piecewise constant graphon.",
        "By imposing conditions upon the index of a self-centralizing subgroup of a\ngroup, and upon the index of the center of the group, we are able to classify\nthe Chermak-Delgado lattice of the group. This is our main result. We use this\nresult to classify the Chermak-Delgado lattices of dicyclic groups and of\nmetabelian $p$-groups of maximal class.",
        "Office Assistant Robots (OARs) offer a promising solution to proactively\nprovide in-situ support to enhance employee well-being and productivity in\noffice spaces. We introduce OfficeMate, a social OAR designed to assist with\npractical tasks, foster social interaction, and promote health and well-being.\nThrough a pilot evaluation with seven participants in an office environment, we\nfound that users see potential in OARs for reducing stress and promoting\nhealthy habits and value the robot's ability to provide companionship and\nphysical activity reminders in the office space. However, concerns regarding\nprivacy, communication, and the robot's interaction timing were also raised.\nThe feedback highlights the need to carefully consider the robot's appearance\nand behaviour to ensure it enhances user experience and aligns with office\nsocial norms. We believe these insights will better inform the development of\nadaptive, intelligent OAR systems for future office space integration.",
        "Computational Fluid Dynamics (CFD) is widely used in aerospace, energy, and\nbiology to model fluid flow, heat transfer, and chemical reactions. While Large\nLanguage Models (LLMs) have transformed various domains, their application in\nCFD remains limited, particularly for complex tasks like post-processing. To\nbridge this gap, we introduce MetaOpenFOAM 2.0, which leverages Chain of\nThought (COT) decomposition and iterative verification to enhance accessibility\nfor non-expert users through natural language inputs. Tested on a new benchmark\ncovering simulation (fluid flow, heat transfer, combustion) and post-processing\n(extraction, visualization), MetaOpenFOAM 2.0 achieved an Executability score\nof 6.3\/7 and a pass rate of 86.9%, significantly outperforming MetaOpenFOAM 1.0\n(2.1\/7, 0%). Additionally, it proved cost-efficient, averaging $0.15 per case.\nAn ablation study confirmed that COT-driven decomposition and iterative\nrefinement substantially improved task performance. Furthermore, scaling laws\nshowed that increasing COT steps enhanced accuracy while raising token usage,\naligning with LLM post-training scaling trends. These results highlight the\ntransformative potential of LLMs in automating CFD workflows for industrial and\nresearch applications. Code is available at\nhttps:\/\/github.com\/Terry-cyx\/MetaOpenFOAM"
      ]
    }
  },
  {
    "id":2411.1742,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Deep learning-based classification of healthy aging controls, mild cognitive impairment and alzheimer's disease using fusion of mri-pet imaging",
    "start_abstract":"Automated detection of dementia stage using multimodal imaging modalities will be helpful for improving the clinical diagnosis. In this study, we develop the Inception-ResNet wrapper model in differentiating the healthy controls (HC), mild cognitive impairment (MCI), and Alzheimer\u2019s disease (AD) using conjoint magnetic resonance imaging (MRI) and positron emission tomography (PET) scans. We use T1-weighted MR and PET images of individuals aged between 42 and 95 years, including HC, MCI and AD patients. We first perform 3D tissue segmentation of MR images after skull striping. The atlas-based segmented MR image tissue is fused with PET image. Then we transform PET images from RGB to HSI color space and apply fusion of MRI with PET images using two-dimensional Fourier and discrete wavelet transform (DWT) and then reconstruct the MR-PET fused image using inverse Fourier and DWT methods. After the fusion of MRI and PET imaging modalities, we used 60 % training, 20 % for validation and the remaining 20 % as a test set using various convolutional neural networks. We found the proposed model as the best classifier with an accuracy of 95.5 %, 94.1 % and 95.9 % in classifying HC vs MCI, MCI vs AD and AD vs HC respectively when compared to the existing methods. We conclude that the proposed deep learning model has potential in automated classification of healthy and dementia stages using combined MRI and PET modalities with good performance.",
    "start_categories":[
      "cs.NE",
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b19"
      ],
      "title":[
        "Bidirectional Mapping of Brain MRI and PET With 3D Reversible GAN for the Diagnosis of Alzheimer\u2019s Disease"
      ],
      "abstract":[
        "<jats:p>Combining multi-modality data for brain disease diagnosis such as Alzheimer\u2019s disease (AD) commonly leads to improved performance than those using a single modality. However, it is still challenging to train a multi-modality model since it is difficult in clinical practice to obtain complete data that includes all modality data. Generally speaking, it is difficult to obtain both magnetic resonance images (MRI) and positron emission tomography (PET) images of a single patient. PET is expensive and requires the injection of radioactive substances into the patient\u2019s body, while MR images are cheaper, safer, and more widely used in practice. Discarding samples without PET data is a common method in previous studies, but the reduction in the number of samples will result in a decrease in model performance. To take advantage of multi-modal complementary information, we first adopt the Reversible Generative Adversarial Network (RevGAN) model to reconstruct the missing data. After that, a 3D convolutional neural network (CNN) classification model with multi-modality input was proposed to perform AD diagnosis. We have evaluated our method on the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) database, and compared the performance of the proposed method with those using state-of-the-art methods. The experimental results show that the structural and functional information of brain tissue can be mapped well and that the image synthesized by our method is close to the real image. In addition, the use of synthetic data is beneficial for the diagnosis and prediction of Alzheimer\u2019s disease, demonstrating the effectiveness of the proposed framework.<\/jats:p>"
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "A 7T fMRI dataset of synthetic images for out-of-distribution modeling\n  of vision",
        "Personal Danger Signals Reprocessing: New Online Group Intervention for\n  Chronic Pain",
        "From Thought to Action: How a Hierarchy of Neural Dynamics Supports\n  Language Production",
        "Dynamical phases of short-term memory mechanisms in RNNs",
        "Stiff-sloppy analysis of brain networks to reveal individual differences\n  in task performance",
        "Neuro-oscillatory models of cortical speech processing",
        "Neural Constraints on Cognitive Experience and Mental Health",
        "Subthreshold moment analysis of neuronal populations driven by\n  synchronous synaptic inputs",
        "From Bedside to Desktop: A Data Protocol for Normative Intracranial EEG\n  and Abnormality Mapping",
        "Towards an integrative approach to the study of brain-environment\n  interactions in human and non-human primate",
        "Phase Alignment Enhances Oscillatory Power in Neural Mass Models\n  Optimized for Class Encoding",
        "How constraints on editing affects cultural evolution",
        "Spaces and sequences in the hippocampus: a homological perspective",
        "2D Surface Brightness Modelling of Large 2MASS Galaxies II: The Role of\n  Classical Bulges and Pseudobulges on Galaxy Scaling Relations and its\n  implication for Supermassive Black Hole Formation",
        "Feasibility of short blocklength Reed-Muller codes for physical layer\n  security in real environment",
        "Compact superconducting vacuum-gap capacitors with low microwave loss\n  and high mechanical coherence for scalable quantum circuits",
        "Probabilistic Shielding for Safe Reinforcement Learning",
        "Bayesian mixture modeling using a mixture of finite mixtures with\n  normalized inverse Gaussian weights",
        "Fitting multivariate Hawkes processes to interval count data with an\n  application to terrorist activity modelling -- a particle Markov chain Monte\n  Carlo approach",
        "Tuning topologically nontrivial states in the BHT-Ni metal organic\n  framework",
        "The R Package WMAP: Tools for Causal Meta-Analysis by Integrating\n  Multiple Observational Studies",
        "Operator-isomorphism pairs and Zak transform methods for the study of\n  Gabor systems",
        "Change Point Detection for Random Objects with Possibly Periodic\n  Behavior",
        "Enhancing energy transport utilising permanent molecular dipoles",
        "Continuum-wise hyperbolicity and periodic points",
        "Limits of nonlinear and dispersive fiber propagation for photonic\n  extreme learning",
        "Enhancement of Superconductivity in WP via Oxide-Assisted Chemical Vapor\n  Transport",
        "Structure factors and quantum geometry in multiband BCS superconductors"
      ],
      "abstract":[
        "Large-scale visual neural datasets such as the Natural Scenes Dataset (NSD)\nare boosting NeuroAI research by enabling computational models of the brain\nwith performances beyond what was possible just a decade ago. However, these\ndatasets lack out-of-distribution (OOD) components, which are crucial for the\ndevelopment of more robust models. Here, we address this limitation by\nreleasing NSD-synthetic, a dataset consisting of 7T fMRI responses from the\neight NSD subjects for 284 carefully controlled synthetic images. We show that\nNSD-synthetic's fMRI responses reliably encode stimulus-related information and\nare OOD with respect to NSD. Furthermore, OOD generalization tests on\nNSD-synthetic reveal differences between models of the brain that are not\ndetected with NSD - specifically, self-supervised deep neural networks better\nexplain neural responses than their task-supervised counterparts. These results\nshowcase how NSD-synthetic enables OOD generalization tests that facilitate the\ndevelopment of more robust models of visual processing, and the formulation of\nmore accurate theories of human vision.",
        "Chronic pain is a significant global health issue, with many patients\nexperiencing persistent pain despite no identifiable organic cause, classified\nas nociplastic pain. Increasing evidence highlights the role of danger signal\nprocessing in the maintenance of chronic pain. In response, we developed\nPersonal Danger Signals Reprocessing (PDSR), an online, group-based\nintervention designed to modify these mechanisms using coaching techniques to\nenhance accessibility and affordability.\n  This study evaluated the efficacy of PDSR in reducing pain and mental health\ncomorbidities. A cohort of women (N=19, mean age 43) participated in an 8-week\nonline program, receiving weekly sessions on chronic pain mechanisms within a\nsystemic framework. Outcomes were assessed at three time points:\npre-intervention, mid-intervention, and post-intervention. A waiting list group\n(N=20, mean age 43.5) completed assessments at the same intervals.\n  Participants in the PDSR group showed significant pain reduction (p < .001),\nwith moderate to large effects observed at mid-intervention (Cohen's D = 0.7)\nand post-intervention (Cohen's D = 1.5) compared to controls. Pain interference\nsignificantly decreased (p < .01), with large reductions in the PDSR group\n(Cohen's D = -1.7, p < .0001). Well-being also improved substantially (p <\n.001, Cohen's D = 1.7-1.8). Secondary outcomes, including pain catastrophizing,\nsleep interference, anxiety, and depressive symptoms, consistently improved\n(all p-values < .01).\n  Findings suggest PDSR is an effective, scalable intervention for reducing\npain, improving function, and enhancing well-being in individuals with chronic\npain.",
        "Humans effortlessly communicate their thoughts through intricate sequences of\nmotor actions. Yet, the neural processes that coordinate language production\nremain largely unknown, in part because speech artifacts limit the use of\nneuroimaging. To elucidate the unfolding of language production in the brain,\nwe investigate with magnetoencephalography (MEG) and electroencephalography\n(EEG) the neurophysiological activity of 35 skilled typists, while they typed\nsentences on a keyboard. This approach confirms the hierarchical predictions of\nlinguistic theories: the neural activity preceding the production of each word\nis marked by the sequential rise and fall of context-, word-, syllable-, and\nletter-level representations. Remarkably, each of these neural representations\nis maintained over long time periods within each level of the language\nhierarchy. This phenomenon results in a superposition of successive\nrepresentations that is supported by a hierarchy of dynamic neural codes.\nOverall, these findings provide a precise computational breakdown of the neural\ndynamics that coordinate the production of language in the human brain.",
        "Short-term memory is essential for cognitive processing, yet our\nunderstanding of its neural mechanisms remains unclear. A key focus in\nneuroscience has been the study of sequential activity patterns, where neurons\nfire one after another within large networks, to explain how information is\nmaintained. While recurrent connections were shown to drive sequential\ndynamics, a mechanistic understanding of this process still remains unknown. In\nthis work, we first introduce two unique mechanisms that can subserve\nshort-term memory: slow-point manifolds generating direct sequences or limit\ncycles providing temporally localized approximations. Then, through analytical\nmodels, we identify fundamental properties that govern the selection of these\nmechanisms, \\textit{i.e.}, we derive theoretical scaling laws for critical\nlearning rates as a function of the delay period length, beyond which no\nlearning is possible. We empirically verify these observations by training and\nevaluating more than 35,000 recurrent neural networks (RNNs) that we will\npublicly release upon publication. Overall, our work provides new insights into\nshort-term memory mechanisms and proposes experimentally testable predictions\nfor systems neuroscience.",
        "Understanding how brain networks recruit resources during cognitive tasks is\nkey to explaining individual differences in task performance. Brain network\nparameters-including activity levels of regions and their connectivity-reflect\nthe integration and segregation of functional subnetworks underlying task\nprocessing. However, the complexity and high dimensionality of these parameters\npose a significant barrier to identifying functionally relevant individual\ndifferences. Here, we introduce stiff-sloppy analysis as a framework for\nuncovering the stiff parameter combinations that critically influence\ntask-state brain dynamics, exemplified by working memory. Using the pairwise\nmaximum entropy model (PMEM) calibrated to fMRI data and Fisher Information\nMatrix (FIM) analysis, we reveal that the stiff dimensions of the model\nparameters capture the most relevant integration and segregation processes of\nthe default mode network and the working memory network. Individual differences\nalong these stiff neural dimensions consistently correlate with working memory\nperformance. Notably, stiff parameters robustly predicted working memory\nperformance, even when the less sensitive (\"sloppy\") parameters were excluded.\nThis study establishes stiff-sloppy analysis as a powerful approach to identify\ncognition-related brain networks, bridging neural dynamics and behavior and\noffering new avenues for personalized neuroscience including therapeutic\ninnovation.",
        "In this review, we examine computational models that explore the role of\nneural oscillations in speech perception, spanning from early auditory\nprocessing to higher cognitive stages. We focus on models that use rhythmic\nbrain activities, such as gamma, theta, and delta oscillations, to encode\nphonemes, segment speech into syllables and words, and integrate linguistic\nelements to infer meaning. We analyze the mechanisms underlying these models,\ntheir biological plausibility, and their potential applications in processing\nand understanding speech in real time, a computational feature that is achieved\nby the human brain but not yet implemented in speech recognition models.\nReal-time processing enables dynamic adaptation to incoming speech, allowing\nsystems to handle the rapid and continuous flow of auditory information\nrequired for effective communication, interactive applications, and accurate\nspeech recognition in a variety of real-world settings. While significant\nprogress has been made in modeling the neural basis of speech perception,\nchallenges remain, particularly in accounting for the complexity of semantic\nprocessing and the integration of contextual influences. Moreover, the high\ncomputational demands of biologically realistic models pose practical\ndifficulties for their implementation and analysis. Despite these limitations,\nthese models provide valuable insights into the neural mechanisms of speech\nperception. We conclude by identifying current limitations, proposing future\nresearch directions, and suggesting how these models can be further developed\nto achieve a more comprehensive understanding of speech processing in the human\nbrain.",
        "Understanding how neural dynamics shape cognitive experiences remains a\ncentral challenge in neuroscience and psychiatry. Here, we present a novel\nframework leveraging state-to-output controllability from dynamical systems\ntheory to model the interplay between cognitive perturbations, neural activity,\nand subjective experience. We demonstrate that large-scale fMRI signals are\nconstrained to low-dimensional manifolds, where affective and cognitive states\nare naturally organized. Furthermore, we provide a theoretically robust method\nto estimate the controllability Gramian from steady-state neural responses,\noffering a direct measure of the energy required to steer cognitive outcomes.\nIn five healthy participants viewing 2,185 emotionally evocative short videos,\nour analyses reveal a strong alignment between neural activations and affective\nratings, with an average correlation of $r \\approx 0.7$. In a clinical cohort\nof 255 patients with major depressive disorder, biweekly Hamilton Rating Scale\ntrajectories over 11 weeks significantly mapped onto these manifolds,\nexplaining approximately 20% more variance than chance ($p < 10^{-10}$,\nnumerically better than chance in 93% reaching statistical significance in\none-third of subjects). Our work bridges dynamical systems theory and clinical\nneuroscience, providing a principled approach to optimize mental health\ntreatments by targeting the most efficient neural pathways for cognitive\nchange.",
        "Even when driven by the same stimulus, neuronal responses are well-known to\nexhibit a striking level of spiking variability. In-vivo electrophysiological\nrecordings also reveal a surprisingly large degree of variability at the\nsubthreshold level. In prior work, we considered biophysically relevant\nneuronal models to account for the observed magnitude of membrane voltage\nfluctuations. We found that accounting for these fluctuations requires weak but\nnonzero synchrony in the spiking activity, in amount that are consistent with\nexperimentally measured spiking correlations. Here we investigate whether such\nsynchrony can explain additional statistical features of the measured neural\nactivity, including neuronal voltage covariability and voltage skewness.\nAddressing this question involves conducting a generalized moment analysis of\nconductance-based neurons in response to input drives modeled as correlated\njump processes. Technically, we perform such an analysis using fixed-point\ntechniques from queuing theory that are applicable in the stationary regime of\nactivity. We found that weak but nonzero synchrony can consistently explain the\nexperimentally reported voltage covariance and skewness. This confirms the role\nof synchrony as a primary driver of cortical variability and supports that\nphysiological neural activity emerges as a population-level phenomenon,\nespecially in the spontaneous regime.",
        "Normative mapping is a framework used to map population-level features of\nhealth-related variables. It is widely used in neuroscience research, but the\nliterature lacks established protocols in modalities that do not support\nhealthy control measurements, such as intracranial EEG (icEEG). An icEEG\nnormative map would allow researchers to learn about population-level brain\nactivity and enable comparison of individual data against these norms to\nidentify abnormalities. Currently, no standardised guide exists for\ntransforming clinical data into a normative, regional icEEG map. Papers often\ncite different software and numerous articles to summarise the lengthy method,\nmaking it laborious for other researchers to understand or apply the process.\nOur protocol seeks to remedy this gap by providing a dataflow guide and key\ndecision points that summarise existing methods. This protocol is used heavily\nin published works from our own lab (twelve peer-reviewed journal\npublications). Briefly, we take as input, icEEG recordings and neuroimaging\ndata from people with epilepsy who are undergoing evaluation for resective\nsurgery. As final outputs, we obtain a normative icEEG map, comprising signal\nproperties localised to brain regions. Optionally, we can also process new\nsubjects through the same pipeline and obtain their z-scores (or centiles) in\neach brain region, for abnormality detection and localisation. To date, a\nsingle, cohesive, dataflow pipeline for generating normative icEEG maps, along\nwith abnormality mapping, has not been created. We envisage that this dataflow\nguide will not only increase understanding and application of normative mapping\nmethods, but will also improve the consistency and quality of studies in the\nfield.",
        "By retracing my scientific journey that began 20 years ago, I highlight in\nthis thesis the need to consider the organization of the brain, which is\ncertainly globally hierarchical, but also highly distributed and mixed in the\nneuronal response of its different areas (by focusing on the sensorimotor\ncortex-basal ganglia network). I also emphasize the importance of adopting\nbehavioral paradigms that reflect as much as possible the characteristics of\nthe scenarios encountered in the real life of animals. And finally, I mention\nthe importance of \"disintegrating\" the way data analysis is traditionally\ncarried out, and of taking into account the dynamic nature of behavior, for\nexample by favoring the study of behavioral variables and so-called \"latent\"\nneuronal activities. I conclude this thesis by presenting the vision of my\nideal laboratory in a perspective of 5 to 10 years from today. This laboratory\nwould have two experimental devices, a first, classic, for carrying out tests\nin a constrained and therefore unnatural environment, the data of which would\nbe compared to those from a second device called \"naturalistic\" in which the\nanimals could potentially express their entire behavioral repertoire. The tasks\ntested would be characterized by strong ecological principles, such as in those\nsimulating the properties of foraging, and the data would make it possible to\ntest hypotheses based on the progressive addition of ecological components in\nthese tasks, all this in order to maintain control over the interpretability of\nthese data which are complex by nature.",
        "Neural encoding of objects and cognitive states remains an elusive yet\ncrucial aspect of brain function. While traditional feed-forward machine\nlearning neural networks have enormous potential to encode information, modern\narchitectures provide little insight into the brain's mechanisms. In this work,\na Jansen and Rit neural mass model was constructed to encode different sets of\ninputs, aiming to understand how simple neural circuits can represent\ninformation. A genetic algorithm was used to optimize parameters that maximized\nthe differences in responses to particular inputs. These differences in\nresponses manifested as phase-shifted oscillations across the set of inputs. By\ndelivering impulses of excitation synchronized with a particular phase-shifted\noscillation, we demonstrated that the encoded phase could be decoded by\nmeasuring oscillatory power. These findings demonstrate the capability of\nneural dynamical circuits to encode and decode information through phase.",
        "When is it beneficial to constrain creativity? Creativity thrives with\nfreedom, but when people collaborate to create artifacts, there is tension\nbetween giving individuals freedom to revise, and protecting prior\nachievements. To test how imposing constraints may affect collective\ncreativity, we performed cultural evolution experiments where participants\ncollaborated to create melodies and images in chains. With melodies, we found\nthat limiting step size (number of musical notes that can be changed) improved\npleasantness ratings for created tunes. Similar results were observed in\ncohorts of musicians, and with different selection regimes. In contrast,\nlimiting step size in creating images consistently reduced pleasantness. These\nconflicting findings suggest that in domains such as music, where artifacts can\nbe easily damaged, and where evolutionary outcomes are hard to foresee,\ncollective creativity may benefit from imposing small step sizes. We discuss\nparallels with search algorithms and the evolution of conservative birdsong\ncultures.",
        "Topological techniques have become a popular tool for studying information\nflows in neural networks. In particular, simplicial homology theory is used to\nanalyze how cognitive representations of space emerge from large conglomerates\nof independent neuronal contributions. Meanwhile, a growing number of studies\nsuggest that many cognitive functions are sustained by serial patterns of\nactivity. Here, we investigate stashes of such patterns using path homology\ntheory -- an impartial, universal approach that does not require a priori\nassumptions about the sequences' nature, functionality, underlying mechanisms,\nor other contexts. We focus on the hippocampus -- a key enabler of learning and\nmemory in mammalian brains -- and quantify the ordinal arrangement of its\nactivity similarly to how its topology has previously been studied in terms of\nsimplicial homologies. The results reveal that the vast majority of sequences\nproduced during spatial navigation are structurally equivalent to one another.\nOnly a few classes of distinct sequences form an ordinal schema of serial\nactivity that remains stable as the pool of sequences consolidates.\nImportantly, the structure of both maps is upheld by combinations of short\nsequences, suggesting that brief activity motifs dominate physiological\ncomputations. This ordinal organization emerges and stabilizes on timescales\ncharacteristic of spatial learning, displaying similar dynamics. Yet, the\nordinal maps generally do not reflect topological affinities -- spatial and\nsequential analyses address qualitatively different aspects of spike flows,\nrepresenting two complementary formats of information processing.",
        "We have generated 2D-multicomponent surface brightness (SB) modelling for 100\ngalaxies in the Large Galaxy Atlas (LGA) together with 19 nearby cD galaxies\nusing the near-infrared (NIR) images from 2MASS (J, H and Ks ). Our final\nsample of 119 galaxies includes cD galaxies, Virgo cluster galaxies, group\ngalaxies, and field galaxies. We revisited known scaling relations (SRs)\ninvolving structural parameters, as well as those involving supermassive black\nholes (SMBHs) and ultramassive black holes (UMBHs). Refining the SRs, we also\nrevisited the bulge classification and considered the Fundamental Plane (FP)\nand its projections, as well as other SRs, such as the colour-magnitude\nrelation (CMR), Tully-Fisher relation (TFR) and luminosity concentration\nrelation (LCR). Classical bulges follow the same relations as elliptical\ngalaxies, while pseudobulges are usually outliers. The NIR colours of classical\nbulges and pseudobulges indicate that their ages are not radically different\ndespite their spread in luminosity, but we noticed that classical bulges are\nmore luminous than pseudobulges, therefore, this property provides a\ncomplementary bulge classification criterion. We included pseudobulges from\nother studies to strengthen the tendencies seen for pseudobulges in our sample.\nFrom the SRs for BHs, we found that pseudobulges do not follow SRs for\nearly-type galaxies and classical bulges. Additionally, the lack of correlation\nbetween BHs and discs may indicate these structures have not coevolved. From\nthe revision of SRs, we present a sample of galaxies likely to host SMBHs or\nUMBHs, which are suitable for dynamical BH mass determination from the ground.",
        "In this paper, we investigate the application of Reed-Muller (RM) codes for\nPhysical-layer security in a real world wiretap channel scenario. Utilizing\nsoftware-defined radios (SDRs) in a real indoor environment, we implement a\ncoset coding scheme that leverages the hierarchical structure of RM codes to\nsecure data transmission. The generator matrix of the RM code is used to\npartition codewords into cosets in the usual way, where each message\ncorresponds to a unique coset, and auxiliary bits select specific codewords\nwithin each coset. This approach enables the legitimate receiver (Bob) can\ndecode the transmitted message with minimal information leakage to eavesdropper\n(Eve) thus protecting the confidentiality of the communication with the help of\ncoset structure. Mutual information neural estimation (MINE) is used to\nquantify information leakage and validate the effectiveness of the scheme.\nExperimental results indicate that RM codes can achieve robust security even in\npractical environments affected by real-world channel impairments. These\nfindings demonstrate the potential of RM codes as an efficient solution for\nphysical-layer security, particularly for applications that require low latency\nand short blocklengths.",
        "Vacuum gap capacitors have recently gained considerable attention in\nsuperconducting circuit platforms due to their compact design and low\ndielectric losses in the microwave regime. Their ability to support mechanical\nvibrational modes makes them ideal candidates for circuit optomechanics.\nHowever, precise control of gap size and achieving high coherence in mechanical\nmodes remain long-standing challenges. Here, we present a detailed fabrication\nprocess for scalable vacuum gap capacitors that support ultra-high-coherence\nmechanical motion, exhibit low microwave loss, and maintain a small footprint\ncompared to planar geometries. We fabricate arrays of up to 24 LC resonators,\nwith capacitors featuring nanometer-scale gap size variations. We demonstrate\nthat the mechanical quality factors can reach up to $40 \\times 10^6$, a\n100-fold improvement over other platforms, with microwave quality factors\n$\\mathcal{O}(10^5)$ at low photon number levels. This platform also achieves a\nsizable single-photon optomechanical coupling rate of approximately 20 Hz.\nUsing this, we cooled the mechanical oscillator to its ground state (0.07\nquanta) and squeezed its motion below the vacuum level by 2.7 dB. We further\ndemonstrate the scalability of this platform by implementing large-scale\noptomechanical arrays, a strained graphene model, and observing quantum\ncollective phenomena in a mechanical hexamer. These vacuum gap capacitors are\npromising candidates for coupling superconducting qubits with mechanical\nsystems, serving as storage elements in quantum computing, and exploring\ngravitational effects on quantum mechanics.",
        "In real-life scenarios, a Reinforcement Learning (RL) agent aiming to\nmaximise their reward, must often also behave in a safe manner, including at\ntraining time. Thus, much attention in recent years has been given to Safe RL,\nwhere an agent aims to learn an optimal policy among all policies that satisfy\na given safety constraint. However, strict safety guarantees are often provided\nthrough approaches based on linear programming, and thus have limited scaling.\nIn this paper we present a new, scalable method, which enjoys strict formal\nguarantees for Safe RL, in the case where the safety dynamics of the Markov\nDecision Process (MDP) are known, and safety is defined as an undiscounted\nprobabilistic avoidance property. Our approach is based on state-augmentation\nof the MDP, and on the design of a shield that restricts the actions available\nto the agent. We show that our approach provides a strict formal safety\nguarantee that the agent stays safe at training and test time. Furthermore, we\ndemonstrate that our approach is viable in practice through experimental\nevaluation.",
        "In Bayesian inference for mixture models with an unknown number of\ncomponents, a finite mixture model is usually employed that assumes prior\ndistributions for mixing weights and the number of components. This model is\ncalled a mixture of finite mixtures (MFM). As a prior distribution for the\nweights, a (symmetric) Dirichlet distribution is widely used for conjugacy and\ncomputational simplicity, while the selection of the concentration parameter\ninfluences the estimate of the number of components. In this paper, we focus on\nestimating the number of components. As a robust alternative Dirichlet weights,\nwe present a method based on a mixture of finite mixtures with normalized\ninverse Gaussian weights. The motivation is similar to the use of normalized\ninverse Gaussian processes instead of Dirichlet processes for infinite mixture\nmodeling. Introducing latent variables, the posterior computation is carried\nout using block Gibbs sampling without using the reversible jump algorithm. The\nperformance of the proposed method is illustrated through some numerical\nexperiments and real data examples, including clustering, density estimation,\nand community detection.",
        "Terrorist activities often exhibit temporal and spatial clustering, making\nthe multivariate Hawkes process (MHP) a useful statistical model for analysing\nterrorism across different geographic regions. However, terror attack data from\nthe Global Terrorism Database is reported as total event counts in disjoint\nobservation periods, with precise event times unknown. When the MHP is only\nobserved discretely, the likelihood function becomes intractable, hindering\nlikelihood-based inference. To address this, we design an unbiased estimate of\nthe intractable likelihood function using sequential Monte Carlo (SMC) based on\na representation of the unobserved event times as latent variables in a\nstate-space model. The unbiasedness of the SMC estimate allows for its use in\nplace of the true likelihood in a Metropolis-Hastings algorithm, from which we\nconstruct a Markov Chain Monte Carlo sample of the distribution over the\nparameters of the MHP. Using simulated data, we assess the performance of our\nmethod and demonstrate that it outperforms an alternative method in the\nliterature based on mean squared error. Terrorist activity in Afghanistan and\nPakistan from 2018 to 2021 is analysed based on daily count data to examine the\nself- and cross-excitation effects of terrorism events.",
        "Using first principles calculations, we have demonstrated the creation of\nmultiple quantum states, in the experimentally accessible metal organic\nframework BHT-Ni. Specifically, quantum spin Hall and quantum anomalous Hall\nstates are induced by two and four electron doping, respectively. The\ngeometrical symmetry breaking, is also investigated. For a low electron doping\nconcentration of two electrons per unit cell, the Fermi energy shifts to a\nnontrivial band gap, between Dirac bands and a quantized spin Hall conductivity\nis predicted. Subsequently in a high electron doping concentration, Anomalous\nHall conductivity with a quantized value was observed. In addition, for\ncentrosymmetric (trans-like) and non-centrosymmetric (cis-like) structures, we\nfound that the trans-like structure preserves quantum spin Hall and quantized\nspin Hall conductivity. In contrast, in the cis-like structure, space inversion\nsymmetry breaking leads to the appearance of valley Hall effect and the\ndisappearance of spin Hall conductivity.",
        "Integrating multiple observational studies for meta-analysis has sparked much\ninterest. The presented R package WMAP (Weighted Meta-Analysis with\nPseudo-Population) addresses a critical gap in the implementation of\nintegrative weighting approaches for multiple observational studies and causal\ninferences about various groups of subjects, such as disease subtypes. The\npackage features three weighting approaches, each representing a special case\nof the unified weighting framework introduced by Guha and Li (2024), which\nincludes an extension of inverse probability weights for data integration\nsettings. It performs meta-analysis on user-inputted datasets as follows: (i)\nit first estimates the propensity scores for study-group combinations,\ncalculates subject balancing weights, and determines the effective sample size\n(ESS) for a user-specified weighting method; and (ii) it then estimates various\nfeatures of multiple counterfactual group outcomes, such as group medians and\ndifferences in group means for the mRNA expression of eight genes.\nAdditionally, bootstrap variability estimates are provided. Among the\nimplemented weighting methods, we highlight the FLEXible, Optimized, and\nRealistic (FLEXOR) method, which is specifically designed to maximize the ESS\nwithin the unified framework. The use of the software is illustrated by\nsimulations as well as a multi-site breast cancer study conducted in seven\nmedical centers.",
        "We collect and summarize results on the unitary equivalence of Gabor systems\nby pairs of unitary operators and global isometries. The methods are then used\nto study Gabor systems with Hermite functions. We provide new proofs of some\nknown results and an outlook on double over-sampling.",
        "Time-varying random objects have been increasingly encountered in modern data\nanalysis. Moreover, in a substantial number of these applications, periodic\nbehavior of the random objects has been observed. We introduce a new, powerful\nscan statistic and corresponding test for the precise identification and\nlocalization of abrupt changes in the distribution of non-Euclidean random\nobjects with possibly periodic behavior. Our approach is nonparametric and\neffectively captures the entire distribution of these random objects.\nRemarkably, it operates with minimal tuning parameters, requiring only the\nspecification of cut-off intervals near endpoints, where change points are\nassumed not to occur. Our theoretical contributions include deriving the\nasymptotic distribution of the test statistic under the null hypothesis of no\nchange points, establishing the consistency of the test in the presence of\nchange points under contiguous alternatives and providing rigorous guarantees\non the near-optimal consistency in estimating the number and locations of\nchange points, whether dealing with a single change point or multiple ones. We\ndemonstrate that the most competitive method currently in the literature for\nchange point detection in random objects is degraded by periodic behavior, as\nperiodicity leads to blurring of the changes that this procedure aims to\ndiscover. Through comprehensive simulation studies, we demonstrate the superior\npower and accuracy of our approach in both detecting change points and\npinpointing their locations, across scenarios involving both periodic and\nnonperiodic random objects. Our main application is to weighted networks,\nrepresented through graph Laplacians. The proposed method delivers highly\ninterpretable results, as evidenced by the identification of meaningful change\npoints in the New York City Citi Bike sharing system that align with\nsignificant historical events.",
        "We study exciton quantum transfer along a molecular chain whilst accounting\nfor the effects of permanent dipoles that are induced by charge displacements\nin the molecular orbitals. These effects are typically neglected as they do not\narise in atomic quantum optics; however, they can play an important role in\nmolecular systems. We also consider novel collective photon-assisted transport\nand compare it against the scaling of phonon-assisted transport in chains\nfeaturing permanent dipoles, and determine a linear scaling with the number of\ndipoles, akin to single-excitation superradiance. We further demonstrate how\npermanent dipoles, dipoles can preferentially arrange energy eigenstates to\nsupport excitation transport. Finally, we show how permanent dipoles can\nenhance the ability of the molecular chain to support excitation transport\ncompared to that of systems that do not possess permanent dipoles across a\nrange of environmental and system configurations.",
        "We prove that cw-hyperbolic homeomorphisms with jointly continuous\nstable\/unstable holonomies satisfy the periodic shadowing property and, if they\nare topologically mixing, the periodic specification property. We discuss\ndifficulties to adapt Bowen's techniques to obtain a measure of maximal entropy\nfor cw-hyperbolic homeomorphisms, exhibit the unique measure of maximal entropy\nfor Walter's pseudo-Anosov diffeomorphism of $\\mathbb{S}^2$, and prove it can\nbe obtained, as in the expansive case, as the weak* limit of an average of\nDirac measures on periodic orbits. As an application, we exhibit the unique\nmeasure of maximal entropy for the homeomorphism on the Sierpi\\'nski Carpet\ndefined in [12], which does not satisfy the specification property.",
        "We report a generalized nonlinear Schr\\\"odinger equation simulation model of\nan extreme learning machine (ELM) based on optical fiber propagation. Using\nhandwritten digit classification as a benchmark, we study how accuracy depends\non propagation dynamics, as well as parameters governing spectral encoding,\nreadout, and noise. Test accuracies of over 91% and 93% are found for\npropagation in the anomalous and normal dispersion regimes respectively. Our\nsimulation results also suggest that quantum noise on the input pulses\nintroduces an intrinsic penalty to ELM performance.",
        "Tungsten monophosphide (WP) has been reported to superconduct below 0.8 K,\nand theoretical work has predicted an unconventional Cooper pairing mechanism.\nHere we present data for WP single crystals grown by means of chemical vapor\ntransport (CVT) of WO3, P, and I2. In comparison to synthesis using WP powder\nas a starting material, this technique results in samples with substantially\ndecreased low-temperature scattering and favors a more three dimensional\nmorphology. We also find that the resistive superconducting transitions in\nthese samples begin above 1 K. Variation in Tc is often found in strongly\ncorrelated superconductors, and its presence in WP could be the result of\ninfluence from a competing order and\/or a non s-wave gap.",
        "We consider multiband BCS superconductors that exhibit time-reversal symmetry\nand uniform pairing, and analyze their dynamic density and spin structure\nfactors using linear-response theory within the mean-field BCS-BEC crossover\nframework at zero temperature. Our results for the multi-orbital Hubbard model\nsatisfy the associated f-sum rules in several limits. In particular, in the\nstrong-coupling limit, they coincide with those of a weakly-interacting Bose\ngas of Cooper pairs, where the low-energy collective Goldstone modes serve as\nBogoliubov phonons. We further reveal that the quantum-geometric origin of the\nlow-energy structure factors, along with related observables such as the\nsuperfluid-weight tensor and the effective-mass tensor of Cooper pairs, can be\ntraced all the way back to the effective-mass theorem for Bloch bands in this\nlimit. As an illustration, we investigate the pyrochlore-Hubbard model\nnumerically and demonstrate that the Goldstone modes are the only relevant\ncollective degrees of freedom in the flat-band regime."
      ]
    }
  },
  {
    "id":2411.1742,
    "research_type":"applied",
    "start_id":"b17",
    "start_title":"Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks",
    "start_abstract":"Image-to-image translation is a class of vision and graphics problems where the goal to learn mapping between an input image output using training set aligned pairs. However, for many tasks, paired data will not be available. We present approach learning translate from source domain X target Y in absence examples. Our G : \u2192 such that distribution images G(X) indistinguishable adversarial loss. Because this highly under-constrained, we couple it with inverse F introduce cycle consistency loss push F(G(X)) \u2248 (and vice versa). Qualitative results are presented on several tasks does exist, including collection style transfer, object transfiguration, season photo enhancement, etc. Quantitative comparisons against prior methods demonstrate superiority our approach.",
    "start_categories":[
      "cs.NE",
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b19"
      ],
      "title":[
        "Bidirectional Mapping of Brain MRI and PET With 3D Reversible GAN for the Diagnosis of Alzheimer\u2019s Disease"
      ],
      "abstract":[
        "<jats:p>Combining multi-modality data for brain disease diagnosis such as Alzheimer\u2019s disease (AD) commonly leads to improved performance than those using a single modality. However, it is still challenging to train a multi-modality model since it is difficult in clinical practice to obtain complete data that includes all modality data. Generally speaking, it is difficult to obtain both magnetic resonance images (MRI) and positron emission tomography (PET) images of a single patient. PET is expensive and requires the injection of radioactive substances into the patient\u2019s body, while MR images are cheaper, safer, and more widely used in practice. Discarding samples without PET data is a common method in previous studies, but the reduction in the number of samples will result in a decrease in model performance. To take advantage of multi-modal complementary information, we first adopt the Reversible Generative Adversarial Network (RevGAN) model to reconstruct the missing data. After that, a 3D convolutional neural network (CNN) classification model with multi-modality input was proposed to perform AD diagnosis. We have evaluated our method on the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) database, and compared the performance of the proposed method with those using state-of-the-art methods. The experimental results show that the structural and functional information of brain tissue can be mapped well and that the image synthesized by our method is close to the real image. In addition, the use of synthetic data is beneficial for the diagnosis and prediction of Alzheimer\u2019s disease, demonstrating the effectiveness of the proposed framework.<\/jats:p>"
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "A 7T fMRI dataset of synthetic images for out-of-distribution modeling\n  of vision",
        "Personal Danger Signals Reprocessing: New Online Group Intervention for\n  Chronic Pain",
        "From Thought to Action: How a Hierarchy of Neural Dynamics Supports\n  Language Production",
        "Dynamical phases of short-term memory mechanisms in RNNs",
        "Stiff-sloppy analysis of brain networks to reveal individual differences\n  in task performance",
        "Neuro-oscillatory models of cortical speech processing",
        "Neural Constraints on Cognitive Experience and Mental Health",
        "Subthreshold moment analysis of neuronal populations driven by\n  synchronous synaptic inputs",
        "From Bedside to Desktop: A Data Protocol for Normative Intracranial EEG\n  and Abnormality Mapping",
        "Towards an integrative approach to the study of brain-environment\n  interactions in human and non-human primate",
        "Phase Alignment Enhances Oscillatory Power in Neural Mass Models\n  Optimized for Class Encoding",
        "How constraints on editing affects cultural evolution",
        "Spaces and sequences in the hippocampus: a homological perspective",
        "2D Surface Brightness Modelling of Large 2MASS Galaxies II: The Role of\n  Classical Bulges and Pseudobulges on Galaxy Scaling Relations and its\n  implication for Supermassive Black Hole Formation",
        "Feasibility of short blocklength Reed-Muller codes for physical layer\n  security in real environment",
        "Compact superconducting vacuum-gap capacitors with low microwave loss\n  and high mechanical coherence for scalable quantum circuits",
        "Probabilistic Shielding for Safe Reinforcement Learning",
        "Bayesian mixture modeling using a mixture of finite mixtures with\n  normalized inverse Gaussian weights",
        "Fitting multivariate Hawkes processes to interval count data with an\n  application to terrorist activity modelling -- a particle Markov chain Monte\n  Carlo approach",
        "Tuning topologically nontrivial states in the BHT-Ni metal organic\n  framework",
        "The R Package WMAP: Tools for Causal Meta-Analysis by Integrating\n  Multiple Observational Studies",
        "Operator-isomorphism pairs and Zak transform methods for the study of\n  Gabor systems",
        "Change Point Detection for Random Objects with Possibly Periodic\n  Behavior",
        "Enhancing energy transport utilising permanent molecular dipoles",
        "Continuum-wise hyperbolicity and periodic points",
        "Limits of nonlinear and dispersive fiber propagation for photonic\n  extreme learning",
        "Enhancement of Superconductivity in WP via Oxide-Assisted Chemical Vapor\n  Transport",
        "Structure factors and quantum geometry in multiband BCS superconductors"
      ],
      "abstract":[
        "Large-scale visual neural datasets such as the Natural Scenes Dataset (NSD)\nare boosting NeuroAI research by enabling computational models of the brain\nwith performances beyond what was possible just a decade ago. However, these\ndatasets lack out-of-distribution (OOD) components, which are crucial for the\ndevelopment of more robust models. Here, we address this limitation by\nreleasing NSD-synthetic, a dataset consisting of 7T fMRI responses from the\neight NSD subjects for 284 carefully controlled synthetic images. We show that\nNSD-synthetic's fMRI responses reliably encode stimulus-related information and\nare OOD with respect to NSD. Furthermore, OOD generalization tests on\nNSD-synthetic reveal differences between models of the brain that are not\ndetected with NSD - specifically, self-supervised deep neural networks better\nexplain neural responses than their task-supervised counterparts. These results\nshowcase how NSD-synthetic enables OOD generalization tests that facilitate the\ndevelopment of more robust models of visual processing, and the formulation of\nmore accurate theories of human vision.",
        "Chronic pain is a significant global health issue, with many patients\nexperiencing persistent pain despite no identifiable organic cause, classified\nas nociplastic pain. Increasing evidence highlights the role of danger signal\nprocessing in the maintenance of chronic pain. In response, we developed\nPersonal Danger Signals Reprocessing (PDSR), an online, group-based\nintervention designed to modify these mechanisms using coaching techniques to\nenhance accessibility and affordability.\n  This study evaluated the efficacy of PDSR in reducing pain and mental health\ncomorbidities. A cohort of women (N=19, mean age 43) participated in an 8-week\nonline program, receiving weekly sessions on chronic pain mechanisms within a\nsystemic framework. Outcomes were assessed at three time points:\npre-intervention, mid-intervention, and post-intervention. A waiting list group\n(N=20, mean age 43.5) completed assessments at the same intervals.\n  Participants in the PDSR group showed significant pain reduction (p < .001),\nwith moderate to large effects observed at mid-intervention (Cohen's D = 0.7)\nand post-intervention (Cohen's D = 1.5) compared to controls. Pain interference\nsignificantly decreased (p < .01), with large reductions in the PDSR group\n(Cohen's D = -1.7, p < .0001). Well-being also improved substantially (p <\n.001, Cohen's D = 1.7-1.8). Secondary outcomes, including pain catastrophizing,\nsleep interference, anxiety, and depressive symptoms, consistently improved\n(all p-values < .01).\n  Findings suggest PDSR is an effective, scalable intervention for reducing\npain, improving function, and enhancing well-being in individuals with chronic\npain.",
        "Humans effortlessly communicate their thoughts through intricate sequences of\nmotor actions. Yet, the neural processes that coordinate language production\nremain largely unknown, in part because speech artifacts limit the use of\nneuroimaging. To elucidate the unfolding of language production in the brain,\nwe investigate with magnetoencephalography (MEG) and electroencephalography\n(EEG) the neurophysiological activity of 35 skilled typists, while they typed\nsentences on a keyboard. This approach confirms the hierarchical predictions of\nlinguistic theories: the neural activity preceding the production of each word\nis marked by the sequential rise and fall of context-, word-, syllable-, and\nletter-level representations. Remarkably, each of these neural representations\nis maintained over long time periods within each level of the language\nhierarchy. This phenomenon results in a superposition of successive\nrepresentations that is supported by a hierarchy of dynamic neural codes.\nOverall, these findings provide a precise computational breakdown of the neural\ndynamics that coordinate the production of language in the human brain.",
        "Short-term memory is essential for cognitive processing, yet our\nunderstanding of its neural mechanisms remains unclear. A key focus in\nneuroscience has been the study of sequential activity patterns, where neurons\nfire one after another within large networks, to explain how information is\nmaintained. While recurrent connections were shown to drive sequential\ndynamics, a mechanistic understanding of this process still remains unknown. In\nthis work, we first introduce two unique mechanisms that can subserve\nshort-term memory: slow-point manifolds generating direct sequences or limit\ncycles providing temporally localized approximations. Then, through analytical\nmodels, we identify fundamental properties that govern the selection of these\nmechanisms, \\textit{i.e.}, we derive theoretical scaling laws for critical\nlearning rates as a function of the delay period length, beyond which no\nlearning is possible. We empirically verify these observations by training and\nevaluating more than 35,000 recurrent neural networks (RNNs) that we will\npublicly release upon publication. Overall, our work provides new insights into\nshort-term memory mechanisms and proposes experimentally testable predictions\nfor systems neuroscience.",
        "Understanding how brain networks recruit resources during cognitive tasks is\nkey to explaining individual differences in task performance. Brain network\nparameters-including activity levels of regions and their connectivity-reflect\nthe integration and segregation of functional subnetworks underlying task\nprocessing. However, the complexity and high dimensionality of these parameters\npose a significant barrier to identifying functionally relevant individual\ndifferences. Here, we introduce stiff-sloppy analysis as a framework for\nuncovering the stiff parameter combinations that critically influence\ntask-state brain dynamics, exemplified by working memory. Using the pairwise\nmaximum entropy model (PMEM) calibrated to fMRI data and Fisher Information\nMatrix (FIM) analysis, we reveal that the stiff dimensions of the model\nparameters capture the most relevant integration and segregation processes of\nthe default mode network and the working memory network. Individual differences\nalong these stiff neural dimensions consistently correlate with working memory\nperformance. Notably, stiff parameters robustly predicted working memory\nperformance, even when the less sensitive (\"sloppy\") parameters were excluded.\nThis study establishes stiff-sloppy analysis as a powerful approach to identify\ncognition-related brain networks, bridging neural dynamics and behavior and\noffering new avenues for personalized neuroscience including therapeutic\ninnovation.",
        "In this review, we examine computational models that explore the role of\nneural oscillations in speech perception, spanning from early auditory\nprocessing to higher cognitive stages. We focus on models that use rhythmic\nbrain activities, such as gamma, theta, and delta oscillations, to encode\nphonemes, segment speech into syllables and words, and integrate linguistic\nelements to infer meaning. We analyze the mechanisms underlying these models,\ntheir biological plausibility, and their potential applications in processing\nand understanding speech in real time, a computational feature that is achieved\nby the human brain but not yet implemented in speech recognition models.\nReal-time processing enables dynamic adaptation to incoming speech, allowing\nsystems to handle the rapid and continuous flow of auditory information\nrequired for effective communication, interactive applications, and accurate\nspeech recognition in a variety of real-world settings. While significant\nprogress has been made in modeling the neural basis of speech perception,\nchallenges remain, particularly in accounting for the complexity of semantic\nprocessing and the integration of contextual influences. Moreover, the high\ncomputational demands of biologically realistic models pose practical\ndifficulties for their implementation and analysis. Despite these limitations,\nthese models provide valuable insights into the neural mechanisms of speech\nperception. We conclude by identifying current limitations, proposing future\nresearch directions, and suggesting how these models can be further developed\nto achieve a more comprehensive understanding of speech processing in the human\nbrain.",
        "Understanding how neural dynamics shape cognitive experiences remains a\ncentral challenge in neuroscience and psychiatry. Here, we present a novel\nframework leveraging state-to-output controllability from dynamical systems\ntheory to model the interplay between cognitive perturbations, neural activity,\nand subjective experience. We demonstrate that large-scale fMRI signals are\nconstrained to low-dimensional manifolds, where affective and cognitive states\nare naturally organized. Furthermore, we provide a theoretically robust method\nto estimate the controllability Gramian from steady-state neural responses,\noffering a direct measure of the energy required to steer cognitive outcomes.\nIn five healthy participants viewing 2,185 emotionally evocative short videos,\nour analyses reveal a strong alignment between neural activations and affective\nratings, with an average correlation of $r \\approx 0.7$. In a clinical cohort\nof 255 patients with major depressive disorder, biweekly Hamilton Rating Scale\ntrajectories over 11 weeks significantly mapped onto these manifolds,\nexplaining approximately 20% more variance than chance ($p < 10^{-10}$,\nnumerically better than chance in 93% reaching statistical significance in\none-third of subjects). Our work bridges dynamical systems theory and clinical\nneuroscience, providing a principled approach to optimize mental health\ntreatments by targeting the most efficient neural pathways for cognitive\nchange.",
        "Even when driven by the same stimulus, neuronal responses are well-known to\nexhibit a striking level of spiking variability. In-vivo electrophysiological\nrecordings also reveal a surprisingly large degree of variability at the\nsubthreshold level. In prior work, we considered biophysically relevant\nneuronal models to account for the observed magnitude of membrane voltage\nfluctuations. We found that accounting for these fluctuations requires weak but\nnonzero synchrony in the spiking activity, in amount that are consistent with\nexperimentally measured spiking correlations. Here we investigate whether such\nsynchrony can explain additional statistical features of the measured neural\nactivity, including neuronal voltage covariability and voltage skewness.\nAddressing this question involves conducting a generalized moment analysis of\nconductance-based neurons in response to input drives modeled as correlated\njump processes. Technically, we perform such an analysis using fixed-point\ntechniques from queuing theory that are applicable in the stationary regime of\nactivity. We found that weak but nonzero synchrony can consistently explain the\nexperimentally reported voltage covariance and skewness. This confirms the role\nof synchrony as a primary driver of cortical variability and supports that\nphysiological neural activity emerges as a population-level phenomenon,\nespecially in the spontaneous regime.",
        "Normative mapping is a framework used to map population-level features of\nhealth-related variables. It is widely used in neuroscience research, but the\nliterature lacks established protocols in modalities that do not support\nhealthy control measurements, such as intracranial EEG (icEEG). An icEEG\nnormative map would allow researchers to learn about population-level brain\nactivity and enable comparison of individual data against these norms to\nidentify abnormalities. Currently, no standardised guide exists for\ntransforming clinical data into a normative, regional icEEG map. Papers often\ncite different software and numerous articles to summarise the lengthy method,\nmaking it laborious for other researchers to understand or apply the process.\nOur protocol seeks to remedy this gap by providing a dataflow guide and key\ndecision points that summarise existing methods. This protocol is used heavily\nin published works from our own lab (twelve peer-reviewed journal\npublications). Briefly, we take as input, icEEG recordings and neuroimaging\ndata from people with epilepsy who are undergoing evaluation for resective\nsurgery. As final outputs, we obtain a normative icEEG map, comprising signal\nproperties localised to brain regions. Optionally, we can also process new\nsubjects through the same pipeline and obtain their z-scores (or centiles) in\neach brain region, for abnormality detection and localisation. To date, a\nsingle, cohesive, dataflow pipeline for generating normative icEEG maps, along\nwith abnormality mapping, has not been created. We envisage that this dataflow\nguide will not only increase understanding and application of normative mapping\nmethods, but will also improve the consistency and quality of studies in the\nfield.",
        "By retracing my scientific journey that began 20 years ago, I highlight in\nthis thesis the need to consider the organization of the brain, which is\ncertainly globally hierarchical, but also highly distributed and mixed in the\nneuronal response of its different areas (by focusing on the sensorimotor\ncortex-basal ganglia network). I also emphasize the importance of adopting\nbehavioral paradigms that reflect as much as possible the characteristics of\nthe scenarios encountered in the real life of animals. And finally, I mention\nthe importance of \"disintegrating\" the way data analysis is traditionally\ncarried out, and of taking into account the dynamic nature of behavior, for\nexample by favoring the study of behavioral variables and so-called \"latent\"\nneuronal activities. I conclude this thesis by presenting the vision of my\nideal laboratory in a perspective of 5 to 10 years from today. This laboratory\nwould have two experimental devices, a first, classic, for carrying out tests\nin a constrained and therefore unnatural environment, the data of which would\nbe compared to those from a second device called \"naturalistic\" in which the\nanimals could potentially express their entire behavioral repertoire. The tasks\ntested would be characterized by strong ecological principles, such as in those\nsimulating the properties of foraging, and the data would make it possible to\ntest hypotheses based on the progressive addition of ecological components in\nthese tasks, all this in order to maintain control over the interpretability of\nthese data which are complex by nature.",
        "Neural encoding of objects and cognitive states remains an elusive yet\ncrucial aspect of brain function. While traditional feed-forward machine\nlearning neural networks have enormous potential to encode information, modern\narchitectures provide little insight into the brain's mechanisms. In this work,\na Jansen and Rit neural mass model was constructed to encode different sets of\ninputs, aiming to understand how simple neural circuits can represent\ninformation. A genetic algorithm was used to optimize parameters that maximized\nthe differences in responses to particular inputs. These differences in\nresponses manifested as phase-shifted oscillations across the set of inputs. By\ndelivering impulses of excitation synchronized with a particular phase-shifted\noscillation, we demonstrated that the encoded phase could be decoded by\nmeasuring oscillatory power. These findings demonstrate the capability of\nneural dynamical circuits to encode and decode information through phase.",
        "When is it beneficial to constrain creativity? Creativity thrives with\nfreedom, but when people collaborate to create artifacts, there is tension\nbetween giving individuals freedom to revise, and protecting prior\nachievements. To test how imposing constraints may affect collective\ncreativity, we performed cultural evolution experiments where participants\ncollaborated to create melodies and images in chains. With melodies, we found\nthat limiting step size (number of musical notes that can be changed) improved\npleasantness ratings for created tunes. Similar results were observed in\ncohorts of musicians, and with different selection regimes. In contrast,\nlimiting step size in creating images consistently reduced pleasantness. These\nconflicting findings suggest that in domains such as music, where artifacts can\nbe easily damaged, and where evolutionary outcomes are hard to foresee,\ncollective creativity may benefit from imposing small step sizes. We discuss\nparallels with search algorithms and the evolution of conservative birdsong\ncultures.",
        "Topological techniques have become a popular tool for studying information\nflows in neural networks. In particular, simplicial homology theory is used to\nanalyze how cognitive representations of space emerge from large conglomerates\nof independent neuronal contributions. Meanwhile, a growing number of studies\nsuggest that many cognitive functions are sustained by serial patterns of\nactivity. Here, we investigate stashes of such patterns using path homology\ntheory -- an impartial, universal approach that does not require a priori\nassumptions about the sequences' nature, functionality, underlying mechanisms,\nor other contexts. We focus on the hippocampus -- a key enabler of learning and\nmemory in mammalian brains -- and quantify the ordinal arrangement of its\nactivity similarly to how its topology has previously been studied in terms of\nsimplicial homologies. The results reveal that the vast majority of sequences\nproduced during spatial navigation are structurally equivalent to one another.\nOnly a few classes of distinct sequences form an ordinal schema of serial\nactivity that remains stable as the pool of sequences consolidates.\nImportantly, the structure of both maps is upheld by combinations of short\nsequences, suggesting that brief activity motifs dominate physiological\ncomputations. This ordinal organization emerges and stabilizes on timescales\ncharacteristic of spatial learning, displaying similar dynamics. Yet, the\nordinal maps generally do not reflect topological affinities -- spatial and\nsequential analyses address qualitatively different aspects of spike flows,\nrepresenting two complementary formats of information processing.",
        "We have generated 2D-multicomponent surface brightness (SB) modelling for 100\ngalaxies in the Large Galaxy Atlas (LGA) together with 19 nearby cD galaxies\nusing the near-infrared (NIR) images from 2MASS (J, H and Ks ). Our final\nsample of 119 galaxies includes cD galaxies, Virgo cluster galaxies, group\ngalaxies, and field galaxies. We revisited known scaling relations (SRs)\ninvolving structural parameters, as well as those involving supermassive black\nholes (SMBHs) and ultramassive black holes (UMBHs). Refining the SRs, we also\nrevisited the bulge classification and considered the Fundamental Plane (FP)\nand its projections, as well as other SRs, such as the colour-magnitude\nrelation (CMR), Tully-Fisher relation (TFR) and luminosity concentration\nrelation (LCR). Classical bulges follow the same relations as elliptical\ngalaxies, while pseudobulges are usually outliers. The NIR colours of classical\nbulges and pseudobulges indicate that their ages are not radically different\ndespite their spread in luminosity, but we noticed that classical bulges are\nmore luminous than pseudobulges, therefore, this property provides a\ncomplementary bulge classification criterion. We included pseudobulges from\nother studies to strengthen the tendencies seen for pseudobulges in our sample.\nFrom the SRs for BHs, we found that pseudobulges do not follow SRs for\nearly-type galaxies and classical bulges. Additionally, the lack of correlation\nbetween BHs and discs may indicate these structures have not coevolved. From\nthe revision of SRs, we present a sample of galaxies likely to host SMBHs or\nUMBHs, which are suitable for dynamical BH mass determination from the ground.",
        "In this paper, we investigate the application of Reed-Muller (RM) codes for\nPhysical-layer security in a real world wiretap channel scenario. Utilizing\nsoftware-defined radios (SDRs) in a real indoor environment, we implement a\ncoset coding scheme that leverages the hierarchical structure of RM codes to\nsecure data transmission. The generator matrix of the RM code is used to\npartition codewords into cosets in the usual way, where each message\ncorresponds to a unique coset, and auxiliary bits select specific codewords\nwithin each coset. This approach enables the legitimate receiver (Bob) can\ndecode the transmitted message with minimal information leakage to eavesdropper\n(Eve) thus protecting the confidentiality of the communication with the help of\ncoset structure. Mutual information neural estimation (MINE) is used to\nquantify information leakage and validate the effectiveness of the scheme.\nExperimental results indicate that RM codes can achieve robust security even in\npractical environments affected by real-world channel impairments. These\nfindings demonstrate the potential of RM codes as an efficient solution for\nphysical-layer security, particularly for applications that require low latency\nand short blocklengths.",
        "Vacuum gap capacitors have recently gained considerable attention in\nsuperconducting circuit platforms due to their compact design and low\ndielectric losses in the microwave regime. Their ability to support mechanical\nvibrational modes makes them ideal candidates for circuit optomechanics.\nHowever, precise control of gap size and achieving high coherence in mechanical\nmodes remain long-standing challenges. Here, we present a detailed fabrication\nprocess for scalable vacuum gap capacitors that support ultra-high-coherence\nmechanical motion, exhibit low microwave loss, and maintain a small footprint\ncompared to planar geometries. We fabricate arrays of up to 24 LC resonators,\nwith capacitors featuring nanometer-scale gap size variations. We demonstrate\nthat the mechanical quality factors can reach up to $40 \\times 10^6$, a\n100-fold improvement over other platforms, with microwave quality factors\n$\\mathcal{O}(10^5)$ at low photon number levels. This platform also achieves a\nsizable single-photon optomechanical coupling rate of approximately 20 Hz.\nUsing this, we cooled the mechanical oscillator to its ground state (0.07\nquanta) and squeezed its motion below the vacuum level by 2.7 dB. We further\ndemonstrate the scalability of this platform by implementing large-scale\noptomechanical arrays, a strained graphene model, and observing quantum\ncollective phenomena in a mechanical hexamer. These vacuum gap capacitors are\npromising candidates for coupling superconducting qubits with mechanical\nsystems, serving as storage elements in quantum computing, and exploring\ngravitational effects on quantum mechanics.",
        "In real-life scenarios, a Reinforcement Learning (RL) agent aiming to\nmaximise their reward, must often also behave in a safe manner, including at\ntraining time. Thus, much attention in recent years has been given to Safe RL,\nwhere an agent aims to learn an optimal policy among all policies that satisfy\na given safety constraint. However, strict safety guarantees are often provided\nthrough approaches based on linear programming, and thus have limited scaling.\nIn this paper we present a new, scalable method, which enjoys strict formal\nguarantees for Safe RL, in the case where the safety dynamics of the Markov\nDecision Process (MDP) are known, and safety is defined as an undiscounted\nprobabilistic avoidance property. Our approach is based on state-augmentation\nof the MDP, and on the design of a shield that restricts the actions available\nto the agent. We show that our approach provides a strict formal safety\nguarantee that the agent stays safe at training and test time. Furthermore, we\ndemonstrate that our approach is viable in practice through experimental\nevaluation.",
        "In Bayesian inference for mixture models with an unknown number of\ncomponents, a finite mixture model is usually employed that assumes prior\ndistributions for mixing weights and the number of components. This model is\ncalled a mixture of finite mixtures (MFM). As a prior distribution for the\nweights, a (symmetric) Dirichlet distribution is widely used for conjugacy and\ncomputational simplicity, while the selection of the concentration parameter\ninfluences the estimate of the number of components. In this paper, we focus on\nestimating the number of components. As a robust alternative Dirichlet weights,\nwe present a method based on a mixture of finite mixtures with normalized\ninverse Gaussian weights. The motivation is similar to the use of normalized\ninverse Gaussian processes instead of Dirichlet processes for infinite mixture\nmodeling. Introducing latent variables, the posterior computation is carried\nout using block Gibbs sampling without using the reversible jump algorithm. The\nperformance of the proposed method is illustrated through some numerical\nexperiments and real data examples, including clustering, density estimation,\nand community detection.",
        "Terrorist activities often exhibit temporal and spatial clustering, making\nthe multivariate Hawkes process (MHP) a useful statistical model for analysing\nterrorism across different geographic regions. However, terror attack data from\nthe Global Terrorism Database is reported as total event counts in disjoint\nobservation periods, with precise event times unknown. When the MHP is only\nobserved discretely, the likelihood function becomes intractable, hindering\nlikelihood-based inference. To address this, we design an unbiased estimate of\nthe intractable likelihood function using sequential Monte Carlo (SMC) based on\na representation of the unobserved event times as latent variables in a\nstate-space model. The unbiasedness of the SMC estimate allows for its use in\nplace of the true likelihood in a Metropolis-Hastings algorithm, from which we\nconstruct a Markov Chain Monte Carlo sample of the distribution over the\nparameters of the MHP. Using simulated data, we assess the performance of our\nmethod and demonstrate that it outperforms an alternative method in the\nliterature based on mean squared error. Terrorist activity in Afghanistan and\nPakistan from 2018 to 2021 is analysed based on daily count data to examine the\nself- and cross-excitation effects of terrorism events.",
        "Using first principles calculations, we have demonstrated the creation of\nmultiple quantum states, in the experimentally accessible metal organic\nframework BHT-Ni. Specifically, quantum spin Hall and quantum anomalous Hall\nstates are induced by two and four electron doping, respectively. The\ngeometrical symmetry breaking, is also investigated. For a low electron doping\nconcentration of two electrons per unit cell, the Fermi energy shifts to a\nnontrivial band gap, between Dirac bands and a quantized spin Hall conductivity\nis predicted. Subsequently in a high electron doping concentration, Anomalous\nHall conductivity with a quantized value was observed. In addition, for\ncentrosymmetric (trans-like) and non-centrosymmetric (cis-like) structures, we\nfound that the trans-like structure preserves quantum spin Hall and quantized\nspin Hall conductivity. In contrast, in the cis-like structure, space inversion\nsymmetry breaking leads to the appearance of valley Hall effect and the\ndisappearance of spin Hall conductivity.",
        "Integrating multiple observational studies for meta-analysis has sparked much\ninterest. The presented R package WMAP (Weighted Meta-Analysis with\nPseudo-Population) addresses a critical gap in the implementation of\nintegrative weighting approaches for multiple observational studies and causal\ninferences about various groups of subjects, such as disease subtypes. The\npackage features three weighting approaches, each representing a special case\nof the unified weighting framework introduced by Guha and Li (2024), which\nincludes an extension of inverse probability weights for data integration\nsettings. It performs meta-analysis on user-inputted datasets as follows: (i)\nit first estimates the propensity scores for study-group combinations,\ncalculates subject balancing weights, and determines the effective sample size\n(ESS) for a user-specified weighting method; and (ii) it then estimates various\nfeatures of multiple counterfactual group outcomes, such as group medians and\ndifferences in group means for the mRNA expression of eight genes.\nAdditionally, bootstrap variability estimates are provided. Among the\nimplemented weighting methods, we highlight the FLEXible, Optimized, and\nRealistic (FLEXOR) method, which is specifically designed to maximize the ESS\nwithin the unified framework. The use of the software is illustrated by\nsimulations as well as a multi-site breast cancer study conducted in seven\nmedical centers.",
        "We collect and summarize results on the unitary equivalence of Gabor systems\nby pairs of unitary operators and global isometries. The methods are then used\nto study Gabor systems with Hermite functions. We provide new proofs of some\nknown results and an outlook on double over-sampling.",
        "Time-varying random objects have been increasingly encountered in modern data\nanalysis. Moreover, in a substantial number of these applications, periodic\nbehavior of the random objects has been observed. We introduce a new, powerful\nscan statistic and corresponding test for the precise identification and\nlocalization of abrupt changes in the distribution of non-Euclidean random\nobjects with possibly periodic behavior. Our approach is nonparametric and\neffectively captures the entire distribution of these random objects.\nRemarkably, it operates with minimal tuning parameters, requiring only the\nspecification of cut-off intervals near endpoints, where change points are\nassumed not to occur. Our theoretical contributions include deriving the\nasymptotic distribution of the test statistic under the null hypothesis of no\nchange points, establishing the consistency of the test in the presence of\nchange points under contiguous alternatives and providing rigorous guarantees\non the near-optimal consistency in estimating the number and locations of\nchange points, whether dealing with a single change point or multiple ones. We\ndemonstrate that the most competitive method currently in the literature for\nchange point detection in random objects is degraded by periodic behavior, as\nperiodicity leads to blurring of the changes that this procedure aims to\ndiscover. Through comprehensive simulation studies, we demonstrate the superior\npower and accuracy of our approach in both detecting change points and\npinpointing their locations, across scenarios involving both periodic and\nnonperiodic random objects. Our main application is to weighted networks,\nrepresented through graph Laplacians. The proposed method delivers highly\ninterpretable results, as evidenced by the identification of meaningful change\npoints in the New York City Citi Bike sharing system that align with\nsignificant historical events.",
        "We study exciton quantum transfer along a molecular chain whilst accounting\nfor the effects of permanent dipoles that are induced by charge displacements\nin the molecular orbitals. These effects are typically neglected as they do not\narise in atomic quantum optics; however, they can play an important role in\nmolecular systems. We also consider novel collective photon-assisted transport\nand compare it against the scaling of phonon-assisted transport in chains\nfeaturing permanent dipoles, and determine a linear scaling with the number of\ndipoles, akin to single-excitation superradiance. We further demonstrate how\npermanent dipoles, dipoles can preferentially arrange energy eigenstates to\nsupport excitation transport. Finally, we show how permanent dipoles can\nenhance the ability of the molecular chain to support excitation transport\ncompared to that of systems that do not possess permanent dipoles across a\nrange of environmental and system configurations.",
        "We prove that cw-hyperbolic homeomorphisms with jointly continuous\nstable\/unstable holonomies satisfy the periodic shadowing property and, if they\nare topologically mixing, the periodic specification property. We discuss\ndifficulties to adapt Bowen's techniques to obtain a measure of maximal entropy\nfor cw-hyperbolic homeomorphisms, exhibit the unique measure of maximal entropy\nfor Walter's pseudo-Anosov diffeomorphism of $\\mathbb{S}^2$, and prove it can\nbe obtained, as in the expansive case, as the weak* limit of an average of\nDirac measures on periodic orbits. As an application, we exhibit the unique\nmeasure of maximal entropy for the homeomorphism on the Sierpi\\'nski Carpet\ndefined in [12], which does not satisfy the specification property.",
        "We report a generalized nonlinear Schr\\\"odinger equation simulation model of\nan extreme learning machine (ELM) based on optical fiber propagation. Using\nhandwritten digit classification as a benchmark, we study how accuracy depends\non propagation dynamics, as well as parameters governing spectral encoding,\nreadout, and noise. Test accuracies of over 91% and 93% are found for\npropagation in the anomalous and normal dispersion regimes respectively. Our\nsimulation results also suggest that quantum noise on the input pulses\nintroduces an intrinsic penalty to ELM performance.",
        "Tungsten monophosphide (WP) has been reported to superconduct below 0.8 K,\nand theoretical work has predicted an unconventional Cooper pairing mechanism.\nHere we present data for WP single crystals grown by means of chemical vapor\ntransport (CVT) of WO3, P, and I2. In comparison to synthesis using WP powder\nas a starting material, this technique results in samples with substantially\ndecreased low-temperature scattering and favors a more three dimensional\nmorphology. We also find that the resistive superconducting transitions in\nthese samples begin above 1 K. Variation in Tc is often found in strongly\ncorrelated superconductors, and its presence in WP could be the result of\ninfluence from a competing order and\/or a non s-wave gap.",
        "We consider multiband BCS superconductors that exhibit time-reversal symmetry\nand uniform pairing, and analyze their dynamic density and spin structure\nfactors using linear-response theory within the mean-field BCS-BEC crossover\nframework at zero temperature. Our results for the multi-orbital Hubbard model\nsatisfy the associated f-sum rules in several limits. In particular, in the\nstrong-coupling limit, they coincide with those of a weakly-interacting Bose\ngas of Cooper pairs, where the low-energy collective Goldstone modes serve as\nBogoliubov phonons. We further reveal that the quantum-geometric origin of the\nlow-energy structure factors, along with related observables such as the\nsuperfluid-weight tensor and the effective-mass tensor of Cooper pairs, can be\ntraced all the way back to the effective-mass theorem for Bloch bands in this\nlimit. As an illustration, we investigate the pyrochlore-Hubbard model\nnumerically and demonstrate that the Goldstone modes are the only relevant\ncollective degrees of freedom in the flat-band regime."
      ]
    }
  },
  {
    "id":2411.1742,
    "research_type":"applied",
    "start_id":"b19",
    "start_title":"Bidirectional Mapping of Brain MRI and PET With 3D Reversible GAN for the Diagnosis of Alzheimer\u2019s Disease",
    "start_abstract":"<jats:p>Combining multi-modality data for brain disease diagnosis such as Alzheimer\u2019s disease (AD) commonly leads to improved performance than those using a single modality. However, it is still challenging to train a multi-modality model since it is difficult in clinical practice to obtain complete data that includes all modality data. Generally speaking, it is difficult to obtain both magnetic resonance images (MRI) and positron emission tomography (PET) images of a single patient. PET is expensive and requires the injection of radioactive substances into the patient\u2019s body, while MR images are cheaper, safer, and more widely used in practice. Discarding samples without PET data is a common method in previous studies, but the reduction in the number of samples will result in a decrease in model performance. To take advantage of multi-modal complementary information, we first adopt the Reversible Generative Adversarial Network (RevGAN) model to reconstruct the missing data. After that, a 3D convolutional neural network (CNN) classification model with multi-modality input was proposed to perform AD diagnosis. We have evaluated our method on the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) database, and compared the performance of the proposed method with those using state-of-the-art methods. The experimental results show that the structural and functional information of brain tissue can be mapped well and that the image synthesized by our method is close to the real image. In addition, the use of synthetic data is beneficial for the diagnosis and prediction of Alzheimer\u2019s disease, demonstrating the effectiveness of the proposed framework.<\/jats:p>",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b3",
        "b17"
      ],
      "title":[
        "Deep learning-based classification of healthy aging controls, mild cognitive impairment and alzheimer's disease using fusion of mri-pet imaging",
        "Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks"
      ],
      "abstract":[
        "Automated detection of dementia stage using multimodal imaging modalities will be helpful for improving the clinical diagnosis. In this study, we develop the Inception-ResNet wrapper model in differentiating the healthy controls (HC), mild cognitive impairment (MCI), and Alzheimer\u2019s disease (AD) using conjoint magnetic resonance imaging (MRI) and positron emission tomography (PET) scans. We use T1-weighted MR and PET images of individuals aged between 42 and 95 years, including HC, MCI and AD patients. We first perform 3D tissue segmentation of MR images after skull striping. The atlas-based segmented MR image tissue is fused with PET image. Then we transform PET images from RGB to HSI color space and apply fusion of MRI with PET images using two-dimensional Fourier and discrete wavelet transform (DWT) and then reconstruct the MR-PET fused image using inverse Fourier and DWT methods. After the fusion of MRI and PET imaging modalities, we used 60 % training, 20 % for validation and the remaining 20 % as a test set using various convolutional neural networks. We found the proposed model as the best classifier with an accuracy of 95.5 %, 94.1 % and 95.9 % in classifying HC vs MCI, MCI vs AD and AD vs HC respectively when compared to the existing methods. We conclude that the proposed deep learning model has potential in automated classification of healthy and dementia stages using combined MRI and PET modalities with good performance.",
        "Image-to-image translation is a class of vision and graphics problems where the goal to learn mapping between an input image output using training set aligned pairs. However, for many tasks, paired data will not be available. We present approach learning translate from source domain X target Y in absence examples. Our G : \u2192 such that distribution images G(X) indistinguishable adversarial loss. Because this highly under-constrained, we couple it with inverse F introduce cycle consistency loss push F(G(X)) \u2248 (and vice versa). Qualitative results are presented on several tasks does exist, including collection style transfer, object transfiguration, season photo enhancement, etc. Quantitative comparisons against prior methods demonstrate superiority our approach."
      ],
      "categories":[
        "cs.NE",
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Chameleon: On the Scene Diversity and Domain Variety of AI-Generated\n  Videos Detection",
        "LXLv2: Enhanced LiDAR Excluded Lean 3D Object Detection with Fusion of\n  4D Radar and Camera",
        "SAFER: Sharpness Aware layer-selective Finetuning for Enhanced\n  Robustness in vision transformers",
        "DroneSplat: 3D Gaussian Splatting for Robust 3D Reconstruction from\n  In-the-Wild Drone Imagery",
        "MExD: An Expert-Infused Diffusion Model for Whole-Slide Image\n  Classification",
        "Test-time Loss Landscape Adaptation for Zero-Shot Generalization in\n  Vision-Language Models",
        "Towards Intelligent Design: A Self-driven Framework for Collocated\n  Clothing Synthesis Leveraging Fashion Styles and Textures",
        "Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using\n  Real-Time Warped Noise",
        "Fine-Grained Image-Text Correspondence with Cost Aggregation for\n  Open-Vocabulary Part Segmentation",
        "Visual Autoregressive Modeling for Image Super-Resolution",
        "Green Video Camouflaged Object Detection",
        "Attention Reallocation: Towards Zero-cost and Controllable Hallucination\n  Mitigation of MLLMs",
        "A mathematical perspective on the paradox that chemotherapy sometimes\n  works backwards",
        "An Efficient Quantum Approximate Optimization Algorithm with Fixed\n  Linear Ramp Schedule for Truss Structure Optimization",
        "The COSMOS-Web ring: Spectroscopic confirmation of the background source\n  at z = 5.1",
        "Financial Fraud Detection with Entropy Computing",
        "Survey on Question Answering over Visually Rich Documents: Methods,\n  Challenges, and Trends",
        "Cracking Vector Search Indexes",
        "Fineness and smoothness of a KSBA moduli of marked cubic surfaces",
        "Aspects of Complexity in Quantum Evolutions on the Bloch Sphere",
        "Fluctuation-driven topological Hall effect in room-temperature itinerant\n  helimagnet Fe3Ga4",
        "Invariant and non-invariant almost complex structures on compact\n  quotients of Lie groups",
        "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search",
        "Self-supervised conformal prediction for uncertainty quantification in\n  Poisson imaging problems",
        "Dynamic Manipulation of Multiphase Fluid in Microgravity Using\n  Photoresponsive Surfactant",
        "InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning\n  and Reflection",
        "EXALT: EXplainable ALgorithmic Tools for Optimization Problems"
      ],
      "abstract":[
        "Artificial intelligence generated content (AIGC), known as DeepFakes, has\nemerged as a growing concern because it is being utilized as a tool for\nspreading disinformation. While much research exists on identifying\nAI-generated text and images, research on detecting AI-generated videos is\nlimited. Existing datasets for AI-generated videos detection exhibit\nlimitations in terms of diversity, complexity, and realism. To address these\nissues, this paper focuses on AI-generated videos detection and constructs a\ndiverse dataset named Chameleon. We generate videos through multiple generation\ntools and various real video sources. At the same time, we preserve the videos'\nreal-world complexity, including scene switches and dynamic perspective\nchanges, and expand beyond face-centered detection to include human actions and\nenvironment generation. Our work bridges the gap between AI-generated dataset\nconstruction and real-world forensic needs, offering a valuable benchmark to\ncounteract the evolving threats of AI-generated content.",
        "As the previous state-of-the-art 4D radar-camera fusion-based 3D object\ndetection method, LXL utilizes the predicted image depth distribution maps and\nradar 3D occupancy grids to assist the sampling-based image view\ntransformation. However, the depth prediction lacks accuracy and consistency,\nand the concatenation-based fusion in LXL impedes the model robustness. In this\nwork, we propose LXLv2, where modifications are made to overcome the\nlimitations and improve the performance. Specifically, considering the position\nerror in radar measurements, we devise a one-to-many depth supervision strategy\nvia radar points, where the radar cross section (RCS) value is further\nexploited to adjust the supervision area for object-level depth consistency.\nAdditionally, a channel and spatial attention-based fusion module named\nCSAFusion is introduced to improve feature adaptiveness. Experimental results\non the View-of-Delft and TJ4DRadSet datasets show that the proposed LXLv2 can\noutperform LXL in detection accuracy, inference speed and robustness,\ndemonstrating the effectiveness of the model.",
        "Vision transformers (ViTs) have become essential backbones in advanced\ncomputer vision applications and multi-modal foundation models. Despite their\nstrengths, ViTs remain vulnerable to adversarial perturbations, comparable to\nor even exceeding the vulnerability of convolutional neural networks (CNNs).\nFurthermore, the large parameter count and complex architecture of ViTs make\nthem particularly prone to adversarial overfitting, often compromising both\nclean and adversarial accuracy.\n  This paper mitigates adversarial overfitting in ViTs through a novel,\nlayer-selective fine-tuning approach: SAFER. Instead of optimizing the entire\nmodel, we identify and selectively fine-tune a small subset of layers most\nsusceptible to overfitting, applying sharpness-aware minimization to these\nlayers while freezing the rest of the model. Our method consistently enhances\nboth clean and adversarial accuracy over baseline approaches. Typical\nimprovements are around 5%, with some cases achieving gains as high as 20%\nacross various ViT architectures and datasets.",
        "Drones have become essential tools for reconstructing wild scenes due to\ntheir outstanding maneuverability. Recent advances in radiance field methods\nhave achieved remarkable rendering quality, providing a new avenue for 3D\nreconstruction from drone imagery. However, dynamic distractors in wild\nenvironments challenge the static scene assumption in radiance fields, while\nlimited view constraints hinder the accurate capture of underlying scene\ngeometry. To address these challenges, we introduce DroneSplat, a novel\nframework designed for robust 3D reconstruction from in-the-wild drone imagery.\nOur method adaptively adjusts masking thresholds by integrating local-global\nsegmentation heuristics with statistical approaches, enabling precise\nidentification and elimination of dynamic distractors in static scenes. We\nenhance 3D Gaussian Splatting with multi-view stereo predictions and a\nvoxel-guided optimization strategy, supporting high-quality rendering under\nlimited view constraints. For comprehensive evaluation, we provide a\ndrone-captured 3D reconstruction dataset encompassing both dynamic and static\nscenes. Extensive experiments demonstrate that DroneSplat outperforms both 3DGS\nand NeRF baselines in handling in-the-wild drone imagery.",
        "Whole Slide Image (WSI) classification poses unique challenges due to the\nvast image size and numerous non-informative regions, which introduce noise and\ncause data imbalance during feature aggregation. To address these issues, we\npropose MExD, an Expert-Infused Diffusion Model that combines the strengths of\na Mixture-of-Experts (MoE) mechanism with a diffusion model for enhanced\nclassification. MExD balances patch feature distribution through a novel\nMoE-based aggregator that selectively emphasizes relevant information,\neffectively filtering noise, addressing data imbalance, and extracting\nessential features. These features are then integrated via a diffusion-based\ngenerative process to directly yield the class distribution for the WSI. Moving\nbeyond conventional discriminative approaches, MExD represents the first\ngenerative strategy in WSI classification, capturing fine-grained details for\nrobust and precise results. Our MExD is validated on three widely-used\nbenchmarks-Camelyon16, TCGA-NSCLC, and BRACS consistently achieving\nstate-of-the-art performance in both binary and multi-class tasks.",
        "Test-time adaptation of pre-trained vision-language models has emerged as a\ntechnique for tackling distribution shifts during the test time. Although\nexisting methods, especially those based on Test-time Prompt Tuning (TPT), have\nshown promising results, their high computational cost associated with\nparameter optimization presents challenges for scalability and practical\napplication. This paper unveils the unnecessary nature of backpropagation in\nexisting methods from a loss landscape perspective. Building on this insight,\nthis paper proposes a simple yet effective framework called Test-time Loss\nLandscape Adaptation (TLLA). TLLA leverages the relative position between the\ntraining minimum and test loss landscapes to guide the adaptation process,\navoiding the update of model parameters at test time. Specifically, it mainly\nconsists of two main stages: In the prompt tuning stage, a Sharpness-Aware\nPrompt Tuning (SAPT) method is introduced to identify the training flat\nminimum, setting the foundation for the subsequent test-time adaptation; In the\ntest stage, a Sharpness-based Test Sample Selection (STSS) approach is utilized\nto ensure the alignment of flat minima within the training loss landscape and\neach augmented test sample's loss landscape. Extensive experiments on both\ndomain generalization and cross-dataset benchmarks demonstrate that TLLA\nachieves state-of-the-art performances while significantly reducing\ncomputational overhead. Notably, TLLA surpasses TPT by an average of 5.32\\% and\n6.98\\% on four ImageNet variant datasets when employing ResNet50 and ViT-B\/16\nimage encoders, respectively. The code will be available soon.",
        "Collocated clothing synthesis (CCS) has emerged as a pivotal topic in fashion\ntechnology, primarily concerned with the generation of a clothing item that\nharmoniously matches a given item. However, previous investigations have relied\non using paired outfits, such as a pair of matching upper and lower clothing,\nto train a generative model for achieving this task. This reliance on the\nexpertise of fashion professionals in the construction of such paired outfits\nhas engendered a laborious and time-intensive process. In this paper, we\nintroduce a new self-driven framework, named style- and texture-guided\ngenerative network (ST-Net), to synthesize collocated clothing without the\nnecessity for paired outfits, leveraging self-supervised learning. ST-Net is\ndesigned to extrapolate fashion compatibility rules from the style and texture\nattributes of clothing, using a generative adversarial network. To facilitate\nthe training and evaluation of our model, we have constructed a large-scale\ndataset specifically tailored for unsupervised CCS. Extensive experiments\nsubstantiate that our proposed method outperforms the state-of-the-art\nbaselines in terms of both visual authenticity and fashion compatibility.",
        "Generative modeling aims to transform random noise into structured outputs.\nIn this work, we enhance video diffusion models by allowing motion control via\nstructured latent noise sampling. This is achieved by just a change in data: we\npre-process training videos to yield structured noise. Consequently, our method\nis agnostic to diffusion model design, requiring no changes to model\narchitectures or training pipelines. Specifically, we propose a novel noise\nwarping algorithm, fast enough to run in real time, that replaces random\ntemporal Gaussianity with correlated warped noise derived from optical flow\nfields, while preserving the spatial Gaussianity. The efficiency of our\nalgorithm enables us to fine-tune modern video diffusion base models using\nwarped noise with minimal overhead, and provide a one-stop solution for a wide\nrange of user-friendly motion control: local object motion control, global\ncamera movement control, and motion transfer. The harmonization between\ntemporal coherence and spatial Gaussianity in our warped noise leads to\neffective motion control while maintaining per-frame pixel quality. Extensive\nexperiments and user studies demonstrate the advantages of our method, making\nit a robust and scalable approach for controlling motion in video diffusion\nmodels. Video results are available on our webpage:\nhttps:\/\/eyeline-research.github.io\/Go-with-the-Flow. Source code and model\ncheckpoints are available on GitHub:\nhttps:\/\/github.com\/Eyeline-Research\/Go-with-the-Flow.",
        "Open-Vocabulary Part Segmentation (OVPS) is an emerging field for recognizing\nfine-grained parts in unseen categories. We identify two primary challenges in\nOVPS: (1) the difficulty in aligning part-level image-text correspondence, and\n(2) the lack of structural understanding in segmenting object parts. To address\nthese issues, we propose PartCATSeg, a novel framework that integrates\nobject-aware part-level cost aggregation, compositional loss, and structural\nguidance from DINO. Our approach employs a disentangled cost aggregation\nstrategy that handles object and part-level costs separately, enhancing the\nprecision of part-level segmentation. We also introduce a compositional loss to\nbetter capture part-object relationships, compensating for the limited part\nannotations. Additionally, structural guidance from DINO features improves\nboundary delineation and inter-part understanding. Extensive experiments on\nPascal-Part-116, ADE20K-Part-234, and PartImageNet datasets demonstrate that\nour method significantly outperforms state-of-the-art approaches, setting a new\nbaseline for robust generalization to unseen part categories.",
        "Image Super-Resolution (ISR) has seen significant progress with the\nintroduction of remarkable generative models. However, challenges such as the\ntrade-off issues between fidelity and realism, as well as computational\ncomplexity, have also posed limitations on their application. Building upon the\ntremendous success of autoregressive models in the language domain, we propose\n\\textbf{VARSR}, a novel visual autoregressive modeling for ISR framework with\nthe form of next-scale prediction. To effectively integrate and preserve\nsemantic information in low-resolution images, we propose using prefix tokens\nto incorporate the condition. Scale-aligned Rotary Positional Encodings are\nintroduced to capture spatial structures and the diffusion refiner is utilized\nfor modeling quantization residual loss to achieve pixel-level fidelity.\nImage-based Classifier-free Guidance is proposed to guide the generation of\nmore realistic images. Furthermore, we collect large-scale data and design a\ntraining process to obtain robust generative priors. Quantitative and\nqualitative results show that VARSR is capable of generating high-fidelity and\nhigh-realism images with more efficiency than diffusion-based methods. Our\ncodes will be released at https:\/\/github.com\/qyp2000\/VARSR.",
        "Camouflaged object detection (COD) aims to distinguish hidden objects\nembedded in an environment highly similar to the object. Conventional\nvideo-based COD (VCOD) methods explicitly extract motion cues or employ complex\ndeep learning networks to handle the temporal information, which is limited by\nhigh complexity and unstable performance. In this work, we propose a green VCOD\nmethod named GreenVCOD. Built upon a green ICOD method, GreenVCOD uses long-\nand short-term temporal neighborhoods (TN) to capture joint spatial\/temporal\ncontext information for decision refinement. Experimental results show that\nGreenVCOD offers competitive performance compared to state-of-the-art VCOD\nbenchmarks.",
        "Multi-Modal Large Language Models (MLLMs) stand out in various tasks but\nstill struggle with hallucinations. While recent training-free mitigation\nmethods mostly introduce additional inference overhead via retrospection\nstrategy and contrastive decoding, we propose attention reallocation (AttnReal)\nto mitigate hallucinations with nearly zero extra cost. Our approach is\nmotivated by the key observations that, MLLM's unreasonable attention\ndistribution causes features to be dominated by historical output tokens, which\nfurther contributes to hallucinated responses because of the distribution gap\nbetween different token types. Based on the observations, AttnReal recycles\nexcessive attention from output tokens and reallocates it to visual tokens,\nwhich reduces MLLM's reliance on language priors and ensures the decoding\nprocess depends more on the visual inputs. More interestingly, we find that, by\ncontrolling the intensity of AttnReal, we can achieve a wide-range trade-off\nbetween the response faithfulness and overall performance. Comprehensive\nresults from different benchmarks validate the effectiveness of AttnReal across\nsix open-source MLLMs and three decoding strategies.",
        "Doctors are well aware that sometimes cancer treatments not only fail, but\neven work backwards, i.e. they make the treated tumor grow. In this work we\npresent a mathematical perspective on this paradox in the case of chemotherapy,\nby studying a minimally parameterized mathematical model for the system\ncomposed of the tumor and the surrounding vasculature. To this end, we will use\na system of two well-established nonlinear ordinary differential equations,\nwhich incorporates the cytotoxic (via the Norton-Simon hypothesis) and\nantiangiogenic effects of chemotherapy. Finally, we provide two theoretical\nways to avoid these anomalies.",
        "This study proposes a novel structural optimization framework based on\nquantum variational circuits, in which the multiplier acting on the\ncross-sectional area of each rod in a truss structure as an updater is used as\na design variable. Specifically, we employ a classical processor for structural\nanalysis with the finite element method, and the Quantum Approximate\nOptimization Algorithm (QAOA) is subsequently performed to update the\ncross-sectional area so that the compliance is minimized. The advantages of\nthis framework can be seen in three key aspects. First, by defining design\nvariables as multipliers, rather than simply reducing the design variable to a\nbinary candidate of inclusion or exclusion (corresponding to qubit states, ``0\"\nand ``1\"), it provides greater flexibility in adjusting the cross-sectional\narea of the rod at each iteration of the optimization process. Second, the\nmultipliers acting on rods are encoded with on-off encoding, eliminating\nadditional constraints in the convergence judgement. As a result, the objective\nfunction is in a simple format, enabling efficient optimization using\nQAOA.Third, a fixed linear ramp schedule (FLRS) for variational parameter\nsetting bypasses the classical optimization process, thereby improving the\noperational efficiency of the framework. In the two structural cases\ninvestigated in this study, the proposed approach highlights the feasibility\nand applicability potential of quantum computing in advancing engineering\ndesign and optimization. Numerical experiments have demonstrated the\neffectiveness of this framework, providing a firm foundation for future\nresearch on quantum-assisted optimization methods in engineering fields.",
        "We report the spectroscopic confirmation of the background source of the most\ndistant Einstein ring known to date, the COSMOS-Web ring. This system consists\nof a complete Einstein ring at $z=5.1$, lensed by a massive early-type galaxy\nat $z\\sim2$. The redshift $z=5.1043\\pm0.0004$ is unambiguously identified with\nour NOEMA and Keck\/MOSFIRE spectroscopy, where the NOEMA observations reveal\nthe CO(4-3) and CO(5-4) lines at $>8\\,\\sigma$, and the MOSFIRE data detect\n[O\\textsc{ii}] at $\\sim 6\\,\\sigma$. Using multi-wavelength photometry spanning\nnear-infrared to radio bands, we find that the lensed galaxy is a dust-obscured\nstarburst ($M_{\\star} \\sim 1.8\\times10^{10}\\,{\\rm M_{\\odot}}$, ${\\rm\nSFR_{IR}\\sim 60\\,{\\rm M_{\\odot}} ~yr^{-1}}$) with high star-formation\nefficiency (gas depletion time $\\tau_{\\rm dep}<100~$Myr) as indicated by the\n[C\\textsc{i}](1-0) non-detection. The redshift confirmation revalidates that\nthe total lens mass budget within the Einstein radius is fully accounted for by\nthe stellar and dark matter components, without the need of modifying the\ninitial mass function or dark matter distribution profile. This work paves the\nway for detailed studies and future follow-ups of this unique lensing system,\nproviding an ideal laboratory for studying mass distribution at $z\\sim2$ and\nphysical conditions of star formation at $z\\sim5$.",
        "We introduce CVQBoost, a novel classification algorithm that leverages early\nhardware implementing Quantum Computing Inc's Entropy Quantum Computing (EQC)\nparadigm, Dirac-3 [Nguyen et. al. arXiv:2407.04512]. We apply CVQBoost to a\nfraud detection test case and benchmark its performance against XGBoost, a\nwidely utilized ML method. Running on Dirac-3, CVQBoost demonstrates a\nsignificant runtime advantage over XGBoost, which we evaluate on\nhigh-performance hardware comprising up to 48 CPUs and four NVIDIA L4 GPUs\nusing the RAPIDS AI framework. Our results show that CVQBoost maintains\ncompetitive accuracy (measured by AUC) while significantly reducing training\ntime, particularly as dataset size and feature complexity increase. To assess\nscalability, we extend our study to large synthetic datasets ranging from 1M to\n70M samples, demonstrating that CVQBoost on Dirac-3 is well-suited for\nlarge-scale classification tasks. These findings position CVQBoost as a\npromising alternative to gradient boosting methods, offering superior\nscalability and efficiency for high-dimensional ML applications such as fraud\ndetection.",
        "The field of visually-rich document understanding, which involves interacting\nwith visually-rich documents (whether scanned or born-digital), is rapidly\nevolving and still lacks consensus on several key aspects of the processing\npipeline. In this work, we provide a comprehensive overview of state-of-the-art\napproaches, emphasizing their strengths and limitations, pointing out the main\nchallenges in the field, and proposing promising research directions.",
        "Retrieval Augmented Generation (RAG) uses vector databases to expand the\nexpertise of an LLM model without having to retrain it. This idea can be\napplied over data lakes, leading to the notion of embeddings data lakes, i.e.,\na pool of vector databases ready to be used by RAGs. The key component in these\nsystems is the indexes enabling Approximated Nearest Neighbor Search (ANNS).\nHowever, in data lakes, one cannot realistically expect to build indexes for\nevery possible dataset. In this paper, we propose an adaptive, partition-based\nindex, CrackIVF, that performs much better than up-front index building.\nCrackIVF starts answering queries by near brute force search and only expands\nas it sees enough queries. It does so by progressively adapting the index to\nthe query workload. That way, queries can be answered right away without having\nto build a full index first. After seeing enough queries, CrackIVF will produce\nan index comparable to the best of those built using conventional techniques.\nAs the experimental evaluation shows, CrackIVF can often answer more than 1\nmillion queries before other approaches have even built the index and can start\nanswering queries immediately, achieving 10-1000x faster initialization times.\nThis makes it ideal when working with cold data or infrequently used data or as\na way to bootstrap access to unseen datasets.",
        "By work of Gallardo-Kerr-Schaffler, it is known that Naruki's\ncompactification of the moduli space of marked cubic surfaces is isomorphic to\nthe normalization of the Koll\\'ar, Shepherd-Barron, and Alexeev\ncompactification parametrizing pairs\n$\\left(S,\\left(\\frac{1}{9}+\\epsilon\\right)D\\right)$, with $D$ the sum of the\n$27$ marked lines on $S$, and their stable degenerations. In the current paper,\nwe show that the normalization assumption is not necessary as we prove that\nthis KSBA compactification is smooth. Additionally, we show it is a fine moduli\nspace. This is done by studying the automorphisms and the\n$\\mathbb{Q}$-Gorenstein obstructions of the stable pairs parametrized by it.",
        "We enhance our quantitative comprehension of the complexity associated with\nboth time-optimal and time sub-optimal quantum Hamiltonian evolutions that\nconnect arbitrary source and target states on the Bloch sphere, as recently\npresented in Nucl. Phys. B1010, 116755 (2025). Initially, we examine each\nunitary Schrodinger quantum evolution selected through various metrics, such as\npath length, geodesic efficiency, speed efficiency, and the curvature\ncoefficient of the corresponding quantum-mechanical trajectory that connects\nthe source state to the target state on the Bloch sphere. Subsequently, we\nevaluate the selected evolutions using our proposed measure of complexity, as\nwell as in relation to the concept of complexity length scale. The choice of\nboth time-optimal and time sub-optimal evolutions, along with the selection of\nsource and target states, enables us to conduct pertinent sanity checks that\nseek to validate the physical relevance of the framework supporting our\nproposed complexity measure. Our research suggests that, in general, efficient\nquantum evolutions possess a lower complexity than their inefficient\ncounterparts. However, it is important to recognize that complexity is not\nsolely determined by length; in fact, longer trajectories that are adequately\ncurved may exhibit a complexity that is less than or equal to that of shorter\ntrajectories with a lower curvature coefficient.",
        "The topological Hall effect (THE) is a hallmark of a non-trivial geometric\nspin arrangement in a magnetic metal, originating from a finite scalar spin\nchirality (SSC). The associated Berry phase is often a consequence of\nnon-coplanar magnetic structures identified by multiple k-vectors. For single-k\nmagnetic structures however with zero SSC, the emergence of a finite\ntopological Hall signal presents a conceptual challenge. Here, we report that a\nfluctuation-driven mechanism involving chiral magnons is responsible for the\nobserved THE in a low-symmetry compound, monoclinic Fe3Ga4. Through neutron\nscattering experiments, we discovered several nontrivial magnetic phases in\nthis system. In our focus is the helical spiral phase at room temperature,\nwhich transforms into a transverse conical state in applied magnetic field,\nsupporting a significant THE signal up to and above room temperature. Our work\noffers a fresh perspective in the search for novel materials with intertwined\ntopological magnetic and transport properties.",
        "In this paper we briefly survey the classical problem of understanding which\nLie algebras admit a complex structure, put in the broader perspective of\nalmost complex structures with special properties. We focus on the different\nbehavior of invariant and non-invariant structures, with a special attention to\ntheir canonical bundle and Kodaira dimension. We provide new examples of\ncomputations of Kodaira dimension of invariant and non-invariant structures.",
        "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy.",
        "Image restoration problems are often ill-posed, leading to significant\nuncertainty in reconstructed images. Accurately quantifying this uncertainty is\nessential for the reliable interpretation of reconstructed images. However,\nimage restoration methods often lack uncertainty quantification capabilities.\nConformal prediction offers a rigorous framework to augment image restoration\nmethods with accurate uncertainty quantification estimates, but it typically\nrequires abundant ground truth data for calibration. This paper presents a\nself-supervised conformal prediction method for Poisson imaging problems which\nleverages Poisson Unbiased Risk Estimator to eliminate the need for ground\ntruth data. The resulting self-calibrating conformal prediction approach is\napplicable to any Poisson linear imaging problem that is ill-conditioned, and\nis particularly effective when combined with modern self-supervised image\nrestoration techniques trained directly on measurement data. The proposed\nmethod is demonstrated through numerical experiments on image denoising and\ndeblurring; its performance are comparable to supervised conformal prediction\nmethods relying on ground truth data.",
        "Control of bubble motion is essential for improving efficiency and creating\nnew functionalities in electrochemistry, heat transfer, and biomedical systems.\nPhotoresponsive surfactants enable bubble manipulation by creating surface\ntension gradients, inducing a photo-Marangoni flow under illumination, without\nneeding any engineered substrates, by leveraging a reversible switch in\nmolecular conformation. Although previous studies have demonstrated bubble\nmanipulation using photo-responsive surfactants, a comprehensive understanding\nof how fluid behavior is affected by critical parameters, such as bubble size,\nillumination, photo-switching kinetics, concentration, and adsorption\ndesorption kinetics, remains elusive. Advances have been limited by the complex\nmultiphysics processed involved, and by the fact that earth-bound experiments\ncannot study bubble photo-Marangoni dynamics without interference from bubble\nbuoyancy and photo-thermal convection. We elucidate the factors enabling fast\nphoto-Marangoni-driven bubble motion, by performing microgravity experiments,\nenabled by a bespoke photo-surfactant, complemented by a detailed modeling\nframework. We identify an optimal bubble size for migration, since smaller and\nlarger bubbles incur weaker photo-Marangoni stresses and larger drag,\nrespectively. Surfactants that switch rapidly under illumination drive fast\nmigration, provided their reverse switch (in darkness) is much slower, yet not\nnegligible. These foundational results enable the synthesis of next-generation\nphoto-surfactants and photo-Marangoni manipulation across multiphase fluid\nsystems.",
        "Graphical User Interface (GUI) Agents, powered by multimodal large language\nmodels (MLLMs), have shown great potential for task automation on computing\ndevices such as computers and mobile phones. However, existing agents face\nchallenges in multi-step reasoning and reliance on textual annotations,\nlimiting their effectiveness. We introduce \\textit{InfiGUIAgent}, an MLLM-based\nGUI Agent trained with a two-stage supervised fine-tuning pipeline. Stage 1\nenhances fundamental skills such as GUI understanding and grounding, while\nStage 2 integrates hierarchical reasoning and expectation-reflection reasoning\nskills using synthesized data to enable native reasoning abilities of the\nagents. \\textit{InfiGUIAgent} achieves competitive performance on several GUI\nbenchmarks, highlighting the impact of native reasoning skills in enhancing GUI\ninteraction for automation tasks. Resources are available at\n\\url{https:\/\/github.com\/Reallm-Labs\/InfiGUIAgent}.",
        "Algorithmic solutions have significant potential to improve decision-making\nacross various domains, from healthcare to e-commerce. However, the widespread\nadoption of these solutions is hindered by a critical challenge: the lack of\nhuman-interpretable explanations. Current approaches to Explainable AI (XAI)\npredominantly focus on complex machine learning models, often producing brittle\nand non-intuitive explanations. This project proposes a novel approach to\ndeveloping explainable algorithms by starting with optimization problems,\nspecifically the assignment problem. The developed software library enriches\nbasic algorithms with human-understandable explanations through four key\nmethodologies: generating meaningful alternative solutions, creating robust\nsolutions through input perturbation, generating concise decision trees and\nproviding reports with comprehensive explanation of the results. Currently\ndeveloped tools are often designed with specific clustering algorithms in mind,\nwhich limits their adaptability and flexibility to incorporate alternative\ntechniques. Additionally, many of these tools fail to integrate expert\nknowledge, which could enhance the clustering process by providing valuable\ninsights and context. This lack of adaptability and integration can hinder the\neffectiveness and robustness of the clustering outcomes in various\napplications. The represents a step towards making algorithmic solutions more\ntransparent, trustworthy, and accessible. By collaborating with industry\npartners in sectors such as sales, we demonstrate the practical relevance and\ntransformative potential of our approach."
      ]
    }
  },
  {
    "id":2412.16425,
    "research_type":"applied",
    "start_id":"b40",
    "start_title":"Whole Slide Imaging Versus Microscopy for Primary Diagnosis in Surgical Pathology: A Multicenter Blinded Randomized Noninferiority Study of 1992 Cases (Pivotal Study)",
    "start_abstract":"Most prior studies of primary diagnosis in surgical pathology using whole slide imaging (WSI) versus microscopy have focused on specific organ systems or included relatively few cases. The objective this study was to demonstrate that WSI is noninferior for pathology. A blinded randomized noninferiority conducted across the entire range cases (biopsies and resections, including hematoxylin eosin, immunohistochemistry, special stains) from 4 institutions original sign-out (baseline diagnosis) as reference standard. Cases were scanned, converted randomized. Sixteen pathologists interpreted by WSI, followed a wash-out period \u22654 weeks, after which read same observers other modality. Major discordances identified an adjudication panel, differences between major discordance rates both (against standard) calculated. total 1992 included, resulting 15,925 reads. rate with standard 4.9% 4.6% microscopy. difference 0.4% (95% confidence interval, -0.30% 1.01%). highest endocrine (1.8%), neoplastic kidney (1.5%), urinary bladder (1.3%), gynecologic (1.2%). Detailed analysis these revealed no instances where interpretation consistently inaccurate compared multiple observers. We conclude pathology, biopsies resections stained immunohistochemistry stains. This conclusion valid wide variety specimen types.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      ],
      "abstract":[
        "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "VideoPure: Diffusion-based Adversarial Purification for Video\n  Recognition",
        "Motion Anything: Any to Motion Generation",
        "Embed Any NeRF: Graph Meta-Networks for Neural Tasks on Arbitrary NeRF\n  Architectures",
        "Multi-modal Fusion and Query Refinement Network for Video Moment\n  Retrieval and Highlight Detection",
        "DNRSelect: Active Best View Selection for Deferred Neural Rendering",
        "MAST-Pro: Dynamic Mixture-of-Experts for Adaptive Segmentation of\n  Pan-Tumors with Knowledge-Driven Prompts",
        "SimLabel: Consistency-Guided OOD Detection with Pretrained\n  Vision-Language Models",
        "Leveraging Textual Anatomical Knowledge for Class-Imbalanced\n  Semi-Supervised Multi-Organ Segmentation",
        "Project-Probe-Aggregate: Efficient Fine-Tuning for Group Robustness",
        "Parameter Efficient Merging for Multimodal Large Language Models with\n  Complementary Parameter Adaptation",
        "EchoVideo: Identity-Preserving Human Video Generation by Multimodal\n  Feature Fusion",
        "EdgeRegNet: Edge Feature-based Multimodal Registration Network between\n  Images and LiDAR Point Clouds",
        "SliceOcc: Indoor 3D Semantic Occupancy Prediction with Vertical Slice\n  Representation",
        "CFT-RAG: An Entity Tree Based Retrieval Augmented Generation Algorithm\n  With Cuckoo Filter",
        "Polyregular Model Checking",
        "Quasinormal modes of nonthermal fixed points",
        "Constrained multi-fidelity Bayesian optimization with automatic stop\n  condition",
        "A Probabilistic WxChallenge Proposal",
        "LIFT-GS: Cross-Scene Render-Supervised Distillation for 3D Language\n  Grounding",
        "AdaptiveLog: An Adaptive Log Analysis Framework with the Collaboration\n  of Large and Small Language Model",
        "MBTSAD: Mitigating Backdoors in Language Models Based on Token Splitting\n  and Attention Distillation",
        "Movable Antenna Enhanced DF and AF Relaying Systems: Performance\n  Analysis and Optimization",
        "Connection between planetary He I $\\lambda$10830 \\AA\\ absorption and\n  extreme-ultraviolet emission of planet-host stars",
        "Low-Complexity Event Detection and Identification in Coherent\n  Correlation OTDR Measurements",
        "Assortment optimization given basket shopping behavior using the Ising\n  model",
        "NavG: Risk-Aware Navigation in Crowded Environments Based on\n  Reinforcement Learning with Guidance Points",
        "Leveraging Randomness in Model and Data Partitioning for Privacy\n  Amplification",
        "Adaptive Retrieval Without Self-Knowledge? Bringing Uncertainty Back\n  Home"
      ],
      "abstract":[
        "Recent work indicates that video recognition models are vulnerable to\nadversarial examples, posing a serious security risk to downstream\napplications. However, current research has primarily focused on adversarial\nattacks, with limited work exploring defense mechanisms. Furthermore, due to\nthe spatial-temporal complexity of videos, existing video defense methods face\nissues of high cost, overfitting, and limited defense performance. Recently,\ndiffusion-based adversarial purification methods have achieved robust defense\nperformance in the image domain. However, due to the additional temporal\ndimension in videos, directly applying these diffusion-based adversarial\npurification methods to the video domain suffers performance and efficiency\ndegradation. To achieve an efficient and effective video adversarial defense\nmethod, we propose the first diffusion-based video purification framework to\nimprove video recognition models' adversarial robustness: VideoPure. Given an\nadversarial example, we first employ temporal DDIM inversion to transform the\ninput distribution into a temporally consistent and trajectory-defined\ndistribution, covering adversarial noise while preserving more video structure.\nThen, during DDIM denoising, we leverage intermediate results at each denoising\nstep and conduct guided spatial-temporal optimization, removing adversarial\nnoise while maintaining temporal consistency. Finally, we input the list of\noptimized intermediate results into the video recognition model for multi-step\nvoting to obtain the predicted class. We investigate the defense performance of\nour method against black-box, gray-box, and adaptive attacks on benchmark\ndatasets and models. Compared with other adversarial purification methods, our\nmethod overall demonstrates better defense performance against different\nattacks. Our code is available at https:\/\/github.com\/deep-kaixun\/VideoPure.",
        "Conditional motion generation has been extensively studied in computer\nvision, yet two critical challenges remain. First, while masked autoregressive\nmethods have recently outperformed diffusion-based approaches, existing masking\nmodels lack a mechanism to prioritize dynamic frames and body parts based on\ngiven conditions. Second, existing methods for different conditioning\nmodalities often fail to integrate multiple modalities effectively, limiting\ncontrol and coherence in generated motion. To address these challenges, we\npropose Motion Anything, a multimodal motion generation framework that\nintroduces an Attention-based Mask Modeling approach, enabling fine-grained\nspatial and temporal control over key frames and actions. Our model adaptively\nencodes multimodal conditions, including text and music, improving\ncontrollability. Additionally, we introduce Text-Music-Dance (TMD), a new\nmotion dataset consisting of 2,153 pairs of text, music, and dance, making it\ntwice the size of AIST++, thereby filling a critical gap in the community.\nExtensive experiments demonstrate that Motion Anything surpasses\nstate-of-the-art methods across multiple benchmarks, achieving a 15%\nimprovement in FID on HumanML3D and showing consistent performance gains on\nAIST++ and TMD. See our project website\nhttps:\/\/steve-zeyu-zhang.github.io\/MotionAnything",
        "Neural Radiance Fields (NeRFs) have emerged as a groundbreaking paradigm for\nrepresenting 3D objects and scenes by encoding shape and appearance information\ninto the weights of a neural network. Recent works have shown how such weights\ncan be used as input to frameworks processing them to solve deep learning\ntasks. Yet, these frameworks can only process NeRFs with a specific, predefined\narchitecture. In this paper, we present the first framework that can ingest\nNeRFs with multiple architectures and perform inference on architectures unseen\nat training time. We achieve this goal by training a Graph Meta-Network in a\nrepresentation learning framework. Moreover, we show how a contrastive\nobjective is conducive to obtaining an architecture-agnostic latent space. In\nexperiments on both MLP-based and tri-planar NeRFs, our approach demonstrates\nrobust performance in classification and retrieval tasks that either matches or\nexceeds that of existing frameworks constrained to single architectures, thus\nproviding the first architecture-agnostic method to perform tasks on NeRFs by\nprocessing their weights.",
        "Given a video and a linguistic query, video moment retrieval and highlight\ndetection (MR&HD) aim to locate all the relevant spans while simultaneously\npredicting saliency scores. Most existing methods utilize RGB images as input,\noverlooking the inherent multi-modal visual signals like optical flow and\ndepth. In this paper, we propose a Multi-modal Fusion and Query Refinement\nNetwork (MRNet) to learn complementary information from multi-modal cues.\nSpecifically, we design a multi-modal fusion module to dynamically combine RGB,\noptical flow, and depth map. Furthermore, to simulate human understanding of\nsentences, we introduce a query refinement module that merges text at different\ngranularities, containing word-, phrase-, and sentence-wise levels.\nComprehensive experiments on QVHighlights and Charades datasets indicate that\nMRNet outperforms current state-of-the-art methods, achieving notable\nimprovements in MR-mAP@Avg (+3.41) and HD-HIT@1 (+3.46) on QVHighlights.",
        "Deferred neural rendering (DNR) is an emerging computer graphics pipeline\ndesigned for high-fidelity rendering and robotic perception. However, DNR\nheavily relies on datasets composed of numerous ray-traced images and demands\nsubstantial computational resources. It remains under-explored how to reduce\nthe reliance on high-quality ray-traced images while maintaining the rendering\nfidelity. In this paper, we propose DNRSelect, which integrates a reinforcement\nlearning-based view selector and a 3D texture aggregator for deferred neural\nrendering. We first propose a novel view selector for deferred neural rendering\nbased on reinforcement learning, which is trained on easily obtained rasterized\nimages to identify the optimal views. By acquiring only a few ray-traced images\nfor these selected views, the selector enables DNR to achieve high-quality\nrendering. To further enhance spatial awareness and geometric consistency in\nDNR, we introduce a 3D texture aggregator that fuses pyramid features from\ndepth maps and normal maps with UV maps. Given that acquiring ray-traced images\nis more time-consuming than generating rasterized images, DNRSelect minimizes\nthe need for ray-traced data by using only a few selected views while still\nachieving high-fidelity rendering results. We conduct detailed experiments and\nablation studies on the NeRF-Synthetic dataset to demonstrate the effectiveness\nof DNRSelect. The code will be released.",
        "Accurate tumor segmentation is crucial for cancer diagnosis and treatment.\nWhile foundation models have advanced general-purpose segmentation, existing\nmethods still struggle with: (1) limited incorporation of medical priors, (2)\nimbalance between generic and tumor-specific features, and (3) high\ncomputational costs for clinical adaptation. To address these challenges, we\npropose MAST-Pro (Mixture-of-experts for Adaptive Segmentation of pan-Tumors\nwith knowledge-driven Prompts), a novel framework that integrates dynamic\nMixture-of-Experts (D-MoE) and knowledge-driven prompts for pan-tumor\nsegmentation. Specifically, text and anatomical prompts provide domain-specific\npriors, guiding tumor representation learning, while D-MoE dynamically selects\nexperts to balance generic and tumor-specific feature learning, improving\nsegmentation accuracy across diverse tumor types. To enhance efficiency, we\nemploy Parameter-Efficient Fine-Tuning (PEFT), optimizing MAST-Pro with\nsignificantly reduced computational overhead. Experiments on multi-anatomical\ntumor datasets demonstrate that MAST-Pro outperforms state-of-the-art\napproaches, achieving up to a 5.20% improvement in average DSC while reducing\ntrainable parameters by 91.04%, without compromising accuracy.",
        "Detecting out-of-distribution (OOD) data is crucial in real-world machine\nlearning applications, particularly in safety-critical domains. Existing\nmethods often leverage language information from vision-language models (VLMs)\nto enhance OOD detection by improving confidence estimation through rich\nclass-wise text information. However, when building OOD detection score upon on\nin-distribution (ID) text-image affinity, existing works either focus on each\nID class or whole ID label sets, overlooking inherent ID classes' connection.\nWe find that the semantic information across different ID classes is beneficial\nfor effective OOD detection. We thus investigate the ability of image-text\ncomprehension among different semantic-related ID labels in VLMs and propose a\nnovel post-hoc strategy called SimLabel. SimLabel enhances the separability\nbetween ID and OOD samples by establishing a more robust image-class similarity\nmetric that considers consistency over a set of similar class labels. Extensive\nexperiments demonstrate the superior performance of SimLabel on various\nzero-shot OOD detection benchmarks. The proposed model is also extended to\nvarious VLM-backbones, demonstrating its good generalization ability. Our\ndemonstration and implementation codes are available at:\nhttps:\/\/github.com\/ShuZou-1\/SimLabel.",
        "Annotating 3D medical images demands substantial time and expertise, driving\nthe adoption of semi-supervised learning (SSL) for segmentation tasks. However,\nthe complex anatomical structures of organs often lead to significant class\nimbalances, posing major challenges for deploying SSL in real-world scenarios.\nDespite the availability of valuable prior information, such as inter-organ\nrelative positions and organ shape priors, existing SSL methods have yet to\nfully leverage these insights. To address this gap, we propose a novel approach\nthat integrates textual anatomical knowledge (TAK) into the segmentation model.\nSpecifically, we use GPT-4o to generate textual descriptions of anatomical\npriors, which are then encoded using a CLIP-based model. These encoded priors\nare injected into the segmentation model as parameters of the segmentation\nhead. Additionally, contrastive learning is employed to enhance the alignment\nbetween textual priors and visual features. Extensive experiments demonstrate\nthe superior performance of our method, significantly surpassing\nstate-of-the-art approaches. The source code will be available at:\nhttps:\/\/github.com\/Lunn88\/TAK-Semi.",
        "While image-text foundation models have succeeded across diverse downstream\ntasks, they still face challenges in the presence of spurious correlations\nbetween the input and label. To address this issue, we propose a simple\nthree-step approach,Project-Probe-Aggregate (PPA), that enables\nparameter-efficient fine-tuning for foundation models without relying on group\nannotations. Building upon the failure-based debiasing scheme, our method, PPA,\nimproves its two key components: minority samples identification and the robust\ntraining algorithm. Specifically, we first train biased classifiers by\nprojecting image features onto the nullspace of class proxies from text\nencoders. Next, we infer group labels using the biased classifier and probe\ngroup targets with prior correction. Finally, we aggregate group weights of\neach class to produce the debiased classifier. Our theoretical analysis shows\nthat our PPA enhances minority group identification and is Bayes optimal for\nminimizing the balanced group error, mitigating spurious correlations.\nExtensive experimental results confirm the effectiveness of our PPA: it\noutperforms the state-of-the-art by an average worst-group accuracy while\nrequiring less than 0.01% tunable parameters without training group labels.",
        "Fine-tuning pre-trained models with custom data leads to numerous expert\nmodels on specific tasks. Merging models into one universal model to empower\nmulti-task ability refraining from data leakage has gained popularity. With the\nexpansion in data and model size, parameter efficient tuning becomes the common\npractice for obtaining task-specific models efficiently. However, we observe\nthat existing methods designed for full fine-tuning merging fail under\nefficient tuning. To address the issues, we analyze from low-rank decomposition\nand reveal that maintaining direction and compensating for gap between singular\nvalues are crucial for efficient model merging. Consequently, we propose\nCoPA-Merging, a training-free parameter efficient merging method with\ncomplementary parameter adaptation. Specifically, we (1) prune parameters and\nconstruct scaling coefficients from inter-parameter relation to compensate for\nperformance drop from task interference and (2) perform cross-task\nnormalization to enhance unseen task generalization. We establish a benchmark\nconsisting of diverse multimodal tasks, on which we conduct experiments to\ncertificate the outstanding performance and generalizability of our method.\nAdditional study and extensive analyses further showcase the effectiveness.",
        "Recent advancements in video generation have significantly impacted various\ndownstream applications, particularly in identity-preserving video generation\n(IPT2V). However, existing methods struggle with \"copy-paste\" artifacts and low\nsimilarity issues, primarily due to their reliance on low-level facial image\ninformation. This dependence can result in rigid facial appearances and\nartifacts reflecting irrelevant details. To address these challenges, we\npropose EchoVideo, which employs two key strategies: (1) an Identity Image-Text\nFusion Module (IITF) that integrates high-level semantic features from text,\ncapturing clean facial identity representations while discarding occlusions,\nposes, and lighting variations to avoid the introduction of artifacts; (2) a\ntwo-stage training strategy, incorporating a stochastic method in the second\nphase to randomly utilize shallow facial information. The objective is to\nbalance the enhancements in fidelity provided by shallow features while\nmitigating excessive reliance on them. This strategy encourages the model to\nutilize high-level features during training, ultimately fostering a more robust\nrepresentation of facial identities. EchoVideo effectively preserves facial\nidentities and maintains full-body integrity. Extensive experiments demonstrate\nthat it achieves excellent results in generating high-quality, controllability\nand fidelity videos.",
        "Cross-modal data registration has long been a critical task in computer\nvision, with extensive applications in autonomous driving and robotics.\nAccurate and robust registration methods are essential for aligning data from\ndifferent modalities, forming the foundation for multimodal sensor data fusion\nand enhancing perception systems' accuracy and reliability. The registration\ntask between 2D images captured by cameras and 3D point clouds captured by\nLight Detection and Ranging (LiDAR) sensors is usually treated as a visual pose\nestimation problem. High-dimensional feature similarities from different\nmodalities are leveraged to identify pixel-point correspondences, followed by\npose estimation techniques using least squares methods. However, existing\napproaches often resort to downsampling the original point cloud and image data\ndue to computational constraints, inevitably leading to a loss in precision.\nAdditionally, high-dimensional features extracted using different feature\nextractors from various modalities require specific techniques to mitigate\ncross-modal differences for effective matching. To address these challenges, we\npropose a method that uses edge information from the original point clouds and\nimages for cross-modal registration. We retain crucial information from the\noriginal data by extracting edge points and pixels, enhancing registration\naccuracy while maintaining computational efficiency. The use of edge points and\nedge pixels allows us to introduce an attention-based feature exchange block to\neliminate cross-modal disparities. Furthermore, we incorporate an optimal\nmatching layer to improve correspondence identification. We validate the\naccuracy of our method on the KITTI and nuScenes datasets, demonstrating its\nstate-of-the-art performance.",
        "3D semantic occupancy prediction is a crucial task in visual perception, as\nit requires the simultaneous comprehension of both scene geometry and\nsemantics. It plays a crucial role in understanding 3D scenes and has great\npotential for various applications, such as robotic vision perception and\nautonomous driving. Many existing works utilize planar-based representations\nsuch as Bird's Eye View (BEV) and Tri-Perspective View (TPV). These\nrepresentations aim to simplify the complexity of 3D scenes while preserving\nessential object information, thereby facilitating efficient scene\nrepresentation. However, in dense indoor environments with prevalent\nocclusions, directly applying these planar-based methods often leads to\ndifficulties in capturing global semantic occupancy, ultimately degrading model\nperformance. In this paper, we present a new vertical slice representation that\ndivides the scene along the vertical axis and projects spatial point features\nonto the nearest pair of parallel planes. To utilize these slice features, we\npropose SliceOcc, an RGB camera-based model specifically tailored for indoor 3D\nsemantic occupancy prediction. SliceOcc utilizes pairs of slice queries and\ncross-attention mechanisms to extract planar features from input images. These\nlocal planar features are then fused to form a global scene representation,\nwhich is employed for indoor occupancy prediction. Experimental results on the\nEmbodiedScan dataset demonstrate that SliceOcc achieves a mIoU of 15.45% across\n81 indoor categories, setting a new state-of-the-art performance among RGB\ncamera-based models for indoor 3D semantic occupancy prediction. Code is\navailable at https:\/\/github.com\/NorthSummer\/SliceOcc.",
        "Although retrieval-augmented generation(RAG) significantly improves\ngeneration quality by retrieving external knowledge bases and integrating\ngenerated content, it faces computational efficiency bottlenecks, particularly\nin knowledge retrieval tasks involving hierarchical structures for Tree-RAG.\nThis paper proposes a Tree-RAG acceleration method based on the improved Cuckoo\nFilter, which optimizes entity localization during the retrieval process to\nachieve significant performance improvements. Tree-RAG effectively organizes\nentities through the introduction of a hierarchical tree structure, while the\nCuckoo Filter serves as an efficient data structure that supports rapid\nmembership queries and dynamic updates. The experiment results demonstrate that\nour method is much faster than naive Tree-RAG while maintaining high levels of\ngenerative quality. When the number of trees is large, our method is hundreds\nof times faster than naive Tree-RAG. Our work is available at\nhttps:\/\/github.com\/TUPYP7180\/CFT-RAG-2025.",
        "We reduce the model checking problem for a subset of Python to the\nsatisfiability of a first-order formula over finite words, which is known to be\ndecidable. The reduction is based on the theory of polyregular functions, a\nrecently developed generalization of regular languages to polynomial output\nstring-to-string functions. We implemented this reduction in a verification\ntool called PolyCheck, that can use both automata-based solvers and classical\nSMT solvers as backends.",
        "Quasinormal modes play a prominent role in relaxation of diverse physical\nsystems to equilibria, ranging from astrophysical black holes to tiny droplets\nof quark-gluon plasma at RHIC and LHC accelerators. We propose that a novel\nkind of quasinormal modes govern the direct approach to self-similar time\nevolution of nonthermal fixed points, whose relevance ranges from high energy\nphysics to cold atom gases. We utilize black hole perturbation theory\ntechniques to compute the spectrum of these far from equilibrium quasinormal\nmodes for a kinetic theory with a Focker-Planck collision kernel in isotropic\nand homogeneous states. Our conclusion is that quasinormal modes of nonthermal\nfixed points give rise to a tower of progressively more decaying power-law\ncontributions. A byproduct of our analysis is a precise determination and\nimproved understanding of the distribution function characterizing nonthermal\nfixed points.",
        "Bayesian optimization (BO) is increasingly employed in critical applications\nto find the optimal design with minimal cost. While BO is known for its sample\nefficiency, relying solely on costly high-fidelity data can still result in\nhigh costs. This is especially the case in constrained search spaces where BO\nmust not only optimize but also ensure feasibility. A related issue in the BO\nliterature is the lack of a systematic stopping criterion. To solve these\nchallenges, we develop a constrained cost-aware multi-fidelity BO (CMFBO)\nframework whose goal is to minimize overall sampling costs by utilizing\ninexpensive low-fidelity sources while ensuring feasibility. In our case, the\nconstraints can change across the data sources and may be even black-box\nfunctions. We also introduce a systematic stopping criterion that addresses the\nlong-lasting issue associated with BO's convergence assessment. Our framework\nis publicly available on GitHub through the GP+ Python package and herein we\nvalidate it's efficacy on multiple benchmark problems.",
        "The national forecasting competition WxChallenge, brainchild of Brad Illston\nat the University of Oklahoma in 2005, has become a cherished institution\nplayed across the United States each year. Participants include students,\nfaculty, alumni, and industry professionals. However, forecasts are given as\nscalar values without expression of uncertainty, probabilities being a keystone\nof meteorological forecasting today, and previous attempts to add probabilistic\nelements to WxChallenge have failed partly due to challenges in making\nprobability forecasting accessible to all, and inability to combine scores with\ndifferent units while also appropriately rewarding forecasts using proper\nscoring rules. Much of the competition's maintenance relies on dedicated\nvolunteers, highlighting need for more automation. Hence I propose three new\nfeatures: (1) automated forecast problems based on morning ensemble guidance,\nforming prediction baselines, thresholds over which the players demonstrate\nskill in their later forecast; (2) a spread betting game, where the players\nallocate 100 confidence credits to the over-under for exceeding a percentile\n(e.g., 50pc) threshold of a variable (e.g., maximum temperature) derived from\nthe ensemble baseline; and (3) a game where players distribute 100 confidence\ncredits across bins of a continuous variable (e.g., accumulated precipitation)\napproximating a probability mass function. Forecasts are evaluated using\nShannon information gained over the baseline forecast, yielding additive units\nof bits that allow score combinations of different variables and units.\nInformation gain parallels the Brier Score and is likewise a sound measure of\nskill due its punishment of hedging. This proposal objective is to augment\nWxChallenge with two new probabilistic games that are accessible,\nscientifically sound, enjoyable, and optional.",
        "Our approach to training 3D vision-language understanding models is to train\na feedforward model that makes predictions in 3D, but never requires 3D labels\nand is supervised only in 2D, using 2D losses and differentiable rendering. The\napproach is new for vision-language understanding. By treating the\nreconstruction as a ``latent variable'', we can render the outputs without\nplacing unnecessary constraints on the network architecture (e.g. can be used\nwith decoder-only models). For training, only need images and camera pose, and\n2D labels. We show that we can even remove the need for 2D labels by using\npseudo-labels from pretrained 2D models. We demonstrate this to pretrain a\nnetwork, and we finetune it for 3D vision-language understanding tasks. We show\nthis approach outperforms baselines\/sota for 3D vision-language grounding, and\nalso outperforms other 3D pretraining techniques. Project page:\nhttps:\/\/liftgs.github.io.",
        "Automated log analysis is crucial to ensure high availability and reliability\nof complex systems. The advent of LLMs in NLP has ushered in a new era of\nlanguage model-driven automated log analysis, garnering significant interest.\nWithin this field, two primary paradigms based on language models for log\nanalysis have become prominent. Small Language Models (SLMs) follow the\npre-train and fine-tune paradigm, focusing on the specific log analysis task\nthrough fine-tuning on supervised datasets. On the other hand, LLMs following\nthe in-context learning paradigm, analyze logs by providing a few examples in\nprompt contexts without updating parameters. Despite their respective\nstrengths, we notice that SLMs are more cost-effective but less powerful,\nwhereas LLMs with large parameters are highly powerful but expensive and\ninefficient. To trade-off between the performance and inference costs of both\nmodels in automated log analysis, this paper introduces an adaptive log\nanalysis framework known as AdaptiveLog, which effectively reduces the costs\nassociated with LLM while ensuring superior results. This framework\ncollaborates an LLM and a small language model, strategically allocating the\nLLM to tackle complex logs while delegating simpler logs to the SLM.\nSpecifically, to efficiently query the LLM, we propose an adaptive selection\nstrategy based on the uncertainty estimation of the SLM, where the LLM is\ninvoked only when the SLM is uncertain. In addition, to enhance the reasoning\nability of the LLM in log analysis tasks, we propose a novel prompt strategy by\nretrieving similar error-prone cases as the reference, enabling the model to\nleverage past error experiences and learn solutions from these cases. Extensive\nexperiments demonstrate that AdaptiveLog achieves state-of-the-art results\nacross different tasks, elevating the overall accuracy of log analysis while\nmaintaining cost efficiency.",
        "In recent years, attention-based models have excelled across various domains\nbut remain vulnerable to backdoor attacks, often from downloading or\nfine-tuning on poisoned datasets. Many current methods to mitigate backdoors in\nNLP models rely on the pre-trained (unfine-tuned) weights, but these methods\nfail in scenarios where the pre-trained weights are not available. In this\nwork, we propose MBTSAD, which can mitigate backdoors in the language model by\nutilizing only a small subset of clean data and does not require pre-trained\nweights. Specifically, MBTSAD retrains the backdoored model on a dataset\ngenerated by token splitting. Then MBTSAD leverages attention distillation, the\nretrained model is the teacher model, and the original backdoored model is the\nstudent model. Experimental results demonstrate that MBTSAD achieves comparable\nbackdoor mitigation performance as the methods based on pre-trained weights\nwhile maintaining the performance on clean data. MBTSAD does not rely on\npre-trained weights, enhancing its utility in scenarios where pre-trained\nweights are inaccessible. In addition, we simplify the min-max problem of\nadversarial training and visualize text representations to discover that the\ntoken splitting method in MBTSAD's first step generates Out-of-Distribution\n(OOD) data, leading the model to learn more generalized features and eliminate\nbackdoor patterns.",
        "Movable antenna (MA) has been deemed as a promising technology to flexibly\nreconfigure wireless channels by adjusting the antenna positions in a given\nlocal region. In this paper, we investigate the application of the MA\ntechnology in both decode-and-forward (DF) and amplify-and-forward (AF)\nrelaying systems, where a relay is equipped with multiple MAs to assist in the\ndata transmission between two single-antenna nodes. For the DF relaying system,\nour objective is to maximize the achievable rate at the destination by jointly\noptimizing the positions of the MAs in two stages for receiving signals from\nthe source and transmitting signals to the destination, respectively. To drive\nessential insights, we first derive a closed-form upper bound on the maximum\nachievable rate of the DF relaying system. Then, a low-complexity algorithm\nbased on projected gradient ascent (PGA) and alternating optimization (AO) is\nproposed to solve the antenna position optimization problem. For the AF\nrelaying system, our objective is to maximize the achievable rate by jointly\noptimizing the two-stage MA positions as well as the AF beamforming matrix at\nthe relay, which results in a more challenging optimization problem due to the\nintricate coupling variables. To tackle this challenge, we first reveal the\nhidden separability among the antenna position optimization in the two stages\nand the beamforming optimization. Based on such separability, we derive a\nclosed-form upper bound on the maximum achievable rate of the AF relaying\nsystem and propose a low-complexity algorithm to obtain a high-quality\nsuboptimal solution to the considered problem. Simulation results validate the\nefficacy of our theoretical analysis and demonstrate the superiority of the\nMA-enhanced relaying systems to the conventional relaying systems with\nfixed-position antennas (FPAs) and other benchmark schemes.",
        "Context. The detection of the He I 10830 A triplet in exoplanet atmospheres\nhas opened a new window for probing planetary properties, including atmospheric\nescape. Unlike Lyman alpha, the triplet is less affected by ISM absorption.\nSufficient XUV stellar irradiation may trigger the formation of the He I\ntriplet via photoionization and posterior recombination processes in the planet\natmospheres. Only a weak trend between stellar XUV and the planetary He I\nstrength has been observed so far. Aims. We aim to confirm this mechanism for\nproducing the He I absorption in exoplanetary atmospheres by examining a sample\nof planetary systems. Methods. We obtained homogeneous measurements of the\nplanetary He I line EW and consistently computed the stellar XUV ionizing\nirradiation. We first derived new coronal models for the planet-host stars. We\nused updated data from the X-exoplanets database, archival X-ray spectra of\nM-type stars (including AU Mic and Proxima Cen), and new XMM-Newton X-ray data\nobtained for the CARMENES project. These data were complemented at longer\nwavelengths with publicly available HST, FUSE, and EUVE spectra. A total of 75\nstars are carefully analyzed to obtain a new calibration between X-ray and EUV\nemission. Results. Two distinct relationships between stellar X-ray emission\n(5-100 A) and EUV_H (100-920 A) or EUV_He (100-504 A) radiation are obtained to\nscale the emission from late-type stellar coronae. A total of 48 systems with\nreported planetary He I 10830 A studies, exhibit a robust relationship between\nthe planetary He I feature and the ionizing XUV_He received by the planet,\ncorrected by stellar and planetary radii, and the planet's gravitational\npotential. Some outliers could be explained by a different atmospheric\ncomposition or the lack of planetary gaseous atmospheres. This relation may be\nused to predict the He I 10830 A absorption in exoplanet atmospheres.",
        "Pairing coherent correlation OTDR with low-complexity analysis methods, we\ninvestigate the detection of fast temperature changes and vibrations in optical\nfibers. A localization accuracy of ~2 m and extraction of vibration amplitudes\nand frequencies is demonstrated.",
        "In markets where customers tend to purchase baskets of products rather than\nsingle products, assortment optimization is a major challenge for retailers.\nRemoving a product from a retailer's assortment can result in a severe drop in\naggregate demand if this product is a complement to other products. Therefore,\naccounting for the complementarity effect is essential when making assortment\ndecisions. In this paper, we develop a modeling framework designed to address\nthis problem. We model customers' choices using a Markov random field -- in\nparticular, the Ising model -- which captures pairwise demand dependencies as\nwell as the individual attractiveness of each product. Using the Ising model\nallows us to leverage existing methodologies for various purposes including\nparameter estimation and efficient simulation of customer choices. We formulate\nthe assortment optimization problem under this model and show that its decision\nversion is NP-hard. We also provide multiple theoretical insights into the\nstructure of the optimal assortments based on the graphical representation of\nthe Ising model, and propose several heuristic algorithms that can be used to\nobtain high-quality solutions to the assortment optimization problem. Our\nnumerical analysis demonstrates that the developed simulated annealing\nprocedure leads to an expected profit gain of 15% compared to offering an\nunoptimized assortment (where all products are included) and around 5% compared\nto using a revenue-ordered heuristic algorithm.",
        "Motion planning in navigation systems is highly susceptible to upstream\nperceptual errors, particularly in human detection and tracking. To mitigate\nthis issue, the concept of guidance points--a novel directional cue within a\nreinforcement learning-based framework--is introduced. A structured method for\nidentifying guidance points is developed, consisting of obstacle boundary\nextraction, potential guidance point detection, and redundancy elimination. To\nintegrate guidance points into the navigation pipeline, a\nperception-to-planning mapping strategy is proposed, unifying guidance points\nwith other perceptual inputs and enabling the RL agent to effectively leverage\nthe complementary relationships among raw laser data, human detection and\ntracking, and guidance points. Qualitative and quantitative simulations\ndemonstrate that the proposed approach achieves the highest success rate and\nnear-optimal travel times, greatly improving both safety and efficiency.\nFurthermore, real-world experiments in dynamic corridors and lobbies validate\nthe robot's ability to confidently navigate around obstacles and robustly avoid\npedestrians.",
        "We study how inherent randomness in the training process -- where each sample\n(or client in federated learning) contributes only to a randomly selected\nportion of training -- can be leveraged for privacy amplification. This\nincludes (1) data partitioning, where a sample participates in only a subset of\ntraining iterations, and (2) model partitioning, where a sample updates only a\nsubset of the model parameters. We apply our framework to model parallelism in\nfederated learning, where each client updates a randomly selected subnetwork to\nreduce memory and computational overhead, and show that existing methods, e.g.\nmodel splitting or dropout, provide a significant privacy amplification gain\nnot captured by previous privacy analysis techniques. Additionally, we\nintroduce Balanced Iteration Subsampling, a new data partitioning method where\neach sample (or client) participates in a fixed number of training iterations.\nWe show that this method yields stronger privacy amplification than Poisson\n(i.i.d.) sampling of data (or clients). Our results demonstrate that randomness\nin the training process, which is structured rather than i.i.d. and interacts\nwith data in complex ways, can be systematically leveraged for significant\nprivacy amplification.",
        "Retrieval Augmented Generation (RAG) improves correctness of Question\nAnswering (QA) and addresses hallucinations in Large Language Models (LLMs),\nyet greatly increase computational costs. Besides, RAG is not always needed as\nmay introduce irrelevant information. Recent adaptive retrieval methods\nintegrate LLMs' intrinsic knowledge with external information appealing to LLM\nself-knowledge, but they often neglect efficiency evaluations and comparisons\nwith uncertainty estimation techniques. We bridge this gap by conducting a\ncomprehensive analysis of 35 adaptive retrieval methods, including 8 recent\napproaches and 27 uncertainty estimation techniques, across 6 datasets using 10\nmetrics for QA performance, self-knowledge, and efficiency. Our findings show\nthat uncertainty estimation techniques often outperform complex pipelines in\nterms of efficiency and self-knowledge, while maintaining comparable QA\nperformance."
      ]
    }
  },
  {
    "id":2412.16425,
    "research_type":"applied",
    "start_id":"b15",
    "start_title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
    "start_abstract":"While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b40"
      ],
      "title":[
        "Whole Slide Imaging Versus Microscopy for Primary Diagnosis in Surgical Pathology: A Multicenter Blinded Randomized Noninferiority Study of 1992 Cases (Pivotal Study)"
      ],
      "abstract":[
        "Most prior studies of primary diagnosis in surgical pathology using whole slide imaging (WSI) versus microscopy have focused on specific organ systems or included relatively few cases. The objective this study was to demonstrate that WSI is noninferior for pathology. A blinded randomized noninferiority conducted across the entire range cases (biopsies and resections, including hematoxylin eosin, immunohistochemistry, special stains) from 4 institutions original sign-out (baseline diagnosis) as reference standard. Cases were scanned, converted randomized. Sixteen pathologists interpreted by WSI, followed a wash-out period \u22654 weeks, after which read same observers other modality. Major discordances identified an adjudication panel, differences between major discordance rates both (against standard) calculated. total 1992 included, resulting 15,925 reads. rate with standard 4.9% 4.6% microscopy. difference 0.4% (95% confidence interval, -0.30% 1.01%). highest endocrine (1.8%), neoplastic kidney (1.5%), urinary bladder (1.3%), gynecologic (1.2%). Detailed analysis these revealed no instances where interpretation consistently inaccurate compared multiple observers. We conclude pathology, biopsies resections stained immunohistochemistry stains. This conclusion valid wide variety specimen types."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions",
        "Nonsuppressible viremia during HIV-1 therapy meets molecular virology",
        "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma",
        "Multilayer Networks in Neuroimaging",
        "Trait-structured chemotaxis: Exploring ligand-receptor dynamics and\n  travelling wave properties in a Keller-Segel model",
        "Origin of $\\alpha$-satellite repeat arrays from mitochondrial molecular\n  fossils -- sequential insertion, expansion, and evolution in the nuclear\n  genome",
        "Integrating protein sequence embeddings with structure via graph-based\n  deep learning for the prediction of single-residue properties",
        "Multicellular self-organization in Escherichia coli",
        "A spatially resolved and lipid-structured model for macrophage\n  populations in early human atherosclerotic lesions",
        "SPM 25: open source neuroimaging analysis software",
        "Optimal Inference of Asynchronous Boolean Network Models",
        "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma",
        "Homeostatic Kinematic Growth Model for Arteries -- Residual Stresses and\n  Active Response",
        "Modular Forms and Certain ${}_2F_1(1)$ Hypergeometric Series",
        "Signal amplification in a solid-state quantum sensor via asymmetric\n  time-reversal of many-body dynamics",
        "Flavor dependence of Energy-energy correlators",
        "Integral Ricci Curvature for Graphs",
        "Optimal Low degree hardness for Broadcasting on Trees",
        "Geodesics for Discrete manifolds",
        "Controlling spin currents with magnon interference in a canted\n  antiferromagnet",
        "Unique continuation for locally uniformly distributed measures",
        "Isospin sum rules for bottom-baryon weak decays",
        "Sampling Binary Data by Denoising through Score Functions",
        "Consonance in music -- the Pythagorean approach revisited",
        "Optimal lower Lipschitz bounds for ReLU layers, saturation, and phase\n  retrieval",
        "The Andreadakis Problem for the McCool groups",
        "Intrinsic superconducting diode effect and nonreciprocal\n  superconductivity in rhombohedral graphene multilayers",
        "On cyclotomic nearly-doubly-regular tournaments"
      ],
      "abstract":[
        "Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis.",
        "HIV-1 replication can be suppressed with antiretroviral therapy (ART), but\nindividuals who stop taking ART soon become viremic again. Some people\nexperience extended times of detectable viremia despite optimal adherence to\nART. In the issue of the JCI, White, Wu, and coauthors elucidate a source of\nnonsuppressible viremia (NSV) in treatment-adherent patients clonally expanded\nT cells harboring HIV-1 proviruses with small deletions or mutations in the\n5'-leader, the UTR that includes the major splice donor site of viral RNA.\nThese mutations altered viral RNA-splicing efficiency and RNA dimerization and\npackaging, yet still allowed production of detectable levels of noninfectious\nvirus particles. These particles lacked the HIV-1 Env surface protein required\nfor cell entry and failed to form the mature capsid cone required for\ninfectivity. These studies improve our understanding of NSV and the regulation\nof viral functions in the 5'-leader with implications for rationalized care in\nindividuals with NSV.",
        "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics.",
        "Recent advances in network science, applied to \\textit{in vivo} brain\nrecordings, have paved the way for better understanding of the structure and\nfunction of the brain. However, despite its obvious usefulness in neuroscience,\ntraditional network science lacks tools for -- so important -- simultaneous\ninvestigation of the inter-relationship between the two domains. In this\nchapter, I explore the increasing role of multilayer networks in building brain\ngenerative models and abilities of such models to uncover the full information\nabout the brain complex spatiotemporal interactions that span across multiple\nscales and modalities. First, I begin with the theoretical foundation of brain\nnetworks accompanied by a brief overview of traditional networks and their role\nin constructing multilayer network models. Then, I delve into the applications\nof multilayer networks in neuroscience, particularly in deciphering\nstructure-function relationship, modelling diseases, and integrating\nmulti-scale and multi-modal data. Finally, I demonstrate how incorporating the\nmultilayer framework into network neuroscience has brought to light previously\nhidden features of brain networks and, how multilayer networks can provide new\ninsights and a description of the structure and function of the brain.",
        "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations.",
        "Alpha satellite DNA is large tandem arrays of 150-400 bp units, and its\norigin remains an evolutionary mystery. In this research, we identified 1,545\nalpha-satellite-like (SatL) repeat units in the nuclear genome of jewel wasp\nNasonia vitripennis. Among them, thirty-nine copies of SatL were organized in\ntwo palindromic arrays in mitochondria, resulting in a 50% increase in the\ngenome size. Strikingly, genomic neighborhood analyses of 1,516 nuclear SatL\nrepeats revealed that they are located in NuMT (nuclear mitochondrial DNA)\nregions, and SatL phylogeny matched perfectly with mitochondrial genes and NuMT\npseudogenes. These results support that SatL arrays originated from ten\nindependent mitochondria insertion events into the nuclear genome within the\nlast 500,000 years, after divergence from its sister species N. giraulti.\nDramatic repeat GC-percent elevation (from 33.9% to 50.4%) is a hallmark of\nrapid SatL sequence evolution in mitochondria due to GC-biased gene conversion\nfacilitated by the palindromic sequence pairing of the two mitochondrial SatL\narrays. The nuclear SatL repeat arrays underwent substantial copy number\nexpansion, from 12-15 (SatL1) to over 400 copies (SatL4). The oldest SatL4B\narray consists of four types of repeat units derived from deletions in the\nAT-rich region of ancestral repeats, and complex high-order structures have\nevolved through duplications. We also discovered similar repeat insertions into\nthe nuclear genome of Muscidifurax, suggesting this mechanism can be common in\ninsects. This is the first report of the mitochondrial origin of nuclear\nsatellite sequences, and our findings shed new light on the origin and\nevolution of satellite DNA.",
        "Understanding the intertwined contributions of amino acid sequence and\nspatial structure is essential to explain protein behaviour. Here, we introduce\nINFUSSE (Integrated Network Framework Unifying Structure and Sequence\nEmbeddings), a Deep Learning framework that combines sequence embeddings,\ngenerated by a Large Language Model (LLM), with graph-based representations of\nprotein structures, integrated through a diffusive Graph Convolutional Network\n(diff-GCN), to predict single-residue properties within proteins. Our approach\nfollows two steps. First, we fine-tune LLM sequence embeddings obtained from\nbidirectional transformers to make predictions from protein sequence alone.\nSecond, we combine these enriched sequence representations with a geometric\ngraph Laplacian within diff-GCN to refine the initial predictions. This\napproach leads to improved predictions while allowing us to systematically\ndisentangle the contribution of sequence and structure. We illustrate our\nframework by applying it to the prediction of local residue flexibility\n(B-factors) of antibody-antigen complexes, and show that it provides improved\nperformance compared to current Machine Learning (ML) approaches. The addition\nof structural information via geometric graphs is shown to enhance predictions\nespecially for intrinsically disordered regions, protein-protein interaction\nsites, and highly variable amino acid positions.",
        "Escherichia coli has long been a trusty companion, maintaining health in our\nguts and advancing biological knowledge in the laboratory. In light of recent\nfindings, we discuss multicellular self-organization in E. coli and develop\ngeneral ideas for multicellularity, including the necessity for multicellular\ndynamics and interpretation by dynamic graphs, applicable to both unicellular\nand multicellular organisms. In this context, we next discuss the documented\nbehaviors of E. coli self-organization (rosette formation, multicellular\nextension, and attached dormancy) and two potential behaviors (internal\ncommunication and mating). Finally, by comparing the dynamic graphs for\ndifferent communities, we develop principles relevant to the theory of\nmulticellularity.",
        "Atherosclerosis is a chronic inflammatory disease of the artery wall. The\nearly stages of atherosclerosis are driven by interactions between lipids and\nmonocyte-derived-macrophages (MDMs). The mechanisms that govern the spatial\ndistribution of lipids and MDMs in the lesion remain poorly understood. In this\npaper, we develop a spatially-resolved and lipid-structured model for early\natherosclerosis. The model development and analysis are guided by images of\nhuman coronary lesions by Nakashima et al. 2007. Consistent with their\nfindings, the model predicts that lipid initially accumulates deep in the\nintima due to a spatially non-uniform LDL retention capacity. The model also\nqualitatively reproduces the global internal maxima in the Nakashima images\nonly when the MDM mobility is sufficiently sensitive to lipid content, and MDM\nlifespan sufficiently insensitive. Introducing lipid content-dependence to MDM\nmobility and mean lifespan produced minimal impact on model behaviour at early\ntimes, but strongly impacted lesion composition at steady state. Increases to\nthe sensitivity of MDM lifespan to lipid content yield lesions with fewer MDMs,\nless total lesion lipid content and reduced mean MDM infiltration depth.\nIncreases to the sensitivity of MDM mobility to lipid content also reduces the\nMDM infiltration depth, but increases the proportion of lipid-laden MDMs. We\nfind that MDM lipid content increases with spatial depth, regardless of blood\nLDL and HDL content. These results shed light on the mechanisms that drive\nspatial variation in the composition of early atherosclerotic lesions, and the\nrole of macrophage lipid content in disease progression.",
        "Statistical Parametric Mapping (SPM) is an integrated set of methods for\ntesting hypotheses about the brain's structure and function, using data from\nimaging devices. These methods are implemented in an open source software\npackage, SPM, which has been in continuous development for more than 30 years\nby an international community of developers. This paper reports the release of\nSPM 25.01, a major new version of the software that incorporates novel analysis\nmethods, optimisations of existing methods, as well as improved practices for\nopen science and software development.",
        "Associations between phenotype and genomic and epigenomic markers are often\nderived by correlation. Systems Biology aims to make more robust connections\nand uncover broader insights by modeling the cellular mechanisms that produce a\nphenotype. The question of choosing the modeling methodology is of central\nimportance. A model that does not capture biological reality closely enough\nwill not explain the system's behavior. At the same time, highly detailed\nmodels suffer from computational limitations and are likely to overfit the\ndata. Boolean networks strike a balance between complexity and descriptiveness\nand thus have received increasing interest. We previously described an\nalgorithm for fitting Boolean networks to high-throughout experimental data\nthat finds the optimal network with respect to the information in a given\ndataset. In this work, we describe a simple extension that enables the modeling\nof asynchronous dynamics, i.e. different reaction times for different network\nnodes. In addition, we present a new method for pseudo-time assignment for\nsingle-cell RNA sequencing data that is derived from the modeling procedure.\nOur approach greatly simplifies the construction of Boolean network models for\ntime-series datasets, where asynchronicity often occurs. We demonstrate our\nmethodology by integrating real data from transcriptomics experiments. These\nresults significantly expand the applicability of the Boolean network model to\nexperimental data.",
        "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics.",
        "A simple kinematic growth model for muscular arteries is presented which\nallows the incorporation of residual stresses such that a homeostatic in-vivo\nstress state under physiological loading is obtained. To this end, new\nevolution equations for growth are proposed, which avoid issues with\ninstability of final growth states known from other kinematric growth models.\nThese evolution equations are connected to a new set of growth driving forces.\nBy introducing a formulation using the principle Cauchy stresses, reasonable in\nvivo stress distributions can be obtained while ensuring realistic geometries\nof arteries after growth. To incorporate realistic Cauchy stresses for muscular\narteries under varying loading conditions, which appear in vivo, e.g., due to\nphysical activity, the growth model is combined with a newly proposed\nstretch-dependent model for smooth muscle activation. To account for the\nchanges of geometry and mechanical behavior during the growth process, an\noptimization procedure is described which leads to a more accurate\nrepresentation of an arterial ring and its mechanical behavior after the\ngrowth-related homogenization of the stresses is reached. Based thereon,\nparameters are adjusted to experimental data of a healthy middle cerebral\nartery of a rat showing that the proposed model accurately describes real\nmaterial behavior. The successful combination of growth and active response\nindicates that the new growth model can be used without problems for modeling\nactive tissues under various conditions.",
        "Using the framework relating hypergeometric motives to modular forms, we\ndefine an explicit family of weight 2 Hecke eigenforms with complex\nmultiplication. We use the theory of ${}_2F_1(1)$ hypergeometric series and\nRamanujan's theory of alternative bases to compute the exact central $L$-value\nof these Hecke eigenforms in terms of special beta values. We also show the\nintegral Fourier coefficients can be written in terms of Jacobi sums,\nreflecting a motivic relation between the hypergeometric series and the modular\nforms.",
        "Electronic spins of nitrogen vacancy (NV) centers in diamond constitute a\npromising system for micro- and nano-scale magnetic sensing, due to their\noperation under ambient conditions, ease of placement in close proximity to\nsensing targets, and biological compatibility. At high densities, the\nelectronic spins interact through dipolar coupling, which typically limits but\ncan also potentially enhance sensing performance. Here we report the\nexperimental demonstration of many-body signal amplification in a solid-state,\nroom temperature quantum sensor. Our approach utilizes time-reversed\ntwo-axis-twisting interactions, engineered through dynamical control of the\nquantization axis and Floquet engineering in a two-dimensional ensemble of NV\ncenters. Strikingly, we observe that the optimal amplification occurs when the\nbackward evolution time equals twice the forward evolution time, in sharp\ncontrast to the conventional Loschmidt echo. These observations can be\nunderstood as resulting from an underlying time-reversed mirror symmetry of the\nmicroscopic dynamics, providing key insights into signal amplification and\nopening the door towards entanglement-enhanced practical quantum sensing.",
        "Energy-energy correlators (EECs) within high energy jets serve as a key\nexperimentally accessible quantity to probe the scale and structure of the\nquark-gluon plasma (QGP) in relativistic heavy-ion collisions. The CMS\nCollaboration's first measurement of the modification to the EEC within single\ninclusive jets in Pb+Pb collisions relative to p+p collisions reveals a\nsignificant enhancement at small angles, which may arise from jet transverse\nmomentum $p_T$ selection biases due to jet energy loss. We investigate the\ndependence of jet EECs on the flavor of the initiating parton. The EEC\ndistribution of a gluon jet is broader and the peak of transition from\nperturbative to non-perturbative regime occurs at a larger angle than a quark\njet. Such flavor dependence leads to the different EECs for $\\gamma$-jets and\nsingle inclusive jets due to their different flavor composition. It is also\nresponsible for a colliding energy dependence of EECs of single inclusive jets\nat fixed jet energy. We also investigate the impact of flavor composition\nvariation on the $p_T$ dependence of the jet EEC. We further propose that a\nchange in the gluon jet fraction in A+A collisions compared to p+p can also\ncontribute to a non-negligible enhancement of the medium modified EEC at small\nangles. Using the \\textsc{Jewel} model, we predict the reduction of the gluon\njet fraction in A+A collisions and estimate its impact on the EEC.",
        "We introduce the notion of integral Ricci curvature $I_{\\kappa_0}$ for\ngraphs, which measures the amount of Ricci curvature below a given threshold\n$\\kappa_0$. We focus our attention on the Lin-Lu-Yau Ricci curvature. As\napplications, we prove a Bonnet-Myers-type diameter estimate, a Moore-type\nestimate on the number of vertices of a graph in terms of the maximum degree\n$d_M$ and diameter $D$, and a Lichnerowicz-type estimate for the first\neigenvalue $\\lambda_1$ of the Graph Laplacian, generalizing the results\nobtained by Lin, Lu, and Yau. All estimates are uniform, depending only on\ngeometric parameters like $\\kappa_0$, $I_{\\kappa_0}$, $d_M$, or $D$, and do not\nrequire the graphs to be positively curved.",
        "Broadcasting on trees is a fundamental model from statistical physics that\nplays an important role in information theory, noisy computation and\nphylogenetic reconstruction within computational biology and linguistics. While\nthis model permits efficient linear-time algorithms for the inference of the\nroot from the leaves, recent work suggests that non-trivial computational\ncomplexity may be required for inference.\n  The inference of the root state can be performed using the celebrated Belief\nPropagation (BP) algorithm, which achieves Bayes-optimal performance. Although\nBP runs in linear time using real arithmetic operations, recent research\nindicates that it requires non-trivial computational complexity using more\nrefined complexity measures.\n  Moitra, Mossel, and Sandon demonstrated such complexity by constructing a\nMarkov chain for which estimating the root better than random guessing (for\ntypical inputs) is $NC^1$-complete. Kohler and Mossel constructed chains where,\nfor trees with $N$ leaves, achieving better-than-random root recovery requires\npolynomials of degree $N^{\\Omega(1)}$. The papers above raised the question of\nwhether such complexity bounds hold generally below the celebrated\nKesten-Stigum bound.\n  In a recent work, Huang and Mossel established a general degree lower bound\nof $\\Omega(\\log N)$ below the Kesten-Stigum bound. Specifically, they proved\nthat any function expressed as a linear combination of functions of at most\n$O(log N)$ leaves has vanishing correlation with the root. In this work, we get\nan exponential improvement of this lower bound by establishing an\n$N^{\\Omega(1)}$ degree lower bound, for any broadcast process in the whole\nregime below the Kesten-Stigum bound.",
        "The geodesic flow on a finite discrete q-manifold with or without boundary is\ndefined as as a permutation of its ordered q-simplices. This allows to define\ngeodesic sheets and a notion of sectional curvature.",
        "Controlling spin current lies at the heart of spintronics and its\napplications. The sign of spin currents is monotonous in ferromagnets once the\ncurrent direction is determined. Spin currents in antiferromagnets can possess\nopposite polarization, but requires enormous magnetic fields to lift the\ndegeneracy. Controlling spin currents with different polarization is urgently\ndemanded but remains hitherto elusive. Here, we demonstrate the control of spin\ncurrents at room temperature by magnon interference in a canted\nantiferromagnet, hematite recently also classified as an altermagnet.\nMagneto-optical characterization by Brillouin light scattering revealed that\nthe spatial periodicity of the beating patterns was tunable via the microwave\nfrequency. The inverse spin-Hall voltage changed sign as the frequency was\nscanned, i.e., a frequency-controlled switching of polarization in pure spin\ncurrents was obtained. Our work marks the use of antiferromagnetic magnon\ninterference to control spin currents, which substantially extends the horizon\nfor the emerging field of coherent antiferromagnetic spintronics.",
        "In this note we show that the support of a locally $k$-uniform measure in\n$\\mathbb R^{n+1}$ satisfies a kind of unique continuation property. As a\nconsequence, we show that locally uniformly distributed measures satisfy a\nweaker unique continuation property. This continues work of Kirchheim and\nPreiss (Math. Scand. 2002) and David, Kenig and Toro (Comm. Pure Appl. Math.\n2001) and lends additional evidence to the conjecture proposed by Kowalski and\nPreiss (J. Reine Angew. Math. 1987) that each connected component of the\nsupport of a locally $n$-uniform measure in $\\mathbb R^{n+1}$ is contained in\nthe zero set of a quadratic polynomial.",
        "Isospin symmetry, as the most precise flavor symmetry, can be used to extract\ninformation about hadronic dynamics. The effective Hamiltonian operators of\nbottom quark weak decays are zero under a series of isospin lowering operators\n$I_-^n$, which permits us to generate isospin sum rules without the\nWigner-Eckhart invariants. In this work, we derive hundreds of isospin sum\nrules for the two- and three-body non-leptonic decays of bottom baryons. They\nprovide hints for new decay modes and the isospin partners of pentaquark\nstates.",
        "Gaussian smoothing combined with a probabilistic framework for denoising via\nthe empirical Bayes formalism, i.e., the Tweedie-Miyasawa formula (TMF), are\nthe two key ingredients in the success of score-based generative models in\nEuclidean spaces. Smoothing holds the key for easing the problem of learning\nand sampling in high dimensions, denoising is needed for recovering the\noriginal signal, and TMF ties these together via the score function of noisy\ndata. In this work, we extend this paradigm to the problem of learning and\nsampling the distribution of binary data on the Boolean hypercube by adopting\nBernoulli noise, instead of Gaussian noise, as a smoothing device. We first\nderive a TMF-like expression for the optimal denoiser for the Hamming loss,\nwhere a score function naturally appears. Sampling noisy binary data is then\nachieved using a Langevin-like sampler which we theoretically analyze for\ndifferent noise levels. At high Bernoulli noise levels sampling becomes easy,\nakin to log-concave sampling in Euclidean spaces. In addition, we extend the\nsequential multi-measurement sampling of Saremi et al. (2024) to the binary\nsetting where we can bring the \"effective noise\" down by sampling multiple\nnoisy measurements at a fixed noise level, without the need for continuous-time\nstochastic processes. We validate our formalism and theoretical findings by\nexperiments on synthetic data and binarized images.",
        "The Pythagorean school attributed consonance in music to simplicity of\nfrequency ratios between musical tones. In the last two centuries, the\nconsonance curves developed by Helmholtz, Plompt and Levelt shifted focus to\npsycho-acoustic considerations in perceiving consonances. The appearance of\npeaks of these curves at the ratios considered by the Pythagorean school, and\nwhich were a consequence of an attempt to understand the world by nice\nmathematical proportions, remained a curiosity. This paper addresses this\ncuriosity, by describing a mathematical model of musical sound, along with a\nmathematical definition of consonance. First, we define pure, complex and mixed\ntones as mathematical models of musical sound. By a sequence of numerical\nexperiments and analytic calculations, we show that continuous cosine\nsimilarity, abbreviated as cosim, applied to these models quantifies the\nelusive concept of consonance as a frequency ratio which gives a local maximum\nof the cosim function. We prove that these maxima occur at the ratios\nconsidered as consonant in classical music theory. Moreover, we provide a\nsimple explanation why the number of musical intervals considered as consonant\nby musicians is finite, but has been increasing over the centuries.\nSpecifically, our formulas show that the number of consonant intervals changes\nwith the depth of the tone (the number of harmonics present).",
        "The injectivity of ReLU layers in neural networks, the recovery of vectors\nfrom clipped or saturated measurements, and (real) phase retrieval in\n$\\mathbb{R}^n$ allow for a similar problem formulation and characterization\nusing frame theory. In this paper, we revisit all three problems with a unified\nperspective and derive lower Lipschitz bounds for ReLU layers and clipping\nwhich are analogous to the previously known result for phase retrieval and are\noptimal up to a constant factor.",
        "In this short paper, we show that the McCool group does not satisfy the\nAndreadakis equality from degree $7$, and we give a lower bound for the size of\nthe difference between the two relevant filtrations. As a consequence, we see\nthat the Andreadakis problem for the McCool group does not stabilize.",
        "Rhombohedral tetralayer graphene has recently emerged as an exciting platform\nfor a possible chiral superconducting state. Here, we theoretically demonstrate\nand study the emergence of nonreciprocal superconductivity and an intrinsic\nsuperconducting diode effect in this system. Our results are based on a fully\nself-consistent framework for determining the superconducting order parameter\nfrom a Kohn-Luttinger mechanism to superconductivity and show that large diode\nefficiencies, $\\sim$ 60%, are achievable and highly tunable by an external\ndisplacement field. Moreover, we also find that the diodicity shows a\ncharacteristic angular dependence with multiple enhanced lobes, which depend on\nthe Fermi surface structure of the underlying normal state. Hence, our results\nsuggest that the intrinsic superconducting diode effect could provide insights\ninto the type of Fermi surface topology from which superconductivity arises.",
        "Nearly-doubly-regular tournaments have played significant roles in extremal\ngraph theory. In this note, we construct new cyclotomic nearly-doubly-regular\ntournaments and determine their spectrum by establishing a new connection\nbetween cyclotomic nearly-doubly-regular tournaments and almost difference sets\nfrom combinatorial design theory. Furthermore, under the celebrated\nHardy-Littlewood conjecture F in analytic number theory, our results confirm\nthe conjecture due to Sergey Savchenko (J. Graph Theory {\\bf 83} (2016),\n44--77) on the existence of infinitely many nearly-doubly-regular tournaments\nwith the canonical spectrum."
      ]
    }
  }
]