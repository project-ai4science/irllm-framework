{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Code for IDR Indentification, Classification, and Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:06<00:00,  1.20s/it]\n"
     ]
    }
   ],
   "source": [
    "from tasks import TaskHandler\n",
    "from evaluation_v2 import Evaluator\n",
    "from utils import load_config\n",
    "\n",
    "\n",
    "# if you want to change model params, logprob, output paths, etc. go to config.yml\n",
    "config_path = './config.yml'\n",
    "CONFIG = load_config(config_path)\n",
    "task_config = CONFIG['task_config']\n",
    "\n",
    "# user decide which exp to run\n",
    "exp_tasks = [\"identification\", \"classification\", \"recommendation\", \"generation\", \"output_evaluation\"]\n",
    "current_task = exp_tasks[1]\n",
    "eval_type = None\n",
    "provider, model_name = \"gpt\", \"gpt-4o-mini-2024-07-18\"\n",
    "\n",
    "# map the args to actual task\n",
    "if current_task == \"output_evaluation\":\n",
    "    eval_type = eval_type\n",
    "    evaluator = Evaluator(type=eval_type)\n",
    "else:\n",
    "    # task_handler = TaskHandler(provider=args.provider, model_name=args.model_name, lm_config_path=config_path, save_path=save_path, **task_config)\n",
    "    task_handler = TaskHandler(provider=provider, model_name=model_name, lm_config_path=config_path, **task_config)\n",
    "    task_func = task_handler[current_task]\n",
    "    task_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swiss Tournament Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Round 1 ---\n",
      "Alice wins against Bob\n",
      "David wins against Charlie\n",
      "May wins against Eva\n",
      "\n",
      "Standings after Round 1\n",
      "Alice (Score: 1)\n",
      "David (Score: 1)\n",
      "May (Score: 1)\n",
      "Bob (Score: 0)\n",
      "Charlie (Score: 0)\n",
      "Eva (Score: 0)\n",
      "\n",
      "--- Round 2 ---\n",
      "David wins against Alice\n",
      "Bob wins against May\n",
      "Charlie wins against Eva\n",
      "\n",
      "Standings after Round 2\n",
      "David (Score: 2)\n",
      "Alice (Score: 1)\n",
      "Bob (Score: 1)\n",
      "Charlie (Score: 1)\n",
      "May (Score: 1)\n",
      "Eva (Score: 0)\n",
      "\n",
      "--- Round 3 ---\n",
      "Bob wins against David\n",
      "Alice wins against Charlie\n",
      "\n",
      "Standings after Round 3\n",
      "Alice (Score: 2)\n",
      "Bob (Score: 2)\n",
      "David (Score: 2)\n",
      "Charlie (Score: 1)\n",
      "May (Score: 1)\n",
      "Eva (Score: 0)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class Player:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.score = 0\n",
    "        self.opponents = []  # Keep track of player names this player has already faced\n",
    "        self.bye = False     # Flag to check if this player already received a bye\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.name} (Score: {self.score})\"\n",
    "\n",
    "def swiss_pairings(players):\n",
    "    \"\"\"\n",
    "    Pair players based on current scores.\n",
    "    If the number of players is odd, a bye is given to the lowest-ranked\n",
    "    player who hasn't already received one.\n",
    "    \"\"\"\n",
    "    # Sort players first by score (highest first), then alphabetically\n",
    "    players_sorted = sorted(players, key=lambda x: (-x.score, x.name))\n",
    "    pairings = []\n",
    "    used = set()  # Keep track of players already paired this round\n",
    "\n",
    "    # Handle odd number of players: give a bye to one player\n",
    "    if len(players_sorted) % 2 == 1:\n",
    "        # choose the lowest-ranked player (last in the sorted list)\n",
    "        for player in reversed(players_sorted):\n",
    "            if not player.bye:\n",
    "                print(f\"{player.name} receives a bye this round.\")\n",
    "                player.score += 1   # Award one point for the bye\n",
    "                player.bye = True\n",
    "                used.add(player)\n",
    "                break\n",
    "\n",
    "    # Pair remaining players\n",
    "    # We loop through the sorted list and for each unpaired player, \n",
    "    # find the next unpaired opponent that they haven't faced before.\n",
    "    for i, player in enumerate(players_sorted):\n",
    "        if player in used:\n",
    "            continue\n",
    "        for j in range(i + 1, len(players_sorted)):\n",
    "            opponent = players_sorted[j]\n",
    "            if opponent in used:\n",
    "                continue\n",
    "            # Pair if the two players haven't met yet\n",
    "            if opponent.name not in player.opponents:\n",
    "                pairings.append((player, opponent))\n",
    "                used.add(player)\n",
    "                used.add(opponent)\n",
    "                break\n",
    "\n",
    "    return pairings\n",
    "\n",
    "def simulate_match(player1, player2):\n",
    "    \"\"\"\n",
    "    Simulate a match between two players by choosing a random winner.\n",
    "    Update their scores and record the matchup.\n",
    "    \"\"\"\n",
    "    winner = random.choice([player1, player2])\n",
    "    if winner == player1:\n",
    "        player1.score += 1\n",
    "        print(f\"{player1.name} wins against {player2.name}\")\n",
    "    else:\n",
    "        player2.score += 1\n",
    "        print(f\"{player2.name} wins against {player1.name}\")\n",
    "    # Record that these players have met\n",
    "    player1.opponents.append(player2.name)\n",
    "    player2.opponents.append(player1.name)\n",
    "\n",
    "def swiss_tournament(players, rounds=3):\n",
    "    \"\"\"\n",
    "    Run a Swiss tournament for a specified number of rounds.\n",
    "    Each round, pair players according to their scores and then simulate their matches.\n",
    "    After each round, print the current standings.\n",
    "    \"\"\"\n",
    "    for round_number in range(1, rounds + 1):\n",
    "        print(f\"\\n--- Round {round_number} ---\")\n",
    "        pairings = swiss_pairings(players)\n",
    "        # Simulate all the matches in this round\n",
    "        for p1, p2 in pairings:\n",
    "            simulate_match(p1, p2)\n",
    "        \n",
    "        # Print the standings after this round\n",
    "        standings = sorted(players, key=lambda x: (-x.score, x.name))\n",
    "        print(\"\\nStandings after Round\", round_number)\n",
    "        for player in standings:\n",
    "            print(player)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a list of players for the tournament\n",
    "    player_names = [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eva\", \"May\"]\n",
    "    players = [Player(name) for name in player_names]\n",
    "    \n",
    "    # Run the Swiss tournament for 3 rounds\n",
    "    swiss_tournament(players, rounds=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verdict': 'Yes', 'score': '90'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "data = None\n",
    "text = \"Your verdict:Yes.\\nConfidence score:90.\\nhhl Reason:The response is correct.\"\n",
    "pattern = r\"Your verdict:\\s*(?P<verdict>Yes|No).?\\s*Confidence score:\\s*(?P<score>\\d+).?\"\n",
    "match = re.search(pattern, text, re.IGNORECASE)\n",
    "if match:\n",
    "    data = match.groupdict()\n",
    "\n",
    "if data:\n",
    "    print(data)\n",
    "else:\n",
    "    print(\"No match found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ECE Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file critical_exp_2_1_gpt-4o-mini-2024-07-18.json...\n",
      "ECE for critical_exp_2_1_gpt-4o-mini-2024-07-18.json: 0.2166\n",
      "\n",
      "Plot saved to ./eval_output/critical_exp_2_1_gpt-4o-mini-2024-07-18.png\n",
      "Reading file critical_exp_1_o3-mini-2025-01-31.json...\n",
      "ECE for critical_exp_1_o3-mini-2025-01-31.json: 0.2661\n",
      "\n",
      "Plot saved to ./eval_output/critical_exp_1_o3-mini-2025-01-31.png\n",
      "Reading file critical_exp_2_1_o3-mini-2025-01-31.json...\n",
      "ECE for critical_exp_2_1_o3-mini-2025-01-31.json: 0.4507\n",
      "\n",
      "Plot saved to ./eval_output/critical_exp_2_1_o3-mini-2025-01-31.png\n",
      "Reading file critical_exp_2_3_o1-mini-2024-09-12.json...\n",
      "ECE for critical_exp_2_3_o1-mini-2024-09-12.json: 0.1585\n",
      "\n",
      "Plot saved to ./eval_output/critical_exp_2_3_o1-mini-2024-09-12.png\n",
      "Reading file critical_exp_2_1_o1-mini-2024-09-12.json...\n",
      "ECE for critical_exp_2_1_o1-mini-2024-09-12.json: 0.2200\n",
      "\n",
      "Plot saved to ./eval_output/critical_exp_2_1_o1-mini-2024-09-12.png\n",
      "Reading file critical_exp_2_3_gpt-4o-mini-2024-07-18.json...\n",
      "ECE for critical_exp_2_3_gpt-4o-mini-2024-07-18.json: 0.0504\n",
      "\n",
      "Plot saved to ./eval_output/critical_exp_2_3_gpt-4o-mini-2024-07-18.png\n",
      "Reading file critical_exp_2_3_o3-mini-2025-01-31.json...\n",
      "ECE for critical_exp_2_3_o3-mini-2025-01-31.json: 0.6816\n",
      "\n",
      "Plot saved to ./eval_output/critical_exp_2_3_o3-mini-2025-01-31.png\n",
      "Reading file critical_exp_1_llama-3.1-70b-instruct.json...\n",
      "ECE for critical_exp_1_llama-3.1-70b-instruct.json: 0.0925\n",
      "\n",
      "Plot saved to ./eval_output/critical_exp_1_llama-3.1-70b-instruct.png\n",
      "Reading file critical_exp_1_gpt-4o-mini-2024-07-18.json...\n",
      "ECE for critical_exp_1_gpt-4o-mini-2024-07-18.json: 0.0571\n",
      "\n",
      "Plot saved to ./eval_output/critical_exp_1_gpt-4o-mini-2024-07-18.png\n",
      "Reading file critical_exp_1_o1-mini-2024-09-12.json...\n",
      "ECE for critical_exp_1_o1-mini-2024-09-12.json: 0.1819\n",
      "\n",
      "Plot saved to ./eval_output/critical_exp_1_o1-mini-2024-09-12.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_ece_and_plot(file_path, output_path, type=None):\n",
    "    \"\"\"\n",
    "    Reads JSON files from the given file path, calculates ECE, and saves plots to the output path.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the directory containing JSON files.\n",
    "        output_path (str): Path to the directory where plots will be saved.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    files = os.listdir(file_path)\n",
    "    files = [f for f in files if f.endswith('.json')]\n",
    "\n",
    "    for f in files:\n",
    "        print(f\"Reading file {f}...\")\n",
    "        # Read single file\n",
    "        df = pd.read_json(os.path.join(file_path, f))\n",
    "        output = df[\"verb_conf\"]\n",
    "        bins = np.linspace(0, 100, 10)\n",
    "        bin_indices = np.digitize(output, bins=bins)\n",
    "        df[\"adj_correct\"] = df.apply(lambda row: row[\"y_true\"] == row[\"y_pred\"], axis=1)\n",
    "\n",
    "        bin_acc = {}\n",
    "        for i, b in enumerate(bin_indices):\n",
    "            which_bin = 0.1 * np.average([b - 1, b])\n",
    "            if which_bin in bin_acc:\n",
    "                bin_acc[which_bin].append(df[\"adj_correct\"].iloc[i])\n",
    "            else:\n",
    "                bin_acc[which_bin] = [df[\"adj_correct\"].iloc[i]]\n",
    "\n",
    "        # Calculate ECE for each bin\n",
    "        ece = 0\n",
    "        for key in bin_acc:\n",
    "            if key > 0.05:\n",
    "                ece += len(bin_acc[key]) / len(df) * np.abs(np.average(bin_acc[key]) - key)\n",
    "\n",
    "        print(f\"ECE for {f}: {format(ece, '.4f')}\\n\")\n",
    "\n",
    "        # Prepare data for plotting\n",
    "        plt_data = sorted(bin_acc.items())\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.title(f\"ECE for {f}: {format(ece, '.4f')}\\n\", fontsize=25)\n",
    "        plt.plot(np.linspace(0, 1, 20), np.linspace(0, 1, 20), linestyle='dashed')\n",
    "        plt.scatter([d[0] for d in plt_data], [np.average(d[1]) for d in plt_data],\n",
    "                    s=[len(d[1]) / len(df) * 10000 for d in plt_data], alpha=0.5, color='red', marker='.')\n",
    "\n",
    "        plt.xlabel(\"Confidence Bin\", fontsize=20)\n",
    "        plt.ylabel(\"Accuracy\", fontsize=20)\n",
    "        plt.xticks(fontsize=20)\n",
    "        plt.yticks(fontsize=20)\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Save the plot\n",
    "        if type:\n",
    "            output_file_name = f\"{os.path.splitext(f)[0]}_{type}.png\"\n",
    "        else:\n",
    "            output_file_name = f\"{os.path.splitext(f)[0]}.png\"\n",
    "        output_file = os.path.join(output_path, output_file_name)\n",
    "        plt.savefig(output_file)\n",
    "        plt.close()\n",
    "        print(f\"Plot saved to {output_file}\")\n",
    "\n",
    "input_path = './output/round_2_critical'\n",
    "output_path = './eval_output'\n",
    "calculate_ece_and_plot(input_path, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
