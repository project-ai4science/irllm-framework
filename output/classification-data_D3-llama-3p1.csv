paper_a_id,method,research_type,paper_b_id,paper_b_text,paper_c_id,paper_c_text,y_true,y_pred,reason,paperA
2411.01019,1,applied,2411.01019-pos1-0,"Incidental Anterior Mediastinal Nodular Lesions on Chest CT in Asymptomatic Subjects; Screening for lung cancer: 2023 guideline update from the American Cancer Society; Objective The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods We included 56,358 consecutive participants (mean age 52.4 ± 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66–0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01–3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes.; Abstract Lung cancer is the leading cause of mortality and person‐years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network‐validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit‐to‐radiation‐risk ratios follow‐up examinations. GDG also examined disease burden data National Institute’s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50–80 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low‐dose computed tomography asymptomatic currently smoke or formerly smoked have ≥20 pack‐year ( , ). Before decision made initiate LCS, should engage shared decision‐making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",2411.01019-pos2-0,"Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms; Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability.",True,True,"Combining the medical knowledge from Paper 1 on incidental anterior mediastinal nodular lesions and the AI-powered segmentation technique from Paper 2, a novel multidisciplinary research idea can be created to develop an automated system for early detection and diagnosis of thymic epithelial tumors using low-dose chest CT scans.",
2411.01019,1,applied,2411.01019-pos1-0,"Incidental Anterior Mediastinal Nodular Lesions on Chest CT in Asymptomatic Subjects; Screening for lung cancer: 2023 guideline update from the American Cancer Society; Objective The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods We included 56,358 consecutive participants (mean age 52.4 ± 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66–0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01–3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes.; Abstract Lung cancer is the leading cause of mortality and person‐years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network‐validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit‐to‐radiation‐risk ratios follow‐up examinations. GDG also examined disease burden data National Institute’s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50–80 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low‐dose computed tomography asymptomatic currently smoke or formerly smoked have ≥20 pack‐year ( , ). Before decision made initiate LCS, should engage shared decision‐making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",2502.02377,"A Minimax Approach to Ad Hoc Teamwork; We propose a minimax-Bayes approach to Ad Hoc Teamwork (AHT) that optimizes
policies against an adversarial prior over partners, explicitly accounting for
uncertainty about partners at time of deployment. Unlike existing methods that
assume a specific distribution over partners, our approach improves worst-case
performance guarantees. Extensive experiments, including evaluations on
coordinated cooking tasks from the Melting Pot suite, show our method's
superior robustness compared to self-play, fictitious play, and best response
learning. Our work highlights the importance of selecting an appropriate
training distribution over teammates to achieve robustness in AHT.",False,False,"Although both papers present interesting ideas, they come from distinct areas (medical imaging and artificial intelligence) with little overlap. The concepts from these papers do not lend themselves to a straightforward combination that would meet the standards of interdisciplinary research.",
2411.01019,1,applied,2411.01019-pos1-0,"Incidental Anterior Mediastinal Nodular Lesions on Chest CT in Asymptomatic Subjects; Screening for lung cancer: 2023 guideline update from the American Cancer Society; Objective The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods We included 56,358 consecutive participants (mean age 52.4 ± 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66–0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01–3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes.; Abstract Lung cancer is the leading cause of mortality and person‐years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network‐validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit‐to‐radiation‐risk ratios follow‐up examinations. GDG also examined disease burden data National Institute’s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50–80 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low‐dose computed tomography asymptomatic currently smoke or formerly smoked have ≥20 pack‐year ( , ). Before decision made initiate LCS, should engage shared decision‐making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",2502.1961,"Program Synthesis Dialog Agents for Interactive Decision-Making; Many real-world eligibility problems, ranging from medical diagnosis to tax
planning, can be mapped to decision problems expressed in natural language,
wherein a model must make a binary choice based on user features. Large-scale
domains such as legal codes or frequently updated funding opportunities render
human annotation (e.g., web forms or decision trees) impractical, highlighting
the need for agents that can automatically assist in decision-making. Since
relevant information is often only known to the user, it is crucial that these
agents ask the right questions. As agents determine when to terminate a
conversation, they face a trade-off between accuracy and the number of
questions asked, a key metric for both user experience and cost. To evaluate
this task, we propose BeNYfits, a new benchmark for determining user
eligibility for multiple overlapping social benefits opportunities through
interactive decision-making. Our experiments show that current language models
struggle with frequent hallucinations, with GPT-4o scoring only 35.7 F1 using a
ReAct-style chain-of-thought. To address this, we introduce ProADA, a novel
approach that leverages program synthesis to assist in decision-making by
mapping dialog planning to a code generation problem and using gaps in
structured data to determine the best next action. Our agent, ProADA, improves
the F1 score to 55.6 while maintaining nearly the same number of dialog turns.",False,True,"I extracted concepts from both papers to create a novel multidisciplinary research idea. By combining the medical domain knowledge from Paper 1 with the program synthesis dialog agents from Paper 2, a potential research idea is to develop an AI-powered decision-making system for lung cancer screening eligibility, which can interactively ask patients questions to determine their eligibility for screening.",
2411.01019,1,applied,2411.01019-pos1-0,"Incidental Anterior Mediastinal Nodular Lesions on Chest CT in Asymptomatic Subjects; Screening for lung cancer: 2023 guideline update from the American Cancer Society; Objective The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods We included 56,358 consecutive participants (mean age 52.4 ± 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66–0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01–3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes.; Abstract Lung cancer is the leading cause of mortality and person‐years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network‐validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit‐to‐radiation‐risk ratios follow‐up examinations. GDG also examined disease burden data National Institute’s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50–80 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low‐dose computed tomography asymptomatic currently smoke or formerly smoked have ≥20 pack‐year ( , ). Before decision made initiate LCS, should engage shared decision‐making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",2502.1062,"ProMRVL-CAD: Proactive Dialogue System with Multi-Round Vision-Language
  Interactions for Computer-Aided Diagnosis; Recent advancements in large language models (LLMs) have demonstrated
extraordinary comprehension capabilities with remarkable breakthroughs on
various vision-language tasks. However, the application of LLMs in generating
reliable medical diagnostic reports remains in the early stages. Currently,
medical LLMs typically feature a passive interaction model where doctors
respond to patient queries with little or no involvement in analyzing medical
images. In contrast, some ChatBots simply respond to predefined queries based
on visual inputs, lacking interactive dialogue or consideration of medical
history. As such, there is a gap between LLM-generated patient-ChatBot
interactions and those occurring in actual patient-doctor consultations. To
bridge this gap, we develop an LLM-based dialogue system, namely proactive
multi-round vision-language interactions for computer-aided diagnosis
(ProMRVL-CAD), to generate patient-friendly disease diagnostic reports. The
proposed ProMRVL-CAD system allows proactive dialogue to provide patients with
constant and reliable medical access via an integration of knowledge graph into
a recommendation system. Specifically, we devise two generators: a Proactive
Question Generator (Pro-Q Gen) to generate proactive questions that guide the
diagnostic procedure and a Multi-Vision Patient-Text Diagnostic Report
Generator (MVP-DR Gen) to produce high-quality diagnostic reports. Evaluating
two real-world publicly available datasets, MIMIC-CXR and IU-Xray, our model
has better quality in generating medical reports. We further demonstrate the
performance of ProMRVL achieves robust under the scenarios with low image
quality. Moreover, we have created a synthetic medical dialogue dataset that
simulates proactive diagnostic interactions between patients and doctors,
serving as a valuable resource for training LLM.",False,True,"The combination of ideas from both papers can lead to a novel multidisciplinary research project that integrates AI-powered computer-aided diagnosis with lung cancer screening. The ProMRVL-CAD system can be applied to analyze CT scans and generate diagnostic reports, while incorporating the guidelines and findings from the lung cancer screening study to improve early detection and treatment outcomes.",
2411.01019,1,applied,2411.01019-pos1-0,"Incidental Anterior Mediastinal Nodular Lesions on Chest CT in Asymptomatic Subjects; Screening for lung cancer: 2023 guideline update from the American Cancer Society; Objective The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods We included 56,358 consecutive participants (mean age 52.4 ± 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66–0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01–3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes.; Abstract Lung cancer is the leading cause of mortality and person‐years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network‐validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit‐to‐radiation‐risk ratios follow‐up examinations. GDG also examined disease burden data National Institute’s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50–80 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low‐dose computed tomography asymptomatic currently smoke or formerly smoked have ≥20 pack‐year ( , ). Before decision made initiate LCS, should engage shared decision‐making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",2502.14491,"Statistical Scenario Modelling and Lookalike Distributions for
  Multi-Variate AI Risk; Evaluating AI safety requires statistically rigorous methods and risk metrics
for understanding how the use of AI affects aggregated risk. However, much AI
safety literature focuses upon risks arising from AI models in isolation,
lacking consideration of how modular use of AI affects risk distribution of
workflow components or overall risk metrics. There is also a lack of
statistical grounding enabling sensitisation of risk models in the presence of
absence of AI to estimate causal contributions of AI. This is in part due to
the dearth of AI impact data upon which to fit distributions. In this work, we
address these gaps in two ways. First, we demonstrate how scenario modelling
(grounded in established statistical techniques such as Markov chains, copulas
and Monte Carlo simulation) can be used to model AI risk holistically. Second,
we show how lookalike distributions from phenomena analogous to AI can be used
to estimate AI impacts in the absence of directly observable data. We
demonstrate the utility of our methods for benchmarking cumulative AI risk via
risk analysis of a logistic scenario simulations.",False,True,"Combining concepts from both papers, a novel multidisciplinary research idea could be developed to create a statistical scenario modeling framework for evaluating the risk of AI-assisted diagnosis of incidental nodular lesions in lung cancer screening. This idea integrates concepts from medical imaging, AI safety, and statistical modeling to advance our understanding of the benefits and risks of AI-assisted diagnosis in lung cancer screening.",
2411.01019,1,applied,2411.01019-pos1-0,"Incidental Anterior Mediastinal Nodular Lesions on Chest CT in Asymptomatic Subjects; Screening for lung cancer: 2023 guideline update from the American Cancer Society; Objective The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods We included 56,358 consecutive participants (mean age 52.4 ± 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66–0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01–3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes.; Abstract Lung cancer is the leading cause of mortality and person‐years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network‐validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit‐to‐radiation‐risk ratios follow‐up examinations. GDG also examined disease burden data National Institute’s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50–80 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low‐dose computed tomography asymptomatic currently smoke or formerly smoked have ≥20 pack‐year ( , ). Before decision made initiate LCS, should engage shared decision‐making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",2502.10906,"PCGRLLM: Large Language Model-Driven Reward Design for Procedural
  Content Generation Reinforcement Learning; Reward design plays a pivotal role in the training of game AIs, requiring
substantial domain-specific knowledge and human effort. In recent years,
several studies have explored reward generation for training game agents and
controlling robots using large language models (LLMs). In the content
generation literature, there has been early work on generating reward functions
for reinforcement learning agent generators. This work introduces PCGRLLM, an
extended architecture based on earlier work, which employs a feedback mechanism
and several reasoning-based prompt engineering techniques. We evaluate the
proposed method on a story-to-reward generation task in a two-dimensional
environment using two state-of-the-art LLMs, demonstrating the generalizability
of our approach. Our experiments provide insightful evaluations that
demonstrate the capabilities of LLMs essential for content generation tasks.
The results highlight significant performance improvements of 415% and 40%
respectively, depending on the zero-shot capabilities of the language model.
Our work demonstrates the potential to reduce human dependency in game AI
development, while supporting and enhancing creative processes.",False,True,"I extracted concepts from both papers to create a novel multidisciplinary research idea. Combining the medical imaging analysis from Paper 1 with the large language model-driven reward design from Paper 2, a potential research idea could be developing an AI-powered diagnostic tool that uses reinforcement learning to analyze medical images and detect early-stage cancers, such as lung cancer, with improved accuracy and reduced human dependency.",
2411.01019,1,applied,2411.01019-pos2-0,"Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms; Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability.",2503.03001,"Multicellular self-organization in Escherichia coli; Escherichia coli has long been a trusty companion, maintaining health in our
guts and advancing biological knowledge in the laboratory. In light of recent
findings, we discuss multicellular self-organization in E. coli and develop
general ideas for multicellularity, including the necessity for multicellular
dynamics and interpretation by dynamic graphs, applicable to both unicellular
and multicellular organisms. In this context, we next discuss the documented
behaviors of E. coli self-organization (rosette formation, multicellular
extension, and attached dormancy) and two potential behaviors (internal
communication and mating). Finally, by comparing the dynamic graphs for
different communities, we develop principles relevant to the theory of
multicellularity.",False,True,"The two papers can be combined to create a novel multidisciplinary research idea by applying the UNet-based neural network with attention mechanisms to analyze the self-organization patterns of Escherichia coli, potentially leading to a deeper understanding of multicellular dynamics and the development of new image analysis tools for microbiology research.",
2411.01019,1,applied,2411.01019-pos2-0,"Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms; Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability.",2501.18661,"Geometric immunosuppression in CAR-T cell treatment: Insights from
  mathematical modeling; Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising
treatment for hematological malignancies, offering a targeted approach to
cancer treatment. Understanding the complexities of CAR-T cell therapy within
solid tumors poses challenges due to the intricate interactions within the
tumor microenvironment. Mathematical modeling may serve as a valuable tool to
unravel the dynamics of CAR-T cell therapy and improve its effectiveness in
solid tumors. This study aimed to investigate the impact of spatial aspects in
CAR-T therapy of solid tumors, utilizing cellular automata for modeling
purposes. Our main objective was to deepen our understanding of treatment
effects by analyzing scenarios with different spatial distributions and varying
the initial quantities of tumor and CAR-T cells. Tumor geometry significantly
influenced treatment efficacy in-silico, with notable differences observed
between tumors with block-like arrangements and those with sparse cell
distributions, leading to the concept of immune suppression due to geometrical
effects. This research delves into the intricate relationship between spatial
dynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting
the relevance of tumor geometry in the outcome of cellular immunotherapy
treatments. Our results provide a basis for improving the efficacy of CAR-T
cell treatments by combining them with other ones reducing the density of
compact tumor areas and thus opening access ways for tumor killing T-cells.",False,True,"The combination of ideas from both papers can lead to a novel multidisciplinary research idea, such as developing a UNet-based neural network with attention mechanisms to segment tumor geometry from medical images, which can be used to inform mathematical modeling of CAR-T cell therapy and improve its effectiveness in solid tumors.",
2411.01019,1,applied,2411.01019-pos2-0,"Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms; Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability.",2502.1113,"Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for
  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering; In osteochondral tissue engineering (OCTE), simultaneously regenerating
subchondral bone and cartilage tissue presents a significant challenge.
Multiphasic scaffolds were created and manufactured using 3D printing to
address this issue. Excellent interfacial mechanical properties and
biocompatibility enhance the growth and chondrogenic differentiation of bone
marrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is
mimicked by incorporating varying concentrations of graphene oxide (GO) (0%,
1%, and 2% w/v) into a bioink composed of alginate (Alg) and gelatin (Gel).
Based on evaluations of mechanical and biocompatibility properties, 1% GO is
selected for further studies. Subsequently, the GO concentration is kept
constant while varying the platelet-rich plasma (PRP) dosage in the multiphasic
scaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w/v) are integrated into
the Alg-Gel bioink to simulate cartilage tissues. Results indicate that
3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical
properties, with no significant differences observed. However, BM-MSCs exposed
to 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,
real-time PCR and Alcian blue staining confirm increased chondrogenic
expression and glycosaminoglycans (GAGs) synthesis. This work highlights the
promising potential of 3D-printed multiphasic frameworks in the development of
OCTE.",False,True,"The combination of computer-aided detection of anterior mediastinal nodular lesions from chest CT imaging and 3D-printed multiphasic scaffolds for osteochondral tissue engineering can lead to a novel research idea. For instance, using the UNet-based neural network to segment and analyze the osteochondral defects in CT images, and then utilizing the 3D-printed scaffolds to regenerate the damaged tissue, can provide a comprehensive solution for osteochondral tissue engineering.",
2411.01019,1,applied,2411.01019-pos2-0,"Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms; Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability.",2503.13516,"A tumor-immune model of chronic myeloid leukemia with optimal
  immunotherapeutic protocols; The interactions between tumor cells and the immune system play a crucial
role in cancer evolution. In this study, we explore how these interactions
influence cancer progression by modeling the relationships among naive T cells,
effector T cells, and chronic myeloid leukemia cells. We examine the existence
of equilibria, the asymptotic stability of the positive steady state, and the
global stability of the tumor-free equilibrium. Additionally, we develop a
partial differential equation to describe the conditions under which the
concentration of cancer cells reaches a level that allows for effective control
of cancer evolution. Finally, we apply our proposed model to investigate
optimal treatment strategies that aim to minimize both the concentration of
cancer cells at the end of treatment and the accumulation of tumor burden, as
well as the cost associated with treatment during the intervention period. Our
study reveals an optimal therapeutic protocol using optimal control theory. We
perform numerical simulations to illustrate our theoretical results and to
explore the dynamic behavior of the system and optimal therapeutic protocols.
The simulations indicate that the optimal treatment strategy can be more
effective than a constant treatment approach, even when applying the same
treatment interval and total drug input.",False,True,"The combination of ideas from both papers can lead to a novel multidisciplinary research project. The UNet-based neural network with attention mechanisms from Paper 1 can be integrated with the tumor-immune model from Paper 2 to develop a predictive model for cancer evolution and optimal treatment strategies, using medical imaging data to inform the model.",
2411.01019,1,applied,2411.01019-pos2-0,"Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms; Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability.",2502.18947,"Trait-structured chemotaxis: Exploring ligand-receptor dynamics and
  travelling wave properties in a Keller-Segel model; A novel trait-structured Keller-Segel model that explores the dynamics of a
migrating cell population guided by chemotaxis in response to an external
ligand concentration is derived and analysed. Unlike traditional Keller-Segel
models, this framework introduces an explicit representation of ligand-receptor
bindings on the cell membrane, where the percentage of occupied receptors
constitutes the trait that influences cellular phenotype. The model posits that
the cell's phenotypic state directly modulates its capacity for chemotaxis and
proliferation, governed by a trade-off due to a finite energy budget: cells
highly proficient in chemotaxis exhibit lower proliferation rates, while more
proliferative cells show diminished chemotactic abilities. The model is derived
from the principles of a biased random walk, resulting in a system of two
non-local partial differential equations, describing the densities of both
cells and ligands. Using a Hopf-Cole transformation, we derive an equation that
characterises the distribution of cellular traits within travelling wave
solutions for the total cell density, allowing us to uncover the monotonicity
properties of these waves. Numerical investigations are conducted to examine
the model's behaviour across various biological scenarios, providing insights
into the complex interplay between chemotaxis, proliferation, and phenotypic
diversity in migrating cell populations.",False,True,"The combination of ideas from both papers can lead to a novel multidisciplinary research idea, such as developing a UNet-based neural network with attention mechanisms to analyze the dynamics of migrating cell populations guided by chemotaxis in response to an external ligand concentration, and to automate the detection of specific cellular traits within these populations.",
2411.01019,1,applied,2411.01019-pos1-0,"Incidental Anterior Mediastinal Nodular Lesions on Chest CT in Asymptomatic Subjects; Screening for lung cancer: 2023 guideline update from the American Cancer Society; Objective The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods We included 56,358 consecutive participants (mean age 52.4 ± 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66–0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01–3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes.; Abstract Lung cancer is the leading cause of mortality and person‐years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network‐validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit‐to‐radiation‐risk ratios follow‐up examinations. GDG also examined disease burden data National Institute’s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50–80 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low‐dose computed tomography asymptomatic currently smoke or formerly smoked have ≥20 pack‐year ( , ). Before decision made initiate LCS, should engage shared decision‐making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",2502.15234,"Optimal convergence analysis of fully discrete SAVs-FEM for the
  Cahn-Hilliard-Navier-Stokes equations; We construct a fully discrete numerical scheme that is linear, decoupled, and
unconditionally energy stable, and analyze its optimal error estimates for the
Cahn-Hilliard-Navier-Stokes equations. For time discretization, we employ the
two scalar auxiliary variables (SAVs) and the pressure-correction projection
method. For spatial discretization, we choose the $P_r \times P_r \times
\mathbf{P}_{r+1} \times P_r$ finite element spaces, where $r$ is the degree of
the local polynomials, and derive the optimal $L^2$ error estimates for the
phase-field variable, chemical potential, and pressure in the case of $r \geq
1$, and for the velocity when $r \geq 2$, without relying on the
quasi-projection operator technique proposed in \textit{[Cai et al. SIAM J
Numer Anal, 2023]}. Numerical experiments validate the theoretical results,
confirming the unconditional energy stability and optimal convergence rates of
the proposed scheme. Additionally, we numerically demonstrate the optimal $L^2$
convergence rate for the velocity when $r=1$.",False,False,"Although both papers present interesting ideas, they come from distinct fields (medical imaging and computational mathematics) that do not directly intersect. The first paper discusses the prevalence of incidental nodular lesions in chest CT scans, while the second paper focuses on numerical analysis of the Cahn-Hilliard-Navier-Stokes equations. There is no clear connection or overlap between the two papers that could be leveraged to create a novel multidisciplinary research idea.",
2411.01019,1,applied,2411.01019-pos1-0,"Incidental Anterior Mediastinal Nodular Lesions on Chest CT in Asymptomatic Subjects; Screening for lung cancer: 2023 guideline update from the American Cancer Society; Objective The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods We included 56,358 consecutive participants (mean age 52.4 ± 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66–0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01–3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes.; Abstract Lung cancer is the leading cause of mortality and person‐years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network‐validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit‐to‐radiation‐risk ratios follow‐up examinations. GDG also examined disease burden data National Institute’s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50–80 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low‐dose computed tomography asymptomatic currently smoke or formerly smoked have ≥20 pack‐year ( , ). Before decision made initiate LCS, should engage shared decision‐making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",2502.06698,"Heisenberg-limited calibration of entangling gates with robust phase
  estimation; The calibration of high-quality two-qubit entangling gates is an essential
component in engineering large-scale, fault-tolerant quantum computers.
However, many standard calibration techniques are based on randomized circuits
that are only quadratically sensitive to calibration errors. As a result, these
approaches are inefficient, requiring many experimental shots to achieve
acceptable performance. In this work, we demonstrate that robust phase
estimation can enable high-precision, Heisenberg-limited estimates of coherent
errors in multi-qubit gates. Equipped with an efficient estimator, the
calibration problem may be reduced to a simple optimization loop that minimizes
the estimated coherent error. We experimentally demonstrate our calibration
protocols by improving the operation of a two-qubit controlled-Z gate on a
superconducting processor, and we validate the improved performance with gate
set tomography. Our methods are applicable to gates in other quantum hardware
platforms such as ion traps and neutral atoms, and on other multi-qubit gates,
such as CNOT or iSWAP.",False,False,"The two papers are from distinct fields of medical research and quantum computing, with no apparent overlap or connection. The concepts and methods presented in each paper do not lend themselves to a novel multidisciplinary research idea that meets the standards of interdisciplinary research, feasibility, novelty, and usefulness.",
2411.01019,1,applied,2411.01019-pos1-0,"Incidental Anterior Mediastinal Nodular Lesions on Chest CT in Asymptomatic Subjects; Screening for lung cancer: 2023 guideline update from the American Cancer Society; Objective The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods We included 56,358 consecutive participants (mean age 52.4 ± 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66–0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01–3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes.; Abstract Lung cancer is the leading cause of mortality and person‐years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network‐validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit‐to‐radiation‐risk ratios follow‐up examinations. GDG also examined disease burden data National Institute’s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50–80 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low‐dose computed tomography asymptomatic currently smoke or formerly smoked have ≥20 pack‐year ( , ). Before decision made initiate LCS, should engage shared decision‐making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",2502.15468,"Multi-messenger detection of black hole binaries in dark matter spikes; We investigate the inspiral of a high mass-ratio black hole binary located in
the nucleus of a galaxy, where the primary central black hole is surrounded by
a dense dark matter spike formed through accretion during the black hole growth
phase. Within this spike, dark matter undergoes strong self-annihilation,
producing a compact source of $\gamma$-ray radiation that is highly sensitive
to spike density, while the binary emits gravitational waves at frequencies
detectable by LISA. As the inspiralling binary interacts with the surrounding
dark matter particles, it alters the density of the spike, thereby influencing
the $\gamma$-ray flux from dark matter annihilation. We demonstrate that the
spike self-annihilation luminosity decreases by $10\%$ to $90\%$ of its initial
value, depending on the initial density profile and binary mass ratio, as the
binary sweeps through the LISA band. This presents a new opportunity to
indirectly probe dark matter through multi-messenger observations of galactic
nuclei.",False,False,"Although both papers present innovative ideas in their respective fields, combining them does not lead to a feasible multidisciplinary research project. The first paper focuses on medical imaging and cancer screening, while the second paper explores astrophysics and dark matter detection. The concepts and methods presented in each paper are too distinct to be integrated into a novel, useful, and feasible research idea.",
2411.01019,1,applied,2411.01019-pos1-0,"Incidental Anterior Mediastinal Nodular Lesions on Chest CT in Asymptomatic Subjects; Screening for lung cancer: 2023 guideline update from the American Cancer Society; Objective The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods We included 56,358 consecutive participants (mean age 52.4 ± 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66–0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01–3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes.; Abstract Lung cancer is the leading cause of mortality and person‐years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network‐validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit‐to‐radiation‐risk ratios follow‐up examinations. GDG also examined disease burden data National Institute’s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50–80 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low‐dose computed tomography asymptomatic currently smoke or formerly smoked have ≥20 pack‐year ( , ). Before decision made initiate LCS, should engage shared decision‐making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",2503.12557,"Efficient optimization and conceptual barriers in variational finite
  Projected Entangled-Pair States; Projected entangled pair states (PEPS) on finite two-dimensional lattices are
a natural ansatz for representing ground states of local many-body
Hamiltonians, as they inherently satisfy the boundary law of entanglement
entropy. In this paper, we propose the optimization of PEPS via an improved
formulation of the time-dependent variational principle (TDVP), namely the
minimum-step stochastic-reconfguration scheme recently introduced for neural
quantum states. We further discuss possible numerical issues that might arise
in such a sampling-based approach. In this context, investigate the
entanglement properties of random PEPS and find an entanglement phase
transition. We note that on one side of this transition, we can identify
positive random tensors as product states. To demonstrate the power of the
framework described in this paper, we apply the PEPS to study the notoriously
challenging chiral spin liquids. Moreover, we exhibit our approach's capability
to naturally handle long-range interactions by exploring the phase diagram of
Rydberg atom arrays with long-range interactions. We further provide
parallelized easy-to-use code, allowing the straightforward application of our
method to general Hamiltonians composed of local interaction terms.",False,False,"The two papers come from distinct areas of medical research and quantum physics, making it challenging to extract concepts that can be combined into a novel multidisciplinary research idea. The papers' topics and methodologies are too disparate to integrate into a feasible and useful interdisciplinary research project.",
2411.01019,1,applied,2411.01019-pos1-0,"Incidental Anterior Mediastinal Nodular Lesions on Chest CT in Asymptomatic Subjects; Screening for lung cancer: 2023 guideline update from the American Cancer Society; Objective The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods We included 56,358 consecutive participants (mean age 52.4 ± 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66–0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01–3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes.; Abstract Lung cancer is the leading cause of mortality and person‐years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network‐validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit‐to‐radiation‐risk ratios follow‐up examinations. GDG also examined disease burden data National Institute’s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50–80 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low‐dose computed tomography asymptomatic currently smoke or formerly smoked have ≥20 pack‐year ( , ). Before decision made initiate LCS, should engage shared decision‐making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",2501.17787,"Detecting Anomalies Using Rotated Isolation Forest; The Isolation Forest (iForest), proposed by Liu, Ting, and Zhou at TKDE 2012,
has become a prominent tool for unsupervised anomaly detection. However, recent
research by Hariri, Kind, and Brunner, published in TKDE 2021, has revealed
issues with iForest. They identified the presence of axis-aligned ghost
clusters that can be misidentified as normal clusters, leading to biased
anomaly scores and inaccurate predictions. In response, they developed the
Extended Isolation Forest (EIF), which effectively solves these issues by
eliminating the ghost clusters introduced by iForest. This enhancement results
in improved consistency of anomaly scores and superior performance. We reveal a
previously overlooked problem in the Extended Isolation Forest (EIF), showing
that it is vulnerable to ghost inter-clusters between normal clusters of data
points. In this paper, we introduce the Rotated Isolation Forest (RIF)
algorithm which effectively addresses both the axis-aligned ghost clusters
observed in iForest and the ghost inter-clusters seen in EIF. RIF accomplishes
this by randomly rotating the dataset (using random rotation matrices and QR
decomposition) before feeding it into the iForest construction, thereby
increasing dataset variation and eliminating ghost clusters. Our experiments
conclusively demonstrate that the RIF algorithm outperforms iForest and EIF, as
evidenced by the results obtained from both synthetic datasets and real-world
datasets.",False,True,"I can extract concepts from both papers to create a novel multidisciplinary research idea. The first paper discusses incidental nodular lesions in the anterior mediastinum and lung cancer screening, while the second paper introduces the Rotated Isolation Forest algorithm for anomaly detection. By combining these concepts, a potential research idea could be to develop an AI-powered system using the Rotated Isolation Forest algorithm to detect anomalies in chest CT scans for early lung cancer detection, which could improve screening accuracy and patient outcomes.",
2411.01019,1,applied,2411.01019-pos2-0,"Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms; Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability.",2503.14672,"Discussion about the assumptions of Category Theory approach to
  agent-based modeling in microeconomics; We investigate a possible category theoretical description for agent based
modeling by outlining justifications for two main principles to describe the
valuations in a realistic way in microeconomics: 1) It is assumed that the
valuations can be expressed as a subcategory of the category of metric space so
that value differences between various objects, that agents assign to them, can
be expressed with a metric that is consistent with the concept of distance in
mathematical metric spaces. 2) In realistic economic models, the category of
valuations does not consist of linear spaces other than in special cases.
  We also discuss how category theoretical concepts such as morphisms and
functors can be used to express transformations within categories and relations
between other categories. We then present examples how functors and morphisms
could be used to describe relationships and operations, such as ownership
changes, transactions, and price formation, in the context of some of the
established theories in microeconomics. Finally, we discuss briefly possible
applications, such as product design and price optimization.",False,False,"Although both papers introduce innovative ideas from distinct disciplines, the combination of ideas from UNet-based neural network for medical image segmentation and category theory approach to agent-based modeling in microeconomics does not seem to lead to a novel and feasible multidisciplinary research idea, as the fields of application and methodologies are too disparate.",
2411.01019,1,applied,2411.01019-pos2-0,"Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms; Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability.",2503.04007,"Planning and Control for Deformable Linear Object Manipulation; Manipulating a deformable linear object (DLO) such as wire, cable, and rope
is a common yet challenging task due to their high degrees of freedom and
complex deformation behaviors, especially in an environment with obstacles.
Existing local control methods are efficient but prone to failure in complex
scenarios, while precise global planners are computationally intensive and
difficult to deploy. This paper presents an efficient, easy-to-deploy framework
for collision-free DLO manipulation using mobile manipulators. We demonstrate
the effectiveness of leveraging standard planning tools for high-dimensional
DLO manipulation without requiring custom planners or extensive data-driven
models. Our approach combines an off-the-shelf global planner with a real-time
local controller. The global planner approximates the DLO as a series of rigid
links connected by spherical joints, enabling rapid path planning without the
need for problem-specific planners or large datasets. The local controller
employs control barrier functions (CBFs) to enforce safety constraints,
maintain the DLO integrity, prevent overstress, and handle obstacle avoidance.
It compensates for modeling inaccuracies by using a state-of-the-art
position-based dynamics technique that approximates physical properties like
Young's and shear moduli. We validate our framework through extensive
simulations and real-world demonstrations. In complex obstacle
scenarios-including tent pole transport, corridor navigation, and tasks
requiring varied stiffness-our method achieves a 100% success rate over
thousands of trials, with significantly reduced planning times compared to
state-of-the-art techniques. Real-world experiments include transportation of a
tent pole and a rope using mobile manipulators. We share our ROS-based
implementation to facilitate adoption in various applications.",False,False,"Although both papers present innovative solutions in their respective fields, there is no clear connection between the ideas presented in the two papers that could be combined to create a novel multidisciplinary research idea. One paper focuses on medical imaging and the other on robotics and manipulation of deformable objects.",
2411.01019,1,applied,2411.01019-pos2-0,"Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms; Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability.",2503.13366,"Follow-the-Regularized-Leader with Adversarial Constraints; Constrained Online Convex Optimization (COCO) can be seen as a generalization
of the standard Online Convex Optimization (OCO) framework. At each round, a
cost function and constraint function are revealed after a learner chooses an
action. The goal is to minimize both the regret and cumulative constraint
violation (CCV) against an adaptive adversary. We show for the first time that
is possible to obtain the optimal $O(\sqrt{T})$ bound on both regret and CCV,
improving the best known bounds of $O \left( \sqrt{T} \right)$ and $\~{O}
\left( \sqrt{T} \right)$ for the regret and CCV, respectively.",False,False,"Although both papers present innovative ideas, they are from distinct fields (medical imaging and online convex optimization) with little overlap. The concepts from Paper 1 (UNet-based neural network with attention mechanisms) and Paper 2 (Follow-the-Regularized-Leader with Adversarial Constraints) do not naturally combine to form a novel multidisciplinary research idea that meets the standards of feasibility, novelty, and usefulness.",
2411.01019,1,applied,2411.01019-pos2-0,"Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms; Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability.",2503.05545,"Discovery of unconventional charge-spin-intertwined density wave in
  magnetic kagome metal GdTi3Bi4; The symmetry breaking and its interplay among spin, charge, and lattice
degrees of freedom is crucial for understanding correlated quantum states such
as charge density waves (CDWs) and unconventional superconductivity. Here, we
report the discovery by low-temperature scanning tunneling
microscopy/spectroscopy of unconventional charge-spin-intertwined density waves
in magnetic kagome metal GdTi3Bi4, which exhibits the one-third magnetization
plateau. We reveal the emergence of 3Q CDWs incommensurate with the crystalline
lattice in both periodicity and orientation, breaking all mirror and rotation
symmetries. The CDW exhibits incommensurate-commensurate transitions in an
applied magnetic field and transitions between 3Q and 1Q CDWs as a function of
field and temperature, accompanied by changes in the spatial symmetries.
Remarkably, the quantum and classic melting of the CDWs exhibits a phase
structure which is consistent with the magnetization phase diagram of bulk
GdTi3Bi4, providing strong evidence for the intertwined charge-spin density
wave order. The origin of the charge-spin intertwinement is further evidenced
by the observed hybridization between itinerant electrons and Gd local moments.
Our findings uncover an unconventional form of charge-spin orders and offer new
insights into a broad class of multi-components density wave formation in
kagome and other correlated quantum materials.",False,False,"Although both papers present innovative ideas, combining them does not lead to a novel multidisciplinary research idea. The first paper focuses on medical imaging and computer-aided detection, while the second paper explores condensed matter physics and materials science. There is no clear connection or overlap between the two topics, making it challenging to integrate them into a coherent and feasible interdisciplinary research project.",
2411.01019,1,applied,2411.01019-pos2-0,"Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms; Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability.",2503.14809,"Learning with Expert Abstractions for Efficient Multi-Task Continuous
  Control; Decision-making in complex, continuous multi-task environments is often
hindered by the difficulty of obtaining accurate models for planning and the
inefficiency of learning purely from trial and error. While precise environment
dynamics may be hard to specify, human experts can often provide high-fidelity
abstractions that capture the essential high-level structure of a task and user
preferences in the target environment. Existing hierarchical approaches often
target discrete settings and do not generalize across tasks. We propose a
hierarchical reinforcement learning approach that addresses these limitations
by dynamically planning over the expert-specified abstraction to generate
subgoals to learn a goal-conditioned policy. To overcome the challenges of
learning under sparse rewards, we shape the reward based on the optimal state
value in the abstract model. This structured decision-making process enhances
sample efficiency and facilitates zero-shot generalization. Our empirical
evaluation on a suite of procedurally generated continuous control environments
demonstrates that our approach outperforms existing hierarchical reinforcement
learning methods in terms of sample efficiency, task completion rate,
scalability to complex tasks, and generalization to novel scenarios.",False,True,"The combination of Paper 1's UNet-based neural network with attention mechanisms for medical image segmentation and Paper 2's hierarchical reinforcement learning approach for efficient multi-task continuous control could lead to a novel multidisciplinary research idea, such as developing an autonomous medical diagnosis system that learns from expert abstractions and adapts to new medical imaging tasks.",
2412.11084,1,applied,2412.11084-pos1-1,"BarcodeBERT: Transformers for Biodiversity Analysis; Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https://github.com/Kari-Genomics-Lab/BarcodeBERT",2412.11084-pos2-1,"Biological identifications through DNA barcodes; Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution.",True,True,The combination of machine learning approaches from Paper 1 and the concept of DNA barcodes from Paper 2 can lead to a novel multidisciplinary research idea. Integrating BarcodeBERT with the COI profiles from Paper 2 could enhance species-level identification and provide a more accurate and efficient solution for biodiversity analysis.,
2412.11084,1,applied,2412.11084-pos1-1,"BarcodeBERT: Transformers for Biodiversity Analysis; Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https://github.com/Kari-Genomics-Lab/BarcodeBERT",2501.04822,"Curated loci prime editing (cliPE) for accessible multiplexed assays of
  variant effect (MAVEs); Multiplexed assays of variant effect (MAVEs) perform simultaneous
characterization of many variants. Prime editing has been recently adopted for
introducing many variants in their native genomic contexts. However, robust
protocols and standards are limited, preventing widespread uptake. Herein, we
describe curated loci prime editing (cliPE) which is an accessible, low-cost
experimental pipeline to perform MAVEs using prime editing of a target gene, as
well as a companion Shiny app (pegRNA Designer) to rapidly and easily design
user-specific MAVE libraries.",False,True,"The combination of BarcodeBERT's machine learning approach for biodiversity analysis and cliPE's prime editing technique for multiplexed assays of variant effect can lead to a novel multidisciplinary research idea. For example, integrating BarcodeBERT with cliPE can enable the simultaneous characterization of many variants in invertebrate species, advancing our understanding of biodiversity and species-level identification.",
2412.11084,1,applied,2412.11084-pos1-1,"BarcodeBERT: Transformers for Biodiversity Analysis; Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https://github.com/Kari-Genomics-Lab/BarcodeBERT",2502.21125,"Eukaryotes evade information storage-replication rate trade-off with
  endosymbiont assistance leading to larger genomes; Genome length varies widely among organisms, from compact genomes of
prokaryotes to vast and complex genomes of eukaryotes. In this study, we
theoretically identify the evolutionary pressures that may have driven this
divergence in genome length. We use a parameter-free model to study genome
length evolution under selection pressure to minimize replication time and
maximize information storage capacity. We show that prokaryotes tend to reduce
genome length, constrained by a single replication origin, while eukaryotes
expand their genomes by incorporating multiple replication origins. We propose
a connection between genome length and cellular energetics, suggesting that
endosymbiotic organelles, mitochondria and chloroplasts, evolutionarily
regulate the number of replication origins, thereby influencing genome length
in eukaryotes. We show that the above two selection pressures also lead to
strict equalization of the number of purines and their corresponding
base-pairing pyrimidines within a single DNA strand, known as Chagraff's second
parity rule, a hitherto unexplained observation in genomes of nearly all known
species. This arises from the symmetrization of replichore length, another
observation that has been shown to hold across species, which our model
reproduces. The model also reproduces other experimentally observed phenomena,
such as a general preference for deletions over insertions, and elongation and
high variance of genome lengths under reduced selection pressure for
replication rate, termed the C-value paradox. We highlight the possibility of
regulation of the firing of latent replication origins in response to cues from
the extracellular environment leading to the regulation of cell cycle rates in
multicellular eukaryotes.",False,True,"The combination of ideas from both papers can lead to a novel multidisciplinary research project that integrates machine learning approaches for biodiversity analysis (Paper 1) with the understanding of genome length evolution and cellular energetics (Paper 2). This integration can potentially lead to the development of new methods for analyzing the relationship between genome length and species diversity, or for predicting the impact of environmental cues on genome evolution.",
2412.11084,1,applied,2412.11084-pos1-1,"BarcodeBERT: Transformers for Biodiversity Analysis; Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https://github.com/Kari-Genomics-Lab/BarcodeBERT",2503.13189,"Causes of evolutionary divergence in prostate cancer; Cancer progression involves the sequential accumulation of genetic
alterations that cumulatively shape the tumour phenotype. In prostate cancer,
tumours can follow divergent evolutionary trajectories that lead to distinct
subtypes, but the causes of this divergence remain unclear. While causal
inference could elucidate the factors involved, conventional methods are
unsuitable due to the possibility of unobserved confounders and ambiguity in
the direction of causality. Here, we propose a method that circumvents these
issues and apply it to genomic data from 829 prostate cancer patients. We
identify several genetic alterations that drive divergence as well as others
that prevent this transition, locking tumours into one trajectory. Further
analysis reveals that these genetic alterations may cause each other, implying
a positive-feedback loop that accelerates divergence. Our findings provide
insights into how cancer subtypes emerge and offer a foundation for genomic
surveillance strategies aimed at monitoring the progression of prostate cancer.",False,True,"I extracted concepts from both papers to create a novel multidisciplinary research idea. By combining BarcodeBERT's machine learning approach for biodiversity analysis with the concept of evolutionary divergence in cancer from the second paper, a new research project could investigate the role of genetic alterations in driving species divergence, using machine learning to analyze genomic data from various species.",
2412.11084,1,applied,2412.11084-pos1-1,"BarcodeBERT: Transformers for Biodiversity Analysis; Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https://github.com/Kari-Genomics-Lab/BarcodeBERT",2502.06253,"Find Central Dogma Again: Leveraging Multilingual Transfer in Large
  Language Models; In recent years, large language models (LLMs) have achieved state-of-the-art
results in various biological sequence analysis tasks, such as sequence
classification, structure prediction, and function prediction. Similar to
advancements in AI for other scientific fields, deeper research into biological
LLMs has begun to focus on using these models to rediscover important existing
biological laws or uncover entirely new patterns in biological sequences. This
study leverages GPT-like LLMs to utilize language transfer capabilities to
rediscover the genetic code rules of the central dogma. In our experimental
design, we transformed the central dogma into a binary classification problem
of aligning DNA sequences with protein sequences, where positive examples are
matching DNA and protein sequences, and negative examples are non-matching
pairs. We first trained a GPT-2 model from scratch using a dataset comprising
protein sequences, DNA sequences, and sequences from languages such as English
and Chinese. Subsequently, we fine-tuned the model using the natural language
sentences similarity judgment dataset from PAWS-X. When tested on a dataset for
DNA and protein sequence alignment judgment, the fine-tuned model achieved a
classification accuracy of 81%. The study also analyzed factors contributing to
this zero-shot capability, including model training stability and types of
training data. This research demonstrates that LLMs can, through the transfer
of natural language capabilities and solely relying on the analysis of
sequences themselves, rediscover the central dogma without prior knowledge of
it. This study bridges natural language and genetic language, opening a new
door for AI-driven biological research.",False,True,"The combination of ideas from both papers can lead to a novel multidisciplinary research project that leverages large language models (LLMs) and machine learning approaches to analyze biodiversity and understand the genetic code rules of the central dogma. By integrating BarcodeBERT with multilingual transfer capabilities, researchers can develop a more comprehensive framework for analyzing biological sequences and predicting species-level identification.",
2412.11084,1,applied,2412.11084-pos1-1,"BarcodeBERT: Transformers for Biodiversity Analysis; Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https://github.com/Kari-Genomics-Lab/BarcodeBERT",2503.1118,"Learnable Group Transform: Enhancing Genotype-to-Phenotype Prediction
  for Rice Breeding with Small, Structured Datasets; Genotype-to-Phenotype (G2P) prediction plays a pivotal role in crop breeding,
enabling the identification of superior genotypes based on genomic data. Rice
(Oryza sativa), one of the most important staple crops, faces challenges in
improving yield and resilience due to the complex genetic architecture of
agronomic traits and the limited sample size in breeding datasets. Current G2P
prediction methods, such as GWAS and linear models, often fail to capture
complex non-linear relationships between genotypes and phenotypes, leading to
suboptimal prediction accuracy. Additionally, population stratification and
overfitting are significant obstacles when models are applied to small datasets
with diverse genetic backgrounds. This study introduces the Learnable Group
Transform (LGT) method, which aims to overcome these challenges by combining
the advantages of traditional linear models with advanced machine learning
techniques. LGT utilizes a group-based transformation of genotype data to
capture spatial relationships and genetic structures across diverse rice
populations, offering flexibility to generalize even with limited data. Through
extensive experiments on the Rice529 dataset, a panel of 529 rice accessions,
LGT demonstrated substantial improvements in prediction accuracy for multiple
agronomic traits, including yield and plant height, compared to
state-of-the-art baselines such as linear models and recent deep learning
approaches. Notably, LGT achieved an R^2 improvement of up to 15\% for yield
prediction, significantly reducing error and demonstrating its ability to
extract meaningful signals from high-dimensional, noisy genomic data. These
results highlight the potential of LGT as a powerful tool for genomic
prediction in rice breeding, offering a promising solution for accelerating the
identification of high-yielding and resilient rice varieties.",False,True,"I can extract concepts from both papers to create a novel multidisciplinary research idea. By combining the BarcodeBERT method for biodiversity analysis with the Learnable Group Transform (LGT) method for genotype-to-phenotype prediction, a new approach for predicting phenotypic traits of invertebrate species based on their DNA barcodes can be developed, leveraging the strengths of both methods to advance our understanding of species diversity and ecology.",
2412.11084,1,applied,2412.11084-pos2-1,"Biological identifications through DNA barcodes; Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution.",2502.02377,"A Minimax Approach to Ad Hoc Teamwork; We propose a minimax-Bayes approach to Ad Hoc Teamwork (AHT) that optimizes
policies against an adversarial prior over partners, explicitly accounting for
uncertainty about partners at time of deployment. Unlike existing methods that
assume a specific distribution over partners, our approach improves worst-case
performance guarantees. Extensive experiments, including evaluations on
coordinated cooking tasks from the Melting Pot suite, show our method's
superior robustness compared to self-play, fictitious play, and best response
learning. Our work highlights the importance of selecting an appropriate
training distribution over teammates to achieve robustness in AHT.",False,True,"I can extract concepts from both papers to create a novel multidisciplinary research idea. For example, combining DNA barcoding with minimax-Bayes approach to develop a robust system for identifying and classifying species in dynamic ecosystems, where the system can adapt to uncertainty and variability in species interactions.",
2412.11084,1,applied,2412.11084-pos2-1,"Biological identifications through DNA barcodes; Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution.",2502.1961,"Program Synthesis Dialog Agents for Interactive Decision-Making; Many real-world eligibility problems, ranging from medical diagnosis to tax
planning, can be mapped to decision problems expressed in natural language,
wherein a model must make a binary choice based on user features. Large-scale
domains such as legal codes or frequently updated funding opportunities render
human annotation (e.g., web forms or decision trees) impractical, highlighting
the need for agents that can automatically assist in decision-making. Since
relevant information is often only known to the user, it is crucial that these
agents ask the right questions. As agents determine when to terminate a
conversation, they face a trade-off between accuracy and the number of
questions asked, a key metric for both user experience and cost. To evaluate
this task, we propose BeNYfits, a new benchmark for determining user
eligibility for multiple overlapping social benefits opportunities through
interactive decision-making. Our experiments show that current language models
struggle with frequent hallucinations, with GPT-4o scoring only 35.7 F1 using a
ReAct-style chain-of-thought. To address this, we introduce ProADA, a novel
approach that leverages program synthesis to assist in decision-making by
mapping dialog planning to a code generation problem and using gaps in
structured data to determine the best next action. Our agent, ProADA, improves
the F1 score to 55.6 while maintaining nearly the same number of dialog turns.",False,True,"I extracted the concept of using DNA barcodes for identification from Paper 1 and the idea of program synthesis for decision-making from Paper 2. A novel multidisciplinary research idea could be to develop a system that uses DNA barcodes to identify species and then employs program synthesis to assist in decision-making for conservation or ecological management, improving the accuracy and efficiency of species conservation efforts.",
2412.11084,1,applied,2412.11084-pos2-1,"Biological identifications through DNA barcodes; Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution.",2502.1062,"ProMRVL-CAD: Proactive Dialogue System with Multi-Round Vision-Language
  Interactions for Computer-Aided Diagnosis; Recent advancements in large language models (LLMs) have demonstrated
extraordinary comprehension capabilities with remarkable breakthroughs on
various vision-language tasks. However, the application of LLMs in generating
reliable medical diagnostic reports remains in the early stages. Currently,
medical LLMs typically feature a passive interaction model where doctors
respond to patient queries with little or no involvement in analyzing medical
images. In contrast, some ChatBots simply respond to predefined queries based
on visual inputs, lacking interactive dialogue or consideration of medical
history. As such, there is a gap between LLM-generated patient-ChatBot
interactions and those occurring in actual patient-doctor consultations. To
bridge this gap, we develop an LLM-based dialogue system, namely proactive
multi-round vision-language interactions for computer-aided diagnosis
(ProMRVL-CAD), to generate patient-friendly disease diagnostic reports. The
proposed ProMRVL-CAD system allows proactive dialogue to provide patients with
constant and reliable medical access via an integration of knowledge graph into
a recommendation system. Specifically, we devise two generators: a Proactive
Question Generator (Pro-Q Gen) to generate proactive questions that guide the
diagnostic procedure and a Multi-Vision Patient-Text Diagnostic Report
Generator (MVP-DR Gen) to produce high-quality diagnostic reports. Evaluating
two real-world publicly available datasets, MIMIC-CXR and IU-Xray, our model
has better quality in generating medical reports. We further demonstrate the
performance of ProMRVL achieves robust under the scenarios with low image
quality. Moreover, we have created a synthetic medical dialogue dataset that
simulates proactive diagnostic interactions between patients and doctors,
serving as a valuable resource for training LLM.",False,True,"The combination of DNA barcoding from Paper 1 and the ProMRVL-CAD system from Paper 2 can lead to a novel multidisciplinary research idea. For instance, integrating DNA barcoding into the ProMRVL-CAD system could enable the identification of species-specific diseases or pathogens, enhancing the diagnostic capabilities of the system.",
2412.11084,1,applied,2412.11084-pos2-1,"Biological identifications through DNA barcodes; Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution.",2502.14491,"Statistical Scenario Modelling and Lookalike Distributions for
  Multi-Variate AI Risk; Evaluating AI safety requires statistically rigorous methods and risk metrics
for understanding how the use of AI affects aggregated risk. However, much AI
safety literature focuses upon risks arising from AI models in isolation,
lacking consideration of how modular use of AI affects risk distribution of
workflow components or overall risk metrics. There is also a lack of
statistical grounding enabling sensitisation of risk models in the presence of
absence of AI to estimate causal contributions of AI. This is in part due to
the dearth of AI impact data upon which to fit distributions. In this work, we
address these gaps in two ways. First, we demonstrate how scenario modelling
(grounded in established statistical techniques such as Markov chains, copulas
and Monte Carlo simulation) can be used to model AI risk holistically. Second,
we show how lookalike distributions from phenomena analogous to AI can be used
to estimate AI impacts in the absence of directly observable data. We
demonstrate the utility of our methods for benchmarking cumulative AI risk via
risk analysis of a logistic scenario simulations.",False,True,"I can extract concepts from both papers to create a novel multidisciplinary research idea. By combining DNA barcoding with statistical scenario modeling, I can develop a system to identify and assess the risks associated with invasive species, using AI-powered predictive models to simulate the potential impacts of species invasions on ecosystems.",
2412.11084,1,applied,2412.11084-pos2-1,"Biological identifications through DNA barcodes; Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution.",2502.10906,"PCGRLLM: Large Language Model-Driven Reward Design for Procedural
  Content Generation Reinforcement Learning; Reward design plays a pivotal role in the training of game AIs, requiring
substantial domain-specific knowledge and human effort. In recent years,
several studies have explored reward generation for training game agents and
controlling robots using large language models (LLMs). In the content
generation literature, there has been early work on generating reward functions
for reinforcement learning agent generators. This work introduces PCGRLLM, an
extended architecture based on earlier work, which employs a feedback mechanism
and several reasoning-based prompt engineering techniques. We evaluate the
proposed method on a story-to-reward generation task in a two-dimensional
environment using two state-of-the-art LLMs, demonstrating the generalizability
of our approach. Our experiments provide insightful evaluations that
demonstrate the capabilities of LLMs essential for content generation tasks.
The results highlight significant performance improvements of 415% and 40%
respectively, depending on the zero-shot capabilities of the language model.
Our work demonstrates the potential to reduce human dependency in game AI
development, while supporting and enhancing creative processes.",False,False,"The two papers come from distinct disciplines, biology and computer science, but there is no clear connection or overlap between the ideas presented. DNA barcoding and large language models are unrelated concepts, and combining them does not yield a novel, feasible, and useful interdisciplinary research idea.",
2412.11084,1,applied,2412.11084-pos1-1,"BarcodeBERT: Transformers for Biodiversity Analysis; Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https://github.com/Kari-Genomics-Lab/BarcodeBERT",2503.14672,"Discussion about the assumptions of Category Theory approach to
  agent-based modeling in microeconomics; We investigate a possible category theoretical description for agent based
modeling by outlining justifications for two main principles to describe the
valuations in a realistic way in microeconomics: 1) It is assumed that the
valuations can be expressed as a subcategory of the category of metric space so
that value differences between various objects, that agents assign to them, can
be expressed with a metric that is consistent with the concept of distance in
mathematical metric spaces. 2) In realistic economic models, the category of
valuations does not consist of linear spaces other than in special cases.
  We also discuss how category theoretical concepts such as morphisms and
functors can be used to express transformations within categories and relations
between other categories. We then present examples how functors and morphisms
could be used to describe relationships and operations, such as ownership
changes, transactions, and price formation, in the context of some of the
established theories in microeconomics. Finally, we discuss briefly possible
applications, such as product design and price optimization.",False,False,"The two papers come from distinct areas of research, but the concepts presented do not naturally lend themselves to integration. BarcodeBERT focuses on biodiversity analysis using machine learning, while the Category Theory approach is applied to agent-based modeling in microeconomics, with no clear overlap or potential for meaningful combination.",
2412.11084,1,applied,2412.11084-pos1-1,"BarcodeBERT: Transformers for Biodiversity Analysis; Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https://github.com/Kari-Genomics-Lab/BarcodeBERT",2502.04665,"0-Dimensional Ideal Approximation Theory; We propose axioms for 0-dimensional ideal approximation theory and note that
extriangulated categories satisfy these axioms.",False,False,"The two papers are from distinct areas, one from bioinformatics and the other from pure mathematics. Although both papers propose novel methods, they don't share a common ground for integration. The ideas from 0-Dimensional Ideal Approximation Theory don't seem to be applicable to biodiversity analysis, and the BarcodeBERT method doesn't rely on abstract algebraic structures.",
2412.11084,1,applied,2412.11084-pos1-1,"BarcodeBERT: Transformers for Biodiversity Analysis; Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https://github.com/Kari-Genomics-Lab/BarcodeBERT",2503.13105,"Managing Hybrid Solid-State Drives Using Large Language Models; Hybrid Solid-State Drives (SSDs), which integrate several types of flash
cells (e.g., single-level cell (SLC) and multiple-level cell (MLC)) in a single
drive and enable them to convert between each other, are designed to deliver
both high performance and high storage capacity. However, compared to
traditional SSDs, hybrid SSDs also introduce a much larger design space,
resulting in higher optimization complexity due to more design factors
involved, including flash conversion timing and data migration between
different flash cells, etc. To address these challenges, large language models
(LLMs) could be a promising technique, as they excel in handling complex,
high-dimensional parameter space exploration by leveraging their advanced
capability to identify patterns and optimize solutions. Recent works have
started exploring the use of LLMs to optimize computer systems. However, to the
best of our knowledge, no study has focused on optimizing SSDs with the
assistance of LLMs.
  In this work, we explore the potential of LLMs in understanding and
efficiently managing hybrid SSD design space. Specifically, two important
questions are exploited and analyzed: 1) Can LLMs offer optimization potential
for Hybrid SSD management? 2) How to leverage LLMs for the performance and
efficiency of hybrid SSD optimization? Based on the observations of
exploration, we propose a comprehensive auto-tuning framework for hybrid SSDs,
integrating LLMs to recommend customized configurations using calibration
prompts derived from hardware, system, and workload information. Experimental
results reveal a 62.35% improvement in throughput and a 57.99% decrease in
write amplification compared to the default hybrid SSD configurations achieved
with the incorporation of LLMs.",False,True,"I can extract concepts from both papers to create a novel multidisciplinary research idea. By combining the use of large language models (LLMs) from Paper 2 with the BarcodeBERT method from Paper 1, a new approach to optimizing biodiversity analysis workflows using hybrid SSDs can be explored, leveraging LLMs to recommend customized storage configurations for efficient barcode data processing.",
2412.11084,1,applied,2412.11084-pos1-1,"BarcodeBERT: Transformers for Biodiversity Analysis; Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https://github.com/Kari-Genomics-Lab/BarcodeBERT",2503.03376,"Magneto-rotation coupling for ferromagnetic nanoelement embedded in
  elastic substrate; This study investigates magneto-rotational coupling as a distinct
contribution to magnetoelastic interactions, which can be influenced by
magnetic anisotropy. We determine magneto-rotational coupling coefficients that
incorporate the shape anisotropy of a magnetic nanoelement (strip) and
demonstrate that this type of coupling can be modified through geometric
adjustments. Furthermore, we analyze the magneto-rotational contribution to the
magnetoelastic field in a ferromagnetic strip embedded in a nonmagnetic
substrate. Both Rayleigh and Love waves are considered sources of the
magnetoelastic field, and we examine how the strength of the magneto-rotational
coupling varies with the direction of the in-plane applied magnetic field. We
found that in the absence of magnetocrystalline anisotropy the
magneto-rotational contribution to the magnetoelastic field decreases with a
reduction in the thickness-to-width ratio of the strip for a Rayleigh wave,
whereas for a Love wave, it changes non-monotonically. These findings enhance
the understanding of magneto-rotational coupling in magnonic nanostructures.",False,False,"Although both papers present innovative ideas in their respective fields, they seem to be too distant to be combined into a novel multidisciplinary research idea. One paper focuses on machine learning for biodiversity analysis, while the other explores magneto-rotational coupling in ferromagnetic nanoelements, with no apparent connection between the two concepts.",
2412.11084,1,applied,2412.11084-pos1-1,"BarcodeBERT: Transformers for Biodiversity Analysis; Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https://github.com/Kari-Genomics-Lab/BarcodeBERT",2503.14809,"Learning with Expert Abstractions for Efficient Multi-Task Continuous
  Control; Decision-making in complex, continuous multi-task environments is often
hindered by the difficulty of obtaining accurate models for planning and the
inefficiency of learning purely from trial and error. While precise environment
dynamics may be hard to specify, human experts can often provide high-fidelity
abstractions that capture the essential high-level structure of a task and user
preferences in the target environment. Existing hierarchical approaches often
target discrete settings and do not generalize across tasks. We propose a
hierarchical reinforcement learning approach that addresses these limitations
by dynamically planning over the expert-specified abstraction to generate
subgoals to learn a goal-conditioned policy. To overcome the challenges of
learning under sparse rewards, we shape the reward based on the optimal state
value in the abstract model. This structured decision-making process enhances
sample efficiency and facilitates zero-shot generalization. Our empirical
evaluation on a suite of procedurally generated continuous control environments
demonstrates that our approach outperforms existing hierarchical reinforcement
learning methods in terms of sample efficiency, task completion rate,
scalability to complex tasks, and generalization to novel scenarios.",False,True,"I can extract concepts from both papers to create a novel multidisciplinary research idea. By combining BarcodeBERT's self-supervised pretraining approach for biodiversity analysis with the hierarchical reinforcement learning approach from the second paper, I can propose a novel method for efficient species-level identification in complex, dynamic environments, leveraging expert abstractions to enhance sample efficiency and generalization.",
2412.11084,1,applied,2412.11084-pos2-1,"Biological identifications through DNA barcodes; Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution.",2501.01098,"Tensor network method for solving the Ising model with a magnetic field; We study the two-dimensional square lattice Ising ferromagnet and
antiferromagnet with a magnetic field by using tensor network method. Focusing
on the role of guage fixing, we present the partition function in terms of a
tensor network. The tensor has a different symmetry property for ferromagnets
and antiferromagnets. The tensor network of the partition function is
interpreted as a multiple product of the one-dimensional quantum Hamiltonian.
We perform infinite density matrix renormalization group to contract the
two-dimensional tensor network. We present the numerical result of
magnetization and entanglement entropy for the Ising ferromagnet and
antiferromagnet side by side. In order to determine the critical line in the
parameter space of temperature and magnetic field, we use the half-chain
entanglement entropy of the one-dimensional quantum state. The entanglement
entropy precisely indicates the critical line forming the parabolic shape for
the antiferromagnetic case, but shows the critical point for the ferromagnetic
case.",False,False,"Although both papers introduce innovative ideas, combining DNA barcoding with tensor network methods for solving the Ising model does not provide a clear connection or a novel interdisciplinary research idea. The fields of biological identification and quantum physics are too disparate, and there is no obvious overlap or synergy between the two concepts.",
2412.11084,1,applied,2412.11084-pos2-1,"Biological identifications through DNA barcodes; Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution.",2501.18401,"MatIR: A Hybrid Mamba-Transformer Image Restoration Model; In recent years, Transformers-based models have made significant progress in
the field of image restoration by leveraging their inherent ability to capture
complex contextual features. Recently, Mamba models have made a splash in the
field of computer vision due to their ability to handle long-range dependencies
and their significant computational efficiency compared to Transformers.
However, Mamba currently lags behind Transformers in contextual learning
capabilities. To overcome the limitations of these two models, we propose a
Mamba-Transformer hybrid image restoration model called MatIR. Specifically,
MatIR cross-cycles the blocks of the Transformer layer and the Mamba layer to
extract features, thereby taking full advantage of the advantages of the two
architectures. In the Mamba module, we introduce the Image Inpainting State
Space (IRSS) module, which traverses along four scan paths to achieve efficient
processing of long sequence data. In the Transformer module, we combine
triangular window-based local attention with channel-based global attention to
effectively activate the attention mechanism over a wider range of image
pixels. Extensive experimental results and ablation studies demonstrate the
effectiveness of our approach.",False,True,"I believe a novel multidisciplinary research idea can be extracted by combining DNA barcoding from Paper 1 with image restoration techniques from Paper 2. For instance, developing a system that uses image restoration to enhance low-quality images of biological specimens, which are then used to generate DNA barcodes for identification.",
2412.11084,1,applied,2412.11084-pos2-1,"Biological identifications through DNA barcodes; Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution.",2502.07525,"Evidence for similar collectivity of high transverse momentum particles
  in pPb and PbPb collisions; Charged hadron elliptic anisotropies ($v_2$) are presented over a wide
transverse momentum ($p_\text{T}$) range for proton-lead (pPb) and lead-lead
(PbPb) collisions at nucleon-nucleon center-of-mass energies of 8.16 and 5.02
TeV, respectively. The data were recorded by the CMS experiment and correspond
to integrated luminosities of 186 nb$^{-1}$ and 0.607 nb$^{-1}$ for the pPb and
PbPb systems, respectively. A four-particle cumulant analysis is performed
using subevents separated in pseudorapidity to effectively suppress
non-collective effects. At high $p_\text{T}$ ($p_\text{T}$ $\gt$ 8 GeV),
significant positive $v_2$ values are observed that are similar between pPb and
PbPb collisions at comparable charged particle multiplicities. This observation
suggests a common origin for the multi-particle collectivity for
high-$p_\text{T}$ particles in the two systems.",False,False,"Although both papers present innovative ideas in their respective fields, there is no clear connection between DNA barcoding in biology and high-energy particle collisions in physics. The concepts and methods presented in the two papers do not overlap or complement each other, making it difficult to extract a novel multidisciplinary research idea.",
2412.11084,1,applied,2412.11084-pos2-1,"Biological identifications through DNA barcodes; Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution.",2502.08406,"Embedding and compact embedding between Bergman and Hardy spaces; For Hardy spaces and weighted Bergman spaces on the open unit ball in
${\mathbb C}^n$, we determine exactly when $A^p_\alpha\subset H^q$ or
$H^p\subset A^q_\alpha$, where $0<q<\infty$, $0<p<\infty$, and
$-\infty<\alpha<\infty$. For each such inclusion we also determine exactly when
it is a compact embedding. Although some special cases were known before, we
are able to completely cover all possible cases here. We also introduce a new
notion called {\it tight fitting} and formulate a conjecture in terms of it,
which places several prominent known results about contractive embeddings in
the same framework.",False,False,"The two papers are from distinct disciplines, biology and mathematics, but there is no apparent connection between DNA barcoding and embedding between Bergman and Hardy spaces. The concepts and techniques used in the papers are unrelated, making it difficult to extract a novel multidisciplinary research idea that meets the standards of interdisciplinary research.",
2412.11084,1,applied,2412.11084-pos2-1,"Biological identifications through DNA barcodes; Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution.",2502.19746,"An Isometric Embedding of the $\ell^\infty$ product space of two bounded
  subspaces of the Gromov-Hausdorff Space into the Gromov-Hausdorff Space; In this paper, we prove the $\ell^\infty$ product space of two bounded
subspaces of the Gromov-Hausdorff space can be isometrically embedded into the
Gromov-Hausdorff space.",False,False,"The two papers come from distinct disciplines, biology and mathematics, but there is no apparent connection between DNA barcoding and the Gromov-Hausdorff space. The concepts and methods presented in the papers do not lend themselves to a straightforward combination, making it difficult to extract a novel multidisciplinary research idea.",
2412.00036,1,applied,2412.00036-pos1-2,"Quant GANs: deep generation of financial time series; Modeling financial time series by stochastic processes is a challenging task
and a central area of research in financial mathematics. As an alternative, we
introduce Quant GANs, a data-driven model which is inspired by the recent
success of generative adversarial networks (GANs). Quant GANs consist of a
generator and discriminator function, which utilize temporal convolutional
networks (TCNs) and thereby achieve to capture long-range dependencies such as
the presence of volatility clusters. The generator function is explicitly
constructed such that the induced stochastic process allows a transition to its
risk-neutral distribution. Our numerical results highlight that distributional
properties for small and large lags are in an excellent agreement and
dependence properties such as volatility clusters, leverage effects, and serial
autocorrelations can be generated by the generator function of Quant GANs,
demonstrably in high fidelity.",2412.00036-pos2-2,"On the Distribution of the Two-Sample Cramer-von Mises Criterion; The Cramer-von Mises $\omega^2$ criterion for testing that a sample, $x_1, \cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \begin{equation*}\tag{1}\omega^2 = \int^\infty_{-\infty} \lbrack F_N(x) - F(x)\rbrack^2 dF(x),\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\omega^2$, namely \begin{equation*}\tag{2} T NM/(N + M)\rbrack G_M(x)\rbrack^2 dH_{N+M}(x),\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\rbrack$. limiting $N\omega^2$ as $N \rightarrow \infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \infty, M \infty$, $N/M \lambda$, $\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4].",True,True,"The combination of Quant GANs for generating financial time series and the Cramer-von Mises criterion for testing distributional differences can lead to a novel research idea. For instance, using Quant GANs to generate synthetic financial time series and then applying the Cramer-von Mises criterion to test for distributional differences between the generated series and real-world data, providing a new approach for evaluating the accuracy of financial time series models.",
2412.00036,1,applied,2412.00036-pos1-2,"Quant GANs: deep generation of financial time series; Modeling financial time series by stochastic processes is a challenging task
and a central area of research in financial mathematics. As an alternative, we
introduce Quant GANs, a data-driven model which is inspired by the recent
success of generative adversarial networks (GANs). Quant GANs consist of a
generator and discriminator function, which utilize temporal convolutional
networks (TCNs) and thereby achieve to capture long-range dependencies such as
the presence of volatility clusters. The generator function is explicitly
constructed such that the induced stochastic process allows a transition to its
risk-neutral distribution. Our numerical results highlight that distributional
properties for small and large lags are in an excellent agreement and
dependence properties such as volatility clusters, leverage effects, and serial
autocorrelations can be generated by the generator function of Quant GANs,
demonstrably in high fidelity.",2501.15557,"Preventing Household Bankruptcy: The One-Third Rule in Financial
  Planning with Mathematical Validation and Game-Theoretic Insights; This paper analyzes the 1/3 Financial Rule, a method of allocating income
equally among debt repayment, savings, and living expenses. Through
mathematical modeling, game theory, behavioral finance, and technological
analysis, we examine the rule's potential for supporting household financial
stability and reducing bankruptcy risk. The research develops theoretical
foundations using utility maximization theory, demonstrating how equal
allocation emerges as a solution under standard economic assumptions. The
game-theoretic analysis explores the rule's effectiveness across different
household structures, revealing potential strategic advantages in financial
decision-making. We investigate psychological factors influencing financial
choices, including cognitive biases and neurobiological mechanisms that impact
economic behavior. Technological approaches, such as AI-driven personalization,
blockchain tracking, and smart contract applications, are examined for their
potential to support financial planning. Empirical validation using U.S. Census
data and longitudinal studies assesses the rule's performance across various
household types. Stress testing under different economic conditions provides
insights into its adaptability and resilience. The research integrates
mathematical analysis with behavioral insights and technological perspectives
to develop a comprehensive approach to household financial management.",False,True,"The combination of Quant GANs' ability to generate realistic financial time series and the One-Third Rule's approach to household financial planning offers a novel multidisciplinary research idea. By using Quant GANs to simulate various financial scenarios, researchers can test the effectiveness of the One-Third Rule in different economic conditions, providing a more comprehensive understanding of household financial stability.",
2412.00036,1,applied,2412.00036-pos1-2,"Quant GANs: deep generation of financial time series; Modeling financial time series by stochastic processes is a challenging task
and a central area of research in financial mathematics. As an alternative, we
introduce Quant GANs, a data-driven model which is inspired by the recent
success of generative adversarial networks (GANs). Quant GANs consist of a
generator and discriminator function, which utilize temporal convolutional
networks (TCNs) and thereby achieve to capture long-range dependencies such as
the presence of volatility clusters. The generator function is explicitly
constructed such that the induced stochastic process allows a transition to its
risk-neutral distribution. Our numerical results highlight that distributional
properties for small and large lags are in an excellent agreement and
dependence properties such as volatility clusters, leverage effects, and serial
autocorrelations can be generated by the generator function of Quant GANs,
demonstrably in high fidelity.",2501.1201,"The role of FDI along transitional dynamics of the host country in an
  endogenous growth model; We investigate the role of foreign direct investment (FDI) in the
transitional dynamics of host countries by using an optimal growth model. FDI
may be beneficial for the host country because local people can work for
multinational firms to get a favorable salary. However, if the host country
only focuses on FDI, it may face a middle-income trap. We show that if the host
country invests in research and development, its economy may have sustained
growth. Moreover, in this case, FDI helps the host country only at the first
stages of its development process.",False,True,"By combining the concepts of Quant GANs from Paper 1 and the role of FDI in economic growth from Paper 2, a novel research idea can be formed to investigate the impact of FDI on financial time series in host countries using Quant GANs, providing a data-driven approach to model the complex relationships between FDI and economic growth.",
2412.00036,1,applied,2412.00036-pos1-2,"Quant GANs: deep generation of financial time series; Modeling financial time series by stochastic processes is a challenging task
and a central area of research in financial mathematics. As an alternative, we
introduce Quant GANs, a data-driven model which is inspired by the recent
success of generative adversarial networks (GANs). Quant GANs consist of a
generator and discriminator function, which utilize temporal convolutional
networks (TCNs) and thereby achieve to capture long-range dependencies such as
the presence of volatility clusters. The generator function is explicitly
constructed such that the induced stochastic process allows a transition to its
risk-neutral distribution. Our numerical results highlight that distributional
properties for small and large lags are in an excellent agreement and
dependence properties such as volatility clusters, leverage effects, and serial
autocorrelations can be generated by the generator function of Quant GANs,
demonstrably in high fidelity.",2502.08875,"Utilizing Pre-trained and Large Language Models for 10-K Items
  Segmentation; Extracting specific items from 10-K reports remains challenging due to
variations in document formats and item presentation. Traditional rule-based
item segmentation approaches often yield suboptimal results. This study
introduces two advanced item segmentation methods leveraging language models:
(1) GPT4ItemSeg, using a novel line-ID-based prompting mechanism to utilize
GPT4 for item segmentation, and (2) BERT4ItemSeg, combining BERT embeddings
with a Bi-LSTM model in a hierarchical structure to overcome context window
constraints. Trained and evaluated on 3,737 annotated 10-K reports,
BERT4ItemSeg achieved a macro-F1 of 0.9825, surpassing GPT4ItemSeg (0.9567),
conditional random field (0.9818), and rule-based methods (0.9048) for core
items (1, 1A, 3, and 7). These approaches enhance item segmentation
performance, improving text analytics in accounting and finance. BERT4ItemSeg
offers satisfactory item segmentation performance, while GPT4ItemSeg can easily
adapt to regulatory changes. Together, they offer practical benefits for
researchers and practitioners, enabling reliable empirical studies and
automated 10-K item segmentation functionality.",False,True,"I can extract concepts from both papers to create a novel multidisciplinary research idea. By combining the Quant GANs' ability to generate financial time series with the language models' capability to segment 10-K reports, a new research project can be formed to generate synthetic financial reports that mimic real-world data, enabling more robust financial analysis and modeling.",
2412.00036,1,applied,2412.00036-pos1-2,"Quant GANs: deep generation of financial time series; Modeling financial time series by stochastic processes is a challenging task
and a central area of research in financial mathematics. As an alternative, we
introduce Quant GANs, a data-driven model which is inspired by the recent
success of generative adversarial networks (GANs). Quant GANs consist of a
generator and discriminator function, which utilize temporal convolutional
networks (TCNs) and thereby achieve to capture long-range dependencies such as
the presence of volatility clusters. The generator function is explicitly
constructed such that the induced stochastic process allows a transition to its
risk-neutral distribution. Our numerical results highlight that distributional
properties for small and large lags are in an excellent agreement and
dependence properties such as volatility clusters, leverage effects, and serial
autocorrelations can be generated by the generator function of Quant GANs,
demonstrably in high fidelity.",2503.17778,"Heterogeneity of household stock portfolios in a national market; We study the long term dynamics of the stock portfolios owned by single
Finnish legal entities in the Helsinki venue of the Nasdaq Nordic between 2001
and 2021. Using the Herfindahl-Hirschman index as a measure of concentration
for the composition of stock portfolios, we investigate the concentration of
Finnish household portfolios both at the level of each individual household and
tracking the time evolution of an aggregated Finnish household portfolio. We
also consider aggregated portfolios of two other macro categories of investors
one comprising Finnish institutional investors and the other comprising foreign
investors. Different macro categories of investors present a different degree
of concentration of aggregated stock portfolios with highest concentration
observed for foreign investors. For individual Finnish retail investors,
portfolio concentration estimated by the Herfindahl-Hirschman index presents
high values for more than half of the total number of retail investors. In
spite of the observation that retail stock portfolios are often composed by
just a few stocks, the concentration of the aggregated stock portfolio for
Finnish retail investors has a portfolio concentration comparable with the one
of Finnish institutional investors. Within retail investors, stock portfolios
of women present a similar pattern of portfolios of men but with a systematic
higher level of concentration observed for women both at individual and at
aggregated level.",False,True,"The combination of Quant GANs from Paper 1 and the analysis of household stock portfolios from Paper 2 can lead to a novel multidisciplinary research idea. For instance, using Quant GANs to generate synthetic financial time series data to simulate and analyze the behavior of diverse household portfolios, allowing for a more comprehensive understanding of portfolio concentration and risk-neutral distribution.",
2412.00036,1,applied,2412.00036-pos1-2,"Quant GANs: deep generation of financial time series; Modeling financial time series by stochastic processes is a challenging task
and a central area of research in financial mathematics. As an alternative, we
introduce Quant GANs, a data-driven model which is inspired by the recent
success of generative adversarial networks (GANs). Quant GANs consist of a
generator and discriminator function, which utilize temporal convolutional
networks (TCNs) and thereby achieve to capture long-range dependencies such as
the presence of volatility clusters. The generator function is explicitly
constructed such that the induced stochastic process allows a transition to its
risk-neutral distribution. Our numerical results highlight that distributional
properties for small and large lags are in an excellent agreement and
dependence properties such as volatility clusters, leverage effects, and serial
autocorrelations can be generated by the generator function of Quant GANs,
demonstrably in high fidelity.",2503.10644,"Combined climate stress testing of supply-chain networks and the
  financial system with nation-wide firm-level emission estimates; On the way towards carbon neutrality, climate stress testing provides
estimates for the physical and transition risks that climate change poses to
the economy and the financial system. Missing firm-level CO2 emissions data
severely impedes the assessment of transition risks originating from carbon
pricing. Based on the individual emissions of all Hungarian firms (410,523), as
estimated from their fossil fuel purchases, we conduct a stress test of both
actual and hypothetical carbon pricing policies. Using a simple 1:1 economic
ABM and introducing the new carbon-to-profit ratio, we identify firms that
become unprofitable and default, and estimate the respective loan write-offs.
We find that 45% of all companies are directly exposed to carbon pricing. At a
price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank
equity losses of 1.2% are expected. Secondary default cascades in supply chain
networks could increase these losses by 300% to 4000%, depending on firms'
ability to substitute essential inputs. To reduce transition risks, firms
should reduce their dependence on essential inputs from supply chains with high
CO2 exposure. We discuss the implications of different policy implementations
on these transition risks.",False,True,"The combination of Quant GANs from Paper 1 and climate stress testing from Paper 2 can create a novel multidisciplinary research idea. By using Quant GANs to generate financial time series under different climate stress scenarios, researchers can better assess the impact of climate change on the financial system and supply-chain networks.",
2412.00036,1,applied,2412.00036-pos2-2,"On the Distribution of the Two-Sample Cramer-von Mises Criterion; The Cramer-von Mises $\omega^2$ criterion for testing that a sample, $x_1, \cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \begin{equation*}\tag{1}\omega^2 = \int^\infty_{-\infty} \lbrack F_N(x) - F(x)\rbrack^2 dF(x),\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\omega^2$, namely \begin{equation*}\tag{2} T NM/(N + M)\rbrack G_M(x)\rbrack^2 dH_{N+M}(x),\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\rbrack$. limiting $N\omega^2$ as $N \rightarrow \infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \infty, M \infty$, $N/M \lambda$, $\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4].",2503.15985,"Exploring the Reliability of Self-explanation and its Relationship with
  Classification in Language Model-driven Financial Analysis; Language models (LMs) have exhibited exceptional versatility in reasoning and
in-depth financial analysis through their proprietary information processing
capabilities. Previous research focused on evaluating classification
performance while often overlooking explainability or pre-conceived that
refined explanation corresponds to higher classification accuracy. Using a
public dataset in finance domain, we quantitatively evaluated self-explanations
by LMs, focusing on their factuality and causality. We identified the
statistically significant relationship between the accuracy of classifications
and the factuality or causality of self-explanations. Our study built an
empirical foundation for approximating classification confidence through
self-explanations and for optimizing classification via proprietary reasoning.",False,True,"The combination of statistical hypothesis testing from Paper 1 and language model-driven financial analysis from Paper 2 can lead to a novel multidisciplinary research idea. For instance, one could develop a framework to statistically evaluate the reliability of self-explanations in language models for financial analysis, using techniques like the Cramer-von Mises criterion to assess the distribution of explanations.",
2412.00036,1,applied,2412.00036-pos2-2,"On the Distribution of the Two-Sample Cramer-von Mises Criterion; The Cramer-von Mises $\omega^2$ criterion for testing that a sample, $x_1, \cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \begin{equation*}\tag{1}\omega^2 = \int^\infty_{-\infty} \lbrack F_N(x) - F(x)\rbrack^2 dF(x),\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\omega^2$, namely \begin{equation*}\tag{2} T NM/(N + M)\rbrack G_M(x)\rbrack^2 dH_{N+M}(x),\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\rbrack$. limiting $N\omega^2$ as $N \rightarrow \infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \infty, M \infty$, $N/M \lambda$, $\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4].",2503.06479,"ExKG-LLM: Leveraging Large Language Models for Automated Expansion of
  Cognitive Neuroscience Knowledge Graphs; The paper introduces ExKG-LLM, a framework designed to automate the expansion
of cognitive neuroscience knowledge graphs (CNKG) using large language models
(LLMs). It addresses limitations in existing tools by enhancing accuracy,
completeness, and usefulness in CNKG. The framework leverages a large dataset
of scientific papers and clinical reports, applying state-of-the-art LLMs to
extract, optimize, and integrate new entities and relationships. Evaluation
metrics include precision, recall, and graph density. Results show significant
improvements: precision (0.80, +6.67%), recall (0.81, +15.71%), F1 score
(0.805, +11.81%), and increased edge nodes (21.13% and 31.92%). Graph density
slightly decreased, reflecting a broader but more fragmented structure.
Engagement rates rose by 20%, while CNKG diameter increased to 15, indicating a
more distributed structure. Time complexity improved to O(n log n), but space
complexity rose to O(n2), indicating higher memory usage. ExKG-LLM demonstrates
potential for enhancing knowledge generation, semantic search, and clinical
decision-making in cognitive neuroscience, adaptable to broader scientific
fields.",False,False,"The two papers come from distinct areas, statistics and cognitive neuroscience, but the concepts presented in the papers do not complement each other in a way that can be integrated to form a novel multidisciplinary research idea. The statistical methods in Paper 1 do not directly relate to the knowledge graph expansion framework in Paper 2.",
2412.00036,1,applied,2412.00036-pos2-2,"On the Distribution of the Two-Sample Cramer-von Mises Criterion; The Cramer-von Mises $\omega^2$ criterion for testing that a sample, $x_1, \cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \begin{equation*}\tag{1}\omega^2 = \int^\infty_{-\infty} \lbrack F_N(x) - F(x)\rbrack^2 dF(x),\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\omega^2$, namely \begin{equation*}\tag{2} T NM/(N + M)\rbrack G_M(x)\rbrack^2 dH_{N+M}(x),\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\rbrack$. limiting $N\omega^2$ as $N \rightarrow \infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \infty, M \infty$, $N/M \lambda$, $\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4].",2503.11951,"SagaLLM: Context Management, Validation, and Transaction Guarantees for
  Multi-Agent LLM Planning; Recent LLM-based agent frameworks have demonstrated impressive capabilities
in task delegation and workflow orchestration, but face significant challenges
in maintaining context awareness and ensuring planning consistency. This paper
presents SagaLLM, a structured multi-agent framework that addresses four
fundamental limitations in current LLM approaches: inadequate self-validation,
context narrowing, lacking transaction properties, and insufficient inter-agent
coordination. By implementing specialized context management agents and
validation protocols, SagaLLM preserves critical constraints and state
information throughout complex planning processes, enabling robust and
consistent decision-making even during disruptions. We evaluate our approach
using selected problems from the REALM benchmark, focusing on sequential and
reactive planning scenarios that challenge both context retention and adaptive
reasoning. Our experiments with state-of-the-art LLMs, Claude 3.7, DeepSeek R1,
GPT-4o, and GPT-o1, demonstrate that while these models exhibit impressive
reasoning capabilities, they struggle with maintaining global constraint
awareness during complex planning tasks, particularly when adapting to
unexpected changes. In contrast, the distributed cognitive architecture of
SagaLLM shows significant improvements in planning consistency, constraint
enforcement, and adaptation to disruptions in various scenarios.",False,False,"The two papers come from distinct areas of statistics and artificial intelligence, but they don't share a common ground that can be combined to create a novel multidisciplinary research idea. The statistical concept of the Cramer-von Mises criterion doesn't relate to the context management and validation protocols in multi-agent LLM planning.",
2412.00036,1,applied,2412.00036-pos2-2,"On the Distribution of the Two-Sample Cramer-von Mises Criterion; The Cramer-von Mises $\omega^2$ criterion for testing that a sample, $x_1, \cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \begin{equation*}\tag{1}\omega^2 = \int^\infty_{-\infty} \lbrack F_N(x) - F(x)\rbrack^2 dF(x),\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\omega^2$, namely \begin{equation*}\tag{2} T NM/(N + M)\rbrack G_M(x)\rbrack^2 dH_{N+M}(x),\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\rbrack$. limiting $N\omega^2$ as $N \rightarrow \infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \infty, M \infty$, $N/M \lambda$, $\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4].",2502.04512,"Safety is Essential for Responsible Open-Ended Systems; AI advancements have been significantly driven by a combination of foundation
models and curiosity-driven learning aimed at increasing capability and
adaptability. A growing area of interest within this field is Open-Endedness -
the ability of AI systems to continuously and autonomously generate novel and
diverse artifacts or solutions. This has become relevant for accelerating
scientific discovery and enabling continual adaptation in AI agents. This
position paper argues that the inherently dynamic and self-propagating nature
of Open-Ended AI introduces significant, underexplored risks, including
challenges in maintaining alignment, predictability, and control. This paper
systematically examines these challenges, proposes mitigation strategies, and
calls for action for different stakeholders to support the safe, responsible
and successful development of Open-Ended AI.",False,False,"The two papers are from distinct areas of statistics and artificial intelligence, but they don't share common concepts or ideas that can be combined to create a novel multidisciplinary research idea. The first paper focuses on statistical testing, while the second paper discusses the safety of open-ended AI systems, with no apparent overlap or connection.",
2412.00036,1,applied,2412.00036-pos2-2,"On the Distribution of the Two-Sample Cramer-von Mises Criterion; The Cramer-von Mises $\omega^2$ criterion for testing that a sample, $x_1, \cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \begin{equation*}\tag{1}\omega^2 = \int^\infty_{-\infty} \lbrack F_N(x) - F(x)\rbrack^2 dF(x),\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\omega^2$, namely \begin{equation*}\tag{2} T NM/(N + M)\rbrack G_M(x)\rbrack^2 dH_{N+M}(x),\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\rbrack$. limiting $N\omega^2$ as $N \rightarrow \infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \infty, M \infty$, $N/M \lambda$, $\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4].",2502.07374,"LLMs Can Easily Learn to Reason from Demonstrations Structure, not
  content, is what matters!; Large reasoning models (LRMs) tackle complex reasoning problems by following
long chain-of-thoughts (Long CoT) that incorporate reflection, backtracking,
and self-validation. However, the training techniques and data requirements to
elicit Long CoT remain poorly understood. In this work, we find that a Large
Language model (LLM) can effectively learn Long CoT reasoning through
data-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank
adaptation (LoRA). With just 17k long CoT training samples, the
Qwen2.5-32B-Instruct model achieves significant improvements on a wide range of
math and coding benchmarks, including 56.7% (+40.0%) on AIME 2024 and 57.0%
(+8.1%) on LiveCodeBench, competitive to the proprietary o1-preview model's
score of 44.6% and 59.1%. More importantly, we find that the structure of Long
CoT is critical to the learning process, whereas the content of individual
reasoning steps has minimal impact. Perturbations affecting content, such as
training on incorrect samples or removing reasoning keywords, have little
impact on performance. In contrast, structural modifications that disrupt
logical consistency in the Long CoT, such as shuffling or deleting reasoning
steps, significantly degrade accuracy. For example, a model trained on Long CoT
samples with incorrect answers still achieves only 3.2% lower accuracy compared
to training with fully correct samples. These insights deepen our understanding
of how to elicit reasoning capabilities in LLMs and highlight key
considerations for efficiently training the next generation of reasoning
models. This is the academic paper of our previous released Sky-T1-32B-Preview
model. Codes are available at https://github.com/NovaSky-AI/SkyThought.",False,False,"Although both papers introduce innovative ideas, they belong to distinct disciplines (statistics and artificial intelligence) with no apparent overlap. The Cramer-von Mises criterion is a statistical method for hypothesis testing, whereas the LLM paper focuses on training language models for reasoning tasks.",
2412.00036,1,applied,2412.00036-pos1-2,"Quant GANs: deep generation of financial time series; Modeling financial time series by stochastic processes is a challenging task
and a central area of research in financial mathematics. As an alternative, we
introduce Quant GANs, a data-driven model which is inspired by the recent
success of generative adversarial networks (GANs). Quant GANs consist of a
generator and discriminator function, which utilize temporal convolutional
networks (TCNs) and thereby achieve to capture long-range dependencies such as
the presence of volatility clusters. The generator function is explicitly
constructed such that the induced stochastic process allows a transition to its
risk-neutral distribution. Our numerical results highlight that distributional
properties for small and large lags are in an excellent agreement and
dependence properties such as volatility clusters, leverage effects, and serial
autocorrelations can be generated by the generator function of Quant GANs,
demonstrably in high fidelity.",2502.04126,"RC Measurement Uncertainty Estimation Method for Directive Antennas and
  Turntable Stirring; This paper investigates measurement uncertainty in a Reverberation Chamber
(RC) within the lower FR2 bands (24.25-29.5 GHz). The study focuses on the
impact of several factors contributing to RC measurement uncertainty, including
finite sample size, polarization imbalance, and spatial non-uniformity. A
series of 24 measurements were conducted using a horn antenna, known for its
directivity in mmWave frequencies, varying antenna parameters such as height,
orientation, position on the turntable, and polarization within a predefined
chamber volume. The measurement uncertainty was evaluated by a method based on
the standardized 3GPP and CTIA approaches, incorporating uncorrelated
measurements and analyzing Pearson correlation coefficients between measurement
pairs. An analysis of variance (ANOVA) was performed on the frequency-averaged
power transfer function to identify the significance and impact of each
variable on measurement variability. Additionally, the K-factor was estimated
for each measurement set as part of the RC characterization, using an
alternative approach to account for the turntable stirring effect. The findings
highlight which variables most significantly influence measurement uncertainty,
where the antenna orientation emerges as the most significant factor for the
mmWave directive antenna setup.",False,False,"Although both papers present innovative methods in their respective fields, the combination of ideas from Quant GANs and RC measurement uncertainty estimation does not seem to lead to a novel multidisciplinary research idea that meets all the standards. The topics appear unrelated, and integrating them may not yield a feasible, novel, and useful research project.",
2412.00036,1,applied,2412.00036-pos1-2,"Quant GANs: deep generation of financial time series; Modeling financial time series by stochastic processes is a challenging task
and a central area of research in financial mathematics. As an alternative, we
introduce Quant GANs, a data-driven model which is inspired by the recent
success of generative adversarial networks (GANs). Quant GANs consist of a
generator and discriminator function, which utilize temporal convolutional
networks (TCNs) and thereby achieve to capture long-range dependencies such as
the presence of volatility clusters. The generator function is explicitly
constructed such that the induced stochastic process allows a transition to its
risk-neutral distribution. Our numerical results highlight that distributional
properties for small and large lags are in an excellent agreement and
dependence properties such as volatility clusters, leverage effects, and serial
autocorrelations can be generated by the generator function of Quant GANs,
demonstrably in high fidelity.",2503.04797,"Parallel Corpora for Machine Translation in Low-resource Indic
  Languages: A Comprehensive Review; Parallel corpora play an important role in training machine translation (MT)
models, particularly for low-resource languages where high-quality bilingual
data is scarce. This review provides a comprehensive overview of available
parallel corpora for Indic languages, which span diverse linguistic families,
scripts, and regional variations. We categorize these corpora into
text-to-text, code-switched, and various categories of multimodal datasets,
highlighting their significance in the development of robust multilingual MT
systems. Beyond resource enumeration, we critically examine the challenges
faced in corpus creation, including linguistic diversity, script variation,
data scarcity, and the prevalence of informal textual content.We also discuss
and evaluate these corpora in various terms such as alignment quality and
domain representativeness. Furthermore, we address open challenges such as data
imbalance across Indic languages, the trade-off between quality and quantity,
and the impact of noisy, informal, and dialectal data on MT performance.
Finally, we outline future directions, including leveraging cross-lingual
transfer learning, expanding multilingual datasets, and integrating multimodal
resources to enhance translation quality. To the best of our knowledge, this
paper presents the first comprehensive review of parallel corpora specifically
tailored for low-resource Indic languages in the context of machine
translation.",False,False,"Although both papers introduce innovative ideas in their respective fields, they seem unrelated and don't offer a clear intersection for a multidisciplinary research idea. One paper focuses on financial time series generation using GANs, while the other reviews parallel corpora for machine translation in low-resource languages, making it challenging to extract a novel and feasible interdisciplinary research concept.",
2412.00036,1,applied,2412.00036-pos1-2,"Quant GANs: deep generation of financial time series; Modeling financial time series by stochastic processes is a challenging task
and a central area of research in financial mathematics. As an alternative, we
introduce Quant GANs, a data-driven model which is inspired by the recent
success of generative adversarial networks (GANs). Quant GANs consist of a
generator and discriminator function, which utilize temporal convolutional
networks (TCNs) and thereby achieve to capture long-range dependencies such as
the presence of volatility clusters. The generator function is explicitly
constructed such that the induced stochastic process allows a transition to its
risk-neutral distribution. Our numerical results highlight that distributional
properties for small and large lags are in an excellent agreement and
dependence properties such as volatility clusters, leverage effects, and serial
autocorrelations can be generated by the generator function of Quant GANs,
demonstrably in high fidelity.",2502.13744,"The Risk-Neutral Equivalent Pricing of Model-Uncertainty; Existing approaches to asset-pricing under model-uncertainty adapt classical
utility-maximisation frameworks and seek theoretical comprehensiveness. We move
toward practice by considering binary model-uncertainties and by switching
attention from 'preference' to 'constraints'. Economic asset-pricing in this
setting is found to decompose into the viable pricing of model-risk and of
non-model risk separately such that the former has a unique and intuitive
risk-neutral equivalent formulation with convenient properties. Its parameter,
a dynamically conserved constant of model-risk inference, allows an integrated
representation of ex-ante risk-pricing and bias, such that their ex-post
price-effects can be disentangled, through well-known price anomalies such as
Moment and Low-Risk.",False,True,"I can extract concepts from both papers to create a novel multidisciplinary research idea. By combining the Quant GANs' ability to generate financial time series with the risk-neutral equivalent pricing framework from the second paper, I can develop a new approach to pricing assets under model uncertainty using generative models.",
2412.00036,1,applied,2412.00036-pos1-2,"Quant GANs: deep generation of financial time series; Modeling financial time series by stochastic processes is a challenging task
and a central area of research in financial mathematics. As an alternative, we
introduce Quant GANs, a data-driven model which is inspired by the recent
success of generative adversarial networks (GANs). Quant GANs consist of a
generator and discriminator function, which utilize temporal convolutional
networks (TCNs) and thereby achieve to capture long-range dependencies such as
the presence of volatility clusters. The generator function is explicitly
constructed such that the induced stochastic process allows a transition to its
risk-neutral distribution. Our numerical results highlight that distributional
properties for small and large lags are in an excellent agreement and
dependence properties such as volatility clusters, leverage effects, and serial
autocorrelations can be generated by the generator function of Quant GANs,
demonstrably in high fidelity.",2502.09008,"Dynamics of the Bose-Hubbard Model Induced by On-Site or Long-Range
  Two-Body Losses; We present a theoretical study of the dissipative dynamics of the
Bose-Hubbard model induced by on-site or long-range two-body losses. We first
consider the one-dimensional chain and the two-dimensional square lattice, and
study the dynamics induced by the sudden switch-on of two-body losses on a
weakly-interacting superfluid state. The time-dependent density is obtained in
the spirit of the Bogolyubov approach by calculating theoretically the
equations of motion associated to the relevant quadratic bosonic correlators.
In the one-dimensional case, our results compare very well with quasi-exact
numerical calculations based on the quantum jump method implemented using
tensor networks. We find that the intermediate-time dynamics of the density
displays an algebraic decay characterized by an interaction-dependent power-law
exponent. The latter property still holds for long-range two-body loss
processes but it is absent in the two-dimensional square lattice with on-site
losses. We finally investigate the dissipative quench dynamics starting from a
strongly-correlated superfluid state or from a Mott-insulating state; for the
Bose-Hubbard chain initially confined in the superfluid-correlated regime, an
unexpected strong decay of the density appearing at short times is revealed.",False,False,"Although both papers introduce innovative ideas from distinct disciplines, the concepts presented in the papers are too disparate to be combined into a novel multidisciplinary research idea. The first paper focuses on financial time series modeling using GANs, while the second paper explores the dynamics of the Bose-Hubbard model in quantum physics. There is no clear connection or overlap between the two ideas, making it challenging to integrate them into a cohesive and feasible research project.",
2412.00036,1,applied,2412.00036-pos1-2,"Quant GANs: deep generation of financial time series; Modeling financial time series by stochastic processes is a challenging task
and a central area of research in financial mathematics. As an alternative, we
introduce Quant GANs, a data-driven model which is inspired by the recent
success of generative adversarial networks (GANs). Quant GANs consist of a
generator and discriminator function, which utilize temporal convolutional
networks (TCNs) and thereby achieve to capture long-range dependencies such as
the presence of volatility clusters. The generator function is explicitly
constructed such that the induced stochastic process allows a transition to its
risk-neutral distribution. Our numerical results highlight that distributional
properties for small and large lags are in an excellent agreement and
dependence properties such as volatility clusters, leverage effects, and serial
autocorrelations can be generated by the generator function of Quant GANs,
demonstrably in high fidelity.",2502.09885,"Comprehensive Review of Neural Differential Equations for Time Series
  Analysis; Time series modeling and analysis has become critical in various domains.
Conventional methods such as RNNs and Transformers, while effective for
discrete-time and regularly sampled data, face significant challenges in
capturing the continuous dynamics and irregular sampling patterns inherent in
real-world scenarios. Neural Differential Equations (NDEs) represent a paradigm
shift by combining the flexibility of neural networks with the mathematical
rigor of differential equations. This paper presents a comprehensive review of
NDE-based methods for time series analysis, including neural ordinary
differential equations, neural controlled differential equations, and neural
stochastic differential equations. We provide a detailed discussion of their
mathematical formulations, numerical methods, and applications, highlighting
their ability to model continuous-time dynamics. Furthermore, we address key
challenges and future research directions. This survey serves as a foundation
for researchers and practitioners seeking to leverage NDEs for advanced time
series analysis.",False,True,"The combination of Quant GANs and Neural Differential Equations (NDEs) can create a novel multidisciplinary research idea. By integrating the generator function of Quant GANs with NDEs, researchers can develop a more robust and flexible model for financial time series analysis, capturing both continuous dynamics and irregular sampling patterns.",
2412.00036,1,applied,2412.00036-pos2-2,"On the Distribution of the Two-Sample Cramer-von Mises Criterion; The Cramer-von Mises $\omega^2$ criterion for testing that a sample, $x_1, \cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \begin{equation*}\tag{1}\omega^2 = \int^\infty_{-\infty} \lbrack F_N(x) - F(x)\rbrack^2 dF(x),\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\omega^2$, namely \begin{equation*}\tag{2} T NM/(N + M)\rbrack G_M(x)\rbrack^2 dH_{N+M}(x),\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\rbrack$. limiting $N\omega^2$ as $N \rightarrow \infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \infty, M \infty$, $N/M \lambda$, $\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4].",2501.0611,"Quantum networks using rare-earth ions; We review concepts and recent work related to creating light-matter
interfaces for future quantum networks based on rare-earth ion-doped crystals.
More precisely, we explore their unique suitability for creating photon
sources, optical quantum memories for light, and qubits that allow quantum
information processing. In addition, we review the state-of-the-art of
elementary quantum repeater links, and provide suggestions for future research.",False,False,"The two papers come from distinct areas of statistics and quantum physics, but there is no clear connection between the concepts presented. The statistical methods in Paper 1 do not have an obvious application to the quantum networks discussed in Paper 2, and vice versa.",
2412.00036,1,applied,2412.00036-pos2-2,"On the Distribution of the Two-Sample Cramer-von Mises Criterion; The Cramer-von Mises $\omega^2$ criterion for testing that a sample, $x_1, \cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \begin{equation*}\tag{1}\omega^2 = \int^\infty_{-\infty} \lbrack F_N(x) - F(x)\rbrack^2 dF(x),\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\omega^2$, namely \begin{equation*}\tag{2} T NM/(N + M)\rbrack G_M(x)\rbrack^2 dH_{N+M}(x),\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\rbrack$. limiting $N\omega^2$ as $N \rightarrow \infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \infty, M \infty$, $N/M \lambda$, $\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4].",2503.07446,"EigenGS Representation: From Eigenspace to Gaussian Image Space; Principal Component Analysis (PCA), a classical dimensionality reduction
technique, and 2D Gaussian representation, an adaptation of 3D Gaussian
Splatting for image representation, offer distinct approaches to modeling
visual data. We present EigenGS, a novel method that bridges these paradigms
through an efficient transformation pipeline connecting eigenspace and
image-space Gaussian representations. Our approach enables instant
initialization of Gaussian parameters for new images without requiring
per-image optimization from scratch, dramatically accelerating convergence.
EigenGS introduces a frequency-aware learning mechanism that encourages
Gaussians to adapt to different scales, effectively modeling varied spatial
frequencies and preventing artifacts in high-resolution reconstruction.
Extensive experiments demonstrate that EigenGS not only achieves superior
reconstruction quality compared to direct 2D Gaussian fitting but also reduces
necessary parameter count and training time. The results highlight EigenGS's
effectiveness and generalization ability across images with varying resolutions
and diverse categories, making Gaussian-based image representation both
high-quality and viable for real-time applications.",False,False,"The two papers come from distinct disciplines, statistics and computer vision, but the concepts presented do not naturally lend themselves to integration. The statistical test in Paper 1 does not have an obvious connection to the Gaussian image representation in Paper 2, making it difficult to form a novel multidisciplinary research idea that meets the standards.",
2412.00036,1,applied,2412.00036-pos2-2,"On the Distribution of the Two-Sample Cramer-von Mises Criterion; The Cramer-von Mises $\omega^2$ criterion for testing that a sample, $x_1, \cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \begin{equation*}\tag{1}\omega^2 = \int^\infty_{-\infty} \lbrack F_N(x) - F(x)\rbrack^2 dF(x),\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\omega^2$, namely \begin{equation*}\tag{2} T NM/(N + M)\rbrack G_M(x)\rbrack^2 dH_{N+M}(x),\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\rbrack$. limiting $N\omega^2$ as $N \rightarrow \infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \infty, M \infty$, $N/M \lambda$, $\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4].",2501.12412,"Ordering digraphs with maximum outdegrees by their $A_{\alpha}$ spectral
  radius; Let $G$ be a strongly connected digraph with $n$ vertices and $m$ arcs. For
any real $\alpha\in[0,1]$, the $A_\alpha$ matrix of a digraph $G$ is defined as
$$A_\alpha(G)=\alpha D(G)+(1-\alpha)A(G),$$ where $A(G)$ is the adjacency
matrix of $G$ and $D(G)$ is the outdegrees diagonal matrix of $G$. The
eigenvalue of $A_\alpha(G)$ with the largest modulus is called the $A_\alpha$
spectral radius of $G$, denoted by $\lambda_{\alpha}(G)$. In this paper, we
first obtain an upper bound on $\lambda_{\alpha}(G)$ for
$\alpha\in[\frac{1}{2},1)$. Employing this upper bound, we prove that for two
strongly connected digraphs $G_1$ and $G_2$ with $n\ge4$ vertices and $m$ arcs,
and $\alpha\in [\frac{1}{\sqrt{2}},1)$, if the maximum outdegree
$\Delta^+(G_1)\ge 2\alpha(1-\alpha)(m-n+1)+2\alpha$ and
$\Delta^+(G_1)>\Delta^+(G_2)$, then $\lambda_\alpha(G_1)>\lambda_\alpha(G_2)$.
Moreover, We also give another upper bound on $\lambda_{\alpha}(G)$ for
$\alpha\in[\frac{1}{2},1)$. Employing this upper bound, we prove that for two
strongly connected digraphs with $m$ arcs, and $\alpha\in[\frac{1}{2},1)$, if
the maximum outdegree $\Delta^+(G_1)>\frac{2m}{3}+1$ and
$\Delta^+(G_1)>\Delta^+(G_2)$, then
$\lambda_\alpha(G_1)+\frac{1}{4}>\lambda_\alpha(G_2)$.",False,False,"The two papers come from distinct areas of statistics and graph theory, but they do not share common concepts or methods that can be combined to create a novel multidisciplinary research idea. The ideas presented in the papers are too specialized and do not lend themselves to integration.",
2412.00036,1,applied,2412.00036-pos2-2,"On the Distribution of the Two-Sample Cramer-von Mises Criterion; The Cramer-von Mises $\omega^2$ criterion for testing that a sample, $x_1, \cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \begin{equation*}\tag{1}\omega^2 = \int^\infty_{-\infty} \lbrack F_N(x) - F(x)\rbrack^2 dF(x),\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\omega^2$, namely \begin{equation*}\tag{2} T NM/(N + M)\rbrack G_M(x)\rbrack^2 dH_{N+M}(x),\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\rbrack$. limiting $N\omega^2$ as $N \rightarrow \infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \infty, M \infty$, $N/M \lambda$, $\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4].",2501.01518,"Reading to Listen at the Cocktail Party: Multi-Modal Speech Separation; The goal of this paper is speech separation and enhancement in multi-speaker
and noisy environments using a combination of different modalities. Previous
works have shown good performance when conditioning on temporal or static
visual evidence such as synchronised lip movements or face identity. In this
paper, we present a unified framework for multi-modal speech separation and
enhancement based on synchronous or asynchronous cues. To that end we make the
following contributions: (i) we design a modern Transformer-based architecture
tailored to fuse different modalities to solve the speech separation task in
the raw waveform domain; (ii) we propose conditioning on the textual content of
a sentence alone or in combination with visual information; (iii) we
demonstrate the robustness of our model to audio-visual synchronisation
offsets; and, (iv) we obtain state-of-the-art performance on the
well-established benchmark datasets LRS2 and LRS3.",False,True,"By combining the statistical analysis of distribution differences from Paper 1 with the multi-modal speech separation framework from Paper 2, a novel research idea emerges: developing a statistical framework for evaluating the distributional differences in speech patterns across multiple speakers in noisy environments, leveraging both audio and visual cues.",
2412.00036,1,applied,2412.00036-pos2-2,"On the Distribution of the Two-Sample Cramer-von Mises Criterion; The Cramer-von Mises $\omega^2$ criterion for testing that a sample, $x_1, \cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \begin{equation*}\tag{1}\omega^2 = \int^\infty_{-\infty} \lbrack F_N(x) - F(x)\rbrack^2 dF(x),\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\omega^2$, namely \begin{equation*}\tag{2} T NM/(N + M)\rbrack G_M(x)\rbrack^2 dH_{N+M}(x),\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\rbrack$. limiting $N\omega^2$ as $N \rightarrow \infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \infty, M \infty$, $N/M \lambda$, $\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4].",2503.14985,"ML-Triton, A Multi-Level Compilation and Language Extension to Triton
  GPU Programming; In the era of LLMs, dense operations such as GEMM and MHA are critical
components. These operations are well-suited for parallel execution using a
tilebased approach. While traditional GPU programming often relies on low level
interfaces like CUDA or SYCL, Triton has emerged as a DSL that offers a more
user-friendly and portable alternative by programming at a higher level. The
current Triton starts at the workgroup (aka threadblock) level, and directly
lowers to per-thread level. And then attempt to coalesce and amend through a
series of passes, promoting information from low-level representation. We
believe this is pre-mature lowering based on the below observations. 1. GPU has
a hierarchical structure both physically and logically. Modern GPUs often
feature SIMD units capable of directly operating on tiles on a warp or
warpgroup basis, such as blocked load and blocked MMA. 2. Multi-level gradual
lowering can make compiler decoupled and clean by separating considerations
inter and intra a logical layer. 3. Kernel developers often need fine control
to get good performance on the latest hardware. FlashAttention2 advocates
explicit data partition between warps to make a performance boost. In this
context, we propose ML-Triton which features multi-level compilation flow and
programming interface. Our approach begins at the workgroup level and
progressively lowers to the warp and intrinsic level, implementing a multilevel
lowering align with the hierarchical nature of GPU. Additionally, we extend
triton language to support user-set compiler hint and warp level programming,
enabling researchers to get good out-of-the box performance without awaiting
compiler updates. Experimental results demonstrate that our approach achieves
performance above 95% of expert-written kernels on Intel GPU, as measured by
the geometric mean.",False,False,"The two papers are from distinct areas, statistics and computer science, but they do not share a common problem or concept that can be combined to create a novel multidisciplinary research idea. The first paper discusses statistical hypothesis testing, while the second paper focuses on GPU programming and compiler design, making it challenging to integrate ideas from both papers.",
2411.0064,1,applied,2411.0064-pos1-3,"The Llama 3 Herd of Models; Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",2411.0064-pos2-3,"Quantifying Variance in Evaluation Benchmarks; Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models.",True,True,"Combining the Llama 3 model's capabilities with the variance quantification methods from the second paper could lead to a novel research idea. By integrating the compositional approach from Llama 3 with the variance metrics, researchers can develop more robust and reliable evaluation benchmarks for large language models.",
2411.0064,1,applied,2411.0064-pos1-3,"The Llama 3 Herd of Models; Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",2503.02427,"Monge-Kantorovich quantiles and ranks for image data; This paper defines quantiles, ranks and statistical depths for image data by
leveraging ideas from measure transportation. The first step is to embed a
distribution of images in a tangent space, with the framework of linear optimal
transport. Therein, Monge-Kantorovich quantiles are shown to provide a
meaningful ordering of image data, with outward images having unusual shapes.
Numerical experiments showcase the relevance of the proposed procedure, for
descriptive analysis, outlier detection or statistical testing.",False,True,"I can extract concepts from both papers to create a novel multidisciplinary research idea. The Llama 3 model's compositional approach to integrating image, video, and speech capabilities can be combined with the Monge-Kantorovich quantiles and ranks for image data to develop a more robust and meaningful image analysis framework, leveraging AI and measure transportation concepts.",
2411.0064,1,applied,2411.0064-pos1-3,"The Llama 3 Herd of Models; Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",2501.10147,"Regularized Sparse Optimal Discriminant Clustering; We propose a new method based on sparse optimal discriminant clustering
(SODC), by a penalty term to scoring matrix based on convex clustering. With
the addition of this penalty term, it is expected to improve the accuracy of
cluster identification by attaching points from the same cluster closer
together and points from different clusters further apart. Moreover, we develop
a novel algorithm to derive the updated formula of this scoring matrix using
majorizing function. It solves the difficulty to satisfy both constraint and
containing the clustering structure to the scoring matrix. We have demonstrated
the numerical simulations and its an application to real data to assess the
performance of the proposed method.",False,True,"I can extract concepts from both papers to create a novel multidisciplinary research idea. For instance, integrating the Llama 3 language model with the Regularized Sparse Optimal Discriminant Clustering method to develop an AI-powered clustering framework that can automatically identify and categorize complex data patterns in multilingual settings.",
2411.0064,1,applied,2411.0064-pos1-3,"The Llama 3 Herd of Models; Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",2503.15418,"A time-to-event three-outcome design for randomized phase II cancer
  trials; Tumor response, a binary variable, has historically been the main measure of
antitumor activity for many cancer phase II single-arm trials. Simon two-stage
designs are often used. Sargent et al. proposed a three-outcome trial design in
this setting which requires smaller sample sizes. For many new, molecularly
targeted therapies, however, tumor response may not be the most reliable
endpoint for measuring anti-tumor activity. Increasingly, time-to-event
endpoints, such as progression-free survival (PFS), are used in the phase II
setting. When such endpoints are the primary measure of efficacy, a randomized
concurrently controlled study design is usually required. Given limited
resources for phase II, studies are often underpowered with relatively large
type I and II error rates, and it is sometimes unavoidable to have a ""gray""
decision zone after phase II where a clear decision regarding further
development actions cannot be made without additional information. Compared
with an underpowered standard two-outcome study, a three-outcome design prompts
clinical trialists to contemplate the likelihood of landing in the ""gray"" zone
at the trial design stage and choose study design parameters more
appropriately. We propose a three-outcome design, with or without interim
analyses, for randomized comparative phase II trials when a time-to-event
endpoint is used.",False,False,"Although both papers present innovative ideas, they are from distinct areas with no direct connection. Paper 1 focuses on AI language models, while Paper 2 discusses cancer trial design. There is no clear intersection or potential for combining concepts to create a novel, feasible, and useful interdisciplinary research idea.",
2411.0064,1,applied,2411.0064-pos1-3,"The Llama 3 Herd of Models; Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",2501.00933,"Optimizing for Rotisserie Fantasy Basketball; Previous work on fantasy basketball has established methods for optimizing
team construction for head-to-head formats. This has been facilitated by the
straightforwardness of calculating the objective function for those formats,
given that underlying performance distributions are known. Rotisserie has not
been optimized in the same way because even with the assumption that
performance distributions are known, directly calculating the most natural
objective function is intractable. This work introduces a system for making a
tractable approximation of that objective function. The resulting simplified
objective function aligns well with the traditional wisdom that balanced teams
are preferable for the format, because it contains an implicit mechanism that
rewards teams for being balanced. Integrating this new objective function into
established optimization methods is shown to perform well in the context of
simulated seasons.",False,True,"Combining the language model capabilities of Llama 3 with the optimization methods for rotisserie fantasy basketball can lead to a novel multidisciplinary research idea. For instance, using Llama 3's natural language processing abilities to analyze player statistics, news, and expert opinions to inform the optimization of fantasy basketball teams.",
2411.0064,1,applied,2411.0064-pos1-3,"The Llama 3 Herd of Models; Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",2501.0692,"The ladder of abstraction in statistical graphics; Graphical forms such as scatterplots, line plots, and histograms are so
familiar that it can be easy to forget how abstract they are. As a result, we
often produce graphs that are difficult to follow. We propose a strategy for
graphical communication by climbing a ladder of abstraction (a term from
linguistics that we borrow from Hayakawa, 1939), starting with simple plots of
special cases and then at each step embedding a graph into a more general
framework. We demonstrate with two examples, first graphing a set of equations
related to a modeled trajectory and then graphing data from an analysis of
income and voting.",False,True,"I can extract concepts from both papers to create a novel multidisciplinary research idea. By integrating the compositional approach from Llama 3 with the ladder of abstraction from statistical graphics, a new framework for explainable AI can be developed, enabling more transparent and interpretable visualizations of complex AI models and their decision-making processes.",
2411.0064,1,applied,2411.0064-pos2-3,"Quantifying Variance in Evaluation Benchmarks; Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models.",2502.12054,"PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning; Large language models demonstrate remarkable capabilities across various
domains, especially mathematics and logic reasoning. However, current
evaluations overlook physics-based reasoning - a complex task requiring physics
theorems and constraints. We present PhysReason, a 1,200-problem benchmark
comprising knowledge-based (25%) and reasoning-based (75%) problems, where the
latter are divided into three difficulty levels (easy, medium, hard). Notably,
problems require an average of 8.1 solution steps, with hard requiring 15.6,
reflecting the complexity of physics-based reasoning. We propose the Physics
Solution Auto Scoring Framework, incorporating efficient answer-level and
comprehensive step-level evaluations. Top-performing models like Deepseek-R1,
Gemini-2.0-Flash-Thinking, and o3-mini-high achieve less than 60% on
answer-level evaluation, with performance dropping from knowledge questions
(75.11%) to hard problems (31.95%). Through step-level evaluation, we
identified four key bottlenecks: Physics Theorem Application, Physics Process
Understanding, Calculation, and Physics Condition Analysis. These findings
position PhysReason as a novel and comprehensive benchmark for evaluating
physics-based reasoning capabilities in large language models. Our code and
data will be published at https:/dxzxy12138.github.io/PhysReason.",False,True,"I can extract concepts from both papers to create a novel multidisciplinary research idea. By combining the variance quantification methods from Paper 1 with the physics-based reasoning benchmark from Paper 2, a new research project can investigate the impact of variance on physics-based reasoning capabilities in large language models, providing a more comprehensive understanding of their limitations and potential improvements.",
2411.0064,1,applied,2411.0064-pos2-3,"Quantifying Variance in Evaluation Benchmarks; Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models.",2502.09054,"Cost-Saving LLM Cascades with Early Abstention; LLM cascades are based on the idea that processing all queries with the
largest and most expensive LLMs is inefficient. Instead, cascades deploy small
LLMs to answer the majority of queries, limiting the use of large and expensive
LLMs to only the most difficult queries. This approach can significantly reduce
costs without impacting performance. However, risk-sensitive domains such as
finance or medicine place an additional premium on avoiding model errors.
Recognizing that even the most expensive models may make mistakes, applications
in these domains benefit from allowing LLM systems to completely abstain from
answering a query when the chance of making a mistake is significant. However,
giving a cascade the ability to abstain poses an immediate design question for
LLM cascades: should abstention only be allowed at the final model or also at
earlier models? Since the error patterns of small and large models are
correlated, the latter strategy may further reduce inference costs by letting
inexpensive models anticipate abstention decisions by expensive models, thereby
obviating the need to run the expensive models. We investigate the benefits of
""early abstention"" in LLM cascades and find that it reduces the overall test
loss by 2.2% on average across six benchmarks (GSM8K, MedMCQA, MMLU, TriviaQA,
TruthfulQA, and XSum). These gains result from a more effective use of
abstention, which trades a 4.1% average increase in the overall abstention rate
for a 13.0% reduction in cost and a 5.0% reduction in error rate. Our findings
demonstrate that it is possible to leverage correlations between the error
patterns of different language models to drive performance improvements for LLM
systems with abstention.",False,True,"I can extract concepts from both papers to create a novel multidisciplinary research idea. By combining the ideas of quantifying variance in evaluation benchmarks (Paper 1) and cost-saving LLM cascades with early abstention (Paper 2), a new research idea could investigate how to reduce variance in LLM cascades by incorporating early abstention strategies, leading to more efficient and accurate language model systems.",
2411.0064,1,applied,2411.0064-pos2-3,"Quantifying Variance in Evaluation Benchmarks; Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models.",2503.13879,"Bridging Social Psychology and LLM Reasoning: Conflict-Aware Meta-Review
  Generation via Cognitive Alignment; The rapid growth of scholarly submissions has overwhelmed traditional peer
review systems, driving the need for intelligent automation to preserve
scientific rigor. While large language models (LLMs) show promise in automating
manuscript critiques, their ability to synthesize high-stakes meta-reviews,
which require conflict-aware reasoning and consensus derivation, remains
underdeveloped. Existing methods fail to effectively handle conflicting
viewpoints within differing opinions, and often introduce additional cognitive
biases, such as anchoring effects and conformity bias.To overcome these
limitations, we propose the Cognitive Alignment Framework (CAF), a dual-process
architecture that transforms LLMs into adaptive scientific arbitrators. By
operationalizing Kahneman's dual-process theory, CAF introduces a three-step
cognitive pipeline: review initialization, incremental integration, and
cognitive alignment.Empirical validation shows that CAF outperforms existing
LLM-based methods, with sentiment consistency gains reaching up to 19.47\% and
content consistency improving by as much as 12.95\%.",False,True,"Both papers deal with large language models (LLMs), but from different angles. Paper 1 focuses on quantifying variance in evaluation benchmarks, while Paper 2 explores the application of LLMs in automating meta-reviews. Combining these ideas could lead to a novel multidisciplinary research project that develops more robust and reliable evaluation benchmarks for LLMs used in meta-review generation, taking into account the cognitive biases and conflicts that arise in this context.",
2411.0064,1,applied,2411.0064-pos2-3,"Quantifying Variance in Evaluation Benchmarks; Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models.",2503.11926,"Monitoring Reasoning Models for Misbehavior and the Risks of Promoting
  Obfuscation; Mitigating reward hacking--where AI systems misbehave due to flaws or
misspecifications in their learning objectives--remains a key challenge in
constructing capable and aligned models. We show that we can monitor a frontier
reasoning model, such as OpenAI o3-mini, for reward hacking in agentic coding
environments by using another LLM that observes the model's chain-of-thought
(CoT) reasoning. CoT monitoring can be far more effective than monitoring agent
actions and outputs alone, and we further found that a LLM weaker than o3-mini,
namely GPT-4o, can effectively monitor a stronger model. Because CoT monitors
can be effective at detecting exploits, it is natural to ask whether those
exploits can be suppressed by incorporating a CoT monitor directly into the
agent's training objective. While we show that integrating CoT monitors into
the reinforcement learning reward can indeed produce more capable and more
aligned agents in the low optimization regime, we find that with too much
optimization, agents learn obfuscated reward hacking, hiding their intent
within the CoT while still exhibiting a significant rate of reward hacking.
Because it is difficult to tell when CoTs have become obfuscated, it may be
necessary to pay a monitorability tax by not applying strong optimization
pressures directly to the chain-of-thought, ensuring that CoTs remain
monitorable and useful for detecting misaligned behavior.",False,True,"I can extract concepts from both papers to create a novel multidisciplinary research idea. By combining the evaluation benchmarks from Paper 1 with the chain-of-thought monitoring from Paper 2, a new research project can investigate the effectiveness of CoT monitoring in reducing variance in evaluation benchmarks for large language models.",
2411.0064,1,applied,2411.0064-pos2-3,"Quantifying Variance in Evaluation Benchmarks; Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models.",2501.17201,"Smart Cubing for Graph Search: A Comparative Study; Parallel solving via cube-and-conquer is a key method for scaling SAT solvers
to hard instances. While cube-and-conquer has proven successful for pure SAT
problems, notably the Pythagorean triples conjecture, its application to SAT
solvers extended with propagators presents unique challenges, as these
propagators learn constraints dynamically during the search.
  We study this problem using SAT Modulo Symmetries (SMS) as our primary test
case, where a symmetry-breaking propagator reduces the search space by learning
constraints that eliminate isomorphic graphs. Through extensive experimentation
comprising over 10,000 CPU hours, we systematically evaluate different
cube-and-conquer variants on three well-studied combinatorial problems. Our
methodology combines prerun phases to collect learned constraints, various
cubing strategies, and parameter tuning via algorithm configuration and
LLM-generated design suggestions.
  The comprehensive empirical evaluation provides new insights into effective
cubing strategies for propagator-based SAT solving, with our best method
achieving speedups of 2-3x from improved cubing and parameter tuning, providing
an additional 1.5-2x improvement on harder instances.",False,False,"The two papers focus on distinct areas of research, with Paper 1 exploring variance in evaluation benchmarks for large language models and Paper 2 discussing cube-and-conquer methods for graph search in SAT solvers. While both papers involve computational methods, there is no clear overlap or intersection of ideas that could be combined to form a novel multidisciplinary research idea.",
2411.0064,1,applied,2411.0064-pos1-3,"The Llama 3 Herd of Models; Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",2503.11535,"mobilityDCAT-AP: a Metadata Specification for Enhanced Cross-border
  Mobility Data Sharing; Integrated and efficient mobility requires data sharing among the involved
stakeholders. In this direction, regulators and transport authorities have been
defining policies to foster the digitalisation and online publication of
mobility data. However, the creation of several heterogeneous data portals for
mobility data resulted in a fragmented ecosystem that challenges data
accessibility. In this context, metadata is a key enabler to foster the
findability and reusability of relevant datasets, but their interoperability
across different data portals should be ensured. Moreover, each domain presents
specificities on the relevant information that should be encoded through
metadata. To solve these issues within the mobility domain, we present
mobilityDCAT-AP, a reference metadata specification for mobility data portals
specified by putting together domain experts and the Semantic Web community. We
report on the work done to develop the metadata model behind mobilityDCAT-AP
and the best practices followed in its implementation and publication. Finally,
we describe the available educational resources and the activities performed to
ensure broader adoption of mobilityDCAT-AP across mobility data portals. We
present success stories from early adopters and discuss the challenges they
encountered in implementing a metadata specification based on Semantic Web
technologies.",False,True,"The combination of AI-powered language models from Paper 1 and metadata specification for mobility data sharing from Paper 2 can create a novel multidisciplinary research idea, such as developing an AI-driven mobility data sharing platform that utilizes language models to enhance data accessibility and reusability.",
2411.0064,1,applied,2411.0064-pos1-3,"The Llama 3 Herd of Models; Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",2503.13423,"SuperBPE: Space Travel for Language Models; The assumption across nearly all language model (LM) tokenization schemes is
that tokens should be subwords, i.e., contained within word boundaries. While
providing a seemingly reasonable inductive bias, is this common practice
limiting the potential of modern LMs? Whitespace is not a reliable delimiter of
meaning, as evidenced by multi-word expressions (e.g., ""by the way""),
crosslingual variation in the number of words needed to express a concept
(e.g., ""spacesuit helmet"" in German is ""raumanzughelm""), and languages that do
not use whitespace at all (e.g., Chinese). To explore the potential of
tokenization beyond subwords, we introduce a ""superword"" tokenizer, SuperBPE,
which incorporates a simple pretokenization curriculum into the byte-pair
encoding (BPE) algorithm to first learn subwords, then superwords that bridge
whitespace. This brings dramatic improvements in encoding efficiency: when
fixing the vocabulary size to 200k, SuperBPE encodes a fixed piece of text with
up to 33% fewer tokens than BPE on average. In experiments, we pretrain 8B
transformer LMs from scratch while fixing the model size, vocabulary size, and
train compute, varying *only* the algorithm for learning the vocabulary. Our
model trained with SuperBPE achieves an average +4.0% absolute improvement over
the BPE baseline across 30 downstream tasks (including +8.2% on MMLU), while
simultaneously requiring 27% less compute at inference time. In analysis, we
find that SuperBPE results in segmentations of text that are more uniform in
per-token difficulty. Qualitatively, this may be because SuperBPE tokens often
capture common multi-word expressions that function semantically as a single
unit. SuperBPE is a straightforward, local modification to tokenization that
improves both encoding efficiency and downstream performance, yielding better
language models overall.",False,True,The combination of Llama 3's multilingual and multimodal capabilities with SuperBPE's efficient tokenization scheme could lead to a novel research idea that integrates the strengths of both approaches. This interdisciplinary research could result in a more efficient and effective language model that can handle diverse languages and modalities.,
2411.0064,1,applied,2411.0064-pos1-3,"The Llama 3 Herd of Models; Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",2502.10424,"QuantSpec: Self-Speculative Decoding with Hierarchical Quantized KV
  Cache; Large Language Models (LLMs) are increasingly being deployed on edge devices
for long-context settings, creating a growing need for fast and efficient
long-context inference. In these scenarios, the Key-Value (KV) cache is the
primary bottleneck in terms of both GPU memory and latency, as the full KV
cache must be loaded for each decoding step. While speculative decoding is a
widely accepted technique to accelerate autoregressive decoding, existing
methods often struggle to achieve significant speedups due to inefficient KV
cache optimization strategies and result in low acceptance rates. To address
these challenges, we propose a novel self-speculative decoding framework,
QuantSpec, where the draft model shares the architecture of the target model
but employs a hierarchical 4-bit quantized KV cache and 4-bit quantized weights
for acceleration. QuantSpec maintains high acceptance rates ($>$90%) and
reliably provides consistent end-to-end speedups upto $\sim2.5\times$,
outperforming other self-speculative decoding methods that use sparse KV cache
for long-context LLM inference. QuantSpec also reduces the memory requirements
by $\sim 1.3\times$ compared to these alternatives.",False,True,"The combination of Llama 3's multilinguality, coding, reasoning, and tool usage capabilities with QuantSpec's self-speculative decoding framework and hierarchical quantized KV cache can lead to a novel multidisciplinary research idea. This idea can explore the development of efficient and fast long-context inference for multilingual and multimodal tasks, leveraging the strengths of both papers to advance fundamental understanding and solve real-world problems.",
2411.0064,1,applied,2411.0064-pos1-3,"The Llama 3 Herd of Models; Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",2501.15682,"Zoll Manifolds of Type $\mathbb{CP}^n$ with Entire Grauert Tubes; We show that a Zoll manifold of type $\mathbb{CP}^n$ with an entire Grauert
tube is isometric to $\mathbb{CP}^n$ with the canonical Fubini-Study metric, up
to constant multiplication.",False,False,"The two papers come from distinct areas of research, one in artificial intelligence and the other in differential geometry. However, there is no clear connection or overlap between the concepts presented in the two papers, making it difficult to extract a novel multidisciplinary research idea that meets the standards of interdisciplinary research.",
2411.0064,1,applied,2411.0064-pos1-3,"The Llama 3 Herd of Models; Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",2502.0771,"Kaluza-Klein Supergravity 2025; We recall our work in the 1980s taking seriously the maximal eleven
dimensions of supergravity, in particular the round, left squashed and right
squashed $S^7$ compactifications to $D=4$ yielding $\mathcal N=8$, $\mathcal
N=1$ and $\mathcal N=0$, respectively. This involved Kaluza-Kein techniques
that have found wider applications such as spontaneous compactification,
holonomy and supersymmetry, topology versus geometry, squashing and Higgs,
vacuum stability and consistent truncations.",False,False,"The two papers are from distinct areas, one from AI and language models, and the other from theoretical physics and supergravity. While both papers present innovative ideas, there is no clear connection or overlap between the two, making it challenging to extract a novel multidisciplinary research idea that meets the standards.",
2411.0064,1,applied,2411.0064-pos2-3,"Quantifying Variance in Evaluation Benchmarks; Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models.",2503.00496,"Flying on Point Clouds with Reinforcement Learning; A long-cherished vision of drones is to autonomously traverse through clutter
to reach every corner of the world using onboard sensing and computation. In
this paper, we combine onboard 3D lidar sensing and sim-to-real reinforcement
learning (RL) to enable autonomous flight in cluttered environments. Compared
to vision sensors, lidars appear to be more straightforward and accurate for
geometric modeling of surroundings, which is one of the most important cues for
successful obstacle avoidance. On the other hand, sim-to-real RL approach
facilitates the realization of low-latency control, without the hierarchy of
trajectory generation and tracking. We demonstrate that, with design choices of
practical significance, we can effectively combine the advantages of 3D lidar
sensing and RL to control a quadrotor through a low-level control interface at
50Hz. The key to successfully learn the policy in a lightweight way lies in a
specialized surrogate of the lidar's raw point clouds, which simplifies
learning while retaining a fine-grained perception to detect narrow free space
and thin obstacles. Simulation statistics demonstrate the advantages of the
proposed system over alternatives, such as performing easier maneuvers and
higher success rates at different speed constraints. With lightweight
simulation techniques, the policy trained in the simulator can control a
physical quadrotor, where the system can dodge thin obstacles and safely
traverse randomly distributed obstacles.",False,True,"The combination of ideas from both papers can lead to a novel multidisciplinary research idea. For instance, integrating the concept of quantifying variance in evaluation benchmarks from Paper 1 with the reinforcement learning approach for autonomous flight in cluttered environments from Paper 2 could lead to the development of more robust and reliable evaluation metrics for navigation systems in dynamic environments.",
2411.0064,1,applied,2411.0064-pos2-3,"Quantifying Variance in Evaluation Benchmarks; Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models.",2503.12373,"Doubly partially conservative sentences; The purpose of the present paper is to analyze several variants of Solovay's
theorem on the existence of doubly partially conservative sentences. First, we
investigate $\Theta$ sentences that are doubly $(\Gamma, \Lambda)$-conservative
over $T$ for several triples $(\Theta, \Gamma, \Lambda)$. Among other things,
we prove that the existence of a $\Delta_{n+1}(\mathsf{PA})$ sentence that is
doubly $(\Sigma_n, \Sigma_n)$-conservative over $T$ is equivalent to the
$\Sigma_{n+1}$-inconsistency of $T$ over $\mathsf{PA}$. Secondly, we study
$\Theta$ sentences that are hereditarily doubly $(\Gamma,
\Lambda)$-conservative over $T$ for several triples $(\Theta, \Gamma,
\Lambda)$.",False,False,"The two papers come from distinct areas of research, one from natural language processing and the other from mathematical logic. While both papers deal with concepts of evaluation and conservativity, they do not share a common thread that can be woven into a novel multidisciplinary research idea that meets the standards of interdisciplinarity, feasibility, novelty, and usefulness.",
2411.0064,1,applied,2411.0064-pos2-3,"Quantifying Variance in Evaluation Benchmarks; Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models.",2502.05424,"SAMGPT: Text-free Graph Foundation Model for Multi-domain Pre-training
  and Cross-domain Adaptation; Graphs are able to model interconnected entities in many online services,
supporting a wide range of applications on the Web. This raises an important
question: How can we train a graph foundational model on multiple source
domains and adapt to an unseen target domain? A major obstacle is that graphs
from different domains often exhibit divergent characteristics. Some studies
leverage large language models to align multiple domains based on textual
descriptions associated with the graphs, limiting their applicability to
text-attributed graphs. For text-free graphs, a few recent works attempt to
align different feature distributions across domains, while generally
neglecting structural differences. In this work, we propose a novel Structure
Alignment framework for text-free Multi-domain Graph Pre-Training and
cross-domain adaptation (SAMGPT). It is designed to learn multi-domain
knowledge from graphs originating in multiple source domains, which can then be
adapted to address applications in an unseen target domain. Specifically, we
introduce a set of structure tokens to harmonize structure-based aggregation
across source domains during the pre-training phase. Next, for cross-domain
adaptation, we design dual prompts, namely, holistic prompts and specific
prompts, which adapt unified multi-domain structural knowledge and
fine-grained, domain-specific information, respectively, to a target domain.
Finally, we conduct comprehensive experiments on seven public datasets to
evaluate and analyze the effectiveness of SAMGPT.",False,True,"I can extract concepts from both papers to create a novel multidisciplinary research idea. By combining the idea of quantifying variance in evaluation benchmarks from Paper 1 with the concept of structure alignment in multi-domain graph pre-training from Paper 2, a new research project can be formed to investigate the variance in graph-based evaluation benchmarks and develop LM-specific techniques to reduce it, enabling more robust cross-domain adaptation in graph foundational models.",
2411.0064,1,applied,2411.0064-pos2-3,"Quantifying Variance in Evaluation Benchmarks; Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models.",2503.10212,"MouseGPT: A Large-scale Vision-Language Model for Mouse Behavior
  Analysis; Analyzing animal behavior is crucial in advancing neuroscience, yet
quantifying and deciphering its intricate dynamics remains a significant
challenge. Traditional machine vision approaches, despite their ability to
detect spontaneous behaviors, fall short due to limited interpretability and
reliance on manual labeling, which restricts the exploration of the full
behavioral spectrum. Here, we introduce MouseGPT, a Vision-Language Model (VLM)
that integrates visual cues with natural language to revolutionize mouse
behavior analysis. Built upon our first-of-its-kind dataset - incorporating
pose dynamics and open-vocabulary behavioral annotations across over 42 million
frames of diverse psychiatric conditions - MouseGPT provides a novel,
context-rich method for comprehensive behavior interpretation. Our holistic
analysis framework enables detailed behavior profiling, clustering, and novel
behavior discovery, offering deep insights without the need for labor -
intensive manual annotation. Evaluations reveal that MouseGPT surpasses
existing models in precision, adaptability, and descriptive richness,
positioning it as a transformative tool for ethology and for unraveling complex
behavioral dynamics in animal models.",False,True,"I extracted the concept of quantifying variance in evaluation benchmarks from Paper 1 and the idea of using Vision-Language Models for behavior analysis from Paper 2. Combining these concepts could lead to a novel multidisciplinary research idea: developing a framework to quantify and reduce variance in mouse behavior analysis using MouseGPT, enhancing the reliability and interpretability of behavior interpretation.",
2411.0064,1,applied,2411.0064-pos2-3,"Quantifying Variance in Evaluation Benchmarks; Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models.",2501.18879,"Understanding Generalization in Physics Informed Models through Affine
  Variety Dimensions; In recent years, physics-informed machine learning has gained significant
attention for its ability to enhance statistical performance and sample
efficiency by integrating physical structures into machine learning models.
These structures, such as differential equations, conservation laws, and
symmetries, serve as inductive biases that can improve the generalization
capacity of the hybrid model. However, the mechanisms by which these physical
structures enhance generalization capacity are not fully understood, limiting
the ability to guarantee the performance of the models. In this study, we show
that the generalization performance of linear regressors incorporating
differential equation structures is determined by the dimension of the
associated affine variety, rather than the number of parameters. This finding
enables a unified analysis of various equations, including nonlinear ones. We
introduce a method to approximate the dimension of the affine variety and
provide experimental evidence to validate our theoretical insights.",False,True,"The combination of ideas from both papers can lead to a novel multidisciplinary research project. By integrating the concept of quantifying variance in evaluation benchmarks from Paper 1 with the understanding of generalization in physics-informed models from Paper 2, researchers can develop new methods to evaluate and improve the generalization capacity of large language models, particularly in physics-related tasks.",
2411.00609,1,applied,2411.00609-pos1-4,"Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype
  Identification Using a Novel AUROC Loss Function for Convolutional Neural
  Networks; Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016).",2411.00609-pos2-4,"Pediatric low-grade glioma: State-of-the-art and ongoing challenges; Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%–40% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence.",True,True,"I extracted concepts from both papers to create a novel multidisciplinary research idea: Developing a CNN-based personalized treatment planning system for pediatric low-grade glioma patients, integrating MRI scans, molecular subtype identification, and targeted inhibitor treatment optimization, to improve functional outcomes and quality-of-life measures.",
2411.00609,1,applied,2411.00609-pos1-4,"Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype
  Identification Using a Novel AUROC Loss Function for Convolutional Neural
  Networks; Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016).",2503.13981,"Neural Constraints on Cognitive Experience and Mental Health; Understanding how neural dynamics shape cognitive experiences remains a
central challenge in neuroscience and psychiatry. Here, we present a novel
framework leveraging state-to-output controllability from dynamical systems
theory to model the interplay between cognitive perturbations, neural activity,
and subjective experience. We demonstrate that large-scale fMRI signals are
constrained to low-dimensional manifolds, where affective and cognitive states
are naturally organized. Furthermore, we provide a theoretically robust method
to estimate the controllability Gramian from steady-state neural responses,
offering a direct measure of the energy required to steer cognitive outcomes.
In five healthy participants viewing 2,185 emotionally evocative short videos,
our analyses reveal a strong alignment between neural activations and affective
ratings, with an average correlation of $r \approx 0.7$. In a clinical cohort
of 255 patients with major depressive disorder, biweekly Hamilton Rating Scale
trajectories over 11 weeks significantly mapped onto these manifolds,
explaining approximately 20% more variance than chance ($p < 10^{-10}$,
numerically better than chance in 93% reaching statistical significance in
one-third of subjects). Our work bridges dynamical systems theory and clinical
neuroscience, providing a principled approach to optimize mental health
treatments by targeting the most efficient neural pathways for cognitive
change.",False,True,I extracted the concept of using neural networks for medical diagnosis from Paper 1 and the idea of modeling cognitive experiences through neural dynamics from Paper 2. A novel multidisciplinary research idea could be to develop a neural network-based framework to diagnose and predict treatment outcomes for pediatric patients with Low-Grade Neuroepithelial Tumors by analyzing their cognitive experiences and neural dynamics.,
2411.00609,1,applied,2411.00609-pos1-4,"Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype
  Identification Using a Novel AUROC Loss Function for Convolutional Neural
  Networks; Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016).",2502.16557,"Predictability of temporal network dynamics in normal ageing and brain
  pathology; Spontaneous brain activity generically displays transient spatiotemporal
coherent structures, which can selectively be affected in various neurological
and psychiatric pathologies. Here we model the full brain's
electroencephalographic activity as a high-dimensional functional network
performing a trajectory in a latent graph phase space. This approach allows us
to investigate the orbital stability of brain's activity and in particular its
short-term predictability. We do this by constructing a non-parametric
statistic quantifying the expansion of initially close functional network
trajectories. We apply the method to cohorts of healthy ageing individuals, and
patients previously diagnosed with Parkinson's or Alzheimer's disease. Results
not only characterise brain dynamics from a new angle, but further show that
functional network predictability varies in a marked scale-dependent way across
healthy controls and patient groups. The path towards both pathologies is
markedly different. Furthermore, healthy ageing's predictability appears to
strongly differ from that of Parkinson's disease, but much less from that of
patients with Alzheimer's disease.",False,True,"Both papers focus on brain-related research, one on tumor identification and the other on brain activity dynamics. Combining ideas from both, a novel research project could explore using the AUROC loss function for CNNs to predict brain tumor growth patterns based on temporal network dynamics, offering a new diagnostic approach.",
2411.00609,1,applied,2411.00609-pos1-4,"Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype
  Identification Using a Novel AUROC Loss Function for Convolutional Neural
  Networks; Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016).",2501.13628,"Language modulates vision: Evidence from neural networks and human
  brain-lesion models; Comparing information structures in between deep neural networks (DNNs) and
the human brain has become a key method for exploring their similarities and
differences. Recent research has shown better alignment of vision-language DNN
models, such as CLIP, with the activity of the human ventral occipitotemporal
cortex (VOTC) than earlier vision models, supporting the idea that language
modulates human visual perception. However, interpreting the results from such
comparisons is inherently limited due to the ""black box"" nature of DNNs. To
address this, we combined model-brain fitness analyses with human brain lesion
data to examine how disrupting the communication pathway between the visual and
language systems causally affects the ability of vision-language DNNs to
explain the activity of the VOTC. Across four diverse datasets, CLIP
consistently outperformed both label-supervised (ResNet) and unsupervised
(MoCo) models in predicting VOTC activity. This advantage was left-lateralized,
aligning with the human language network. Analyses of the data of 33 stroke
patients revealed that reduced white matter integrity between the VOTC and the
language region in the left angular gyrus was correlated with decreased CLIP
performance and increased MoCo performance, indicating a dynamic influence of
language processing on the activity of the VOTC. These findings support the
integration of language modulation in neurocognitive models of human vision,
reinforcing concepts from vision-language DNN models. The sensitivity of
model-brain similarity to specific brain lesions demonstrates that leveraging
manipulation of the human brain is a promising framework for evaluating and
developing brain-like computer models.",False,True,"The combination of ideas from both papers can lead to a novel multidisciplinary research idea, such as developing a vision-language neural network model for non-invasive diagnosis of pediatric brain tumors. This idea integrates concepts from computer science, neuroscience, and medicine, and has the potential to improve diagnostic accuracy and treatment planning for pediatric brain tumors.",
2411.00609,1,applied,2411.00609-pos1-4,"Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype
  Identification Using a Novel AUROC Loss Function for Convolutional Neural
  Networks; Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016).",2502.12092,"Using economic value signals from primate prefrontal cortex in
  neuro-engineering applications; Neural signals related to movement can be measured from intracranial
recordings and used in brain-machine interface devices (BMI) to restore
physical function in impaired patients. In this study, we explore the use of
more abstract neural signals related to economic value in a BMI context. Using
data collected from the orbitofrontal cortex in non-human primates, we develop
deep learning-based neural decoders that can predict the monkey's choice in a
value-based decision-making task. Out-of-sample performance was improved by
augmenting the training set with synthesized data, showing the feasibility of
using limited training data. We further demonstrate that we can predict the
monkey's choice sooner using a neural forecasting module that is equipped with
task-related information. These findings support the feasibility of user
preference-informed neuroengineering devices that leverage abstract cognitive
signals.",False,True,The combination of ideas from both papers can lead to a novel multidisciplinary research project that integrates CNNs for non-invasive diagnostic imaging with neural signals related to economic value in BMI devices. This fusion can enable personalized treatment planning for pediatric cancer patients by incorporating patient preferences and values into decision-making processes.,
2411.00609,1,applied,2411.00609-pos1-4,"Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype
  Identification Using a Novel AUROC Loss Function for Convolutional Neural
  Networks; Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016).",2503.05806,"I Think, Therefore I Hallucinate: Minds, Machines, and the Art of Being
  Wrong; This theoretical work examines 'hallucinations' in both human cognition and
large language models, comparing how each system can produce perceptions or
outputs that deviate from reality. Drawing on neuroscience and machine learning
research, we highlight the predictive processes that underlie human and
artificial thought. In humans, complex neural mechanisms interpret sensory
information under uncertainty, sometimes filling in gaps and creating false
perceptions. This inference occurs hierarchically: higher cortical levels send
top-down predictions to lower-level regions, while mismatches (prediction
errors) propagate upward to refine the model. LLMs, in contrast, rely on
auto-regressive modeling of text and can generate erroneous statements in the
absence of robust grounding. Despite these different foundations - biological
versus computational - the similarities in their predictive architectures help
explain why hallucinations occur. We propose that the propensity to generate
incorrect or confabulated responses may be an inherent feature of advanced
intelligence. In both humans and AI, adaptive predictive processes aim to make
sense of incomplete information and anticipate future states, fostering
creativity and flexibility, but also introducing the risk of errors. Our
analysis illuminates how factors such as feedback, grounding, and error
correction affect the likelihood of 'being wrong' in each system. We suggest
that mitigating AI hallucinations (e.g., through improved training,
post-processing, or knowledge-grounding methods) may also shed light on human
cognitive processes, revealing how error-prone predictions can be harnessed for
innovation without compromising reliability. By exploring these converging and
divergent mechanisms, the paper underscores the broader implications for
advancing both AI reliability and scientific understanding of human thought.",False,True,"I can extract concepts from both papers to create a novel multidisciplinary research idea. By combining the AUROC loss function for CNNs from Paper 1 with the concept of hallucinations in AI from Paper 2, a new research project could investigate the use of AI-driven diagnostic tools for medical imaging, such as detecting PLGNT subtypes, while also exploring the potential for AI hallucinations in these systems and developing methods to mitigate them.",
2411.00609,1,applied,2411.00609-pos2-4,"Pediatric low-grade glioma: State-of-the-art and ongoing challenges; Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%–40% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence.",2502.06501,"Learning Clustering-based Prototypes for Compositional Zero-shot
  Learning; Learning primitive (i.e., attribute and object) concepts from seen
compositions is the primary challenge of Compositional Zero-Shot Learning
(CZSL). Existing CZSL solutions typically rely on oversimplified data
assumptions, e.g., modeling each primitive with a single centroid primitive
representation, ignoring the natural diversities of the attribute (resp.
object) when coupled with different objects (resp. attribute). In this work, we
develop ClusPro, a robust clustering-based prototype mining framework for CZSL
that defines the conceptual boundaries of primitives through a set of
diversified prototypes. Specifically, ClusPro conducts within-primitive
clustering on the embedding space for automatically discovering and dynamically
updating prototypes. These representative prototypes are subsequently used to
repaint a well-structured and independent primitive embedding space, ensuring
intra-primitive separation and inter-primitive decorrelation through
prototype-based contrastive learning and decorrelation learning. Moreover,
ClusPro efficiently performs prototype clustering in a non-parametric fashion
without the introduction of additional learnable parameters or computational
budget during testing. Experiments on three benchmarks demonstrate ClusPro
outperforms various top-leading CZSL solutions under both closed-world and
open-world settings.",False,False,"Although both papers introduce innovative ideas, they are from distinct areas with no direct connections. The first paper discusses pediatric low-grade glioma, a medical topic, while the second paper focuses on compositional zero-shot learning, a machine learning concept. There is no clear intersection or potential for integration.",
2411.00609,1,applied,2411.00609-pos2-4,"Pediatric low-grade glioma: State-of-the-art and ongoing challenges; Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%–40% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence.",2502.19797,"MFSR: Multi-fractal Feature for Super-resolution Reconstruction with
  Fine Details Recovery; In the process of performing image super-resolution processing, the
processing of complex localized information can have a significant impact on
the quality of the image generated. Fractal features can capture the rich
details of both micro and macro texture structures in an image. Therefore, we
propose a diffusion model-based super-resolution method incorporating fractal
features of low-resolution images, named MFSR. MFSR leverages these fractal
features as reinforcement conditions in the denoising process of the diffusion
model to ensure accurate recovery of texture information. MFSR employs
convolution as a soft assignment to approximate the fractal features of
low-resolution images. This approach is also used to approximate the density
feature maps of these images. By using soft assignment, the spatial layout of
the image is described hierarchically, encoding the self-similarity properties
of the image at different scales. Different processing methods are applied to
various types of features to enrich the information acquired by the model. In
addition, a sub-denoiser is integrated in the denoising U-Net to reduce the
noise in the feature maps during the up-sampling process in order to improve
the quality of the generated images. Experiments conducted on various face and
natural image datasets demonstrate that MFSR can generate higher quality
images.",False,True,"I can extract concepts from both papers to create a novel multidisciplinary research idea. By combining the medical knowledge of pediatric low-grade glioma from Paper 1 with the image super-resolution technique from Paper 2, a new research idea can be formed to develop an improved imaging diagnosis system for pLGG, utilizing the MFSR method to enhance the quality of MRI images, which can lead to better treatment outcomes and quality-of-life measures for patients.",
2411.00609,1,applied,2411.00609-pos2-4,"Pediatric low-grade glioma: State-of-the-art and ongoing challenges; Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%–40% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence.",2503.07418,"AR-Diffusion: Asynchronous Video Generation with Auto-Regressive
  Diffusion; The task of video generation requires synthesizing visually realistic and
temporally coherent video frames. Existing methods primarily use asynchronous
auto-regressive models or synchronous diffusion models to address this
challenge. However, asynchronous auto-regressive models often suffer from
inconsistencies between training and inference, leading to issues such as error
accumulation, while synchronous diffusion models are limited by their reliance
on rigid sequence length. To address these issues, we introduce Auto-Regressive
Diffusion (AR-Diffusion), a novel model that combines the strengths of
auto-regressive and diffusion models for flexible, asynchronous video
generation. Specifically, our approach leverages diffusion to gradually corrupt
video frames in both training and inference, reducing the discrepancy between
these phases. Inspired by auto-regressive generation, we incorporate a
non-decreasing constraint on the corruption timesteps of individual frames,
ensuring that earlier frames remain clearer than subsequent ones. This setup,
together with temporal causal attention, enables flexible generation of videos
with varying lengths while preserving temporal coherence. In addition, we
design two specialized timestep schedulers: the FoPP scheduler for balanced
timestep sampling during training, and the AD scheduler for flexible timestep
differences during inference, supporting both synchronous and asynchronous
generation. Extensive experiments demonstrate the superiority of our proposed
method, which achieves competitive and state-of-the-art results across four
challenging benchmarks.",False,False,"The two papers are from distinct fields of pediatric low-grade glioma and asynchronous video generation, with no apparent connection between them. There is no feasible way to combine concepts from these two papers to create a novel multidisciplinary research idea that meets the standards of being interdisciplinary, feasible, novel, and useful.",
2411.00609,1,applied,2411.00609-pos2-4,"Pediatric low-grade glioma: State-of-the-art and ongoing challenges; Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%–40% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence.",2503.15005,"Universal Scene Graph Generation; Scene graph (SG) representations can neatly and efficiently describe scene
semantics, which has driven sustained intensive research in SG generation. In
the real world, multiple modalities often coexist, with different types, such
as images, text, video, and 3D data, expressing distinct characteristics.
Unfortunately, current SG research is largely confined to single-modality scene
modeling, preventing the full utilization of the complementary strengths of
different modality SG representations in depicting holistic scene semantics. To
this end, we introduce Universal SG (USG), a novel representation capable of
fully characterizing comprehensive semantic scenes from any given combination
of modality inputs, encompassing modality-invariant and modality-specific
scenes. Further, we tailor a niche-targeting USG parser, USG-Par, which
effectively addresses two key bottlenecks of cross-modal object alignment and
out-of-domain challenges. We design the USG-Par with modular architecture for
end-to-end USG generation, in which we devise an object associator to relieve
the modality gap for cross-modal object alignment. Further, we propose a
text-centric scene contrasting learning mechanism to mitigate domain imbalances
by aligning multimodal objects and relations with textual SGs. Through
extensive experiments, we demonstrate that USG offers a stronger capability for
expressing scene semantics than standalone SGs, and also that our USG-Par
achieves higher efficacy and performance.",False,True,"Combining ideas from both papers, a novel multidisciplinary research project can be created, such as developing a universal scene graph generation framework to analyze and visualize medical imaging data of pediatric low-grade glioma, enhancing diagnosis and treatment planning by integrating multimodal data and semantic scene understanding.",
2411.00609,1,applied,2411.00609-pos2-4,"Pediatric low-grade glioma: State-of-the-art and ongoing challenges; Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%–40% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence.",2502.10674,"Occlusion-aware Text-Image-Point Cloud Pretraining for Open-World 3D
  Object Recognition; Recent open-world representation learning approaches have leveraged CLIP to
enable zero-shot 3D object recognition. However, performance on real point
clouds with occlusions still falls short due to unrealistic pretraining
settings. Additionally, these methods incur high inference costs because they
rely on Transformer's attention modules. In this paper, we make two
contributions to address these limitations. First, we propose occlusion-aware
text-image-point cloud pretraining to reduce the training-testing domain gap.
From 52K synthetic 3D objects, our framework generates nearly 630K partial
point clouds for pretraining, consistently improving real-world recognition
performances of existing popular 3D networks. Second, to reduce computational
requirements, we introduce DuoMamba, a two-stream linear state space model
tailored for point clouds. By integrating two space-filling curves with 1D
convolutions, DuoMamba effectively models spatial dependencies between point
tokens, offering a powerful alternative to Transformer. When pretrained with
our framework, DuoMamba surpasses current state-of-the-art methods while
reducing latency and FLOPs, highlighting the potential of our approach for
real-world applications. Our code and data are available at
https://ndkhanh360.github.io/project-occtip.",False,False,"The two papers are from distinct disciplines (pediatric medicine and computer vision) with little overlap, making it challenging to extract concepts that can be combined into a novel multidisciplinary research idea. The ideas and methods presented in the papers do not lend themselves to a straightforward integration.",
2411.00609,1,applied,2411.00609-pos1-4,"Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype
  Identification Using a Novel AUROC Loss Function for Convolutional Neural
  Networks; Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016).",2503.01992,"Cuscuton Bounce Beyond the Linear Regime: Bispectrum and Strong Coupling
  Constraints; Cuscuton Gravity is characterized as a scalar field that can be added to
general relativity without introducing any new dynamical degrees of freedom on
a cosmological background. Yet, it modifies gravity such that spacetime
singularities can be avoided. This has led to the Cuscuton bounce, a
nonsingular cosmology that has been shown to be linearly stable, which is a
rare feat. Upon introducing mechanisms known to generate a near-scale-invariant
power spectrum of isocurvature perturbations in the prebounce contracting
phase, we perform an extensive linear analysis of all scalar perturbations as
they evolve through the Cuscuton bounce, both analytically and numerically.
Then, after deriving the third-order perturbed action for our theory, we
compare the magnitude of its terms (on shell) to those in the second-order
action. We show that perturbativity is maintained in the infrared throughout
the evolution, including through the bounce. In the ultraviolet, we show that a
hierarchy of scales is maintained, with the strong coupling scale well above
the relevant background energy scale at all times. We reconfirm these results
by computing the three-point functions in various limits and demonstrate that
the models do not have any strong coupling problems and furthermore that there
is negligible non-Gaussianities on observable scales. Consequently, the primary
potential source of observable non-Gaussianities may only arise from the
conversion of isocurvature perturbations to curvature perturbations. The whole
scenario is thus a robust, stable, weakly coupled nonsingular cosmological
model, consistent with observations.",False,False,"The two papers come from distinct areas of research, namely medical imaging and cosmology, which are difficult to integrate into a single interdisciplinary research idea. The concepts and methods presented in each paper are highly specialized and do not lend themselves to straightforward combination or application to a new problem.",
2411.00609,1,applied,2411.00609-pos1-4,"Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype
  Identification Using a Novel AUROC Loss Function for Convolutional Neural
  Networks; Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016).",2501.15352,"Tensor renormalization group study of the two-dimensional lattice U(1)
  gauge-Higgs model with a topological $\theta$ term under L\""uscher's
  admissibility condition; We investigate the two-dimensional lattice U(1) gauge-Higgs model with a
topological term, employing L\""uscher's admissibility condition. The standard
Monte Carlo simulation for this model is hindered not only by the complex
action problem due to the topological term but also by the topological freezing
problem originating from the admissibility condition. Resolving both obstacles
simultaneously with the tensor renormalization group approach, we show the
advantage of the admissibility condition in dealing with the topological term
discretized with the so-called field-theoretical definition.",False,False,"The two papers are from distinct areas of research, one in medical imaging and the other in theoretical physics. While both papers employ advanced computational methods, there is no clear connection or overlap between the ideas presented, making it challenging to extract a novel multidisciplinary research idea that meets the standards.",
2411.00609,1,applied,2411.00609-pos1-4,"Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype
  Identification Using a Novel AUROC Loss Function for Convolutional Neural
  Networks; Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016).",2502.00311,"Sparse Gradient Compression for Fine-Tuning Large Language Models; Fine-tuning large language models (LLMs) for downstream tasks has become
increasingly crucial due to their widespread use and the growing availability
of open-source models. However, the high memory costs associated with
fine-tuning remain a significant challenge, especially as models increase in
size. To address this, parameter efficient fine-tuning (PEFT) methods have been
proposed to minimize the number of parameters required for fine-tuning LLMs.
However, these approaches often tie the number of optimizer states to
dimensions of model parameters, limiting flexibility and control during
fine-tuning. In this paper, we propose sparse gradient compression (SGC), a
training regime designed to address these limitations. Our approach leverages
inherent sparsity in gradients to compress optimizer states by projecting them
onto a low-dimensonal subspace, with dimensionality independent of the original
model's parameters. By enabling optimizer state updates in an arbitrary
low-dimensional subspace, SGC offers a flexible tradeoff between memory
efficiency and performance. We demonstrate through experiments that SGC can
decrease memory usage in optimizer states more effectively than existing PEFT
methods. Furthermore, by fine-tuning LLMs on various downstream tasks, we show
that SGC can deliver superior performance while substantially lowering
optimizer state memory requirements, particularly in both data-limited and
memory-limited settings.",False,False,"The two papers are from distinct areas, one in medical imaging and the other in natural language processing. While both papers propose novel methods for improving performance, there is no clear intersection or potential for combining ideas to create a novel multidisciplinary research project that meets the standards of interdisciplinary research.",
2411.00609,1,applied,2411.00609-pos1-4,"Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype
  Identification Using a Novel AUROC Loss Function for Convolutional Neural
  Networks; Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016).",2503.10715,"Economic Impact of China's Retaliatory Soybean Tariff on U.S. Soybean
  Farmers; This paper analyzes the economic impact of China's retaliatory soybean tariff
on U.S. soybean farmers using advanced econometric methods and comprehensive
datasets including USDA reports, trade data, and historical price movements.
The analysis employs a Structural Vector Autoregression (SVAR), a
Difference-in-Differences (DiD) estimation, and a Dynamic Stochastic General
Equilibrium (DSGE) model, revealing the impacts of China's retaliatory tariff
on soybean prices, exports, farm incomes, and acreage decisions. U.S. policy
responses, including direct subsidies and market diversification strategies,
are also evaluated.",False,False,"The two papers come from distinct areas of research, one from medical imaging and the other from economics. While both papers introduce innovative methods, there is no clear connection or overlap between the ideas presented, making it challenging to extract a novel multidisciplinary research concept that meets the standards.",
2411.00609,1,applied,2411.00609-pos1-4,"Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype
  Identification Using a Novel AUROC Loss Function for Convolutional Neural
  Networks; Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016).",2502.02362,"Premise-Augmented Reasoning Chains Improve Error Identification in Math
  reasoning with LLMs; Chain-of-Thought (CoT) prompting enhances mathematical reasoning in large
language models (LLMs) by enabling detailed step-by-step solutions. However,
due to the verbosity of LLMs, the resulting reasoning chains can be long,
making it harder to verify the reasoning steps and trace issues resulting from
dependencies between the steps that may be farther away in the sequence of
steps. Importantly, mathematical reasoning allows each step to be derived from
a small set of premises, which are a subset of the preceding steps in the
reasoning chain. In this paper, we present a framework that identifies the
premises for each step, to improve the evaluation of reasoning. We restructure
conventional linear reasoning chains into Premise Augmented Reasoning Chains
(PARC) by introducing premise links, resulting in a directed acyclic graph
where the nodes are the steps and the edges are the premise links. Through
experiments with a PARC-based dataset that we built, namely PERL (Premises and
ERrors identification in LLMs), we demonstrate that LLMs can reliably identify
premises within complex reasoning chains. In particular, even open-source LLMs
achieve 90% recall in premise identification. We also show that PARC helps to
identify errors in reasoning chains more reliably. The accuracy of error
identification improves by 6% to 16% absolute when step-by-step verification is
carried out in PARC under the premises. Our findings highlight the utility of
premise-centric representations in addressing complex problem-solving tasks and
open new avenues for improving the reliability of LLM-based reasoning
evaluations.",False,False,"Although both papers introduce innovative ideas, combining them does not yield a novel multidisciplinary research idea that meets the standards. The first paper focuses on improving CNNs for medical diagnosis, while the second paper enhances mathematical reasoning in LLMs. There is no clear connection or overlap between the two research areas, making it challenging to extract a feasible and useful interdisciplinary research idea.",
2411.00609,1,applied,2411.00609-pos2-4,"Pediatric low-grade glioma: State-of-the-art and ongoing challenges; Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%–40% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence.",2503.01011,"AlphaPIG: The Nicest Way to Prolong Interactive Gestures in Extended
  Reality; Mid-air gestures serve as a common interaction modality across Extended
Reality (XR) applications, enhancing engagement and ownership through intuitive
body movements. However, prolonged arm movements induce shoulder fatigue, known
as ""Gorilla Arm Syndrome"", degrading user experience and reducing interaction
duration. Although existing ergonomic techniques derived from Fitts' law (such
as reducing target distance, increasing target width, and modifying
control-display gain) provide some fatigue mitigation, their implementation in
XR applications remains challenging due to the complex balance between user
engagement and physical exertion. We present AlphaPIG, a meta-technique
designed to Prolong Interactive Gestures by leveraging real-time fatigue
predictions. AlphaPIG assists designers in extending and improving XR
interactions by enabling automated fatigue-based interventions. Through
adjustment of intervention timing and intensity decay rate, designers can
explore and control the trade-off between fatigue reduction and potential
effects such as decreased body ownership. We validated AlphaPIG's effectiveness
through a study (N=22) implementing the widely-used Go-Go technique. Results
demonstrated that AlphaPIG significantly reduces shoulder fatigue compared to
non-adaptive Go-Go, while maintaining comparable perceived body ownership and
agency. Based on these findings, we discuss positive and negative perceptions
of the intervention. By integrating real-time fatigue prediction with adaptive
intervention mechanisms, AlphaPIG constitutes a critical first step towards
creating fatigue-aware applications in XR.",False,False,"The two papers come from distinct areas of pediatric medicine and human-computer interaction in extended reality, with no apparent overlap or connection between the topics. There is no clear way to combine concepts from these papers to create a novel multidisciplinary research idea that meets the standards of interdisciplinary research.",
2411.00609,1,applied,2411.00609-pos2-4,"Pediatric low-grade glioma: State-of-the-art and ongoing challenges; Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%–40% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence.",2501.17139,"New Method for Robust Critical Analysis of Magnetic Systems; Here, we present new methods for critical analysis to determine the range of
exchange interaction(s) and appropriate values of critical exponents for
different magnetic systems. From computational and experimental investigations
of magnetic behavior of Ni and Gd, we show that the critical behavior remains
the same on either side of transition temperature. Using our proposed method of
analysis for Gd, we find a critical role of competing interactions where the
local electron moments follow 3D Ising type short-range interactions whereas
the itinerant electron moments constitute mean-field type long-range RKKY
interaction.",False,False,"Although both papers introduce innovative ideas, they are too distinct and lack a clear connection to be combined into a novel multidisciplinary research idea. The first paper focuses on pediatric low-grade glioma, a medical topic, while the second paper discusses magnetic systems, a physics topic, making it challenging to integrate them into a cohesive research project.",
2411.00609,1,applied,2411.00609-pos2-4,"Pediatric low-grade glioma: State-of-the-art and ongoing challenges; Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%–40% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence.",2503.04367,"Concurrent Multifractality and Anomalous Hall Response in the Nodal Line
  Semimetal Fe$_3$GeTe$_2$ Near Localization; Topological states of matter exhibit unique protection against scattering by
disorder. Different topological classes exhibit distinct forms and degrees of
protection. Here, we investigate the response of the ferromagnetic nodal line
semimetal Fe$_3$GeTe$_2$ to disorder and electronic interactions. By combining
global magneto-transport with atomic-scale scanning tunneling spectroscopy we
find a simultaneous onset of diverse phenomena below a common temperature scale
of about 15 K: A crossover from metallic to insulating temperature dependence
of the longitudinal resistivity, saturation of the anomalous Hall conductivity
to its maximal value, formation of a sharp zero-bias dip in the tunneling
density of state, and emergence of multi-fractal structure of the electronic
wavefunction peaking at the Fermi energy. These concurrent observations reflect
the emergence of a novel energy scale possibly related to the opening of a gap
in the nodal line band of Fe$_3$GeTe$_2$. Our study provides overarching
insight into the role of disorder, electronic interactions and Berry curvature
in setting the micro- and macro-scale responses of topological semimetals.",False,False,"The two papers are from distinct disciplines, pediatric oncology and condensed matter physics, with no apparent overlap or connection. The concepts and ideas presented in the papers do not lend themselves to integration or combination, making it difficult to extract a novel multidisciplinary research idea.",
2411.00609,1,applied,2411.00609-pos2-4,"Pediatric low-grade glioma: State-of-the-art and ongoing challenges; Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%–40% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence.",2501.06951,"Rigidity Results Involving Stabilized Scalar Curvature; We establish a rigidity theorem for Brendle and Hung's recent systolic
inequality, which involves Gromov's notion of \(T^{\rtimes}\)-stabilized scalar
curvature. Our primary technique is the construction of foliations by free
boundary weighted constant mean curvature hypersurfaces, enabling us to
generalize several classical scalar curvature rigidity results to the
\(T^{\rtimes}\)-stabilized setting. Additionally, we develop a monotone
quantity using Ricci flow coupled with a heat equation, which is essential for
rigidity analysis.",False,False,"The two papers come from distinct areas, one from pediatric oncology and the other from differential geometry. While both papers present innovative ideas, there is no apparent connection or overlap between the two disciplines, making it challenging to extract a novel multidisciplinary research idea that meets the standards of interdisciplinarity, feasibility, novelty, and usefulness.",
2411.00609,1,applied,2411.00609-pos2-4,"Pediatric low-grade glioma: State-of-the-art and ongoing challenges; Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%–40% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence.",2503.11248,"Reasoning-Grounded Natural Language Explanations for Language Models; We propose a large language model explainability technique for obtaining
faithful natural language explanations by grounding the explanations in a
reasoning process. When converted to a sequence of tokens, the outputs of the
reasoning process can become part of the model context and later be decoded to
natural language as the model produces either the final answer or the
explanation. To improve the faithfulness of the explanations, we propose to use
a joint predict-explain approach, in which the answers and explanations are
inferred directly from the reasoning sequence, without the explanations being
dependent on the answers and vice versa. We demonstrate the plausibility of the
proposed technique by achieving a high alignment between answers and
explanations in several problem domains, observing that language models often
simply copy the partial decisions from the reasoning sequence into the final
answers or explanations. Furthermore, we show that the proposed use of
reasoning can also improve the quality of the answers.",False,False,"Although both papers present innovative ideas in their respective fields, there is no clear connection between pediatric low-grade glioma and natural language explanations for language models. The concepts and methods presented in the two papers do not complement or intersect with each other in a way that would enable the creation of a novel multidisciplinary research idea.",
2411.00726,1,applied,2411.00726-pos1-5,"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",2411.00726-pos2-5,"Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema; This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection",True,True,"The concept of transformer architecture from Paper 1 can be combined with the medical imaging analysis from Paper 2 to create a novel research idea. For instance, applying transformer-based image recognition to analyze retinal vasculature and thickness in macular edema patients, potentially improving disease diagnosis and detection.",
2411.00726,1,applied,2411.00726-pos1-5,"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",2501.18661,"Geometric immunosuppression in CAR-T cell treatment: Insights from
  mathematical modeling; Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising
treatment for hematological malignancies, offering a targeted approach to
cancer treatment. Understanding the complexities of CAR-T cell therapy within
solid tumors poses challenges due to the intricate interactions within the
tumor microenvironment. Mathematical modeling may serve as a valuable tool to
unravel the dynamics of CAR-T cell therapy and improve its effectiveness in
solid tumors. This study aimed to investigate the impact of spatial aspects in
CAR-T therapy of solid tumors, utilizing cellular automata for modeling
purposes. Our main objective was to deepen our understanding of treatment
effects by analyzing scenarios with different spatial distributions and varying
the initial quantities of tumor and CAR-T cells. Tumor geometry significantly
influenced treatment efficacy in-silico, with notable differences observed
between tumors with block-like arrangements and those with sparse cell
distributions, leading to the concept of immune suppression due to geometrical
effects. This research delves into the intricate relationship between spatial
dynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting
the relevance of tumor geometry in the outcome of cellular immunotherapy
treatments. Our results provide a basis for improving the efficacy of CAR-T
cell treatments by combining them with other ones reducing the density of
compact tumor areas and thus opening access ways for tumor killing T-cells.",False,True,"The combination of image recognition using Transformers (Paper 1) and mathematical modeling of CAR-T cell therapy (Paper 2) can lead to a novel multidisciplinary research idea, such as using image analysis to predict tumor geometry and optimize CAR-T cell treatment efficacy. This idea integrates concepts from computer vision and immunotherapy, meets the standards of interdisciplinary research, and has the potential to be feasible, novel, and useful.",
2411.00726,1,applied,2411.00726-pos1-5,"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",2502.03661,"Bridging high resolution sub-cellular imaging with physiologically
  relevant engineered tissues; While high-resolution microscopic techniques are crucial for studying
cellular structures in cell biology, obtaining such images from thick 3D
engineered tissues remains challenging. In this review, we explore advancements
in fluorescence microscopy, alongside the use of various fluorescent probes and
material processing techniques to address these challenges. We navigate through
the diverse array of imaging options available in tissue engineering field,
from wide field to super-resolution microscopy, so researchers can make more
informed decisions based on the specific tissue and cellular structures of
interest. Finally, we provide some recent examples of how traditional
limitations on obtaining high-resolution images on sub-cellular architecture
within 3D tissues have been overcome by combining imaging advancements with
innovative tissue engineering approaches.",False,True,"The combination of transformer-based image recognition from Paper 1 and high-resolution imaging techniques for 3D engineered tissues from Paper 2 can lead to a novel multidisciplinary research idea. For example, using transformers to analyze high-resolution images of 3D tissues can help identify patterns and structures that are not apparent through traditional imaging techniques, advancing our understanding of tissue engineering and cell biology.",
2411.00726,1,applied,2411.00726-pos1-5,"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",2503.12234,"Tumor microenvironment (Part I): Tissue integrity in a rat model of
  peripheral neural cancer; ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are
thought to be a factor in the integrity of nerve tissues. In this report, we
attempted to trace the expression of ICAM-1, responsible for cell-to-cell
adhesion, and of MPZ, the main constituent of myelin sheath, in malignant
tissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells
(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of
the SNs were collected after 7, 14 and 21 days and compared to a sham-operated
group of rats (n = 6 each). Tissues were sectioned and histologically examined,
under light microscope, and stained for measuring the immunoreactivity of
ICAM-1 and MPZ under laser scanning microscope. The cancer model was
established, and the tumor growth was confirmed. ICAM-1 showed severe
decreases, proportional to the growing anaplastic cells, as compared to the
sham group. MPZ revealed, however, a distinct defensive pattern before
substantially decreasing in a comparison with sham. These results support the
notion that malignancies damage peripheral nerves and cause severe axonal
injury and loss of neuronal integrity, and clearly define the role of ICAM-1
and MPZ in safeguarding the nerve tissues.",False,True,"I can extract concepts from both papers to create a novel multidisciplinary research idea. For instance, combining the transformer architecture from Paper 1 with the tumor microenvironment analysis from Paper 2 could lead to a new approach for analyzing medical images of tumors, potentially improving diagnosis and treatment.",
2411.00726,1,applied,2411.00726-pos1-5,"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",2503.00197,"Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental
  differences between male and female heart shapes; Sex-based differences in cardiovascular disease are well documented, yet the
precise nature and extent of these discrepancies in cardiac anatomy remain
incompletely understood. Traditional scaling models often fail to capture the
interplay of age, blood pressure, and body size, prompting a more nuanced
investigation. Here, we employ statistical shape modeling in a healthy subset
(n=456) of the UK Biobank to explore sex-specific variations in biventricular
anatomy. We reconstruct 3D meshes and perform multivariate analyses of shape
coefficients, controlling for age, blood pressure, and various body size
metrics. Our findings reveal that sex alone explains at least 25 percent of
morphological variability, with strong discrimination between men and women
(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the
most discriminative modes highlight pronounced differences in cardiac chamber
volumes, the anterior-posterior width of the right ventricle, and the relative
positioning of the cardiac chambers. These results underscore that sex has a
fundamental influence on cardiac morphology, which may have important clinical
implications for differing cardiac structural assessments in men and women.
Future work should investigate how these anatomical differences manifest in
various cardiovascular conditions, ultimately paving the way for more precise
risk stratification and personalized therapeutic strategies for both men and
women.",False,True,"I can extract concepts from both papers to create a novel multidisciplinary research idea. By combining the transformer architecture from Paper 1 with the statistical shape modeling from Paper 2, a new approach to analyzing medical images, such as cardiac anatomy, can be developed, enabling more accurate and personalized diagnoses.",
2411.00726,1,applied,2411.00726-pos1-5,"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",2503.13516,"A tumor-immune model of chronic myeloid leukemia with optimal
  immunotherapeutic protocols; The interactions between tumor cells and the immune system play a crucial
role in cancer evolution. In this study, we explore how these interactions
influence cancer progression by modeling the relationships among naive T cells,
effector T cells, and chronic myeloid leukemia cells. We examine the existence
of equilibria, the asymptotic stability of the positive steady state, and the
global stability of the tumor-free equilibrium. Additionally, we develop a
partial differential equation to describe the conditions under which the
concentration of cancer cells reaches a level that allows for effective control
of cancer evolution. Finally, we apply our proposed model to investigate
optimal treatment strategies that aim to minimize both the concentration of
cancer cells at the end of treatment and the accumulation of tumor burden, as
well as the cost associated with treatment during the intervention period. Our
study reveals an optimal therapeutic protocol using optimal control theory. We
perform numerical simulations to illustrate our theoretical results and to
explore the dynamic behavior of the system and optimal therapeutic protocols.
The simulations indicate that the optimal treatment strategy can be more
effective than a constant treatment approach, even when applying the same
treatment interval and total drug input.",False,True,"I extracted the concept of transformers for image recognition from Paper 1 and the concept of modeling tumor-immune interactions from Paper 2. Combining these ideas, a novel multidisciplinary research project could develop a transformer-based model to analyze medical images of tumors and predict optimal immunotherapeutic protocols, integrating computer vision and cancer biology.",
2411.00726,1,applied,2411.00726-pos2-5,"Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema; This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection",2503.06617,"Pixel to Gaussian: Ultra-Fast Continuous Super-Resolution with 2D
  Gaussian Modeling; Arbitrary-scale super-resolution (ASSR) aims to reconstruct high-resolution
(HR) images from low-resolution (LR) inputs with arbitrary upsampling factors
using a single model, addressing the limitations of traditional SR methods
constrained to fixed-scale factors (\textit{e.g.}, $\times$ 2). Recent advances
leveraging implicit neural representation (INR) have achieved great progress by
modeling coordinate-to-pixel mappings. However, the efficiency of these methods
may suffer from repeated upsampling and decoding, while their reconstruction
fidelity and quality are constrained by the intrinsic representational
limitations of coordinate-based functions. To address these challenges, we
propose a novel ContinuousSR framework with a Pixel-to-Gaussian paradigm, which
explicitly reconstructs 2D continuous HR signals from LR images using Gaussian
Splatting. This approach eliminates the need for time-consuming upsampling and
decoding, enabling extremely fast arbitrary-scale super-resolution. Once the
Gaussian field is built in a single pass, ContinuousSR can perform
arbitrary-scale rendering in just 1ms per scale. Our method introduces several
key innovations. Through statistical ana",False,True,"I can extract concepts from both papers to create a novel multidisciplinary research idea. The first paper's investigation of retinal vasculature and thickness in Macular Edema can be combined with the second paper's super-resolution technique to develop a new method for analyzing retinal images, enabling faster and more accurate diagnosis of Macular Edema.",
2411.00726,1,applied,2411.00726-pos2-5,"Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema; This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection",2503.10403,"Hyper3D: Efficient 3D Representation via Hybrid Triplane and Octree
  Feature for Enhanced 3D Shape Variational Auto-Encoders; Recent 3D content generation pipelines often leverage Variational
Autoencoders (VAEs) to encode shapes into compact latent representations,
facilitating diffusion-based generation. Efficiently compressing 3D shapes
while preserving intricate geometric details remains a key challenge. Existing
3D shape VAEs often employ uniform point sampling and 1D/2D latent
representations, such as vector sets or triplanes, leading to significant
geometric detail loss due to inadequate surface coverage and the absence of
explicit 3D representations in the latent space. Although recent work explores
3D latent representations, their large scale hinders high-resolution encoding
and efficient training. Given these challenges, we introduce Hyper3D, which
enhances VAE reconstruction through efficient 3D representation that integrates
hybrid triplane and octree features. First, we adopt an octree-based feature
representation to embed mesh information into the network, mitigating the
limitations of uniform point sampling in capturing geometric distributions
along the mesh surface. Furthermore, we propose a hybrid latent space
representation that integrates a high-resolution triplane with a low-resolution
3D grid. This design not only compensates for the lack of explicit 3D
representations but also leverages a triplane to preserve high-resolution
details. Experimental results demonstrate that Hyper3D outperforms traditional
representations by reconstructing 3D shapes with higher fidelity and finer
details, making it well-suited for 3D generation pipelines.",False,True,"The combination of ideas from both papers can lead to a novel multidisciplinary research project. For instance, using the Hyper3D representation to analyze the 3D structure of retinal vasculature and its relation to retinal thickness in Macular Edema, can provide a more accurate and detailed understanding of the disease.",
2411.00726,1,applied,2411.00726-pos2-5,"Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema; This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection",2503.06976,"Task-Specific Knowledge Distillation from the Vision Foundation Model
  for Enhanced Medical Image Segmentation; Large-scale pre-trained models, such as Vision Foundation Models (VFMs), have
demonstrated impressive performance across various downstream tasks by
transferring generalized knowledge, especially when target data is limited.
However, their high computational cost and the domain gap between natural and
medical images limit their practical application in medical segmentation tasks.
Motivated by this, we pose the following important question: ""How can we
effectively utilize the knowledge of large pre-trained VFMs to train a small,
task-specific model for medical image segmentation when training data is
limited?"" To address this problem, we propose a novel and generalizable
task-specific knowledge distillation framework. Our method fine-tunes the VFM
on the target segmentation task to capture task-specific features before
distilling the knowledge to smaller models, leveraging Low-Rank Adaptation
(LoRA) to reduce the computational cost of fine-tuning. Additionally, we
incorporate synthetic data generated by diffusion models to augment the
transfer set, enhancing model performance in data-limited scenarios.
Experimental results across five medical image datasets demonstrate that our
method consistently outperforms task-agnostic knowledge distillation and
self-supervised pretraining approaches like MoCo v3 and Masked Autoencoders
(MAE). For example, on the KidneyUS dataset, our method achieved a 28% higher
Dice score than task-agnostic KD using 80 labeled samples for fine-tuning. On
the CHAOS dataset, it achieved an 11% improvement over MAE with 100 labeled
samples. These results underscore the potential of task-specific knowledge
distillation to train accurate, efficient models for medical image segmentation
in data-constrained settings.",False,True,"I made this verdict because the two papers can be combined to create a novel multidisciplinary research idea. For instance, the first paper's analysis of retinal vasculature and thickness in Macular Edema can be enhanced by using the task-specific knowledge distillation framework proposed in the second paper to improve the segmentation of retinal images, potentially leading to more accurate diagnosis and treatment of ME.",
2411.00726,1,applied,2411.00726-pos2-5,"Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema; This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection",2503.00591,"AesthetiQ: Enhancing Graphic Layout Design via Aesthetic-Aware
  Preference Alignment of Multi-modal Large Language Models; Visual layouts are essential in graphic design fields such as advertising,
posters, and web interfaces. The application of generative models for
content-aware layout generation has recently gained traction. However, these
models fail to understand the contextual aesthetic requirements of layout
design and do not align with human-like preferences, primarily treating it as a
prediction task without considering the final rendered output. To overcome
these problems, we offer Aesthetic-Aware Preference Alignment(AAPA), a novel
technique to train a Multi-modal Large Language Model (MLLM) for layout
prediction that uses MLLM's aesthetic preferences for Direct Preference
Optimization over graphic layouts. We propose a data filtering protocol
utilizing our layout-quality heuristics for AAPA to ensure training happens on
high-quality layouts. Additionally, we introduce a novel evaluation metric that
uses another MLLM to compute the win rate of the generated layout against the
ground-truth layout based on aesthetics criteria. We also demonstrate the
applicability of AAPA for MLLMs of varying scales (1B to 8B parameters) and LLM
families (Qwen, Phi, InternLM). By conducting thorough qualitative and
quantitative analyses, we verify the efficacy of our approach on two
challenging benchmarks - Crello and Webui, showcasing 17%, and 16 improvement
over current State-of-The-Art methods, thereby highlighting the potential of
MLLMs in aesthetic-aware layout generation.",False,False,"The two papers are from distinct disciplines, ophthalmology and computer science, but there is no apparent connection between the analysis of retinal vasculature and aesthetic-aware layout generation. Combining ideas from these papers would not lead to a feasible, novel, and useful interdisciplinary research idea.",
2411.00726,1,applied,2411.00726-pos2-5,"Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema; This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection",2503.04207,"Bridging the Vision-Brain Gap with an Uncertainty-Aware Blur Prior; Can our brain signals faithfully reflect the original visual stimuli, even
including high-frequency details? Although human perceptual and cognitive
capacities enable us to process and remember visual information, these
abilities are constrained by several factors, such as limited attentional
resources and the finite capacity of visual memory. When visual stimuli are
processed by human visual system into brain signals, some information is
inevitably lost, leading to a discrepancy known as the \textbf{System GAP}.
Additionally, perceptual and cognitive dynamics, along with technical noise in
signal acquisition, degrade the fidelity of brain signals relative to the
visual stimuli, known as the \textbf{Random GAP}. When encoded brain
representations are directly aligned with the corresponding pretrained image
features, the System GAP and Random GAP between paired data challenge the
model, requiring it to bridge these gaps. However, in the context of limited
paired data, these gaps are difficult for the model to learn, leading to
overfitting and poor generalization to new data. To address these GAPs, we
propose a simple yet effective approach called the \textbf{Uncertainty-aware
Blur Prior (UBP)}. It estimates the uncertainty within the paired data,
reflecting the mismatch between brain signals and visual stimuli. Based on this
uncertainty, UBP dynamically blurs the high-frequency details of the original
images, reducing the impact of the mismatch and improving alignment. Our method
achieves a top-1 accuracy of \textbf{50.9\%} and a top-5 accuracy of
\textbf{79.7\%} on the zero-shot brain-to-image retrieval task, surpassing
previous state-of-the-art methods by margins of \textbf{13.7\%} and
\textbf{9.8\%}, respectively. Code is available at
\href{https://github.com/HaitaoWuTJU/Uncertainty-aware-Blur-Prior}{GitHub}.",False,True,I extracted the concept of fractal dimension analysis from retinal vasculature in Paper 1 and the uncertainty-aware blur prior from Paper 2 to create a novel multidisciplinary research idea. This idea involves using fractal dimension analysis to quantify retinal vasculature changes in Macular Edema and applying the uncertainty-aware blur prior to improve the accuracy of brain-to-image retrieval tasks in diagnosing retinal diseases.,
2411.00726,1,applied,2411.00726-pos1-5,"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",2502.09801,"Unit Testing Past vs. Present: Examining LLMs' Impact on Defect
  Detection and Efficiency; The integration of Large Language Models (LLMs), such as ChatGPT and GitHub
Copilot, into software engineering workflows has shown potential to enhance
productivity, particularly in software testing. This paper investigates whether
LLM support improves defect detection effectiveness during unit testing.
Building on prior studies comparing manual and tool-supported testing, we
replicated and extended an experiment where participants wrote unit tests for a
Java-based system with seeded defects within a time-boxed session, supported by
LLMs. Comparing LLM supported and manual testing, results show that LLM support
significantly increases the number of unit tests generated, defect detection
rates, and overall testing efficiency. These findings highlight the potential
of LLMs to improve testing and defect detection outcomes, providing empirical
insights into their practical application in software testing.",False,True,"The combination of transformer architecture for image recognition and large language models for software testing can lead to a novel multidisciplinary research idea, such as using transformer-based image recognition to automatically generate test cases for software testing, leveraging the strengths of both disciplines to improve testing efficiency and defect detection rates.",
2411.00726,1,applied,2411.00726-pos1-5,"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",2501.19381,"Using gradient of Lagrangian function to compute efficient channels for
  the ideal observer; It is widely accepted that the Bayesian ideal observer (IO) should be used to
guide the objective assessment and optimization of medical imaging systems. The
IO employs complete task-specific information to compute test statistics for
making inference decisions and performs optimally in signal detection tasks.
However, the IO test statistic typically depends non-linearly on the image data
and cannot be analytically determined. The ideal linear observer, known as the
Hotelling observer (HO), can sometimes be used as a surrogate for the IO.
However, when image data are high dimensional, HO computation can be difficult.
Efficient channels that can extract task-relevant features have been
investigated to reduce the dimensionality of image data to approximate IO and
HO performance. This work proposes a novel method for generating efficient
channels by use of the gradient of a Lagrangian-based loss function that was
designed to learn the HO. The generated channels are referred to as the
Lagrangian-gradient (L-grad) channels. Numerical studies are conducted that
consider binary signal detection tasks involving various backgrounds and
signals. It is demonstrated that channelized HO (CHO) using L-grad channels can
produce significantly better signal detection performance compared to the CHO
using PLS channels. Moreover, it is shown that the proposed L-grad method can
achieve significantly lower computation time compared to the PLS method.",False,True,"I can extract concepts from both papers to create a novel multidisciplinary research idea. By combining the transformer architecture from Paper 1 with the Lagrangian-gradient channels from Paper 2, a new approach to image recognition in medical imaging systems can be developed, potentially leading to more efficient and accurate disease diagnosis.",
2411.00726,1,applied,2411.00726-pos1-5,"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",2501.01007,"Deep Reinforcement Learning for Job Scheduling and Resource Management
  in Cloud Computing: An Algorithm-Level Review; Cloud computing has revolutionized the provisioning of computing resources,
offering scalable, flexible, and on-demand services to meet the diverse
requirements of modern applications. At the heart of efficient cloud operations
are job scheduling and resource management, which are critical for optimizing
system performance and ensuring timely and cost-effective service delivery.
However, the dynamic and heterogeneous nature of cloud environments presents
significant challenges for these tasks, as workloads and resource availability
can fluctuate unpredictably. Traditional approaches, including heuristic and
meta-heuristic algorithms, often struggle to adapt to these real-time changes
due to their reliance on static models or predefined rules. Deep Reinforcement
Learning (DRL) has emerged as a promising solution to these challenges by
enabling systems to learn and adapt policies based on continuous observations
of the environment, facilitating intelligent and responsive decision-making.
This survey provides a comprehensive review of DRL-based algorithms for job
scheduling and resource management in cloud computing, analyzing their
methodologies, performance metrics, and practical applications. We also
highlight emerging trends and future research directions, offering valuable
insights into leveraging DRL to advance both job scheduling and resource
management in cloud computing.",False,True,"By combining the transformer architecture for image recognition from Paper 1 with the deep reinforcement learning approach for job scheduling and resource management from Paper 2, a novel multidisciplinary research idea can be formed. For instance, a system can be designed to recognize images in real-time and dynamically adjust cloud computing resources using DRL, optimizing performance and efficiency.",
2411.00726,1,applied,2411.00726-pos1-5,"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",2503.09705,"Altermagnets with topological order in Kitaev bilayers; Building on recent advancements in altermagnetism, we develop a
highly-frustrated magnetic model with Kitaev-like interactions that integrates
key aspects of both quantum spin liquids and altermagnets. While the ground
state is a gapless quantum spin liquid, our analysis indicates that an
altermagnetic local order emerges upon the introduction of additional
interactions that gap the excitation spectrum and give rise to a $\mathbb{Z}_2
$ topological order. This magnetically-fragmented topological altermagnet has
fractionalized fermionic excitations with momentum-dependent splitting, in
stark contrast to both standard altermagnets and Kitaev spin liquids. In
addition, we discover two more altermagnetic phases, including a
pseudo-altermagnet that exhibits splitting in the absence of a local order and
a half-altermagnet that possesses only one type of fractionalized excitations,
similar to a half-metal. We discuss experimental approaches for detecting these
phases, including layer-dependent spin and heat transport. Our results
highlight the rich physics that can arise due to the interplay between
altermagnetism and fractionalized excitations in quantum magnets.",False,False,"The two papers are from distinct areas of computer vision and quantum magnetism, with no direct overlap or connection. The concepts and techniques presented in the papers, such as transformers for image recognition and altermagnetism with topological order, do not lend themselves to a straightforward combination or integration.",
2411.00726,1,applied,2411.00726-pos1-5,"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",2502.03989,"Radii of light nuclei from the Jacobi No-Core Shell Model; Accurately determining the size of the atomic nucleus with realistic nuclear
forces is a long outstanding issue of nuclear physics. The no-core shell model
(NCSM), one of the powerful ab initio methods for nuclear structure, can
achieve accurate energies of light nuclei. The extraction of converged radii is
more difficult. In this work, we present a novel method to effectively extract
the radius of light nuclei by restoring the long-range behavior of densities
from NCSM calculations. The correct large distance asymptotic of two-body
relative densities are deduced based on the NCSM densities in limited basis
size. The resulting radii using the corrected densities show a nice
convergence. The root-mean-square matter and charge radii of $^{4,6,8}$He and
$^{6,7,8}$Li can be accurately obtained based on Jacobi-NCSM calculations with
the high-precision chiral two-nucleon and three-nucleon forces combined with
this new method. Our method can be straightforwardly extended to other ab
initio calculations, potentially providing a better description of nuclear
sizes with realistic nuclear forces.",False,False,"The two papers come from distinct areas of computer vision and nuclear physics, making it challenging to extract a novel multidisciplinary research idea that integrates concepts from both papers. The ideas and methods presented in the papers seem to be unrelated, and there is no clear connection between image recognition and nuclear physics.",
2411.00726,1,applied,2411.00726-pos2-5,"Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema; This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection",2503.04705,"One Extension to Explain Them All, One Parameter to Minimize $\chi^2$,
  One Framework to Bring Them All, and in One Model Bind Them; The increasing precision of Cosmic Microwave Background (CMB) observations
has unveiled significant tensions between different datasets, notably between
Planck and the Atacama Cosmology Telescope (ACT), as well as with the
late-Universe measurements of the Hubble constant. In this work, we explore a
variety of $\Lambda$CDM extensions to assess their ability to reconcile these
discrepancies. The statistical preference for these extensions remains
moderate, and imposing $n_s=1$ often worsens model performance. Our findings
highlight the limitations of incremental modifications to $\Lambda$CDM and
suggest that either more complex new physics or, more likely, improved
systematic understanding in the CMB sector may be required to fully address the
observed tensions. While CMB experiments are often considered the gold standard
of precision cosmology, our results reinforce that these measurements are not
immune to systematic uncertainties, which may be underestimated in current
analyses.",False,False,"The two papers are from distinct disciplines (ophthalmology and cosmology) with no apparent connection. The concepts and methods used in each paper are not complementary, and there is no clear way to integrate them to create a novel multidisciplinary research idea that meets the standards of interdisciplinary research.",
2411.00726,1,applied,2411.00726-pos2-5,"Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema; This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection",2502.13373,"Fighter Jet Navigation and Combat using Deep Reinforcement Learning with
  Explainable AI; This paper presents the development of an Artificial Intelligence (AI) based
fighter jet agent within a customized Pygame simulation environment, designed
to solve multi-objective tasks via deep reinforcement learning (DRL). The jet's
primary objectives include efficiently navigating the environment, reaching a
target, and selectively engaging or evading an enemy. A reward function
balances these goals while optimized hyperparameters enhance learning
efficiency. Results show more than 80\% task completion rate, demonstrating
effective decision-making. To enhance transparency, the jet's action choices
are analyzed by comparing the rewards of the actual chosen action (factual
action) with those of alternate actions (counterfactual actions), providing
insights into the decision-making rationale. This study illustrates DRL's
potential for multi-objective problem-solving with explainable AI. Project page
is available at:
\href{https://github.com/swatikar95/Autonomous-Fighter-Jet-Navigation-and-Combat}{Project
GitHub Link}.",False,False,"Although both papers present innovative ideas, they belong to distinct disciplines (medical imaging and artificial intelligence/computer science) with no apparent overlap. The concepts and techniques used in each paper are not directly compatible, making it challenging to extract a novel multidisciplinary research idea that meets the standards of interdisciplinarity, feasibility, novelty, and usefulness.",
2411.00726,1,applied,2411.00726-pos2-5,"Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema; This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection",2503.12205,"PredicateFix: Repairing Static Analysis Alerts with Bridging Predicates; Using Large Language Models (LLMs) to fix static analysis alerts in program
code is becoming increasingly popular and helpful. However, these models often
have the problem of hallucination and perform poorly for complex and less
common alerts, limiting their performance. Retrieval-augmented generation (RAG)
aims to solve this problem by providing the model with a relevant example, but
the unsatisfactory quality of such examples challenges the effectiveness of
existing approaches.
  To address this challenge, this paper utilizes the predicates in the analysis
rule, which can serve as a bridge between the alert and relevant code snippets
within a clean code corpus, called key examples. Based on the above insight, we
propose an algorithm to retrieve key examples for an alert automatically. Then,
we build PredicateFix as a RAG pipeline to fix alerts flagged by the CodeQL
code checker and another imperative static analyzer for Golang. Evaluation with
multiple LLMs shows that PredicateFix increases the number of correct repairs
by 27.1% ~ 72.5%, significantly outperforming other baseline RAG approaches.",False,False,"The two papers are from distinct areas of medical imaging and computer science, making it challenging to extract a novel multidisciplinary research idea. The concepts of retinal vasculature and static analysis alerts are unrelated, and combining them does not seem to lead to a feasible, novel, and useful research idea.",
2411.00726,1,applied,2411.00726-pos2-5,"Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema; This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection",2501.18024,"Zeros of symmetric power period polynomials; Suppose that $k$ and $N$ are positive integers. Let $f$ be a newform on
$\Gamma_0(N)$ of weight $k$ with $L$-function $L_f(s)$.
  Previous works have studied the zeros of the period polynomial $r_f(z)$,
which is a generating function for the critical values of $L_f(s)$ and has a
functional equation relating $z$ and $-1/Nz$.
  In particular, $r_f(z)$ satisfies a version of the Riemann hypothesis: all of
its zeros are on the circle of symmetry
  $\{z \in \C \ : \ |z|=1/\sqrt{N}\}$.
  In this paper, for a positive integer $m$, we define a natural analogue of
$r_f(z)$ for the $m^{\operatorname{th}}$ symmetric power $L$-function of $f$
when $N$ is squarefree. Our analogue also has a functional equation relating
$z$ and $-1/Nz$. We prove the corresponding version of the Riemann hypothesis
when $k$ is large enough. Moreover, when
$k>2(\operatorname{log}_2(13e^{2\pi}/9)+m)+1$, we prove our result when $N$ is
large enough.",False,False,"The two papers come from distinct disciplines, ophthalmology and number theory, with no apparent connection between them. The concepts and methods used in each paper are highly specialized and do not lend themselves to a straightforward combination, making it difficult to extract a novel multidisciplinary research idea.",
2411.00726,1,applied,2411.00726-pos2-5,"Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema; This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection",2502.09506,"Journey from the Wilson exact RG towards the Wegner-Morris Fokker-Planck
  RG and the Carosso field-coarsening via Langevin stochastic processes; Within the Wilson RG of 'incomplete integration' as a function of the RG-time
$t$, the non-linear differential RG flow for the energy $E_t[\phi(.)]$
translates for the probability distribution $P_t[\phi(.)] \sim e^{-
E_t[\phi(.)]} $ into the linear Fokker-Planck RG flow associated to independent
non-identical Ornstein-Uhlenbeck processes for the Fourier modes. The
corresponding Langevin stochastic differential equation for the real-space
field $\phi_t(\vec x)$ can be then interpreted within the Carosso perspective
as genuine infinitesimal coarsening-transformations that are the analog of
spin-blocking, and whose irreversible character is essential to overcome the
paradox of the naive description of the Wegner-Morris RG flow as a mere
infinitesimal change of variables in the partition function integral. This
interpretation suggests to consider new RG-schemes, in particular the Carosso
RG where the Langevin SDE corresponds to the well known stochastic heat
equation or the Edwards-Wilkinson dynamics. We stress the advantages of this
stochastic formulation of exact RG flows. While statistical field theory is
usually written in infinite space, we focus here on the formulation on a large
volume $L^d$ with periodic boundary conditions, in order to distinguish between
extensive and intensives observables while keeping the translation-invariance.
Since the empirical magnetization $m_e \equiv \frac{1}{L^d} \int_{L^d} d^d \vec
x \ \phi(\vec x) $ is an intensive variable corresponding to the zero-momentum
Fourier coefficient of the field, its probability distribution $p_L(m_e)$ can
be obtained from the gradual integration over all the other Fourier
coefficients associated to non-vanishing-momenta via exact differential RG, in
order to obtain the large deviation properties with respect to the volume
$L^d$.",False,False,"Although both papers introduce innovative ideas from distinct disciplines, the concepts presented are too disparate to be combined into a novel multidisciplinary research idea. The first paper focuses on medical imaging and ophthalmology, while the second paper delves into statistical physics and field theory, making it challenging to identify a common ground for integration.",
2411.05236,1,applied,2411.05236-pos1-6,"DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT; Shannon capacity of signal transduction for multiple independent receptors; Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver.",2411.05236-pos2-6,"Channelrhodopsin-2, a directly light-gated cation-selective membrane channel; Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane α helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination.",True,True,"The combination of visible light communication (VLC) technology from Paper 1 and the light-gated channelrhodopsin-2 (ChR2) from Paper 2 can lead to a novel multidisciplinary research idea. This idea can integrate VLC with ChR2 to develop a new method for high-speed wireless data transmission in biological systems, such as optogenetics or biohybrid devices.",
2411.05236,1,applied,2411.05236-pos1-6,"DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT; Shannon capacity of signal transduction for multiple independent receptors; Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver.",2501.05607,"Prediction of Binding Affinity for ErbB Inhibitors Using Deep Neural
  Network Model with Morgan Fingerprints as Features; The ErbB receptor family, including EGFR and HER2, plays a crucial role in
cell growth and survival and is associated with the progression of various
cancers such as breast and lung cancer. In this study, we developed a deep
learning model to predict the binding affinity of ErbB inhibitors using
molecular fingerprints derived from SMILES representations. The SMILES
representations for each ErbB inhibitor were obtained from the ChEMBL database.
We first generated Morgan fingerprints from the SMILES strings and applied
AutoDock Vina docking to calculate the binding affinity values. After filtering
the dataset based on binding affinity, we trained a deep neural network (DNN)
model to predict binding affinity values from the molecular fingerprints. The
model achieved significant performance, with a Mean Squared Error (MSE) of
0.2591, Mean Absolute Error (MAE) of 0.3658, and an R-squared value of 0.9389
on the training set. Although performance decreased slightly on the test set (R
squared = 0.7731), the model still demonstrated robust generalization
capabilities. These results indicate that the deep learning approach is highly
effective for predicting the binding affinity of ErbB inhibitors, offering a
valuable tool for virtual screening and drug discovery.",False,True,"I extracted concepts from both papers to create a novel multidisciplinary research idea. The idea combines visible light communication (VLC) technology with the prediction of binding affinity for ErbB inhibitors using deep neural networks. The VLC system can be used to transmit molecular fingerprints of ErbB inhibitors to a receiver, which can then use a deep learning model to predict the binding affinity. This integrated system can potentially accelerate virtual screening and drug discovery in cancer research.",
2411.05236,1,applied,2411.05236-pos1-6,"DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT; Shannon capacity of signal transduction for multiple independent receptors; Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver.",2503.16996,"An Energy-Adaptive Elastic Equivariant Transformer Framework for Protein
  Structure Representation; Structure-informed protein representation learning is essential for effective
protein function annotation and \textit{de novo} design. However, the presence
of inherent noise in both crystal and AlphaFold-predicted structures poses
significant challenges for existing methods in learning robust protein
representations. To address these issues, we propose a novel equivariant
Transformer-State Space Model(SSM) hybrid framework, termed $E^3$former,
designed for efficient protein representation. Our approach uses energy
function-based receptive fields to construct proximity graphs and incorporates
an equivariant high-tensor-elastic selective SSM within the transformer
architecture. These components enable the model to adapt to complex atom
interactions and extract geometric features with higher signal-to-noise ratios.
Empirical results demonstrate that our model outperforms existing methods in
structure-intensive tasks, such as inverse folding and binding site prediction,
particularly when using predicted structures, owing to its enhanced tolerance
to data deviation and noise. Our approach offers a novel perspective for
conducting biological function research and drug discovery using noisy protein
structure data.",False,False,"Although both papers present innovative ideas, they come from distinct disciplines (communication engineering and bioinformatics) with no apparent overlap. The concepts and techniques presented in each paper do not seem to complement or integrate with each other in a way that would enable the creation of a novel multidisciplinary research idea.",
2411.05236,1,applied,2411.05236-pos1-6,"DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT; Shannon capacity of signal transduction for multiple independent receptors; Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver.",2503.01376,"Pushing the boundaries of Structure-Based Drug Design through
  Collaboration with Large Language Models; Structure-Based Drug Design (SBDD) has revolutionized drug discovery by
enabling the rational design of molecules for specific protein targets. Despite
significant advancements in improving docking scores, advanced 3D-SBDD
generative models still face challenges in producing drug-like candidates that
meet medicinal chemistry standards and pharmacokinetic requirements. These
limitations arise from their inherent focus on molecular interactions, often
neglecting critical aspects of drug-likeness. To address these shortcomings, we
introduce the Collaborative Intelligence Drug Design (CIDD) framework, which
combines the structural precision of 3D-SBDD models with the chemical reasoning
capabilities of large language models (LLMs). CIDD begins by generating
supporting molecules with 3D-SBDD models and then refines these molecules
through LLM-supported modules to enhance drug-likeness and structural
reasonability. When evaluated on the CrossDocked2020 dataset, CIDD achieved a
remarkable success ratio of 37.94%, significantly outperforming the previous
state-of-the-art benchmark of 15.72%. Although improving molecular interactions
and drug-likeness is often seen as a trade-off, CIDD uniquely achieves a
balanced improvement in both by leveraging the complementary strengths of
different models, offering a robust and innovative pathway for designing
therapeutically promising drug candidates.",False,False,"Although both papers present innovative ideas in their respective fields, combining Visible Light Communication (VLC) and Structure-Based Drug Design (SBDD) does not yield a novel multidisciplinary research idea. The concepts and methods presented in the papers do not share a common ground or problem to solve, making it challenging to integrate them into a coherent and feasible research project.",
2411.05236,1,applied,2411.05236-pos1-6,"DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT; Shannon capacity of signal transduction for multiple independent receptors; Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver.",2502.03606,"Artificial Intelligence Approaches for Anti-Addiction Drug Discovery; Drug addiction is a complex and pervasive global challenge that continues to
pose significant public health concerns. Traditional approaches to
anti-addiction drug discovery have struggled to deliver effective therapeutics,
facing high attrition rates, long development timelines, and inefficiencies in
processing large-scale data. Artificial intelligence (AI) has emerged as a
transformative solution to address these issues. Using advanced algorithms, AI
is revolutionizing drug discovery by enhancing the speed and precision of key
processes. This review explores the transformative role of AI in the pipeline
for anti-addiction drug discovery, including data collection, target
identification, and compound optimization. By highlighting the potential of AI
to overcome traditional barriers, this review systematically examines how AI
addresses critical gaps in anti-addiction research, emphasizing its potential
to revolutionize drug discovery and development, overcome challenges, and
advance more effective therapeutic strategies.",False,False,"The two papers are from distinct areas, VLC system and AI for anti-addiction drug discovery. Although both papers present innovative ideas, there is no clear connection or overlap between them that can be combined to create a novel multidisciplinary research idea that meets the standards of interdisciplinary research.",
2411.05236,1,applied,2411.05236-pos1-6,"DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT; Shannon capacity of signal transduction for multiple independent receptors; Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver.",2502.07671,"Steering Protein Family Design through Profile Bayesian Flow; Protein family design emerges as a promising alternative by combining the
advantages of de novo protein design and mutation-based directed evolution.In
this paper, we propose ProfileBFN, the Profile Bayesian Flow Networks, for
specifically generative modeling of protein families. ProfileBFN extends the
discrete Bayesian Flow Network from an MSA profile perspective, which can be
trained on single protein sequences by regarding it as a degenerate profile,
thereby achieving efficient protein family design by avoiding large-scale MSA
data construction and training. Empirical results show that ProfileBFN has a
profound understanding of proteins. When generating diverse and novel family
proteins, it can accurately capture the structural characteristics of the
family. The enzyme produced by this method is more likely than the previous
approach to have the corresponding function, offering better odds of generating
diverse proteins with the desired functionality.",False,False,"Although both papers present innovative ideas in their respective fields, they seem unrelated and lack a clear connection. Visible Light Communication (VLC) is a technology for wireless data transmission, while Profile Bayesian Flow Networks is a method for protein family design. Without a clear link or overlap between the two disciplines, it's challenging to extract a novel multidisciplinary research idea that meets the standards.",
2411.05236,1,applied,2411.05236-pos2-6,"Channelrhodopsin-2, a directly light-gated cation-selective membrane channel; Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane α helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination.",2502.01841,"Diffusion Model for Multiple Antenna Communications; The potential of applying diffusion models (DMs) for multiple antenna
communications is discussed. A unified framework of applying DM for multiple
antenna tasks is first proposed. Then, the tasks are innovatively divided into
two categories, i.e., decision-making tasks and generation tasks, depending on
whether an optimization of system parameters is involved. For each category, it
is conceived 1) how the framework can be used for each task and 2) why the DM
is superior to traditional artificial intelligence (TAI) and conventional
optimization tasks. It is highlighted that the DMs are well-suited for
scenarios with strong interference and noise, excelling in modeling complex
data distribution and exploring better actions. A case study of learning
beamforming with a DM is then provided, to demonstrate the superiority of the
DMs with simulation results. Finally, the applications of DM for emerging
multiple antenna technologies and promising research directions are discussed.",False,False,"The two papers are from distinct areas, one from biophysics and the other from telecommunications. Although they both discuss novel concepts, there is no clear connection or overlap between the ideas presented, making it challenging to extract a novel multidisciplinary research idea that meets the standards of interdisciplinarity, feasibility, novelty, and usefulness.",
2411.05236,1,applied,2411.05236-pos2-6,"Channelrhodopsin-2, a directly light-gated cation-selective membrane channel; Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane α helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination.",2502.13437,"Research on the Offshore Marine Communication Environment Based on
  Satellite Remote Sensing Data; Air-sea interface fluxes significantly impact the reliability and efficiency
of maritime communication. Compared to sparse in-situ ocean observations,
satellite remote sensing data offers broader coverage and extended temporal
span. This study utilizes COARE V3.5 algorithm to calculate momentum flux,
sensible heat flux, and latent heat flux at the air-sea interface, based on
satellite synthetic aperture radar (SAR) wind speed data, reanalysis data, and
buoy measurements, combined with neural network methods. Findings indicate that
SAR wind speed data corrected via neural networks show improved consistency
with buoy-measured wind speeds in flux calculations. Specifically, the bias in
friction velocity decreased from -0.03 m/s to 0.01 m/s, wind stress bias from
-0.03 N/m^2 to 0.00 N/m^2, drag coefficient bias from -0.29 to -0.21, latent
heat flux bias from -8.32 W/m^2 to 5.41 W/m^2, and sensible heat flux bias from
0.67 W/m^2 to 0.06 W/m^2. Results suggest that the neural network-corrected SAR
wind speed data can provide more reliable environmental data for maritime
communication.",False,False,"Although both papers present innovative ideas in their respective fields, they seem unrelated, and combining them does not yield a straightforward interdisciplinary research idea. One paper focuses on a biological light-gated channel, while the other deals with satellite remote sensing data for maritime communication, making it challenging to integrate them into a novel and feasible research project.",
2411.05236,1,applied,2411.05236-pos2-6,"Channelrhodopsin-2, a directly light-gated cation-selective membrane channel; Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane α helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination.",2501.18378,"A Hybrid Dynamic Subarray Architecture for Efficient DOA Estimation in
  THz Ultra-Massive Hybrid MIMO Systems; Terahertz (THz) communication combined with ultra-massive multiple-input
multiple-output (UM-MIMO) technology is promising for 6G wireless systems,
where fast and precise direction-of-arrival (DOA) estimation is crucial for
effective beamforming. However, finding DOAs in THz UM-MIMO systems faces
significant challenges: while reducing hardware complexity, the hybrid
analog-digital (HAD) architecture introduces inherent difficulties in spatial
information acquisition the large-scale antenna array causes significant
deviations in eigenvalue decomposition results; and conventional
two-dimensional DOA estimation methods incur prohibitively high computational
overhead, hindering fast and accurate realization. To address these challenges,
we propose a hybrid dynamic subarray (HDS) architecture that strategically
divides antenna elements into subarrays, ensuring phase differences between
subarrays correlate exclusively with single-dimensional DOAs. Leveraging this
architectural innovation, we develop two efficient algorithms for DOA
estimation: a reduced-dimension MUSIC (RD-MUSIC) algorithm that enables fast
processing by correcting large-scale array estimation bias, and an improved
version that further accelerates estimation by exploiting THz channel sparsity
to obtain initial closed-form solutions through specialized two-RF-chain
configuration. Furthermore, we develop a theoretical framework through
Cram\'{e}r-Rao lower bound analysis, providing fundamental insights for
different HDS configurations. Extensive simulations demonstrate that our
solution achieves both superior estimation accuracy and computational
efficiency, making it particularly suitable for practical THz UM-MIMO systems.",False,False,"The two papers are from distinct disciplines, one from biophysics (Channelrhodopsin-2) and the other from electrical engineering (THz Ultra-Massive Hybrid MIMO Systems). While they are both innovative in their respective fields, there is no clear connection or overlap between the ideas presented, making it challenging to extract a novel multidisciplinary research idea that meets the standards.",
2411.05236,1,applied,2411.05236-pos2-6,"Channelrhodopsin-2, a directly light-gated cation-selective membrane channel; Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane α helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination.",2501.11986,"Diffeomorphic ICP Registration for Single and Multiple Point Sets; We propose a generalization of the iterative closest point (ICP) algorithm
for point set registration, in which the registration functions are non-rigid
and follow the large deformation diffeomorphic metric mapping (LDDMM)
framework. The algorithm is formulated as a well-posed probabilistic inference,
and requires to solve a novel variation of LDDMM landmark registration with an
additional term involving the Jacobian of the mapping. The algorithm can easily
be generalized to construct a diffeomorphic, statistical atlas of multiple
point sets. The method is successfully validated on a first set of synthetic
data.",False,False,"Although both papers introduce innovative ideas, they are from distinct fields (biophysics and computer science) and do not share a common ground for integration. The concepts of channelrhodopsin-2 and diffeomorphic ICP registration are too disparate to be combined into a novel multidisciplinary research idea that meets the standards of interdisciplinary research.",
2411.05236,1,applied,2411.05236-pos2-6,"Channelrhodopsin-2, a directly light-gated cation-selective membrane channel; Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane α helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination.",2501.18765,"SDM Optical Systems with MMSE Equalizers: Information Rates and
  Performance Monitoring; The information rate of coupled space-division multiplexing (SDM)
transmission systems is impaired by the stochastic effects of mode-dependent
gain (MDG) and mode-dependent loss (MDL), turning it into a random variable and
reducing its average value. In systems operating with minimum mean squared
error (MMSE) equalizers and no channel-state information (CSI), co-channel
interference further reduces the instantaneous and average information rates.
Analytical solutions for the average information rate in MDG- and MDL-impaired
systems under strong coupling have been presented in early studies assuming
ideal maximum-likelihood (ML) equalization. However, to the best of our
knowledge, a solution encompassing co-channel interference under MMSE
equalization has not been presented yet. In this work, we derive statistical
models for the MMSE equalizer coefficients and develop analytical solutions for
the post-filtering information rate. We also use these statistical models and
analytical solutions to carry out MDG and signal-to-noise ratio (SNR)
monitoring in coupled SDM systems. The derived analytical solutions and
monitoring techniques are validated by Monte-Carlo simulations, exhibiting a
suitable accuracy within practical operational values.",False,False,"The two papers are from distinct areas of research, one from biology and the other from telecommunications. Although they both involve the concept of light, the ideas presented in the papers do not complement each other in a way that would allow for the creation of a novel multidisciplinary research idea that meets the standards of being feasible, novel, and useful.",
2411.05236,1,applied,2411.05236-pos1-6,"DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT; Shannon capacity of signal transduction for multiple independent receptors; Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver.",2501.15571,"Cross-Cultural Fashion Design via Interactive Large Language Models and
  Diffusion Models; Fashion content generation is an emerging area at the intersection of
artificial intelligence and creative design, with applications ranging from
virtual try-on to culturally diverse design prototyping. Existing methods often
struggle with cultural bias, limited scalability, and alignment between textual
prompts and generated visuals, particularly under weak supervision. In this
work, we propose a novel framework that integrates Large Language Models (LLMs)
with Latent Diffusion Models (LDMs) to address these challenges. Our method
leverages LLMs for semantic refinement of textual prompts and introduces a weak
supervision filtering module to effectively utilize noisy or weakly labeled
data. By fine-tuning the LDM on an enhanced DeepFashion+ dataset enriched with
global fashion styles, the proposed approach achieves state-of-the-art
performance. Experimental results demonstrate that our method significantly
outperforms baselines, achieving lower Frechet Inception Distance (FID) and
higher Inception Scores (IS), while human evaluations confirm its ability to
generate culturally diverse and semantically relevant fashion content. These
results highlight the potential of LLM-guided diffusion models in driving
scalable and inclusive AI-driven fashion innovation.",False,True,"I extracted concepts from both papers to create a novel multidisciplinary research idea: ""Smart Fashion Display using Visible Light Communication and AI-Generated Fashion Content"". This idea combines VLC technology with AI-driven fashion design to create an interactive and culturally diverse fashion display system, enabling real-time transmission of fashion content using VLC and AI-generated designs.",
2411.05236,1,applied,2411.05236-pos1-6,"DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT; Shannon capacity of signal transduction for multiple independent receptors; Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver.",2503.15918,"Denoising-based Contractive Imitation Learning; A fundamental challenge in imitation learning is the \emph{covariate shift}
problem. Existing methods to mitigate covariate shift often require additional
expert interactions, access to environment dynamics, or complex adversarial
training, which may not be practical in real-world applications. In this paper,
we propose a simple yet effective method (DeCIL) to mitigate covariate shift by
incorporating a denoising mechanism that enhances the contraction properties of
the state transition mapping. Our approach involves training two neural
networks: a dynamics model ( f ) that predicts the next state from the current
state, and a joint state-action denoising policy network ( d ) that refines
this state prediction via denoising and outputs the corresponding action. We
provide theoretical analysis showing that the denoising network acts as a local
contraction mapping, reducing the error propagation of the state transition and
improving stability. Our method is straightforward to implement and can be
easily integrated with existing imitation learning frameworks without requiring
additional expert data or complex modifications to the training procedure.
Empirical results demonstrate that our approach effectively improves success
rate of various imitation learning tasks under noise perturbation.",False,True,"The idea of combining visible light communication (VLC) with denoising-based contractive imitation learning can create a novel multidisciplinary research project. VLC can be used to transmit data in indoor environments, while denoising-based imitation learning can be applied to improve the stability and accuracy of VLC systems in noisy environments.",
2411.05236,1,applied,2411.05236-pos1-6,"DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT; Shannon capacity of signal transduction for multiple independent receptors; Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver.",2503.02506,"Robust Multi-Source Domain Adaptation under Label Shift; As the volume of data continues to expand, it becomes increasingly common for
data to be aggregated from multiple sources. Leveraging multiple sources for
model training typically achieves better predictive performance on test
datasets. Unsupervised multi-source domain adaptation aims to predict labels of
unlabeled samples in the target domain by using labeled samples from source
domains. This work focuses on robust multi-source domain adaptation for
multi-category classification problems against the heterogeneity of label shift
and data contamination. We investigate a domain-weighted empirical risk
minimization framework for robust estimation of the target domain's class
proportion. Inspired by outlier detection techniques, we propose a refinement
procedure within this framework. With the estimated class proportion, robust
classifiers for the target domain can be constructed. Theoretically, we study
the finite-sample error bounds of the domain-weighted empirical risk
minimization and highlight the improvement of the refinement step. Numerical
simulations and real-data applications demonstrate the superiority of the
proposed method.",False,True,"I extracted concepts from both papers to create a novel multidisciplinary research idea. By integrating the visible light communication system from Paper 1 with the robust multi-source domain adaptation framework from Paper 2, a novel research idea on ""Robust Visible Light Communication Systems for Multi-Source Domain Adaptation"" can be formed, which aims to develop a VLC system that can adapt to different indoor environments with varying light sources and noise levels, and can robustly transmit data with high accuracy.",
2411.05236,1,applied,2411.05236-pos1-6,"DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT; Shannon capacity of signal transduction for multiple independent receptors; Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver.",2502.06812,"Harness Local Rewards for Global Benefits: Effective Text-to-Video
  Generation Alignment with Patch-level Reward Models; The emergence of diffusion models (DMs) has significantly improved the
quality of text-to-video generation models (VGMs). However, current VGM
optimization primarily emphasizes the global quality of videos, overlooking
localized errors, which leads to suboptimal generation capabilities. To address
this issue, we propose a post-training strategy for VGMs, HALO, which
explicitly incorporates local feedback from a patch reward model, providing
detailed and comprehensive training signals with the video reward model for
advanced VGM optimization. To develop an effective patch reward model, we
distill GPT-4o to continuously train our video reward model, which enhances
training efficiency and ensures consistency between video and patch reward
distributions. Furthermore, to harmoniously integrate patch rewards into VGM
optimization, we introduce a granular DPO (Gran-DPO) algorithm for DMs,
allowing collaborative use of both patch and video rewards during the
optimization process. Experimental results indicate that our patch reward model
aligns well with human annotations and HALO substantially outperforms the
baselines across two evaluation methods. Further experiments quantitatively
prove the existence of patch defects, and our proposed method could effectively
alleviate this issue.",False,False,"The two papers are from distinct areas of research, visible light communication and text-to-video generation. While both papers present innovative ideas, there is no clear connection or overlap between the two disciplines that could be combined to create a novel multidisciplinary research idea that meets the standards of interdisciplinary research.",
2411.05236,1,applied,2411.05236-pos1-6,"DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT; Shannon capacity of signal transduction for multiple independent receptors; Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver.",2503.06002,"Knowledge Workers' Perspectives on AI Training for Responsible AI Use; AI expansion has accelerated workplace adoption of new technologies. Yet, it
is unclear whether and how knowledge workers are supported and trained to
safely use AI. Inadequate training may lead to unrealized benefits if workers
abandon tools, or perpetuate biases if workers misinterpret AI-based outcomes.
In a workshop with 39 workers from 26 countries specializing in human
resources, labor law, standards creation, and worker training, we explored
questions and ideas they had about safely adopting AI. We held 17 follow-up
interviews to further investigate what skills and training knowledge workers
need to achieve safe and effective AI in practice. We synthesize nine training
topics participants surfaced for knowledge workers related to challenges around
understanding what AI is, misinterpreting outcomes, exacerbating biases, and
worker rights. We reflect how these training topics might be addressed under
different contexts, imagine HCI research prototypes as potential training
tools, and consider ways to ensure training does not perpetuate harmful values.",False,False,"The two papers come from distinct disciplines (optical communication and human-computer interaction) with little overlap. While VLC technology could potentially be used to support AI training, the connection is too indirect to form a novel multidisciplinary research idea that integrates concepts from both papers in a meaningful way.",
2411.05236,1,applied,2411.05236-pos2-6,"Channelrhodopsin-2, a directly light-gated cation-selective membrane channel; Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane α helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination.",2501.0517,"De-centering the (Traditional) User: Multistakeholder Evaluation of
  Recommender Systems; Multistakeholder recommender systems are those that account for the impacts
and preferences of multiple groups of individuals, not just the end users
receiving recommendations. Due to their complexity, evaluating these systems
cannot be restricted to the overall utility of a single stakeholder, as is
often the case of more mainstream recommender system applications. In this
article, we focus our discussion on the intricacies of the evaluation of
multistakeholder recommender systems. We bring attention to the different
aspects involved in the evaluation of multistakeholder recommender systems -
from the range of stakeholders involved (including but not limited to producers
and consumers) to the values and specific goals of each relevant stakeholder.
Additionally, we discuss how to move from theoretical principles to practical
implementation, providing specific use case examples. Finally, we outline open
research directions for the RecSys community to explore. We aim to provide
guidance to researchers and practitioners about how to think about these
complex and domain-dependent issues of evaluation in the course of designing,
developing, and researching applications with multistakeholder aspects.",False,False,"The two papers come from distinct areas of biology and computer science, with Paper 1 focusing on light-gated ion channels and Paper 2 on multistakeholder recommender systems. There is no clear connection or overlap between the concepts presented in the two papers, making it difficult to extract a novel multidisciplinary research idea that meets the standards.",
2411.05236,1,applied,2411.05236-pos2-6,"Channelrhodopsin-2, a directly light-gated cation-selective membrane channel; Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane α helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination.",2501.09275,"MagnetDB: A Longitudinal Torrent Discovery Dataset with IMDb-Matched
  Movies and TV Shows; BitTorrent remains a prominent channel for illicit distribution of
copyrighted material, yet the supply side of such content remains understudied.
We introduce MagnetDB, a longitudinal dataset of torrents discovered through
the BitTorrent DHT between 2018 and 2024, containing more than 28.6 million
torrents and metadata of more than 950 million files. While our primary focus
is on enabling research based on the supply of pirated movies and TV shows, the
dataset also encompasses other legitimate and illegitimate torrents. By
applying IMDb-matching and annotation to movie and TV show torrents, MagnetDB
facilitates detailed analyses of pirated content evolution in the BitTorrent
network. Researchers can leverage MagnetDB to examine distribution trends,
subcultural practices, and the gift economy within piracy ecosystems. Through
its scale and temporal scope, MagnetDB presents a unique opportunity for
investigating the broader dynamics of BitTorrent and advancing empirical
knowledge on digital piracy.",False,False,"The two papers are from vastly different fields, with one focusing on biophysics and optogenetics, and the other on computer science and digital piracy. There is no clear connection or overlap between the two papers, making it difficult to extract a novel multidisciplinary research idea that meets the standards of being feasible, novel, and useful.",
2411.05236,1,applied,2411.05236-pos2-6,"Channelrhodopsin-2, a directly light-gated cation-selective membrane channel; Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane α helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination.",2502.00505,"The Limiting Spectral Distribution of Various Matrix Ensembles Under the
  Anticommutator Operation; Inspired by the quantization of classical quantities and Rankin Selberg
convolution, we study the anticommutator operation $\{\cdot, \cdot\}$, where
$\{A,B\} = AB + BA$, applied to real symmetric random matrix ensembles
including Gaussian orthogonal ensemble (GOE), the palindromic Toeplitz ensemble
(PTE), the $k$-checkerboard ensemble, and the block $k$-circulant ensemble
($k$-BCE). Using combinatorial and topological techniques related to
non-crossing and free matching properties of GOE and PTE, we obtain closed-form
formulae for the moments of the limiting spectral distributions of $\{$GOE,
GOE$\}$, $\{$PTE, PTE$\}$, $\{$GOE, PTE$\}$ and establish the corresponding
limiting spectral distributions with generating functions and convolution. On
the other hand, $\{$GOE, $k$-checkerboard$\}$ and $\{$$k$-checkerboard,
$j$-checkerboard$\}$ exhibit entirely different spectral behavior than the
other anticommutator ensembles: while the spectrum of $\{$GOE,
$k$-checkerboard$\}$ consists of 1 bulk regime of size $\Theta(N)$ and 1 blip
regime of size $\Theta(N^{3/2})$, the spectrum of $\{$$k$-checkerboard,
$j$-checkerboard$\}$ consists of 1 bulk regime of size $\Theta(N)$, 2
intermediary blip regimes of size $\Theta(N^{3/2})$, and 1 largest blip regime
of size $\Theta(N^2)$. In both cases, with the appropriate weight function, we
are able to isolate the largest regime for other regime(s) and analyze its
moments and convergence results via combinatorics. We end with numerical
computation of lower even moments of $\{$GOE, $k$-BCE$\}$ and $\{$$k$-BCE,
$k$-BCE$\}$ based on genus expansion and discussion on the challenge with
analyzing the intermediary blip regimes of $\{$$k$-checkerboard,
$j$-checkerboard$\}$.",False,False,"The two papers are from distinct areas of biology and mathematics, with no apparent connection between the channelrhodopsin-2 protein and the anticommutator operation on matrix ensembles. There is no clear way to combine these concepts to create a novel multidisciplinary research idea that meets the standards of feasibility, novelty, and usefulness.",
2411.05236,1,applied,2411.05236-pos2-6,"Channelrhodopsin-2, a directly light-gated cation-selective membrane channel; Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane α helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination.",2503.08052,"Phase-matching of high harmonic generation in twisted solids; High harmonic generation (HHG) in solids could enable attosecond and
ultraviolet light sources with high compactness, great controllability and rich
functions. However, the HHG process is accompanied by a quite large wavevector
mismatch that is uncompensated by any traditional phase-matching method,
directly limiting its energy conversion efficiency. Here, we propose an
effective strategy for phase-matching of HHG with arbitrary harmonic orders in
solids. Two flakes of solids with an interlayer twist induce a nonlinear
optical phase that depends on the crystal symmetry, twist angle and harmonic
order, which can be accurately designed to compensate for the phase mismatch in
HHG. Guided by the twist-phase-matching theory, we achieved a record-high
conversion efficiency of $~1.5\times10^{-5}$ for the fifth HHG in twisted
hexagonal boron nitride crystals with a total thickness of only 1 ${\mu}m$. Our
work establishes a foundation for developing ultrashort-wavelength and
ultrafast-pulse laser sources in compact solid-state tabletop systems for
fundamental and applied sciences.",False,True,"The combination of optogenetics from Paper 1 and phase-matching of high harmonic generation from Paper 2 can lead to a novel multidisciplinary research idea. For example, using Channelrhodopsin-2 to control and manipulate the phase-matching of high harmonic generation in solids, enabling more efficient and compact ultrashort-wavelength laser sources.",
2411.05236,1,applied,2411.05236-pos2-6,"Channelrhodopsin-2, a directly light-gated cation-selective membrane channel; Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane α helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination.",2503.16222,"Efficient Bayesian Computation Using Plug-and-Play Priors for Poisson
  Inverse Problems; This paper introduces a novel plug-and-play (PnP) Langevin sampling
methodology for Bayesian inference in low-photon Poisson imaging problems, a
challenging class of problems with significant applications in astronomy,
medicine, and biology. PnP Langevin sampling algorithms offer a powerful
framework for Bayesian image restoration, enabling accurate point estimation as
well as advanced inference tasks, including uncertainty quantification and
visualization analyses, and empirical Bayesian inference for automatic model
parameter tuning. However, existing PnP Langevin algorithms are not well-suited
for low-photon Poisson imaging due to high solution uncertainty and poor
regularity properties, such as exploding gradients and non-negativity
constraints. To address these challenges, we propose two strategies for
extending Langevin PnP sampling to Poisson imaging models: (i) an accelerated
PnP Langevin method that incorporates boundary reflections and a Poisson
likelihood approximation and (ii) a mirror sampling algorithm that leverages a
Riemannian geometry to handle the constraints and the poor regularity of the
likelihood without approximations. The effectiveness of these approaches is
demonstrated through extensive numerical experiments and comparisons with
state-of-the-art methods.",False,True,"I can extract concepts from both papers to create a novel multidisciplinary research idea. The idea is to use Channelrhodopsin-2, a light-gated ion channel, as a sensor for low-photon imaging problems, and apply the Bayesian computation method from the second paper to process the data and improve image restoration.",
2411.02815,1,applied,2411.02815-pos1-7,"Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation; In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate.",2411.02815-pos2-7,"Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network; An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images.; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",True,True,"The combination of liver anatomy knowledge from Paper 1 and the automated segmentation technique using 3D convolutional neural networks from Paper 2 can lead to a novel multidisciplinary research idea. By integrating the embryological and anatomical understanding of liver segmentation with advanced image recognition techniques, a more accurate and efficient liver segment segmentation method can be developed, which can be useful for liver surgery and diagnosis.",
2411.02815,1,applied,2411.02815-pos1-7,"Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation; In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate.",2501.18594,"Foundational Models for 3D Point Clouds: A Survey and Outlook; The 3D point cloud representation plays a crucial role in preserving the
geometric fidelity of the physical world, enabling more accurate complex 3D
environments. While humans naturally comprehend the intricate relationships
between objects and variations through a multisensory system, artificial
intelligence (AI) systems have yet to fully replicate this capacity. To bridge
this gap, it becomes essential to incorporate multiple modalities. Models that
can seamlessly integrate and reason across these modalities are known as
foundation models (FMs). The development of FMs for 2D modalities, such as
images and text, has seen significant progress, driven by the abundant
availability of large-scale datasets. However, the 3D domain has lagged due to
the scarcity of labelled data and high computational overheads. In response,
recent research has begun to explore the potential of applying FMs to 3D tasks,
overcoming these challenges by leveraging existing 2D knowledge. Additionally,
language, with its capacity for abstract reasoning and description of the
environment, offers a promising avenue for enhancing 3D understanding through
large pre-trained language models (LLMs). Despite the rapid development and
adoption of FMs for 3D vision tasks in recent years, there remains a gap in
comprehensive and in-depth literature reviews. This article aims to address
this gap by presenting a comprehensive overview of the state-of-the-art methods
that utilize FMs for 3D visual understanding. We start by reviewing various
strategies employed in the building of various 3D FMs. Then we categorize and
summarize use of different FMs for tasks such as perception tasks. Finally, the
article offers insights into future directions for research and development in
this field. To help reader, we have curated list of relevant papers on the
topic: https://github.com/vgthengane/Awesome-FMs-in-3D.",False,True,"The concept of 3D point cloud representation from Paper 2 can be combined with the liver anatomy segmentation from Paper 1 to create a novel multidisciplinary research idea. For example, using 3D point cloud models to analyze and segment liver anatomy, enabling more accurate and personalized liver surgery planning.",
2411.02815,1,applied,2411.02815-pos1-7,"Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation; In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate.",2501.13277,"MEDFORM: A Foundation Model for Contrastive Learning of CT Imaging and
  Clinical Numeric Data in Multi-Cancer Analysis; Computed tomography (CT) and clinical numeric data are essential modalities
for cancer evaluation, but building large-scale multimodal training datasets
for developing medical foundation models remains challenging due to the
structural complexity of multi-slice CT data and high cost of expert
annotation. In this study, we propose MEDFORM, a multimodal pre-training
strategy that guides CT image representation learning using complementary
information from clinical data for medical foundation model development.
MEDFORM efficiently processes CT slice through multiple instance learning (MIL)
and adopts a dual pre-training strategy: first pretraining the CT slice feature
extractor using SimCLR-based self-supervised learning, then aligning CT and
clinical modalities through cross-modal contrastive learning. Our model was
pre-trained on three different cancer types: lung cancer (141,171 slices),
breast cancer (8,100 slices), colorectal cancer (10,393 slices). The
experimental results demonstrated that this dual pre-training strategy improves
cancer classification performance and maintains robust performance in few-shot
learning scenarios. Code available at
https://github.com/DigitalHealthcareLab/25MultiModalFoundationModel.git",False,True,"The concept of liver anatomy segmentation from Paper 1 can be combined with the multimodal pre-training strategy from Paper 2 to develop a novel approach for liver cancer diagnosis. By integrating liver anatomy knowledge with CT imaging and clinical numeric data, a more accurate and robust medical foundation model can be created.",
2411.02815,1,applied,2411.02815-pos1-7,"Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation; In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate.",2501.11097,"Unit Region Encoding: A Unified and Compact Geometry-aware
  Representation for Floorplan Applications; We present the Unit Region Encoding of floorplans, which is a unified and
compact geometry-aware encoding representation for various applications,
ranging from interior space planning, floorplan metric learning to floorplan
generation tasks. The floorplans are represented as the latent encodings on a
set of boundary-adaptive unit region partition based on the clustering of the
proposed geometry-aware density map. The latent encodings are extracted by a
trained network (URE-Net) from the input dense density map and other available
semantic maps. Compared to the over-segmented rasterized images and the
room-level graph structures, our representation can be flexibly adapted to
different applications with the sliced unit regions while achieving higher
accuracy performance and better visual quality. We conduct a variety of
experiments and compare to the state-of-the-art methods on the aforementioned
applications to validate the superiority of our representation, as well as
extensive ablation studies to demonstrate the effect of our slicing choices.",False,False,"The two papers are from distinct disciplines, liver anatomy and computer-aided design, with no apparent connection between them. The concepts presented in the papers, such as liver segmentation and unit region encoding, do not seem to be directly related or combinable to create a novel multidisciplinary research idea.",
2411.02815,1,applied,2411.02815-pos1-7,"Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation; In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate.",2503.09366,"Post-interactive Multimodal Trajectory Prediction for Autonomous Driving; Modeling the interactions among agents for trajectory prediction of
autonomous driving has been challenging due to the inherent uncertainty in
agents' behavior. The interactions involved in the predicted trajectories of
agents, also called post-interactions, have rarely been considered in
trajectory prediction models. To this end, we propose a coarse-to-fine
Transformer for multimodal trajectory prediction, i.e., Pioformer, which
explicitly extracts the post-interaction features to enhance the prediction
accuracy. Specifically, we first build a Coarse Trajectory Network to generate
coarse trajectories based on the observed trajectories and lane segments, in
which the low-order interaction features are extracted with the graph neural
networks. Next, we build a hypergraph neural network-based Trajectory Proposal
Network to generate trajectory proposals, where the high-order interaction
features are learned by the hypergraphs. Finally, the trajectory proposals are
sent to the Proposal Refinement Network for further refinement. The observed
trajectories and trajectory proposals are concatenated together as the inputs
of the Proposal Refinement Network, in which the post-interaction features are
learned by combining the previous interaction features and trajectory
consistency features. Moreover, we propose a three-stage training scheme to
facilitate the learning process. Extensive experiments on the Argoverse 1
dataset demonstrate the superiority of our method. Compared with the baseline
HiVT-64, our model has reduced the prediction errors by 4.4%, 8.4%, 14.4%, 5.7%
regarding metrics minADE6, minFDE6, MR6, and brier-minFDE6, respectively.",False,False,"The two papers are from distinct disciplines (anatomy and computer science) but there is no apparent connection or overlap between the ideas presented. Liver anatomy and autonomous driving trajectory prediction are unrelated, making it difficult to extract a novel multidisciplinary research idea that meets the standards.",
2411.02815,1,applied,2411.02815-pos1-7,"Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation; In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate.",2501.12386,"InternVideo2.5: Empowering Video MLLMs with Long and Rich Context
  Modeling; This paper aims to improve the performance of video multimodal large language
models (MLLM) via long and rich context (LRC) modeling. As a result, we develop
a new version of InternVideo2.5 with a focus on enhancing the original MLLMs'
ability to perceive fine-grained details and capture long-form temporal
structure in videos. Specifically, our approach incorporates dense vision task
annotations into MLLMs using direct preference optimization and develops
compact spatiotemporal representations through adaptive hierarchical token
compression. Experimental results demonstrate this unique design of LRC greatly
improves the results of video MLLM in mainstream video understanding benchmarks
(short & long), enabling the MLLM to memorize significantly longer video inputs
(at least 6x longer than the original), and master specialized vision
capabilities like object tracking and segmentation. Our work highlights the
importance of multimodal context richness (length and fineness) in empowering
MLLM's innate abilites (focus and memory), providing new insights for future
research on video MLLM. Code and models are available at
https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5",False,False,The two papers are from distinct disciplines (anatomy and computer science) but do not share a common thread that can be integrated to form a novel multidisciplinary research idea. The concepts and methods presented in the papers are too disparate to be combined into a feasible and useful research project.,
2411.02815,1,applied,2411.02815-pos2-7,"Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network; An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images.; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",2502.01319,"Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative
  with broad processability and staining-free antibacterial capability; Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free
strategy for broad-spectrum infection control in chronic wounds, minimising
bacterial resistance risks. However, rapid photosensitiser diffusion, tissue
staining, side toxicity, and short-lived antimicrobial effects present
significant clinical limitations for integrating APDT into wound dressings. To
address these challenges, we present the design of a bespoke polyvinyl alcohol
(PVA) derivative conjugated with both phenothiazine and methacrylate
functionalities, enabling staining-free antibacterial photodynamic effects,
cellular tolerability and processability into various wound dressing formats,
including films, textile fibres and nanoscale coatings. Tosylation of PVA is
leveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis
spectroscopy and the minimal release of TB in vitro. UV-induced network
formation is exploited to accomplish cast films and nanoscale integrated wound
dressing coatings. UV curing is also successfully coupled with an in-house wet
spinning process to realise individual, water-insoluble fibres as the building
blocks of fibrous wound dressings. A fluorometric assay supports the generation
of reactive oxygen species when the UV-cured samples are exposed to work, but
not UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and
the complete eradication of P. aeruginosa. Direct and extract cytotoxicity
tests with UV-cured films and fibres demonstrate the viability of L929
fibroblasts following 60-min light irradiation and 72-hour cell culture. The
bespoke molecular architecture, broad processability and cellular tolerability
of this PVA derivative are highly attractive aiming to integrate durable
staining-free photodynamic capability in a wide range of healthcare
technologies, from chronic wound dressings up to minimally invasive localised
therapy.",False,True,"The combination of deep learning-based image segmentation (Paper 1) and photodynamic, UV-curable materials (Paper 2) could lead to the development of smart wound dressings with integrated imaging capabilities for real-time monitoring of wound healing and infection control.",
2411.02815,1,applied,2411.02815-pos2-7,"Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network; An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images.; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",2501.19343,"Effect of a new type of healthy and live food supplement on osteoporosis
  blood parameters and induced rheumatoid arthritis in Wistar rats; Summary Osteoporosis is a skeletal disorder, characterized by a decrease in
bone strength and puts the individual at risk for fracture. On the other hand,
rheumatoid arthritis is a systemic disease of unknown etiology that causes
inflammation of the joints of the organs. Purpose Due to the destructive
effects of these diseases and its increasing prevalence and lack of appropriate
medication for treatment, the present study aimed to evaluate the therapeutic
effect of a new type of healthy and live food supplement on rheumatoid
arthritis and induced osteoporosis in rats. Methods In this research, healthy
and live food powder were synthesized by a new and green route. This organic
biomaterial was named NBS. The NBS food supplement had various vitamins, macro
and micro molecules, and ingredients. The new healthy and nutritious diet
showed that the use of this supplement led to the return of the parameters to
normal levels. Results The concentration of 12.5 mg/ kg showed the least
therapeutic effect and 50 mg/ kg had the highest therapeutic effect for
osteoporosis. The results of blood parameters involved in inflammation in both
healthy and patient groups showed that the use of complete adjuvant induction
causes joint inflammation. In the study of the interaction of the
concentrations, it was observed that the concentration of 50 mg/ kg had the
highest therapeutic effect against the disease in the studied mice. Conclusion
The results showed that the new healthy and viable supplement restores the
blood osteoporotic and rheumatoid factors of the mice to normal.",False,True,"I extracted concepts from both papers to create a novel multidisciplinary research idea. By combining the 3D convolutional neural network (3D-CNN) from Paper 1 with the healthy and live food supplement from Paper 2, a new research project can be proposed to investigate the effect of the supplement on osteoporosis using MRI images analyzed by 3D-CNN, which can provide a more accurate and efficient diagnosis and monitoring of the disease.",
2411.02815,1,applied,2411.02815-pos2-7,"Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network; An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images.; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",2501.08533,"Practical parameter identifiability of respiratory mechanics in the
  extremely preterm infant; The complexity of mathematical models describing respiratory mechanics has
grown in recent years, however, parameter identifiability of such models has
only been studied in the last decade in the context of observable data. This
study investigates parameter identifiability of a nonlinear respiratory
mechanics model tuned to the physiology of an extremely preterm infant, using
global Morris screening, local deterministic sensitivity analysis, and singular
value decomposition-based subset selection. The model predicts airflow and
dynamic pulmonary volumes and pressures under varying levels of continuous
positive airway pressure, and a range of parameters characterizing both
surfactant-treated and surfactant-deficient lung. Sensitivity analyses
indicated eleven parameters influence model outputs over the range of
continuous positive airway pressure and lung health scenarios. The model was
adapted to data from a spontaneously breathing 1 kg infant using gradient-based
optimization to estimate the parameter subset characterizing the patient's
state of health.",False,True,"The two papers introduce concepts from medical imaging and respiratory mechanics, respectively. Combining the ideas, a novel multidisciplinary research project could be developed, such as using deep learning techniques (e.g., 3D-CNN or Transformer) to analyze MRI images of lungs and estimate respiratory mechanics parameters in preterm infants, potentially improving diagnosis and treatment of respiratory diseases.",
2411.02815,1,applied,2411.02815-pos2-7,"Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network; An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images.; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",2501.04504,"Integrating anatomy and electrophysiology in the healthy human heart:
  Insights from biventricular statistical shape analysis using universal
  coordinates; A cardiac digital twin is a virtual replica of a patient-specific heart,
mimicking its anatomy and physiology. A crucial step of building a cardiac
digital twin is anatomical twinning, where the computational mesh of the
digital twin is tailored to the patient-specific cardiac anatomy. In a number
of studies, the effect of anatomical variation on clinically relevant
functional measurements like electrocardiograms (ECGs) is investigated, using
computational simulations. While such a simulation environment provides
researchers with a carefully controlled ground truth, the impact of anatomical
differences on functional measurements in real-world patients remains
understudied. In this study, we develop a biventricular statistical shape model
and use it to quantify the effect of biventricular anatomy on ECG-derived and
demographic features, providing novel insights for the development of digital
twins of cardiac electrophysiology. To this end, a dataset comprising
high-resolution cardiac CT scans from 271 healthy individuals, including
athletes, is utilized. Furthermore, a novel, universal, ventricular
coordinate-based method is developed to establish lightweight shape
correspondence. The performance of the shape model is rigorously established,
focusing on its dimensionality reduction capabilities and the training data
requirements. Additionally, a comprehensive synthetic cohort is made available,
featuring ready-to-use biventricular meshes with fiber structures and
anatomical region annotations. These meshes are well-suited for
electrophysiological simulations.",False,True,"The combination of ideas from both papers can lead to a novel multidisciplinary research project that integrates computer vision and cardiac anatomy to develop an automated system for annotating anatomical regions in cardiac images, potentially improving the accuracy of cardiac digital twin simulations.",
2411.02815,1,applied,2411.02815-pos2-7,"Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network; An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images.; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",2503.12234,"Tumor microenvironment (Part I): Tissue integrity in a rat model of
  peripheral neural cancer; ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are
thought to be a factor in the integrity of nerve tissues. In this report, we
attempted to trace the expression of ICAM-1, responsible for cell-to-cell
adhesion, and of MPZ, the main constituent of myelin sheath, in malignant
tissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells
(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of
the SNs were collected after 7, 14 and 21 days and compared to a sham-operated
group of rats (n = 6 each). Tissues were sectioned and histologically examined,
under light microscope, and stained for measuring the immunoreactivity of
ICAM-1 and MPZ under laser scanning microscope. The cancer model was
established, and the tumor growth was confirmed. ICAM-1 showed severe
decreases, proportional to the growing anaplastic cells, as compared to the
sham group. MPZ revealed, however, a distinct defensive pattern before
substantially decreasing in a comparison with sham. These results support the
notion that malignancies damage peripheral nerves and cause severe axonal
injury and loss of neuronal integrity, and clearly define the role of ICAM-1
and MPZ in safeguarding the nerve tissues.",False,True,The idea of combining a 3D convolutional neural network (3D-CNN) for automatic liver segment segmentation with the understanding of tumor microenvironment and tissue integrity in cancer models can lead to a novel multidisciplinary research project. This project can utilize the 3D-CNN to analyze the impact of cancer on tissue integrity and identify potential biomarkers for early cancer detection.,
2411.02815,1,applied,2411.02815-pos1-7,"Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation; In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate.",2501.02694,"Footprint in fitting $B\to D$ vector form factor and determination for
  $D$-meson leading-twist LCDA; In this paper, we fit the $B\to D$ vector transition form factor (TFF) by
using the data measured by BABAR and Belle Collaborations within Monte Carlo
(MC) method. Meanwhile, the $B\to D$ TFF is also calculated by using the QCD
light-cone sum rules approach (LCSRs) within right-handed chiral current
correlation function. In which, the $D$-meson leading-twist light-cone
distribution amplitude (LCDA) serves as crucial input parameter is
reconstructed with light-cone harmonic oscillator model where its longitudinal
behavior primarily determined by the model-free parameter $B_{2;D}$. After
matching the TFF with two scenarios from MC and LCSRs, we have $B_{2;D}=0.17$.
Then, we present the curve of $D$-meson leading-twist LCDA in comparison with
other theoretical approaches. Subsequently, the $B\to D$ TFF $f_{+}^{BD}(q^2)$
at the large recoil region is $f_{+}^{BD}(0)=0.625^{+0.087}_{-0.113}$, which is
compared in detail with theoretical estimates and experimental measurements.
Furthermore, we calculate the decay width and branching ratio of the
Cabibbo-favored semileptonic decays $B\to D\ell \bar{\nu}_{\ell}$, which lead
to the results $\mathcal{B}(B^0\to D^-\ell ^+\nu _{\ell})
=(1.96_{-0.55}^{+0.51})\times 10^{-2}$ and $\mathcal{B}(B^+\to \bar{D}^0\ell
^+\nu _{\ell}) =(2.12_{-0.59}^{+0.55})\times 10^{-2}$. Finally, we predict the
CKM matrix element with two scenarios $|V_{cb}|_{\rm
SR}=42.97_{-2.57}^{+2.42}\times 10^{-3}$ and $|V_{cb} |_{\rm
MC}=42.82_{-1.29}^{+1.07}\times 10^{-3}$ from $B^0\to D^-\ell^+\nu_{\ell}$,
$|V_{cb}|_{\rm SR}=41.93_{-1.05}^{+1.03}\times 10^{-3}$ and $|V_{cb} |_{\rm
MC}=41.82_{-0.25}^{+0.23}\times 10^{-3}$ from $B^+\to
\bar{D}^0\ell^+\nu_{\ell}$ which are in good agreement with theoretical and
experimental predictions.",False,False,"The two papers are from distinct fields of anatomy and particle physics, making it challenging to extract concepts that can be combined to create a novel multidisciplinary research idea. The topics seem unrelated, and there is no apparent overlap or intersection between liver anatomy and B-meson decays.",
2411.02815,1,applied,2411.02815-pos1-7,"Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation; In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate.",2503.0938,"Infinite series identities involving $\pi$ and $\ln 2$; We prove identities for six infinite series whose values involve linear
combinations of $\pi$ and $\ln 2$, that do not appear in standard infinite
series references.",False,False,"The two papers come from distinct areas of research, one from anatomy and surgery, and the other from mathematics. However, the concepts and ideas presented in both papers do not seem to overlap or complement each other in a way that could lead to a novel multidisciplinary research idea.",
2411.02815,1,applied,2411.02815-pos1-7,"Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation; In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate.",2502.03451,"Contextuality with Pauli observables in cycle scenarios; Contextuality is a fundamental marker of quantum non-classicality, which has
emerged as a key resource for quantum computational advantage in multiple
settings. Many such results hinge specifically on contextuality witnessed
through Pauli measurements. In this work, we initiate a systematic study of
(state-dependent) Pauli contextuality, focusing on cycle measurement scenarios,
the simplest scenarios capable of exhibiting contextual behaviour. First, we
study realizability of cycle scenarios with multi-qubit Pauli observables: we
show that the maximum size of a cycle faithfully realizable by $m$-qubit Paulis
is upper bounded by $3m$, while we construct explicit realizations of cycles of
size $2m-1$ or $2m$, depending on whether $m \not\equiv 1 \pmod{3}$ or $m
\equiv 1 \pmod{3}$. Then, we investigate the presence of contextuality: we
prove that no $n$-cycle Pauli realization for $n > 4$ can witness contextuality
(on any quantum state), whereas for $n = 4$ every Pauli realization exhibits
contextuality, attaining the quantum bound for all noncontextuality
inequalities on some pure state. Finally, we discuss arbitrary Pauli scenarios
in light of Vorob'ev's theorem, and show that, contrary to what our cycle
characterization might suggest, the presence of $4$-cycles is not necessary for
witnessing contextuality in general Pauli scenarios.",False,False,"The two papers come from distinct areas of liver anatomy and quantum physics, making it challenging to extract a novel multidisciplinary research idea that meets the standards of interdisciplinary research. The concepts and methods presented in the papers seem too disparate to be integrated into a feasible and useful research project.",
2411.02815,1,applied,2411.02815-pos1-7,"Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation; In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate.",2502.17304,"Child vs. machine language learning: Can the logical structure of human
  language unleash LLMs?; We argue that human language learning proceeds in a manner that is different
in nature from current approaches to training LLMs, predicting a difference in
learning biases. We then present evidence from German plural formation by LLMs
that confirm our hypothesis that even very powerful implementations produce
results that miss aspects of the logic inherent to language that humans have no
problem with. We conclude that attention to the different structures of human
language and artificial neural networks is likely to be an avenue to improve
LLM performance.",False,False,"Although both papers introduce distinct ideas from different disciplines, liver anatomy and machine language learning, the concepts presented in the papers do not seem to overlap or complement each other in a way that could lead to a novel interdisciplinary research idea.",
2411.02815,1,applied,2411.02815-pos1-7,"Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation; In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate.",2501.09743,"Improvement of Data Analytics Techniques in Reflection High Energy
  Electron Diffraction to Enable Machine Learning; Perovskite oxides such as LaFeO$_3$ are a well-studied family of materials
that possess a wide range of useful and novel properties. Successfully
synthesizing perovskite oxide samples usually requires a significant number of
growth attempts and a detailed film characterization on each sample to find the
optimal growth window of a material. The most common real-time \textit{in situ}
diagnostic technique available during molecular beam epitaxy (MBE) synthesis is
reflection high-energy electron diffraction (RHEED). Conventional use of RHEED
allows a highly experienced operator to determine growth rate by monitoring
intensity osciallations and make some qualitative observations during growth,
such as recognizing the sample has become amorphous or recognizing that large
islands have formed on the surface. However, due to a lack of theoretical
understanding of the diffraction patterns, finer, more precise levels of
observations are challenging. To address these limitations, we implement new
data analytics techniques in the growth of three LaFeO$_3$ samples on Nb-doped
SrTiO$_3$ by MBE. These techniques improve our ability to perform unsupervised
machine learning using principal component analysis (PCA) and k-means
clustering by using drift correction to overcome sample or stage motion during
growth and intensity transformations that highlight more subtle features in the
images such as Kikuchi bands. With this approach, we enable the first
demonstration of PCA and k-means across multiple samples, allowing for
quantitative comparison of RHEED videos for two LaFeO$_3$ film samples. These
capabilities set the stage for real-time processing of RHEED data during growth
to enable machine learning-accelerated film synthesis.",False,False,"The two papers are from distinct disciplines (anatomy and materials science) with no apparent connection. The concepts and techniques presented in each paper do not lend themselves to integration or combination to form a novel multidisciplinary research idea that meets the standards of interdisciplinarity, feasibility, novelty, and usefulness.",
2411.02815,1,applied,2411.02815-pos2-7,"Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network; An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images.; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",2502.01558,"Search-Based Adversarial Estimates for Improving Sample Efficiency in
  Off-Policy Reinforcement Learning; Sample inefficiency is a long-lasting challenge in deep reinforcement
learning (DRL). Despite dramatic improvements have been made, the problem is
far from being solved and is especially challenging in environments with sparse
or delayed rewards. In our work, we propose to use Adversarial Estimates as a
new, simple and efficient approach to mitigate this problem for a class of
feedback-based DRL algorithms. Our approach leverages latent similarity search
from a small set of human-collected trajectories to boost learning, using only
five minutes of human-recorded experience. The results of our study show
algorithms trained with Adversarial Estimates converge faster than their
original version. Moreover, we discuss how our approach could enable learning
in feedback-based algorithms in extreme scenarios with very sparse rewards.",False,True,"I extracted concepts from both papers to create a novel multidisciplinary research idea: using transformer-based architectures for image recognition to improve the sample efficiency in off-policy reinforcement learning. This idea combines computer vision and reinforcement learning to tackle the challenge of sample inefficiency in DRL, meeting all the standards.",
2411.02815,1,applied,2411.02815-pos2-7,"Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network; An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images.; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",2503.00312,"Classical and quantum cosmology of $f(R)$ gravity's rainbow in Schutz's
  formalism; We investigate the classical and quantum dynamics of $f(R)$ gravity's rainbow
in the presence of a perfect fluid, employing Schutz's formalism to establish a
well-defined notion of time. In the classical regime, we derive and solve the
equations of motion, obtaining both analytical and numerical solutions. Through
canonical quantisation, we formulate the Schr\""{o}dinger-Wheeler-DeWitt (SWD)
equation for the quantum model. By solving its eigenfunctions, we construct the
wave function of the Universe and obtain analytical solutions in scenarios
dominated by stiff matter. Our results highlight the impact of rainbow gravity
on quantum evolution, particularly in modifying the structure of the wave
function and shaping the transition from the quantum to the classical regime.",False,False,"The two papers are from distinct areas, one from medical image processing and computer vision, and the other from theoretical physics and cosmology. There is no clear connection or overlap between the two papers, making it challenging to extract concepts to create a novel multidisciplinary research idea that meets the standards of interdisciplinary research.",
2411.02815,1,applied,2411.02815-pos2-7,"Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network; An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images.; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",2502.16215,"The influence of Wilson lines on heavy quark anti-quark potential and
  mass; The holographic heavy quark potential is investigated via holographic Wilson
loops in the AdS soliton with gauge potential. We analyze two types of
holographic Wilson loops. {In the first type, holographic heavy quark potential
shows the area law behavior. In the second type, the potential becomes zero at
a critical length and physics analogous to the dissociation occurs. The mass of
heavy quarkonia and the binding energy are examined.} Lastly, the mass of
$0^{++}$ glueball-like operators dual to massless dilaton is calculated. The
mass of $0^{++}$ glueball-like operator decreases with increase of the gauge
potential as expected in arXiv:2309.03491 [hep-th]. The results are comparable
with lattice QCD.",False,False,"Although both papers introduce innovative ideas in their respective fields, combining them does not yield a feasible interdisciplinary research project. The first paper focuses on image recognition and medical imaging, while the second paper explores theoretical physics and holographic Wilson loops, making it challenging to find a meaningful connection between the two.",
2411.02815,1,applied,2411.02815-pos2-7,"Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network; An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images.; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",2503.16005,"Left Jacobson Rings; We say that a ring is strongly (resp. weakly) left Jacobson if every
semiprime (resp. prime) left ideal is an intersection of maximal left ideals.
There exist Jacobson rings that are not weakly left Jacobson, e.g. the Weyl
algebra. Our main result is the following one-sided noncommutative
Nullstellensatz: For any finite-dimensional F-algebra A the ring
A[$x_1$,...,$x_n$] of polynomials with coefficients in A is strongly left
Jacobson and every maximal left ideal of A[$x_1$,...,$x_n$] has a special form.
We also prove that an algebra that is a finitely generated module over its
center is weakly left Jacobson iff it is Jacobson, and that an Azumaya algebra
is strongly left Jacobson iff its center is Jacobson.",False,False,"The two papers come from distinct areas of research, one in medical imaging and computer vision, and the other in abstract algebra. While they are both innovative in their respective fields, there is no clear connection or overlap between the ideas presented, making it difficult to extract a novel multidisciplinary research idea that meets the standards of interdisciplinary research.",
2411.02815,1,applied,2411.02815-pos2-7,"Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network; An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images.; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",2501.03649,"On the Locality of Hall's Theorem; The last five years of research on distributed graph algorithms have seen
huge leaps of progress, both regarding algorithmic improvements and
impossibility results: new strong lower bounds have emerged for many central
problems and exponential improvements over the state of the art have been
achieved for the runtimes of many algorithms. Nevertheless, there are still
large gaps between the best known upper and lower bounds for many important
problems. The current lower bound techniques for deterministic algorithms are
often tailored to obtaining a logarithmic bound and essentially cannot be used
to prove lower bounds beyond $\Omega(\log n)$. In contrast, the best
deterministic upper bounds are often polylogarithmic, raising the fundamental
question of how to resolve the gap between logarithmic lower and
polylogarithmic upper bounds and finally obtain tight bounds. We develop a
novel algorithm design technique aimed at closing this gap. In essence, each
node finds a carefully chosen local solution in $O(\log n)$ rounds and we
guarantee that this solution is consistent with the other nodes' solutions
without coordination. The local solutions are based on a distributed version of
Hall's theorem that may be of independent interest and motivates the title of
this work. We showcase our framework by improving on the state of the art for
the following fundamental problems: edge coloring, bipartite saturating
matchings and hypergraph sinkless orientation. In particular, we obtain an
asymptotically optimal $O(\log n)$-round algorithm for $3\Delta/2$-edge
coloring in bounded degree graphs. The previously best bound for the problem
was $O(\log^4 n)$ rounds, obtained by plugging in the state-of-the-art maximal
independent set algorithm from arXiv:2303.16043 into the $3\Delta/2$-edge
coloring algorithm from arXiv:1711.05469 .",False,False,"Although both papers introduce innovative ideas in their respective fields, they are quite distant in terms of their research domains. Paper 1 focuses on image recognition and segmentation using deep learning, while Paper 2 deals with distributed graph algorithms and lower bound techniques. The intersection of these two areas is not apparent, making it challenging to extract a novel multidisciplinary research idea that meets the standards of being interdisciplinary, feasible, novel, and useful.",
2411.00561,1,applied,2411.00561-pos1-8,"What is a cell type, really? The quest to categorize life’s myriad forms; The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. “I realized that there were probably hundreds of flavours,” recalls Buenrostro, who was a graduate student at Stanford University in California at the time.",2411.00561-pos2-8,"Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study; In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric.",True,True,The concept of categorizing cell types from Paper 1 can be combined with the shape-based object classification technique from Paper 2 to develop a novel method for classifying cell types based on their nuclear shape and DNA arrangement. This interdisciplinary approach could lead to a more accurate and efficient way of identifying cell types.,
2411.00561,1,applied,2411.00561-pos1-8,"What is a cell type, really? The quest to categorize life’s myriad forms; The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. “I realized that there were probably hundreds of flavours,” recalls Buenrostro, who was a graduate student at Stanford University in California at the time.",2503.13879,"Bridging Social Psychology and LLM Reasoning: Conflict-Aware Meta-Review
  Generation via Cognitive Alignment; The rapid growth of scholarly submissions has overwhelmed traditional peer
review systems, driving the need for intelligent automation to preserve
scientific rigor. While large language models (LLMs) show promise in automating
manuscript critiques, their ability to synthesize high-stakes meta-reviews,
which require conflict-aware reasoning and consensus derivation, remains
underdeveloped. Existing methods fail to effectively handle conflicting
viewpoints within differing opinions, and often introduce additional cognitive
biases, such as anchoring effects and conformity bias.To overcome these
limitations, we propose the Cognitive Alignment Framework (CAF), a dual-process
architecture that transforms LLMs into adaptive scientific arbitrators. By
operationalizing Kahneman's dual-process theory, CAF introduces a three-step
cognitive pipeline: review initialization, incremental integration, and
cognitive alignment.Empirical validation shows that CAF outperforms existing
LLM-based methods, with sentiment consistency gains reaching up to 19.47\% and
content consistency improving by as much as 12.95\%.",False,False,"Although both papers introduce innovative ideas, they are from distinct disciplines (biology and computer science/social psychology) and do not share a common problem or theme that can be integrated to form a novel multidisciplinary research idea.",
2411.00561,1,applied,2411.00561-pos1-8,"What is a cell type, really? The quest to categorize life’s myriad forms; The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. “I realized that there were probably hundreds of flavours,” recalls Buenrostro, who was a graduate student at Stanford University in California at the time.",2503.02221,"Attention Bootstrapping for Multi-Modal Test-Time Adaptation; Test-time adaptation aims to adapt a well-trained model to potential
distribution shifts at test time using only unlabeled test data, without access
to the original training data. While previous efforts mainly focus on a single
modality, test-time distribution shift in the multi-modal setting is more
complex and calls for new solutions. This paper tackles the problem of
multi-modal test-time adaptation by proposing a novel method named Attention
Bootstrapping with Principal Entropy Minimization (ABPEM). We observe that
test-time distribution shift causes misalignment across modalities, leading to
a large gap between intra-modality discrepancies (measured by self-attention)
and inter-modality discrepancies (measured by cross-attention). We name this
the attention gap. This attention gap widens with more severe distribution
shifts, hindering effective modality fusion. To mitigate this attention gap and
encourage better modality fusion, we propose attention bootstrapping that
promotes cross-attention with the guidance of self-attention. Moreover, to
reduce the gradient noise in the commonly-used entropy minimization, we adopt
principal entropy minimization, a refinement of entropy minimization that
reduces gradient noise by focusing on the principal parts of entropy, excluding
less reliable gradient information. Extensive experiments on the benchmarks
validate the effectiveness of the proposed ABPEM in comparison with competing
baselines.",False,False,"Although the two papers come from distinct disciplines, biology and computer science, the ideas presented in the papers are not directly related, and there is no clear connection between cell type categorization and multi-modal test-time adaptation. Combining these ideas would require a significant stretch, and it is unclear what novel insights or solutions would emerge from their integration.",
2411.00561,1,applied,2411.00561-pos1-8,"What is a cell type, really? The quest to categorize life’s myriad forms; The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. “I realized that there were probably hundreds of flavours,” recalls Buenrostro, who was a graduate student at Stanford University in California at the time.",2501.06682,"Generative AI in Education: From Foundational Insights to the Socratic
  Playground for Learning; This paper explores the synergy between human cognition and Large Language
Models (LLMs), highlighting how generative AI can drive personalized learning
at scale. We discuss parallels between LLMs and human cognition, emphasizing
both the promise and new perspectives on integrating AI systems into education.
After examining challenges in aligning technology with pedagogy, we review
AutoTutor-one of the earliest Intelligent Tutoring Systems (ITS)-and detail its
successes, limitations, and unfulfilled aspirations. We then introduce the
Socratic Playground, a next-generation ITS that uses advanced transformer-based
models to overcome AutoTutor's constraints and provide personalized, adaptive
tutoring. To illustrate its evolving capabilities, we present a JSON-based
tutoring prompt that systematically guides learner reflection while tracking
misconceptions. Throughout, we underscore the importance of placing pedagogy at
the forefront, ensuring that technology's power is harnessed to enhance
teaching and learning rather than overshadow it.",False,True,"By combining the concepts of cell type categorization from Paper 1 and generative AI in education from Paper 2, a novel research idea emerges: using AI-powered tools to develop personalized learning systems for cell biology education, where AI-driven models can help students understand and categorize complex cell types.",
2411.00561,1,applied,2411.00561-pos1-8,"What is a cell type, really? The quest to categorize life’s myriad forms; The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. “I realized that there were probably hundreds of flavours,” recalls Buenrostro, who was a graduate student at Stanford University in California at the time.",2501.18299,"Model-Free RL Agents Demonstrate System 1-Like Intentionality; This paper argues that model-free reinforcement learning (RL) agents, while
lacking explicit planning mechanisms, exhibit behaviours that can be analogised
to System 1 (""thinking fast"") processes in human cognition. Unlike model-based
RL agents, which operate akin to System 2 (""thinking slow"") reasoning by
leveraging internal representations for planning, model-free agents react to
environmental stimuli without anticipatory modelling. We propose a novel
framework linking the dichotomy of System 1 and System 2 to the distinction
between model-free and model-based RL. This framing challenges the prevailing
assumption that intentionality and purposeful behaviour require planning,
suggesting instead that intentionality can manifest in the structured, reactive
behaviours of model-free agents. By drawing on interdisciplinary insights from
cognitive psychology, legal theory, and experimental jurisprudence, we explore
the implications of this perspective for attributing responsibility and
ensuring AI safety. These insights advocate for a broader, contextually
informed interpretation of intentionality in RL systems, with implications for
their ethical deployment and regulation.",False,False,"The two papers are from distinct areas of biology and artificial intelligence, but there is no clear connection between the concepts presented. The first paper discusses cell type categorization, while the second paper explores model-free reinforcement learning and intentionality. There is no obvious overlap or intersection that could be leveraged to create a novel multidisciplinary research idea.",
2411.00561,1,applied,2411.00561-pos1-8,"What is a cell type, really? The quest to categorize life’s myriad forms; The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. “I realized that there were probably hundreds of flavours,” recalls Buenrostro, who was a graduate student at Stanford University in California at the time.",2503.02976,"Teaching AI to Handle Exceptions: Supervised Fine-Tuning with
  Human-Aligned Judgment; Large language models (LLMs), initially developed for generative AI, are now
evolving into agentic AI systems, which make decisions in complex, real-world
contexts. Unfortunately, while their generative capabilities are
well-documented, their decision-making processes remain poorly understood. This
is particularly evident when models are handling exceptions, a critical and
challenging aspect of decision-making made relevant by the inherent
incompleteness of contracts. Here we demonstrate that LLMs, even ones that
excel at reasoning, deviate significantly from human judgments because they
adhere strictly to policies, even when such adherence is impractical,
suboptimal, or even counterproductive. We then evaluate three approaches to
tuning AI agents to handle exceptions: ethical framework prompting,
chain-of-thought reasoning, and supervised fine-tuning. We find that while
ethical framework prompting fails and chain-of-thought prompting provides only
slight improvements, supervised fine-tuning, specifically with human
explanations, yields markedly better results. Surprisingly, in our experiments,
supervised fine-tuning even enabled models to generalize human-like
decision-making to novel scenarios, demonstrating transfer learning of
human-aligned decision-making across contexts. Furthermore, fine-tuning with
explanations, not just labels, was critical for alignment, suggesting that
aligning LLMs with human judgment requires explicit training on how decisions
are made, not just which decisions are made. These findings highlight the need
to address LLMs' shortcomings in handling exceptions in order to guide the
development of agentic AI toward models that can effectively align with human
judgment and simultaneously adapt to novel contexts.",False,True,"I extracted concepts from both papers to create a novel multidisciplinary research idea: using AI to categorize cell types based on their unique characteristics, such as DNA packaging. By fine-tuning AI models with human explanations, researchers can develop a more accurate and adaptive system for cell type classification, advancing our understanding of cellular biology.",
2411.00561,1,applied,2411.00561-pos2-8,"Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study; In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric.",2503.042,"DeepSilencer: A Novel Deep Learning Model for Predicting siRNA Knockdown
  Efficiency; Background: Small interfering RNA (siRNA) is a promising therapeutic agent
due to its ability to silence disease-related genes via RNA interference. While
traditional machine learning and early deep learning methods have made progress
in predicting siRNA efficacy, there remains significant room for improvement.
Advanced deep learning techniques can enhance prediction accuracy, reducing the
reliance on extensive wet-lab experiments and accelerating the identification
of effective siRNA sequences. This approach also provides deeper insights into
the mechanisms of siRNA efficacy, facilitating more targeted and efficient
therapeutic strategies.
  Methods: We introduce DeepSilencer, an innovative deep learning model
designed to predict siRNA knockdown efficiency. DeepSilencer utilizes advanced
neural network architectures to capture the complex features of siRNA
sequences. Our key contributions include a specially designed deep learning
model, an innovative online data sampling method, and an improved loss function
tailored for siRNA prediction. These enhancements collectively boost the
model's prediction accuracy and robustness.
  Results: Extensive evaluations on multiple test sets demonstrate that
DeepSilencer achieves state-of-the-art performance using only siRNA sequences
and basic physicochemical properties. Our model surpasses several other methods
and shows superior predictive performance, particularly when incorporating
thermodynamic parameters.
  Conclusion: The advancements in data sampling, model design, and loss
function significantly enhance the predictive capabilities of DeepSilencer.
These improvements underscore its potential to advance RNAi therapeutic design
and development, offering a powerful tool for researchers and clinicians.",False,True,"By combining shape-based object classification techniques from Paper 1 with the deep learning model for predicting siRNA knockdown efficiency from Paper 2, a novel multidisciplinary research idea can be formed. For example, using shape-based descriptors to analyze the 3D structure of siRNA molecules and predicting their efficacy using a deep learning model.",
2411.00561,1,applied,2411.00561-pos2-8,"Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study; In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric.",2503.00165,"Leveraging Sequence Purification for Accurate Prediction of Multiple
  Conformational States with AlphaFold2; AlphaFold2 (AF2) has transformed protein structure prediction by harnessing
co-evolutionary constraints embedded in multiple sequence alignments (MSAs).
MSAs not only encode static structural information, but also hold critical
details about protein dynamics, which underpin biological functions. However,
these subtle co-evolutionary signatures, which dictate conformational state
preferences, are often obscured by noise within MSA data and thus remain
challenging to decipher. Here, we introduce AF-ClaSeq, a systematic framework
that isolates these co-evolutionary signals through sequence purification and
iterative enrichment. By extracting sequence subsets that preferentially encode
distinct structural states, AF-ClaSeq enables high-confidence predictions of
alternative conformations. Our findings reveal that the successful sampling of
alternative states depends not on MSA depth but on sequence purity.
Intriguingly, purified sequences encoding specific structural states are
distributed across phylogenetic clades and superfamilies, rather than confined
to specific lineages. Expanding upon AF2's transformative capabilities,
AF-ClaSeq provides a powerful approach for uncovering hidden structural
plasticity, advancing allosteric protein and drug design, and facilitating
dynamics-based protein function annotation.",False,True,"The combination of shape-based object classification from Paper 1 and protein structure prediction from Paper 2 can lead to a novel multidisciplinary research idea. For instance, applying Fourier or wavelet-Fourier descriptors to classify protein shapes and predict their conformational states, leveraging sequence purification techniques to improve accuracy.",
2411.00561,1,applied,2411.00561-pos2-8,"Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study; In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric.",2502.06107,"An Evaluation on the Role of Non-Coding RNA in HIV Transcription and
  Latency: A Review; The existence of latent cellular reservoirs is recognized as the major
barrier to an HIV cure. Reactivating and eliminating ""shock and kill"" or
permanently silencing ""block and lock"" the latent HIV reservoir, as well as
gene editing, remain promising approaches, but so far have proven to be only
partially successful. Moreover, using latency reversing agents or ""block and
lock"" drugs pose additional considerations, including the ability to cause
cellular toxicity, a potential lack of specificity for HIV, or low potency when
each agent is used alone. RNA molecules, such as microRNAs (miRNAs) and long
non-coding RNAs (lncRNAs) are becoming increasingly recognized as important
regulators of gene expression. RNA-based approaches for combatting HIV latency
represent a promising strategy since both miRNAs and lncRNAs are more cell-type
and tissue specific than protein coding genes. Thus, a higher specificity of
targeting the latent HIV reservoir with less overall cellular toxicity can
likely be achieved. In this review, we summarize current knowledge about HIV
gene expression regulation by miRNAs and lncRNAs encoded in the human genome,
as well as regulatory molecules encoded in the HIV genome. We discuss both the
transcriptional and post-transcriptional regulation of HIV gene expression to
align with the current definition of latency, and describe RNA molecules that
either promote HIV latency or have anti-latency properties. Finally, we provide
perspectives on using each class of RNAs as potential targets for combatting
HIV latency, and describe the complexity of the interactions between different
RNA molecules, their protein targets, and HIV.",False,False,"Although both papers introduce innovative ideas from distinct disciplines, combining them does not immediately suggest a novel multidisciplinary research idea. The first paper focuses on shape-based object retrieval and classification using mathematical techniques, while the second paper explores the role of non-coding RNA in HIV transcription and latency, a topic in molecular biology. There is no clear connection between the two papers that would enable the creation of a feasible, novel, and useful interdisciplinary research idea.",
2411.00561,1,applied,2411.00561-pos2-8,"Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study; In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric.",2503.06123,"Mechanism of Electricacupuncture Treating Detrusor Bladder Neck
  Dyscoordination After Suprasacral Spinal Cord Injury by Proteomics; Objectives This study aimed to elucidate the potential mechanisms of
electroacupuncture (EA) in restoring detrusor-bladder neck dyssynergesia (DBND)
following suprasacral spinal cord injury.
  Methods A total of 52 adult female Sprague-Dawley rats were randomly assigned
to either a sham group (n=12) or a spinal cord injury model group (n=40). In
the model group, DBND was induced in 40 rats through Hassan Shaker spinal cord
transection, with 24 rats surviving spinal shock and subsequently randomized
into two groups: a model-only group (DBND, n=12) and an EA intervention group
(DBND+EA, n=12). DBND+EA was administered at Ciliao (BL32), Zhongji (RN3), and
Sanyinjiao (SP6) acupoints, for 20 minutes per session, once daily for 10
consecutive days. On day 29 post-injury, all rats underwent urodynamic
assessments, followed by hematoxylin and eosin (HE) staining, tandem mass tag
(TMT) proteomics, and Western blot (WB) analysis of the detrusor and bladder
neck tissues.
  Results Urodynamic evaluation demonstrated that EA intervention enhanced
bladder function in DBND rats. HE staining indicated reduced fibroplasia in the
detrusor muscle and alleviated inflammation in the bladder neck following EA.
TMT proteomic analysis revealed 30 differentially expressed proteins (DEPs) in
the detrusor and 59 DEPs in the bladder neck post-EA treatment. WB results
corroborated these TMT findings.
  Conclusion EA effectively promotes synergy between the detrusor muscle and
bladder neck in DBND, likely by enhancing detrusor contractility and
facilitating bladder neck relaxation during urination. This study provides
mechanistic insights into the therapeutic role of EA in managing DBND.",False,False,"The two papers come from distinct areas, one from computer science and the other from biomedical research. However, there is no apparent connection or overlap between the ideas presented in the two papers, making it difficult to extract a novel multidisciplinary research idea that meets the standards of interdisciplinary research.",
2411.00561,1,applied,2411.00561-pos2-8,"Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study; In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric.",2503.08115,"The standard coil or globule phases cannot describe the denatured state
  of structured proteins and intrinsically disordered proteins; The concepts of globule and random coil were developed to describe the phases
of homopolymers and then used to characterize the denatured state of structured
cytosolic proteins and intrinsically disordered proteins. Using multi-scale
molecular dynamics simulations, we were able to explore the conformational
space of the disordered conformations of both types of protein under biological
conditions in an affordable amount of computational time. By studying the size
of the protein and the density correlations in space, we conclude that the
standard phases of homopolymers and the tools to detect them cannot be applied
straightforwardly to proteins.",False,True,"Combining shape-based object retrieval techniques from Paper 1 with protein structure analysis from Paper 2 could lead to novel methods for protein structure classification and retrieval. For instance, using Fourier or wavelet-Fourier descriptors to analyze protein shapes and predict their functional properties, overcoming limitations of traditional coil or globule phases.",
2411.00561,1,applied,2411.00561-pos1-8,"What is a cell type, really? The quest to categorize life’s myriad forms; The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. “I realized that there were probably hundreds of flavours,” recalls Buenrostro, who was a graduate student at Stanford University in California at the time.",2502.1988,"Non-interacting holographic dark energy with Torsion via Hubble Radius; We reconstruct a holographic dark energy model with a scalar torsion $\phi$
under the assumption of no interaction between dark energy and dark matter. We
show that the accelerating expansion can occur by setting the IR cut-off to the
Hubble radius as $L=H^{-1}$ with the Hubble parameter $H$. Motivated by two
physical choices based on Friedmann equations and the holographic principle,
tellingly, spin-induced torsion at non-zero matter density $\rho_m \neq 0$ and
$H$-dependence of torsion in empty space $\rho_m =0$, we propose a
time-dependent scalar torsion $\phi(t) = k H(t) ({\rho_{m}(t)}/ \rho_c^0)$ with
a dimensionless constant $k$ and the current critical density $\rho_c^0$. As a
result, we find minima $(\omega_X^{0})_{min}$ of the current equation of state
for dark energy for various values of the free parameter $c$, yielding $-1 <
(\omega_X^{0})_{min} < -0.778$ as $c$ changes from $1$ to $0.654$. Also, we get
the values of the current dimensionless ratio, $(\phi_0/H_{0})_{min}$
corresponding to the minima $(\omega^{0}_{X})_{min}$, which is consistent with
weak torsion assumption $|\phi_0| /H_{0} < 1$, and thus it ensuring model
viability.",False,False,"The two papers are from distinct areas, cell biology and cosmology, with no apparent connection. The concepts presented in the papers, such as cell type categorization and holographic dark energy with torsion, do not lend themselves to a straightforward integration.",
2411.00561,1,applied,2411.00561-pos1-8,"What is a cell type, really? The quest to categorize life’s myriad forms; The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. “I realized that there were probably hundreds of flavours,” recalls Buenrostro, who was a graduate student at Stanford University in California at the time.",2503.12364,"The Beltrami -- de Sitter model; We combine the well -- known Beltrami -- Klein model of non -- Euclidean
geometry on a two -- dimensional disk, where the geodesics are the chords of
the disk, with the two -- dimensional de Sitter space. The geometry of the de
Sitter space is defined on the complement of the Beltrami -- Klein disk in the
plane, with the de Sitter metric being the unique Lorentzian Einstein metric
whose light cones form cones tangent to the disk in this complement. This leads
to a Beltrami -- de Sitter model on the plane ${\bf R}^2$, which is endowed
with the Riemannian Beltrami metric on the disk and the Lorentzian de Sitter
metric outside the disk in ${\bf R}^2$.
  We explore the relevance of this model for Penrose's Conformal Cyclic
Cosmology, first in the two -- dimensional setting and subsequently in higher
dimensions, including the physically significant case of four dimensions. In
this context, we define a Radon-like transform between the de Sitter and
Beltrami spaces, facilitating the purely geometric transformation of physical
fields from the Lorentzian de Sitter space to the Riemannian Beltrami space.
  In the two -- dimensional case, we also uncover a hidden ${\bf G}_2$ symmetry
associated with the two -- dimensional de Sitter space, which is related to a
vector distribution naturally defined by the geometry of the model. We suggest
the potential for discovering similar hidden symmetries in the $n$ --
dimensional Beltrami-de Sitter model.",False,False,"The two papers come from distinct disciplines, biology and physics/mathematics, but there is no clear connection or overlap between the concepts presented. The first paper discusses cell type categorization, while the second paper explores non-Euclidean geometry and cosmology, making it challenging to extract a novel, feasible, and useful interdisciplinary research idea.",
2411.00561,1,applied,2411.00561-pos1-8,"What is a cell type, really? The quest to categorize life’s myriad forms; The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. “I realized that there were probably hundreds of flavours,” recalls Buenrostro, who was a graduate student at Stanford University in California at the time.",2502.13786,"Uncertainty Principle, annihilating pairs and Fourier restriction; Let $G$ be a locally compact abelian group, and let $\widehat{G}$ denote its
dual group, equipped with a Haar measure. A variant of the uncertainty
principle states that for any $S \subset G$ and $\Sigma \subset \widehat{G}$,
there exists a constant $C(S, \Sigma)$ such that for any $f \in L^2(G)$, the
following inequality holds: \[\|f\|_{L^2(G)} \leq C(S, \Sigma) \bigl(
\|f\|_{L^2(G \setminus S)} + \|\widehat{f}\|_{L^2(\widehat{G} \setminus
\Sigma)} \bigr),\] where $\widehat{f}$ denotes the Fourier transform of $f$.
This variant of the uncertainty principle is particularly useful in
applications such as signal processing and control theory.The purpose of this
paper is to show that such estimates can be strengthened when $S$ or $\Sigma$
satisfies a restriction theorem and to provide an estimate for the constant
$C(S, \Sigma)$. This result serves as a quantitative counterpart to a recent
finding by the first and last author. In the setting of finite groups, the
results also extend those of Matolcsi-Sz\""ucs and Donoho-Stark.",False,False,"The two papers are from distinct disciplines, but the concepts presented are too disparate to be integrated into a novel multidisciplinary research idea. The first paper deals with cell biology and genomics, while the second paper focuses on mathematical concepts in signal processing and control theory, with no clear connection or overlap.",
2411.00561,1,applied,2411.00561-pos1-8,"What is a cell type, really? The quest to categorize life’s myriad forms; The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. “I realized that there were probably hundreds of flavours,” recalls Buenrostro, who was a graduate student at Stanford University in California at the time.",2501.09788,"Large-Range Tuning and Stabilization of the Optical Transition of
  Diamond Tin-Vacancy Centers by In-Situ Strain Control; The negatively charged tin-vacancy (SnV-) center in diamond has emerged as a
promising platform for quantum computing and quantum networks. To connect SnV-
qubits in large networks, in-situ tuning and stabilization of their optical
transitions are essential to overcome static and dynamic frequency offsets
induced by the local environment. Here we report on the large-range optical
frequency tuning of diamond SnV- centers using micro-electro-mechanically
mediated strain control in photonic integrated waveguide devices. We realize a
tuning range of >40 GHz, covering a major part of the inhomogeneous
distribution. In addition, we employ real-time feedback on the strain
environment to stabilize the resonant frequency and mitigate spectral
wandering. These results provide a path for on-chip scaling of diamond
SnV-based quantum networks.",False,False,"The two papers come from distinct areas, biology and quantum physics, but they don't share a common problem or research question that can be addressed by combining their concepts. The first paper focuses on cell type categorization, while the second paper deals with quantum computing and strain control in diamond SnV- centers.",
2411.00561,1,applied,2411.00561-pos1-8,"What is a cell type, really? The quest to categorize life’s myriad forms; The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. “I realized that there were probably hundreds of flavours,” recalls Buenrostro, who was a graduate student at Stanford University in California at the time.",2502.12181,"3D ReX: Causal Explanations in 3D Neuroimaging Classification; Explainability remains a significant problem for AI models in medical
imaging, making it challenging for clinicians to trust AI-driven predictions.
We introduce 3D ReX, the first causality-based post-hoc explainability tool for
3D models. 3D ReX uses the theory of actual causality to generate
responsibility maps which highlight the regions most crucial to the model's
decision. We test 3D ReX on a stroke detection model, providing insight into
the spatial distribution of features relevant to stroke.",False,True,"I extracted the concept of cell type categorization from Paper 1 and the idea of explainability in 3D medical imaging from Paper 2. Combining these concepts, a novel multidisciplinary research idea could be to develop an explainable AI model that categorizes cell types in 3D medical images, providing insights into the spatial distribution of features relevant to cell type identification.",
2411.00561,1,applied,2411.00561-pos2-8,"Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study; In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric.",2502.04492,"Multi-Agent Reinforcement Learning with Focal Diversity Optimization; The advancement of Large Language Models (LLMs) and their finetuning
strategies has triggered the renewed interests in multi-agent reinforcement
learning. In this paper, we introduce a focal diversity-optimized multi-agent
reinforcement learning approach, coined as MARL-Focal, with three unique
characteristics. First, we develop an agent-fusion framework for encouraging
multiple LLM based agents to collaborate in producing the final inference
output for each LLM query. Second, we develop a focal-diversity optimized agent
selection algorithm that can choose a small subset of the available agents
based on how well they can complement one another to generate the query output.
Finally, we design a conflict-resolution method to detect output inconsistency
among multiple agents and produce our MARL-Focal output through reward-aware
and policy-adaptive inference fusion. Extensive evaluations on five benchmarks
show that MARL-Focal is cost-efficient and adversarial-robust. Our multi-agent
fusion model achieves performance improvement of 5.51\% compared to the best
individual LLM-agent and offers stronger robustness over the TruthfulQA
benchmark. Code is available at https://github.com/sftekin/rl-focal",False,True,"I can extract concepts from both papers to create a novel multidisciplinary research idea. By combining the shape-based object classification technique from Paper 1 with the multi-agent reinforcement learning approach from Paper 2, a new research idea emerges: using multi-agent reinforcement learning to optimize shape-based object classification, where agents learn to collaborate and improve classification accuracy.",
2411.00561,1,applied,2411.00561-pos2-8,"Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study; In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric.",2503.04422,"PDX: A Data Layout for Vector Similarity Search; We propose Partition Dimensions Across (PDX), a data layout for vectors
(e.g., embeddings) that, similar to PAX [6], stores multiple vectors in one
block, using a vertical layout for the dimensions (Figure 1). PDX accelerates
exact and approximate similarity search thanks to its dimension-by-dimension
search strategy that operates on multiple-vectors-at-a-time in tight loops. It
beats SIMD-optimized distance kernels on standard horizontal vector storage
(avg 40% faster), only relying on scalar code that gets auto-vectorized. We
combined the PDX layout with recent dimension-pruning algorithms ADSampling
[19] and BSA [52] that accelerate approximate vector search. We found that
these algorithms on the horizontal vector layout can lose to SIMD-optimized
linear scans, even if they are SIMD-optimized. However, when used on PDX, their
benefit is restored to 2-7x. We find that search on PDX is especially fast if a
limited number of dimensions has to be scanned fully, which is what the
dimension-pruning approaches do. We finally introduce PDX-BOND, an even more
flexible dimension-pruning strategy, with good performance on exact search and
reasonable performance on approximate search. Unlike previous pruning
algorithms, it can work on vector data ""as-is"" without preprocessing; making it
attractive for vector databases with frequent updates.",False,True,I can extract concepts from both papers to create a novel multidisciplinary research idea. The shape-based object retrieval and classification technique from Paper 1 can be combined with the vector similarity search data layout from Paper 2 to develop an efficient shape-based object retrieval system using vector embeddings.,
2411.00561,1,applied,2411.00561-pos2-8,"Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study; In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric.",2502.17914,"Upper Mid-Band Spectrum for 6G: Vision, Opportunity and Challenges; Driven by the pursuit of gigabit-per-second data speeds for future 6G mobile
networks, in addition to the support of sensing and artificial intelligence
applications, the industry is expanding beyond crowded sub-6 GHz bands with
innovative new spectrum allocations. In this paper, we chart a compelling
vision for 6G within the frequency range 3 (FR3) spectrum, i.e. $7.125$-$24.25$
$\GHz$, by delving into its key enablers and addressing the multifaceted
challenges that lie ahead for these new frequency bands. Here we highlight the
physical properties of this \textcolor{black}{never-before} used spectrum by
reviewing recent channel measurements for outdoor and indoor environments,
including path loss, delay and angular spreads, and material penetration loss,
all which offer insights that underpin future 5G/6G wireless communication
designs. Building on the fundamental knowledge of the channel properties, we
explore FR3 spectrum agility strategies that balance coverage and capacity
(e.g. data rate) tradeoffs, while also examining coexistence with incumbent
systems, such as satellites, radio astronomy, and earth exploration. Moreover,
we discuss the potential of massive multiple-input multiple-output, compact and
digital architectures, and evaluate the potential of multiband sensing for FR3
integrated sensing and communications. Finally, we outline 6G standardization
features that are likely to emerge from 3GPP radio frame innovations and open
radio access network developments.",False,False,"Although both papers introduce innovative ideas, they belong to distinct disciplines (signal processing and telecommunications). The concepts presented in these papers do not naturally intersect or complement each other, making it challenging to create a novel multidisciplinary research idea that integrates both disciplines.",
2411.00561,1,applied,2411.00561-pos2-8,"Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study; In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric.",2502.05898,"Rising Marginal Costs, Rising Prices?; We present empirical evidence on the relationship between demand shocks and
price changes, conditional on returns to scale. We find that in industries with
decreasing returns to scale, demand increases (which raise costs) correspond to
price increases. Whereas, in industries with increasing returns to scale,
demand increases (which lower costs) correspond to stable prices. We interpret
the results with a theory of imperfect competition and returns to scale. For
prices to remain stable following a cost decrease, markups necessarily rise.
For prices to increase as cost increases, it is not necessary for markups to
change but does not preclude their role. From a macroeconomic perspective, our
results imply that inflation dynamics and the effectiveness of monetary policy
depend on market structures.",False,False,"The two papers are from distinct areas, one from computer science and the other from economics. Although they are interdisciplinary, there is no clear connection between the techniques used for shape classification and the economic concepts of returns to scale and price changes.",
2411.00561,1,applied,2411.00561-pos2-8,"Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study; In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric.",2503.0017,"Elastic Restaking Networks; Decentralized services for blockchains often require their validators
(operators) to deposit stake (collateral), which is forfeited (slashed) if they
misbehave. Restaking networks let validators secure multiple services by
reusing stake, giving rise to a strategic game: Validators can coordinate to
misbehave across multiple services, extracting digital assets while forfeiting
their stake only once.
  Previous work focused either on preventing coordinated misbehavior or on
protecting services if all other services are Byzantine and might unjustly
cause slashing due to bugs or malice. The first model overlooks how a single
Byzantine service can collapse the network, while the second ignores
shared-stake benefits.
  To bridge the gap, we model the strategic game of coordinated misbehavior
when a given fraction of services are Byzantine. We introduce elastic restaking
networks, where validators can allocate portions of their stake that may
cumulatively exceed their total stake, and when allocations are lost, the
remaining stake stretches to cover remaining allocations. We show that elastic
networks exhibit superior robustness compared to previous approaches, and
demonstrate a synergistic effect where an elastic restaking network enhances
its blockchain's security, contrary to community concerns of an opposite effect
in existing networks. We then design incentives for tuning validators'
allocations.
  Our elastic restaking system and incentive design have immediate practical
implications for deployed restaking networks, which have billions of dollars in
stake.",False,False,"The two papers come from distinct areas of research, one in computer vision and shape classification, and the other in blockchain and decentralized networks. There is no clear intersection or potential for integration of concepts that would lead to a novel, feasible, and useful interdisciplinary research idea.",
