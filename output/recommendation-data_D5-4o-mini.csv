paper_a_id,research_type,paper_main_id,paper_main_text,paper_target_id,paper_target_text,list_true,llm_rec_list,reasoning,list_str,mrr_scores,ndcg10_scores
2411.01019,applied,2411.01019-pos1-0,"Incidental Anterior Mediastinal Nodular Lesions on Chest CT in Asymptomatic Subjects; Screening for lung cancer: 2023 guideline update from the American Cancer Society; Objective The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods We included 56,358 consecutive participants (mean age 52.4 ± 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66–0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01–3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes.; Abstract Lung cancer is the leading cause of mortality and person‐years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network‐validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit‐to‐radiation‐risk ratios follow‐up examinations. GDG also examined disease burden data National Institute’s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50–80 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low‐dose computed tomography asymptomatic currently smoke or formerly smoked have ≥20 pack‐year ( , ). Before decision made initiate LCS, should engage shared decision‐making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",2411.01019-pos2-0,"Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms; Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability.",50,"['50', '49', '40', '4', '5', '7', '20', '16', '21', '25']","The best candidate paper that aligns with the main paper on incidental anterior mediastinal nodular lesions and lung cancer screening is paper 50. It focuses on a UNet-based neural network to segment anterior mediastinal nodular lesions from chest CT imaging. This directly contributes to improving detection methods mentioned in the main paper. Furthermore, it combines advanced image processing techniques with clinical application, making it a novel and useful approach. The other candidates, although interesting, do not directly address the clinical significance or technological advancement in the context of lung cancer screening. Papers 49 and 40 also touch on medical imaging and diagnostics but are less relevant to the specific focus on mediastinal lesions and their implications for lung cancer.","1) Non-extendablity of Shelukhin's quasimorphism and non-triviality of
  Reznikov's class; Shelukhin constructed a quasimorphism on the universal covering of the group
of Hamiltonian diffeomorphisms for a general closed symplectic manifold. In the
present paper, we prove the non-extendability of that quasimorphism for certain
symplectic manifolds, such as a blow-up of torus and the product of a surface
of genus at least two and a closed symplectic manifold. As its application, we
prove the non-vanishing of Reznikov's characteristic class for the above
symplectic manifolds.; 2) Towards low-dimensionalization of four-dimensional QCD; Inspired by the one-dimensional color-electric flux-tube in a hadron, we
propose a possible way of low-dimensionalization of 4D QCD. As a strategy, we
use gauge degrees of freedom and propose a new gauge fixing of ``dimensional
reduction (DR) gauge"". The DR gauge is defined so as to minimize $R_{\rm DR}
\equiv \int d^4s~{\rm Tr}~[A^2_x(s)+A^2_y(s)]$, which preserves the 2D
SU($N_{c}$) gauge symmetry. We investigate low-dimensionalization properties of
4D DR-gauged QCD in SU(3) lattice QCD at $\beta$ = 6.0. In the DR gauge, the
amplitudes of two gluon components $A_{x}(s)$ and $A_{y}(s)$ are found to be
strongly suppressed, and these components have a large effective mass of
$M_{\perp} \simeq 1.7$ GeV. In the DR gauge, the static interquark potential is
well reproduced only with the two components $A_{t}(s)$ and $A_{z}(s)$, while
$A_{x}(s)$ and $A_{y}(s)$ seem to be inactive. We investigate the spatial
correlation of two $t$-directed gluons and find that the correlation decreases
as $e^{-mr}$ with $m \simeq$ 0.64 GeV, corresponding to the correlation length
$\xi \equiv 1/m \simeq$ 0.31 fm. We reduce 4D QCD in the DR gauge to 2D QCD
with the coupling $g_{2D} = g/\xi$, which approximately reproduces the string
tension.; 3) Impacts of Dynamical Dark Energy on the Neutrino Mass Constraints; The difference between the total neutrino mass estimates ($\sum m_\nu$)
derived from cosmological data within the standard $\Lambda$CDM model and those
obtained from terrestrial particle physics experiments underscores the need to
explore alternative scenarios. Recent analyses have shown that a dynamic dark
energy modeled by the CPL parameterization of the dark energy equation of state
(EoS) can ease the constraints on $\sum m_\nu$, thus alleviating this tension.
This study investigates the robustness of this discrepancy by assessing the
extent to which the CPL assumption influences the results. We examine how other
EoS parameterizations, such as the Barboza-Alcaniz (BA) and
Jassal-Bagla-Padmanabhan (JBP) parameterizations, affect $\sum m_\nu$
estimates. We perform a Markov Chain Monte Carlo (MCMC) analysis combining the
latest baryon acoustic oscillation data from DESI with the Planck 2018 cosmic
microwave background data, which includes information on temperature,
polarization, and lensing, as well as the Pantheon+ Type Ia supernovae
observations. While both the BA and JBP parameterizations can also resolve the
tension, our results show a correlation between the dark energy EoS and the
constraints on neutrino mass.; 4) Quantum learning advantage on a scalable photonic platform; Recent advancements in quantum technologies have opened new horizons for
exploring the physical world in ways once deemed impossible. Central to these
breakthroughs is the concept of quantum advantage, where quantum systems
outperform their classical counterparts in solving specific tasks. While much
attention has been devoted to computational speedups, quantum advantage in
learning physical systems remains a largely untapped frontier. Here, we present
a photonic implementation of a quantum-enhanced protocol for learning the
probability distribution of a multimode bosonic displacement process. By
harnessing the unique properties of continuous-variable quantum entanglement,
we obtain a massive advantage in sample complexity with respect to conventional
methods without entangled resources. With approximately 5 dB of two-mode
squeezing -- corresponding to imperfect Einstein--Podolsky--Rosen (EPR)
entanglement -- we learn a 100-mode bosonic displacement process using 11.8
orders of magnitude fewer samples than a conventional scheme. Our results
demonstrate that even with non-ideal, noisy entanglement, a significant quantum
advantage can be realized in continuous-variable quantum systems. This marks an
important step towards practical quantum-enhanced learning protocols with
implications for quantum metrology, certification, and machine learning.; 5) COLP: Scaffolding Children's Online Long-term Collaborative Learning; Online collaborative learning and working are important for everyone
including children. However, children still face a lot of difficulties
communicating and working together while online, which keeps them from engaging
in long-term project-based teamwork. We aim to investigate online long-term
collaborative learning opportunities to address this gap. We design COLP, an
online, 16-week, project-based learning program, as an educational intervention
based on multiple learning theories for primary school students. We conducted
this program with 67 primary school students ages 8-13, across more than five
provinces of China. We found that this program could engage more than one-third
of children in teamwork after long-term study. Furthermore, we interview
children and their parents to help us understand the communication channel,
benefits, and challenges of this program. Interestingly, we discovered that
parents play multiple roles in their children's collaborative learning,
particularly modeling and guiding the children's collaborative skills. Given
the lack of programs designed for children's long-term online collaboration,
this study may inspire intervention design in computer-supported collaborative
learning communities.; 6) Grid Cost Allocation in Peer-to-Peer Electricity Markets: Benchmarking
  Classical and Quantum Optimization Approaches; This paper presents a novel optimization approach for allocating grid
operation costs in Peer-to-Peer (P2P) electricity markets using Quantum
Computing (QC). We develop a Quadratic Unconstrained Binary Optimization (QUBO)
model that matches logical power flows between producer-consumer pairs with the
physical power flow to distribute grid usage costs fairly. The model is
evaluated on IEEE test cases with up to 57 nodes, comparing Quantum Annealing
(QA), hybrid quantum-classical algorithms, and classical optimization
approaches. Our results show that while the model effectively allocates grid
operation costs, QA performs poorly in comparison despite extensive
hyperparameter optimization. The classical branch-and-cut method outperforms
all solvers, including classical heuristics, and shows the most advantageous
scaling behavior. The findings may suggest that binary least-squares
optimization problems may not be suitable candidates for near-term quantum
utility.; 7) Simulated Bifurcation with High-dimensional Expansion for Traffic Signal
  Optimization on Real-world Networks; With accelerating urbanization and worsening traffic congestion, optimizing
traffic signal systems to improve road throughput and alleviate congestion has
become a critical issue. This study proposes a short-term traffic prediction
model based on real-world road topologies and a typical four-way, eight-phase
traffic signal control scheme. The model accounts for traffic flow disparities
across directions and signal phase change frequencies, integrating these
factors into an optimization objective for global traffic optimization. The
structure of this objective function is similar to spin-glass systems in
statistical physics. A Simulated Bifurcation optimization algorithm is
introduced, with traditional simulated annealing as a benchmark. The results
show that Simulated Bifurcation outperforms simulated annealing in both
efficiency and effectiveness. Using real traffic flow and road network data
from Beijing, we initialized the model and conducted numerical optimization
experiments. The results indicate that Simulated Bifurcation significantly
outperforms simulated annealing in computational efficiency, effectively
solving combinatorial optimization problems with multiple spin interactions,
and reducing the time complexity to $O(N^{1.35})$. This solution addresses the
NP-hard problem of global traffic signal optimization. Importantly, the signal
phase patterns generated by Simulated Bifurcation align with the operational
requirements of real traffic signal systems, showcasing its potential in
optimizing signal control for large, complex urban traffic networks. This work
provides solid theoretical and practical foundations for future urban traffic
management and intelligent transportation systems.; 8) Dynamic Multimodal Sentiment Analysis: Leveraging Cross-Modal Attention
  for Enabled Classification; This paper explores the development of a multimodal sentiment analysis model
that integrates text, audio, and visual data to enhance sentiment
classification. The goal is to improve emotion detection by capturing the
complex interactions between these modalities, thereby enabling more accurate
and nuanced sentiment interpretation. The study evaluates three feature fusion
strategies -- late stage fusion, early stage fusion, and multi-headed attention
-- within a transformer-based architecture. Experiments were conducted using
the CMU-MOSEI dataset, which includes synchronized text, audio, and visual
inputs labeled with sentiment scores. Results show that early stage fusion
significantly outperforms late stage fusion, achieving an accuracy of 71.87\%,
while the multi-headed attention approach offers marginal improvement, reaching
72.39\%. The findings suggest that integrating modalities early in the process
enhances sentiment classification, while attention mechanisms may have limited
impact within the current framework. Future work will focus on refining feature
fusion techniques, incorporating temporal data, and exploring dynamic feature
weighting to further improve model performance.; 9) On principal eigenvalues of linear time-periodic parabolic systems:
  symmetric mutation case; The paper is concerned with the effect of the spatio-temporal heterogeneity
on the principal eigenvalue of some linear time-periodic parabolic system.
Various asymptotic behaviors of the principal eigenvalue and its monotonicity,
as a function of the diffusion rate and frequency, are first derived. In
particular, some singular behaviors of the principal eigenvalues are observed
when both diffusion rate and frequency approach zero, with some scalar
time-periodic Hamilton-Jacobi equation as the limiting equation. Furthermore,
we completely classify the topological structures of the level sets for the
principal eigenvalues in the plane of frequency and diffusion rate. Our results
not only generalize most of the findings in [S. Liu and Y. Lou, J. Funct.
Anal., 282 (2022), 109338] for scalar periodic-parabolic operators, but also
reveal more rich global information, for time-periodic parabolic systems, on
the dependence of the principal eigenvalues upon the spatio-temporal
heterogeneity.; 10) Low-energy spectra of nobelium isotopes: Skyrme
  random-phase-approximation analysis; Low-energy spectra in the isotopic chain $^{250-262}$No are systematically
investigated within the fully self-consistent Quasiparticle
Random-Phase-Approximation (QRPA) using Skyrme forces SLy6, SkM* and SVbas.
QRPA states of multipolarity $\lambda\mu$=20, 22, 30, 31, 32, 33, 43, 44 and 98
are considered. The main attention is paid to isotopes $^{252}$No and
$^{254}$No, for which the most extensive experimental spectroscopic information
is available. In these two nuclei, a reasonable description of $K^{\pi}=8^-,
2^-$and $3^+$ isomers is obtained with the force SLy6. The disputed $8^-$
isomer in $^{254}$No is assigned as neutron two-quasiparticle configuration
$nn[734\uparrow,613\uparrow]$. At the energies 1.2 - 1.4 MeV, the 2qp K-isomers
$4^-, 7^-$ in $^{252}$No and $4^-, 6^-, 7^-$ in $^{254}$No are predicted. In
$^{254}$No, the $K^{\pi}=3^+$ isomer should be accompanied by the nearby
$K^{\pi}=4^+$ counterpart. It is shown that, in the chain $^{250-262}$No, some
features of $^{252}$No and $^{254}$No exhibit essential irregularities caused
by a shell gap in the neutron single-particle spectrum and corresponding
breakdown of the neutron pairing. In particular, low-energy pairing-vibrational
$K^{\pi}=0^+$ states in $^{252,254}$No are predicted.; 11) The Worse The Better: Content-Aware Viewpoint Generation Network for
  Projection-related Point Cloud Quality Assessment; Through experimental studies, however, we observed the instability of final
predicted quality scores, which change significantly over different viewpoint
settings. Inspired by the ""wooden barrel theory"", given the default
content-independent viewpoints of existing projection-related PCQA approaches,
this paper presents a novel content-aware viewpoint generation network (CAVGN)
to learn better viewpoints by taking the distribution of geometric and
attribute features of degraded point clouds into consideration. Firstly, the
proposed CAVGN extracts multi-scale geometric and texture features of the
entire input point cloud, respectively. Then, for each default
content-independent viewpoint, the extracted geometric and texture features are
refined to focus on its corresponding visible part of the input point cloud.
Finally, the refined geometric and texture features are concatenated to
generate an optimized viewpoint. To train the proposed CAVGN, we present a
self-supervised viewpoint ranking network (SSVRN) to select the viewpoint with
the worst quality projected image to construct a default-optimized viewpoint
dataset, which consists of thousands of paired default viewpoints and
corresponding optimized viewpoints. Experimental results show that the
projection-related PCQA methods can achieve higher performance using the
viewpoints generated by the proposed CAVGN.; 12) STGAN: Spatial-temporal Graph Autoregression Network for Pavement
  Distress Deterioration Prediction; Pavement distress significantly compromises road integrity and poses risks to
drivers. Accurate prediction of pavement distress deterioration is essential
for effective road management, cost reduction in maintenance, and improvement
of traffic safety. However, real-world data on pavement distress is usually
collected irregularly, resulting in uneven, asynchronous, and sparse
spatial-temporal datasets. This hinders the application of existing
spatial-temporal models, such as DCRNN, since they are only applicable to
regularly and synchronously collected data. To overcome these challenges, we
propose the Spatial-Temporal Graph Autoregression Network (STGAN), a novel
graph neural network model designed for accurately predicting irregular
pavement distress deterioration using complex spatial-temporal data.
Specifically, STGAN integrates the temporal domain into the spatial domain,
creating a larger graph where nodes are represented by spatial-temporal tuples
and edges are formed based on a similarity-based connection mechanism.
Furthermore, based on the constructed spatiotemporal graph, we formulate
pavement distress deterioration prediction as a graph autoregression task,
i.e., the graph size increases incrementally and the prediction is performed
sequentially. This is accomplished by a novel spatial-temporal attention
mechanism deployed by STGAN. Utilizing the ConTrack dataset, which contains
pavement distress records collected from different locations in Shanghai, we
demonstrate the superior performance of STGAN in capturing spatial-temporal
correlations and addressing the aforementioned challenges. Experimental results
further show that STGAN outperforms baseline models, and ablation studies
confirm the effectiveness of its novel modules. Our findings contribute to
promoting proactive road maintenance decision-making and ultimately enhancing
road safety and resilience.; 13) X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from
  Multi-Turn Jailbreaks without Compromising Usability; Despite the rapid development of safety alignment techniques for LLMs,
defending against multi-turn jailbreaks is still a challenging task. In this
paper, we conduct a comprehensive comparison, revealing that some existing
defense methods can improve the robustness of LLMs against multi-turn
jailbreaks but compromise usability, i.e., reducing general capabilities or
causing the over-refusal problem. From the perspective of mechanism
interpretability of LLMs, we discover that these methods fail to establish a
boundary that exactly distinguishes safe and harmful feature representations.
Therefore, boundary-safe representations close to harmful representations are
inevitably disrupted, leading to a decline in usability. To address this issue,
we propose X-Boundary to push harmful representations away from boundary-safe
representations and obtain an exact distinction boundary. In this way, harmful
representations can be precisely erased without disrupting safe ones.
Experimental results show that X-Boundary achieves state-of-the-art defense
performance against multi-turn jailbreaks, while reducing the over-refusal rate
by about 20% and maintaining nearly complete general capability. Furthermore,
we theoretically prove and empirically verify that X-Boundary can accelerate
the convergence process during training. Please see our code at:
https://github.com/AI45Lab/X-Boundary.; 14) Gorenstein analogues of a projectivity criterion over group algebras; We formulate and answer Gorenstein projective, flat, and injective analogues
of a classical projectivity question for group rings under some mild additional
assumptions. Although the original question, that was proposed by Jang-Hyun Jo
in 2007, was for integral group rings, in this article, we deal with more
general commutative base rings. We make use of the vast developments that have
happened in the field of Gorenstein homological algebra over group rings in
recent years, and we also improve and generalize several existing results from
this area along the way.; 15) On the periodic orbits of C0-typical impulsive semiflows; Impulsive semiflows modeled by a Lipschitz continuous vector field and
continuous impulse functions, defined over an impulsive region, are piece-wise
Lipschitz continuous semiflows with piecewise smooth trajectories. In this
paper we contribute to the topological description of typical impulsive
semi-flows, parameterized by both vector fields and impulses. We prove that
$C^0$-generic Lipschitz vector fields generate impulsive semiflows with
denseness of periodic orbits on the non-wandering set. Additionally, we show
that $C^0$-generic impulses generate impulsive semiflows with denseness of
periodic orbits on the impulsive non-wandering set.; 16) Adaptive Cybersecurity: Dynamically Retrainable Firewalls for Real-Time
  Network Protection; The growing complexity of cyber attacks has necessitated the evolution of
firewall technologies from static models to adaptive, machine learning-driven
systems. This research introduces ""Dynamically Retrainable Firewalls"", which
respond to emerging threats in real-time. Unlike traditional firewalls that
rely on static rules to inspect traffic, these advanced systems leverage
machine learning algorithms to analyze network traffic pattern dynamically and
identify threats. The study explores architectures such as micro-services and
distributed systems for real-time adaptability, data sources for model
retraining, and dynamic threat identification through reinforcement and
continual learning. It also discusses strategies to improve performance, reduce
latency, optimize resource utilization, and address integration issues with
present-day concepts such as Zero Trust and mixed environments. By critically
assessing the literature, analyzing case studies, and elucidating areas of
future research, this work suggests dynamically retrainable firewalls as a more
robust form of network security. Additionally, it considers emerging trends
such as advancements in AI and quantum computing, ethical issues, and other
regulatory questions surrounding future AI systems. These findings provide
valuable information on the future state of adaptive cyber security, focusing
on the need for proactive and adaptive measures that counter cyber threats that
continue to evolve.; 17) Fluid-Structure Interaction with Porous Media: The Beaver-Joseph
  condition in the strong sense; This article considers fluid structure interaction describing the motion of a
fluid contained in a porous medium. The fluid is modelled by Navier-Stokes
equations and the coupling between fluid and the porous medium is described by
the classical Beaver-Joseph or the Beaver-Joseph-Saffman interface condition.
In contrast to previous work these conditions are investigated for the first
time in the strong sense and it is shown that the coupled system admits a
unique, global strong solution in critical spaces provided the data are small
enough. Furthermore, a Serrin-type blow-up criterium is developed and higher
regularity estimates at the interface are established, which say that the
solution is even analytic provided the forces are so.; 18) Post-training an LLM for RAG? Train on Self-Generated Demonstrations; Large language models (LLMs) often struggle with knowledge intensive NLP
tasks, such as answering ""Who won the latest World Cup?"" because the knowledge
they learn during training may be insufficient or outdated. Conditioning
generation on retrieved documents -- a technique known as retrieval augmented
generation (RAG) -- mitigates these shortcomings by allowing the model to
leverage in-context information. Practitioners can improve LLM RAG performance
by fine-tuning on retrieval-augmented instructions, but must beware that this
can cause undesirable model behaviors like hallucinations. We attribute this
degradation to the fact that the training data is likely to be
out-of-distribution for the model and may suffer from quality issues, such as
misalignment between retrievals and target responses (since retrievals are
frequently added post-hoc). We propose a recipe for training RAG-enabled LLMs
using self-generated demonstrations, thereby avoiding training on
out-of-distribution text and integrating retrievals into the LLM responses. We
evaluate our method on knowledge intensive question answering (QA) tasks and
show that our method teaches LLMs to properly handle in-context retrievals and
abstain from questions it will likely get wrong. Compared to conventional RA-IT
methods, our method prevents model degradation in non-RAG settings while
exhibiting superior QA performance.; 19) In-situ graph reasoning and knowledge expansion using Graph-PReFLexOR; The pursuit of automated scientific discovery has fueled progress from
symbolic logic to modern AI, forging new frontiers in reasoning and pattern
recognition. Transformers function as potential systems, where every possible
relationship remains latent potentiality until tasks impose constraints, akin
to measurement. Yet, refining their sampling requires more than probabilistic
selection: solutions must conform to specific structures or rules, ensuring
consistency and the invocation of general principles. We present
Graph-PReFLexOR (Graph-based Preference-based Recursive Language Modeling for
Exploratory Optimization of Reasoning), a framework that combines graph
reasoning with symbolic abstraction to dynamically expand domain knowledge.
Inspired by reinforcement learning, Graph-PReFLexOR defines reasoning as a
structured mapping, where tasks yield knowledge graphs, abstract patterns, and
ultimately, final answers. Inspired by category theory, it encodes concepts as
nodes and their relationships as edges, supporting hierarchical inference and
adaptive learning through isomorphic representations. Demonstrations include
hypothesis generation, materials design, and creative reasoning, such as
discovering relationships between mythological concepts like 'thin places' with
materials science. We propose a 'knowledge garden growth' strategy that
integrates insights across domains, promoting interdisciplinary connections.
Results with a 3-billion-parameter Graph-PReFLexOR model show superior
reasoning depth and adaptability, underscoring the potential for transparent,
multidisciplinary AI-driven discovery. It lays the groundwork for general
autonomous reasoning solutions.; 20) EmoTech: A Multi-modal Speech Emotion Recognition Using Multi-source
  Low-level Information with Hybrid Recurrent Network; Emotion recognition is a critical task in human-computer interaction,
enabling more intuitive and responsive systems. This study presents a
multimodal emotion recognition system that combines low-level information from
audio and text, leveraging both Convolutional Neural Networks (CNNs) and
Bidirectional Long Short-Term Memory Networks (BiLSTMs). The proposed system
consists of two parallel networks: an Audio Block and a Text Block. Mel
Frequency Cepstral Coefficients (MFCCs) are extracted and processed by a BiLSTM
network and a 2D convolutional network to capture low-level intrinsic and
extrinsic features from speech. Simultaneously, a combined BiLSTM-CNN network
extracts the low-level sequential nature of text from word embeddings
corresponding to the available audio. This low-level information from speech
and text is then concatenated and processed by several fully connected layers
to classify the speech emotion. Experimental results demonstrate that the
proposed EmoTech accurately recognizes emotions from combined audio and text
inputs, achieving an overall accuracy of 84%. This solution outperforms
previously proposed approaches for the same dataset and modalities.; 21) Improving Residential Safety by Multiple Sensors on Multiple Nodes for
  Joint Emergency Detection; Recent advances in low-cost microcontrollers have enabled innovative smart
home applications. However, existing systems typically consist of
single-purpose devices that only report sensed data to a controller. Given the
potential for residential emergencies, we propose to integrate emergency
detection systems into smart home environments. We present an ad-hoc
distributed sensor network (DSN) designed to detect five common residential
emergencies: fires, gas and water leakages, earthquakes, and intrusions. Our
novel approach combines diverse sensors with a voting-based consensus algorithm
among multiple nodes, improving accuracy and reliability over traditional alert
systems. The consensus algorithm employs a majority rule with weighted votes,
allowing adjustments for various scenarios. An experimental evaluation confirms
our approach's effectiveness in accurately detecting emergencies while
demonstrating reliability in mitigating node failures, ensuring system
longevity, and maintaining robust communication. Additionally, our approach
significantly reduces power consumption compared to alternatives.; 22) Full-cycle device-scale simulations of memory materials with a tailored
  atomic-cluster-expansion potential; Computer simulations have long been key to understanding and designing
phase-change materials (PCMs) for memory technologies. Machine learning is now
increasingly being used to accelerate the modelling of PCMs, and yet it remains
challenging to simultaneously reach the length and time scales required to
simulate the operation of real-world PCM devices. Here, we show how ultra-fast
machine-learned interatomic potentials, based on the atomic cluster expansion
(ACE) framework, enable simulations of PCMs reflecting applications in devices
with excellent scalability on high-performance computing platforms. We report
full-cycle simulations -- including the time-consuming crystallisation process
(from digital ""zeroes"" to ""ones"") -- thus representing the entire programming
cycle for cross-point memory devices. We also showcase a simulation of
full-cycle operations, relevant to neuromorphic computing, in a mushroom-type
device geometry. Our work provides a springboard for the atomistic modelling of
PCM-based memory and neuromorphic computing devices -- and, more widely, it
illustrates the power of highly efficient ACE ML models for materials science
and engineering.; 23) Multidimensional moment problem and diagonal Schur algorithm; The multidimensional moment problem is studied in terms of the Steiltjes
transform. The diagonal step-by-step algorithm is constructed for the
multidimensional moment problem. The set of solutions of the full
multidimensional moment problem is found in terms of the continued fractions.
Moreover, the diagonal step-by-step algorithm can be applied to the special
truncated multidimensional moment problem.; 24) SAPPHIRES: A Galaxy Over-Density in the Heart of Cosmic Reionization at
  $z=8.47$; We report the discovery of a galaxy proto-cluster candidate (dubbed
MACS0416-OD-z8p5) at a spectroscopic redshift of $z\sim8.47$, dating back to
$\sim550$Myr after the Big Bang. The observations are part of the JWST Cycle-3
treasury program, Slitless Areal Pure-Parallel HIgh-Redshift Emission Survey
(SAPPHIRES) with NIRCam-grism. Using wide field slitless spectroscopy (WFSS)
obtained in the MACS0416 parallel field, we robustly confirm nine galaxies at
$z_{\rm spec}\sim8.47$ via emission line detections of [OIII]5008A (with
$>5\,\sigma$) and tentatively confirm one additional galaxy (at
$\sim3\,\sigma$). This discovery represents the highest-redshift,
spectroscopically confirmed galaxy over-density known to date, which is
$\sim6$--$8$ times more dense than the average volume density of galaxies at
the same redshift. Furthermore, a galaxy hosting a low-mass active galactic
nucleus (``Little-Red-Dot'') is found as a member, suggesting an early
emergence of active, massive black holes and feedback between these black holes
and their surrounding environments. We also discuss the spatial structures
connecting the galaxy over-density to nearby massive star-forming galaxies
(separated by $\sim 5$pMpc, including MACS0416-Y1 and MACS0416-JD. This finding
of a massive dark matter halo hosting a galaxy over-density at $z\sim8.5$ is
surprising given that our survey covered only a small, random field
($16.5\,{\rm arcmin^2}$) as part of a pure parallel observation. The comparison
with cosmological simulations shows that the likelihood of finding such a
large-scale structure is $<5\,\%$ under the current galaxy formation scenario
and the observed survey volume. Our results demonstrate the power of WFSS
observations to build a complete line-emitter sample and suggest an important
role for over-densities in enhancing galaxy formation by funneling large-scale
gas supplies into small cosmological volumes.; 25) Explicit Solution of Tunable Input-to-State Safe-Based Controller Under
  High-Relative-Degree Constraints; This paper investigates the safety analysis and verification of nonlinear
systems subject to high-relative-degree constraints and unknown disturbance.
The closed-form solution of the high-order control barrier functions (HOCBF)
optimization problem with and without a nominal controller is first provided,
making it unnecessary to solve the quadratic program problem online and
facilitating the analysis. Further, we introduce the concept of tunable
input-to-state safety(ISSf), and a new tunable function in conjunction with
HOCBF is provided. When combined with the existing ISSf theorem, produces
controllers for constrained nonlinear systems with external disturbances. The
theoretical results are proven and supported by numerical simulations.; 26) SynchroStore: A Cost-Based Fine-Grained Incremental Compaction for
  Hybrid Workloads; This study proposes a novel storage engine, SynchroStore, designed to address
the inefficiency of update operations in columnar storage systems based on
Log-Structured Merge Trees (LSM-Trees) under hybrid workload scenarios. While
columnar storage formats demonstrate significant query performance advantages
when handling large-scale datasets, traditional columnar storage systems face
challenges such as high update complexity and poor real-time performance in
data-intensive applications. SynchroStore introduces an incremental row storage
mechanism and a fine-grained row-to-column transformation and compaction
strategy, effectively balancing data update efficiency and query performance.
The storage system employs an in-memory row storage structure to support
efficient update operations, and the data is converted to a columnar format
after freezing to support high-performance read operations. The core
innovations of SynchroStore are reflected in the following aspects:(1) the
organic combination of incremental row storage and columnar storage; (2) a
fine-grained row-to-column transformation and compaction mechanism; (3) a
cost-based scheduling strategy. These innovative features allow SynchroStore to
leverage background computational resources for row-to-column transformation
and compaction operations, while ensuring query performance is unaffected, thus
effectively solving the update performance bottleneck of columnar storage under
hybrid workloads. Experimental evaluation results show that, compared to
existing columnar storage systems like DuckDB, SynchroStore exhibits
significant advantages in update performance under hybrid workloads.; 27) The Best Soules Basis for the Estimation of a Spectral Barycentre
  Network; The main contribution of this work is a fast algorithm to compute the
barycentre of a set of networks based on a Laplacian spectral pseudo-distance.
The core engine for the reconstruction of the barycentre is an algorithm that
explores the large library of Soules bases, and returns a basis that yields a
sparse approximation of the sample mean adjacency matrix. We prove that when
the networks are random realizations of stochastic block models, then our
algorithm reconstructs the population mean adjacency matrix. In addition to the
theoretical analysis of the estimator of the barycentre network, we perform
Monte Carlo simulations to validate the theoretical properties of the
estimator. This work is significant because it opens the door to the design of
new spectral-based network synthesis that have theoretical guarantees.; 28) Initial Guess Generation for Low-Thrust Trajectory Design with
  Robustness to Missed-Thrust-Events; The growing interest in cislunar space exploration in recent years has driven
an increasing demand for efficient low-thrust missions to key cislunar orbits.
These missions, typically possessing long thrust arcs, are particularly
susceptible to operational uncertainties such as missed thrust events.
Addressing these challenges requires efficient robust trajectory design
frameworks during the preliminary mission design phase, where it is necessary
to explore the solution space at a rapid cadence under evolving operational
constraints. However, existing methods for missed thrust design rely on solving
high-dimensional nonlinear programs, where generating effective initial guesses
becomes challenging. To enhance computational efficiency, quality, and depth of
robustness of solutions from global search, we compare two initial guess
strategies: a baseline non-conditional global search, which samples from a
static distribution with global support, and a conditional global search, which
generates initial guesses conditioned on solutions to problems with less depth
of robustness. The conditional search provides a sequential procedure for
solving increasingly robust problems. We validate the improvements in the
conditional approach using a low-thrust case study for the Lunar Gateway Power
and Propulsion Element, where our results demonstrate that it significantly
improves convergence rate and solution quality, highlighting its potential in
preliminary robust trajectory design.; 29) The Missing Link of Sulfur Chemistry in TMC-1: The Detection of c-C3H2S
  from the GOTHAM Survey; We present the spectroscopic characterization of cyclopropenethione (c-C3H2S)
in the laboratory and detect it in space using the Green Bank Telescope (GBT)
Observations of TMC-1: Hunting Aromatic Molecules (GOTHAM) survey. The
detection of this molecule - the missing link in understanding the C3H2S
isomeric family in TMC-1 - completes the detection of all 3 low-energy isomers
of C3H2S as both CH2CCS and HCCCHS have been previously detected in this
source. The total column density of this molecule (N_T of 5.72+2.65/-1.61x10^10
cm^-2 at an excitation temperature of 4.7+1.3/-1.1 K) is smaller than both
CH2CCS and HCCCHS and follows nicely the relative dipole principle (RDP), a
kinetic rule-of-thumb for predicting isomer abundances which suggests that, all
other chemistry among a family of isomers being the same, the member with the
smallest dipole should be the most abundant. The RDP now holds for the
astronomical abundance ratios of both the S-bearing and O-bearing counterparts
observed in TMC-1; however, CH2CCO continues to elude detection in any
astronomical source.; 30) Deep Learning Assisted Denoising of Experimental Micrographs; Microstructure imaging is crucial in materials science, but experimental
images often introduce noise that obscures critical structural details. This
study presents a novel deep learning approach for robust microstructure image
denoising, combining phase-field simulations, Fourier transform techniques, and
an attention-based neural network. The innovative framework addresses dataset
limitations by synthetically generating training data by combining
computational phase-field microstructures with experimental optical
micrographs. The neural network architecture features an attention mechanism
that dynamically focuses on important microstructural features while
systematically eliminating noise types like scratches and surface
imperfections. Testing on a FeMnNi alloy system demonstrated the model's
exceptional performance across multiple magnifications. By successfully
removing diverse noise patterns while maintaining grain boundary integrity, the
research provides a generalizable deep-learning framework for microstructure
image enhancement with broad applicability in materials science.; 31) Benchmarking direct and indirect dipolar spin-exchange interactions
  between two Rydberg atoms; We report on the experimental characterization of various types of
spin-exchange interactions between two individual atoms, where pseudo-spin
degrees of freedom are encoded in different Rydberg states. For the case of the
direct dipole-dipole interaction between states of opposite parity, such as
between $nS$ and $nP$, we investigate the effects of positional disorder
arising from the residual atomic motion, on the coherence of spin-exchange
oscillations. We then characterize an indirect dipolar spin exchange, i.e., the
off-diagonal part of the van der Waals effective Hamiltonian that couples the
states $nS$ and $(n+1)S$. Finally, we report on the observation of a new type
of dipolar coupling, made resonant using addressable light-shifts and involving
four different Rydberg levels: this exchange process is akin to electrically
induced F\""orster resonance, but featuring local control. It exhibits an
angular dependence distinct from the usual $1-3\cos^2(\theta)$ form of the
resonant dipolar spin-exchange.; 32) Real-Time Operator Takeover for Visuomotor Diffusion Policy Training; We present a Real-Time Operator Takeover (RTOT) paradigm enabling operators
to seamlessly take control of a live visuomotor diffusion policy, guiding the
system back into desirable states or reinforcing specific demonstrations. We
present new insights in using the Mahalonobis distance to automatically
identify undesirable states. Once the operator has intervened and redirected
the system, the control is seamlessly returned to the policy, which resumes
generating actions until further intervention is required. We demonstrate that
incorporating the targeted takeover demonstrations significantly improves
policy performance compared to training solely with an equivalent number of,
but longer, initial demonstrations. We provide an in-depth analysis of using
the Mahalanobis distance to detect out-of-distribution states, illustrating its
utility for identifying critical failure points during execution. Supporting
materials, including videos of initial and takeover demonstrations and all rice
scooping experiments, are available on the project website:
https://operator-takeover.github.io/; 33) Optimisation of space-time periodic eigenvalues; The goal of this paper is to provide a qualitative analysis of the
optimisation of space-time periodic principal eigenvalues. Namely, considering
a fixed time horizon $T$ and the $d$-dimensional torus $\mathbb{T}^d$, let, for
any $m\in L^\infty((0,T)\times\mathbb{T}^d)$, $\lambda(m)$ be the principal
eigenvalue of the operator $\partial_t-\Delta-m$ endowed with (time-space)
periodic boundary conditions. The main question we set out to answer is the
following: how to choose $m$ so as to minimise $\lambda(m)$? This question
stems from population dynamics. We prove that in several cases it is always
beneficial to rearrange $m$ with respect to time in a symmetric way, which is
the first comparison result for the rearrangement in time of parabolic
equations. Furthermore, we investigate the validity (or lack thereof) of
Talenti inequalities for the rearrangement in time of parabolic equations. The
numerical simulations which illustrate our results were obtained by developing
a framework within which it is possible to optimise criteria with respect to
functions having a prescribed rearrangement (or distribution function).; 34) On the Fly Adaptation of Behavior Tree-Based Policies through
  Reinforcement Learning; With the rising demand for flexible manufacturing, robots are increasingly
expected to operate in dynamic environments where local -- such as slight
offsets or size differences in workpieces -- are common. We propose to address
the problem of adapting robot behaviors to these task variations with a
sample-efficient hierarchical reinforcement learning approach adapting Behavior
Tree (BT)-based policies. We maintain the core BT properties as an
interpretable, modular framework for structuring reactive behaviors, but extend
their use beyond static tasks by inherently accommodating local task
variations. To show the efficiency and effectiveness of our approach, we
conduct experiments both in simulation and on a Franka Emika Panda 7-DoF, with
the manipulator adapting to different obstacle avoidance and pivoting tasks.; 35) Asynchronous Stochastic Block Projection Algorithm for Solving Linear
  Systems under Predefined Communication Patterns; Distributed computation over networks is now receiving an increasing
attention in many fields such as engineering and machine learning, where the
solution of a linear system of equations is a basic task. This paper presents
an asynchronous distributed randomized block Kaczmarz projection algorithm for
solving large-scale linear systems over a multi-agent networks, where each
agent only holds a part of the problem data. An event-triggered communication
mechanism is integrated to minimize the communication overhead and reduce the
overall communication costs. This communication mechanism allows each agent to
update independently in an asynchronous environment and dynamically regulate
communication frequency. In addition, this article analyzes the inefficiency
caused by communication in asynchronous algorithms, explores the potential of
event triggering mechanisms in alleviating these problems, and provides general
conditions for global convergence in such environments. Moreover, a modified
stochastic block Kaczmarz algorithm is used for each agent to update their
local estimate. Through rigorous mathematical analysis, the exponential
convergence rate of the proposed algorithm is established for a consistent
system and its computational efficiency, robustness, and communication
efficiency is validated through extensive numerical experiments. Furthermore,
to address inconsistent systems, the algorithm introduces auxiliary variables
to facilitate convergence toward an approximate least-squares solution,
accompanied by a formal error analysis. The experimental results demonstrate
that the algorithm maintains stability even under extreme asynchrony,
communication failures, and node failures, while achieving significantly lower
communication overhead and faster convergence rates compared to traditional
methods.; 36) Jasmine: Harnessing Diffusion Prior for Self-supervised Depth Estimation; In this paper, we propose Jasmine, the first Stable Diffusion (SD)-based
self-supervised framework for monocular depth estimation, which effectively
harnesses SD's visual priors to enhance the sharpness and generalization of
unsupervised prediction. Previous SD-based methods are all supervised since
adapting diffusion models for dense prediction requires high-precision
supervision. In contrast, self-supervised reprojection suffers from inherent
challenges (e.g., occlusions, texture-less regions, illumination variance), and
the predictions exhibit blurs and artifacts that severely compromise SD's
latent priors. To resolve this, we construct a novel surrogate task of hybrid
image reconstruction. Without any additional supervision, it preserves the
detail priors of SD models by reconstructing the images themselves while
preventing depth estimation from degradation. Furthermore, to address the
inherent misalignment between SD's scale and shift invariant estimation and
self-supervised scale-invariant depth estimation, we build the Scale-Shift GRU.
It not only bridges this distribution gap but also isolates the fine-grained
texture of SD output against the interference of reprojection loss. Extensive
experiments demonstrate that Jasmine achieves SoTA performance on the KITTI
benchmark and exhibits superior zero-shot generalization across multiple
datasets.; 37) PhysTwin: Physics-Informed Reconstruction and Simulation of Deformable
  Objects from Videos; Creating a physical digital twin of a real-world object has immense potential
in robotics, content creation, and XR. In this paper, we present PhysTwin, a
novel framework that uses sparse videos of dynamic objects under interaction to
produce a photo- and physically realistic, real-time interactive virtual
replica. Our approach centers on two key components: (1) a physics-informed
representation that combines spring-mass models for realistic physical
simulation, generative shape models for geometry, and Gaussian splats for
rendering; and (2) a novel multi-stage, optimization-based inverse modeling
framework that reconstructs complete geometry, infers dense physical
properties, and replicates realistic appearance from videos. Our method
integrates an inverse physics framework with visual perception cues, enabling
high-fidelity reconstruction even from partial, occluded, and limited
viewpoints. PhysTwin supports modeling various deformable objects, including
ropes, stuffed animals, cloth, and delivery packages. Experiments show that
PhysTwin outperforms competing methods in reconstruction, rendering, future
prediction, and simulation under novel interactions. We further demonstrate its
applications in interactive real-time simulation and model-based robotic motion
planning.; 38) Equilibrium phases and phase transitions in multicritical magnetic
  polymers; Magnetic polymers are examples of composite soft materials in which the
competition between the large configurational entropy of the soft substrate
(polymer) and the magnetic interaction may give rise to rich equilibrium phase
diagrams as well as non-standard critical phenomena. Here, we study a
self-avoiding walk model decorated by Ising spins of value $0$ and $\pm 1$ that
interact according to a Blume-Emery-Griffith-like Hamiltonian. By using
mean-field approximations and Monte Carlo simulations, we report the existence
of three distinct equilibrium phases: swollen disordered, compact ordered, and
compact disordered. Notably, these phases are separated by phase boundaries
that meet at multicritical points, whose nature and location are tunable and
depend on the strength of the interactions. In our conclusion, we discuss the
relevance of the phase diagrams we have obtained to the physics of magnetic
polymers and their application to chromatin biophysics.; 39) Flow Q-Learning; We present flow Q-learning (FQL), a simple and performant offline
reinforcement learning (RL) method that leverages an expressive flow-matching
policy to model arbitrarily complex action distributions in data. Training a
flow policy with RL is a tricky problem, due to the iterative nature of the
action generation process. We address this challenge by training an expressive
one-step policy with RL, rather than directly guiding an iterative flow policy
to maximize values. This way, we can completely avoid unstable recursive
backpropagation, eliminate costly iterative action generation at test time, yet
still mostly maintain expressivity. We experimentally show that FQL leads to
strong performance across 73 challenging state- and pixel-based OGBench and
D4RL tasks in offline RL and offline-to-online RL. Project page:
https://seohong.me/projects/fql/; 40) Passive Heart Rate Monitoring During Smartphone Use in Everyday Life; Resting heart rate (RHR) is an important biomarker of cardiovascular health
and mortality, but tracking it longitudinally generally requires a wearable
device, limiting its availability. We present PHRM, a deep learning system for
passive heart rate (HR) and RHR measurements during everyday smartphone use,
using facial video-based photoplethysmography. Our system was developed using
225,773 videos from 495 participants and validated on 185,970 videos from 205
participants in laboratory and free-living conditions, representing the largest
validation study of its kind. Compared to reference electrocardiogram, PHRM
achieved a mean absolute percentage error (MAPE) < 10% for HR measurements
across three skin tone groups of light, medium and dark pigmentation; MAPE for
each skin tone group was non-inferior versus the others. Daily RHR measured by
PHRM had a mean absolute error < 5 bpm compared to a wearable HR tracker, and
was associated with known risk factors. These results highlight the potential
of smartphones to enable passive and equitable heart health monitoring.; 41) On the Functional Dependence of Transition-Potential Coupled Cluster; Orbital relaxation of the core region is a primary source of error in the
computation of core ionization and core excitation energies. Recently,
Transition-Potential Coupled Cluster (TP-CC) methods have been used to
explicitly treat orbital relaxation using non-variational molecular orbitals
determined by reoccupation of orbitals optimized for a fractional core
occupation. The amount of fractional occupation is governed by parameter
$\lambda$, and recommended values for accurate TP-CCSD and XTP-CCSD
computations of carbon, nitrogen, oxygen, and fluorine 1s K-edges were
previously determined. Herein, we explore the performance of a several density
functionals for generating the fractionally occupied orbitals used in TP-CCSD.
These functionals include HF, BP86, BH&HLYP, B3LYP, M06-2X, and $\omega$B97m-V.
The fractionally occupied orbitals computed across the various functionals were
subsequently employed as the initial orbitals for our TP-CCSD calculations of
organic K-edge x-ray absorption and photoelectron spectra. Regardless of the
functional used to generate the fractionally occupied orbitals, the TP-CCSD
calculations yield accurate and comparable core ionization energies, core
excitation energies, and oscillator strengths.; 42) Discovering highly efficient low-weight quantum error-correcting codes
  with reinforcement learning; The realization of scalable fault-tolerant quantum computing is expected to
hinge on quantum error-correcting codes. In the quest for more efficient
quantum fault tolerance, a critical code parameter is the weight of
measurements that extract information about errors to enable error correction:
as higher measurement weights require higher implementation costs and introduce
more errors, it is important in code design to optimize measurement weight.
This underlies the surging interest in quantum low-density parity-check (qLDPC)
codes, the study of which has primarily focused on the asymptotic
(large-code-limit) properties. In this work, we introduce a versatile and
computationally efficient approach to stabilizer code weight reduction based on
reinforcement learning (RL), which produces new low-weight codes that
substantially outperform the state of the art in practically relevant parameter
regimes, extending significantly beyond previously accessible small distances.
For example, our approach demonstrates savings in physical qubit overhead
compared to existing results by 1 to 2 orders of magnitude for weight 6 codes
and brings the overhead into a feasible range for near-future experiments. We
also investigate the interplay between code parameters using our RL framework,
offering new insights into the potential efficiency and power of practically
viable coding strategies. Overall, our results demonstrate how RL can
effectively advance the crucial yet challenging problem of quantum code
discovery and thereby facilitate a faster path to the practical implementation
of fault-tolerant quantum technologies.; 43) Revisiting DRAM Read Disturbance: Identifying Inconsistencies Between
  Experimental Characterization and Device-Level Studies; Modern DRAM is vulnerable to read disturbance (e.g., RowHammer and RowPress)
that significantly undermines the robust operation of the system. Repeatedly
opening and closing a DRAM row (RowHammer) or keeping a DRAM row open for a
long period of time (RowPress) induces bitflips in nearby unaccessed DRAM rows.
Prior works on DRAM read disturbance either 1) perform experimental
characterization using commercial-off-the-shelf (COTS) DRAM chips to
demonstrate the high-level characteristics of the read disturbance bitflips, or
2) perform device-level simulations to understand the low-level error
mechanisms of the read disturbance bitflips.
  In this paper, we attempt to align and cross-validate the real-chip
experimental characterization results and state-of-the-art device-level studies
of DRAM read disturbance. To do so, we first identify and extract the key
bitflip characteristics of RowHammer and RowPress from the device-level error
mechanisms studied in prior works. Then, we perform experimental
characterization on 96 COTS DDR4 DRAM chips that directly match the data and
access patterns studied in the device-level works. Through our experiments, we
identify fundamental inconsistencies in the RowHammer and RowPress bitflip
directions and access pattern dependence between experimental characterization
results and the device-level error mechanisms.
  Based on our results, we hypothesize that either 1) the retention failure
based DRAM architecture reverse-engineering methodologies do not fully work on
modern DDR4 DRAM chips, or 2) existing device-level works do not fully uncover
all the major read disturbance error mechanisms. We hope our findings inspire
and enable future works to build a more fundamental and comprehensive
understanding of DRAM read disturbance.; 44) Thermodynamic limits of the Mpemba effect: A unified resource theory
  analysis; The Mpemba effect, a counterintuitive thermodynamic phenomenon in which a
hotter system cools more rapidly than a colder one, has been observed in both
classical and quantum systems. However, its fundamental mechanisms remain
inadequately understood. In this letter, we investigate the role of classical
and quantum correlations in driving anomalous relaxation behaviors within the
framework of quantum resource theories. Through an analysis of multi-qubit
systems in local thermal equilibrium, we establish that classical correlations
alone can give rise to the Mpemba effect, while quantum correlations become
relevant under specific energy degeneracy conditions. Furthermore, we
demonstrate that non-Markovian memory effects and Hilbert space dimensionality
play a crucial role in determining the temperature range over which this effect
manifests. Finally, we discuss the possibility that the original anomalous
cooling behavior observed in water may also arise from classical or quantum
correlations, offering new insights into the underlying mechanisms of the
phenomena collectively referred to as the Mpemba effect.; 45) A controllable theory of superconductivity due to strong repulsion in a
  polarized band; Can strong repulsive interaction be shown to give rise to pairing in a
controllable way? We find that for a single flavor polarized band, there is a
small expansion parameter in the low density limit, once the Bloch wavefunction
overlap is taken into account. A perturbative expansion is possible, even if
the interaction is much stronger than the Fermi energy $\epsilon_F$. We
illustrate our method with a two-band model that is often used to describe
multi-layer rhombohedral graphene and comment on the relationship with
experiments. This work opens a reliable pathway to search for topological
superconductors with high-$T_c$ (relative to $\epsilon_F$) in materials with
strongly interactions. We summarize the requirements and suggest some
illustrative design structures.; 46) Quantum synchronization of twin limit-cycle oscillators; Quantum synchronization has been a subject of intensive research in the last
decade. In this work, we propose a quantum Li\'enard system whose classical
equivalent features two limit cycles to one of which the system will converge.
In the quantum case, both limit cycles coexist in a single steady state. Each
of these limit cycles localizes to a distinct phase if coupled to an external
drive: one quantum state can thus be assigned two phases. Furthermore, coupling
two such oscillators leads to the simultaneous appearance of synchronization
and a synchronization blockade. To shed light on this apparent paradoxical
result, we introduce finer measures of quantum synchronization.; 47) Knowledge Augmentation in Federation: Rethinking What Collaborative
  Learning Can Bring Back to Decentralized Data; Data, as an observable form of knowledge, has become one of the most
important factors of production for the development of Artificial Intelligence
(AI). Meanwhile, increasing legislation and regulations on private and
proprietary information results in scattered data sources also known as the
""data islands"". Although some collaborative learning paradigms such as
Federated Learning (FL) can enable privacy-preserving training over
decentralized data, they have inherent deficiencies in fairness, costs and
reproducibility because of being learning-centric, which greatly limits the way
how participants cooperate with each other. In light of this, we present a
knowledge-centric paradigm termed Knowledge Augmentation in Federation (KAF),
with focus on how to enhance local knowledge through collaborative effort. We
provide the suggested system architecture, formulate the prototypical
optimization objective, and review emerging studies that employ methodologies
suitable for KAF. On our roadmap, with a three-way categorization we describe
the methods for knowledge expansion, knowledge filtering, and label and feature
space correction in the federation. Further, we highlight several challenges
and open questions that deserve more attention from the community. With our
investigation, we intend to offer new insights for what collaborative learning
can bring back to decentralized data.; 48) MM-Verify: Enhancing Multimodal Reasoning with Chain-of-Thought
  Verification; According to the Test-Time Scaling, the integration of External Slow-Thinking
with the Verify mechanism has been demonstrated to enhance multi-round
reasoning in large language models (LLMs). However, in the multimodal (MM)
domain, there is still a lack of a strong MM-Verifier. In this paper, we
introduce MM-Verifier and MM-Reasoner to enhance multimodal reasoning through
longer inference and more robust verification. First, we propose a two-step MM
verification data synthesis method, which combines a simulation-based tree
search with verification and uses rejection sampling to generate high-quality
Chain-of-Thought (COT) data. This data is then used to fine-tune the
verification model, MM-Verifier. Additionally, we present a more efficient
method for synthesizing MMCOT data, bridging the gap between text-based and
multimodal reasoning. The synthesized data is used to fine-tune MM-Reasoner.
Our MM-Verifier outperforms all larger models on the MathCheck, MathVista, and
MathVerse benchmarks. Moreover, MM-Reasoner demonstrates strong effectiveness
and scalability, with performance improving as data size increases. Finally,
our approach achieves strong performance when combining MM-Reasoner and
MM-Verifier, reaching an accuracy of 65.3 on MathVista, surpassing GPT-4o
(63.8) with 12 rollouts.; 49) Artificial Intelligence-Driven Prognostic Classification of COVID-19
  Using Chest X-rays: A Deep Learning Approach; Background: The COVID-19 pandemic has overwhelmed healthcare systems,
emphasizing the need for AI-driven tools to assist in rapid and accurate
patient prognosis. Chest X-ray imaging is a widely available diagnostic tool,
but existing methods for prognosis classification lack scalability and
efficiency. Objective: This study presents a high-accuracy deep learning model
for classifying COVID-19 severity (Mild, Moderate, and Severe) using Chest
X-ray images, developed on Microsoft Azure Custom Vision. Methods: Using a
dataset of 1,103 confirmed COVID-19 X-ray images from AIforCOVID, we trained
and validated a deep learning model leveraging Convolutional Neural Networks
(CNNs). The model was evaluated on an unseen dataset to measure accuracy,
precision, and recall. Results: Our model achieved an average accuracy of 97%,
with specificity of 99%, sensitivity of 87%, and an F1-score of 93.11%. When
classifying COVID-19 severity, the model achieved accuracies of 89.03% (Mild),
95.77% (Moderate), and 81.16% (Severe). These results demonstrate the model's
potential for real-world clinical applications, aiding in faster
decision-making and improved resource allocation. Conclusion: AI-driven
prognosis classification using deep learning can significantly enhance COVID-19
patient management, enabling early intervention and efficient triaging. Our
study provides a scalable, high-accuracy AI framework for integrating deep
learning into routine clinical workflows. Future work should focus on expanding
datasets, external validation, and regulatory compliance to facilitate clinical
adoption.; 50) Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms; Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability.; 51) From strong to weak correlations in breathing-mode kagome van der Waals
  materials: Nb$_3$(F,Cl,Br,I)$_8$ as a robust and versatile platform for
  many-body engineering; By combining ab initio downfolding with cluster dynamical mean-field theory,
we study the degree of correlations in the low-temperature structures of the
breathing-mode kagome van der Waals materials Nb$_3$(F,Cl,Br,I)$_8$. We show
that the Coulomb correlation strength steadily increases from I to Br, Cl, and
F, allowing us to identify Nb$_3$I$_8$ as a weakly correlated (obstructed
atomic) insulator whose gap is only mildly affected by the local Coulomb
interaction. Nb$_3$Br$_8$ and Nb$_3$Cl$_8$ are strongly correlated insulators,
whose gaps are significantly influenced by Coulomb-induced vertex corrections.
Nb$_3$F$_8$ is a prototypical bulk Mott-insulator whose gap is initially opened
by strong correlation effects. Angle-resolved photoemission spectroscopy
measurements comparing Nb$_3$Br$_8$ and Nb$_3$I$_8$ allow us to experimentally
confirm these findings by revealing spectroscopic footprints of the degree of
correlation. Our calculations further predict that the entire material family
can be tuned into correlated charge-transfer or Mott-insulating phases upon
electron or hole doping. Our magnetic property analysis additionally shows that
inter-layer magnetic frustrations in the high-temperature phase drive the
lattice phase transition to the low-temperature structures. The accompanying
bilayer hybridization through inter-layer dimerization yields magnetic singlet
ground states in the Cl, Br, and I compounds. Our findings establish
Nb$_3$X$_8$ as a robust, versatile, and tunable class for van der Waals-based
Coulomb and Mott engineering and allow us to speculate on the symmetry-breaking
effects necessary for the recently observed Josephson diode effect in
NbSe$_2$/Nb$_3$Br$_8$/NbSe$_2$ heterostructures.; 52) Music for All: Exploring Multicultural Representations in Music
  Generation Models; The advent of Music-Language Models has greatly enhanced the automatic music
generation capability of AI systems, but they are also limited in their
coverage of the musical genres and cultures of the world. We present a study of
the datasets and research papers for music generation and quantify the bias and
under-representation of genres. We find that only 5.7% of the total hours of
existing music datasets come from non-Western genres, which naturally leads to
disparate performance of the models across genres. We then investigate the
efficacy of Parameter-Efficient Fine-Tuning (PEFT) techniques in mitigating
this bias. Our experiments with two popular models -- MusicGen and Mustango,
for two underrepresented non-Western music traditions -- Hindustani Classical
and Turkish Makam music, highlight the promises as well as the non-triviality
of cross-genre adaptation of music through small datasets, implying the need
for more equitable baseline music-language models that are designed for
cross-cultural transfer learning.; 53) Eggly: Designing Mobile Augmented Reality Neurofeedback Training Games
  for Children with Autism Spectrum Disorder; Autism Spectrum Disorder (ASD) is a neurodevelopmental disorder that affects
how children communicate and relate to other people and the world around them.
Emerging studies have shown that neurofeedback training (NFT) games are an
effective and playful intervention to enhance social and attentional
capabilities for autistic children. However, NFT is primarily available in a
clinical setting that is hard to scale. Also, the intervention demands
deliberately-designed gamified feedback with fun and enjoyment, where little
knowledge has been acquired in the HCI community. Through a ten-month iterative
design process with four domain experts, we developed Eggly, a mobile NFT game
based on a consumer-grade EEG headband and a tablet. Eggly uses novel augmented
reality (AR) techniques to offer engagement and personalization, enhancing
their training experience. We conducted two field studies (a single-session
study and a three-week multi-session study) with a total of five autistic
children to assess Eggly in practice at a special education center. Both
quantitative and qualitative results indicate the effectiveness of the approach
as well as contribute to the design knowledge of creating mobile AR NFT games.; 54) Recurrent Auto-Encoders for Enhanced Deep Reinforcement Learning in
  Wilderness Search and Rescue Planning; Wilderness search and rescue operations are often carried out over vast
landscapes. The search efforts, however, must be undertaken in minimum time to
maximize the chance of survival of the victim. Whilst the advent of cheap
multicopters in recent years has changed the way search operations are handled,
it has not solved the challenges of the massive areas at hand. The problem
therefore is not one of complete coverage, but one of maximizing the
information gathered in the limited time available. In this work we propose
that a combination of a recurrent autoencoder and deep reinforcement learning
is a more efficient solution to the search problem than previous pure deep
reinforcement learning or optimisation approaches. The autoencoder training
paradigm efficiently maximizes the information throughput of the encoder into
its latent space representation which deep reinforcement learning is primed to
leverage. Without the overhead of independently solving the problem that the
recurrent autoencoder is designed for, it is more efficient in learning the
control task. We further implement three additional architectures for a
comprehensive comparison of the main proposed architecture. Similarly, we apply
both soft actor-critic and proximal policy optimisation to provide an insight
into the performance of both in a highly non-linear and complex application
with a large observation Results show that the proposed architecture is vastly
superior to the benchmarks, with soft actor-critic achieving the best
performance. This model further outperformed work from the literature whilst
having below a fifth of the total learnable parameters and training in a
quarter of the time.; 55) Decentralized and Robust Privacy-Preserving Model Using
  Blockchain-Enabled Federated Deep Learning in Intelligent Enterprises; In Federated Deep Learning (FDL), multiple local enterprises are allowed to
train a model jointly. Then, they submit their local updates to the central
server, and the server aggregates the updates to create a global model.
However, trained models usually perform worse than centralized models,
especially when the training data distribution is non-independent and
identically distributed (nonIID). NonIID data harms the accuracy and
performance of the model. Additionally, due to the centrality of federated
learning (FL) and the untrustworthiness of enterprises, traditional FL
solutions are vulnerable to security and privacy attacks. To tackle this issue,
we propose FedAnil, a secure blockchain enabled Federated Deep Learning Model
that improves enterprise models decentralization, performance, and tamper proof
properties, incorporating two main phases. The first phase addresses the nonIID
challenge (label and feature distribution skew). The second phase addresses
security and privacy concerns against poisoning and inference attacks through
three steps. Extensive experiments were conducted using the Sent140,
FashionMNIST, FEMNIST, and CIFAR10 new real world datasets to evaluate FedAnils
robustness and performance. The simulation results demonstrate that FedAnil
satisfies FDL privacy preserving requirements. In terms of convergence
analysis, the model parameter obtained with FedAnil converges to the optimum of
the model parameter. In addition, it performs better in terms of accuracy (more
than 11, 15, and 24%) and computation overhead (less than 8, 10, and 15%)
compared with baseline approaches, namely ShieldFL, RVPFL, and RFA.; 56) Embodied multi-modal sensing with a soft modular arm powered by physical
  reservoir computing; Soft robots have become increasingly popular for complex manipulation tasks
requiring gentle and safe contact. However, their softness makes accurate
control challenging, and high-fidelity sensing is a prerequisite to adequate
control performance. To this end, many flexible and embedded sensors have been
created over the past decade, but they inevitably increase the robot's
complexity and stiffness. This study demonstrates a novel approach that uses
simple bending strain gauges embedded inside a modular arm to extract complex
information regarding its deformation and working conditions. The core idea is
based on physical reservoir computing (PRC): A soft body's rich nonlinear
dynamic responses, captured by the inter-connected bending sensor network,
could be utilized for complex multi-modal sensing with a simple linear
regression algorithm. Our results show that the soft modular arm reservoir can
accurately predict body posture (bending angle), estimate payload weight,
determine payload orientation, and even differentiate two payloads with only
minimal difference in weight -- all using minimal digital computing power.; 57) Development of a Cost-Effective Simulation Tool for Loss of Flow
  Accident Transients in High-Temperature Gas-cooled Reactors; The aim of this work is to further expand the capability of the coarse-grid
Computational Fluid Dynamics (CFD) approach, SubChCFD, to effectively simulate
transient and buoyancy-influenced flows, which are critical in accident
analyses of High-Temperature Gas-cooled Reactors (HTGRs). It has been
demonstrated in our previous work that SubChCFD is highly adaptable to HTGR
fuel designs and performs exceptionally well in modelling steady-state
processes. In this study, the approach is extended to simulate a Loss of Flow
Accident (LOFA) transient, where coolant circulation is disrupted, causing the
transition from forced convection to buoyancy-driven natural circulation within
the reactor core. To enable SubChCFD to capture the complex physics involved,
corrections were introduced to the empirical correlations to account for the
effects of flow unsteadiness, property variation and buoyancy.
  A 1/12th sector of the reactor core, representing the smallest symmetric
unit, was modelled using a coarse mesh of approximately 60 million cells. This
mesh size is about 6% of that required for a Reynolds Averaged Navier Stokes
(RANS) model, where mesh sizes can typically reach the order of 1 billion cells
for such configurations. Simulation results show that SubChCFD effectively
captures the thermal hydraulic behaviours of the reactor during a LOFA
transient, producing predictions in good agreement with RANS simulations while
significantly reducing computational cost.; 58) Panoptic-CUDAL Technical Report: Rural Australia Point Cloud Dataset in
  Rainy Conditions; Existing autonomous driving datasets are predominantly oriented towards
well-structured urban settings and favorable weather conditions, leaving the
complexities of rural environments and adverse weather conditions largely
unaddressed. Although some datasets encompass variations in weather and
lighting, bad weather scenarios do not appear often. Rainfall can significantly
impair sensor functionality, introducing noise and reflections in LiDAR and
camera data and reducing the system's capabilities for reliable environmental
perception and safe navigation. We introduce the Panoptic-CUDAL dataset, a
novel dataset purpose-built for panoptic segmentation in rural areas subject to
rain. By recording high-resolution LiDAR, camera, and pose data, Panoptic-CUDAL
offers a diverse, information-rich dataset in a challenging scenario. We
present analysis of the recorded data and provide baseline results for panoptic
and semantic segmentation methods on LiDAR point clouds. The dataset can be
found here:
https://robotics.sydney.edu.au/our-research/intelligent-transportation-systems/; 59) Machine Learning Calabi-Yau Three-Folds, Four-Folds, and Five-Folds; In this manuscript, we demonstrate, by using several regression techniques,
that one can machine learn the other independent Hodge numbers of complete
intersection Calabi-Yau four-folds and five-folds in terms of $h^{1,1}$ and
$h^{2,1}$. Consequently, we combine the Hodge numbers $h^{1,1}$ and $h^{2,1}$
from the complete intersection of Calabi-Yau three-folds, four-folds, and
five-folds into a single dataset. We then implemented various classification
algorithms on this dataset. For example, the accuracy of the Gaussian process
and the naive Bayes classifications are all $100\%$ when a binary
classification of three-folds and four-folds is performed. With the Support
Vector Machine (SVM) algorithm plots, a special corner is detected in the
Calabi-Yau three-folds landscape (characterized by $17\leq h^{1,1}\leq 30$ and
$20\leq h^{2,1}\leq 40$) when multiclass classification is performed.
Furthermore, the best accuracy, $0.996459$, in classifying Calabi-Yau
three-folds, four-folds, and five-folds, is obtained with the naive Bayes
classification.; 60) Entanglement entropy evolution during gravitational collapse; We investigate the dynamics of the ground state entanglement entropy for a
discretized scalar field propagating within the Oppenheimer-Snyder collapse
metric. Starting from a well-controlled initial configuration, we follow the
system as it evolves toward the formation of a horizon and, eventually, a
singularity. Our approach employs an Ermakov-like equation to determine the
time-dependent ground state of the field and calculates the resulting
entanglement entropy by tracing out the degrees of freedom inside a spherical
region within the matter sphere. We find that the entanglement entropy exhibits
nontrivial scaling and time dependence during collapse. Close to the horizon,
the entropy can deviate from the simple area law, reflecting the rapid changes
in geometry and field configuration. Although the model is idealized, these
results provide insights into the generation and scaling of entanglement in the
presence of realistic, dynamically evolving gravitational fields.; 61) DIAL: Distribution-Informed Adaptive Learning of Multi-Task Constraints
  for Safety-Critical Systems; Safe reinforcement learning has traditionally relied on predefined constraint
functions to ensure safety in complex real-world tasks, such as autonomous
driving. However, defining these functions accurately for varied tasks is a
persistent challenge. Recent research highlights the potential of leveraging
pre-acquired task-agnostic knowledge to enhance both safety and sample
efficiency in related tasks. Building on this insight, we propose a novel
method to learn shared constraint distributions across multiple tasks. Our
approach identifies the shared constraints through imitation learning and then
adapts to new tasks by adjusting risk levels within these learned
distributions. This adaptability addresses variations in risk sensitivity
stemming from expert-specific biases, ensuring consistent adherence to general
safety principles even with imperfect demonstrations. Our method can be applied
to control and navigation domains, including multi-task and meta-task
scenarios, accommodating constraints such as maintaining safe distances or
adhering to speed limits. Experimental results validate the efficacy of our
approach, demonstrating superior safety performance and success rates compared
to baselines, all without requiring task-specific constraint definitions. These
findings underscore the versatility and practicality of our method across a
wide range of real-world tasks.; 62) Curiosity-Driven Imagination: Discovering Plan Operators and Learning
  Associated Policies for Open-World Adaptation; Adapting quickly to dynamic, uncertain environments-often called ""open
worlds""-remains a major challenge in robotics. Traditional Task and Motion
Planning (TAMP) approaches struggle to cope with unforeseen changes, are
data-inefficient when adapting, and do not leverage world models during
learning. We address this issue with a hybrid planning and learning system that
integrates two models: a low level neural network based model that learns
stochastic transitions and drives exploration via an Intrinsic Curiosity Module
(ICM), and a high level symbolic planning model that captures abstract
transitions using operators, enabling the agent to plan in an ""imaginary"" space
and generate reward machines. Our evaluation in a robotic manipulation domain
with sequential novelty injections demonstrates that our approach converges
faster and outperforms state-of-the-art hybrid methods.; 63) Composite Nonlinear Trajectory Tracking Control of Co-Driving Vehicles
  Using Self-Triggered Adaptive Dynamic Programming; This article presents a composite nonlinear feedback (CNF) control method
using self-triggered (ST) adaptive dynamic programming (ADP) algorithm in a
human-machine shared steering framework. For the overall system dynamics, a
two-degrees-of-freedom (2-DOF) vehicle model is established and a two-point
preview driver model is adopted. A dynamic authority allocation strategy based
on cooperation level is proposed to combine the steering input of the human
driver and the automatic controller. To make further improvements in the
controller design, three main contributions are put forward. Firstly, the CNF
controller is designed for trajectory tracking control with refined transient
performance. Besides, the self-triggered rule is applied such that the system
will update in discrete times to save computing resources and increase
efficiency. Moreover, by introducing the data-based ADP algorithm, the optimal
control problem can be solved through iteration using system input and output
information, reducing the need for accurate knowledge of system dynamics. The
effectiveness of the proposed control method is validated through
Carsim-Simulink co-simulations in diverse driving scenarios.; 64) Observation of $D^+\to \bar K_1(1270)^0\mu^+\nu_\mu$ and $D^0\to
  K_1(1270)^-\mu^+\nu_\mu$; By analyzing 7.93 $\rm fb^{-1}$ of $e^+e^-$ collision data collected at the
center-of-mass energy of 3.773 GeV with the BESIII detector operated at the
BEPCII collider, we report the observation of the semimuonic decays of $D^+\to
\bar K_1(1270)^0\mu^+\nu_\mu$ and $D^0\to K_1(1270)^-\mu^+\nu_\mu$ with
statistical significances of $12.5\sigma$ and $6.0\sigma$, respectively. Their
decay branching fractions are determined to be ${\mathcal B}[D^{+}\to
\bar{K}_1(1270)^0 \mu^{+}\nu_{\mu}]=(2.36\pm0.20^{+0.18}_{-0.27}\pm
0.48)\times10^{-3}$ and ${\mathcal B}[D^{0}\to K_1(1270)^{-}
\mu^{+}\nu_{\mu}]=(0.78\pm0.11^{+0.05}_{-0.09}\pm 0.15)\times10^{-3}$, where
the first and second uncertainties are statistical and systematic,
respectively, and the third originates from the input branching fraction of
$\bar K_{1}(1270)^0\to K^- \pi^+\pi^0$ or $K_1(1270)^-\to K^-\pi^+\pi^-$.
Combining our branching fractions with the previous measurements of ${\mathcal
B}[D^+\to \bar K_1(1270)^0e^+\nu_{e}]$ and ${\mathcal B}[D^0\to
K_1(1270)^-e^+\nu_{e}]$, we determine the branching fraction ratios to be
${\mathcal B}[D^+\to \bar K_1(1270)^0\mu^+\nu_{\mu}]/{\mathcal B}[D^+\to \bar
K_1(1270)^0e^+\nu_{e}]=1.03 \pm 0.14 \substack{+0.11\\-0.15}$ and ${\mathcal
B}[D^0\to K_1(1270)^-\mu^+\nu_{\mu}]/{\mathcal B}[D^0\to
K_1(1270)^-e^+\nu_{e}]=0.74\pm 0.13 \substack{+0.08\\-0.13}$. Using the
branching fractions measured in this work and the world-average lifetimes of
the $D^+$ and $D^0$ mesons, we determine the semimuonic partial decay width
ratio to be $\Gamma [D^+\to \bar K_1(1270)^0 \mu^+\nu_\mu]/\Gamma [D^0\to
K_1(1270)^- \mu^+\nu_\mu]=1.22\pm 0.10\substack{+0.06\\-0.09}$, which is
consistent with unity as predicted by isospin conservation.; 65) Rate-Aware Learned Speech Compression; The rapid rise of real-time communication and large language models has
significantly increased the importance of speech compression. Deep
learning-based neural speech codecs have outperformed traditional signal-level
speech codecs in terms of rate-distortion (RD) performance. Typically, these
neural codecs employ an encoder-quantizer-decoder architecture, where audio is
first converted into latent code feature representations and then into discrete
tokens. However, this architecture exhibits insufficient RD performance due to
two main drawbacks: (1) the inadequate performance of the quantizer,
challenging training processes, and issues such as codebook collapse; (2) the
limited representational capacity of the encoder and decoder, making it
difficult to meet feature representation requirements across various bitrates.
In this paper, we propose a rate-aware learned speech compression scheme that
replaces the quantizer with an advanced channel-wise entropy model to improve
RD performance, simplify training, and avoid codebook collapse. We employ
multi-scale convolution and linear attention mixture blocks to enhance the
representational capacity and flexibility of the encoder and decoder.
Experimental results demonstrate that the proposed method achieves
state-of-the-art RD performance, obtaining 53.51% BD-Rate bitrate saving in
average, and achieves 0.26 BD-VisQol and 0.44 BD-PESQ gains.; 66) Latency-Aware Resource Allocation for Integrated Communications,
  Computation, and Sensing in Cell-Free mMIMO Systems; In this paper, we investigate a cell-free massive multiple-input and
multiple-output (MIMO)-enabled integration communication, computation, and
sensing (ICCS) system, aiming to minimize the maximum computation latency to
guarantee the stringent sensing requirements. We consider a two-tier offloading
framework, where each multi-antenna terminal can optionally offload its local
tasks to either multiple mobile-edge servers for distributed computation or the
cloud server for centralized computation while satisfying the sensing
requirements and power constraint. The above offloading problem is formulated
as a mixed-integer programming and non-convex problem, which can be decomposed
into three sub-problems, namely, distributed offloading decision, beamforming
design, and execution scheduling mechanism. First, the continuous relaxation
and penalty-based techniques are applied to tackle the distributed offloading
strategy. Then, the weighted minimum mean square error (WMMSE) and successive
convex approximation (SCA)-based lower bound are utilized to design the
integrated communication and sensing (ISAC) beamforming. Finally, the other
resources can be judiciously scheduled to minimize the maximum latency. A
rigorous convergence analysis and numerical results substantiate the
effectiveness of our method. Furthermore, simulation results demonstrate that
multi-point cooperation in cell-free massive MIMO-enabled ICCS significantly
reduces overall computation latency, in comparison to the benchmark schemes.; 67) Bandit Multiclass List Classification; We study the problem of multiclass list classification with (semi-)bandit
feedback, where input examples are mapped into subsets of size $m$ of a
collection of $K$ possible labels, and the feedback consists of the predicted
labels which lie in the set of true labels of the given example. Our main
result is for the $(\varepsilon,\delta)$-PAC variant of the problem for which
we design an algorithm that returns an $\varepsilon$-optimal hypothesis with
high probability using a sample complexity of $O \big( (\mathrm{poly}(K/m) + sm
/ \varepsilon^2) \log (|H|/\delta) \big)$ where $H$ is the underlying (finite)
hypothesis class and $s$ is an upper bound on the number of true labels for a
given example. This bound improves upon known bounds for combinatorial
semi-bandits whenever $s \ll K$. Moreover, in the regime where $s = O(1)$ the
leading terms in our bound match the corresponding full-information rates,
implying that bandit feedback essentially comes at no cost. Our PAC learning
algorithm is also computationally efficient given access to an ERM oracle for
$H$. Additionally, we consider the regret minimization setting where data can
be generated adversarially, and establish a regret bound of $\widetilde O(|H| +
\sqrt{smT \log |H|})$. Our results generalize and extend those of Erez et al.
(2024) who consider the simpler single-label setting corresponding to $s=m=1$,
and in fact hold for the more general contextual combinatorial semi-bandit
problem with $s$-sparse rewards.; 68) Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with
  Leading Short-Context Accuracy; We introduce Long-VITA, a simple yet effective large multi-modal model for
long-context visual-language understanding tasks. It is adept at concurrently
processing and analyzing modalities of image, video, and text over 4K frames or
1M tokens while delivering advanced performances on short-context multi-modal
tasks. We propose an effective multi-modal training schema that starts with
large language models and proceeds through vision-language alignment, general
knowledge learning, and two sequential stages of long-sequence fine-tuning. We
further implement context-parallelism distributed inference and logits-masked
language modeling head to scale Long-VITA to infinitely long inputs of images
and texts during model inference. Regarding training data, Long-VITA is built
on a mix of 17M samples from public datasets only and demonstrates the
state-of-the-art performance on various multi-modal benchmarks, compared
against recent cutting-edge models with internal data. Long-VITA is fully
reproducible and supports both NPU and GPU platforms for training and testing.
By leveraging our inference designs, Long-VITA models achieve a remarkable 2x
prefill speedup and 4x context length extension in single node with 8 GPUs. We
hope Long-VITA can serve as a competitive baseline and offer valuable insights
for the open-source community in advancing long-context multi-modal
understanding.; 69) Cracking the PUMA Challenge in 24 Hours with CellViT++ and nnU-Net; Automatic tissue segmentation and nuclei detection is an important task in
pathology, aiding in biomarker extraction and discovery. The panoptic
segmentation of nuclei and tissue in advanced melanoma (PUMA) challenge aims to
improve tissue segmentation and nuclei detection in melanoma histopathology.
Unlike many challenge submissions focusing on extensive model tuning, our
approach emphasizes delivering a deployable solution within a 24-hour
development timeframe, using out-of-the-box frameworks. The pipeline combines
two models, namely CellViT++ for nuclei detection and nnU-Net for tissue
segmentation. Our results demonstrate a significant improvement in tissue
segmentation, achieving a Dice score of 0.750, surpassing the baseline score of
0.629. For nuclei detection, we obtained results comparable to the baseline in
both challenge tracks. The code is publicly available at
https://github.com/TIO-IKIM/PUMA.; 70) Facies Classification with Copula Entropy; In this paper we propose to apply copula entropy (CE) to facies
classification. In our method, the correlations between geological variables
and facies classes are measured with CE and then the variables associated with
large negative CEs are selected for classification. We verified the proposed
method on a typical facies dataset for facies classification and the
experimental results show that the proposed method can select less geological
variables for facies classification without sacrificing classification
performance. The geological variables such selected are also interpretable to
geologists with geological meanings due to the rigorous definition of CE.; 71) Constraining electromagnetic couplings of ultralight scalars from
  compact stars; If an ultralight scalar interacts with the electromagnetic fields of a
compact rotating star, then a long-range scalar field is developed outside the
star. The Coulomb-like profile of the scalar field is equivalent to an
effective scalar charge on the star. In a binary star system, the
scalar-induced charge would result in a long-range force between the stars,
with the scalar field acting as the mediator. The scalar-photon interactions
would modify Maxwell's equations for electromagnetic fields in vacuum,
resulting in a modified dispersion relation. This could be observed as an
apparent redshift for photons emitted by such sources. The scalar field would
also induce additional electric and magnetic fields and hence affect the
electromagnetic energy radiated from such compact objects. A scalar field
sourced by time-varying electromagnetic fields can also carry away energy from
a compact star in the form of radiation, and hence contribute to its spin-down
luminosity. We constrain the scalar-photon coupling from the measurements of
the electromagnetic radiation of the compact star and its spin-down luminosity.
We also project the prospective bounds on these couplings with future
measurements of the apparent redshifts of compact stars and of the long-range
force between two magnetars in a binary. We analyze the systems of the binary
pulsar PSR J0737-3039, the Crab pulsar, the soft gamma repeater SGR 1806-20,
and the gamma ray burst GRB 080905A. The bounds on the coupling can be
significantly improved by future measurements of compact stars with large
magnetic fields, experiments with better sensitivity, and precision clock
measurements.; 72) Peculiar Rainbows in Saturn's E Ring: Uncovering Luminous Bands Near
  Enceladus; We report observations of stripe-like features in Enceladus' plumes captured
simultaneously by Cassini's VIMS-IR and ISS NAC instruments during flyby E17,
with similar patterns seen in VIMS-IR data from flyby E13 and E19. These
parallel stripes, inclined at approximately 16$^{\circ}$ to the ecliptic and
43$^{\circ}$ to Saturn's ring plane, appear continuous across images when
projected in the J2000 frame. A bright stripe, most visible at wavelengths
around 5 $\mu$m, acts as the zeroth-order diffraction peak of a reflection
grating with an estimated groove spacing of 0.12$-$2.60 mm, while adjacent
stripes are attributed to higher-order diffraction peaks. We suggest that this
light-dispersing phenomenon originates from an inclined periodic structure
within Saturn's E ring. This structure, constrained between Saturn's G-ring and
Rhea's orbit, likely consists of fresh ice particles supplied by Enceladus'
plumes.; 73) Physical knowledge improves prediction of EM Fields; We propose a 3D U-Net model to predict the spatial distribution of
electromagnetic fields inside a radio-frequency (RF) coil with a subject
present, using the phase, amplitude, and position of the coils, along with the
density, permittivity, and conductivity of the surrounding medium as inputs. To
improve accuracy, we introduce a physics-augmented variant, U-Net Phys, which
incorporates Gauss's law of magnetism into the loss function using finite
differences. We train our models on electromagnetic field simulations from CST
Studio Suite for an eight-channel dipole array RF coil at 7T MRI. Experimental
results show that U-Net Phys significantly outperforms the standard U-Net,
particularly in predicting fields within the subject, demonstrating the
advantage of integrating physical constraints into deep learning-based field
prediction.; 74) GPU Memory Usage Optimization for Backward Propagation in Deep Network
  Training; In modern Deep Learning, it has been a trend to design larger Deep Neural
Networks (DNNs) for the execution of more complex tasks and better accuracy. On
the other hand, Convolutional Neural Networks (CNNs) have become the standard
method for most of computer vision tasks. However, the memory allocation for
the intermediate data in convolution layers can cause severe memory pressure
during model training. Many solutions have been proposed to resolve the
problem. Besides hardware-dependent solutions, a general methodology
rematerialization can reduce GPU memory usage by trading computation for memory
efficiently. The idea is to select a set of intermediate results during the
forward phase as checkpoints, and only save them in memory to reduce memory
usage. The backward phase recomputes the intermediate data from the closest
checkpoints in memory as needed. This recomputation increases execution time
but saves memory by not storing all intermediate results in memory during the
forward phase. In this paper, we will focus on efficiently finding the optimal
checkpoint subset to achieve the least peak memory usage during the model
training. We first describe the theoretical background of the training of a
neural network using mathematical equations. We use these equations to identify
all essential data required during both forward and backward phases to compute
the gradient of weights of the model. We first identify the checkpoint
selection problem and propose a dynamic programming algorithm with time
complexity O(n3) to solve the problem of finding the optimal checkpoint subset.
With extensive experiments, we formulate a more accurate description of the
problem using our theoretical analysis and revise the objective function based
on the tracing, and propose an O(n)-time algorithm for finding the optimal
checkpoint subset.; 75) Controlled Floquet Dynamics and Topological Bound States in Continuum
  via Colored Quantum Random Walks; We demonstrate the emergence and control of Floquet states and topological
bound states in the continuum (TBICs) in a two-dimensional colored quantum
random walk (cQRW) on a square lattice. By introducing three internal degrees
of freedom-termed ""colors""-and leveraging SU(3) group representations, we
realize dispersive TBICs and intrinsic Floquet dynamics without the need for
external periodic driving. Through Chern number calculations, we identify three
distinct topological bands, revealing color-induced band mixing as a key
mechanism underlying the natural formation of Floquet states. The cQRW
framework enables precise tuning of quasi-energy spectra, supporting the
emergence of localized edge states in topological band gaps and dispersive
TBICs embedded within the bulk of other bands. These TBICs exhibit tunable
group velocity, controllable excitation across energy regimes, and robustness,
providing theoretical validation for their existence in a first-order Floquet
system. Our findings position cQRWs as a powerful platform for investigating
and harnessing TBICs and Floquet states, with potential applications in quantum
information and communication technologies.; 76) When the Future Becomes the Past: Taming Temporal Correspondence for
  Self-supervised Video Representation Learning; The past decade has witnessed notable achievements in self-supervised
learning for video tasks. Recent efforts typically adopt the Masked Video
Modeling (MVM) paradigm, leading to significant progress on multiple video
tasks. However, two critical challenges remain: 1) Without human annotations,
the random temporal sampling introduces uncertainty, increasing the difficulty
of model training. 2) Previous MVM methods primarily recover the masked patches
in the pixel space, leading to insufficient information compression for
downstream tasks. To address these challenges jointly, we propose a
self-supervised framework that leverages Temporal Correspondence for video
Representation learning (T-CoRe). For challenge 1), we propose a sandwich
sampling strategy that selects two auxiliary frames to reduce reconstruction
uncertainty in a two-side-squeezing manner. Addressing challenge 2), we
introduce an auxiliary branch into a self-distillation architecture to restore
representations in the latent space, generating high-level semantic
representations enriched with temporal information. Experiments of T-CoRe
consistently present superior performance across several downstream tasks,
demonstrating its effectiveness for video representation learning. The code is
available at https://github.com/yafeng19/T-CORE.; 77) Causality violation of Schr\""{o}dinger-Newton equation: direct test on
  the horizon?; We quote a definitive simple proof that neither classical stochastic dynamics
nor quantum dynamics can be nonlinear if we stick to their standard statistical
interpretations. A recently proposed optomechanical test of gravity's
classicality versus quantumness is based on the nonlinear Schr\""odinger-Newton
equation (SNE) which is the nonrelativistic limit of standard semiclassical
gravity. While in typical cosmological applications of semiclassical gravity
the predicted violation of causality is ignored, it cannot be disregarded in
applications of the SNE in high sensitive laboratory tests hoped for the coming
years. We reveal that, in a recently designed experiment, quantum optical
monitoring of massive probes predicts fake action-at-a-distance (acausality) on
a single probe already. The proposed experiment might first include the direct
test of this acausality.; 78) A Nonlocal size modified Poisson-Boltzmann Model and Its Finite Element
  Solver for Protein in Multi-Species Ionic Solution; The Poisson-Boltzmann (PB) model is a widely used implicit solvent model in
protein simulations. Although variants, such as the size modified PB and
nonlocal modified PB models, have been developed to account for ionic size
effects and nonlocal dielectric correlations, no existing PB variants
simultaneously incorporate both, due to significant modeling and computational
challenges. To address this gap, in this paper, a nonlocal size modified PB
(NSMPB) model is introduced and solved using a finite element method for a
protein with a three-dimensional molecular structure and an ionic solution
containing multiple ion species. In particular, a novel solution decomposition
is proposed to overcome the difficulties caused by the increased nonlinearity,
nonlocality, and solution singularities of the model. It is then applied to the
development of the NSMPB finite element solver, which includes an efficient
modified Newton iterative method, an effective damping parameter selection
strategy, and good selections of initial iterations. Moreover, the construction
of the modified Newton iterative method is mathematically justified.
Furthermore, an NSMPB finite element package is developed by integrating a mesh
generation tool, a protein data bank file retrieval program, and the PDB2PQR
package to simplify and accelerate its usage and application. Finally,
numerical experiments are conducted on an ionic solution with four species,
proteins with up to 11439 atoms, and irregular interface-fitted tetrahedral box
meshes with up to 1188840 vertices. The numerical results confirm the fast
convergence and strong robustness of the modified Newton iterative method,
demonstrate the high performance of the package, and highlight the crucial
roles played by the damping parameter and initial iteration selections in
enhancing the method's convergence. The package will be a valuable tool in
protein simulations.; 79) Physics-Informed Deep B-Spline Networks for Dynamical Systems; Physics-informed machine learning provides an approach to combining data and
governing physics laws for solving complex partial differential equations
(PDEs). However, efficiently solving PDEs with varying parameters and changing
initial conditions and boundary conditions (ICBCs) with theoretical guarantees
remains an open challenge. We propose a hybrid framework that uses a neural
network to learn B-spline control points to approximate solutions to PDEs with
varying system and ICBC parameters. The proposed network can be trained
efficiently as one can directly specify ICBCs without imposing losses,
calculate physics-informed loss functions through analytical formulas, and
requires only learning the weights of B-spline functions as opposed to both
weights and basis as in traditional neural operator learning methods. We
provide theoretical guarantees that the proposed B-spline networks serve as
universal approximators for the set of solutions of PDEs with varying ICBCs
under mild conditions and establish bounds on the generalization errors in
physics-informed learning. We also demonstrate in experiments that the proposed
B-spline network can solve problems with discontinuous ICBCs and outperforms
existing methods, and is able to learn solutions of 3D dynamics with diverse
initial conditions.; 80) Metering Error Estimation of Fast-Charging Stations Using Charging Data
  Analytics; Accurate electric energy metering (EEM) of fast charging stations (FCSs),
serving as critical infrastructure in the electric vehicle (EV) industry and as
significant carriers of vehicle-to-grid (V2G) technology, is the cornerstone
for ensuring fair electric energy transactions. Traditional on-site
verification methods, constrained by their high costs and low efficiency,
struggle to keep pace with the rapid global expansion of FCSs. In response,
this paper adopts a data-driven approach and proposes the measuring performance
comparison (MPC) method. By utilizing the estimation value of state-of-charge
(SOC) as a medium, MPC establishes comparison chains of EEM performance of
multiple FCSs. Therefore, the estimation of EEM errors for FCSs with high
efficiency is enabled. Moreover, this paper summarizes the interfering factors
of estimation results and establishes corresponding error models and
uncertainty models. Also, a method for discriminating whether there are EEM
performance defects in FCSs is proposed. Finally, the feasibility of MPC method
is validated, with results indicating that for FCSs with an accuracy grade of
2\%, the discriminative accuracy exceeds 95\%. The MPC provides a viable
approach for the online monitoring of EEM performance for FCSs, laying a
foundation for a fair and just electricity trading market.; 81) Technical Report: Generating the WEB-IDS23 Dataset; Anomaly-based Network Intrusion Detection Systems (NIDS) require correctly
labelled, representative and diverse datasets for an accurate evaluation and
development. However, several widely used datasets do not include labels which
are fine-grained enough and, together with small sample sizes, can lead to
overfitting issues that also remain undetected when using test data.
Additionally, the cybersecurity sector is evolving fast, and new attack
mechanisms require the continuous creation of up-to-date datasets. To address
these limitations, we developed a modular traffic generator that can simulate a
wide variety of benign and malicious traffic. It incorporates multiple
protocols, variability through randomization techniques and can produce attacks
along corresponding benign traffic, as it occurs in real-world scenarios. Using
the traffic generator, we create a dataset capturing over 12 million samples
with 82 flow-level features and 21 fine-grained labels. Additionally, we
include several web attack types which are often underrepresented in other
datasets.; 82) Compression and Distillation of Data Quadruplets in Non-intrusive
  Reduced-order Modeling; This paper introduces a quadrature-free, data-driven approach to balanced
truncation for both continuous-time and discrete-time systems. The method
non-intrusively constructs reduced-order models using available transfer
function samples from the right half of the $s$-plane. It is highlighted that
the proposed data-driven balanced truncation and existing quadrature-based
balanced truncation algorithms share a common feature: both compress their
respective data quadruplets to derive reduced-order models. Additionally, it is
demonstrated that by using different compression strategies, these quadruplets
can be utilized to develop three data-driven formulations of the IRKA. These
formulations non-intrusively generate near-optimal reduced models using
transfer function samples from the $j\omega$-axis or the right half of the
$s$-plane, or impulse response samples. Notably, these IRKA formulations
eliminate the necessity of computing new transfer function samples as IRKA
iteratively updates the sampling points. The results are also extended to
discrete-time systems. The efficacy of the proposed algorithms is validated
through numerical examples, which show that the proposed data-driven approaches
perform comparably to their intrusive counterparts.; 83) On Storage Neural Network Augmented Approximate Nearest Neighbor Search; Large-scale approximate nearest neighbor search (ANN) has been gaining
attention along with the latest machine learning researches employing ANNs. If
the data is too large to fit in memory, it is necessary to search for the most
similar vectors to a given query vector from the data stored in storage
devices, not from that in memory. The storage device such as NAND flash memory
has larger capacity than the memory device such as DRAM, but they also have
larger latency to read data. Therefore, ANN methods for storage require
completely different approaches from conventional in-memory ANN methods. Since
the approximation that the time required for search is determined only by the
amount of data fetched from storage holds under reasonable assumptions, our
goal is to minimize it while maximizing recall. For partitioning-based ANNs,
vectors are partitioned into clusters in the index building phase. In the
search phase, some of the clusters are chosen, the vectors in the chosen
clusters are fetched from storage, and the nearest vector is retrieved from the
fetched vectors. Thus, the key point is to accurately select the clusters
containing the ground truth nearest neighbor vectors. We accomplish this by
proposing a method to predict the correct clusters by means of a neural network
that is gradually refined by alternating supervised learning and duplicated
cluster assignment. Compared to state-of-the-art SPANN and an exhaustive method
using k-means clustering and linear search, the proposed method achieves 90%
recall on SIFT1M with 80% and 58% less data fetched from storage, respectively.; 84) Adaptive routing protocols for determining optimal paths in AI
  multi-agent systems: a priority- and learning-enhanced approach; As distributed artificial intelligence (AI) and multi-agent architectures
grow increasingly complex, the need for adaptive, context-aware routing becomes
paramount. This paper introduces an enhanced, adaptive routing algorithm
tailored for AI multi-agent networks, integrating priority-based cost functions
and dynamic learning mechanisms. Building on an extended Dijkstra-based
framework, we incorporate multi-faceted parameters such as task complexity,
user request priority, agent capabilities, bandwidth, latency, load, model
sophistication, and reliability. We further propose dynamically adaptive
weighting factors, tuned via reinforcement learning (RL), to continuously
evolve routing policies based on observed network performance. Additionally,
heuristic filtering and hierarchical routing structures improve scalability and
responsiveness. Our approach yields context-sensitive, load-aware, and
priority-focused routing decisions that not only reduce latency for critical
tasks but also optimize overall resource utilization, ultimately enhancing the
robustness, flexibility, and efficiency of multi-agent systems.; 85) USAM-Net: A U-Net-based Network for Improved Stereo Correspondence and
  Scene Depth Estimation using Features from a Pre-trained Image Segmentation
  network; The increasing demand for high-accuracy depth estimation in autonomous
driving and augmented reality applications necessitates advanced neural
architectures capable of effectively leveraging multiple data modalities. In
this context, we introduce the Unified Segmentation Attention Mechanism Network
(USAM-Net), a novel convolutional neural network that integrates stereo image
inputs with semantic segmentation maps and attention to enhance depth
estimation performance. USAM-Net employs a dual-pathway architecture, which
combines a pre-trained segmentation model (SAM) and a depth estimation model.
The segmentation pathway preprocesses the stereo images to generate semantic
masks, which are then concatenated with the stereo images as inputs to the
depth estimation pathway. This integration allows the model to focus on
important features such as object boundaries and surface textures which are
crucial for accurate depth perception. Empirical evaluation on the
DrivingStereo dataset demonstrates that USAM-Net achieves superior performance
metrics, including a Global Difference (GD) of 3.61\% and an End-Point Error
(EPE) of 0.88, outperforming traditional models such as CFNet, SegStereo, and
iResNet. These results underscore the effectiveness of integrating segmentation
information into stereo depth estimation tasks, highlighting the potential of
USAM-Net in applications demanding high-precision depth data.; 86) Dataset Distillation via Committee Voting; Dataset distillation aims to synthesize a smaller, representative dataset
that preserves the essential properties of the original data, enabling
efficient model training with reduced computational resources. Prior work has
primarily focused on improving the alignment or matching process between
original and synthetic data, or on enhancing the efficiency of distilling large
datasets. In this work, we introduce ${\bf C}$ommittee ${\bf V}$oting for ${\bf
D}$ataset ${\bf D}$istillation (CV-DD), a novel and orthogonal approach that
leverages the collective wisdom of multiple models or experts to create
high-quality distilled datasets. We start by showing how to establish a strong
baseline that already achieves state-of-the-art accuracy through leveraging
recent advancements and thoughtful adjustments in model design and optimization
processes. By integrating distributions and predictions from a committee of
models while generating high-quality soft labels, our method captures a wider
spectrum of data features, reduces model-specific biases and the adverse
effects of distribution shifts, leading to significant improvements in
generalization. This voting-based strategy not only promotes diversity and
robustness within the distilled dataset but also significantly reduces
overfitting, resulting in improved performance on post-eval tasks. Extensive
experiments across various datasets and IPCs (images per class) demonstrate
that Committee Voting leads to more reliable and adaptable distilled data
compared to single/multi-model distillation methods, demonstrating its
potential for efficient and accurate dataset distillation. Code is available
at: https://github.com/Jiacheng8/CV-DD.; 87) MMKE-Bench: A Multimodal Editing Benchmark for Diverse Visual Knowledge; Knowledge editing techniques have emerged as essential tools for updating the
factual knowledge of large language models (LLMs) and multimodal models (LMMs),
allowing them to correct outdated or inaccurate information without retraining
from scratch. However, existing benchmarks for multimodal knowledge editing
primarily focus on entity-level knowledge represented as simple triplets, which
fail to capture the complexity of real-world multimodal information. To address
this issue, we introduce MMKE-Bench, a comprehensive MultiModal Knowledge
Editing Benchmark, designed to evaluate the ability of LMMs to edit diverse
visual knowledge in real-world scenarios. MMKE-Bench addresses these
limitations by incorporating three types of editing tasks: visual entity
editing, visual semantic editing, and user-specific editing. Besides,
MMKE-Bench uses free-form natural language to represent and edit knowledge,
offering a more flexible and effective format. The benchmark consists of 2,940
pieces of knowledge and 8,363 images across 33 broad categories, with
evaluation questions automatically generated and human-verified. We assess five
state-of-the-art knowledge editing methods on three prominent LMMs, revealing
that no method excels across all criteria, and that visual and user-specific
edits are particularly challenging. MMKE-Bench sets a new standard for
evaluating the robustness of multimodal knowledge editing techniques, driving
progress in this rapidly evolving field.; 88) A High Order IMEX Method for Generalized Korteweg de-Vries Equations; In this paper, we introduce a high order space-time approximation of
generalized Korteweg de-Vries equations. More specifically, the method uses
continuous $H^1$-conforming finite elements for the spatial approximation and
implicit-explicit methods for the temporal approximation. The method is high
order in both space, provably stable, and mass-conservative. The scheme is
formulated, its properties are proven, and numerical simulations are provided
to illustrate the proposed methodology.; 89) Quantum Zeno blockade in optomechanical systems; We investigate the application of the quantum Zeno effect (QZE) for the
preparation of non-Gaussian states in optomechanical systems. By frequently
monitoring the system, the QZE can suppress transitions away from desired
subspaces of states. We show that this enables the preparation of states in
qubit subspaces even in the presence of noise and decoherence. Through
analytical and numerical analysis, we demonstrate that QZE-based protocols can
significantly improve the robustness of state preparation of qubit states in
continuous variable architectures. Our results extend the utility of the QZE
beyond discrete systems, highlighting its potential for enhancing quantum
control in more complex quantum information processing environments. These
findings offer a promising approach for achieving reliable non-Gaussian states
in optomechanical systems, with implications for the development of photonic
quantum computing and quantum sensing.; 90) Bright Sungrazing Comets in a Great Historical Controversy and Prospects
  for Their Appearance in the Near Future; Until the second half of the 19th century, two or more brief appearances of
bright comets, such as the ones in 1668 and 1702, alike in aspect and motion,
seen with a tail near the Sun, were almost universally believed to be periodic
returns of a single object. It is likely that the exceptional story of Halley's
comet was the compelling precedent for this school of thought. Application to
sungrazers was discredited by the observed fragmentation of the nucleus of the
giant sungrazer of 1882 shortly after perihelion. Generally, separations and
orbital periods of the Kreutz comets are known to be governed in such events by
the solar tidal force, while the range in the longitude of the nodal line is
linked to the pyramidal architecture caused by nontidal, cascading
fragmentation along the entire orbit and described by an updated contact-binary
model. Perception of the sungrazer system was changed dramatically by
coronagraphic imaging from space, which led to discovery of up to ten
populations of dwarf comets. Past fragmentation patterns have been used to
tentatively predict the arrivals of two bright Kreutz sungrazers -- a
Population II member around 2027 (and before 2040) and a Population I member
around 2050.; 91) Geometric-combinatorial approaches to tilting theory for weighted
  projective lines; We provide a geometric-combinatorial model for the category of coherent
sheaves on the weighted projective line of type (2,2,n) via a cylindrical
surface with n marked points on each of its upper and lower boundaries,
equipped with an order 2 self-homeomorphism. A bijection is established between
indecomposable sheaves on the weighted projective line and skew-curves on the
surface. Moreover, by defining a skew-arc as a self-compatible skew-curve and a
pseudo-triangulation as a maximal set of distinct pairwise compatible
skew-arcs, we show that pseudo-triangulations correspond bijectively to tilting
sheaves. Under this bijection, the flip of a skew-arc within a
pseudo-triangulation coincides with the tilting mutation. As an application, we
prove the connectivity of the tilting graph for the category of coherent
sheaves.; 92) ArcPro: Architectural Programs for Structured 3D Abstraction of Sparse
  Points; We introduce ArcPro, a novel learning framework built on architectural
programs to recover structured 3D abstractions from highly sparse and
low-quality point clouds. Specifically, we design a domain-specific language
(DSL) to hierarchically represent building structures as a program, which can
be efficiently converted into a mesh. We bridge feedforward and inverse
procedural modeling by using a feedforward process for training data synthesis,
allowing the network to make reverse predictions. We train an encoder-decoder
on the points-program pairs to establish a mapping from unstructured point
clouds to architectural programs, where a 3D convolutional encoder extracts
point cloud features and a transformer decoder autoregressively predicts the
programs in a tokenized form. Inference by our method is highly efficient and
produces plausible and faithful 3D abstractions. Comprehensive experiments
demonstrate that ArcPro outperforms both traditional architectural proxy
reconstruction and learning-based abstraction methods. We further explore its
potential to work with multi-view image and natural language inputs.; 93) Generative AI as a Playful yet Offensive Tourist: Exploring Tensions
  Between Playful Features and Citizen Concerns in Designing Urban Play; Play is pivotal in fostering the emotional, social, and cultural dimensions
of urban spaces. While generative AI (GAI) potentially supports playful urban
interaction, a balanced and critical approach to the design opportunities and
challenges is needed. This work develops iWonder, an image-to-image GAI tool
engaging fourteen designers in urban explorations to identify GAI's playful
features and create design ideas. Fourteen citizens then evaluated these ideas,
providing expectations and critical concerns from a bottom-up perspective. Our
findings reveal the dynamic interplay between users, GAI, and urban contexts,
highlighting GAI's potential to facilitate playful urban experiences through
generative agency, meaningful unpredictability, social performativity, and the
associated offensive qualities. We propose design considerations to address
citizen concerns and the `tourist metaphor' to deepen our understanding of
GAI's impact, offering insights to enhance cities' socio-cultural fabric.
Overall, this research contributes to the effort to harness GAI's capabilities
for urban enrichment.; 94) On extensivity of morphisms; Extensivity of a category may be described as a property of coproducts in the
category, namely, that they are disjoint and universal. An alternative
viewpoint is that it is a property of morphisms in a category. This paper
explores this point of view through a natural notion of extensive and
coextensive morphism. Through these notions, topics in universal algebra, such
as the strict refinement and Fraser-Horn properties, take categorical form and
thereby enjoy the benefits of categorical generalisation. On the other hand,
the universal algebraic theory surrounding these topics inspire categorical
results. One such result we establish in this paper is that a Barr-exact
category is coextensive if and only if every split monomorphism in the category
is coextensive.; 95) Demystifying 5G Polar and LDPC Codes: A Comprehensive Review and
  Foundations; This paper serves as a comprehensive guide for practitioners and scholars
aiming to understand the channel coding and decoding schemes integral to the 5G
NR standard, with a particular focus on LDPC and polar codes. We start by
explaining the design procedures that underlie these channel codes, offering
fundamental information from the perspectives of both encoding and decoding. In
order to determine the present status of research in this area, we also provide
a thorough literature review. Notably, we add comprehensive, standard-specific
information to these foundational evaluations that is frequently difficult to
extract from technical specification documents. The significance of reviewing
and refining the foundations of the aforementioned codes lies in their
potential to serve as candidate error-correcting codes for the future 6G
standard and beyond.; 96) Dynamics and lifetime of geometric excitations in moir\'e systems; We show that spin-2 geometric excitations, known as graviton modes, generally
exhibit vanishing lifetimes in lattice Chern bands, including in moir\'e
systems. In contrast to the Landau levels, we first numerically demonstrate
that the prominent graviton peaks in spectral functions diminish rapidly with
increasing system sizes. We explore how the choice of interaction affects the
strength of these peaks, with short-ranged interactions pushing the graviton
mode far into the continuum of excitations, where it can be significantly
scattered due to the increased density of states. We also analytically
investigate the short lifetime of the graviton mode. In lattice systems,
continuous rotational symmetry is broken, leading to highly anisotropic gapped
excitations that mix different angular momentum or ""spins''. This is despite
the surprising emergence of a ""guiding center"" continuous rotational symmetry
in the ground state, which is shared by the graviton mode. Consequently, the
graviton mode in Chern bands can be strongly scattered by the anisotropic
gapped excitations. However, the emergent rotational symmetry implies that
gravitons can be robust in principle, and we propose experimental tuning
strategies to lower the graviton mode energy below the continuum. We argue this
is a necessary condition enabling the observation of graviton modes and
geometric excitations in realistic moir\'e systems.; 97) Near-Linear Runtime for a Classical Matrix Preconditioning Algorithm; In 1960, Osborne proposed a simple iterative algorithm for matrix balancing
with outstanding numerical performance. Today, it is the default
preconditioning procedure before eigenvalue computation and other linear
algebra subroutines in mainstream software packages such as Python, Julia,
MATLAB, EISPACK, LAPACK, and more. Despite its widespread usage, Osborne's
algorithm has long resisted theoretical guarantees for its runtime: the first
polynomial-time guarantees were obtained only in the past decade, and recent
near-linear runtimes remain confined to variants of Osborne's algorithm with
important differences that make them simpler to analyze but empirically slower.
In this paper, we address this longstanding gap between theory and practice by
proving that Osborne's original algorithm -- the de facto preconditioner in
practice -- in fact has a near-linear runtime. This runtime guarantee (1) is
optimal in the input size up to at most a single logarithm, (2) is the first
runtime for Osborne's algorithm that does not dominate the runtime of
downstream tasks like eigenvalue computation, and (3) improves upon the
theoretical runtimes for all other variants of Osborne's algorithm.; 98) Further constraining the neutron star-black hole merger rate; Current template-based gravitational-wave searches for compact binary mergers
neglect the general relativistic phenomenon of spin-induced orbital precession.
Owing to their asymmetric masses, gravitational-waves from neutron star-black
hole (NSBH) binaries are prime candidates for displaying strong imprints of
spin-precession. As a result, current searches may be missing a significant
fraction of the astrophysical population, and the detected NSBH population may
be significantly suppressed or biased. Here we report the most sensitive search
for NSBH binaries to date by including spin-precession for the first time. We
analyze data from the entirety of the third LIGO-Virgo-KAGRA gravitational-wave
observing run and show that when accounting for spin-precession, our search is
up to 100% more sensitive than the search techniques currently adopted by the
LIGO-Virgo-KAGRA collaboration (for systems with strong precessional effects).
This allows us to more tightly constrain the rate of NSBH mergers in the local
Universe. Firstly, we focus on a precessing subpopulation of NSBH mergers; the
lack of observed candidates allows us to place an upper limit on the merger
rate of $R_{90} = 79\, \mathrm{Gpc}^{-3}\mathrm{yr}^{-1}$ with 90% confidence.
Secondly, we tighten the overall rate of NSBH mergers; we show that if there is
no preferred direction of component spin, the rate of NSBH mergers is on
average 16% smaller than previously believed. Finally, we report four new
subthreshold NSBH candidates, all with strong imprints of spin precession, but
note that these are most likely to be of terrestrial origin.; 99) Smoothing Accelerated Proximal Gradient Method with Backtracking for
  Nonsmooth Multiobjective Optimization; For the composite multi-objective optimization problem composed of two
nonsmooth terms, a smoothing method is used to overcome the nonsmoothness of
the objective function, making the objective function contain at most one
nonsmooth term. Then, inspired by the design idea of the aforementioned
backtracking strategy, an update rule is proposed by constructing a
relationship between an estimation sequence of the Lipschitz constant and a
smoothing factor, which results in a backtracking strategy suitable for this
problem, allowing the estimation sequence to be updated in a non-increasing
manner. On this basis, a smoothing accelerated proximal gradient algorithm
based on the backtracking strategy is further proposed. Under appropriate
conditions, it is proven that all accumulation points of the sequence generated
by this algorithm are weak Pareto optimal solutions. Additionally, the
convergence rate of the algorithm under different parameters is established
using a utility function. Numerical experiments show that, compared with the
subgradient algorithm, the proposed algorithm demonstrates significant
advantages in terms of runtime, iteration count, and function evaluations.; 100) Distributed Stochastic Zeroth-Order Optimization with Compressed
  Communication; The dual challenges of prohibitive communication overhead and the
impracticality of gradient computation due to data privacy or black-box
constraints in distributed systems motivate this work on
communication-constrained gradient-free optimization. We propose a stochastic
distributed zeroth-order algorithm (Com-DSZO) requiring only two function
evaluations per iteration, integrated with general compression operators.
Rigorous analysis establishes its sublinear convergence rate for both smooth
and nonsmooth objectives, while explicitly elucidating the
compression-convergence trade-off. Furthermore, we develop a variance-reduced
variant (VR-Com-DSZO) under stochastic mini-batch feedback. The empirical
algorithm performance are illustrated with numerical examples.",1.0,0.38685280723454163
2411.01019,applied,2411.01019-pos2-0,"Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms; Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability.",2411.01019-pos1-0,"Incidental Anterior Mediastinal Nodular Lesions on Chest CT in Asymptomatic Subjects; Screening for lung cancer: 2023 guideline update from the American Cancer Society; Objective The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods We included 56,358 consecutive participants (mean age 52.4 ± 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66–0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01–3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes.; Abstract Lung cancer is the leading cause of mortality and person‐years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network‐validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit‐to‐radiation‐risk ratios follow‐up examinations. GDG also examined disease burden data National Institute’s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50–80 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low‐dose computed tomography asymptomatic currently smoke or formerly smoked have ≥20 pack‐year ( , ). Before decision made initiate LCS, should engage shared decision‐making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",70,"['1', '2', '5', '3', '4', '9', '10', '6', '7', '8']","The first candidate paper, 'SigN: SIMBox Activity Detection Through Latency Anomalies at the Cellular Edge', complements the main paper on anterior mediastinal nodular lesion segmentation by applying advanced detection methods to a different domain, showcasing similar challenges in accurately identifying and interpreting complex signals. This multidisciplinary convergence between medical imaging and telecommunications enhances diagnostic processes and automation in both fields. The other candidate papers, while relevant to their respective domains, do not align as closely with the specific focus on segmentation and detection methodologies relevant to the main paper.","1) SigN: SIMBox Activity Detection Through Latency Anomalies at the
  Cellular Edge; Despite their widespread adoption, cellular networks face growing
vulnerabilities due to their inherent complexity and the integration of
advanced technologies. One of the major threats in this landscape is Voice over
IP (VoIP) to GSM gateways, known as SIMBox devices. These devices use multiple
SIM cards to route VoIP traffic through cellular networks, enabling
international bypass fraud with losses of up to $3.11 billion annually. Beyond
financial impact, SIMBox activity degrades network performance, threatens
national security, and facilitates eavesdropping on communications. Existing
detection methods for SIMBox activity are hindered by evolving fraud techniques
and implementation complexities, limiting their practical adoption in operator
networks.This paper addresses the limitations of current detection methods by
introducing SigN , a novel approach to identifying SIMBox activity at the
cellular edge. The proposed method focuses on detecting remote SIM card
association, a technique used by SIMBox appliances to mimic human mobility
patterns. The method detects latency anomalies between SIMBox and standard
devices by analyzing cellular signaling during network attachment. Extensive
indoor and outdoor experiments demonstrate that SIMBox devices generate
significantly higher attachment latencies, particularly during the
authentication phase, where latency is up to 23 times greater than that of
standard devices. We attribute part of this overhead to immutable factors such
as LTE authentication standards and Internet-based communication protocols.
Therefore, our approach offers a robust, scalable, and practical solution to
mitigate SIMBox activity risks at the network edge.; 2) Safe Multi-agent Satellite Servicing with Control Barrier Functions; The use of control barrier functions under uncertain pose information of
multiple small servicing agents is analyzed for a satellite servicing
application. The application consists of modular servicing agents deployed
towards a tumbling space object from a mothership. Relative position and
orientation of each agent is obtained via fusion of relative range and inertial
measurement sensors. The control barrier functions are utilized to avoid
collisions with other agents for the application of simultaneously relocating
servicing agents on a tumbling body. A differential collision detection and
avoidance framework using the polytopic hull of the tumbling space object is
utilized to safely guide the agents away from the tumbling object.; 3) Network Simulator-centric Compositional Testing; This article introduces a novel methodology, Network Simulator-centric
Compositional Testing (NSCT), to enhance the verification of network protocols
with a particular focus on time-varying network properties. NSCT follows a
Model-Based Testing (MBT) approach. These approaches usually struggle to test
and represent time-varying network properties. NSCT also aims to achieve more
accurate and reproducible protocol testing. It is implemented using the Ivy
tool and the Shadow network simulator. This enables online debugging of real
protocol implementations. A case study on an implementation of QUIC (picoquic)
is presented, revealing an error in its compliance with a time-varying
specification. This error has subsequently been rectified, highlighting NSCT's
effectiveness in uncovering and addressing real-world protocol implementation
issues. The article underscores NSCT's potential in advancing protocol testing
methodologies, offering a notable contribution to the field of network protocol
verification.; 4) Proceedings of the 14th International Computational Accelerator Physics
  Conference (ICAP24); This is the proceedings of the 14th International Computational Accelerator
Physics Conference, ICAP'24, which was held at the Lufthansa Seeheim Conference
Hotel in Germany from October 2-5, 2024, hosted by TU Darmstadt and GSI
Helmholtzzentrum f\""ur Schwerionenforschung. ICAP'24 has focused on advances in
Computational Accelerator Physics and their application to existing machines
and future facilities. It has provided a forum for researchers in modeling and
simulation to exchange information and discuss new ideas that benefit a wide
area of accelerator science and technology. Topics of the conference have
included computational needs and challenges, beam dynamics and electromagnetic
field calculations, code development and validation, data processing and
visualization, high performance computing, machine learning and advanced
optimization as well as emerging technologies that will impact computing for
accelerator design.; 5) A Survey of Zero-Knowledge Proof Based Verifiable Machine Learning; As machine learning technologies advance rapidly across various domains,
concerns over data privacy and model security have grown significantly. These
challenges are particularly pronounced when models are trained and deployed on
cloud platforms or third-party servers due to the computational resource
limitations of users' end devices. In response, zero-knowledge proof (ZKP)
technology has emerged as a promising solution, enabling effective validation
of model performance and authenticity in both training and inference processes
without disclosing sensitive data. Thus, ZKP ensures the verifiability and
security of machine learning models, making it a valuable tool for
privacy-preserving AI. Although some research has explored the verifiable
machine learning solutions that exploit ZKP, a comprehensive survey and summary
of these efforts remain absent. This survey paper aims to bridge this gap by
reviewing and analyzing all the existing Zero-Knowledge Machine Learning (ZKML)
research from June 2017 to December 2024. We begin by introducing the concept
of ZKML and outlining its ZKP algorithmic setups under three key categories:
verifiable training, verifiable inference, and verifiable testing. Next, we
provide a comprehensive categorization of existing ZKML research within these
categories and analyze the works in detail. Furthermore, we explore the
implementation challenges faced in this field and discuss the improvement works
to address these obstacles. Additionally, we highlight several commercial
applications of ZKML technology. Finally, we propose promising directions for
future advancements in this domain.; 6) Inverse Stefan problems of determining the time-dependent source
  coefficient and heat flux function; This paper delves into the Inverse Stefan problem, specifically focusing on
determining the time-dependent source coefficient in the parabolic heat
equation governing heat transfer in a semi-infinite rod. The problem entails
the intricate task of uncovering both temperature- and time-dependent
coefficients of the source while accommodating Dirichlet and Neumann boundary
conditions. Through a comprehensive mathematical model and rigorous theoretical
analysis, our study aims to provide a robust methodology for accurately
determining the source coefficient from observed temperature and heat flux data
in problems with different cases of the source functions. Importantly, we
establish the existence and uniqueness, and estimate the continuous dependence
of a weak solution upon the given data for some inverse problems, offering a
foundational understanding of its solvability.; 7) HiCoCS: High Concurrency Cross-Sharding on Permissioned Blockchains; As the foundation of the Web3 trust system, blockchain technology faces
increasing demands for scalability. Sharding emerges as a promising solution,
but it struggles to handle highly concurrent cross-shard transactions
(\textsf{CSTx}s), primarily due to simultaneous ledger operations on the same
account. Hyperledger Fabric, a permissioned blockchain, employs multi-version
concurrency control for parallel processing. Existing solutions use channels
and intermediaries to achieve cross-sharding in Hyperledger Fabric. However,
the conflict problem caused by highly concurrent \textsf{CSTx}s has not been
adequately resolved. To fill this gap, we propose HiCoCS, a high concurrency
cross-shard scheme for permissioned blockchains. HiCoCS creates a unique
virtual sub-broker for each \textsf{CSTx} by introducing a composite key
structure, enabling conflict-free concurrent transaction processing while
reducing resource overhead. The challenge lies in managing large numbers of
composite keys and mitigating intermediary privacy risks. HiCoCS utilizes
virtual sub-brokers to receive and process \textsf{CSTx}s concurrently while
maintaining a transaction pool. Batch processing is employed to merge multiple
\textsf{CSTx}s in the pool, improving efficiency. We explore composite key
reuse to reduce the number of virtual sub-brokers and lower system overhead.
Privacy preservation is enhanced using homomorphic encryption. Evaluations show
that HiCoCS improves cross-shard transaction throughput by 3.5-20.2 times
compared to the baselines.; 8) Variable Bregman Majorization-Minimization Algorithm and its Application
  to Dirichlet Maximum Likelihood Estimation; We propose a novel Bregman descent algorithm for minimizing a convex function
that is expressed as the sum of a differentiable part (defined over an open
set) and a possibly nonsmooth term. The approach, referred to as the Variable
Bregman Majorization-Minimization (VBMM) algorithm, extends the Bregman
Proximal Gradient method by allowing the Bregman function used in the
divergence to adaptively vary at each iteration, provided it satisfies a
majorizing condition on the objective function. This adaptive framework enables
the algorithm to approximate the objective more precisely at each iteration,
thereby allowing for accelerated convergence compared to the traditional
Bregman Proximal Gradient descent. We establish the convergence of the VBMM
algorithm to a minimizer under mild assumptions on the family of metrics used.
Furthermore, we introduce a novel application of both the Bregman Proximal
Gradient method and the VBMM algorithm to the estimation of the
multidimensional parameters of a Dirichlet distribution through the
maximization of its log-likelihood. Numerical experiments confirm that the VBMM
algorithm outperforms existing approaches in terms of convergence speed.; 9) Finger-to-Chest Style Transfer-assisted Deep Learning Method For
  Photoplethysmogram Waveform Restoration with Timing Preservation; Wearable measurements, such as those obtained by photoplethysmogram (PPG)
sensors are highly susceptible to motion artifacts and noise, affecting
cardiovascular measures. Chest-acquired PPG signals are especially vulnerable,
with signal degradation primarily resulting from lower perfusion,
breathing-induced motion, and mechanical interference from chest movements.
Traditional restoration methods often degrade the signal, and supervised deep
learning (DL) struggles with random and systematic distortions, requiring very
large datasets for successful training. To efficiently restore chest PPG
waveform, we propose a style transfer-assisted cycle-consistent generative
adversarial network, called starGAN, whose performance is evaluated on a
three-channel PPG signal (red, green,and infrared) acquired by a chest-worn
multi-modal sensor, called Soundi. Two identical devices are adopted, one
sensor to collect the PPG signal on the chest, considered to feature low
quality and undergoing restoration, and another sensor to obtain a high-quality
PPG signal measured on the finger, considered the reference signal. Extensive
validation over some 8,000 5-second chunks collected from 40 subjects showed
about 90% correlation of the restored chest PPG with the reference finger PPG,
with a 30% improvement over raw chest PPG. Likewise, the signal-to-noise ratio
improved on average of about 125%, over the three channels. The agreement with
heart-rate computed from concurrent ECG was extremely high, overcoming 84% on
average. These results demonstrate effective signal restoration, comparable
with findings in recent literature papers. Significance: PPG signals collected
from wearable devices are highly susceptible to artifacts, making innovative
AI-based techniques fundamental towards holistic health assessments in a single
device.; 10) The Signed Two-Space Proximity Model for Learning Representations in
  Protein-Protein Interaction Networks; Accurately predicting complex protein-protein interactions (PPIs) is crucial
for decoding biological processes, from cellular functioning to disease
mechanisms. However, experimental methods for determining PPIs are
computationally expensive. Thus, attention has been recently drawn to machine
learning approaches. Furthermore, insufficient effort has been made toward
analyzing signed PPI networks, which capture both activating (positive) and
inhibitory (negative) interactions. To accurately represent biological
relationships, we present the Signed Two-Space Proximity Model (S2-SPM) for
signed PPI networks, which explicitly incorporates both types of interactions,
reflecting the complex regulatory mechanisms within biological systems. This is
achieved by leveraging two independent latent spaces to differentiate between
positive and negative interactions while representing protein similarity
through proximity in these spaces. Our approach also enables the identification
of archetypes representing extreme protein profiles. S2-SPM's superior
performance in predicting the presence and sign of interactions in SPPI
networks is demonstrated in link prediction tasks against relevant baseline
methods. Additionally, the biological prevalence of the identified archetypes
is confirmed by an enrichment analysis of Gene Ontology (GO) terms, which
reveals that distinct biological tasks are associated with archetypal groups
formed by both interactions. This study is also validated regarding statistical
significance and sensitivity analysis, providing insights into the functional
roles of different interaction types. Finally, the robustness and consistency
of the extracted archetype structures are confirmed using the Bayesian
Normalized Mutual Information (BNMI) metric, proving the model's reliability in
capturing meaningful SPPI patterns.; 11) Chameleon2++: An Efficient Chameleon2 Clustering with Approximate
  Nearest Neighbors; Clustering algorithms are fundamental tools in data analysis, with
hierarchical methods being particularly valuable for their flexibility.
Chameleon is a widely used hierarchical clustering algorithm that excels at
identifying high-quality clusters of arbitrary shapes, sizes, and densities.
Chameleon2 is the most recent variant that has demonstrated significant
improvements, but suffers from critical failings and there are certain
improvements that can be made.
  The first failure we address is that the complexity of Chameleon2 is claimed
to be $O(n^2)$, while we demonstrate that it is actually $O(n^2\log{n})$, with
$n$ being the number of data points. Furthermore, we suggest improvements to
Chameleon2 that ensure that the complexity remains $O(n^2)$ with minimal to no
loss of performance. The second failing of Chameleon2 is that it lacks
transparency and it does not provide the fine-tuned algorithm parameters used
to obtain the claimed results. We meticulously provide all such parameter
values to enhance replicability.
  The improvement which we make in Chameleon2 is that we replace the exact
$k$-NN search with an approximate $k$-NN search. This further reduces the
algorithmic complexity down to $O(n\log{n})$ without any performance loss.
Here, we primarily configure three approximate nearest neighbor search
algorithms (Annoy, FLANN and NMSLIB) to align with the overarching Chameleon2
clustering framework. Experimental evaluations on standard benchmark datasets
demonstrate that the proposed Chameleon2++ algorithm is more efficient, robust,
and computationally optimal.; 12) A Taxonomy of Functional Security Features and How They Can Be Located; Security must be considered in almost every software system. Unfortunately,
selecting and implementing security features remains challenging due to the
variety of security threats and possible countermeasures. While security
standards are intended to help developers, they are usually too abstract and
vague to help implement security features, or they merely help configure such.
A resource that describes security features at an abstraction level between
high-level (i.e., rather too general) and low-level (i.e., rather too specific)
security standards could facilitate secure systems development. To realize
security features, developers typically use external security frameworks, to
minimize implementation mistakes. Even then, developers still make mistakes,
often resulting in security vulnerabilities. When security incidents occur or
the system needs to be audited or maintained, it is essential to know the
implemented security features and, more importantly, where they are located.
This task, commonly referred to as feature location, is often tedious and
error-prone. Therefore, we have to support long-term tracking of implemented
security features.
  We present a study of security features in the literature and their coverage
in popular security frameworks. We contribute (1) a taxonomy of 68 functional
implementation-level security features including a mapping to widely used
security standards, (2) an examination of 21 popular security frameworks
concerning which of these security features they provide, and (3) a discussion
on the representation of security features in source code. Our taxonomy aims to
aid developers in selecting appropriate security features and frameworks and
relating them to security standards when they need to choose and implement
security features for a software system.; 13) Hierarchical Clustering Algorithms on Poisson and Cox Point Processes; Clustering is a widely used technique in unsupervised learning to identify
groups within a dataset based on the similarities between its elements. This
paper introduces three new hierarchical clustering models, Clustroid
Hierarchical Nearest Neighbor (\(\mathrm{CHN}^2\)), Single Linkage Hierarchical
Nearest Neighbor (\(\mathrm{SHN}^2\)), and Hausdorff (Complete Linkage)
Hierarchical Nearest Neighbor (\(\mathrm{H}^2\mathrm{N}^2\)), all designed for
datasets with a countably infinite number of points. These algorithms proceed
through multiple levels of clustering and construct clusters by connecting
nearest-neighbor points or clusters, but differ in the distance metrics they
employ (clustroid, single linkage, or Hausdorff, respectively). Each method is
first applied to the homogeneous Poisson point process on the Euclidean space,
where it defines a phylogenetic forest, which is a factor of the point process
and therefore unimodular. The results established for the \(\mathrm{CHN}^2\)
algorithm include the almost-sure finiteness of the clusters and bounds on the
mean cluster size at each level of the algorithm. The mean size of the typical
cluster is shown to be infinite. Moreover, the limiting structure of all three
algorithms is examined as the number of levels tends to infinity, and
properties such as the one-endedness of the limiting connected components are
derived. In the specific case of \(\mathrm{SHN}^2\) on the Poisson point
process, the limiting graph is shown to be a subgraph of the Minimal Spanning
Forest. The \(\mathrm{CHN}^2\) algorithm is also extended beyond the Poisson
setting, to certain stationary Cox point processes. Similar finite-cluster
properties are shown to hold in these cases. It is also shown that efficient
detection of Cox-triggered aggregation can be achieved through this clustering
algorithm.; 14) Inclusive STEAM Education: A Framework for Teaching Cod-2 ing and
  Robotics to Students with Visually Impairment Using 3 Advanced Computer
  Vision; STEAM education integrates Science, Technology, Engineering, Arts, and
Mathematics to foster creativity and problem-solving. However, students with
visual impairments (VI) encounter significant challenges in programming and
robotics, particularly in tracking robot movements and developing spatial
awareness. This paper presents a framework that leverages pre-constructed
robots and algorithms, such as maze-solving techniques, within an accessible
learning environment. The proposed system employs Contrastive Language-Image
Pre-training (CLIP) to process global camera-captured maze layouts, converting
visual data into textual descriptions that generate spatial audio prompts in an
Audio Virtual Reality (AVR) system. Students issue verbal commands, which are
refined through CLIP, while robot-mounted stereo cameras provide real-time data
processed via Simultaneous Localization and Mapping (SLAM) for continuous
feedback. By integrating these technologies, the framework empowers VI students
to develop coding skills and engage in complex problem-solving tasks. Beyond
maze-solving applications, this approach demonstrates the broader potential of
computer vision in special education, contributing to improved accessibility
and learning experiences in STEAM disciplines.; 15) Higher derivative holography and temperature dependence of QGP
  viscosities; Recent Bayesian analyses of heavy ion collision data have established a
non-trivial temperature dependence of the shear and bulk viscosity per entropy.
Motivated by this, we consider higher derivative corrections to realistic,
bottom-up holographic models of quark-gluon plasma based on five-dimensional
Einstein-dilaton theories and determine the dilaton potentials in the higher
derivative terms by matching the Bayesian analyses. A byproduct of our analysis
is the bulk viscosity that follows from the holographic V-QCD theory. Higher
derivative corrections when treated perturbatively lead to tension with
existing data. We investigate possible resolutions.; 16) Birational geometry of the twofold symmetric product of a Hirzebruch
  surface via secant maps; In this paper, extending some ideas of Fano, we study the birational geometry
of the Hilbert scheme of 0-dimensional subschemes of length 2 of a rational
normal scroll. This fourfold has three elementary contractions associated to
the three faces of its nef cone. We study natural projective realizations of
these contractions. In particular, given a smooth rational normal scroll
$S_{a,b}$ of degree $r$ in ${\mathbb P}^{r+1}$ with $1 \leq a \leq b$ and
a+b=r, i.e., $S_{a,b}$ is the relative Proj of the vector bundle $O_{{\mathbb
P}^1}(a)\oplus O_{{\mathbb P}^1}(b)$ embedded in ${\mathbb P}^{r+1}$ with its
O(1) line bundle (from an abstract viewpoint $S_{a,b}\cong {\mathbb F}_{b-a}$),
we consider the subvariety $X_{a,b}$ of the Grassmannian $G(1,r+1)$ described
by all lines that are secant or tangent to $S_{a,b}$. The variety $X_{a,b}$ is
the image of some of the aforementioned contractions, it is smooth if a>1, and
it is singular at a unique point if a=1. We compute the degree of $X_{a,b}$ and
the local structure of the singularity of $X_{a,b}$ when a=1. Finally we
discuss in some detail the case r=4, originally considered by Fano, because the
smooth hyperplane sections of $X_{2,2}$ and $X_{1,3}$ are the Fano 3-folds that
appear as number 16 in the Mori-Mukai list of Fano 3-folds with Picard number
2. We prove that any smooth hyperplane section of $X_{2,2}$ is also a
hyperplane section of $X_{1,3}$, and we discuss the GIT-stability of the smooth
hyperplane sections of $X_{1,3}$ where $G$ is the subgroup of the projective
automorphisms of $X_{1,3}$ coming from the ones of $S_{1,3}.$; 17) Turn That Frown Upside Down: FaceID Customization via Cross-Training
  Data; Existing face identity (FaceID) customization methods perform well but are
limited to generating identical faces as the input, while in real-world
applications, users often desire images of the same person but with variations,
such as different expressions (e.g., smiling, angry) or angles (e.g., side
profile). This limitation arises from the lack of datasets with controlled
input-output facial variations, restricting models' ability to learn effective
modifications.
  To address this issue, we propose CrossFaceID, the first large-scale,
high-quality, and publicly available dataset specifically designed to improve
the facial modification capabilities of FaceID customization models.
Specifically, CrossFaceID consists of 40,000 text-image pairs from
approximately 2,000 persons, with each person represented by around 20 images
showcasing diverse facial attributes such as poses, expressions, angles, and
adornments. During the training stage, a specific face of a person is used as
input, and the FaceID customization model is forced to generate another image
of the same person but with altered facial features. This allows the FaceID
customization model to acquire the ability to personalize and modify known
facial features during the inference stage. Experiments show that models
fine-tuned on the CrossFaceID dataset retain its performance in preserving
FaceID fidelity while significantly improving its face customization
capabilities.
  To facilitate further advancements in the FaceID customization field, our
code, constructed datasets, and trained models are fully available to the
public.; 18) RUM-NN: A Neural Network Model Compatible with Random Utility
  Maximisation for Discrete Choice Setups; This paper introduces a framework for capturing stochasticity of choice
probabilities in neural networks, derived from and fully consistent with the
Random Utility Maximization (RUM) theory, referred to as RUM-NN. Neural network
models show remarkable performance compared with statistical models; however,
they are often criticized for their lack of transparency and interoperability.
The proposed RUM-NN is introduced in both linear and nonlinear structures. The
linear RUM-NN retains the interpretability and identifiability of traditional
econometric discrete choice models while using neural network-based estimation
techniques. The nonlinear RUM-NN extends the model's flexibility and predictive
capabilities to capture nonlinear relationships between variables within
utility functions. Additionally, the RUM-NN allows for the implementation of
various parametric distributions for unobserved error components in the utility
function and captures correlations among error terms. The performance of RUM-NN
in parameter recovery and prediction accuracy is rigorously evaluated using
synthetic datasets through Monte Carlo experiments. Additionally, RUM-NN is
evaluated on the Swissmetro and the London Passenger Mode Choice (LPMC)
datasets with different sets of distribution assumptions for the error
component. The results demonstrate that RUM-NN under a linear utility structure
and IID Gumbel error terms can replicate the performance of the Multinomial
Logit (MNL) model, but relaxing those constraints leads to superior performance
for both Swissmetro and LPMC datasets. By introducing a novel estimation
approach aligned with statistical theories, this study empowers econometricians
to harness the advantages of neural network models.; 19) Gaussian quantum data hiding; Quantum data hiding encodes a hidden classical bit to a pair of quantum
states that is difficult to distinguish using a particular set of measurement,
denoted as $M$. In this work, we explore quantum data hiding in two contexts
involving Gaussian operations or states. First, we consider the set of
measurement $M$ as Gaussian local quantum operations and classical
communication, a new set of operations not previously discussed in the
literature for data hiding. We hide one classical bit in the two different
mixture of displaced two-mode squeezed states. Second, we consider the set of
measurement $M$ as general Gaussian measurement and construct the data hiding
states using two-mode thermal states. This data hiding scheme is effective in
the weak strength limit, providing a new example compared to existing
discussions for the set of general Gaussian measurement.; 20) Summarization Metrics for Spanish and Basque: Do Automatic Scores and
  LLM-Judges Correlate with Humans?; Studies on evaluation metrics and LLM-as-a-Judge models for automatic text
summarization have largely been focused on English, limiting our understanding
of their effectiveness in other languages. Through our new dataset BASSE
(BAsque and Spanish Summarization Evaluation), we address this situation by
collecting human judgments on 2,040 abstractive summaries in Basque and
Spanish, generated either manually or by five LLMs with four different prompts.
For each summary, annotators evaluated five criteria on a 5-point Likert scale:
coherence, consistency, fluency, relevance, and 5W1H. We use these data to
reevaluate traditional automatic metrics used for evaluating summaries, as well
as several LLM-as-a-Judge models that show strong performance on this task in
English. Our results show that currently proprietary judge LLMs have the
highest correlation with human judgments, followed by criteria-specific
automatic metrics, while open-sourced judge LLMs perform poorly. We release
BASSE and our code publicly, along with the first large-scale Basque
summarization dataset containing 22,525 news articles with their subheads.; 21) Solving the Catastrophic Forgetting Problem in Generalized Category
  Discovery; Generalized Category Discovery (GCD) aims to identify a mix of known and
novel categories within unlabeled data sets, providing a more realistic setting
for image recognition. Essentially, GCD needs to remember existing patterns
thoroughly to recognize novel categories. Recent state-of-the-art method SimGCD
transfers the knowledge from known-class data to the learning of novel classes
through debiased learning. However, some patterns are catastrophically forgot
during adaptation and thus lead to poor performance in novel categories
classification. To address this issue, we propose a novel learning approach,
LegoGCD, which is seamlessly integrated into previous methods to enhance the
discrimination of novel classes while maintaining performance on previously
encountered known classes. Specifically, we design two types of techniques
termed as Local Entropy Regularization (LER) and Dual-views Kullback Leibler
divergence constraint (DKL). The LER optimizes the distribution of potential
known class samples in unlabeled data, thus ensuring the preservation of
knowledge related to known categories while learning novel classes. Meanwhile,
DKL introduces Kullback Leibler divergence to encourage the model to produce a
similar prediction distribution of two view samples from the same image. In
this way, it successfully avoids mismatched prediction and generates more
reliable potential known class samples simultaneously. Extensive experiments
validate that the proposed LegoGCD effectively addresses the known category
forgetting issue across all datasets, eg, delivering a 7.74% and 2.51% accuracy
boost on known and novel classes in CUB, respectively. Our code is available
at: https://github.com/Cliffia123/LegoGCD.; 22) Formally exact fluorescence spectroscopy simulations for mesoscale
  molecular aggregates with $N^0$ scaling; We present a size-invariant (i.e., $N^0$) scaling algorithm for simulating
fluorescence spectroscopy in large molecular aggregates. We combine the dyadic
adaptive hierarchy of pure states (DadHOPS) equation-of-motion with an operator
decomposition scheme and an efficient Monte Carlo sampling algorithm to enable
a formally exact, local description of the fluorescence spectrum in large
molecular aggregates. Furthermore, we demonstrate that the ensemble average
inverse participation ratio (IPR) of DadHOPS wave functions reproduces the
delocalization extent extracted from fluorescence spectroscopy of J-aggregates
with strong vibronic transitions. This work provides a computationally
efficient framework for fluorescence simulations, offering a new tool for
understanding the optical properties of mesoscale molecular systems.; 23) Is fixed-node diffusion quantum Monte Carlo reproducible?; Fixed-node diffusion quantum Monte Carlo (FN-DMC) is a widely-trusted
many-body method for solving the Schr\""{o}dinger equation, known for its
reliable predictions of material and molecular properties. Furthermore, its
excellent scalability with system complexity and near-perfect utilization of
computational power makes FN-DMC ideally positioned to leverage new advances in
computing to address increasingly complex scientific problems. Even though the
method is widely used as a computational gold standard, reproducibility across
the numerous FN-DMC code implementations has yet to be demonstrated. This
difficulty stems from the diverse array of DMC algorithms and trial wave
functions, compounded by the method's inherent stochastic nature. This study
represents a community-wide effort to address the titular question, affirming
that: Yes, FN-DMC is reproducible (when handled with care). Using the
water-methane dimer as the canonical test case, we compare results from eleven
different FN-DMC codes and show that the approximations to treat the
non-locality of pseudopotentials are the primary source of the discrepancies
between them. In particular, we demonstrate that, for the same choice of
determinantal component in the trial wave function, reliable and reproducible
predictions can be achieved by employing the T-move (TM), the determinant
locality approximation (DLA), or the determinant T-move (DTM) schemes, while
the older locality approximation (LA) leads to considerable variability in
results. This work lays the foundation to establish accurate and reproducible
FN-DMC estimates for all future studies across applications in materials
science, physics, chemistry, and biology.; 24) Analyzing the Impact of AC False Data Injection Attacks on Power System
  Operation; False Data Injection (FDI) attacks are a significant threat to modern power
systems. Although numerous research studies have focused on FDI attacks on
power systems, these studies have primarily concentrated on designing or
detecting DC FDI attacks, with less attention given to the impact analysis of
AC FDI attacks. AC FDI attacks are potentially more harmful as they can easily
bypass bad data detection (BDD) algorithms. In this paper, we present a unified
approach to investigate the impact of AC FDI attacks on power transmission
lines using the PowerWorld simulator. We also investigate the impact of
different FDI attack designs, including those optimally designed to evade BDD
algorithms and compare them accordingly. Our findings demonstrate that in
designing optimal AC FDI attacks, a trade-off between the residuals of state
variables and the corresponding impacts of the proposed attack should be
considered. This is because optimal attacks result in fewer changes in the
attacked variable states and their estimated residuals compared to arbitrary AC
FDI attacks. Moreover, the impacts of optimal AC FDI attacks can be less severe
than those of arbitrary attacks. We implement and analyze the proposed approach
on the IEEE 39-bus test system using PowerWorld simulator.; 25) Community Detection for Contextual-LSBM: Theoretical Limitations of
  Misclassification Rate and Efficient Algorithms; The integration of network information and node attribute information has
recently gained significant attention in the community detection literature. In
this work, we consider community detection in the Contextual Labeled Stochastic
Block Model (CLSBM), where the network follows an LSBM and node attributes
follow a Gaussian Mixture Model (GMM). Our primary focus is the
misclassification rate, which measures the expected number of nodes
misclassified by community detection algorithms. We first establish a lower
bound on the optimal misclassification rate that holds for any algorithm. When
we specialize our setting to the LSBM (which preserves only network
information) or the GMM (which preserves only node attribute information), our
lower bound recovers prior results. Moreover, we present an efficient
spectral-based algorithm tailored for the CLSBM and derive an upper bound on
its misclassification rate. Although the algorithm does not attain the lower
bound, it serves as a reliable starting point for designing more accurate
community detection algorithms (as many algorithms use spectral method as an
initial step, followed by refinement procedures to enhance accuracy).; 26) Scalar field source Teleparallel Robertson-Walker F(T)-gravity solutions; This paper investigates the teleparallel Robertson--Walker (TRW) $F(T)$
gravity solutions for a scalar field source. We use the TRW $F(T)$ gravity
field equations (FEs) for each $k$-parameter value case added by a scalar field
to find new teleparallel $F(T)$ solutions. For $k=0$, we find an
easy-to-compute $F(T)$ solution formula applicable for any scalar field source.
Then, we obtain, for $k=-1$ and $+1$ situations, some new analytical $F(T)$
solutions, only for specific $n$-parameter values and well-determined scalar
field cases. We can find by those computations a large number of analytical
teleparallel $F(T)$ solutions independent of any scalar potential $V(\phi)$
expression. The $V(\phi)$ independence makes the FE solving and computations
easier. The new solutions will be relevant for future cosmological applications
in dark matter, dark energy (DE) quintessence, phantom energy and quintom
models of physical processes.; 27) Multiple change point detection based on Hodrick-Prescott and $l_1$
  filtering method for random walk time series data; We propose new methods for detecting multiple change points in time series,
specifically designed for random walk processes, where stationarity and
variance changes present challenges. Our approach combines two trend estimation
methods: the Hodrick Prescott (HP) filter and the l1 filter. A major challenge
in these methods is selecting the tuning parameter lambda, which we address by
introducing two selection techniques. For the HP based change point detection,
we propose a probability-based threshold to select lambda under the assumption
of an exponential distribution. For the l1 based method, we suggest a selection
strategy assuming normality. Additionally, we introduce a technique to estimate
the maximum number of change points in time segments using the l1 based method.
We validate our methods by comparing them to similar techniques, such as PELT,
using simulated data. We also demonstrate the practical application of our
approach to real-world SNP stock data, showcasing its effectiveness in
detecting change points.; 28) Debiasing physico-chemical models in air quality monitoring by combining
  different pollutant concentration measures; Air quality monitoring requires to produce accurate estimation of nitrogen
dioxide or fine particulate matter concentration maps, at different moments. A
typical strategy is to combine different types of data. On the one hand,
concentration maps produced by deterministic physicochemical models at urban
scale, and on the other hand, concentration measures made at different points,
different moments, and by different devices. These measures are provided first
by a small number of reference stations, which give reliable measurements of
the concentration, and second by a larger number of micro-sensors, which give
biased and noisier measurements. The proposed approach consists in modeling the
bias of the physicochemical model and estimating the parameters of this bias
using all the available concentration measures. Our model relies on a partition
of the geographical space of interest into different zones within which the
bias is assumed to be modeled by a single affine transformation of the actual
concentration. Our approach allows to improve the concentration maps provided
by the deterministic models but also to understand the behavior of
micro-sensors and their contribution in improving air quality monitoring. We
introduce the model, detail its implementation and experiment it through
numerical results using datasets collected in Grenoble (France).; 29) Decoupling Appearance Variations with 3D Consistent Features in Gaussian
  Splatting; Gaussian Splatting has emerged as a prominent 3D representation in novel view
synthesis, but it still suffers from appearance variations, which are caused by
various factors, such as modern camera ISPs, different time of day, weather
conditions, and local light changes. These variations can lead to floaters and
color distortions in the rendered images/videos. Recent appearance modeling
approaches in Gaussian Splatting are either tightly coupled with the rendering
process, hindering real-time rendering, or they only account for mild global
variations, performing poorly in scenes with local light changes. In this
paper, we propose DAVIGS, a method that decouples appearance variations in a
plug-and-play and efficient manner. By transforming the rendering results at
the image level instead of the Gaussian level, our approach can model
appearance variations with minimal optimization time and memory overhead.
Furthermore, our method gathers appearance-related information in 3D space to
transform the rendered images, thus building 3D consistency across views
implicitly. We validate our method on several appearance-variant scenes, and
demonstrate that it achieves state-of-the-art rendering quality with minimal
training time and memory usage, without compromising rendering speeds.
Additionally, it provides performance improvements for different Gaussian
Splatting baselines in a plug-and-play manner.; 30) Suppressing grid instability and noise in particle-in-cell simulation by
  smoothing; Smoothing short-wavelength charge density variations can stabilize explicit
electrostatic particle-in-cell (PIC) plasma simulations against grid heating
and cold beam instabilities, which cause unphysical heating when the Debye
length is poorly resolved. We demonstrate this by solving the dispersion and by
running 1D electrostatic PIC simulations, using an efficient smoothing
algorithm that leverages the Poisson solve. To ensure stability, the smoothing
radius must increase with the number of Debye lengths per cell. Smoothing also
suppresses particle noise, which is severely exacerbated by poor resolution of
the Debye length. To help determine optimal PIC configuration, we empirically
characterize electric field noise, particle velocity diffusion, and unphysical
energy exchanges in 1D PIC simulation, as a function of Debye-length
resolution, smoothing, and particles per cell. We also show how PIC noise
causes test particles to exhibit misleading behavior. Since smoothing reduces
the effective resolution, the optimal cell size is less than the desired
resolution but can be much greater than the Debye length, reducing
computational expense.; 31) Derivation of a Multiscale Ferrofluid Model: Superparamagnetic Behavior
  due to Fast Spin Flip; We consider a microscopic model of $N$ magnetic nanoparticles in a Stokes
flow. We assume that the temperature is above the critical N\'eel temperature
such that the particles' magnetizations undergo random flip with rate
$1/\varepsilon$. The microscopic system is the modeled through a piecewise
deterministic Markov jump process. We show that for large $N$, small particle
volume fraction and small $\varepsilon$, the system can be effectively
described by a multiscale model.; 32) Proton Flows, Proton Gradients and Subcellular Architecture in
  Biological Energy Conversion; Hydrogen ions, or protons, provide the medium by which energy is stored and
converted in biological systems. Such pre-eminence relies on the interplay
between interfacial and bulk chemical transformations, according to mechanisms
that are shared by organisms in all phyla of life. The present work provides an
introduction to the fundamental aspects of biological energy management by
focusing on the relationship between vectorial proton flows and the geometry of
energy producing organelles in eukaryotes. The leading models of
proton-mediated energy conversion, the delocalised proton (or chemiosmotic)
model and the localised proton model, are presented in a complementary
perspective. While the delocalised model provides a description that relies on
equilibrium thermodynamics, the localised model addresses dynamic processes
that are better described using out-of-equilibrium thermodynamics. The work
reviews the salient aspects of such mechanisms, traces the development of our
present understanding, and highlights areas that are open to future
developments.; 33) Quasinormal mode frequencies and gravitational perturbations of spinning
  black holes in modified gravity through METRICS: The dynamical Chern-Simons
  gravity case; We present the first precise calculations of the gravitational
quasinormal-mode (QNM) frequencies for spinning black holes with dimensionless
angular momenta $J/M^2 := a \lesssim 0.75$ in dynamical Chern-Simons gravity.
Using the \textit{Metric pErTuRbations wIth speCtral methodS} (METRICS)
framework, we compute the QNM frequencies of both axial and polar metric
perturbations, focusing on the $nl m = 022$, $033$, and $032$ modes. The
METRICS frequencies for the 022 mode achieve numerical uncertainties $\lesssim
10^{-4}$ when $0 \leq a \leq 0.5$ and $\lesssim 10^{-3}$ for $0.5 \leq a \leq
0.75$, without decoupling or simplifying the linearized field equations. We
also derive optimal fitting polynomials to enable efficient and accurate
evaluations of the leading-order frequency shifts in these modes. The METRICS
frequencies and fitting expressions are a robust and indispensable step toward
enabling gravitational-wave ringdown tests of dynamical Chern-Simons gravity.; 34) NeurOp-Diff:Continuous Remote Sensing Image Super-Resolution via Neural
  Operator Diffusion; Most publicly accessible remote sensing data suffer from low resolution,
limiting their practical applications. To address this, we propose a diffusion
model guided by neural operators for continuous remote sensing image
super-resolution (NeurOp-Diff). Neural operators are used to learn resolution
representations at arbitrary scales, encoding low-resolution (LR) images into
high-dimensional features, which are then used as prior conditions to guide the
diffusion model for denoising. This effectively addresses the artifacts and
excessive smoothing issues present in existing super-resolution (SR) methods,
enabling the generation of high-quality, continuous super-resolution images.
Specifically, we adjust the super-resolution scale by a scaling factor s,
allowing the model to adapt to different super-resolution magnifications.
Furthermore, experiments on multiple datasets demonstrate the effectiveness of
NeurOp-Diff. Our code is available at https://github.com/zerono000/NeurOp-Diff.; 35) Splicer$^{+}$: Secure Hub Placement and Deadlock-Free Routing for
  Payment Channel Network Scalability; Payment channel hub (PCH) is a promising approach for payment channel
networks (PCNs) to improve efficiency by deploying robust hubs to steadily
process off-chain transactions. However, existing PCHs, often preplaced without
considering payment request distribution across PCNs, can lead to load
imbalance. PCNs' reliance on source routing, which makes decisions based solely
on individual sender requests, can degrade performance by overlooking other
requests, thus further impairing scalability. In this paper, we introduce
Splicer$^{+}$, a highly scalable multi-PCH solution based on the trusted
execution environment (TEE). We study tradeoffs in communication overhead
between participants, transform the original NP-hard PCH placement problem by
mixed-integer linear programming, and propose optimal/approximate solutions
with load balancing for different PCN scales using supermodular techniques.
Considering global PCN states and local directly connected sender requests, we
design a deadlock-free routing protocol for PCHs. It dynamically adjusts the
payment processing rate across multiple channels and, combined with TEE,
ensures high-performance routing with confidential computation. We provide a
formal security proof for the Splicer$^{+}$ protocol in the UC-framework.
Extensive evaluations demonstrate the effectiveness of Splicer$^{+}$, with
transaction success ratio ($\uparrow$51.1%), throughput ($\uparrow$181.5%), and
latency outperforming state-of-the-art PCNs.; 36) On type 1 active galactic nuclei with double-peaked [O~{\sc iii}]. I.
  data sample and basic results; Double-peaked narrow emission lines (DPNELs) might be evidence for the
existence of kpc-scale dual AGNs. There are so far large samples of objects
with DPNELs in narrow emission line galaxies. Here, a systematic search is made
to build a sample of type 1 AGNs with double-peaked [O~{\sc~iii}] from Data
Release 16 of the Sloan Digital Sky Survey (SDSS). Through visually inspecting
and fitting [O~{\sc~iii}], fitting broad H$\alpha$ emission lines, performing
F-test for [O~{\sc~iii}] profiles, and checking broad H$\beta$ and
[O~{\sc~iii}] emission lines, we select 62 type 1 AGNs with reliable
double-peaked [O~{\sc~iii}] from 11557 QSOs with z < 0.3. After visually
checking the 62 SDSS multi-color images, we find only seven objects with signs
of merging. Four possible models for the double-peaked [O~{\sc~iii}] observed
in our sample are discussed: the superposition model, AGN outflow model, dual
AGN model, and rotating disk model. However, the current results can not
provide any one explanation conclusively, and additional observational data are
needed to provide the details of narrow line regions. But at least 22 objects
with different velocity offsets between double-peaked [O~{\sc~iii}] and narrow
H$\alpha$ emission lines could be excluded as dual AGN candidates. The relative
velocity offsets of the [O~{\sc~iii}] blue-shifted/red-shifted components are
negative to their line flux ratios, which is consistent with dual AGN model.
This work provides a new sample of 62 type 1 AGNs with double-peaked
[O~{\sc~iii}] for further study.; 37) From division to extension; We present a short proof of a version of the Ohsawa-Takegoshi-Manivel $L^2$
extension theorem as a corollary of a Skoda-type $L^2$ division theorem with
bounded generators. The new division theorem is of independent interest: the
boundedness of generators allows to send the parameter $\alpha>1$ of the usual
$L^2$ division theorems to 1 in the norm of the datum of the division. As an
aside, we also use the new division theorem to prove a Brian\c{c}on-Skoda-type
result.; 38) Efficient Reconciliation of Continuous Variable Quantum Key Distribution
  with Multiplicatively Repeated Non-Binary LDPC Codes; Continuous variable quantum key distribution bears the promise of simple
quantum key distribution directly compatible with commercial off the shelf
equipment. However, for a long time its performance was hindered by the absence
of good classical postprocessing capable of distilling secret-keys in the noisy
regime. Advanced coding solutions in the past years have partially addressed
this problem enabling record transmission distances of up to 165 km, and 206 km
over ultra-low loss fiber. In this paper, we show that a very simple coding
solution with a single code is sufficient to extract keys at all noise levels.
This solution has performance competitive with prior results for all levels of
noise, and we show that non-zero keys can be distilled up to a record distance
of 192 km assuming the standard loss of a single-mode optical fiber, and 240 km
over ultra-low loss fibers. Low-rate codes are constructed using
multiplicatively repeated non-binary low-density parity-check codes over a
finite field of characteristic two. This construction only makes use of a
(2,k)-regular non-binary low-density parity-check code as mother code, such
that code design is in fact not required, thus trivializing the code
construction procedure. The construction is also inherently rate-adaptive
thereby allowing to easily create codes of any rate. Rate-adaptive codes are of
special interest for the efficient reconciliation of errors over time or
arbitrary varying channels, as is the case with quantum key distribution. In
short, these codes are highly efficient when reconciling errors over a very
noisy communication channel, and perform well even for short block-length
codes. Finally, the proposed solution is known to be easily amenable to
hardware implementations, thus addressing also the requirements for practical
reconciliation in continuous variable quantum key distribution.; 39) Multi-task Learning for Identification of Porcelain in Song and Yuan
  Dynasties; Chinese porcelain holds immense historical and cultural value, making its
accurate classification essential for archaeological research and cultural
heritage preservation. Traditional classification methods rely heavily on
expert analysis, which is time-consuming, subjective, and difficult to scale.
This paper explores the application of DL and transfer learning techniques to
automate the classification of porcelain artifacts across four key attributes:
dynasty, glaze, ware, and type. We evaluate four Convolutional Neural Networks
(CNNs) - ResNet50, MobileNetV2, VGG16, and InceptionV3 - comparing their
performance with and without pre-trained weights. Our results demonstrate that
transfer learning significantly enhances classification accuracy, particularly
for complex tasks like type classification, where models trained from scratch
exhibit lower performance. MobileNetV2 and ResNet50 consistently achieve high
accuracy and robustness across all tasks, while VGG16 struggles with more
diverse classifications. We further discuss the impact of dataset limitations
and propose future directions, including domain-specific pre-training,
integration of attention mechanisms, explainable AI methods, and generalization
to other cultural artifacts.; 40) Memorization and Generalization in Generative Diffusion under the
  Manifold Hypothesis; We study the memorization and generalization capabilities of a Diffusion
Model (DM) in the case of structured data defined on a latent manifold. We
specifically consider a set of $P$ mono-modal data points in $N$ dimensions
lying on a latent subspace of dimension $D = \alpha_D N$, according to the
Hidden Manifold Model (HMM). Our analysis leverages the recently introduced
formalism based on the statistical physics of the Random Energy Model (REM). We
provide evidence for the existence of an onset time $t_{o} > t_c$ when traps
appear in the potential without affecting the typical diffusive trajectory. The
size of the basins of attraction of such traps is computed as a function of
time. Moreover, we derive the collapse time $t_{c}$ at which trajectories fall
in the basin of one of the training points, implying memorization. An explicit
formula for $t_c$ is given as a function of $P$ and the ratio $\alpha_D$,
proving that the curse of dimensionality issue does not hold for highly
structured data, i.e. $\alpha_D\ll 1$, regardless of the non-linearity of the
manifold surface. We also prove that collapse coincides with the condensation
transition in the REM. Eventually, the degree of generalization of DMs is
formulated in terms of the Kullback-Leibler divergence between the exact and
the empirical distribution of the sampled configurations: we show the existence
of an additional time $t_{g}<t_{c}<t_{o}$ such that the distance between the
empirical measure of the data and the ground-truth is minimal.
Counter-intuitively, the best generalization performance is found within the
memorization phase of the model. We conclude that the generalization
performance of DMs benefit from highly structured data since $t_g$ approaches
zero faster than $t_c$ when $\alpha_D \rightarrow 0$.; 41) Measuring ultrafast laser pulses using a single-shot amplitude swing
  implementation; Single-shot characterization techniques are crucial when dealing with
shot-to-shot pulse-shape fluctuations (e.g., unstable laser systems,
high-power, or with low repetition rate) since the scanning configurations
cannot measure single pulses. The demand for simple setups that can be easily
adapted to a wide variety of experimental conditions is continuously rising. In
this work, we propose a single-shot implementation of amplitude swing,
maintaining the compactness, versatility, and robustness of the scanning
versions of this technique. First, we theoretically study the proposed
implementation, based on a pair of uniaxial wedges. Then, we present the
retrieval ptychographic algorithm. Finally, we experimentally demonstrate the
setup by comparing the single-shot and scanning traces and their retrieved
pulses. In sum, we provide the ultrafast science community with a simple and
versatile setup capable of measuring single laser pulses, which is necessary
for characterizing fluctuating pulse trains, meeting the current increasing
demand.; 42) An extensive simulation study evaluating the interaction of resampling
  techniques across multiple causal discovery contexts; Despite the accelerating presence of exploratory causal analysis in modern
science and medicine, the available non-experimental methods for validating
causal models are not well characterized. One of the most popular methods is to
evaluate the stability of model features after resampling the data, similar to
resampling methods for estimating confidence intervals in statistics. Many
aspects of this approach have received little to no attention, however, such as
whether the choice of resampling method should depend on the sample size,
algorithms being used, or algorithm tuning parameters. We present theoretical
results proving that certain resampling methods closely emulate the assignment
of specific values to algorithm tuning parameters. We also report the results
of extensive simulation experiments, which verify the theoretical result and
provide substantial data to aid researchers in further characterizing
resampling in the context of causal discovery analysis. Together, the
theoretical work and simulation results provide specific guidance on how
resampling methods and tuning parameters should be selected in practice.; 43) Detecting Topological Phase Transition in Superconductor-Semiconductor
  Hybrids by Electronic Raman Spectroscopy; In superconductor-semiconductor hybrids, applying a magnetic field closes a
trivial bulk gap and causes a topological phase transition (TPT), resulting in
the emergence of Majorana zero modes at both ends of the wires. However,
trivial Andreev bound states formed at the interface with metallic leads mimic
the local Majorana properties, making it difficult to detect the TPT through
local conductance measurements. In this work, we investigate the detection of
the bulk TPT by exploiting the static and dynamic density response of the
hybrid system. In particular, we demonstrate that the dynamical renormalized
responses reveal the characteristic electronic structure and detect the TPT,
which we then show to produce strong intensities of Raman scattering.
Furthermore, we find that gapless plasmons emerge in the normal state,
signaling the bulk Lifshitz transition. Our results thus predict that the bulk
response of superconducting nanowires is a powerful spectroscopic approach to
detect the bulk topological phase transition.; 44) Exploring Gender Disparities in Automatic Speech Recognition Technology; This study investigates factors influencing Automatic Speech Recognition
(ASR) systems' fairness and performance across genders, beyond the conventional
examination of demographics. Using the LibriSpeech dataset and the Whisper
small model, we analyze how performance varies across different gender
representations in training data. Our findings suggest a complex interplay
between the gender ratio in training data and ASR performance. Optimal fairness
occurs at specific gender distributions rather than a simple 50-50 split.
Furthermore, our findings suggest that factors like pitch variability can
significantly affect ASR accuracy. This research contributes to a deeper
understanding of biases in ASR systems, highlighting the importance of
carefully curated training data in mitigating gender bias.; 45) Tensor-based Dinkelbach method for computing generalized tensor
  eigenvalues and its applications; In this paper, we propose a novel tensor-based Dinkelbach--Type method for
computing extremal tensor generalized eigenvalues. We show that the extremal
tensor generalized eigenvalue can be reformulated as a critical subproblem of
the classical Dinkelbach--Type method, which can subsequently be expressed as a
multilinear optimization problem (MOP). The MOP is solved under a spherical
constraint using an efficient proximal alternative minimization method, in
which we rigorously establish the global convergence. Additionally, the
equivalent MOP is reformulated as an unconstrained optimization problem,
allowing for the analysis of the Kurdyka-Lojasiewicz (KL) exponent and
providing an explicit expression for the convergence rate of the proposed
algorithm. Preliminary numerical experiments on solving extremal tensor
generalized eigenvalues and minimizing high-order trust-region subproblems are
provided, validating the efficacy and practical utility of the proposed method.; 46) Phase-matching of high harmonic generation in twisted solids; High harmonic generation (HHG) in solids could enable attosecond and
ultraviolet light sources with high compactness, great controllability and rich
functions. However, the HHG process is accompanied by a quite large wavevector
mismatch that is uncompensated by any traditional phase-matching method,
directly limiting its energy conversion efficiency. Here, we propose an
effective strategy for phase-matching of HHG with arbitrary harmonic orders in
solids. Two flakes of solids with an interlayer twist induce a nonlinear
optical phase that depends on the crystal symmetry, twist angle and harmonic
order, which can be accurately designed to compensate for the phase mismatch in
HHG. Guided by the twist-phase-matching theory, we achieved a record-high
conversion efficiency of $~1.5\times10^{-5}$ for the fifth HHG in twisted
hexagonal boron nitride crystals with a total thickness of only 1 ${\mu}m$. Our
work establishes a foundation for developing ultrashort-wavelength and
ultrafast-pulse laser sources in compact solid-state tabletop systems for
fundamental and applied sciences.; 47) Multivariable Stochastic Newton-Based Extremum Seeking with Delays; This paper presents a Newton-based stochastic extremum-seeking control method
for real-time optimization in multi-input systems with distinct input delays.
It combines predictor-based feedback and Hessian inverse estimation via
stochastic perturbations to enable delay compensation with user-defined
convergence rates. The method ensures exponential stability and convergence
near the unknown extremum, even under long delays. It extends to multi-input,
single-output systems with cross-coupled channels. Stability is analyzed using
backstepping and infinite-dimensional averaging. Numerical simulations
demonstrate its effectiveness in handling time-delayed channels, showcasing
both the challenges and benefits of real-time optimization in distributed
parameter settings.; 48) The erasure of intensive livestock farming in text-to-image generative
  AI; Generative AI (e.g., ChatGPT) is increasingly integrated into people's daily
lives. While it is known that AI perpetuates biases against marginalized human
groups, their impact on non-human animals remains understudied. We found that
ChatGPT's text-to-image model (DALL-E 3) introduces a strong bias toward
romanticizing livestock farming as dairy cows on pasture and pigs rooting in
mud. This bias remained when we requested realistic depictions and was only
mitigated when the automatic prompt revision was inhibited. Most farmed animal
in industrialized countries are reared indoors with limited space per animal,
which fail to resonate with societal values. Inhibiting prompt revision
resulted in images that more closely reflected modern farming practices; for
example, cows housed indoors accessing feed through metal headlocks, and pigs
behind metal railings on concrete floors in indoor facilities. While OpenAI
introduced prompt revision to mitigate bias, in the case of farmed animal
production systems, it paradoxically introduces a strong bias towards
unrealistic farming practices.; 49) Explicit Construction of Classical and Quantum Quasi-Cyclic Low-Density
  Parity-Check Codes with Column Weight 2 and Girth 12; This study proposes an explicit construction method for classical and quantum
quasi-cyclic low-density parity-check (QC-LDPC) codes with a girth of 12. The
proposed method designs parity-check matrices that maximize the girth while
maintaining an orthogonal structure suitable for quantum error correction. By
utilizing algebraic techniques, short cycles are eliminated, which improves
error correction performance. Additionally, this method is extended to
non-binary LDPC codes and spatially-coupled LDPC codes, demonstrating that both
the girth and orthogonality can be preserved. The results of this study enable
the design of high-performance quantum error correction codes without the need
for random search.; 50) Building Interval Type-2 Fuzzy Membership Function: A Deck of Cards
  based Co-constructive Approach; Since its inception, Fuzzy Set has been widely used to handle uncertainty and
imprecision in decision-making. However, conventional fuzzy sets, often
referred to as type-1 fuzzy sets (T1FSs) have limitations in capturing higher
levels of uncertainty, particularly when decision-makers (DMs) express
hesitation or ambiguity in membership degree. To address this, Interval Type-2
Fuzzy Sets (IT2FSs) have been introduced by incorporating uncertainty in
membership degree allocation, which enhanced flexibility in modelling
subjective judgments. Despite their advantages, existing IT2FS construction
methods often lack active involvement from DMs and that limits the
interpretability and effectiveness of decision models. This study proposes a
socio-technical co-constructive approach for developing IT2FS models of
linguistic terms by facilitating the active involvement of DMs in preference
elicitation and its application in multicriteria decision-making (MCDM)
problems. Our methodology is structured in two phases. The first phase involves
an interactive process between the DM and the decision analyst, in which a
modified version of Deck-of-Cards (DoC) method is proposed to construct T1FS
membership functions on a ratio scale. We then extend this method to
incorporate ambiguity in subjective judgment and that resulted in an IT2FS
model that better captures uncertainty in DM's linguistic assessments. The
second phase formalizes the constructed IT2FS model for application in MCDM by
defining an appropriate mathematical representation of such information,
aggregation rules, and an admissible ordering principle. The proposed framework
enhances the reliability and effectiveness of fuzzy decision-making not only by
accurately representing DM's personalized semantics of linguistic information.; 51) Examining the Representation of Youth in the US Policy Documents through
  the Lens of Research; This study explores the representation of youth in US policy documents by
analyzing how research on youth topics is cited within these policies. The
research focuses on three key questions: identifying the frequently discussed
topics in youth research that receive citations in policy documents, discerning
patterns in youth research that contribute to higher citation rates in policy,
and comparing the alignment between topics in youth research and those in
citing policy documents. Through this analysis, the study aims to shed light on
the relationship between academic research and policy formulation, highlighting
areas where youth issues are effectively integrated into policy and
contributing to the broader goal of enhancing youth engagement in societal
decision-making processes.; 52) Test-Time Optimization for Domain Adaptive Open Vocabulary Segmentation; We present Seg-TTO, a novel framework for zero-shot, open-vocabulary semantic
segmentation (OVSS), designed to excel in specialized domain tasks. While
current open-vocabulary approaches show impressive performance on standard
segmentation benchmarks under zero-shot settings, they fall short of supervised
counterparts on highly domain-specific datasets. We focus on
segmentation-specific test-time optimization to address this gap. Segmentation
requires an understanding of multiple concepts within a single image while
retaining the locality and spatial structure of representations. We propose a
novel self-supervised objective adhering to these requirements and use it to
align the model parameters with input images at test time. In the textual
modality, we learn multiple embeddings for each category to capture diverse
concepts within an image, while in the visual modality, we calculate
pixel-level losses followed by embedding aggregation operations specific to
preserving spatial structure. Our resulting framework termed Seg-TTO is a
plug-and-play module. We integrate Seg-TTO with three state-of-the-art OVSS
approaches and evaluate across 22 challenging OVSS tasks covering a range of
specialized domains. Our Seg-TTO demonstrates clear performance improvements
(up to 27% mIoU increase on some datasets) establishing new state-of-the-art.
Our code and models will be released publicly.; 53) A LP-rounding based algorithm for soft capacitated facility location
  problem with submodular penalties; The soft capacitated facility location problem (SCFLP) is a classic
combinatorial optimization problem, with its variants widely applied in the
fields of operations research and computer science. In the SCFLP, given a set
$\mathcal{F}$ of facilities and a set $\mathcal{D}$ of clients, each facility
has a capacity and an open cost, allowing to open multiple times, and each
client has a demand.
  This problem is to find a subset of facilities in $\mathcal{F}$ and connect
each client to the facilities opened, such that the total cost including open
cost and connection cost is minimied. SCFLP is a NP-hard problem, which has led
to a focus on approximation algorithms. Based on this, we consider a variant,
that is, soft capacitated facility location problem with submodular penalties
(SCFLPSP), which allows some clients not to be served by accepting the penalty
cost. And we consider the integer splittable case of demand, that is, the
demand of each client is served by multiple facilities with the integer service
amount by each facility. Based on LP-rounding, we propose a $(\lambda
R+4)$-approximation algorithm, where $R=\frac{\max_{i \in \mathcal{F}
}f_i}{\min_{i \in \mathcal{F} }f_i},\lambda=\frac{R+\sqrt{R^2+8R}}{2R}$. In
particular, when the open cost is uniform, the approximation ratio is 6.; 54) A Variational Theory for Soft Shells; Three general modes are distinguished in the deformation of a thin shell;
these are stretching, drilling, and bending. Of these, the drilling mode is the
one more likely to emerge in a soft matter shell (as compared to a hard,
structural one), as it is ignited by a swerve of material fibers about the
local normal. We propose a hyperelastic theory for soft shells, based on a
separation criterion that envisages the strain-energy density as the sum of
three independent pure measures of stretching, drilling, and bending. Each
individual measure is prescribed to vanish on all other companion modes. The
result is a direct, second-grade theory featuring a bending energy quartic in
an invariant strain descriptor that stems from the polar rotation hidden in the
deformation gradient (although quadratic energies are also appropriate in
special cases). The proposed energy functional has a multi-well character,
which fosters cases of soft elasticity (with a continuum of ground states)
related to minimal surfaces.; 55) CopySpec: Accelerating LLMs with Speculative Copy-and-Paste Without
  Compromising Quality; We introduce CopySpec, an innovative technique designed to tackle the
inefficiencies LLMs face when generating responses that closely resemble
previous outputs. CopySpec identifies repeated sequences in the model's chat
history and speculates that the same tokens will follow, enabling seamless
copying without compromising output quality or requiring additional GPU memory.
To evaluate the effectiveness of our approach, we conducted experiments using
five LLMs and five datasets: MT-Bench, CNN/DM, GSM-8K, HumanEval, and our newly
created dataset, MT-Redundant. MT-Redundant, introduced in this paper,
transforms the second turn of MT-Bench into a request for variations of the
first turn's answer, simulating real-world scenarios where users request
modifications to prior responses. Our results demonstrate significant
speed-ups: up to 2.35x on CNN/DM, 3.08x on the second turn of select
MT-Redundant categories, and 2.66x on the third turn of GSM-8K's
self-correction tasks. Moreover, we show that CopySpec integrates seamlessly
with speculative decoding, yielding an average 49% additional speed-up over
speculative decoding for the second turn of MT-Redundant across all eight
categories. While LLMs, even with speculative decoding, suffer from slower
inference as context sizes grow, CopySpec leverages the expanded context to
accelerate inference, making it faster as the context size increases. Our code
and dataset are publicly available at https://github.com/RazvanDu/CopySpec.; 56) FlexQuant: Elastic Quantization Framework for Locally Hosted LLM on Edge
  Devices; Deploying LLMs on edge devices presents serious technical challenges. Memory
elasticity is crucial for edge devices with unified memory, where memory is
shared and fluctuates dynamically. Existing solutions suffer from either poor
transition granularity or high storage costs. We propose FlexQuant, a novel
elasticity framework that generates an ensemble of quantized models, providing
an elastic hosting solution with 15x granularity improvement and 10x storage
reduction compared to SoTA methods. FlexQuant works with most quantization
methods and creates a family of trade-off options under various storage limits
through our pruning method. It brings great performance and flexibility to the
edge deployment of LLMs.; 57) The quantum nature of ubiquitous vibrational features revealed for
  ethylene glycol; Vibrational properties of molecules are of widespread interest and importance
in chemistry and biochemistry. The reliability of widely employed approximate
computational methods is questioned here against the complex experimental
spectrum of ethylene glycol. Comparisons between quantum vibrational
self-consistent field and virtual-state configuration interaction (VSCF/VCI),
adiabatically switched semiclassical initial value representation (AS SCIVR),
and thermostatted ring polymer molecular dynamics (TRPMD) calculations are made
using a full-dimensional machine-learned potential energy surface. Calculations
are done for five low-lying conformers and compared with the experiment, with a
focus on the high-frequency, OH-stretches, and CH-stretches, part of the
spectrum. Fermi resonances are found in the analysis of VSCF/VCI eigenstates
belonging to the CH-stretching band. Results of comparable accuracy, quality,
and level of detail are obtained by means of AS SCIVR. The current VSCF/VCI and
AS-SCIVR power spectra largely close the gaps between the experiment and TRPMD
and classical MD calculations. Analysis of these results provide guidance on
what level of accuracy to expect from TRPMD and classical MD calculations of
the vibrational spectra for ubiquitous CH and OH-stretches bands. This work
shows that even general vibrational features require a proper quantum treatment
usually not achievable by the most popular theoretical approaches.; 58) MedForge: Building Medical Foundation Models Like Open Source Software
  Development; Foundational models (FMs) have made significant strides in the healthcare
domain. Yet the data silo challenge and privacy concern remain in healthcare
systems, hindering safe medical data sharing and collaborative model
development among institutions. The collection and curation of scalable
clinical datasets increasingly become the bottleneck for training strong FMs.
In this study, we propose Medical Foundation Models Merging (MedForge), a
cooperative framework enabling a community-driven medical foundation model
development, meanwhile preventing the information leakage of raw patient data
and mitigating synchronization model development issues across clinical
institutions. MedForge offers a bottom-up model construction mechanism by
flexibly merging task-specific Low-Rank Adaptation (LoRA) modules, which can
adapt to downstream tasks while retaining original model parameters. Through an
asynchronous LoRA module integration scheme, the resulting composite model can
progressively enhance its comprehensive performance on various clinical tasks.
MedForge shows strong performance on multiple clinical datasets (e.g., breast
cancer, lung cancer, and colon cancer) collected from different institutions.
Our major findings highlight the value of collaborative foundation models in
advancing multi-center clinical collaboration effectively and cohesively. Our
code is publicly available at https://github.com/TanZheling/MedForge.; 59) AdaFlow: Efficient Long Video Editing via Adaptive Attention Slimming
  And Keyframe Selection; Despite great progress, text-driven long video editing is still notoriously
challenging mainly due to excessive memory overhead. Although recent efforts
have simplified this task into a two-step process of keyframe translation and
interpolation generation, the token-wise keyframe translation still plagues the
upper limit of video length. In this paper, we propose a novel and
training-free approach towards efficient and effective long video editing,
termed AdaFlow. We first reveal that not all tokens of video frames hold equal
importance for keyframe translation, based on which we propose an Adaptive
Attention Slimming scheme for AdaFlow to squeeze the $KV$ sequence, thus
increasing the number of keyframes for translations by an order of magnitude.
In addition, an Adaptive Keyframe Selection scheme is also equipped to select
the representative frames for joint editing, further improving generation
quality. With these innovative designs, AdaFlow achieves high-quality long
video editing of minutes in one inference, i.e., more than 1$k$ frames on one
A800 GPU, which is about ten times longer than the compared methods, e.g.,
TokenFlow. To validate AdaFlow, we also build a new benchmark for long video
editing with high-quality annotations, termed LongV-EVAL. Our code is released
at: https://github.com/jidantang55/AdaFlow.; 60) Induction and Recursion Principles in a Higher-Order Quantitative Logic; Quantitative logic reasons about the degree to which formulas are satisfied.
This paper studies the fundamental reasoning principles of higher-order
quantitative logic and their application to reasoning about probabilistic
programs and processes.
  We construct an affine calculus for 1-bounded complete metric spaces and the
monad for probability measures equipped with the Kantorovic distance. The
calculus includes a form of guarded recursion interpreted via Banach's fixed
point theorem, useful, e.g., for recursive programming with processes. We then
define an affine higher-order quantitative logic for reasoning about terms of
our calculus. The logic includes novel principles for guarded recursion, and
induction over probability measures and natural numbers.
  Examples of reasoning in the logic include proofs of upper bounds on
distances of processes. We also show how our logic can express coupling proofs
- a powerful technique for comparing probabilistic processes.; 61) Efficient Prompting for Continual Adaptation to Missing Modalities; Missing modality issues are common in real-world applications, arising from
factors such as equipment failures and privacy concerns. When fine-tuning
pre-trained models on downstream datasets with missing modalities, performance
can degrade significantly. Current methods often aggregate various missing
cases to train recovery modules or align multimodal features, resulting in
suboptimal performance, high computational costs, and the risk of catastrophic
forgetting in continual environments where data arrives sequentially. In this
paper, we formulate the dynamic missing modality problem as a continual
learning task and introduce the continual multimodal missing modality task. To
address this challenge efficiently, we introduce three types of prompts:
modality-specific, task-aware, and task-specific prompts. These prompts enable
the model to learn intra-modality, inter-modality, intra-task, and inter-task
features. Furthermore, we propose a contrastive task interaction strategy to
explicitly learn prompts correlating different modalities. We conduct extensive
experiments on three public datasets, where our method consistently outperforms
state-of-the-art approaches.; 62) Sample-Efficient Reinforcement Learning from Human Feedback via
  Information-Directed Sampling; We study the problem of reinforcement learning from human feedback (RLHF), a
critical problem in training large language models, from a theoretical
perspective. Our main contribution is the design of novel sample-efficient RLHF
algorithms based on information-directed sampling (IDS), an online
decision-making principle inspired by information theory. Our algorithms
maximize the sum of the value function and a mutual information term that
encourages exploration of the unknown environment (which quantifies the
information gained about the environment through observed human feedback data).
To tackle the challenge of large state spaces and improve sample efficiency, we
construct a simplified \emph{surrogate environment} and introduce a novel
distance measure (named the \emph{$\ell_g$-distance}), enabling our IDS-based
algorithm to achieve a Bayesian regret upper bound of order
$O(H^{\frac{3}{2}}\sqrt{\log(K(\epsilon)) T})$, where $H$ is the episode
length, $T$ is the number of episode and $K(\epsilon)$ is related to the
covering number of the environment. Specializing to the tabular settings, this
regret bound is of order $\tilde{O}(H^2\sqrt{SAT})$, where $S$ and $A$ are the
numbers of states and actions. Finally, we propose an Approximate-IDS algorithm
that is computationally more efficient while maintaining nearly the same sample
efficiency. The design principle of this approximate algorithm is not only
effective in RLHF settings but also applicable to the standard RL framework.
Moreover, our work showcases the value of information theory in reinforcement
learning and in the training of large language models.; 63) LazyMAR: Accelerating Masked Autoregressive Models via Feature Caching; Masked Autoregressive (MAR) models have emerged as a promising approach in
image generation, expected to surpass traditional autoregressive models in
computational efficiency by leveraging the capability of parallel decoding.
However, their dependence on bidirectional self-attention inherently conflicts
with conventional KV caching mechanisms, creating unexpected computational
bottlenecks that undermine their expected efficiency. To address this problem,
this paper studies the caching mechanism for MAR by leveraging two types of
redundancy: Token Redundancy indicates that a large portion of tokens have very
similar representations in the adjacent decoding steps, which allows us to
first cache them in previous steps and then reuse them in the later steps.
Condition Redundancy indicates that the difference between conditional and
unconditional output in classifier-free guidance exhibits very similar values
in adjacent steps. Based on these two redundancies, we propose LazyMAR, which
introduces two caching mechanisms to handle them one by one. LazyMAR is
training-free and plug-and-play for all MAR models. Experimental results
demonstrate that our method achieves 2.83 times acceleration with almost no
drop in generation quality. Our codes will be released in
https://github.com/feihongyan1/LazyMAR.; 64) On the origin of bulk-related anisotropies in surface optical spectra; Reflection anisotropy spectroscopy (RAS) is a powerful method for probing the
optical properties of surfaces, used routinely in research and industrial
applications, yet the origin of 'bulk-related' features that appear in the
spectra of various surfaces has been debated for nearly 40 years. It is often
argued that these features are related to surface-induced bulk anisotropy
(SIBA) because they coincide with critical energies of the bulk dielectric
function. In general, any quantitative RAS theory must include excitonic
effects as they significantly influence the spectra and are believed to be the
key to determining the origin of SIBA features. Here, we introduce a
layer-resolved exciton localization (LREL) measure within the framework of
many-body perturbation theory, which enables a quantitative analysis of the
origins of 'bulk-related' RAS features. Applying LREL to arsenic-modified
silicon reconstructions reveals that, depending on the surface reconstruction,
the 'apparent' SIBA features arise primarily from states localized at the
surface, with only a small contribution from the underlying layers. Our
findings, further supported by the fact that the calculated spectra agree well
with low-temperature RAS measurements, challenge the conventional explanation
of 'bulk-related' RAS features. They indicate that in many instances
bulk-enhanced surface anisotropies (BESA)-the opposite of SIBA-contribute to,
or are even responsible for, 'bulk-related' RAS features. Therefore, we suggest
that previously studied semiconductor surfaces, which exhibit 'bulk-related'
features in their spectra, should be reanalyzed using the presented method.; 65) Contrastive Token-level Explanations for Graph-based Rumour Detection; The widespread use of social media has accelerated the dissemination of
information, but it has also facilitated the spread of harmful rumours, which
can disrupt economies, influence political outcomes, and exacerbate public
health crises, such as the COVID-19 pandemic. While Graph Neural Network
(GNN)-based approaches have shown significant promise in automated rumour
detection, they often lack transparency, making their predictions difficult to
interpret. Existing graph explainability techniques fall short in addressing
the unique challenges posed by the dependencies among feature dimensions in
high-dimensional text embeddings used in GNN-based models. In this paper, we
introduce Contrastive Token Layerwise Relevance Propagation (CT-LRP), a novel
framework designed to enhance the explainability of GNN-based rumour detection.
CT-LRP extends current graph explainability methods by providing token-level
explanations that offer greater granularity and interpretability. We evaluate
the effectiveness of CT-LRP across multiple GNN models trained on three
publicly available rumour detection datasets, demonstrating that it
consistently produces high-fidelity, meaningful explanations, paving the way
for more robust and trustworthy rumour detection systems.; 66) Plastic computing, the cloud continuum journey beyond infinity; The ever increasing challenges introduced by the diversity of current and
envisioned network technologies and IT infrastructure draw a highly distributed
and heterogeneous topology where innovative services must be optimally deployed
to guarantee maximum level of quality for users. Indeed, paradigms such as the
cloud continuum, bringing together edge and cloud computing, along with the new
opportunities coming out by considering non-terrestrial networks connecting
future 6G ecosystems, all with no doubt facilitate the development of
innovative services in many different areas and verticals. However, considering
the intensive data and quality requirements demanded by these services, the
distribution of the execution tasks must be optimally designed. On the
infrastructure side, several initiatives are already active aimed at providing
a Meta-OS that may seamlessly manage the different actors (services,
infrastructure and users) playing under this paradigm. However, several aspects
remain yet limited, particularly when referring to the mapping of resources
into services, where innovative technologies based on bidirectional
coordination and modeling may be pivotal for an optimal performance. In
addition, the upcoming demands coming from the adoption of network technologies
easing users connection with high levels of quality, such as 6G, as well the
study of NTN open up the traditional cloud continuum to include also satellites
that may extend the cloud paradigm further than ever considered. This paper
shows a seed work toward an extendable paradigm so called as plastic computing
whose main objective is to optimize service performance and users satisfaction,
through considering a bidirectional strategy, easily extendable to adopt novel
network and IT technologies and paradigms. Finally, two examples are briefly
introduced to highlight the potential benefits of the plastic computing
adoption; 67) Precision mass measurements of $^{74-76}$Sr using TITAN's
  Multiple-Reflection Time-of-Flight Mass Spectrometer; We report precision mass measurements of $^{74-76}$Sr performed with the
TITAN Multiple-Reflection Time-of-Flight Mass Spectrometer. This marks a first
time mass measurement of $^{74}$Sr and gives increased mass precision to both
$^{75}$Sr and $^{76}$Sr which were previously measured using storage ring and
Penning trap methods, respectively. This completes the A = 74, T = 1 isospin
triplet and gives increased precision to the A = 75, T = 1/2 isospin doublet
which are both the heaviest experimentally evaluated triplets and doublets to
date. The new data allow us to evaluate coefficients of the isobaric multiplet
mass equation for the first time at A = 74, and with increased precision at A =
75. With increased precision of 75Sr, we confirm the recent measurement
reported by CSRe which was used to remove a staggering anomaly in the doublets.
New ab initio valence-space in-medium similarity renormalization group
calculations of the T = 1 triplet are presented at A = 74. We also investigate
the impact of the new mass data on the reaction flow of the rapid proton
capture process in type I x-ray bursts using a single-zone model.; 68) What is a Feature, Really? Toward a Unified Understanding Across SE
  Disciplines; In software engineering, the concept of a ``feature'' is widely used but
inconsistently defined across disciplines such as requirements engineering (RE)
and software product lines (SPL). This lack of consistency often results in
communication gaps, rework, and inefficiencies in projects. To address these
challenges, this paper proposes an empirical, data-driven approach to explore
how features are described, implemented, and managed across real-world
projects, starting with open-source software (OSS). By analyzing
feature-related branches in OSS repositories, we identify patterns in
contributor behavior, feature implementation, and project management
activities. Our findings provide actionable insights to improve project
planning, resource allocation, and team coordination. Additionally, we outline
a roadmap to unify the understanding of features across software engineering
disciplines. This research aims to bridge gaps between academic inquiry and
practical strategies, fostering better feature planning and development
workflows in diverse project environments.; 69) AI Mimicry and Human Dignity: Chatbot Use as a Violation of Self-Respect; This paper investigates how human interactions with AI-powered chatbots may
offend human dignity. Current chatbots, driven by large language models (LLMs),
mimic human linguistic behaviour but lack the moral and rational capacities
essential for genuine interpersonal respect. Human beings are prone to
anthropomorphise chatbots. Indeed, chatbots appear to be deliberately designed
to elicit that response. As a result, human beings' behaviour toward chatbots
often resembles behaviours typical of interaction between moral agents. Drawing
on a second-personal, relational account of dignity, we argue that interacting
with chatbots in this way is incompatible with the dignity of users. We show
that, since second-personal respect is premised on reciprocal recognition of
second-personal authority, behaving towards chatbots in ways that convey
second-personal respect is bound to misfire in morally problematic ways, given
the lack of reciprocity. Consequently, such chatbot interactions amount to
subtle but significant violations of self-respect: the respect we are dutybound
to show for our own dignity. We illustrate this by discussing four actual
chatbot use cases (information retrieval, customer service, advising, and
companionship), and propound that the increasing societal pressure to engage in
such interactions with chatbots poses a hitherto underappreciated threat to
human dignity.; 70) Incidental Anterior Mediastinal Nodular Lesions on Chest CT in Asymptomatic Subjects; Screening for lung cancer: 2023 guideline update from the American Cancer Society; Objective The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods We included 56,358 consecutive participants (mean age 52.4 ± 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66–0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01–3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes.; Abstract Lung cancer is the leading cause of mortality and person‐years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network‐validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit‐to‐radiation‐risk ratios follow‐up examinations. GDG also examined disease burden data National Institute’s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50–80 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low‐dose computed tomography asymptomatic currently smoke or formerly smoked have ≥20 pack‐year ( , ). Before decision made initiate LCS, should engage shared decision‐making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.; 71) MM-PoisonRAG: Disrupting Multimodal RAG with Local and Global Poisoning
  Attacks; Multimodal large language models (MLLMs) equipped with Retrieval Augmented
Generation (RAG) leverage both their rich parametric knowledge and the dynamic,
external knowledge to excel in tasks such as Question Answering. While RAG
enhances MLLMs by grounding responses in query-relevant external knowledge,
this reliance poses a critical yet underexplored safety risk: knowledge
poisoning attacks, where misinformation or irrelevant knowledge is
intentionally injected into external knowledge bases to manipulate model
outputs to be incorrect and even harmful. To expose such vulnerabilities in
multimodal RAG, we propose MM-PoisonRAG, a novel knowledge poisoning attack
framework with two attack strategies: Localized Poisoning Attack (LPA), which
injects query-specific misinformation in both text and images for targeted
manipulation, and Globalized Poisoning Attack (GPA) to provide false guidance
during MLLM generation to elicit nonsensical responses across all queries. We
evaluate our attacks across multiple tasks, models, and access settings,
demonstrating that LPA successfully manipulates the MLLM to generate
attacker-controlled answers, with a success rate of up to 56% on MultiModalQA.
Moreover, GPA completely disrupts model generation to 0% accuracy with just a
single irrelevant knowledge injection. Our results highlight the urgent need
for robust defenses against knowledge poisoning to safeguard multimodal RAG
frameworks.; 72) $\mathbb{L}^p$ $(p>1)$-solutions for BSDEs with jumps and stochastic
  monotone generator; We study multidimensional discontinuous backward stochastic differential
equations in a filtration that supports both a Brownian motion and an
independent integer-valued random measure. Under suitable
$\mathbb{L}^p$-integrability conditions on the data, we establish the existence
and uniqueness of $\mathbb{L}^p$-solutions for both cases: $p \geq 2$ and $p
\in (1,2)$. The generator is assumed to be stochastically monotone in the state
variable $y$, stochastically Lipschitz in the control variables $(z, u)$, and
to satisfy a stochastic linear growth condition, along with an appropriate
$\mathbb{L}^p$-integrability requirement.; 73) COS(M+O)S: Curiosity and RL-Enhanced MCTS for Exploring Story Space via
  Language Models; We present COS(M+O)S, a System 2-inspired framework for open-ended plot
development that systematically explores the vast space of possible story
expansions, enabling a 3B-parameter language model to approach the plot quality
of a 70B model on select short-story tasks. The method accomplishes this by
combining Monte Carlo Tree Search (MCTS), guided by a step-level value model
that rewards moderate surprisal (curiosity) while penalizing incoherence, and
Odds Ratio Preference Optimization (ORPO) to fine-tune the policy on high-value
plot expansions. This iterative reinforcement learning loop systematically
explores multiple candidate plot branches, backpropagates quality signals, and
adapts the policy for faster convergence, notably shifting the policy from
puzzle-based Chain-of-Thought to more character-driven storytelling. In
small-scale tests with short-story prompts, 67%-77% of participants favored
COS(M+O)S's highest-rated expansions over lower-rated ones, suggesting that our
learned value function aligns. GPT-4o ratings further show that COS(M+O)S
surpasses naive single-pass decoding from Llama 3.2 3B by 0.59 SD, coming
within 0.06 SD of Llama 3.1 70B (no significant difference, p=0.93). Pairwise
comparisons with o1 place COS(M+O)S 1.5 SD above the 3B baseline and find no
statistically significant gap from 70B. Nevertheless, absolute story quality
remains modest, constrained by the small model's capacity and limited training
data.; 74) Unshackling Context Length: An Efficient Selective Attention Approach
  through Query-Key Compression; Handling long-context sequences efficiently remains a significant challenge
in large language models (LLMs). Existing methods for token selection in
sequence extrapolation either employ a permanent eviction strategy or select
tokens by chunk, which may lead to the loss of critical information. We propose
Efficient Selective Attention (ESA), a novel approach that extends context
length by efficiently selecting the most critical tokens at the token level to
compute attention. ESA reduces the computational complexity of token selection
by compressing query and key vectors into lower-dimensional representations. We
evaluate ESA on long sequence benchmarks with maximum lengths up to 256k using
open-source LLMs with context lengths of 8k and 32k. ESA outperforms other
selective attention methods, especially in tasks requiring the retrieval of
multiple pieces of information, achieving comparable performance to
full-attention extrapolation methods across various tasks, with superior
results in certain tasks.; 75) Training an LLM-as-a-Judge Model: Pipeline, Insights, and Practical
  Lessons; The rapid advancement of large language models (LLMs) has opened new
possibilities for their adoption as evaluative judges. This paper introduces
Themis, a fine-tuned LLM judge that delivers sophisticated context-aware
evaluations. We provide a comprehensive overview of the development pipeline
for Themis, highlighting its scenario-dependent evaluation prompts and two
novel methods for controlled instruction generation. These designs enable
Themis to effectively distill evaluative skills from teacher models, while
retaining flexibility for continuous development. We introduce two
human-labeled benchmarks for meta-evaluation, demonstrating that Themis can
achieve high alignment with human preferences in an economical manner.
Additionally, we explore insights into the LLM-as-a-judge paradigm, revealing
nuances in performance and the varied effects of reference answers. Notably, we
observe that pure knowledge distillation from strong LLMs, though common, does
not guarantee performance improvement through scaling. We propose a mitigation
strategy based on instruction-following difficulty. Furthermore, we provide
practical guidelines covering data balancing, prompt customization,
multi-objective training, and metric aggregation. We aim for our method and
findings, along with the fine-tuning data, benchmarks, and model checkpoints,
to support future research and development in this area.; 76) Observational constraints on vector-like dark energy; The canonical cosmological model to explain the recent acceleration of the
universe relies on a cosmological constant, and most dynamical dark energy and
modified gravity model alternatives are based on scalar fields. Still, further
alternatives are possible. One of these involves vector fields: under certain
conditions, they can lead to accelerating universes while preserving
large-scale homogeneity and isotropy. We report quantitative observational
constraints on a model previously proposed by Armend\'ariz-Pic\'on and known as
the cosmic triad. We consider several subclasses of the model, which
generically is a parametric extension of the canonical $\Lambda$CDM model, as
well as two possible choices of the triad's potential. Our analysis shows that
any deviations from this limit are constrained to be small. In particular the
preferred present-day values of the matter density and the dark energy equation
of state are fully consistent with those obtained, for the same datasets, in
flat $\Lambda$CDM and $w_0$CDM. The constraints mildly depend on the priors on
the dark energy equation of state, specifically on whether phantom values
thereof are allowed, while the choice of potential does not play a significant
role since any such potential is constrained to be relatively flat.; 77) Driving towards net-zero: The impact of electric vehicle flexibility
  participation on a future Norwegian electricity system; Electric vehicle batteries have a proven flexibility potential which could
serve as an alternative to conventional electricity storage solutions. EV
batteries could support the balancing of supply and demand and the integration
of variable renewable energy into the electricity system. The flexibility
potential from electric vehicles, in distinction to conventional battery
storage, depends on the vehicle user's willingness and opportunity to make
their vehicle available for flexibility. This rate of participation is often
not considered in studies, despite the impact electric vehicle flexibility
could have on the electricity system. This work presents a modelling study of
the Norwegian electricity system, demonstrating how a future net-zero
electricity system can benefit from electric vehicles in terms of integrating
renewables and balancing supply and demand, while considering the rate of
participation. Our findings show electric vehicles' potential to eliminate the
need for stationary battery storage with just 50% participation in
vehicle-to-grid. We find that the flexibility of electric vehicles contributes
to relative reductions in the total cost of the electricity system by almost 4%
and 15% assuming 100% participation in flexible charging and vehicle-to-grid,
respectively.; 78) HyperZero: A Customized End-to-End Auto-Tuning System for Recommendation
  with Hourly Feedback; Modern recommendation systems can be broadly divided into two key stages: the
ranking stage, where the system predicts various user engagements (e.g.,
click-through rate, like rate, follow rate, watch time), and the value model
stage, which aggregates these predictive scores through a function (e.g., a
linear combination defined by a weight vector) to measure the value of each
content by a single numerical score. Both stages play roughly equally important
roles in real industrial systems; however, how to optimize the model weights
for the second stage still lacks systematic study. This paper focuses on
optimizing the second stage through auto-tuning technology. Although general
auto-tuning systems and solutions - both from established production practices
and open-source solutions - can address this problem, they typically require
weeks or even months to identify a feasible solution. Such prolonged tuning
processes are unacceptable in production environments for recommendation
systems, as suboptimal value models can severely degrade user experience. An
effective auto-tuning solution is required to identify a viable model within
2-3 days, rather than the extended timelines typically associated with existing
approaches. In this paper, we introduce a practical auto-tuning system named
HyperZero that addresses these time constraints while effectively solving the
unique challenges inherent in modern recommendation systems. Moreover, this
framework has the potential to be expanded to broader tuning tasks within
recommendation systems.; 79) ChatGPT's advice drives moral judgments with or without justification; Why do users follow moral advice from chatbots? A chatbot is not an
authoritative moral advisor, but it can generate seemingly plausible arguments.
Users do not follow reasoned more readily than unreasoned advice, though, we
find in an experiment. However, this is also true if we attribute advice to a
moral advisor, not a chatbot. Hence, it seems that advice offers users a cheap
way to escape from a moral dilemma. This is a concern that chatbots do not
raise, but they exacerbate it as they make advice easily accessible. We
conclude that it takes ethical in addition to digital literacy to harness users
against moral advice from chatbots.; 80) CoFinDiff: Controllable Financial Diffusion Model for Time Series
  Generation; The generation of synthetic financial data is a critical technology in the
financial domain, addressing challenges posed by limited data availability.
Traditionally, statistical models have been employed to generate synthetic
data. However, these models fail to capture the stylized facts commonly
observed in financial data, limiting their practical applicability. Recently,
machine learning models have been introduced to address the limitations of
statistical models; however, controlling synthetic data generation remains
challenging. We propose CoFinDiff (Controllable Financial Diffusion model), a
synthetic financial data generation model based on conditional diffusion models
that accept conditions about the synthetic time series. By incorporating
conditions derived from price data into the conditional diffusion model via
cross-attention, CoFinDiff learns the relationships between the conditions and
the data, generating synthetic data that align with arbitrary conditions.
Experimental results demonstrate that: (i) synthetic data generated by
CoFinDiff capture stylized facts; (ii) the generated data accurately meet
specified conditions for trends and volatility; (iii) the diversity of the
generated data surpasses that of the baseline models; and (iv) models trained
on CoFinDiff-generated data achieve improved performance in deep hedging task.; 81) Test-Time Code-Switching for Cross-lingual Aspect Sentiment Triplet
  Extraction; Aspect Sentiment Triplet Extraction (ASTE) is a thriving research area with
impressive outcomes being achieved on high-resource languages. However, the
application of cross-lingual transfer to the ASTE task has been relatively
unexplored, and current code-switching methods still suffer from term boundary
detection issues and out-of-dictionary problems. In this study, we introduce a
novel Test-Time Code-SWitching (TT-CSW) framework, which bridges the gap
between the bilingual training phase and the monolingual test-time prediction.
During training, a generative model is developed based on bilingual
code-switched training data and can produce bilingual ASTE triplets for
bilingual inputs. In the testing stage, we employ an alignment-based
code-switching technique for test-time augmentation. Extensive experiments on
cross-lingual ASTE datasets validate the effectiveness of our proposed method.
We achieve an average improvement of 3.7% in terms of weighted-averaged F1 in
four datasets with different languages. Additionally, we set a benchmark using
ChatGPT and GPT-4, and demonstrate that even smaller generative models
fine-tuned with our proposed TT-CSW framework surpass ChatGPT and GPT-4 by
14.2% and 5.0% respectively.; 82) Retrieval Augmented Generation and Understanding in Vision: A Survey and
  New Outlook; Retrieval-augmented generation (RAG) has emerged as a pivotal technique in
artificial intelligence (AI), particularly in enhancing the capabilities of
large language models (LLMs) by enabling access to external, reliable, and
up-to-date knowledge sources. In the context of AI-Generated Content (AIGC),
RAG has proven invaluable by augmenting model outputs with supplementary,
relevant information, thus improving their quality. Recently, the potential of
RAG has extended beyond natural language processing, with emerging methods
integrating retrieval-augmented strategies into the computer vision (CV)
domain. These approaches aim to address the limitations of relying solely on
internal model knowledge by incorporating authoritative external knowledge
bases, thereby improving both the understanding and generation capabilities of
vision models. This survey provides a comprehensive review of the current state
of retrieval-augmented techniques in CV, focusing on two main areas: (I) visual
understanding and (II) visual generation. In the realm of visual understanding,
we systematically review tasks ranging from basic image recognition to complex
applications such as medical report generation and multimodal question
answering. For visual content generation, we examine the application of RAG in
tasks related to image, video, and 3D generation. Furthermore, we explore
recent advancements in RAG for embodied AI, with a particular focus on
applications in planning, task execution, multimodal perception, interaction,
and specialized domains. Given that the integration of retrieval-augmented
techniques in CV is still in its early stages, we also highlight the key
limitations of current approaches and propose future research directions to
drive the development of this promising area.; 83) Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs; Large language models (LLMs) such as OpenAI's o1 have demonstrated remarkable
abilities in complex reasoning tasks by scaling test-time compute and
exhibiting human-like deep thinking. However, we identify a phenomenon we term
underthinking, where o1-like LLMs frequently switch between different reasoning
thoughts without sufficiently exploring promising paths to reach a correct
solution. This behavior leads to inadequate depth of reasoning and decreased
performance, particularly on challenging mathematical problems. To
systematically analyze this issue, we conduct experiments on three challenging
test sets and two representative open-source o1-like models, revealing that
frequent thought switching correlates with incorrect responses. We introduce a
novel metric to quantify underthinking by measuring token efficiency in
incorrect answers. To address underthinking, we propose a decoding strategy
with thought switching penalty TIP that discourages premature transitions
between thoughts, encouraging deeper exploration of each reasoning path.
Experimental results demonstrate that our approach improves accuracy across
challenging datasets without requiring model fine-tuning. Our findings
contribute to understanding reasoning inefficiencies in o1-like LLMs and offer
a practical solution to enhance their problem-solving capabilities.; 84) Effect Size-Driven Pathway Meta-Analysis for Gene Expression Data; The proliferation of omics datasets in public repositories has created
unprecedented opportunities for biomedical research but has also posed
significant challenges for their integration, particularly due to missing genes
and platform-specific discrepancies. Traditional gene expression metaanalysis
often focuses on individual genes, leading to data loss and limited biological
insights when there are missing genes across different studies. To address
these limitations, we propose GSEMA (Gene Set Enrichment Meta-Analysis), a
novel methodology that leverages singlesample enrichment scoring to aggregate
gene expression data into pathway-level matrices. By applying meta-analysis
techniques to enrichment scores, GSEMA preserves the magnitude and
directionality of effects, enabling the definition of pathway activity across
datasets. Using simulated data and case studies on Systemic Lupus Erythematosus
(SLE) and Parkinson's Disease (PD), we demonstrate that GSEMA outperforms other
methods in controlling false positive rates while providing meaningful
biological interpretations. GSEMA methodology is implemented as an R package
available on CRAN repository; 85) PH-VAE: A Polynomial Hierarchical Variational Autoencoder Towards
  Disentangled Representation Learning; The variational autoencoder (VAE) is a simple and efficient generative
artificial intelligence method for modeling complex probability distributions
of various types of data, such as images and texts. However, it suffers some
main shortcomings, such as lack of interpretability in the latent variables,
difficulties in tuning hyperparameters while training, producing blurry,
unrealistic downstream outputs or loss of information due to how it calculates
loss functions and recovers data distributions, overfitting, and origin gravity
effect for small data sets, among other issues. These and other limitations
have caused unsatisfactory generation effects for the data with complex
distributions. In this work, we proposed and developed a polynomial
hierarchical variational autoencoder (PH-VAE), in which we used a polynomial
hierarchical date format to generate or to reconstruct the data distributions.
In doing so, we also proposed a novel Polynomial Divergence in the loss
function to replace or generalize the Kullback-Leibler (KL) divergence, which
results in systematic and drastic improvements in both accuracy and
reproducibility of the re-constructed distribution function as well as the
quality of re-constructed data images while keeping the dataset size the same
but capturing fine resolution of the data. Moreover, we showed that the
proposed PH-VAE has some form of disentangled representation learning ability.; 86) SS-MPC: A Sequence-Structured Multi-Party Conversation System; Recent Multi-Party Conversation (MPC) models typically rely on graph-based
approaches to capture dialogue structures. However, these methods have
limitations, such as information loss during the projection of utterances into
structural embeddings and constraints in leveraging pre-trained language models
directly. In this paper, we propose \textbf{SS-MPC}, a response generation
model for MPC that eliminates the need for explicit graph structures. Unlike
existing models that depend on graphs to analyze conversation structures,
SS-MPC internally encodes the dialogue structure as a sequential input,
enabling direct utilization of pre-trained language models. Experimental
results show that \textbf{SS-MPC} achieves \textbf{15.60\% BLEU-1} and
\textbf{12.44\% ROUGE-L} score, outperforming the current state-of-the-art MPC
response generation model by \textbf{3.91\%p} in \textbf{BLEU-1} and
\textbf{0.62\%p} in \textbf{ROUGE-L}. Additionally, human evaluation confirms
that SS-MPC generates more fluent and accurate responses compared to existing
MPC models.; 87) Predicting Cognitive Decline: A Multimodal AI Approach to Dementia
  Screening from Speech; Recent progress has been made in detecting early stage dementia entirely
through recordings of patient speech. Multimodal speech analysis methods were
applied to the PROCESS challenge, which requires participants to use audio
recordings of clinical interviews to predict patients as healthy control, mild
cognitive impairment (MCI), or dementia and regress the patient's Mini-Mental
State Exam (MMSE) scores. The approach implemented in this work combines
acoustic features (eGeMAPS and Prosody) with embeddings from Whisper and
RoBERTa models, achieving competitive results in both regression (RMSE: 2.7666)
and classification (Macro-F1 score: 0.5774) tasks. Additionally, a novel
two-tiered classification setup is utilized to better differentiate between MCI
and dementia. Our approach achieved strong results on the test set, ranking
seventh on regression and eleventh on classification out of thirty-seven teams,
exceeding the baseline results.; 88) Spin squeezing in an ensemble of nitrogen-vacancy centers in diamond; Spin squeezed states provide a seminal example of how the structure of
quantum mechanical correlations can be controlled to produce metrologically
useful entanglement. Such squeezed states have been demonstrated in a wide
variety of artificial quantum systems ranging from atoms in optical cavities to
trapped ion crystals. By contrast, despite their numerous advantages as
practical sensors, spin ensembles in solid-state materials have yet to be
controlled with sufficient precision to generate targeted entanglement such as
spin squeezing. In this work, we present the first experimental demonstration
of spin squeezing in a solid-state spin system. Our experiments are performed
on a strongly-interacting ensemble of nitrogen-vacancy (NV) color centers in
diamond at room temperature, and squeezing (-0.5 $\pm$ 0.1 dB) is generated by
the native magnetic dipole-dipole interaction between NVs. In order to generate
and detect squeezing in a solid-state spin system, we overcome a number of key
challenges of broad experimental and theoretical interest. First, we develop a
novel approach, using interaction-enabled noise spectroscopy, to characterize
the quantum projection noise in our system without directly resolving the spin
probability distribution. Second, noting that the random positioning of spin
defects severely limits the generation of spin squeezing, we implement a pair
of strategies aimed at isolating the dynamics of a relatively ordered
sub-ensemble of NV centers. Our results open the door to entanglement-enhanced
metrology using macroscopic ensembles of optically active spins in solids.; 89) Temperature-dependent radiative lifetime measurement of the
  $6^1\Sigma_g^+(v=9,J=31)$ state of sodium molecules; We report the measurement of radiative lifetimes of the
$6^1\Sigma_g^+(v=9,J=31)$ state of gas-phase molecular sodium using a
high-resolution double-resonance spectroscopy. Measurements were done using a
time-correlated photon counting technique at various pressures and
temperatures. Lifetimes were extracted from extrapolations to the zero buffer
gas pressure, called Stern-Volmer plot, and the temperature-dependence of the
radiative lifetimes were measured over a temperature range from 593 K to 653 K.
Our result agrees well within the error limits with the theoretical
calculations.; 90) Employing deep-learning techniques for the conservative-to-primitive
  recovery in binary neutron star simulations; The detection of GW170817, together with its electromagnetic counterparts,
has proven that binary neutron star mergers are of central importance to the
field of nuclear astrophysics, e.g., through a better understanding of the
formation of elements and novel constraints on the supranuclear dense equation
of state governing the matter inside neutron stars. Essential for understanding
the binary coalescence are numerical-relativity simulations, which typically
come with high computational costs requiring high-performance computing
facilities. In this work, we build on recent studies to investigate whether
novel techniques, such as neural networks, can be employed in the conversion of
conservative variables to primitive hydrodynamical variables, such as pressure
and density. In this regard, we perform -- to the best of our knowledge -- the
first binary neutron star merger simulations in which such methods are
employed. We show that this method results in stable simulations, reaching
accuracies similar to traditional methods with an overall comparable
computational cost. These simulations serve as a proof of principle that, in
the future, deep learning techniques could be used within numerical-relativity
simulations. However, further improvements are necessary to offer a
computational advantage compared to traditional methods.; 91) FPGA Innovation Research in the Netherlands: Present Landscape and
  Future Outlook; FPGAs have transformed digital design by enabling versatile and customizable
solutions that balance performance and power efficiency, yielding them
essential for today's diverse computing challenges. Research in the
Netherlands, both in academia and industry, plays a major role in developing
new innovative FPGA solutions. This survey presents the current landscape of
FPGA innovation research in the Netherlands by delving into ongoing projects,
advancements, and breakthroughs in the field. Focusing on recent research
outcome (within the past 5 years), we have identified five key research areas:
a) FPGA architecture, b) FPGA robustness, c) data center infrastructure and
high-performance computing, d) programming models and tools, and e)
applications. This survey provides in-depth insights beyond a mere snapshot of
the current innovation research landscape by highlighting future research
directions within each key area; these insights can serve as a foundational
resource to inform potential national-level investments in FPGA technology.; 92) Spin wave interactions in the pyrochlore Heisenberg antiferromagnet with
  Dzyaloshinskii-Moriya interactions; We study the effect of magnon interactions on the spin wave spectra of the
all-in-all-out phase of the pyrochlore nearest neighbour antiferromagnet with a
Dzyaloshinskii-Moriya interaction ($D$). The leading order corrections to spin
wave energies indicate a significant renormalisation for commonly encountered
strengths of the Dzyaloshinskii-Moriya term. For low values of $D$ we find a
potential instability of the phase itself, indicated by the renormalisation of
magnon frequencies to negative values. We have also studied the renormalized
spectra in the presence of magnetic fields along three high symmetry directions
of the lattice, namely the $[111]$, $[100]$ and $[110]$ directions.
Generically, we find that for a fixed value of the Dzyaloshinskii-Moriya
interaction renormalized spectra for the lowest band decrease with an
increasing strength of the field. We have also analyzed the limits of the two
magnon continuum and probed the possibility of magnon decay. For a range of $D$
and the field strength we identify possible parameter regimes where the decay
of the higher bands of the system are kinematically allowed.; 93) Collision Risk Quantification and Conflict Resolution in Trajectory
  Tracking for Acceleration-Actuated Multi-Robot Systems; One of the pivotal challenges in a multi-robot system is how to give
attention to accuracy and efficiency while ensuring safety. Prior arts cannot
strictly guarantee collision-free for an arbitrarily large number of robots or
the results are considerably conservative. Smoothness of the avoidance
trajectory also needs to be further optimized. This paper proposes an
accelerationactuated simultaneous obstacle avoidance and trajectory tracking
method for arbitrarily large teams of robots, that provides a nonconservative
collision avoidance strategy and gives approaches for deadlock avoidance. We
propose two ways of deadlock resolution, one involves incorporating an
auxiliary velocity vector into the error function of the trajectory tracking
module, which is proven to have no influence on global convergence of the
tracking error. Furthermore, unlike the traditional methods that they address
conflicts after a deadlock occurs, our decision-making mechanism avoids the
near-zero velocity, which is much more safer and efficient in crowed
environments. Extensive comparison show that the proposed method is superior to
the existing studies when deployed in a large-scale robot system, with minimal
invasiveness.; 94) Teacher Encoder-Student Decoder Denoising Guided Segmentation Network
  for Anomaly Detection; Visual anomaly detection is a highly challenging task, often categorized as a
one-class classification and segmentation problem. Recent studies have
demonstrated that the student-teacher (S-T) framework effectively addresses
this challenge. However, most S-T frameworks rely solely on pre-trained teacher
networks to guide student networks in learning multi-scale similar features,
overlooking the potential of the student networks to enhance learning through
multi-scale feature fusion. In this study, we propose a novel model named
PFADSeg, which integrates a pre-trained teacher network, a denoising student
network with multi-scale feature fusion, and a guided anomaly segmentation
network into a unified framework. By adopting a unique teacher-encoder and
student-decoder denoising mode, the model improves the student network's
ability to learn from teacher network features. Furthermore, an adaptive
feature fusion mechanism is introduced to train a self-supervised segmentation
network that synthesizes anomaly masks autonomously, significantly increasing
detection performance. Evaluated on the MVTec AD dataset, PFADSeg achieves
state-of-the-art results with an image-level AUC of 98.9%, a pixel-level mean
precision of 76.4%, and an instance-level mean precision of 78.7%.; 95) Iterative Counterfactual Data Augmentation; Counterfactual data augmentation (CDA) is a method for controlling
information or biases in training datasets by generating a complementary
dataset with typically opposing biases. Prior work often either relies on
hand-crafted rules or algorithmic CDA methods which can leave unwanted
information in the augmented dataset. In this work, we show iterative CDA
(ICDA) with initial, high-noise interventions can converge to a state with
significantly lower noise. Our ICDA procedure produces a dataset where one
target signal in the training dataset maintains high mutual information with a
corresponding label and the information of spurious signals are reduced. We
show training on the augmented datasets produces rationales on documents that
better align with human annotation. Our experiments include six human produced
datasets and two large-language model generated datasets.; 96) The Golden Ratio Primal-Dual Algorithm with Two New Stepsize Rules for
  Convex-Concave Saddle Point Problems; In this paper, we present two stepsize rules for the extended Golden Ratio
primal-dual algorithm (E-GRPDA) designed to address structured convex
optimization problems in finite-dimensional real Hilbert spaces. The first rule
features a nonincreasing primal stepsize that remains bounded below by a
positive constant and is updated adaptively at each iteration, eliminating the
need for the Lipschitz constant of the gradient of the function and the norm of
the operator involved. The second stepsize rule is adaptive, adjusting based on
the local smoothness of the smooth component function and the local estimate of
the norm of the operator. In other words, we present an adaptive version of the
E-GRPDA algorithm. Importantly, both methods avoid the use of backtracking to
estimate the operator norm. We prove that E-GRPDA achieves an ergodic sublinear
convergence rate with both stepsize rules, evaluated using the primal-dual gap
function. Additionally, we establish an R-linear convergence rate for E-GRPDA
with the first stepsize rule, under some standard assumptions and with
appropriately chosen parameters. Through numerical experiments on various
convex optimization problems, we demonstrate the effectiveness of our
approaches and compare their performance to the existing ones.; 97) Constraining a scalar dark matter component through anomalous stellar
  light scattering; In a recent work [Phys. Rev. D 110, 023520 (2024)] a baryogenesis scenario
was proposed where our visible Universe is a 3-brane coevolving with a hidden
3-brane in a multidimensional bulk. This model introduced a new pseudo-scalar
boson. In the present paper, it is shown that this boson can exist today as a
relic from the Big Bang and constitute a minor dark matter component.
Additionally, one identifies a one-loop process that allows for a non-zero,
albeit very small, photon-boson scattering amplitude. One explores this
phenomenon and suggests that observations of a subtle light scattering around
blue giant stars could constrain the properties of the scalar boson. These
constraints would, in turn, provide valuable insights into the parameters of
the proposed baryogenesis model.; 98) UnPuzzle: A Unified Framework for Pathology Image Analysis; Pathology image analysis plays a pivotal role in medical diagnosis, with deep
learning techniques significantly advancing diagnostic accuracy and research.
While numerous studies have been conducted to address specific pathological
tasks, the lack of standardization in pre-processing methods and model/database
architectures complicates fair comparisons across different approaches. This
highlights the need for a unified pipeline and comprehensive benchmarks to
enable consistent evaluation and accelerate research progress. In this paper,
we present UnPuzzle, a novel and unified framework for pathological AI research
that covers a broad range of pathology tasks with benchmark results. From
high-level to low-level, upstream to downstream tasks, UnPuzzle offers a
modular pipeline that encompasses data pre-processing, model
composition,taskconfiguration,andexperimentconduction.Specifically, it
facilitates efficient benchmarking for both Whole Slide Images (WSIs) and
Region of Interest (ROI) tasks. Moreover, the framework supports
variouslearningparadigms,includingself-supervisedlearning,multi-task
learning,andmulti-modallearning,enablingcomprehensivedevelopment of pathology
AI models. Through extensive benchmarking across multiple datasets, we
demonstrate the effectiveness of UnPuzzle in streamlining pathology AI research
and promoting reproducibility. We envision UnPuzzle as a cornerstone for future
advancements in pathology AI, providing a more accessible, transparent, and
standardized approach to model evaluation. The UnPuzzle repository is publicly
available at https://github.com/Puzzle-AI/UnPuzzle.; 99) Effect of metal (Ti) interlayer on fracture toughness of TiN thin films; Titanium nitride (TiN) is widely used as a protective coating due to its high
hardness, but suffers from inherent brittleness and low fracture toughness,
limiting its applicability. The layering of TiN films with metallic titanium
(Ti) improves the overall fracture behaviour of the architecture by modifying
the crack driving force due to elastic-plastic mismatch between the layers.
Microcantilever fracture tests were carried out on bilayer (Ti-TiN, TiN-Ti) and
trilayer (Ti-TiN-Ti) architectures to determine the fracture toughness and
study the fundamental crack growth behaviour. The initiation fracture toughness
in bilayer architecture with crack in Ti layer is almost 70% higher, when
compared to crack in TiN layer. In Ti-TiN bilayer the crack propagated
catastrophically post linear elastic deformation, whereas the crack was
arrested at the TiN/Ti interface in both TiN-Ti and Ti-TiN-Ti architectures due
to plastic energy dissipation in the Ti layer. Crack tip plasticity originated
in the metallic Ti layer in TiN-Ti and Ti-TiN-Ti architectures increased the
total fracture resistance by more than eight-fold compared to the Ti-TiN
bilayer.; 100) Large Language Model-Based Benchmarking Experiment Settings for
  Evolutionary Multi-Objective Optimization; When we manually design an evolutionary optimization algorithm, we implicitly
or explicitly assume a set of target optimization problems. In the case of
automated algorithm design, target optimization problems are usually explicitly
shown. Recently, the use of large language models (LLMs) for the design of
evolutionary multi-objective optimization (EMO) algorithms have been examined
in some studies. In those studies, target multi-objective problems are not
always explicitly shown. It is well known in the EMO community that the
performance evaluation results of EMO algorithms depend on not only test
problems but also many other factors such as performance indicators, reference
point, termination condition, and population size. Thus, it is likely that the
designed EMO algorithms by LLMs depends on those factors. In this paper, we try
to examine the implicit assumption about the performance comparison of EMO
algorithms in LLMs. For this purpose, we ask LLMs to design a benchmarking
experiment of EMO algorithms. Our experiments show that LLMs often suggest
classical benchmark settings: Performance examination of NSGA-II, MOEA/D and
NSGA-III on ZDT, DTLZ and WFG by HV and IGD under the standard parameter
specifications.",0.0,0.3010299956639812
2412.11084,applied,2412.11084-pos1-1,"BarcodeBERT: Transformers for Biodiversity Analysis; Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https://github.com/Kari-Genomics-Lab/BarcodeBERT",2412.11084-pos2-1,"Biological identifications through DNA barcodes; Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution.",49,"['2', '6', '1', '5', '32', '49', '10', '13', '4', '3']","The main paper discusses environmentally focused machine learning approaches for biodiversity analysis using DNA barcodes. The best candidate paper, 'MM-PoisonRAG', aligns well as it addresses the vulnerabilities in large language models through knowledge poisoning, which can be impactful when integrating machine learning methods in biodiversity analysis. It also highlights the need for robust and secure methodologies, which is essential in the application of ML in sensitive areas like biodiversity datasets. Other candidates, while relevant in their own fields (like speech recognition and gene expression), do not align as closely with the themes of biodiversity and machine learning robustness that are central to the main paper.","1) SigN: SIMBox Activity Detection Through Latency Anomalies at the
  Cellular Edge; Despite their widespread adoption, cellular networks face growing
vulnerabilities due to their inherent complexity and the integration of
advanced technologies. One of the major threats in this landscape is Voice over
IP (VoIP) to GSM gateways, known as SIMBox devices. These devices use multiple
SIM cards to route VoIP traffic through cellular networks, enabling
international bypass fraud with losses of up to $3.11 billion annually. Beyond
financial impact, SIMBox activity degrades network performance, threatens
national security, and facilitates eavesdropping on communications. Existing
detection methods for SIMBox activity are hindered by evolving fraud techniques
and implementation complexities, limiting their practical adoption in operator
networks.This paper addresses the limitations of current detection methods by
introducing SigN , a novel approach to identifying SIMBox activity at the
cellular edge. The proposed method focuses on detecting remote SIM card
association, a technique used by SIMBox appliances to mimic human mobility
patterns. The method detects latency anomalies between SIMBox and standard
devices by analyzing cellular signaling during network attachment. Extensive
indoor and outdoor experiments demonstrate that SIMBox devices generate
significantly higher attachment latencies, particularly during the
authentication phase, where latency is up to 23 times greater than that of
standard devices. We attribute part of this overhead to immutable factors such
as LTE authentication standards and Internet-based communication protocols.
Therefore, our approach offers a robust, scalable, and practical solution to
mitigate SIMBox activity risks at the network edge.; 2) MM-PoisonRAG: Disrupting Multimodal RAG with Local and Global Poisoning
  Attacks; Multimodal large language models (MLLMs) equipped with Retrieval Augmented
Generation (RAG) leverage both their rich parametric knowledge and the dynamic,
external knowledge to excel in tasks such as Question Answering. While RAG
enhances MLLMs by grounding responses in query-relevant external knowledge,
this reliance poses a critical yet underexplored safety risk: knowledge
poisoning attacks, where misinformation or irrelevant knowledge is
intentionally injected into external knowledge bases to manipulate model
outputs to be incorrect and even harmful. To expose such vulnerabilities in
multimodal RAG, we propose MM-PoisonRAG, a novel knowledge poisoning attack
framework with two attack strategies: Localized Poisoning Attack (LPA), which
injects query-specific misinformation in both text and images for targeted
manipulation, and Globalized Poisoning Attack (GPA) to provide false guidance
during MLLM generation to elicit nonsensical responses across all queries. We
evaluate our attacks across multiple tasks, models, and access settings,
demonstrating that LPA successfully manipulates the MLLM to generate
attacker-controlled answers, with a success rate of up to 56% on MultiModalQA.
Moreover, GPA completely disrupts model generation to 0% accuracy with just a
single irrelevant knowledge injection. Our results highlight the urgent need
for robust defenses against knowledge poisoning to safeguard multimodal RAG
frameworks.; 3) Higher derivative holography and temperature dependence of QGP
  viscosities; Recent Bayesian analyses of heavy ion collision data have established a
non-trivial temperature dependence of the shear and bulk viscosity per entropy.
Motivated by this, we consider higher derivative corrections to realistic,
bottom-up holographic models of quark-gluon plasma based on five-dimensional
Einstein-dilaton theories and determine the dilaton potentials in the higher
derivative terms by matching the Bayesian analyses. A byproduct of our analysis
is the bulk viscosity that follows from the holographic V-QCD theory. Higher
derivative corrections when treated perturbatively lead to tension with
existing data. We investigate possible resolutions.; 4) Temperature-dependent radiative lifetime measurement of the
  $6^1\Sigma_g^+(v=9,J=31)$ state of sodium molecules; We report the measurement of radiative lifetimes of the
$6^1\Sigma_g^+(v=9,J=31)$ state of gas-phase molecular sodium using a
high-resolution double-resonance spectroscopy. Measurements were done using a
time-correlated photon counting technique at various pressures and
temperatures. Lifetimes were extracted from extrapolations to the zero buffer
gas pressure, called Stern-Volmer plot, and the temperature-dependence of the
radiative lifetimes were measured over a temperature range from 593 K to 653 K.
Our result agrees well within the error limits with the theoretical
calculations.; 5) Exploring Gender Disparities in Automatic Speech Recognition Technology; This study investigates factors influencing Automatic Speech Recognition
(ASR) systems' fairness and performance across genders, beyond the conventional
examination of demographics. Using the LibriSpeech dataset and the Whisper
small model, we analyze how performance varies across different gender
representations in training data. Our findings suggest a complex interplay
between the gender ratio in training data and ASR performance. Optimal fairness
occurs at specific gender distributions rather than a simple 50-50 split.
Furthermore, our findings suggest that factors like pitch variability can
significantly affect ASR accuracy. This research contributes to a deeper
understanding of biases in ASR systems, highlighting the importance of
carefully curated training data in mitigating gender bias.; 6) Effect Size-Driven Pathway Meta-Analysis for Gene Expression Data; The proliferation of omics datasets in public repositories has created
unprecedented opportunities for biomedical research but has also posed
significant challenges for their integration, particularly due to missing genes
and platform-specific discrepancies. Traditional gene expression metaanalysis
often focuses on individual genes, leading to data loss and limited biological
insights when there are missing genes across different studies. To address
these limitations, we propose GSEMA (Gene Set Enrichment Meta-Analysis), a
novel methodology that leverages singlesample enrichment scoring to aggregate
gene expression data into pathway-level matrices. By applying meta-analysis
techniques to enrichment scores, GSEMA preserves the magnitude and
directionality of effects, enabling the definition of pathway activity across
datasets. Using simulated data and case studies on Systemic Lupus Erythematosus
(SLE) and Parkinson's Disease (PD), we demonstrate that GSEMA outperforms other
methods in controlling false positive rates while providing meaningful
biological interpretations. GSEMA methodology is implemented as an R package
available on CRAN repository; 7) Efficient Prompting for Continual Adaptation to Missing Modalities; Missing modality issues are common in real-world applications, arising from
factors such as equipment failures and privacy concerns. When fine-tuning
pre-trained models on downstream datasets with missing modalities, performance
can degrade significantly. Current methods often aggregate various missing
cases to train recovery modules or align multimodal features, resulting in
suboptimal performance, high computational costs, and the risk of catastrophic
forgetting in continual environments where data arrives sequentially. In this
paper, we formulate the dynamic missing modality problem as a continual
learning task and introduce the continual multimodal missing modality task. To
address this challenge efficiently, we introduce three types of prompts:
modality-specific, task-aware, and task-specific prompts. These prompts enable
the model to learn intra-modality, inter-modality, intra-task, and inter-task
features. Furthermore, we propose a contrastive task interaction strategy to
explicitly learn prompts correlating different modalities. We conduct extensive
experiments on three public datasets, where our method consistently outperforms
state-of-the-art approaches.; 8) Memorization and Generalization in Generative Diffusion under the
  Manifold Hypothesis; We study the memorization and generalization capabilities of a Diffusion
Model (DM) in the case of structured data defined on a latent manifold. We
specifically consider a set of $P$ mono-modal data points in $N$ dimensions
lying on a latent subspace of dimension $D = \alpha_D N$, according to the
Hidden Manifold Model (HMM). Our analysis leverages the recently introduced
formalism based on the statistical physics of the Random Energy Model (REM). We
provide evidence for the existence of an onset time $t_{o} > t_c$ when traps
appear in the potential without affecting the typical diffusive trajectory. The
size of the basins of attraction of such traps is computed as a function of
time. Moreover, we derive the collapse time $t_{c}$ at which trajectories fall
in the basin of one of the training points, implying memorization. An explicit
formula for $t_c$ is given as a function of $P$ and the ratio $\alpha_D$,
proving that the curse of dimensionality issue does not hold for highly
structured data, i.e. $\alpha_D\ll 1$, regardless of the non-linearity of the
manifold surface. We also prove that collapse coincides with the condensation
transition in the REM. Eventually, the degree of generalization of DMs is
formulated in terms of the Kullback-Leibler divergence between the exact and
the empirical distribution of the sampled configurations: we show the existence
of an additional time $t_{g}<t_{c}<t_{o}$ such that the distance between the
empirical measure of the data and the ground-truth is minimal.
Counter-intuitively, the best generalization performance is found within the
memorization phase of the model. We conclude that the generalization
performance of DMs benefit from highly structured data since $t_g$ approaches
zero faster than $t_c$ when $\alpha_D \rightarrow 0$.; 9) Constraining a scalar dark matter component through anomalous stellar
  light scattering; In a recent work [Phys. Rev. D 110, 023520 (2024)] a baryogenesis scenario
was proposed where our visible Universe is a 3-brane coevolving with a hidden
3-brane in a multidimensional bulk. This model introduced a new pseudo-scalar
boson. In the present paper, it is shown that this boson can exist today as a
relic from the Big Bang and constitute a minor dark matter component.
Additionally, one identifies a one-loop process that allows for a non-zero,
albeit very small, photon-boson scattering amplitude. One explores this
phenomenon and suggests that observations of a subtle light scattering around
blue giant stars could constrain the properties of the scalar boson. These
constraints would, in turn, provide valuable insights into the parameters of
the proposed baryogenesis model.; 10) Formally exact fluorescence spectroscopy simulations for mesoscale
  molecular aggregates with $N^0$ scaling; We present a size-invariant (i.e., $N^0$) scaling algorithm for simulating
fluorescence spectroscopy in large molecular aggregates. We combine the dyadic
adaptive hierarchy of pure states (DadHOPS) equation-of-motion with an operator
decomposition scheme and an efficient Monte Carlo sampling algorithm to enable
a formally exact, local description of the fluorescence spectrum in large
molecular aggregates. Furthermore, we demonstrate that the ensemble average
inverse participation ratio (IPR) of DadHOPS wave functions reproduces the
delocalization extent extracted from fluorescence spectroscopy of J-aggregates
with strong vibronic transitions. This work provides a computationally
efficient framework for fluorescence simulations, offering a new tool for
understanding the optical properties of mesoscale molecular systems.; 11) Suppressing grid instability and noise in particle-in-cell simulation by
  smoothing; Smoothing short-wavelength charge density variations can stabilize explicit
electrostatic particle-in-cell (PIC) plasma simulations against grid heating
and cold beam instabilities, which cause unphysical heating when the Debye
length is poorly resolved. We demonstrate this by solving the dispersion and by
running 1D electrostatic PIC simulations, using an efficient smoothing
algorithm that leverages the Poisson solve. To ensure stability, the smoothing
radius must increase with the number of Debye lengths per cell. Smoothing also
suppresses particle noise, which is severely exacerbated by poor resolution of
the Debye length. To help determine optimal PIC configuration, we empirically
characterize electric field noise, particle velocity diffusion, and unphysical
energy exchanges in 1D PIC simulation, as a function of Debye-length
resolution, smoothing, and particles per cell. We also show how PIC noise
causes test particles to exhibit misleading behavior. Since smoothing reduces
the effective resolution, the optimal cell size is less than the desired
resolution but can be much greater than the Debye length, reducing
computational expense.; 12) NeurOp-Diff:Continuous Remote Sensing Image Super-Resolution via Neural
  Operator Diffusion; Most publicly accessible remote sensing data suffer from low resolution,
limiting their practical applications. To address this, we propose a diffusion
model guided by neural operators for continuous remote sensing image
super-resolution (NeurOp-Diff). Neural operators are used to learn resolution
representations at arbitrary scales, encoding low-resolution (LR) images into
high-dimensional features, which are then used as prior conditions to guide the
diffusion model for denoising. This effectively addresses the artifacts and
excessive smoothing issues present in existing super-resolution (SR) methods,
enabling the generation of high-quality, continuous super-resolution images.
Specifically, we adjust the super-resolution scale by a scaling factor s,
allowing the model to adapt to different super-resolution magnifications.
Furthermore, experiments on multiple datasets demonstrate the effectiveness of
NeurOp-Diff. Our code is available at https://github.com/zerono000/NeurOp-Diff.; 13) Proton Flows, Proton Gradients and Subcellular Architecture in
  Biological Energy Conversion; Hydrogen ions, or protons, provide the medium by which energy is stored and
converted in biological systems. Such pre-eminence relies on the interplay
between interfacial and bulk chemical transformations, according to mechanisms
that are shared by organisms in all phyla of life. The present work provides an
introduction to the fundamental aspects of biological energy management by
focusing on the relationship between vectorial proton flows and the geometry of
energy producing organelles in eukaryotes. The leading models of
proton-mediated energy conversion, the delocalised proton (or chemiosmotic)
model and the localised proton model, are presented in a complementary
perspective. While the delocalised model provides a description that relies on
equilibrium thermodynamics, the localised model addresses dynamic processes
that are better described using out-of-equilibrium thermodynamics. The work
reviews the salient aspects of such mechanisms, traces the development of our
present understanding, and highlights areas that are open to future
developments.; 14) Training an LLM-as-a-Judge Model: Pipeline, Insights, and Practical
  Lessons; The rapid advancement of large language models (LLMs) has opened new
possibilities for their adoption as evaluative judges. This paper introduces
Themis, a fine-tuned LLM judge that delivers sophisticated context-aware
evaluations. We provide a comprehensive overview of the development pipeline
for Themis, highlighting its scenario-dependent evaluation prompts and two
novel methods for controlled instruction generation. These designs enable
Themis to effectively distill evaluative skills from teacher models, while
retaining flexibility for continuous development. We introduce two
human-labeled benchmarks for meta-evaluation, demonstrating that Themis can
achieve high alignment with human preferences in an economical manner.
Additionally, we explore insights into the LLM-as-a-judge paradigm, revealing
nuances in performance and the varied effects of reference answers. Notably, we
observe that pure knowledge distillation from strong LLMs, though common, does
not guarantee performance improvement through scaling. We propose a mitigation
strategy based on instruction-following difficulty. Furthermore, we provide
practical guidelines covering data balancing, prompt customization,
multi-objective training, and metric aggregation. We aim for our method and
findings, along with the fine-tuning data, benchmarks, and model checkpoints,
to support future research and development in this area.; 15) Explicit Construction of Classical and Quantum Quasi-Cyclic Low-Density
  Parity-Check Codes with Column Weight 2 and Girth 12; This study proposes an explicit construction method for classical and quantum
quasi-cyclic low-density parity-check (QC-LDPC) codes with a girth of 12. The
proposed method designs parity-check matrices that maximize the girth while
maintaining an orthogonal structure suitable for quantum error correction. By
utilizing algebraic techniques, short cycles are eliminated, which improves
error correction performance. Additionally, this method is extended to
non-binary LDPC codes and spatially-coupled LDPC codes, demonstrating that both
the girth and orthogonality can be preserved. The results of this study enable
the design of high-performance quantum error correction codes without the need
for random search.; 16) Employing deep-learning techniques for the conservative-to-primitive
  recovery in binary neutron star simulations; The detection of GW170817, together with its electromagnetic counterparts,
has proven that binary neutron star mergers are of central importance to the
field of nuclear astrophysics, e.g., through a better understanding of the
formation of elements and novel constraints on the supranuclear dense equation
of state governing the matter inside neutron stars. Essential for understanding
the binary coalescence are numerical-relativity simulations, which typically
come with high computational costs requiring high-performance computing
facilities. In this work, we build on recent studies to investigate whether
novel techniques, such as neural networks, can be employed in the conversion of
conservative variables to primitive hydrodynamical variables, such as pressure
and density. In this regard, we perform -- to the best of our knowledge -- the
first binary neutron star merger simulations in which such methods are
employed. We show that this method results in stable simulations, reaching
accuracies similar to traditional methods with an overall comparable
computational cost. These simulations serve as a proof of principle that, in
the future, deep learning techniques could be used within numerical-relativity
simulations. However, further improvements are necessary to offer a
computational advantage compared to traditional methods.; 17) What is a Feature, Really? Toward a Unified Understanding Across SE
  Disciplines; In software engineering, the concept of a ``feature'' is widely used but
inconsistently defined across disciplines such as requirements engineering (RE)
and software product lines (SPL). This lack of consistency often results in
communication gaps, rework, and inefficiencies in projects. To address these
challenges, this paper proposes an empirical, data-driven approach to explore
how features are described, implemented, and managed across real-world
projects, starting with open-source software (OSS). By analyzing
feature-related branches in OSS repositories, we identify patterns in
contributor behavior, feature implementation, and project management
activities. Our findings provide actionable insights to improve project
planning, resource allocation, and team coordination. Additionally, we outline
a roadmap to unify the understanding of features across software engineering
disciplines. This research aims to bridge gaps between academic inquiry and
practical strategies, fostering better feature planning and development
workflows in diverse project environments.; 18) HiCoCS: High Concurrency Cross-Sharding on Permissioned Blockchains; As the foundation of the Web3 trust system, blockchain technology faces
increasing demands for scalability. Sharding emerges as a promising solution,
but it struggles to handle highly concurrent cross-shard transactions
(\textsf{CSTx}s), primarily due to simultaneous ledger operations on the same
account. Hyperledger Fabric, a permissioned blockchain, employs multi-version
concurrency control for parallel processing. Existing solutions use channels
and intermediaries to achieve cross-sharding in Hyperledger Fabric. However,
the conflict problem caused by highly concurrent \textsf{CSTx}s has not been
adequately resolved. To fill this gap, we propose HiCoCS, a high concurrency
cross-shard scheme for permissioned blockchains. HiCoCS creates a unique
virtual sub-broker for each \textsf{CSTx} by introducing a composite key
structure, enabling conflict-free concurrent transaction processing while
reducing resource overhead. The challenge lies in managing large numbers of
composite keys and mitigating intermediary privacy risks. HiCoCS utilizes
virtual sub-brokers to receive and process \textsf{CSTx}s concurrently while
maintaining a transaction pool. Batch processing is employed to merge multiple
\textsf{CSTx}s in the pool, improving efficiency. We explore composite key
reuse to reduce the number of virtual sub-brokers and lower system overhead.
Privacy preservation is enhanced using homomorphic encryption. Evaluations show
that HiCoCS improves cross-shard transaction throughput by 3.5-20.2 times
compared to the baselines.; 19) Proceedings of the 14th International Computational Accelerator Physics
  Conference (ICAP24); This is the proceedings of the 14th International Computational Accelerator
Physics Conference, ICAP'24, which was held at the Lufthansa Seeheim Conference
Hotel in Germany from October 2-5, 2024, hosted by TU Darmstadt and GSI
Helmholtzzentrum f\""ur Schwerionenforschung. ICAP'24 has focused on advances in
Computational Accelerator Physics and their application to existing machines
and future facilities. It has provided a forum for researchers in modeling and
simulation to exchange information and discuss new ideas that benefit a wide
area of accelerator science and technology. Topics of the conference have
included computational needs and challenges, beam dynamics and electromagnetic
field calculations, code development and validation, data processing and
visualization, high performance computing, machine learning and advanced
optimization as well as emerging technologies that will impact computing for
accelerator design.; 20) On type 1 active galactic nuclei with double-peaked [O~{\sc iii}]. I.
  data sample and basic results; Double-peaked narrow emission lines (DPNELs) might be evidence for the
existence of kpc-scale dual AGNs. There are so far large samples of objects
with DPNELs in narrow emission line galaxies. Here, a systematic search is made
to build a sample of type 1 AGNs with double-peaked [O~{\sc~iii}] from Data
Release 16 of the Sloan Digital Sky Survey (SDSS). Through visually inspecting
and fitting [O~{\sc~iii}], fitting broad H$\alpha$ emission lines, performing
F-test for [O~{\sc~iii}] profiles, and checking broad H$\beta$ and
[O~{\sc~iii}] emission lines, we select 62 type 1 AGNs with reliable
double-peaked [O~{\sc~iii}] from 11557 QSOs with z < 0.3. After visually
checking the 62 SDSS multi-color images, we find only seven objects with signs
of merging. Four possible models for the double-peaked [O~{\sc~iii}] observed
in our sample are discussed: the superposition model, AGN outflow model, dual
AGN model, and rotating disk model. However, the current results can not
provide any one explanation conclusively, and additional observational data are
needed to provide the details of narrow line regions. But at least 22 objects
with different velocity offsets between double-peaked [O~{\sc~iii}] and narrow
H$\alpha$ emission lines could be excluded as dual AGN candidates. The relative
velocity offsets of the [O~{\sc~iii}] blue-shifted/red-shifted components are
negative to their line flux ratios, which is consistent with dual AGN model.
This work provides a new sample of 62 type 1 AGNs with double-peaked
[O~{\sc~iii}] for further study.; 21) Gaussian quantum data hiding; Quantum data hiding encodes a hidden classical bit to a pair of quantum
states that is difficult to distinguish using a particular set of measurement,
denoted as $M$. In this work, we explore quantum data hiding in two contexts
involving Gaussian operations or states. First, we consider the set of
measurement $M$ as Gaussian local quantum operations and classical
communication, a new set of operations not previously discussed in the
literature for data hiding. We hide one classical bit in the two different
mixture of displaced two-mode squeezed states. Second, we consider the set of
measurement $M$ as general Gaussian measurement and construct the data hiding
states using two-mode thermal states. This data hiding scheme is effective in
the weak strength limit, providing a new example compared to existing
discussions for the set of general Gaussian measurement.; 22) The Golden Ratio Primal-Dual Algorithm with Two New Stepsize Rules for
  Convex-Concave Saddle Point Problems; In this paper, we present two stepsize rules for the extended Golden Ratio
primal-dual algorithm (E-GRPDA) designed to address structured convex
optimization problems in finite-dimensional real Hilbert spaces. The first rule
features a nonincreasing primal stepsize that remains bounded below by a
positive constant and is updated adaptively at each iteration, eliminating the
need for the Lipschitz constant of the gradient of the function and the norm of
the operator involved. The second stepsize rule is adaptive, adjusting based on
the local smoothness of the smooth component function and the local estimate of
the norm of the operator. In other words, we present an adaptive version of the
E-GRPDA algorithm. Importantly, both methods avoid the use of backtracking to
estimate the operator norm. We prove that E-GRPDA achieves an ergodic sublinear
convergence rate with both stepsize rules, evaluated using the primal-dual gap
function. Additionally, we establish an R-linear convergence rate for E-GRPDA
with the first stepsize rule, under some standard assumptions and with
appropriately chosen parameters. Through numerical experiments on various
convex optimization problems, we demonstrate the effectiveness of our
approaches and compare their performance to the existing ones.; 23) Birational geometry of the twofold symmetric product of a Hirzebruch
  surface via secant maps; In this paper, extending some ideas of Fano, we study the birational geometry
of the Hilbert scheme of 0-dimensional subschemes of length 2 of a rational
normal scroll. This fourfold has three elementary contractions associated to
the three faces of its nef cone. We study natural projective realizations of
these contractions. In particular, given a smooth rational normal scroll
$S_{a,b}$ of degree $r$ in ${\mathbb P}^{r+1}$ with $1 \leq a \leq b$ and
a+b=r, i.e., $S_{a,b}$ is the relative Proj of the vector bundle $O_{{\mathbb
P}^1}(a)\oplus O_{{\mathbb P}^1}(b)$ embedded in ${\mathbb P}^{r+1}$ with its
O(1) line bundle (from an abstract viewpoint $S_{a,b}\cong {\mathbb F}_{b-a}$),
we consider the subvariety $X_{a,b}$ of the Grassmannian $G(1,r+1)$ described
by all lines that are secant or tangent to $S_{a,b}$. The variety $X_{a,b}$ is
the image of some of the aforementioned contractions, it is smooth if a>1, and
it is singular at a unique point if a=1. We compute the degree of $X_{a,b}$ and
the local structure of the singularity of $X_{a,b}$ when a=1. Finally we
discuss in some detail the case r=4, originally considered by Fano, because the
smooth hyperplane sections of $X_{2,2}$ and $X_{1,3}$ are the Fano 3-folds that
appear as number 16 in the Mori-Mukai list of Fano 3-folds with Picard number
2. We prove that any smooth hyperplane section of $X_{2,2}$ is also a
hyperplane section of $X_{1,3}$, and we discuss the GIT-stability of the smooth
hyperplane sections of $X_{1,3}$ where $G$ is the subgroup of the projective
automorphisms of $X_{1,3}$ coming from the ones of $S_{1,3}.$; 24) $\mathbb{L}^p$ $(p>1)$-solutions for BSDEs with jumps and stochastic
  monotone generator; We study multidimensional discontinuous backward stochastic differential
equations in a filtration that supports both a Brownian motion and an
independent integer-valued random measure. Under suitable
$\mathbb{L}^p$-integrability conditions on the data, we establish the existence
and uniqueness of $\mathbb{L}^p$-solutions for both cases: $p \geq 2$ and $p
\in (1,2)$. The generator is assumed to be stochastically monotone in the state
variable $y$, stochastically Lipschitz in the control variables $(z, u)$, and
to satisfy a stochastic linear growth condition, along with an appropriate
$\mathbb{L}^p$-integrability requirement.; 25) Efficient Reconciliation of Continuous Variable Quantum Key Distribution
  with Multiplicatively Repeated Non-Binary LDPC Codes; Continuous variable quantum key distribution bears the promise of simple
quantum key distribution directly compatible with commercial off the shelf
equipment. However, for a long time its performance was hindered by the absence
of good classical postprocessing capable of distilling secret-keys in the noisy
regime. Advanced coding solutions in the past years have partially addressed
this problem enabling record transmission distances of up to 165 km, and 206 km
over ultra-low loss fiber. In this paper, we show that a very simple coding
solution with a single code is sufficient to extract keys at all noise levels.
This solution has performance competitive with prior results for all levels of
noise, and we show that non-zero keys can be distilled up to a record distance
of 192 km assuming the standard loss of a single-mode optical fiber, and 240 km
over ultra-low loss fibers. Low-rate codes are constructed using
multiplicatively repeated non-binary low-density parity-check codes over a
finite field of characteristic two. This construction only makes use of a
(2,k)-regular non-binary low-density parity-check code as mother code, such
that code design is in fact not required, thus trivializing the code
construction procedure. The construction is also inherently rate-adaptive
thereby allowing to easily create codes of any rate. Rate-adaptive codes are of
special interest for the efficient reconciliation of errors over time or
arbitrary varying channels, as is the case with quantum key distribution. In
short, these codes are highly efficient when reconciling errors over a very
noisy communication channel, and perform well even for short block-length
codes. Finally, the proposed solution is known to be easily amenable to
hardware implementations, thus addressing also the requirements for practical
reconciliation in continuous variable quantum key distribution.; 26) RUM-NN: A Neural Network Model Compatible with Random Utility
  Maximisation for Discrete Choice Setups; This paper introduces a framework for capturing stochasticity of choice
probabilities in neural networks, derived from and fully consistent with the
Random Utility Maximization (RUM) theory, referred to as RUM-NN. Neural network
models show remarkable performance compared with statistical models; however,
they are often criticized for their lack of transparency and interoperability.
The proposed RUM-NN is introduced in both linear and nonlinear structures. The
linear RUM-NN retains the interpretability and identifiability of traditional
econometric discrete choice models while using neural network-based estimation
techniques. The nonlinear RUM-NN extends the model's flexibility and predictive
capabilities to capture nonlinear relationships between variables within
utility functions. Additionally, the RUM-NN allows for the implementation of
various parametric distributions for unobserved error components in the utility
function and captures correlations among error terms. The performance of RUM-NN
in parameter recovery and prediction accuracy is rigorously evaluated using
synthetic datasets through Monte Carlo experiments. Additionally, RUM-NN is
evaluated on the Swissmetro and the London Passenger Mode Choice (LPMC)
datasets with different sets of distribution assumptions for the error
component. The results demonstrate that RUM-NN under a linear utility structure
and IID Gumbel error terms can replicate the performance of the Multinomial
Logit (MNL) model, but relaxing those constraints leads to superior performance
for both Swissmetro and LPMC datasets. By introducing a novel estimation
approach aligned with statistical theories, this study empowers econometricians
to harness the advantages of neural network models.; 27) CoFinDiff: Controllable Financial Diffusion Model for Time Series
  Generation; The generation of synthetic financial data is a critical technology in the
financial domain, addressing challenges posed by limited data availability.
Traditionally, statistical models have been employed to generate synthetic
data. However, these models fail to capture the stylized facts commonly
observed in financial data, limiting their practical applicability. Recently,
machine learning models have been introduced to address the limitations of
statistical models; however, controlling synthetic data generation remains
challenging. We propose CoFinDiff (Controllable Financial Diffusion model), a
synthetic financial data generation model based on conditional diffusion models
that accept conditions about the synthetic time series. By incorporating
conditions derived from price data into the conditional diffusion model via
cross-attention, CoFinDiff learns the relationships between the conditions and
the data, generating synthetic data that align with arbitrary conditions.
Experimental results demonstrate that: (i) synthetic data generated by
CoFinDiff capture stylized facts; (ii) the generated data accurately meet
specified conditions for trends and volatility; (iii) the diversity of the
generated data surpasses that of the baseline models; and (iv) models trained
on CoFinDiff-generated data achieve improved performance in deep hedging task.; 28) Quasinormal mode frequencies and gravitational perturbations of spinning
  black holes in modified gravity through METRICS: The dynamical Chern-Simons
  gravity case; We present the first precise calculations of the gravitational
quasinormal-mode (QNM) frequencies for spinning black holes with dimensionless
angular momenta $J/M^2 := a \lesssim 0.75$ in dynamical Chern-Simons gravity.
Using the \textit{Metric pErTuRbations wIth speCtral methodS} (METRICS)
framework, we compute the QNM frequencies of both axial and polar metric
perturbations, focusing on the $nl m = 022$, $033$, and $032$ modes. The
METRICS frequencies for the 022 mode achieve numerical uncertainties $\lesssim
10^{-4}$ when $0 \leq a \leq 0.5$ and $\lesssim 10^{-3}$ for $0.5 \leq a \leq
0.75$, without decoupling or simplifying the linearized field equations. We
also derive optimal fitting polynomials to enable efficient and accurate
evaluations of the leading-order frequency shifts in these modes. The METRICS
frequencies and fitting expressions are a robust and indispensable step toward
enabling gravitational-wave ringdown tests of dynamical Chern-Simons gravity.; 29) Induction and Recursion Principles in a Higher-Order Quantitative Logic; Quantitative logic reasons about the degree to which formulas are satisfied.
This paper studies the fundamental reasoning principles of higher-order
quantitative logic and their application to reasoning about probabilistic
programs and processes.
  We construct an affine calculus for 1-bounded complete metric spaces and the
monad for probability measures equipped with the Kantorovic distance. The
calculus includes a form of guarded recursion interpreted via Banach's fixed
point theorem, useful, e.g., for recursive programming with processes. We then
define an affine higher-order quantitative logic for reasoning about terms of
our calculus. The logic includes novel principles for guarded recursion, and
induction over probability measures and natural numbers.
  Examples of reasoning in the logic include proofs of upper bounds on
distances of processes. We also show how our logic can express coupling proofs
- a powerful technique for comparing probabilistic processes.; 30) Observational constraints on vector-like dark energy; The canonical cosmological model to explain the recent acceleration of the
universe relies on a cosmological constant, and most dynamical dark energy and
modified gravity model alternatives are based on scalar fields. Still, further
alternatives are possible. One of these involves vector fields: under certain
conditions, they can lead to accelerating universes while preserving
large-scale homogeneity and isotropy. We report quantitative observational
constraints on a model previously proposed by Armend\'ariz-Pic\'on and known as
the cosmic triad. We consider several subclasses of the model, which
generically is a parametric extension of the canonical $\Lambda$CDM model, as
well as two possible choices of the triad's potential. Our analysis shows that
any deviations from this limit are constrained to be small. In particular the
preferred present-day values of the matter density and the dark energy equation
of state are fully consistent with those obtained, for the same datasets, in
flat $\Lambda$CDM and $w_0$CDM. The constraints mildly depend on the priors on
the dark energy equation of state, specifically on whether phantom values
thereof are allowed, while the choice of potential does not play a significant
role since any such potential is constrained to be relatively flat.; 31) Driving towards net-zero: The impact of electric vehicle flexibility
  participation on a future Norwegian electricity system; Electric vehicle batteries have a proven flexibility potential which could
serve as an alternative to conventional electricity storage solutions. EV
batteries could support the balancing of supply and demand and the integration
of variable renewable energy into the electricity system. The flexibility
potential from electric vehicles, in distinction to conventional battery
storage, depends on the vehicle user's willingness and opportunity to make
their vehicle available for flexibility. This rate of participation is often
not considered in studies, despite the impact electric vehicle flexibility
could have on the electricity system. This work presents a modelling study of
the Norwegian electricity system, demonstrating how a future net-zero
electricity system can benefit from electric vehicles in terms of integrating
renewables and balancing supply and demand, while considering the rate of
participation. Our findings show electric vehicles' potential to eliminate the
need for stationary battery storage with just 50% participation in
vehicle-to-grid. We find that the flexibility of electric vehicles contributes
to relative reductions in the total cost of the electricity system by almost 4%
and 15% assuming 100% participation in flexible charging and vehicle-to-grid,
respectively.; 32) Predicting Cognitive Decline: A Multimodal AI Approach to Dementia
  Screening from Speech; Recent progress has been made in detecting early stage dementia entirely
through recordings of patient speech. Multimodal speech analysis methods were
applied to the PROCESS challenge, which requires participants to use audio
recordings of clinical interviews to predict patients as healthy control, mild
cognitive impairment (MCI), or dementia and regress the patient's Mini-Mental
State Exam (MMSE) scores. The approach implemented in this work combines
acoustic features (eGeMAPS and Prosody) with embeddings from Whisper and
RoBERTa models, achieving competitive results in both regression (RMSE: 2.7666)
and classification (Macro-F1 score: 0.5774) tasks. Additionally, a novel
two-tiered classification setup is utilized to better differentiate between MCI
and dementia. Our approach achieved strong results on the test set, ranking
seventh on regression and eleventh on classification out of thirty-seven teams,
exceeding the baseline results.; 33) Plastic computing, the cloud continuum journey beyond infinity; The ever increasing challenges introduced by the diversity of current and
envisioned network technologies and IT infrastructure draw a highly distributed
and heterogeneous topology where innovative services must be optimally deployed
to guarantee maximum level of quality for users. Indeed, paradigms such as the
cloud continuum, bringing together edge and cloud computing, along with the new
opportunities coming out by considering non-terrestrial networks connecting
future 6G ecosystems, all with no doubt facilitate the development of
innovative services in many different areas and verticals. However, considering
the intensive data and quality requirements demanded by these services, the
distribution of the execution tasks must be optimally designed. On the
infrastructure side, several initiatives are already active aimed at providing
a Meta-OS that may seamlessly manage the different actors (services,
infrastructure and users) playing under this paradigm. However, several aspects
remain yet limited, particularly when referring to the mapping of resources
into services, where innovative technologies based on bidirectional
coordination and modeling may be pivotal for an optimal performance. In
addition, the upcoming demands coming from the adoption of network technologies
easing users connection with high levels of quality, such as 6G, as well the
study of NTN open up the traditional cloud continuum to include also satellites
that may extend the cloud paradigm further than ever considered. This paper
shows a seed work toward an extendable paradigm so called as plastic computing
whose main objective is to optimize service performance and users satisfaction,
through considering a bidirectional strategy, easily extendable to adopt novel
network and IT technologies and paradigms. Finally, two examples are briefly
introduced to highlight the potential benefits of the plastic computing
adoption; 34) Network Simulator-centric Compositional Testing; This article introduces a novel methodology, Network Simulator-centric
Compositional Testing (NSCT), to enhance the verification of network protocols
with a particular focus on time-varying network properties. NSCT follows a
Model-Based Testing (MBT) approach. These approaches usually struggle to test
and represent time-varying network properties. NSCT also aims to achieve more
accurate and reproducible protocol testing. It is implemented using the Ivy
tool and the Shadow network simulator. This enables online debugging of real
protocol implementations. A case study on an implementation of QUIC (picoquic)
is presented, revealing an error in its compliance with a time-varying
specification. This error has subsequently been rectified, highlighting NSCT's
effectiveness in uncovering and addressing real-world protocol implementation
issues. The article underscores NSCT's potential in advancing protocol testing
methodologies, offering a notable contribution to the field of network protocol
verification.; 35) Finger-to-Chest Style Transfer-assisted Deep Learning Method For
  Photoplethysmogram Waveform Restoration with Timing Preservation; Wearable measurements, such as those obtained by photoplethysmogram (PPG)
sensors are highly susceptible to motion artifacts and noise, affecting
cardiovascular measures. Chest-acquired PPG signals are especially vulnerable,
with signal degradation primarily resulting from lower perfusion,
breathing-induced motion, and mechanical interference from chest movements.
Traditional restoration methods often degrade the signal, and supervised deep
learning (DL) struggles with random and systematic distortions, requiring very
large datasets for successful training. To efficiently restore chest PPG
waveform, we propose a style transfer-assisted cycle-consistent generative
adversarial network, called starGAN, whose performance is evaluated on a
three-channel PPG signal (red, green,and infrared) acquired by a chest-worn
multi-modal sensor, called Soundi. Two identical devices are adopted, one
sensor to collect the PPG signal on the chest, considered to feature low
quality and undergoing restoration, and another sensor to obtain a high-quality
PPG signal measured on the finger, considered the reference signal. Extensive
validation over some 8,000 5-second chunks collected from 40 subjects showed
about 90% correlation of the restored chest PPG with the reference finger PPG,
with a 30% improvement over raw chest PPG. Likewise, the signal-to-noise ratio
improved on average of about 125%, over the three channels. The agreement with
heart-rate computed from concurrent ECG was extremely high, overcoming 84% on
average. These results demonstrate effective signal restoration, comparable
with findings in recent literature papers. Significance: PPG signals collected
from wearable devices are highly susceptible to artifacts, making innovative
AI-based techniques fundamental towards holistic health assessments in a single
device.; 36) A Variational Theory for Soft Shells; Three general modes are distinguished in the deformation of a thin shell;
these are stretching, drilling, and bending. Of these, the drilling mode is the
one more likely to emerge in a soft matter shell (as compared to a hard,
structural one), as it is ignited by a swerve of material fibers about the
local normal. We propose a hyperelastic theory for soft shells, based on a
separation criterion that envisages the strain-energy density as the sum of
three independent pure measures of stretching, drilling, and bending. Each
individual measure is prescribed to vanish on all other companion modes. The
result is a direct, second-grade theory featuring a bending energy quartic in
an invariant strain descriptor that stems from the polar rotation hidden in the
deformation gradient (although quadratic energies are also appropriate in
special cases). The proposed energy functional has a multi-well character,
which fosters cases of soft elasticity (with a continuum of ground states)
related to minimal surfaces.; 37) AdaFlow: Efficient Long Video Editing via Adaptive Attention Slimming
  And Keyframe Selection; Despite great progress, text-driven long video editing is still notoriously
challenging mainly due to excessive memory overhead. Although recent efforts
have simplified this task into a two-step process of keyframe translation and
interpolation generation, the token-wise keyframe translation still plagues the
upper limit of video length. In this paper, we propose a novel and
training-free approach towards efficient and effective long video editing,
termed AdaFlow. We first reveal that not all tokens of video frames hold equal
importance for keyframe translation, based on which we propose an Adaptive
Attention Slimming scheme for AdaFlow to squeeze the $KV$ sequence, thus
increasing the number of keyframes for translations by an order of magnitude.
In addition, an Adaptive Keyframe Selection scheme is also equipped to select
the representative frames for joint editing, further improving generation
quality. With these innovative designs, AdaFlow achieves high-quality long
video editing of minutes in one inference, i.e., more than 1$k$ frames on one
A800 GPU, which is about ten times longer than the compared methods, e.g.,
TokenFlow. To validate AdaFlow, we also build a new benchmark for long video
editing with high-quality annotations, termed LongV-EVAL. Our code is released
at: https://github.com/jidantang55/AdaFlow.; 38) A LP-rounding based algorithm for soft capacitated facility location
  problem with submodular penalties; The soft capacitated facility location problem (SCFLP) is a classic
combinatorial optimization problem, with its variants widely applied in the
fields of operations research and computer science. In the SCFLP, given a set
$\mathcal{F}$ of facilities and a set $\mathcal{D}$ of clients, each facility
has a capacity and an open cost, allowing to open multiple times, and each
client has a demand.
  This problem is to find a subset of facilities in $\mathcal{F}$ and connect
each client to the facilities opened, such that the total cost including open
cost and connection cost is minimied. SCFLP is a NP-hard problem, which has led
to a focus on approximation algorithms. Based on this, we consider a variant,
that is, soft capacitated facility location problem with submodular penalties
(SCFLPSP), which allows some clients not to be served by accepting the penalty
cost. And we consider the integer splittable case of demand, that is, the
demand of each client is served by multiple facilities with the integer service
amount by each facility. Based on LP-rounding, we propose a $(\lambda
R+4)$-approximation algorithm, where $R=\frac{\max_{i \in \mathcal{F}
}f_i}{\min_{i \in \mathcal{F} }f_i},\lambda=\frac{R+\sqrt{R^2+8R}}{2R}$. In
particular, when the open cost is uniform, the approximation ratio is 6.; 39) HyperZero: A Customized End-to-End Auto-Tuning System for Recommendation
  with Hourly Feedback; Modern recommendation systems can be broadly divided into two key stages: the
ranking stage, where the system predicts various user engagements (e.g.,
click-through rate, like rate, follow rate, watch time), and the value model
stage, which aggregates these predictive scores through a function (e.g., a
linear combination defined by a weight vector) to measure the value of each
content by a single numerical score. Both stages play roughly equally important
roles in real industrial systems; however, how to optimize the model weights
for the second stage still lacks systematic study. This paper focuses on
optimizing the second stage through auto-tuning technology. Although general
auto-tuning systems and solutions - both from established production practices
and open-source solutions - can address this problem, they typically require
weeks or even months to identify a feasible solution. Such prolonged tuning
processes are unacceptable in production environments for recommendation
systems, as suboptimal value models can severely degrade user experience. An
effective auto-tuning solution is required to identify a viable model within
2-3 days, rather than the extended timelines typically associated with existing
approaches. In this paper, we introduce a practical auto-tuning system named
HyperZero that addresses these time constraints while effectively solving the
unique challenges inherent in modern recommendation systems. Moreover, this
framework has the potential to be expanded to broader tuning tasks within
recommendation systems.; 40) Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs; Large language models (LLMs) such as OpenAI's o1 have demonstrated remarkable
abilities in complex reasoning tasks by scaling test-time compute and
exhibiting human-like deep thinking. However, we identify a phenomenon we term
underthinking, where o1-like LLMs frequently switch between different reasoning
thoughts without sufficiently exploring promising paths to reach a correct
solution. This behavior leads to inadequate depth of reasoning and decreased
performance, particularly on challenging mathematical problems. To
systematically analyze this issue, we conduct experiments on three challenging
test sets and two representative open-source o1-like models, revealing that
frequent thought switching correlates with incorrect responses. We introduce a
novel metric to quantify underthinking by measuring token efficiency in
incorrect answers. To address underthinking, we propose a decoding strategy
with thought switching penalty TIP that discourages premature transitions
between thoughts, encouraging deeper exploration of each reasoning path.
Experimental results demonstrate that our approach improves accuracy across
challenging datasets without requiring model fine-tuning. Our findings
contribute to understanding reasoning inefficiencies in o1-like LLMs and offer
a practical solution to enhance their problem-solving capabilities.; 41) UnPuzzle: A Unified Framework for Pathology Image Analysis; Pathology image analysis plays a pivotal role in medical diagnosis, with deep
learning techniques significantly advancing diagnostic accuracy and research.
While numerous studies have been conducted to address specific pathological
tasks, the lack of standardization in pre-processing methods and model/database
architectures complicates fair comparisons across different approaches. This
highlights the need for a unified pipeline and comprehensive benchmarks to
enable consistent evaluation and accelerate research progress. In this paper,
we present UnPuzzle, a novel and unified framework for pathological AI research
that covers a broad range of pathology tasks with benchmark results. From
high-level to low-level, upstream to downstream tasks, UnPuzzle offers a
modular pipeline that encompasses data pre-processing, model
composition,taskconfiguration,andexperimentconduction.Specifically, it
facilitates efficient benchmarking for both Whole Slide Images (WSIs) and
Region of Interest (ROI) tasks. Moreover, the framework supports
variouslearningparadigms,includingself-supervisedlearning,multi-task
learning,andmulti-modallearning,enablingcomprehensivedevelopment of pathology
AI models. Through extensive benchmarking across multiple datasets, we
demonstrate the effectiveness of UnPuzzle in streamlining pathology AI research
and promoting reproducibility. We envision UnPuzzle as a cornerstone for future
advancements in pathology AI, providing a more accessible, transparent, and
standardized approach to model evaluation. The UnPuzzle repository is publicly
available at https://github.com/Puzzle-AI/UnPuzzle.; 42) PH-VAE: A Polynomial Hierarchical Variational Autoencoder Towards
  Disentangled Representation Learning; The variational autoencoder (VAE) is a simple and efficient generative
artificial intelligence method for modeling complex probability distributions
of various types of data, such as images and texts. However, it suffers some
main shortcomings, such as lack of interpretability in the latent variables,
difficulties in tuning hyperparameters while training, producing blurry,
unrealistic downstream outputs or loss of information due to how it calculates
loss functions and recovers data distributions, overfitting, and origin gravity
effect for small data sets, among other issues. These and other limitations
have caused unsatisfactory generation effects for the data with complex
distributions. In this work, we proposed and developed a polynomial
hierarchical variational autoencoder (PH-VAE), in which we used a polynomial
hierarchical date format to generate or to reconstruct the data distributions.
In doing so, we also proposed a novel Polynomial Divergence in the loss
function to replace or generalize the Kullback-Leibler (KL) divergence, which
results in systematic and drastic improvements in both accuracy and
reproducibility of the re-constructed distribution function as well as the
quality of re-constructed data images while keeping the dataset size the same
but capturing fine resolution of the data. Moreover, we showed that the
proposed PH-VAE has some form of disentangled representation learning ability.; 43) Is fixed-node diffusion quantum Monte Carlo reproducible?; Fixed-node diffusion quantum Monte Carlo (FN-DMC) is a widely-trusted
many-body method for solving the Schr\""{o}dinger equation, known for its
reliable predictions of material and molecular properties. Furthermore, its
excellent scalability with system complexity and near-perfect utilization of
computational power makes FN-DMC ideally positioned to leverage new advances in
computing to address increasingly complex scientific problems. Even though the
method is widely used as a computational gold standard, reproducibility across
the numerous FN-DMC code implementations has yet to be demonstrated. This
difficulty stems from the diverse array of DMC algorithms and trial wave
functions, compounded by the method's inherent stochastic nature. This study
represents a community-wide effort to address the titular question, affirming
that: Yes, FN-DMC is reproducible (when handled with care). Using the
water-methane dimer as the canonical test case, we compare results from eleven
different FN-DMC codes and show that the approximations to treat the
non-locality of pseudopotentials are the primary source of the discrepancies
between them. In particular, we demonstrate that, for the same choice of
determinantal component in the trial wave function, reliable and reproducible
predictions can be achieved by employing the T-move (TM), the determinant
locality approximation (DLA), or the determinant T-move (DTM) schemes, while
the older locality approximation (LA) leads to considerable variability in
results. This work lays the foundation to establish accurate and reproducible
FN-DMC estimates for all future studies across applications in materials
science, physics, chemistry, and biology.; 44) A Taxonomy of Functional Security Features and How They Can Be Located; Security must be considered in almost every software system. Unfortunately,
selecting and implementing security features remains challenging due to the
variety of security threats and possible countermeasures. While security
standards are intended to help developers, they are usually too abstract and
vague to help implement security features, or they merely help configure such.
A resource that describes security features at an abstraction level between
high-level (i.e., rather too general) and low-level (i.e., rather too specific)
security standards could facilitate secure systems development. To realize
security features, developers typically use external security frameworks, to
minimize implementation mistakes. Even then, developers still make mistakes,
often resulting in security vulnerabilities. When security incidents occur or
the system needs to be audited or maintained, it is essential to know the
implemented security features and, more importantly, where they are located.
This task, commonly referred to as feature location, is often tedious and
error-prone. Therefore, we have to support long-term tracking of implemented
security features.
  We present a study of security features in the literature and their coverage
in popular security frameworks. We contribute (1) a taxonomy of 68 functional
implementation-level security features including a mapping to widely used
security standards, (2) an examination of 21 popular security frameworks
concerning which of these security features they provide, and (3) a discussion
on the representation of security features in source code. Our taxonomy aims to
aid developers in selecting appropriate security features and frameworks and
relating them to security standards when they need to choose and implement
security features for a software system.; 45) Examining the Representation of Youth in the US Policy Documents through
  the Lens of Research; This study explores the representation of youth in US policy documents by
analyzing how research on youth topics is cited within these policies. The
research focuses on three key questions: identifying the frequently discussed
topics in youth research that receive citations in policy documents, discerning
patterns in youth research that contribute to higher citation rates in policy,
and comparing the alignment between topics in youth research and those in
citing policy documents. Through this analysis, the study aims to shed light on
the relationship between academic research and policy formulation, highlighting
areas where youth issues are effectively integrated into policy and
contributing to the broader goal of enhancing youth engagement in societal
decision-making processes.; 46) Test-Time Optimization for Domain Adaptive Open Vocabulary Segmentation; We present Seg-TTO, a novel framework for zero-shot, open-vocabulary semantic
segmentation (OVSS), designed to excel in specialized domain tasks. While
current open-vocabulary approaches show impressive performance on standard
segmentation benchmarks under zero-shot settings, they fall short of supervised
counterparts on highly domain-specific datasets. We focus on
segmentation-specific test-time optimization to address this gap. Segmentation
requires an understanding of multiple concepts within a single image while
retaining the locality and spatial structure of representations. We propose a
novel self-supervised objective adhering to these requirements and use it to
align the model parameters with input images at test time. In the textual
modality, we learn multiple embeddings for each category to capture diverse
concepts within an image, while in the visual modality, we calculate
pixel-level losses followed by embedding aggregation operations specific to
preserving spatial structure. Our resulting framework termed Seg-TTO is a
plug-and-play module. We integrate Seg-TTO with three state-of-the-art OVSS
approaches and evaluate across 22 challenging OVSS tasks covering a range of
specialized domains. Our Seg-TTO demonstrates clear performance improvements
(up to 27% mIoU increase on some datasets) establishing new state-of-the-art.
Our code and models will be released publicly.; 47) Turn That Frown Upside Down: FaceID Customization via Cross-Training
  Data; Existing face identity (FaceID) customization methods perform well but are
limited to generating identical faces as the input, while in real-world
applications, users often desire images of the same person but with variations,
such as different expressions (e.g., smiling, angry) or angles (e.g., side
profile). This limitation arises from the lack of datasets with controlled
input-output facial variations, restricting models' ability to learn effective
modifications.
  To address this issue, we propose CrossFaceID, the first large-scale,
high-quality, and publicly available dataset specifically designed to improve
the facial modification capabilities of FaceID customization models.
Specifically, CrossFaceID consists of 40,000 text-image pairs from
approximately 2,000 persons, with each person represented by around 20 images
showcasing diverse facial attributes such as poses, expressions, angles, and
adornments. During the training stage, a specific face of a person is used as
input, and the FaceID customization model is forced to generate another image
of the same person but with altered facial features. This allows the FaceID
customization model to acquire the ability to personalize and modify known
facial features during the inference stage. Experiments show that models
fine-tuned on the CrossFaceID dataset retain its performance in preserving
FaceID fidelity while significantly improving its face customization
capabilities.
  To facilitate further advancements in the FaceID customization field, our
code, constructed datasets, and trained models are fully available to the
public.; 48) Inverse Stefan problems of determining the time-dependent source
  coefficient and heat flux function; This paper delves into the Inverse Stefan problem, specifically focusing on
determining the time-dependent source coefficient in the parabolic heat
equation governing heat transfer in a semi-infinite rod. The problem entails
the intricate task of uncovering both temperature- and time-dependent
coefficients of the source while accommodating Dirichlet and Neumann boundary
conditions. Through a comprehensive mathematical model and rigorous theoretical
analysis, our study aims to provide a robust methodology for accurately
determining the source coefficient from observed temperature and heat flux data
in problems with different cases of the source functions. Importantly, we
establish the existence and uniqueness, and estimate the continuous dependence
of a weak solution upon the given data for some inverse problems, offering a
foundational understanding of its solvability.; 49) Biological identifications through DNA barcodes; Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution.; 50) Community Detection for Contextual-LSBM: Theoretical Limitations of
  Misclassification Rate and Efficient Algorithms; The integration of network information and node attribute information has
recently gained significant attention in the community detection literature. In
this work, we consider community detection in the Contextual Labeled Stochastic
Block Model (CLSBM), where the network follows an LSBM and node attributes
follow a Gaussian Mixture Model (GMM). Our primary focus is the
misclassification rate, which measures the expected number of nodes
misclassified by community detection algorithms. We first establish a lower
bound on the optimal misclassification rate that holds for any algorithm. When
we specialize our setting to the LSBM (which preserves only network
information) or the GMM (which preserves only node attribute information), our
lower bound recovers prior results. Moreover, we present an efficient
spectral-based algorithm tailored for the CLSBM and derive an upper bound on
its misclassification rate. Although the algorithm does not attain the lower
bound, it serves as a reliable starting point for designing more accurate
community detection algorithms (as many algorithms use spectral method as an
initial step, followed by refinement procedures to enhance accuracy).; 51) Inclusive STEAM Education: A Framework for Teaching Cod-2 ing and
  Robotics to Students with Visually Impairment Using 3 Advanced Computer
  Vision; STEAM education integrates Science, Technology, Engineering, Arts, and
Mathematics to foster creativity and problem-solving. However, students with
visual impairments (VI) encounter significant challenges in programming and
robotics, particularly in tracking robot movements and developing spatial
awareness. This paper presents a framework that leverages pre-constructed
robots and algorithms, such as maze-solving techniques, within an accessible
learning environment. The proposed system employs Contrastive Language-Image
Pre-training (CLIP) to process global camera-captured maze layouts, converting
visual data into textual descriptions that generate spatial audio prompts in an
Audio Virtual Reality (AVR) system. Students issue verbal commands, which are
refined through CLIP, while robot-mounted stereo cameras provide real-time data
processed via Simultaneous Localization and Mapping (SLAM) for continuous
feedback. By integrating these technologies, the framework empowers VI students
to develop coding skills and engage in complex problem-solving tasks. Beyond
maze-solving applications, this approach demonstrates the broader potential of
computer vision in special education, contributing to improved accessibility
and learning experiences in STEAM disciplines.; 52) Detecting Topological Phase Transition in Superconductor-Semiconductor
  Hybrids by Electronic Raman Spectroscopy; In superconductor-semiconductor hybrids, applying a magnetic field closes a
trivial bulk gap and causes a topological phase transition (TPT), resulting in
the emergence of Majorana zero modes at both ends of the wires. However,
trivial Andreev bound states formed at the interface with metallic leads mimic
the local Majorana properties, making it difficult to detect the TPT through
local conductance measurements. In this work, we investigate the detection of
the bulk TPT by exploiting the static and dynamic density response of the
hybrid system. In particular, we demonstrate that the dynamical renormalized
responses reveal the characteristic electronic structure and detect the TPT,
which we then show to produce strong intensities of Raman scattering.
Furthermore, we find that gapless plasmons emerge in the normal state,
signaling the bulk Lifshitz transition. Our results thus predict that the bulk
response of superconducting nanowires is a powerful spectroscopic approach to
detect the bulk topological phase transition.; 53) Spin wave interactions in the pyrochlore Heisenberg antiferromagnet with
  Dzyaloshinskii-Moriya interactions; We study the effect of magnon interactions on the spin wave spectra of the
all-in-all-out phase of the pyrochlore nearest neighbour antiferromagnet with a
Dzyaloshinskii-Moriya interaction ($D$). The leading order corrections to spin
wave energies indicate a significant renormalisation for commonly encountered
strengths of the Dzyaloshinskii-Moriya term. For low values of $D$ we find a
potential instability of the phase itself, indicated by the renormalisation of
magnon frequencies to negative values. We have also studied the renormalized
spectra in the presence of magnetic fields along three high symmetry directions
of the lattice, namely the $[111]$, $[100]$ and $[110]$ directions.
Generically, we find that for a fixed value of the Dzyaloshinskii-Moriya
interaction renormalized spectra for the lowest band decrease with an
increasing strength of the field. We have also analyzed the limits of the two
magnon continuum and probed the possibility of magnon decay. For a range of $D$
and the field strength we identify possible parameter regimes where the decay
of the higher bands of the system are kinematically allowed.; 54) The Signed Two-Space Proximity Model for Learning Representations in
  Protein-Protein Interaction Networks; Accurately predicting complex protein-protein interactions (PPIs) is crucial
for decoding biological processes, from cellular functioning to disease
mechanisms. However, experimental methods for determining PPIs are
computationally expensive. Thus, attention has been recently drawn to machine
learning approaches. Furthermore, insufficient effort has been made toward
analyzing signed PPI networks, which capture both activating (positive) and
inhibitory (negative) interactions. To accurately represent biological
relationships, we present the Signed Two-Space Proximity Model (S2-SPM) for
signed PPI networks, which explicitly incorporates both types of interactions,
reflecting the complex regulatory mechanisms within biological systems. This is
achieved by leveraging two independent latent spaces to differentiate between
positive and negative interactions while representing protein similarity
through proximity in these spaces. Our approach also enables the identification
of archetypes representing extreme protein profiles. S2-SPM's superior
performance in predicting the presence and sign of interactions in SPPI
networks is demonstrated in link prediction tasks against relevant baseline
methods. Additionally, the biological prevalence of the identified archetypes
is confirmed by an enrichment analysis of Gene Ontology (GO) terms, which
reveals that distinct biological tasks are associated with archetypal groups
formed by both interactions. This study is also validated regarding statistical
significance and sensitivity analysis, providing insights into the functional
roles of different interaction types. Finally, the robustness and consistency
of the extracted archetype structures are confirmed using the Bayesian
Normalized Mutual Information (BNMI) metric, proving the model's reliability in
capturing meaningful SPPI patterns.; 55) MedForge: Building Medical Foundation Models Like Open Source Software
  Development; Foundational models (FMs) have made significant strides in the healthcare
domain. Yet the data silo challenge and privacy concern remain in healthcare
systems, hindering safe medical data sharing and collaborative model
development among institutions. The collection and curation of scalable
clinical datasets increasingly become the bottleneck for training strong FMs.
In this study, we propose Medical Foundation Models Merging (MedForge), a
cooperative framework enabling a community-driven medical foundation model
development, meanwhile preventing the information leakage of raw patient data
and mitigating synchronization model development issues across clinical
institutions. MedForge offers a bottom-up model construction mechanism by
flexibly merging task-specific Low-Rank Adaptation (LoRA) modules, which can
adapt to downstream tasks while retaining original model parameters. Through an
asynchronous LoRA module integration scheme, the resulting composite model can
progressively enhance its comprehensive performance on various clinical tasks.
MedForge shows strong performance on multiple clinical datasets (e.g., breast
cancer, lung cancer, and colon cancer) collected from different institutions.
Our major findings highlight the value of collaborative foundation models in
advancing multi-center clinical collaboration effectively and cohesively. Our
code is publicly available at https://github.com/TanZheling/MedForge.; 56) Test-Time Code-Switching for Cross-lingual Aspect Sentiment Triplet
  Extraction; Aspect Sentiment Triplet Extraction (ASTE) is a thriving research area with
impressive outcomes being achieved on high-resource languages. However, the
application of cross-lingual transfer to the ASTE task has been relatively
unexplored, and current code-switching methods still suffer from term boundary
detection issues and out-of-dictionary problems. In this study, we introduce a
novel Test-Time Code-SWitching (TT-CSW) framework, which bridges the gap
between the bilingual training phase and the monolingual test-time prediction.
During training, a generative model is developed based on bilingual
code-switched training data and can produce bilingual ASTE triplets for
bilingual inputs. In the testing stage, we employ an alignment-based
code-switching technique for test-time augmentation. Extensive experiments on
cross-lingual ASTE datasets validate the effectiveness of our proposed method.
We achieve an average improvement of 3.7% in terms of weighted-averaged F1 in
four datasets with different languages. Additionally, we set a benchmark using
ChatGPT and GPT-4, and demonstrate that even smaller generative models
fine-tuned with our proposed TT-CSW framework surpass ChatGPT and GPT-4 by
14.2% and 5.0% respectively.; 57) Sample-Efficient Reinforcement Learning from Human Feedback via
  Information-Directed Sampling; We study the problem of reinforcement learning from human feedback (RLHF), a
critical problem in training large language models, from a theoretical
perspective. Our main contribution is the design of novel sample-efficient RLHF
algorithms based on information-directed sampling (IDS), an online
decision-making principle inspired by information theory. Our algorithms
maximize the sum of the value function and a mutual information term that
encourages exploration of the unknown environment (which quantifies the
information gained about the environment through observed human feedback data).
To tackle the challenge of large state spaces and improve sample efficiency, we
construct a simplified \emph{surrogate environment} and introduce a novel
distance measure (named the \emph{$\ell_g$-distance}), enabling our IDS-based
algorithm to achieve a Bayesian regret upper bound of order
$O(H^{\frac{3}{2}}\sqrt{\log(K(\epsilon)) T})$, where $H$ is the episode
length, $T$ is the number of episode and $K(\epsilon)$ is related to the
covering number of the environment. Specializing to the tabular settings, this
regret bound is of order $\tilde{O}(H^2\sqrt{SAT})$, where $S$ and $A$ are the
numbers of states and actions. Finally, we propose an Approximate-IDS algorithm
that is computationally more efficient while maintaining nearly the same sample
efficiency. The design principle of this approximate algorithm is not only
effective in RLHF settings but also applicable to the standard RL framework.
Moreover, our work showcases the value of information theory in reinforcement
learning and in the training of large language models.; 58) FPGA Innovation Research in the Netherlands: Present Landscape and
  Future Outlook; FPGAs have transformed digital design by enabling versatile and customizable
solutions that balance performance and power efficiency, yielding them
essential for today's diverse computing challenges. Research in the
Netherlands, both in academia and industry, plays a major role in developing
new innovative FPGA solutions. This survey presents the current landscape of
FPGA innovation research in the Netherlands by delving into ongoing projects,
advancements, and breakthroughs in the field. Focusing on recent research
outcome (within the past 5 years), we have identified five key research areas:
a) FPGA architecture, b) FPGA robustness, c) data center infrastructure and
high-performance computing, d) programming models and tools, and e)
applications. This survey provides in-depth insights beyond a mere snapshot of
the current innovation research landscape by highlighting future research
directions within each key area; these insights can serve as a foundational
resource to inform potential national-level investments in FPGA technology.; 59) Debiasing physico-chemical models in air quality monitoring by combining
  different pollutant concentration measures; Air quality monitoring requires to produce accurate estimation of nitrogen
dioxide or fine particulate matter concentration maps, at different moments. A
typical strategy is to combine different types of data. On the one hand,
concentration maps produced by deterministic physicochemical models at urban
scale, and on the other hand, concentration measures made at different points,
different moments, and by different devices. These measures are provided first
by a small number of reference stations, which give reliable measurements of
the concentration, and second by a larger number of micro-sensors, which give
biased and noisier measurements. The proposed approach consists in modeling the
bias of the physicochemical model and estimating the parameters of this bias
using all the available concentration measures. Our model relies on a partition
of the geographical space of interest into different zones within which the
bias is assumed to be modeled by a single affine transformation of the actual
concentration. Our approach allows to improve the concentration maps provided
by the deterministic models but also to understand the behavior of
micro-sensors and their contribution in improving air quality monitoring. We
introduce the model, detail its implementation and experiment it through
numerical results using datasets collected in Grenoble (France).; 60) Safe Multi-agent Satellite Servicing with Control Barrier Functions; The use of control barrier functions under uncertain pose information of
multiple small servicing agents is analyzed for a satellite servicing
application. The application consists of modular servicing agents deployed
towards a tumbling space object from a mothership. Relative position and
orientation of each agent is obtained via fusion of relative range and inertial
measurement sensors. The control barrier functions are utilized to avoid
collisions with other agents for the application of simultaneously relocating
servicing agents on a tumbling body. A differential collision detection and
avoidance framework using the polytopic hull of the tumbling space object is
utilized to safely guide the agents away from the tumbling object.; 61) ChatGPT's advice drives moral judgments with or without justification; Why do users follow moral advice from chatbots? A chatbot is not an
authoritative moral advisor, but it can generate seemingly plausible arguments.
Users do not follow reasoned more readily than unreasoned advice, though, we
find in an experiment. However, this is also true if we attribute advice to a
moral advisor, not a chatbot. Hence, it seems that advice offers users a cheap
way to escape from a moral dilemma. This is a concern that chatbots do not
raise, but they exacerbate it as they make advice easily accessible. We
conclude that it takes ethical in addition to digital literacy to harness users
against moral advice from chatbots.; 62) Phase-matching of high harmonic generation in twisted solids; High harmonic generation (HHG) in solids could enable attosecond and
ultraviolet light sources with high compactness, great controllability and rich
functions. However, the HHG process is accompanied by a quite large wavevector
mismatch that is uncompensated by any traditional phase-matching method,
directly limiting its energy conversion efficiency. Here, we propose an
effective strategy for phase-matching of HHG with arbitrary harmonic orders in
solids. Two flakes of solids with an interlayer twist induce a nonlinear
optical phase that depends on the crystal symmetry, twist angle and harmonic
order, which can be accurately designed to compensate for the phase mismatch in
HHG. Guided by the twist-phase-matching theory, we achieved a record-high
conversion efficiency of $~1.5\times10^{-5}$ for the fifth HHG in twisted
hexagonal boron nitride crystals with a total thickness of only 1 ${\mu}m$. Our
work establishes a foundation for developing ultrashort-wavelength and
ultrafast-pulse laser sources in compact solid-state tabletop systems for
fundamental and applied sciences.; 63) Scalar field source Teleparallel Robertson-Walker F(T)-gravity solutions; This paper investigates the teleparallel Robertson--Walker (TRW) $F(T)$
gravity solutions for a scalar field source. We use the TRW $F(T)$ gravity
field equations (FEs) for each $k$-parameter value case added by a scalar field
to find new teleparallel $F(T)$ solutions. For $k=0$, we find an
easy-to-compute $F(T)$ solution formula applicable for any scalar field source.
Then, we obtain, for $k=-1$ and $+1$ situations, some new analytical $F(T)$
solutions, only for specific $n$-parameter values and well-determined scalar
field cases. We can find by those computations a large number of analytical
teleparallel $F(T)$ solutions independent of any scalar potential $V(\phi)$
expression. The $V(\phi)$ independence makes the FE solving and computations
easier. The new solutions will be relevant for future cosmological applications
in dark matter, dark energy (DE) quintessence, phantom energy and quintom
models of physical processes.; 64) Spin squeezing in an ensemble of nitrogen-vacancy centers in diamond; Spin squeezed states provide a seminal example of how the structure of
quantum mechanical correlations can be controlled to produce metrologically
useful entanglement. Such squeezed states have been demonstrated in a wide
variety of artificial quantum systems ranging from atoms in optical cavities to
trapped ion crystals. By contrast, despite their numerous advantages as
practical sensors, spin ensembles in solid-state materials have yet to be
controlled with sufficient precision to generate targeted entanglement such as
spin squeezing. In this work, we present the first experimental demonstration
of spin squeezing in a solid-state spin system. Our experiments are performed
on a strongly-interacting ensemble of nitrogen-vacancy (NV) color centers in
diamond at room temperature, and squeezing (-0.5 $\pm$ 0.1 dB) is generated by
the native magnetic dipole-dipole interaction between NVs. In order to generate
and detect squeezing in a solid-state spin system, we overcome a number of key
challenges of broad experimental and theoretical interest. First, we develop a
novel approach, using interaction-enabled noise spectroscopy, to characterize
the quantum projection noise in our system without directly resolving the spin
probability distribution. Second, noting that the random positioning of spin
defects severely limits the generation of spin squeezing, we implement a pair
of strategies aimed at isolating the dynamics of a relatively ordered
sub-ensemble of NV centers. Our results open the door to entanglement-enhanced
metrology using macroscopic ensembles of optically active spins in solids.; 65) Measuring ultrafast laser pulses using a single-shot amplitude swing
  implementation; Single-shot characterization techniques are crucial when dealing with
shot-to-shot pulse-shape fluctuations (e.g., unstable laser systems,
high-power, or with low repetition rate) since the scanning configurations
cannot measure single pulses. The demand for simple setups that can be easily
adapted to a wide variety of experimental conditions is continuously rising. In
this work, we propose a single-shot implementation of amplitude swing,
maintaining the compactness, versatility, and robustness of the scanning
versions of this technique. First, we theoretically study the proposed
implementation, based on a pair of uniaxial wedges. Then, we present the
retrieval ptychographic algorithm. Finally, we experimentally demonstrate the
setup by comparing the single-shot and scanning traces and their retrieved
pulses. In sum, we provide the ultrafast science community with a simple and
versatile setup capable of measuring single laser pulses, which is necessary
for characterizing fluctuating pulse trains, meeting the current increasing
demand.; 66) LazyMAR: Accelerating Masked Autoregressive Models via Feature Caching; Masked Autoregressive (MAR) models have emerged as a promising approach in
image generation, expected to surpass traditional autoregressive models in
computational efficiency by leveraging the capability of parallel decoding.
However, their dependence on bidirectional self-attention inherently conflicts
with conventional KV caching mechanisms, creating unexpected computational
bottlenecks that undermine their expected efficiency. To address this problem,
this paper studies the caching mechanism for MAR by leveraging two types of
redundancy: Token Redundancy indicates that a large portion of tokens have very
similar representations in the adjacent decoding steps, which allows us to
first cache them in previous steps and then reuse them in the later steps.
Condition Redundancy indicates that the difference between conditional and
unconditional output in classifier-free guidance exhibits very similar values
in adjacent steps. Based on these two redundancies, we propose LazyMAR, which
introduces two caching mechanisms to handle them one by one. LazyMAR is
training-free and plug-and-play for all MAR models. Experimental results
demonstrate that our method achieves 2.83 times acceleration with almost no
drop in generation quality. Our codes will be released in
https://github.com/feihongyan1/LazyMAR.; 67) Contrastive Token-level Explanations for Graph-based Rumour Detection; The widespread use of social media has accelerated the dissemination of
information, but it has also facilitated the spread of harmful rumours, which
can disrupt economies, influence political outcomes, and exacerbate public
health crises, such as the COVID-19 pandemic. While Graph Neural Network
(GNN)-based approaches have shown significant promise in automated rumour
detection, they often lack transparency, making their predictions difficult to
interpret. Existing graph explainability techniques fall short in addressing
the unique challenges posed by the dependencies among feature dimensions in
high-dimensional text embeddings used in GNN-based models. In this paper, we
introduce Contrastive Token Layerwise Relevance Propagation (CT-LRP), a novel
framework designed to enhance the explainability of GNN-based rumour detection.
CT-LRP extends current graph explainability methods by providing token-level
explanations that offer greater granularity and interpretability. We evaluate
the effectiveness of CT-LRP across multiple GNN models trained on three
publicly available rumour detection datasets, demonstrating that it
consistently produces high-fidelity, meaningful explanations, paving the way
for more robust and trustworthy rumour detection systems.; 68) On the origin of bulk-related anisotropies in surface optical spectra; Reflection anisotropy spectroscopy (RAS) is a powerful method for probing the
optical properties of surfaces, used routinely in research and industrial
applications, yet the origin of 'bulk-related' features that appear in the
spectra of various surfaces has been debated for nearly 40 years. It is often
argued that these features are related to surface-induced bulk anisotropy
(SIBA) because they coincide with critical energies of the bulk dielectric
function. In general, any quantitative RAS theory must include excitonic
effects as they significantly influence the spectra and are believed to be the
key to determining the origin of SIBA features. Here, we introduce a
layer-resolved exciton localization (LREL) measure within the framework of
many-body perturbation theory, which enables a quantitative analysis of the
origins of 'bulk-related' RAS features. Applying LREL to arsenic-modified
silicon reconstructions reveals that, depending on the surface reconstruction,
the 'apparent' SIBA features arise primarily from states localized at the
surface, with only a small contribution from the underlying layers. Our
findings, further supported by the fact that the calculated spectra agree well
with low-temperature RAS measurements, challenge the conventional explanation
of 'bulk-related' RAS features. They indicate that in many instances
bulk-enhanced surface anisotropies (BESA)-the opposite of SIBA-contribute to,
or are even responsible for, 'bulk-related' RAS features. Therefore, we suggest
that previously studied semiconductor surfaces, which exhibit 'bulk-related'
features in their spectra, should be reanalyzed using the presented method.; 69) Teacher Encoder-Student Decoder Denoising Guided Segmentation Network
  for Anomaly Detection; Visual anomaly detection is a highly challenging task, often categorized as a
one-class classification and segmentation problem. Recent studies have
demonstrated that the student-teacher (S-T) framework effectively addresses
this challenge. However, most S-T frameworks rely solely on pre-trained teacher
networks to guide student networks in learning multi-scale similar features,
overlooking the potential of the student networks to enhance learning through
multi-scale feature fusion. In this study, we propose a novel model named
PFADSeg, which integrates a pre-trained teacher network, a denoising student
network with multi-scale feature fusion, and a guided anomaly segmentation
network into a unified framework. By adopting a unique teacher-encoder and
student-decoder denoising mode, the model improves the student network's
ability to learn from teacher network features. Furthermore, an adaptive
feature fusion mechanism is introduced to train a self-supervised segmentation
network that synthesizes anomaly masks autonomously, significantly increasing
detection performance. Evaluated on the MVTec AD dataset, PFADSeg achieves
state-of-the-art results with an image-level AUC of 98.9%, a pixel-level mean
precision of 76.4%, and an instance-level mean precision of 78.7%.; 70) Collision Risk Quantification and Conflict Resolution in Trajectory
  Tracking for Acceleration-Actuated Multi-Robot Systems; One of the pivotal challenges in a multi-robot system is how to give
attention to accuracy and efficiency while ensuring safety. Prior arts cannot
strictly guarantee collision-free for an arbitrarily large number of robots or
the results are considerably conservative. Smoothness of the avoidance
trajectory also needs to be further optimized. This paper proposes an
accelerationactuated simultaneous obstacle avoidance and trajectory tracking
method for arbitrarily large teams of robots, that provides a nonconservative
collision avoidance strategy and gives approaches for deadlock avoidance. We
propose two ways of deadlock resolution, one involves incorporating an
auxiliary velocity vector into the error function of the trajectory tracking
module, which is proven to have no influence on global convergence of the
tracking error. Furthermore, unlike the traditional methods that they address
conflicts after a deadlock occurs, our decision-making mechanism avoids the
near-zero velocity, which is much more safer and efficient in crowed
environments. Extensive comparison show that the proposed method is superior to
the existing studies when deployed in a large-scale robot system, with minimal
invasiveness.; 71) COS(M+O)S: Curiosity and RL-Enhanced MCTS for Exploring Story Space via
  Language Models; We present COS(M+O)S, a System 2-inspired framework for open-ended plot
development that systematically explores the vast space of possible story
expansions, enabling a 3B-parameter language model to approach the plot quality
of a 70B model on select short-story tasks. The method accomplishes this by
combining Monte Carlo Tree Search (MCTS), guided by a step-level value model
that rewards moderate surprisal (curiosity) while penalizing incoherence, and
Odds Ratio Preference Optimization (ORPO) to fine-tune the policy on high-value
plot expansions. This iterative reinforcement learning loop systematically
explores multiple candidate plot branches, backpropagates quality signals, and
adapts the policy for faster convergence, notably shifting the policy from
puzzle-based Chain-of-Thought to more character-driven storytelling. In
small-scale tests with short-story prompts, 67%-77% of participants favored
COS(M+O)S's highest-rated expansions over lower-rated ones, suggesting that our
learned value function aligns. GPT-4o ratings further show that COS(M+O)S
surpasses naive single-pass decoding from Llama 3.2 3B by 0.59 SD, coming
within 0.06 SD of Llama 3.1 70B (no significant difference, p=0.93). Pairwise
comparisons with o1 place COS(M+O)S 1.5 SD above the 3B baseline and find no
statistically significant gap from 70B. Nevertheless, absolute story quality
remains modest, constrained by the small model's capacity and limited training
data.; 72) SS-MPC: A Sequence-Structured Multi-Party Conversation System; Recent Multi-Party Conversation (MPC) models typically rely on graph-based
approaches to capture dialogue structures. However, these methods have
limitations, such as information loss during the projection of utterances into
structural embeddings and constraints in leveraging pre-trained language models
directly. In this paper, we propose \textbf{SS-MPC}, a response generation
model for MPC that eliminates the need for explicit graph structures. Unlike
existing models that depend on graphs to analyze conversation structures,
SS-MPC internally encodes the dialogue structure as a sequential input,
enabling direct utilization of pre-trained language models. Experimental
results show that \textbf{SS-MPC} achieves \textbf{15.60\% BLEU-1} and
\textbf{12.44\% ROUGE-L} score, outperforming the current state-of-the-art MPC
response generation model by \textbf{3.91\%p} in \textbf{BLEU-1} and
\textbf{0.62\%p} in \textbf{ROUGE-L}. Additionally, human evaluation confirms
that SS-MPC generates more fluent and accurate responses compared to existing
MPC models.; 73) Hierarchical Clustering Algorithms on Poisson and Cox Point Processes; Clustering is a widely used technique in unsupervised learning to identify
groups within a dataset based on the similarities between its elements. This
paper introduces three new hierarchical clustering models, Clustroid
Hierarchical Nearest Neighbor (\(\mathrm{CHN}^2\)), Single Linkage Hierarchical
Nearest Neighbor (\(\mathrm{SHN}^2\)), and Hausdorff (Complete Linkage)
Hierarchical Nearest Neighbor (\(\mathrm{H}^2\mathrm{N}^2\)), all designed for
datasets with a countably infinite number of points. These algorithms proceed
through multiple levels of clustering and construct clusters by connecting
nearest-neighbor points or clusters, but differ in the distance metrics they
employ (clustroid, single linkage, or Hausdorff, respectively). Each method is
first applied to the homogeneous Poisson point process on the Euclidean space,
where it defines a phylogenetic forest, which is a factor of the point process
and therefore unimodular. The results established for the \(\mathrm{CHN}^2\)
algorithm include the almost-sure finiteness of the clusters and bounds on the
mean cluster size at each level of the algorithm. The mean size of the typical
cluster is shown to be infinite. Moreover, the limiting structure of all three
algorithms is examined as the number of levels tends to infinity, and
properties such as the one-endedness of the limiting connected components are
derived. In the specific case of \(\mathrm{SHN}^2\) on the Poisson point
process, the limiting graph is shown to be a subgraph of the Minimal Spanning
Forest. The \(\mathrm{CHN}^2\) algorithm is also extended beyond the Poisson
setting, to certain stationary Cox point processes. Similar finite-cluster
properties are shown to hold in these cases. It is also shown that efficient
detection of Cox-triggered aggregation can be achieved through this clustering
algorithm.; 74) A Survey of Zero-Knowledge Proof Based Verifiable Machine Learning; As machine learning technologies advance rapidly across various domains,
concerns over data privacy and model security have grown significantly. These
challenges are particularly pronounced when models are trained and deployed on
cloud platforms or third-party servers due to the computational resource
limitations of users' end devices. In response, zero-knowledge proof (ZKP)
technology has emerged as a promising solution, enabling effective validation
of model performance and authenticity in both training and inference processes
without disclosing sensitive data. Thus, ZKP ensures the verifiability and
security of machine learning models, making it a valuable tool for
privacy-preserving AI. Although some research has explored the verifiable
machine learning solutions that exploit ZKP, a comprehensive survey and summary
of these efforts remain absent. This survey paper aims to bridge this gap by
reviewing and analyzing all the existing Zero-Knowledge Machine Learning (ZKML)
research from June 2017 to December 2024. We begin by introducing the concept
of ZKML and outlining its ZKP algorithmic setups under three key categories:
verifiable training, verifiable inference, and verifiable testing. Next, we
provide a comprehensive categorization of existing ZKML research within these
categories and analyze the works in detail. Furthermore, we explore the
implementation challenges faced in this field and discuss the improvement works
to address these obstacles. Additionally, we highlight several commercial
applications of ZKML technology. Finally, we propose promising directions for
future advancements in this domain.; 75) Multiple change point detection based on Hodrick-Prescott and $l_1$
  filtering method for random walk time series data; We propose new methods for detecting multiple change points in time series,
specifically designed for random walk processes, where stationarity and
variance changes present challenges. Our approach combines two trend estimation
methods: the Hodrick Prescott (HP) filter and the l1 filter. A major challenge
in these methods is selecting the tuning parameter lambda, which we address by
introducing two selection techniques. For the HP based change point detection,
we propose a probability-based threshold to select lambda under the assumption
of an exponential distribution. For the l1 based method, we suggest a selection
strategy assuming normality. Additionally, we introduce a technique to estimate
the maximum number of change points in time segments using the l1 based method.
We validate our methods by comparing them to similar techniques, such as PELT,
using simulated data. We also demonstrate the practical application of our
approach to real-world SNP stock data, showcasing its effectiveness in
detecting change points.; 76) Analyzing the Impact of AC False Data Injection Attacks on Power System
  Operation; False Data Injection (FDI) attacks are a significant threat to modern power
systems. Although numerous research studies have focused on FDI attacks on
power systems, these studies have primarily concentrated on designing or
detecting DC FDI attacks, with less attention given to the impact analysis of
AC FDI attacks. AC FDI attacks are potentially more harmful as they can easily
bypass bad data detection (BDD) algorithms. In this paper, we present a unified
approach to investigate the impact of AC FDI attacks on power transmission
lines using the PowerWorld simulator. We also investigate the impact of
different FDI attack designs, including those optimally designed to evade BDD
algorithms and compare them accordingly. Our findings demonstrate that in
designing optimal AC FDI attacks, a trade-off between the residuals of state
variables and the corresponding impacts of the proposed attack should be
considered. This is because optimal attacks result in fewer changes in the
attacked variable states and their estimated residuals compared to arbitrary AC
FDI attacks. Moreover, the impacts of optimal AC FDI attacks can be less severe
than those of arbitrary attacks. We implement and analyze the proposed approach
on the IEEE 39-bus test system using PowerWorld simulator.; 77) Decoupling Appearance Variations with 3D Consistent Features in Gaussian
  Splatting; Gaussian Splatting has emerged as a prominent 3D representation in novel view
synthesis, but it still suffers from appearance variations, which are caused by
various factors, such as modern camera ISPs, different time of day, weather
conditions, and local light changes. These variations can lead to floaters and
color distortions in the rendered images/videos. Recent appearance modeling
approaches in Gaussian Splatting are either tightly coupled with the rendering
process, hindering real-time rendering, or they only account for mild global
variations, performing poorly in scenes with local light changes. In this
paper, we propose DAVIGS, a method that decouples appearance variations in a
plug-and-play and efficient manner. By transforming the rendering results at
the image level instead of the Gaussian level, our approach can model
appearance variations with minimal optimization time and memory overhead.
Furthermore, our method gathers appearance-related information in 3D space to
transform the rendered images, thus building 3D consistency across views
implicitly. We validate our method on several appearance-variant scenes, and
demonstrate that it achieves state-of-the-art rendering quality with minimal
training time and memory usage, without compromising rendering speeds.
Additionally, it provides performance improvements for different Gaussian
Splatting baselines in a plug-and-play manner.; 78) FlexQuant: Elastic Quantization Framework for Locally Hosted LLM on Edge
  Devices; Deploying LLMs on edge devices presents serious technical challenges. Memory
elasticity is crucial for edge devices with unified memory, where memory is
shared and fluctuates dynamically. Existing solutions suffer from either poor
transition granularity or high storage costs. We propose FlexQuant, a novel
elasticity framework that generates an ensemble of quantized models, providing
an elastic hosting solution with 15x granularity improvement and 10x storage
reduction compared to SoTA methods. FlexQuant works with most quantization
methods and creates a family of trade-off options under various storage limits
through our pruning method. It brings great performance and flexibility to the
edge deployment of LLMs.; 79) Effect of metal (Ti) interlayer on fracture toughness of TiN thin films; Titanium nitride (TiN) is widely used as a protective coating due to its high
hardness, but suffers from inherent brittleness and low fracture toughness,
limiting its applicability. The layering of TiN films with metallic titanium
(Ti) improves the overall fracture behaviour of the architecture by modifying
the crack driving force due to elastic-plastic mismatch between the layers.
Microcantilever fracture tests were carried out on bilayer (Ti-TiN, TiN-Ti) and
trilayer (Ti-TiN-Ti) architectures to determine the fracture toughness and
study the fundamental crack growth behaviour. The initiation fracture toughness
in bilayer architecture with crack in Ti layer is almost 70% higher, when
compared to crack in TiN layer. In Ti-TiN bilayer the crack propagated
catastrophically post linear elastic deformation, whereas the crack was
arrested at the TiN/Ti interface in both TiN-Ti and Ti-TiN-Ti architectures due
to plastic energy dissipation in the Ti layer. Crack tip plasticity originated
in the metallic Ti layer in TiN-Ti and Ti-TiN-Ti architectures increased the
total fracture resistance by more than eight-fold compared to the Ti-TiN
bilayer.; 80) Retrieval Augmented Generation and Understanding in Vision: A Survey and
  New Outlook; Retrieval-augmented generation (RAG) has emerged as a pivotal technique in
artificial intelligence (AI), particularly in enhancing the capabilities of
large language models (LLMs) by enabling access to external, reliable, and
up-to-date knowledge sources. In the context of AI-Generated Content (AIGC),
RAG has proven invaluable by augmenting model outputs with supplementary,
relevant information, thus improving their quality. Recently, the potential of
RAG has extended beyond natural language processing, with emerging methods
integrating retrieval-augmented strategies into the computer vision (CV)
domain. These approaches aim to address the limitations of relying solely on
internal model knowledge by incorporating authoritative external knowledge
bases, thereby improving both the understanding and generation capabilities of
vision models. This survey provides a comprehensive review of the current state
of retrieval-augmented techniques in CV, focusing on two main areas: (I) visual
understanding and (II) visual generation. In the realm of visual understanding,
we systematically review tasks ranging from basic image recognition to complex
applications such as medical report generation and multimodal question
answering. For visual content generation, we examine the application of RAG in
tasks related to image, video, and 3D generation. Furthermore, we explore
recent advancements in RAG for embodied AI, with a particular focus on
applications in planning, task execution, multimodal perception, interaction,
and specialized domains. Given that the integration of retrieval-augmented
techniques in CV is still in its early stages, we also highlight the key
limitations of current approaches and propose future research directions to
drive the development of this promising area.; 81) Solving the Catastrophic Forgetting Problem in Generalized Category
  Discovery; Generalized Category Discovery (GCD) aims to identify a mix of known and
novel categories within unlabeled data sets, providing a more realistic setting
for image recognition. Essentially, GCD needs to remember existing patterns
thoroughly to recognize novel categories. Recent state-of-the-art method SimGCD
transfers the knowledge from known-class data to the learning of novel classes
through debiased learning. However, some patterns are catastrophically forgot
during adaptation and thus lead to poor performance in novel categories
classification. To address this issue, we propose a novel learning approach,
LegoGCD, which is seamlessly integrated into previous methods to enhance the
discrimination of novel classes while maintaining performance on previously
encountered known classes. Specifically, we design two types of techniques
termed as Local Entropy Regularization (LER) and Dual-views Kullback Leibler
divergence constraint (DKL). The LER optimizes the distribution of potential
known class samples in unlabeled data, thus ensuring the preservation of
knowledge related to known categories while learning novel classes. Meanwhile,
DKL introduces Kullback Leibler divergence to encourage the model to produce a
similar prediction distribution of two view samples from the same image. In
this way, it successfully avoids mismatched prediction and generates more
reliable potential known class samples simultaneously. Extensive experiments
validate that the proposed LegoGCD effectively addresses the known category
forgetting issue across all datasets, eg, delivering a 7.74% and 2.51% accuracy
boost on known and novel classes in CUB, respectively. Our code is available
at: https://github.com/Cliffia123/LegoGCD.; 82) Chameleon2++: An Efficient Chameleon2 Clustering with Approximate
  Nearest Neighbors; Clustering algorithms are fundamental tools in data analysis, with
hierarchical methods being particularly valuable for their flexibility.
Chameleon is a widely used hierarchical clustering algorithm that excels at
identifying high-quality clusters of arbitrary shapes, sizes, and densities.
Chameleon2 is the most recent variant that has demonstrated significant
improvements, but suffers from critical failings and there are certain
improvements that can be made.
  The first failure we address is that the complexity of Chameleon2 is claimed
to be $O(n^2)$, while we demonstrate that it is actually $O(n^2\log{n})$, with
$n$ being the number of data points. Furthermore, we suggest improvements to
Chameleon2 that ensure that the complexity remains $O(n^2)$ with minimal to no
loss of performance. The second failing of Chameleon2 is that it lacks
transparency and it does not provide the fine-tuned algorithm parameters used
to obtain the claimed results. We meticulously provide all such parameter
values to enhance replicability.
  The improvement which we make in Chameleon2 is that we replace the exact
$k$-NN search with an approximate $k$-NN search. This further reduces the
algorithmic complexity down to $O(n\log{n})$ without any performance loss.
Here, we primarily configure three approximate nearest neighbor search
algorithms (Annoy, FLANN and NMSLIB) to align with the overarching Chameleon2
clustering framework. Experimental evaluations on standard benchmark datasets
demonstrate that the proposed Chameleon2++ algorithm is more efficient, robust,
and computationally optimal.; 83) AI Mimicry and Human Dignity: Chatbot Use as a Violation of Self-Respect; This paper investigates how human interactions with AI-powered chatbots may
offend human dignity. Current chatbots, driven by large language models (LLMs),
mimic human linguistic behaviour but lack the moral and rational capacities
essential for genuine interpersonal respect. Human beings are prone to
anthropomorphise chatbots. Indeed, chatbots appear to be deliberately designed
to elicit that response. As a result, human beings' behaviour toward chatbots
often resembles behaviours typical of interaction between moral agents. Drawing
on a second-personal, relational account of dignity, we argue that interacting
with chatbots in this way is incompatible with the dignity of users. We show
that, since second-personal respect is premised on reciprocal recognition of
second-personal authority, behaving towards chatbots in ways that convey
second-personal respect is bound to misfire in morally problematic ways, given
the lack of reciprocity. Consequently, such chatbot interactions amount to
subtle but significant violations of self-respect: the respect we are dutybound
to show for our own dignity. We illustrate this by discussing four actual
chatbot use cases (information retrieval, customer service, advising, and
companionship), and propound that the increasing societal pressure to engage in
such interactions with chatbots poses a hitherto underappreciated threat to
human dignity.; 84) From division to extension; We present a short proof of a version of the Ohsawa-Takegoshi-Manivel $L^2$
extension theorem as a corollary of a Skoda-type $L^2$ division theorem with
bounded generators. The new division theorem is of independent interest: the
boundedness of generators allows to send the parameter $\alpha>1$ of the usual
$L^2$ division theorems to 1 in the norm of the datum of the division. As an
aside, we also use the new division theorem to prove a Brian\c{c}on-Skoda-type
result.; 85) Large Language Model-Based Benchmarking Experiment Settings for
  Evolutionary Multi-Objective Optimization; When we manually design an evolutionary optimization algorithm, we implicitly
or explicitly assume a set of target optimization problems. In the case of
automated algorithm design, target optimization problems are usually explicitly
shown. Recently, the use of large language models (LLMs) for the design of
evolutionary multi-objective optimization (EMO) algorithms have been examined
in some studies. In those studies, target multi-objective problems are not
always explicitly shown. It is well known in the EMO community that the
performance evaluation results of EMO algorithms depend on not only test
problems but also many other factors such as performance indicators, reference
point, termination condition, and population size. Thus, it is likely that the
designed EMO algorithms by LLMs depends on those factors. In this paper, we try
to examine the implicit assumption about the performance comparison of EMO
algorithms in LLMs. For this purpose, we ask LLMs to design a benchmarking
experiment of EMO algorithms. Our experiments show that LLMs often suggest
classical benchmark settings: Performance examination of NSGA-II, MOEA/D and
NSGA-III on ZDT, DTLZ and WFG by HV and IGD under the standard parameter
specifications.; 86) Variable Bregman Majorization-Minimization Algorithm and its Application
  to Dirichlet Maximum Likelihood Estimation; We propose a novel Bregman descent algorithm for minimizing a convex function
that is expressed as the sum of a differentiable part (defined over an open
set) and a possibly nonsmooth term. The approach, referred to as the Variable
Bregman Majorization-Minimization (VBMM) algorithm, extends the Bregman
Proximal Gradient method by allowing the Bregman function used in the
divergence to adaptively vary at each iteration, provided it satisfies a
majorizing condition on the objective function. This adaptive framework enables
the algorithm to approximate the objective more precisely at each iteration,
thereby allowing for accelerated convergence compared to the traditional
Bregman Proximal Gradient descent. We establish the convergence of the VBMM
algorithm to a minimizer under mild assumptions on the family of metrics used.
Furthermore, we introduce a novel application of both the Bregman Proximal
Gradient method and the VBMM algorithm to the estimation of the
multidimensional parameters of a Dirichlet distribution through the
maximization of its log-likelihood. Numerical experiments confirm that the VBMM
algorithm outperforms existing approaches in terms of convergence speed.; 87) Multi-task Learning for Identification of Porcelain in Song and Yuan
  Dynasties; Chinese porcelain holds immense historical and cultural value, making its
accurate classification essential for archaeological research and cultural
heritage preservation. Traditional classification methods rely heavily on
expert analysis, which is time-consuming, subjective, and difficult to scale.
This paper explores the application of DL and transfer learning techniques to
automate the classification of porcelain artifacts across four key attributes:
dynasty, glaze, ware, and type. We evaluate four Convolutional Neural Networks
(CNNs) - ResNet50, MobileNetV2, VGG16, and InceptionV3 - comparing their
performance with and without pre-trained weights. Our results demonstrate that
transfer learning significantly enhances classification accuracy, particularly
for complex tasks like type classification, where models trained from scratch
exhibit lower performance. MobileNetV2 and ResNet50 consistently achieve high
accuracy and robustness across all tasks, while VGG16 struggles with more
diverse classifications. We further discuss the impact of dataset limitations
and propose future directions, including domain-specific pre-training,
integration of attention mechanisms, explainable AI methods, and generalization
to other cultural artifacts.; 88) The erasure of intensive livestock farming in text-to-image generative
  AI; Generative AI (e.g., ChatGPT) is increasingly integrated into people's daily
lives. While it is known that AI perpetuates biases against marginalized human
groups, their impact on non-human animals remains understudied. We found that
ChatGPT's text-to-image model (DALL-E 3) introduces a strong bias toward
romanticizing livestock farming as dairy cows on pasture and pigs rooting in
mud. This bias remained when we requested realistic depictions and was only
mitigated when the automatic prompt revision was inhibited. Most farmed animal
in industrialized countries are reared indoors with limited space per animal,
which fail to resonate with societal values. Inhibiting prompt revision
resulted in images that more closely reflected modern farming practices; for
example, cows housed indoors accessing feed through metal headlocks, and pigs
behind metal railings on concrete floors in indoor facilities. While OpenAI
introduced prompt revision to mitigate bias, in the case of farmed animal
production systems, it paradoxically introduces a strong bias towards
unrealistic farming practices.; 89) Derivation of a Multiscale Ferrofluid Model: Superparamagnetic Behavior
  due to Fast Spin Flip; We consider a microscopic model of $N$ magnetic nanoparticles in a Stokes
flow. We assume that the temperature is above the critical N\'eel temperature
such that the particles' magnetizations undergo random flip with rate
$1/\varepsilon$. The microscopic system is the modeled through a piecewise
deterministic Markov jump process. We show that for large $N$, small particle
volume fraction and small $\varepsilon$, the system can be effectively
described by a multiscale model.; 90) Unshackling Context Length: An Efficient Selective Attention Approach
  through Query-Key Compression; Handling long-context sequences efficiently remains a significant challenge
in large language models (LLMs). Existing methods for token selection in
sequence extrapolation either employ a permanent eviction strategy or select
tokens by chunk, which may lead to the loss of critical information. We propose
Efficient Selective Attention (ESA), a novel approach that extends context
length by efficiently selecting the most critical tokens at the token level to
compute attention. ESA reduces the computational complexity of token selection
by compressing query and key vectors into lower-dimensional representations. We
evaluate ESA on long sequence benchmarks with maximum lengths up to 256k using
open-source LLMs with context lengths of 8k and 32k. ESA outperforms other
selective attention methods, especially in tasks requiring the retrieval of
multiple pieces of information, achieving comparable performance to
full-attention extrapolation methods across various tasks, with superior
results in certain tasks.; 91) The quantum nature of ubiquitous vibrational features revealed for
  ethylene glycol; Vibrational properties of molecules are of widespread interest and importance
in chemistry and biochemistry. The reliability of widely employed approximate
computational methods is questioned here against the complex experimental
spectrum of ethylene glycol. Comparisons between quantum vibrational
self-consistent field and virtual-state configuration interaction (VSCF/VCI),
adiabatically switched semiclassical initial value representation (AS SCIVR),
and thermostatted ring polymer molecular dynamics (TRPMD) calculations are made
using a full-dimensional machine-learned potential energy surface. Calculations
are done for five low-lying conformers and compared with the experiment, with a
focus on the high-frequency, OH-stretches, and CH-stretches, part of the
spectrum. Fermi resonances are found in the analysis of VSCF/VCI eigenstates
belonging to the CH-stretching band. Results of comparable accuracy, quality,
and level of detail are obtained by means of AS SCIVR. The current VSCF/VCI and
AS-SCIVR power spectra largely close the gaps between the experiment and TRPMD
and classical MD calculations. Analysis of these results provide guidance on
what level of accuracy to expect from TRPMD and classical MD calculations of
the vibrational spectra for ubiquitous CH and OH-stretches bands. This work
shows that even general vibrational features require a proper quantum treatment
usually not achievable by the most popular theoretical approaches.; 92) Tensor-based Dinkelbach method for computing generalized tensor
  eigenvalues and its applications; In this paper, we propose a novel tensor-based Dinkelbach--Type method for
computing extremal tensor generalized eigenvalues. We show that the extremal
tensor generalized eigenvalue can be reformulated as a critical subproblem of
the classical Dinkelbach--Type method, which can subsequently be expressed as a
multilinear optimization problem (MOP). The MOP is solved under a spherical
constraint using an efficient proximal alternative minimization method, in
which we rigorously establish the global convergence. Additionally, the
equivalent MOP is reformulated as an unconstrained optimization problem,
allowing for the analysis of the Kurdyka-Lojasiewicz (KL) exponent and
providing an explicit expression for the convergence rate of the proposed
algorithm. Preliminary numerical experiments on solving extremal tensor
generalized eigenvalues and minimizing high-order trust-region subproblems are
provided, validating the efficacy and practical utility of the proposed method.; 93) An extensive simulation study evaluating the interaction of resampling
  techniques across multiple causal discovery contexts; Despite the accelerating presence of exploratory causal analysis in modern
science and medicine, the available non-experimental methods for validating
causal models are not well characterized. One of the most popular methods is to
evaluate the stability of model features after resampling the data, similar to
resampling methods for estimating confidence intervals in statistics. Many
aspects of this approach have received little to no attention, however, such as
whether the choice of resampling method should depend on the sample size,
algorithms being used, or algorithm tuning parameters. We present theoretical
results proving that certain resampling methods closely emulate the assignment
of specific values to algorithm tuning parameters. We also report the results
of extensive simulation experiments, which verify the theoretical result and
provide substantial data to aid researchers in further characterizing
resampling in the context of causal discovery analysis. Together, the
theoretical work and simulation results provide specific guidance on how
resampling methods and tuning parameters should be selected in practice.; 94) Precision mass measurements of $^{74-76}$Sr using TITAN's
  Multiple-Reflection Time-of-Flight Mass Spectrometer; We report precision mass measurements of $^{74-76}$Sr performed with the
TITAN Multiple-Reflection Time-of-Flight Mass Spectrometer. This marks a first
time mass measurement of $^{74}$Sr and gives increased mass precision to both
$^{75}$Sr and $^{76}$Sr which were previously measured using storage ring and
Penning trap methods, respectively. This completes the A = 74, T = 1 isospin
triplet and gives increased precision to the A = 75, T = 1/2 isospin doublet
which are both the heaviest experimentally evaluated triplets and doublets to
date. The new data allow us to evaluate coefficients of the isobaric multiplet
mass equation for the first time at A = 74, and with increased precision at A =
75. With increased precision of 75Sr, we confirm the recent measurement
reported by CSRe which was used to remove a staggering anomaly in the doublets.
New ab initio valence-space in-medium similarity renormalization group
calculations of the T = 1 triplet are presented at A = 74. We also investigate
the impact of the new mass data on the reaction flow of the rapid proton
capture process in type I x-ray bursts using a single-zone model.; 95) CopySpec: Accelerating LLMs with Speculative Copy-and-Paste Without
  Compromising Quality; We introduce CopySpec, an innovative technique designed to tackle the
inefficiencies LLMs face when generating responses that closely resemble
previous outputs. CopySpec identifies repeated sequences in the model's chat
history and speculates that the same tokens will follow, enabling seamless
copying without compromising output quality or requiring additional GPU memory.
To evaluate the effectiveness of our approach, we conducted experiments using
five LLMs and five datasets: MT-Bench, CNN/DM, GSM-8K, HumanEval, and our newly
created dataset, MT-Redundant. MT-Redundant, introduced in this paper,
transforms the second turn of MT-Bench into a request for variations of the
first turn's answer, simulating real-world scenarios where users request
modifications to prior responses. Our results demonstrate significant
speed-ups: up to 2.35x on CNN/DM, 3.08x on the second turn of select
MT-Redundant categories, and 2.66x on the third turn of GSM-8K's
self-correction tasks. Moreover, we show that CopySpec integrates seamlessly
with speculative decoding, yielding an average 49% additional speed-up over
speculative decoding for the second turn of MT-Redundant across all eight
categories. While LLMs, even with speculative decoding, suffer from slower
inference as context sizes grow, CopySpec leverages the expanded context to
accelerate inference, making it faster as the context size increases. Our code
and dataset are publicly available at https://github.com/RazvanDu/CopySpec.; 96) Multivariable Stochastic Newton-Based Extremum Seeking with Delays; This paper presents a Newton-based stochastic extremum-seeking control method
for real-time optimization in multi-input systems with distinct input delays.
It combines predictor-based feedback and Hessian inverse estimation via
stochastic perturbations to enable delay compensation with user-defined
convergence rates. The method ensures exponential stability and convergence
near the unknown extremum, even under long delays. It extends to multi-input,
single-output systems with cross-coupled channels. Stability is analyzed using
backstepping and infinite-dimensional averaging. Numerical simulations
demonstrate its effectiveness in handling time-delayed channels, showcasing
both the challenges and benefits of real-time optimization in distributed
parameter settings.; 97) Building Interval Type-2 Fuzzy Membership Function: A Deck of Cards
  based Co-constructive Approach; Since its inception, Fuzzy Set has been widely used to handle uncertainty and
imprecision in decision-making. However, conventional fuzzy sets, often
referred to as type-1 fuzzy sets (T1FSs) have limitations in capturing higher
levels of uncertainty, particularly when decision-makers (DMs) express
hesitation or ambiguity in membership degree. To address this, Interval Type-2
Fuzzy Sets (IT2FSs) have been introduced by incorporating uncertainty in
membership degree allocation, which enhanced flexibility in modelling
subjective judgments. Despite their advantages, existing IT2FS construction
methods often lack active involvement from DMs and that limits the
interpretability and effectiveness of decision models. This study proposes a
socio-technical co-constructive approach for developing IT2FS models of
linguistic terms by facilitating the active involvement of DMs in preference
elicitation and its application in multicriteria decision-making (MCDM)
problems. Our methodology is structured in two phases. The first phase involves
an interactive process between the DM and the decision analyst, in which a
modified version of Deck-of-Cards (DoC) method is proposed to construct T1FS
membership functions on a ratio scale. We then extend this method to
incorporate ambiguity in subjective judgment and that resulted in an IT2FS
model that better captures uncertainty in DM's linguistic assessments. The
second phase formalizes the constructed IT2FS model for application in MCDM by
defining an appropriate mathematical representation of such information,
aggregation rules, and an admissible ordering principle. The proposed framework
enhances the reliability and effectiveness of fuzzy decision-making not only by
accurately representing DM's personalized semantics of linguistic information.; 98) Splicer$^{+}$: Secure Hub Placement and Deadlock-Free Routing for
  Payment Channel Network Scalability; Payment channel hub (PCH) is a promising approach for payment channel
networks (PCNs) to improve efficiency by deploying robust hubs to steadily
process off-chain transactions. However, existing PCHs, often preplaced without
considering payment request distribution across PCNs, can lead to load
imbalance. PCNs' reliance on source routing, which makes decisions based solely
on individual sender requests, can degrade performance by overlooking other
requests, thus further impairing scalability. In this paper, we introduce
Splicer$^{+}$, a highly scalable multi-PCH solution based on the trusted
execution environment (TEE). We study tradeoffs in communication overhead
between participants, transform the original NP-hard PCH placement problem by
mixed-integer linear programming, and propose optimal/approximate solutions
with load balancing for different PCN scales using supermodular techniques.
Considering global PCN states and local directly connected sender requests, we
design a deadlock-free routing protocol for PCHs. It dynamically adjusts the
payment processing rate across multiple channels and, combined with TEE,
ensures high-performance routing with confidential computation. We provide a
formal security proof for the Splicer$^{+}$ protocol in the UC-framework.
Extensive evaluations demonstrate the effectiveness of Splicer$^{+}$, with
transaction success ratio ($\uparrow$51.1%), throughput ($\uparrow$181.5%), and
latency outperforming state-of-the-art PCNs.; 99) Summarization Metrics for Spanish and Basque: Do Automatic Scores and
  LLM-Judges Correlate with Humans?; Studies on evaluation metrics and LLM-as-a-Judge models for automatic text
summarization have largely been focused on English, limiting our understanding
of their effectiveness in other languages. Through our new dataset BASSE
(BAsque and Spanish Summarization Evaluation), we address this situation by
collecting human judgments on 2,040 abstractive summaries in Basque and
Spanish, generated either manually or by five LLMs with four different prompts.
For each summary, annotators evaluated five criteria on a 5-point Likert scale:
coherence, consistency, fluency, relevance, and 5W1H. We use these data to
reevaluate traditional automatic metrics used for evaluating summaries, as well
as several LLM-as-a-Judge models that show strong performance on this task in
English. Our results show that currently proprietary judge LLMs have the
highest correlation with human judgments, followed by criteria-specific
automatic metrics, while open-sourced judge LLMs perform poorly. We release
BASSE and our code publicly, along with the first large-scale Basque
summarization dataset containing 22,525 news articles with their subheads.; 100) Iterative Counterfactual Data Augmentation; Counterfactual data augmentation (CDA) is a method for controlling
information or biases in training datasets by generating a complementary
dataset with typically opposing biases. Prior work often either relies on
hand-crafted rules or algorithmic CDA methods which can leave unwanted
information in the augmented dataset. In this work, we show iterative CDA
(ICDA) with initial, high-noise interventions can converge to a state with
significantly lower noise. Our ICDA procedure produces a dataset where one
target signal in the training dataset maintains high mutual information with a
corresponding label and the information of spurious signals are reduced. We
show training on the augmented datasets produces rationales on documents that
better align with human annotation. Our experiments include six human produced
datasets and two large-language model generated datasets.",0.16666666666666666,0.3010299956639812
2412.11084,applied,2412.11084-pos2-1,"Biological identifications through DNA barcodes; Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution.",2412.11084-pos1-1,"BarcodeBERT: Transformers for Biodiversity Analysis; Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https://github.com/Kari-Genomics-Lab/BarcodeBERT",47,"['47', '1', '39', '3', '12', '8', '6', '2', '41', '5']","The paper titled 'BarcodeBERT: Transformers for Biodiversity Analysis' (number 47) aligns best with the main paper on biological identification through DNA barcodes. It explores the application of machine learning, specifically transformer models, to analyze DNA barcodes, which directly relates to the identification of species based on genetic data. This combination creates a novel and useful approach to improving species identification and biodiversity assessment, addressing the challenges of taxonomic expertise collapse highlighted in the main paper.","1) Efficiently Solving Discounted MDPs with Predictions on Transition
  Matrices; We study infinite-horizon Discounted Markov Decision Processes (DMDPs) under
a generative model. Motivated by the Algorithm with Advice framework
Mitzenmacher and Vassilvitskii 2022, we propose a novel framework to
investigate how a prediction on the transition matrix can enhance the sample
efficiency in solving DMDPs and improve sample complexity bounds. We focus on
the DMDPs with $N$ state-action pairs and discounted factor $\gamma$. Firstly,
we provide an impossibility result that, without prior knowledge of the
prediction accuracy, no sampling policy can compute an $\epsilon$-optimal
policy with a sample complexity bound better than $\tilde{O}((1-\gamma)^{-3}
N\epsilon^{-2})$, which matches the state-of-the-art minimax sample complexity
bound with no prediction. In complement, we propose an algorithm based on
minimax optimization techniques that leverages the prediction on the transition
matrix. Our algorithm achieves a sample complexity bound depending on the
prediction error, and the bound is uniformly better than
$\tilde{O}((1-\gamma)^{-4} N \epsilon^{-2})$, the previous best result derived
from convex optimization methods. These theoretical findings are further
supported by our numerical experiments.; 2) Entanglement entropy evolution during gravitational collapse; We investigate the dynamics of the ground state entanglement entropy for a
discretized scalar field propagating within the Oppenheimer-Snyder collapse
metric. Starting from a well-controlled initial configuration, we follow the
system as it evolves toward the formation of a horizon and, eventually, a
singularity. Our approach employs an Ermakov-like equation to determine the
time-dependent ground state of the field and calculates the resulting
entanglement entropy by tracing out the degrees of freedom inside a spherical
region within the matter sphere. We find that the entanglement entropy exhibits
nontrivial scaling and time dependence during collapse. Close to the horizon,
the entropy can deviate from the simple area law, reflecting the rapid changes
in geometry and field configuration. Although the model is idealized, these
results provide insights into the generation and scaling of entanglement in the
presence of realistic, dynamically evolving gravitational fields.; 3) Quantum autoencoders for image classification; Classical machine learning often struggles with complex, high-dimensional
data. Quantum machine learning offers a potential solution, promising more
efficient processing. While the quantum convolutional neural network (QCNN), a
hybrid quantum-classical algorithm, is suitable for current noisy
intermediate-scale quantum-era hardware, its learning process relies heavily on
classical computation. Future large-scale, gate-based quantum computers could
unlock the full potential of quantum effects in machine learning. In contrast
to QCNNs, quantum autoencoders (QAEs) leverage classical optimization solely
for parameter tuning. Data compression and reconstruction are handled entirely
within quantum circuits, enabling purely quantum-based feature extraction. This
study introduces a novel image-classification approach using QAEs, achieving
classification without requiring additional qubits compared with conventional
QAE implementations. The quantum circuit structure significantly impacts
classification accuracy. Unlike hybrid methods such as QCNN, QAE-based
classification emphasizes quantum computation. Our experiments demonstrate high
accuracy in a four-class classification task, evaluating various quantum-gate
configurations to understand the impact of different parameterized quantum
circuit (ansatz) structures on classification performance. Our results reveal
that specific ansatz structures achieve superior accuracy, and we provide an
analysis of their effectiveness. Moreover, the proposed approach achieves
performance comparable to that of conventional machine-learning methods while
significantly reducing the number of parameters requiring optimization. These
findings indicate that QAEs can serve as efficient classification models with
fewer parameters and highlight the potential of utilizing quantum circuits for
complete end-to-end learning, a departure from hybrid approaches such as QCNN.; 4) Simultaneous bifurcation of limit cycles for Piecewise Holomorphic
  systems; Let $\dot{z}=f(z)$ be a holomorphic differential equation with center at $p$.
In this paper we are concerned about studying the piecewise perturbation
systems $\dot{z}=f(z)+\epsilon R^\pm(z,\overline{z}),$ where
$R^\pm(z,\overline{z})$ are complex polynomials defined for
$\pm\operatorname{Im}(z)> 0.$ We provide an integral expression, similar to an
Abelian integral, for the period annulus of $p.$ The zeros of this integral
control the bifurcating limit cycles from the periodic orbits of this annular
region. This expression is given in terms of the conformal conjugation between
$\dot{z}=f(z)$ and its linearization $\dot{z}=f'(p)z$ at $p$. We use this
result to control the simultaneous bifurcation of limit cycles of the two
annular periods of $\dot{z}={\rm i} (z^2-1)/2$, after both complex and
holomorphic piecewise polynomial perturbations. In particular, as far as we
know, we provide the first proof of the existence of non nested limit cycles
for piecewise holomorphic systems.; 5) Multiphoton fluorescence excitation with real intermediary states; We demonstrate fluorescence generation through a sequential multiphoton
process with real intermediary states. Our findings suggest new directions for
optical control of previously unexplored molecular excitation pathways.; 6) A Multi-tiered Solution for Personalized Baggage Item Recommendations
  using FastText and Association Rule Mining; This paper introduces an intelligent baggage item recommendation system to
optimize packing for air travelers by providing tailored suggestions based on
specific travel needs and destinations. Using FastText word embeddings and
Association Rule Mining (ARM), the system ensures efficient luggage space
utilization, compliance with weight limits, and an enhanced travel experience.
The methodology comprises four phases: (1) data collection and preprocessing
with pre-trained FastText embeddings for text representation and similarity
scoring (2) a content-based recommendation system enriched by user search
history (3) application of ARM to user interactions to uncover meaningful item
associations and (4) integration of FastText and ARM for accurate, personalized
recommendations. Performance is evaluated using metrics such as coverage,
support, confidence, lift, leverage, and conviction. Results demonstrate the
system's effectiveness in providing relevant suggestions, improving customer
satisfaction, and simplifying the packing process. These insights advance
personalized recommendations, targeted marketing, and product optimization in
air travel and beyond.; 7) Table as Thought: Exploring Structured Thoughts in LLM Reasoning; Large language models' reasoning abilities benefit from methods that organize
their thought processes, such as chain-of-thought prompting, which employs a
sequential structure to guide the reasoning process step-by-step. However,
existing approaches focus primarily on organizing the sequence of thoughts,
leaving structure in individual thought steps underexplored. To address this
gap, we propose Table as Thought, a framework inspired by cognitive
neuroscience theories on human thought. Table as Thought organizes reasoning
within a tabular schema, where rows represent sequential thought steps and
columns capture critical constraints and contextual information to enhance
reasoning. The reasoning process iteratively populates the table until
self-verification ensures completeness and correctness. Our experiments show
that Table as Thought excels in planning tasks and demonstrates a strong
potential for enhancing LLM performance in mathematical reasoning compared to
unstructured thought baselines. This work provides a novel exploration of
refining thought representation within LLMs, paving the way for advancements in
reasoning and AI cognition.; 8) Speaker Embedding Informed Audiovisual Active Speaker Detection for
  Egocentric Recordings; Audiovisual active speaker detection (ASD) addresses the task of determining
the speech activity of a candidate speaker given acoustic and visual data.
Typically, systems model the temporal correspondence of audiovisual cues, such
as the synchronisation between speech and lip movement. Recent work has
explored extending this paradigm by additionally leveraging speaker embeddings
extracted from candidate speaker reference speech. This paper proposes the
speaker comparison auxiliary network (SCAN) which uses speaker-specific
information from both reference speech and the candidate audio signal to
disambiguate challenging scenes when the visual signal is unresolvable.
Furthermore, an improved method for enrolling face-speaker libraries is
developed, which implements a self-supervised approach to video-based face
recognition. Fitting with the recent proliferation of wearable devices, this
work focuses on improving speaker-embedding-informed ASD in the context of
egocentric recordings, which can be characterised by acoustic noise and highly
dynamic scenes. SCAN is implemented with two well-established baselines, namely
TalkNet and Light-ASD; yielding a relative improvement in mAP of 14.5% and
10.3% on the Ego4D benchmark, respectively.; 9) Gradient-Based Multi-Objective Deep Learning: Algorithms, Theories,
  Applications, and Beyond; Multi-objective optimization (MOO) in deep learning aims to simultaneously
optimize multiple conflicting objectives, a challenge frequently encountered in
areas like multi-task learning and multi-criteria learning. Recent advancements
in gradient-based MOO methods have enabled the discovery of diverse types of
solutions, ranging from a single balanced solution to finite or even infinite
Pareto sets, tailored to user needs. These developments have broad applications
across domains such as reinforcement learning, computer vision, recommendation
systems, and large language models. This survey provides the first
comprehensive review of gradient-based MOO in deep learning, covering
algorithms, theories, and practical applications. By unifying various
approaches and identifying critical challenges, it serves as a foundational
resource for driving innovation in this evolving field. A comprehensive list of
MOO algorithms in deep learning is available at
https://github.com/Baijiong-Lin/Awesome-Multi-Objective-Deep-Learning.; 10) Stability of 2-class groups in the $\mathbb{Z}_2$-extension of certain
  real biquadratic fields; Greenberg's conjecture on the stability of $\ell$-class groups in the
cyclotomic $\mathbb{Z}_{\ell}$-extension of a real field has been proven for
various infinite families of real quadratic fields for the prime $\ell=2$. In
this work, we consider an infinite family of real biquadratic fields $K$. With
some extensive use of elementary group theoretic and class field theoretic
arguments, we investigate the $2$-class groups of the $n$-th layers $K_n$ of
the cyclotomic $\mathbb{Z}_2$-extension of $K$ and verify Greenberg's
conjecture. We also relate capitulation of ideal classes of certain
sub-extensions of $K_n$ to the relative sizes of the $2$-class groups.; 11) Posterior SBC: Simulation-Based Calibration Checking Conditional on Data; Simulation-based calibration checking (SBC) refers to the validation of an
inference algorithm and model implementation through repeated inference on data
simulated from a generative model. In the original and commonly used approach,
the generative model uses parameters drawn from the prior, and thus the
approach is testing whether the inference works for simulated data generated
with parameter values plausible under that prior. This approach is natural and
desirable when we want to test whether the inference works for a wide range of
datasets we might observe. However, after observing data, we are interested in
answering whether the inference works conditional on that particular data. In
this paper, we propose posterior SBC and demonstrate how it can be used to
validate the inference conditionally on observed data. We illustrate the
utility of posterior SBC in three case studies: (1) A simple multilevel model;
(2) a model that is governed by differential equations; and (3) a joint
integrative neuroscience model which is approximated via amortized Bayesian
inference with neural networks.; 12) AgroLLM: Connecting Farmers and Agricultural Practices through Large
  Language Models for Enhanced Knowledge Transfer and Practical Application; AgroLLM is an AI-powered chatbot designed to enhance knowledge-sharing and
education in agriculture using Large Language Models (LLMs) and a
Retrieval-Augmented Generation (RAG) framework. By using a comprehensive
open-source agricultural database, AgroLLM provides accurate, contextually
relevant responses while reducing incorrect information retrieval. The system
utilizes the FAISS vector database for efficient similarity searches, ensuring
rapid access to agricultural knowledge. A comparative study of three advanced
models: Gemini 1.5 Flash, ChatGPT-4o Mini, and Mistral-7B-Instruct-v0.2 was
conducted to evaluate performance across four key agricultural domains:
Agriculture and Life Sciences, Agricultural Management, Agriculture and
Forestry, and Agriculture Business. Key evaluation metrics included embedding
quality, search efficiency, and response relevance. Results indicated that
ChatGPT-4o Mini with RAG achieved the highest accuracy at 93%. Continuous
feedback mechanisms enhance response quality, making AgroLLM a benchmark
AI-driven educational tool for farmers, researchers, and professionals,
promoting informed decision-making and improved agricultural practices.; 13) Erasing Without Remembering: Safeguarding Knowledge Forgetting in Large
  Language Models; In this paper, we explore machine unlearning from a novel dimension, by
studying how to safeguard model unlearning in large language models (LLMs). Our
goal is to prevent unlearned models from recalling any related memory of the
targeted knowledge.We begin by uncovering a surprisingly simple yet overlooked
fact: existing methods typically erase only the exact expressions of the
targeted knowledge, leaving paraphrased or related information intact. To
rigorously measure such oversights, we introduce UGBench, the first benchmark
tailored for evaluating the generalisation performance across 13
state-of-the-art methods.UGBench reveals that unlearned models can still recall
paraphrased answers and retain target facts in intermediate layers. To address
this, we propose PERMU, a perturbation-based method that significantly enhances
the generalisation capabilities for safeguarding LLM unlearning.Experiments
demonstrate that PERMU delivers up to a 50.13% improvement in unlearning while
maintaining a 43.53% boost in robust generalisation. Our code can be found in
https://github.com/MaybeLizzy/UGBench.; 14) Feather-SQL: A Lightweight NL2SQL Framework with Dual-Model
  Collaboration Paradigm for Small Language Models; Natural Language to SQL (NL2SQL) has seen significant advancements with large
language models (LLMs). However, these models often depend on closed-source
systems and high computational resources, posing challenges in data privacy and
deployment. In contrast, small language models (SLMs) struggle with NL2SQL
tasks, exhibiting poor performance and incompatibility with existing
frameworks. To address these issues, we introduce Feather-SQL, a new
lightweight framework tailored for SLMs. Feather-SQL improves SQL executability
and accuracy through 1) schema pruning and linking, 2) multi-path and
multi-candidate generation. Additionally, we introduce the 1+1 Model
Collaboration Paradigm, which pairs a strong general-purpose chat model with a
fine-tuned SQL specialist, combining strong analytical reasoning with
high-precision SQL generation. Experimental results on BIRD demonstrate that
Feather-SQL improves NL2SQL performance on SLMs, with around 10% boost for
models without fine-tuning. The proposed paradigm raises the accuracy ceiling
of SLMs to 54.76%, highlighting its effectiveness.; 15) Key Historical Experiments in Hadron Physics; The experimental observations that led to the quark structure of matter and
the development of hadron physics are reviewed with emphasis on the discoveries
of mesons and baryons, starting in the 1940s with the pion and kaon which
mediate the strong hadronic force. The evidence for an internal structure of
the hadrons consisting of two or three elementary spin 1/2 particles is
reviewed. The discoveries of hadrons made of the heavier charm and bottom
quarks are described. In 2003 more complex multi-quark hadrons began to emerge.
The subsequent developments beyond the early 2000s are covered in the Review of
Particle Physics (Phys. Rev. D 110 (2024) 030001). Given the very large number
of observed hadrons, the choice of key experiments is somewhat subjective.; 16) 2DMCG:2DMambawith Change Flow Guidance for Change Detection in Remote
  Sensing; Remote sensing change detection (CD) has made significant advancements with
the adoption of Convolutional Neural Networks (CNNs) and Transformers. While
CNNs offer powerful feature extraction, they are constrained by receptive field
limitations, and Transformers suffer from quadratic complexity when processing
long sequences, restricting scalability. The Mamba architecture provides an
appealing alternative, offering linear complexity and high parallelism.
However, its inherent 1D processing structure causes a loss of spatial
information in 2D vision tasks. This paper addresses this limitation by
proposing an efficient framework based on a Vision Mamba variant that enhances
its ability to capture 2D spatial information while maintaining the linear
complexity characteristic of Mamba. The framework employs a 2DMamba encoder to
effectively learn global spatial contextual information from multi-temporal
images. For feature fusion, we introduce a 2D scan-based, channel-parallel
scanning strategy combined with a spatio-temporal feature fusion method, which
adeptly captures both local and global change information, alleviating spatial
discontinuity issues during fusion. In the decoding stage, we present a feature
change flow-based decoding method that improves the mapping of feature change
information from low-resolution to high-resolution feature maps, mitigating
feature shift and misalignment. Extensive experiments on benchmark datasets
such as LEVIR-CD+ and WHU-CD demonstrate the superior performance of our
framework compared to state-of-the-art methods, showcasing the potential of
Vision Mamba for efficient and accurate remote sensing change detection.; 17) Study of long-term spectral evolution and X-ray and Gamma-ray
  correlation of blazars seen by HAWC; The HAWC Observatory collected 6 years of extensive data, providing an ideal
platform for long-term monitoring of blazars in the Very High Energy (VHE)
band, without bias towards specific flux states. HAWC continuously monitors
blazar activity at TeV energies, focusing on sources with a redshift of {z \lt
0.3}, based on the Third Fermi-LAT Catalog of High-Energy sources. We
specifically focused our analysis on Mrk 421 and Mrk 501, as they are the
brightest blazars observed by the HAWC Observatory. With a dataset of 2143
days, this work significantly extends the monitoring previously published,
which was based on 511 days of observation. By utilizing HAWC data for the VHE
{\gamma}-ray emission in the 300 GeV to 100 TeV energy range, in conjunction
with Swift-XRT data for the 0.3 to 10 keV X-ray emission, we aim to explore
potential correlations between these two bands. For Mrk 501, we found evidence
of a long-term correlation. Additionally, we identified a period in the light
curve where the flux was very low for more than two years. On the other hand,
our analysis of Mrk 421 measured a strong linear correlation for
quasi-simultaneous observations collected by HAWC and Swift-XRT. This result is
consistent with a linear dependence and a multiple-zone synchrotron
self-Compton model to explain the X-ray and the {\gamma}-ray emission. Finally,
as suggested by previous findings, we confirm a harder-when-brighter behavior
in the spectral evolution of the flux properties for Mrk 421. These findings
contribute to the understanding of blazar emissions and their underlying
mechanisms.; 18) RAGO: Systematic Performance Optimization for Retrieval-Augmented
  Generation Serving; Retrieval-augmented generation (RAG), which combines large language models
(LLMs) with retrievals from external knowledge databases, is emerging as a
popular approach for reliable LLM serving. However, efficient RAG serving
remains an open challenge due to the rapid emergence of many RAG variants and
the substantial differences in workload characteristics across them. In this
paper, we make three fundamental contributions to advancing RAG serving. First,
we introduce RAGSchema, a structured abstraction that captures the wide range
of RAG algorithms, serving as a foundation for performance optimization.
Second, we analyze several representative RAG workloads with distinct
RAGSchema, revealing significant performance variability across these
workloads. Third, to address this variability and meet diverse performance
requirements, we propose RAGO (Retrieval-Augmented Generation Optimizer), a
system optimization framework for efficient RAG serving. Our evaluation shows
that RAGO achieves up to a 2x increase in QPS per chip and a 55% reduction in
time-to-first-token latency compared to RAG systems built on LLM-system
extensions.; 19) De Finetti's problem with fixed transaction costs and regime switching; In this paper, we examine a modified version of de Finetti's optimal dividend
problem, incorporating fixed transaction costs and altering the surplus process
by introducing two-valued drift and two-valued volatility coefficients. This
modification aims to capture the transitions or adjustments in the company's
financial status. We identify the optimal dividend strategy, which maximizes
the expected total net dividend payments (after accounting for transaction
costs) until ruin, as a two-barrier impulsive dividend strategy. Notably, the
optimal strategy can be explicitly determined for almost all scenarios
involving different drifts and volatility coefficients. Our primary focus is on
exploring how changes in drift and volatility coefficients influence the
optimal dividend strategy.; 20) A Comprehensive Experimentation Framework for Energy-Efficient Design of
  Cloud-Native Applications; Current approaches to designing energy-efficient applications typically rely
on measuring individual components using readily available local metrics, like
CPU utilization. However, these metrics fall short when applied to cloud-native
applications, which operate within the multi-tenant, shared environments of
distributed cloud providers. Assessing and optimizing the energy efficiency of
cloud-native applications requires consideration of the complex, layered nature
of modern cloud stacks.
  To address this need, we present a comprehensive, automated, and extensible
experimentation framework that enables developers to measure energy efficiency
across all relevant layers of a cloud-based application and evaluate associated
quality trade-offs. Our framework integrates a suite of service quality and
sustainability metrics, providing compatibility with any Kubernetes-based
application. We demonstrate the feasibility and effectiveness of this approach
through initial experimental results, comparing architectural design
alternatives for a widely used open-source cloud-native application.; 21) ACT-JEPA: Joint-Embedding Predictive Architecture Improves Policy
  Representation Learning; Learning efficient representations for decision-making policies is a
challenge in imitation learning (IL). Current IL methods require expert
demonstrations, which are expensive to collect. Consequently, they often have
underdeveloped world models. Self-supervised learning (SSL) offers an
alternative by allowing models to learn from diverse, unlabeled data, including
failures. However, SSL methods often operate in raw input space, making them
inefficient. In this work, we propose ACT-JEPA, a novel architecture that
integrates IL and SSL to enhance policy representations. We train a policy to
predict (1) action sequences and (2) abstract observation sequences. The first
objective uses action chunking to improve action prediction and reduce
compounding errors. The second objective extends this idea of chunking by
predicting abstract observation sequences. We utilize Joint-Embedding
Predictive Architecture to predict in abstract representation space, allowing
the model to filter out irrelevant details, improve efficiency, and develop a
robust world model. Our experiments show that ACT-JEPA improves the quality of
representations by learning temporal environment dynamics. Additionally, the
model's ability to predict abstract observation sequences results in
representations that effectively generalize to action sequence prediction.
ACT-JEPA performs on par with established baselines across a range of
decision-making tasks.; 22) The cross-over from viscous to inertial lengthscales in rapidly-rotating
  convection; Convection is the main heat transport mechanism in the Earth's liquid core
and is thought to power the dynamo that generates the geomagnetic field. Core
convection is strongly constrained by rotation while being turbulent. Given the
difficulty in modelling these conditions, some key properties of core
convection are still debated, including the dominant energy-carrying
lengthscale. Different regimes of rapidly-rotating, unmagnetised, turbulent
convection exist depending on the importance of viscous and inertial forces in
the dynamics, and hence different theoretical predictions for the dominant flow
lengthscale have been proposed. Here we study the transition from
viscously-dominated to inertia-dominated regimes using numerical simulations in
spherical and planar geometries. We find that the cross-over occurs when the
inertial lengthscale approximately equals the viscous lengthscale. This
suggests that core convection in the absence of magnetic fields is dominated by
the inertial scale, which is hundred times larger than the viscous scale.; 23) Efficient space-time discretizations for tracking the boundaries of
  reachable sets; The reachable sets of nonlinear control systems can in general only be
numerically approximated, and are often very expensive to calculate. In this
paper, we propose an algorithm that tracks only the boundaries of the reachable
sets and that chooses the temporal and spatial discretizations in a non-uniform
way to reduce the computational complexity.; 24) TGV: Tabular Data-Guided Learning of Visual Cardiac Representations; Contrastive learning methods in computer vision typically rely on different
views of the same image to form pairs. However, in medical imaging, we often
seek to compare entire patients with different phenotypes rather than just
multiple augmentations of one scan. We propose harnessing clinically relevant
tabular data to identify distinct patient phenotypes and form more meaningful
pairs in a contrastive learning framework. Our method uses tabular attributes
to guide the training of visual representations, without requiring a joint
embedding space. We demonstrate its strength using short-axis cardiac MR images
and clinical attributes from the UK Biobank, where tabular data helps to more
effectively distinguish between patient subgroups. Evaluation on downstream
tasks, including fine-tuning and zero-shot prediction of cardiovascular artery
diseases and cardiac phenotypes, shows that incorporating tabular data yields
stronger visual representations than conventional methods that rely solely on
image augmentations or combined image-tabular embeddings. Furthermore, we
demonstrate that image encoders trained with tabular guidance are capable of
embedding demographic information in their representations, allowing them to
use insights from tabular data for unimodal predictions, making them
well-suited to real-world medical settings where extensive clinical annotations
may not be routinely available at inference time. The code will be available on
GitHub.; 25) Dislocation correlations in GaN epitaxial films revealed by EBSD and XRD; Correlations between dislocations in crystals reduce the elastic energy via
screening of the strain by the surrounding dislocations. We study the
correlations of threading dislocations in GaN epitaxial films with dislocation
densities of 5x10^8 cm^-2 and 1.8x10^10 cm^-2 by X-ray diffraction (XRD) in
reciprocal space and by high-resolution electron backscatter diffraction (EBSD)
in real space, where the strain is derived from a cross-correlation analysis of
the Kikuchi patterns. The measured XRD curves and EBSD strain and rotation maps
are compared with Monte Carlo simulations within one and the same model for the
dislocation distributions. The screening of the dislocation strains is provided
by creating pairs of dislocations with opposite Burgers vectors, with the mean
distance between dislocations in a pair equal to the screening distance. The
pairs overlap and cannot be distinguished as separate dipoles. The
EBSD-measured autocorrelation functions of the strain and rotation components
follow the expected logarithmic law for distances smaller than the screening
distances and become zero for larger distances, which is confirmed by the Monte
Carlo simulations. Screening distances of 2 \textmu m and 0.3 \textmu m are
obtained for the samples with low and high dislocation densities, respectively.
The dislocation strain is thus screened by only 4 neighboring dislocations.
High-resolution EBSD allows for a more precise determination of the screening
distances than from fits of the XRD curves. In addition, an anisotropic
resolution of the EBSD measurements is observed and quantified.; 26) Leveraging Large Language Models For Scalable Vector Graphics
  Processing: A Review; In recent years, rapid advances in computer vision have significantly
improved the processing and generation of raster images. However, vector
graphics, which is essential in digital design, due to its scalability and ease
of editing, have been relatively understudied. Traditional vectorization
techniques, which are often used in vector generation, suffer from long
processing times and excessive output complexity, limiting their usability in
practical applications. The advent of large language models (LLMs) has opened
new possibilities for the generation, editing, and analysis of vector graphics,
particularly in the SVG format, which is inherently text-based and well-suited
for integration with LLMs.
  This paper provides a systematic review of existing LLM-based approaches for
SVG processing, categorizing them into three main tasks: generation, editing,
and understanding. We observe notable models such as IconShop, StrokeNUWA, and
StarVector, highlighting their strengths and limitations. Furthermore, we
analyze benchmark datasets designed for assessing SVG-related tasks, including
SVGEditBench, VGBench, and SGP-Bench, and conduct a series of experiments to
evaluate various LLMs in these domains. Our results demonstrate that for vector
graphics reasoning-enhanced models outperform standard LLMs, particularly in
generation and understanding tasks. Furthermore, our findings underscore the
need to develop more diverse and richly annotated datasets to further improve
LLM capabilities in vector graphics tasks.; 27) Mathematical reasoning and the computer; Computers have already changed the way that humans do mathematics: they
enable us to compute efficiently. But will they soon be helping us to reason?
And will they one day start reasoning themselves? We give an overview of recent
developments in neural networks, computer theorem provers and large language
models.; 28) General Table Question Answering via Answer-Formula Joint Generation; Advanced table question answering (TableQA) methods prompt large language
models (LLMs) to generate answer text, SQL query, Python code, or custom
operations, which impressively improve the complex reasoning problems in the
TableQA task. However, these methods lack the versatility to cope with specific
question types or table structures. In contrast, the Spreadsheet Formula, the
widely-used and well-defined operation language for tabular data, has not been
thoroughly explored to solve TableQA. In this paper, we first attempt to use
Formula as the logical form for solving complex reasoning on the tables with
different structures. Specifically, we construct a large Formula-annotated
TableQA dataset \texttt{FromulaQA} from existing datasets. In addition, we
propose \texttt{TabAF}, a general table answering framework to solve multiple
types of tasks over multiple types of tables simultaneously. Unlike existing
methods, \texttt{TabAF} decodes answers and Formulas with a single LLM
backbone, demonstrating great versatility and generalization. \texttt{TabAF}
based on Llama3.1-70B achieves new state-of-the-art performance on the
WikiTableQuestion, HiTab and TabFact.; 29) SWIFT: Mapping Sub-series with Wavelet Decomposition Improves Time
  Series Forecasting; In recent work on time-series prediction, Transformers and even large
language models have garnered significant attention due to their strong
capabilities in sequence modeling. However, in practical deployments,
time-series prediction often requires operation in resource-constrained
environments, such as edge devices, which are unable to handle the
computational overhead of large models. To address such scenarios, some
lightweight models have been proposed, but they exhibit poor performance on
non-stationary sequences. In this paper, we propose $\textit{SWIFT}$, a
lightweight model that is not only powerful, but also efficient in deployment
and inference for Long-term Time Series Forecasting (LTSF). Our model is based
on three key points: (i) Utilizing wavelet transform to perform lossless
downsampling of time series. (ii) Achieving cross-band information fusion with
a learnable filter. (iii) Using only one shared linear layer or one shallow MLP
for sub-series' mapping. We conduct comprehensive experiments, and the results
show that $\textit{SWIFT}$ achieves state-of-the-art (SOTA) performance on
multiple datasets, offering a promising method for edge computing and
deployment in this task. Moreover, it is noteworthy that the number of
parameters in $\textit{SWIFT-Linear}$ is only 25\% of what it would be with a
single-layer linear model for time-domain prediction. Our code is available at
https://github.com/LancelotXWX/SWIFT.; 30) Generative Trajectory Stitching through Diffusion Composition; Effective trajectory stitching for long-horizon planning is a significant
challenge in robotic decision-making. While diffusion models have shown promise
in planning, they are limited to solving tasks similar to those seen in their
training data. We propose CompDiffuser, a novel generative approach that can
solve new tasks by learning to compositionally stitch together shorter
trajectory chunks from previously seen tasks. Our key insight is modeling the
trajectory distribution by subdividing it into overlapping chunks and learning
their conditional relationships through a single bidirectional diffusion model.
This allows information to propagate between segments during generation,
ensuring physically consistent connections. We conduct experiments on benchmark
tasks of various difficulties, covering different environment sizes, agent
state dimension, trajectory types, training data quality, and show that
CompDiffuser significantly outperforms existing methods.; 31) Integrated Multiphysics Modeling of a Piezoelectric Micropump; This paper presents an integrated multiphysics simulation approach of
piezoelectric micropumps. Micropumps and micro blowers are essential devices in
various cutting-edge industries like laboratory equipment, medical devices, and
fuel cells. A piezoelectric micropump involves complex physics including
microfluidics, flow-structure interaction, electricity, and piezoelectric
material. Hence, a comprehensive analysis of the interactions between different
physical phenomena, would be essential for the effective design and
optimization of these micropumps. Prior studies on piezoelectric micropump were
mainly focused on isolated physical aspects of these pumps, such as
piezoelectric mechanics, fluid dynamics, electrical properties, and also
fluid-structure Interactions. The present paper fills this gap by integrating
these aspects into a holistic simulation and design approach, introducing a new
methodology for micropump analysis. Advanced simulation and design tools like
COMSOL and SolidWorks were employed in accordance. A brief review of
piezoelectric materials, and an exploration of different types of micropumps
and their operating principles is discussed. Also, a comparison of various
piezoelectric materials, including their properties and applications is
investigated. Further, the paper discusses the simulation process of the
micropumps, using COMSOL software, and presents an in-depth analysis of the
simulation results. This structured approach provides a comprehensive
understanding of piezoelectric micropumps, from theoretical underpinnings to
practical design considerations. ..; 32) DDAT: Diffusion Policies Enforcing Dynamically Admissible Robot
  Trajectories; Diffusion models excel at creating images and videos thanks to their
multimodal generative capabilities. These same capabilities have made diffusion
models increasingly popular in robotics research, where they are used for
generating robot motion. However, the stochastic nature of diffusion models is
fundamentally at odds with the precise dynamical equations describing the
feasible motion of robots. Hence, generating dynamically admissible robot
trajectories is a challenge for diffusion models. To alleviate this issue, we
introduce DDAT: Diffusion policies for Dynamically Admissible Trajectories to
generate provably admissible trajectories of black-box robotic systems using
diffusion models. A sequence of states is a dynamically admissible trajectory
if each state of the sequence belongs to the reachable set of its predecessor
by the robot's equations of motion. To generate such trajectories, our
diffusion policies project their predictions onto a dynamically admissible
manifold during both training and inference to align the objective of the
denoiser neural network with the dynamical admissibility constraint. The
auto-regressive nature of these projections along with the black-box nature of
robot dynamics render these projections immensely challenging. We thus enforce
admissibility by iteratively sampling a polytopic under-approximation of the
reachable set of a state onto which we project its predicted successor, before
iterating this process with the projected successor. By producing accurate
trajectories, this projection eliminates the need for diffusion models to
continually replan, enabling one-shot long-horizon trajectory planning. We
demonstrate that our framework generates higher quality dynamically admissible
robot trajectories through extensive simulations on a quadcopter and various
MuJoCo environments, along with real-world experiments on a Unitree GO1 and
GO2.; 33) A hybrid framework integrating classical computers and quantum annealers
  for optimisation of truss structures; This work proposes a hybrid framework combining classical computers with
quantum annealers for structural optimisation. At each optimisation iteration
of an iterative process, two minimisation problems are formulated one for the
underlying mechanical boundary value problem through the minimisation potential
energy principle and one for the minimisation problem to update the design
variables. Our hybrid approach leverages the strength of quantum computing to
solve these two minimisation problems at each step, thanks to the developed
quantum annealing-assisted sequential programming strategy introduced in
[Nguyen, Wu, Remacle, and Noels. A quantum annealing-sequential quadratic
programming assisted finite element simulation for non-linear and
history-dependent mechanical problems. European Journal of Mechanics-A/Solids
105 (2024): 105254]. The applicability of the proposed framework is
demonstrated through several case studies of truss optimisation, highlighting
its capability to perform optimisation with quantum computers. The proposed
framework offers a promising direction for future structural optimisation
applications, particularly in scenarios where the quantum computer could
resolve the size limitations of the classical computers due to problem
complexities.; 34) Generation of reusable learning objects from digital medical
  collections: An analysis based on the MASMDOA framework; Learning Objects represent a widespread approach to structuring instructional
materials in a large variety of educational contexts. The main aim of this work
consists of analyzing from a qualitative point of view the process of
generating reusable learning objects (RLOs) followed by Clavy, a tool that can
be used to retrieve data from multiple medical knowledge sources and
reconfigure such sources in diverse multimedia-based structures and
organizations. From these organizations, Clavy is able to generate learning
objects which can be adapted to various instructional healthcare scenarios with
several types of user profiles and distinct learning requirements. Moreover,
Clavy provides the capability of exporting these learning objects through
educational standard specifications, which improves their reusability features.
The analysis insights highlight the importance of having a tool able to
transfer knowledge from the available digital medical collections to learning
objects that can be easily accessed by medical students and healthcare
practitioners through the most popular e-learning platforms.; 35) Can LLM Agents Maintain a Persona in Discourse?; Large Language Models (LLMs) are widely used as conversational agents,
exploiting their capabilities in various sectors such as education, law,
medicine, and more. However, LLMs are often subjected to context-shifting
behaviour, resulting in a lack of consistent and interpretable
personality-aligned interactions. Adherence to psychological traits lacks
comprehensive analysis, especially in the case of dyadic (pairwise)
conversations. We examine this challenge from two viewpoints, initially using
two conversation agents to generate a discourse on a certain topic with an
assigned personality from the OCEAN framework (Openness, Conscientiousness,
Extraversion, Agreeableness, and Neuroticism) as High/Low for each trait. This
is followed by using multiple judge agents to infer the original traits
assigned to explore prediction consistency, inter-model agreement, and
alignment with the assigned personality. Our findings indicate that while LLMs
can be guided toward personality-driven dialogue, their ability to maintain
personality traits varies significantly depending on the combination of models
and discourse settings. These inconsistencies emphasise the challenges in
achieving stable and interpretable personality-aligned interactions in LLMs.; 36) Deep End-to-End Posterior ENergy (DEEPEN) for image recovery; Current end-to-end (E2E) and plug-and-play (PnP) image reconstruction
algorithms approximate the maximum a posteriori (MAP) estimate but cannot offer
sampling from the posterior distribution, like diffusion models. By contrast,
it is challenging for diffusion models to be trained in an E2E fashion. This
paper introduces a Deep End-to-End Posterior ENergy (DEEPEN) framework, which
enables MAP estimation as well as sampling. We learn the parameters of the
posterior, which is the sum of the data consistency error and the negative
log-prior distribution, using maximum likelihood optimization in an E2E
fashion. The proposed approach does not require algorithm unrolling, and hence
has a smaller computational and memory footprint than current E2E methods,
while it does not require contraction constraints typically needed by current
PnP methods. Our results demonstrate that DEEPEN offers improved performance
than current E2E and PnP models in the MAP setting, while it also offers faster
sampling compared to diffusion models. In addition, the learned energy-based
model is observed to be more robust to changes in image acquisition settings.; 37) A Law Reasoning Benchmark for LLM with Tree-Organized Structures
  including Factum Probandum, Evidence and Experiences; While progress has been made in legal applications, law reasoning, crucial
for fair adjudication, remains unexplored. We propose a transparent law
reasoning schema enriched with hierarchical factum probandum, evidence, and
implicit experience, enabling public scrutiny and preventing bias. Inspired by
this schema, we introduce the challenging task, which takes a textual case
description and outputs a hierarchical structure justifying the final decision.
We also create the first crowd-sourced dataset for this task, enabling
comprehensive evaluation. Simultaneously, we propose an agent framework that
employs a comprehensive suite of legal analysis tools to address the challenge
task. This benchmark paves the way for transparent and accountable AI-assisted
law reasoning in the ``Intelligent Court''.; 38) External Large Foundation Model: How to Efficiently Serve Trillions of
  Parameters for Online Ads Recommendation; Ads recommendation is a prominent service of online advertising systems and
has been actively studied. Recent studies indicate that scaling-up and advanced
design of the recommendation model can bring significant performance
improvement. However, with a larger model scale, such prior studies have a
significantly increasing gap from industry as they often neglect two
fundamental challenges in industrial-scale applications. First, training and
inference budgets are restricted for the model to be served, exceeding which
may incur latency and impair user experience. Second, large-volume data arrive
in a streaming mode with data distributions dynamically shifting, as new
users/ads join and existing users/ads leave the system. We propose the External
Large Foundation Model (ExFM) framework to address the overlooked challenges.
Specifically, we develop external distillation and a data augmentation system
(DAS) to control the computational cost of training/inference while maintaining
high performance. We design the teacher in a way like a foundation model (FM)
that can serve multiple students as vertical models (VMs) to amortize its
building cost. We propose Auxiliary Head and Student Adapter to mitigate the
data distribution gap between FM and VMs caused by the streaming data issue.
Comprehensive experiments on internal industrial-scale applications and public
datasets demonstrate significant performance gain by ExFM.; 39) Towards an AI co-scientist; Scientific discovery relies on scientists generating novel hypotheses that
undergo rigorous experimental validation. To augment this process, we introduce
an AI co-scientist, a multi-agent system built on Gemini 2.0. The AI
co-scientist is intended to help uncover new, original knowledge and to
formulate demonstrably novel research hypotheses and proposals, building upon
prior evidence and aligned to scientist-provided research objectives and
guidance. The system's design incorporates a generate, debate, and evolve
approach to hypothesis generation, inspired by the scientific method and
accelerated by scaling test-time compute. Key contributions include: (1) a
multi-agent architecture with an asynchronous task execution framework for
flexible compute scaling; (2) a tournament evolution process for self-improving
hypotheses generation. Automated evaluations show continued benefits of
test-time compute, improving hypothesis quality. While general purpose, we
focus development and validation in three biomedical areas: drug repurposing,
novel target discovery, and explaining mechanisms of bacterial evolution and
anti-microbial resistance. For drug repurposing, the system proposes candidates
with promising validation findings, including candidates for acute myeloid
leukemia that show tumor inhibition in vitro at clinically applicable
concentrations. For novel target discovery, the AI co-scientist proposed new
epigenetic targets for liver fibrosis, validated by anti-fibrotic activity and
liver cell regeneration in human hepatic organoids. Finally, the AI
co-scientist recapitulated unpublished experimental results via a parallel in
silico discovery of a novel gene transfer mechanism in bacterial evolution.
These results, detailed in separate, co-timed reports, demonstrate the
potential to augment biomedical and scientific discovery and usher an era of AI
empowered scientists.; 40) Connecting a Magnetized Disk to a Convective Low-mass Protostar: A
  Global Three-dimensional Model of Boundary Layer Accretion; In the early stages of star formation, boundary layer accretion, where
protostars accrete material from disks extending down to their surfaces, plays
a crucial role. Understanding how a magneto-rotational-instability (MRI)-active
disk connects to a protostar's surface remains a significant challenge. To
investigate the mechanisms of mass and angular momentum transfer, we develop a
global, three-dimensional magnetohydrodynamic model of boundary layer accretion
around a magnetized, convective low-mass protostar. Our results reveal that
angular momentum transport mechanisms transition significantly from the outer
MRI-active disk to the protostellar surface. Various mechanisms--MRI, spiral
shocks, coronal accretion, jets, and disk winds--contribute to angular momentum
transfer, resulting in three distinct disk structures: (1) the MRI-active disk,
(2) the transition layer, and (3) the boundary layer. The simulated protostar
is strongly magnetized due to the accumulation of the disk fields, wrapping by
disk toroidal fields, and stellar dynamo activity. Magnetic concentrations
analogous to starspots form on the protostar and interact with the rotating
disk gas to generate spiral shocks. These shocks play a key role in driving
accretion. These findings demonstrate the necessity of global MHD models for a
comprehensive understanding of angular momentum transport. Additionally, we
identify explosive events triggered by magnetic reconnection in both the
protostar and the disk atmosphere. We also find decretion flows in the disk
midplane, which may be important for the radial transport of refractory
materials, such as Calcium-Aluminium-rich Inclusions (CAIs) precursor gas, to
the outer disk.; 41) Habitable Worlds Formed at Cosmic Dawn; Primordial supernovae were the first, great nucleosynthetic engines in the
Universe, forging the elements required for the later formation of planets and
life. Here we show that planetesimals, the precursors of terrestrial planets,
formed around low-mass stars in the debris of the first cosmic explosions 200
Myr after the Big Bang, before the first galaxies and far earlier than
previously thought. A dense core in one of these explosions collapsed to a
protoplanetary disk in which several Earth masses of planetesimals formed 0.46
- 1.66 AU from their parent 0.7 M$_{\odot}$ star, where equilibrium
temperatures varied from 269 K to 186 K, in water mass fractions that were only
a factor of a few less than in the Solar System today. Habitable worlds thus
formed among the first generation of stars in the Universe, before the advent
of the first galaxies.; 42) Discrete Lagrangian multiforms for ABS equations I: quad equations; Discrete Lagrangian multiform theory is a variational perspective on lattice
equations that are integrable in the sense of multidimensional consistency. The
Lagrangian multiforms for the equations of the ABS classification formed the
start of this theory, but the Lagrangian multiforms that are usually considered
in this context produce equations that are slightly weaker than the ABS
equations. In this work, we present alternative Lagrangian multiforms that have
Euler-Lagrange equations equivalent to the ABS equations.
  In addition, the treatment of the ABS Lagrangian multiforms in the existing
literature fails to acknowledge that the complex functions in their definitions
have branch cuts. The choice of branch affects both the existence of an
additive three-leg form for the ABS equations and the closure property of the
Lagrangian multiforms. We give counterexamples for both these properties, but
we recover them by including integer-valued fields, related to possible the
branch choices, in the action sums.; 43) One-dimensional confined Rashba states in a two-dimensional
  Si$_{2}$Bi$_{2}$ induced by vacancy line defects; Advanced defect engineering techniques have enabled the creation of unique
quantum phases from pristine materials. One-dimensional (1D) atomic defects in
low-dimensional systems are particularly intriguing due to their distinct
quantum properties, such as 1D Rashba states that allow for the generation of
nondissipative spin currents, making them ideal for spintronic devices. Using
density-functional calculations and model-based symmetry analysis, we report
the emergence of 1D Rashba states in a two-dimensional Si$_{2}$Bi$_{2}$
monolayer (ML) with vacancy line defects (VLDs). We show that introducing VLDs
in the Si$_{2}$Bi$_{2}$ ML induces 1D confined defect states near the Fermi
level, which are strongly localized along the extended defect line. Notably, we
observed 1D Rashba spin-split bands in these defect states with significant
spin splitting originating mainly from the strong $p-p$ coupling orbitals
between Si and Bi atoms near the defect sites. These spin-split defect states
exhibit perfectly collinear spin polarization in momentum $\vec{k}$-space,
which is oriented perpendicularly to the VLD orientation. Moreover, using
$\vec{k}\cdot\vec{p}$ perturbation theory supplemented with symmetry analysis,
we show that the 1D Rashba states with collinear spin polarization are enforced
by the lowering of symmetry of the VLDs into the $C_{s}$ point group, which
retains the $M_{xz}$ mirror symmetry along with the 1D nature of the VLDs. The
observed 1D Rashba states in this system protect carriers against spin
decoherence and support an exceptionally long spin lifetime, which could be
promising for developing highly efficient spintronic devices.; 44) Symmetries and Anomalies of Hamiltonian Staggered Fermions; We review the shift and time reversal symmetries of Hamiltonian staggered
fermions and their connection to continuum symmetries concentrating in
particular on the case of massless fermions and (3+1) dimensions. We construct
operators using the staggered fields that implement these symmetries on finite
lattices. We show that the elementary shift symmetry of a single staggered
field depends on a $Z_4$ subgroup of an additional $U(1)$ phase symmetry and
anti-commutes with time reversal. This latter property implies that time
reversal symmetry will be broken if this phase symmetry is gauged - a mixed 't
Hooft anomaly. However, this anomaly can be canceled for multiples of four
staggered fields. Finally we observe that the naive continuum limit of the
minimal anomaly free lattice model has the symmetries and matter
representations of the Pati-Salam GUT.; 45) Causal Inference on Outcomes Learned from Text; We propose a machine-learning tool that yields causal inference on text in
randomized trials. Based on a simple econometric framework in which text may
capture outcomes of interest, our procedure addresses three questions: First,
is the text affected by the treatment? Second, which outcomes is the effect on?
And third, how complete is our description of causal effects? To answer all
three questions, our approach uses large language models (LLMs) that suggest
systematic differences across two groups of text documents and then provides
valid inference based on costly validation. Specifically, we highlight the need
for sample splitting to allow for statistical validation of LLM outputs, as
well as the need for human labeling to validate substantive claims about how
documents differ across groups. We illustrate the tool in a proof-of-concept
application using abstracts of academic manuscripts.; 46) Punctuation patterns in ""Finnegans Wake"" by James Joyce are largely
  translation-invariant; The complexity characteristics of texts written in natural languages are
significantly related to the rules of punctuation. In particular, the distances
between punctuation marks measured by the number of words quite universally
follow the family of Weibull distributions known from survival analyses.
However, the values of two parameters marking specific forms of these
distributions distinguish specific languages. This is such a strong constraint
that the punctuation distributions of texts translated from the original
language into another adopt quantitative characteristics of the target
language. All these changes take place within Weibull distributions such that
the corresponding hazard functions are always increasing. Recent previous
research shows that James Joyce's famous ""Finnegans Wake"" is subject to such
extreme distribution from the Weibull family that the corresponding hazard
function is clearly decreasing. At the same time, the distances of sentence
ending punctuation marks, determining the variability of sentence length, have
an almost perfect multifractal organization, so far to such an extent found
nowhere else in the literature. In the present contribution based on several
available translations (Dutch, French, German, Polish, Russian) of ""Finnegans
Wake"", it is shown that the punctuation characteristics of this work remain
largely translation invariant, contrary to the common cases. These observations
may constitute further evidence that ""Finnegans Wake"" is a translinguistic work
in this respect as well, in line with Joyce's original intention.; 47) BarcodeBERT: Transformers for Biodiversity Analysis; Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https://github.com/Kari-Genomics-Lab/BarcodeBERT; 48) Rolling Ahead Diffusion for Traffic Scene Simulation; Realistic driving simulation requires that NPCs not only mimic natural
driving behaviors but also react to the behavior of other simulated agents.
Recent developments in diffusion-based scenario generation focus on creating
diverse and realistic traffic scenarios by jointly modelling the motion of all
the agents in the scene. However, these traffic scenarios do not react when the
motion of agents deviates from their modelled trajectories. For example, the
ego-agent can be controlled by a stand along motion planner. To produce
reactive scenarios with joint scenario models, the model must regenerate the
scenario at each timestep based on new observations in a Model Predictive
Control (MPC) fashion. Although reactive, this method is time-consuming, as one
complete possible future for all NPCs is generated per simulation step.
Alternatively, one can utilize an autoregressive model (AR) to predict only the
immediate next-step future for all NPCs. Although faster, this method lacks the
capability for advanced planning. We present a rolling diffusion based traffic
scene generation model which mixes the benefits of both methods by predicting
the next step future and simultaneously predicting partially noised further
future steps at the same time. We show that such model is efficient compared to
diffusion model based AR, achieving a beneficial compromise between reactivity
and computational efficiency.; 49) $\phi$ meson in nuclear matter and atomic nuclei; The properties (masses and decay widths) of the $\phi$ meson are investigated
in nuclear matter from the $\phi$ meson self-energy, using the tree-level $\phi
K\bar{K}$ Lagrangian, and, incorporating in-medium masses of (anti)kaons
calculated within the quark meson coupling (QMC) model. These mass shifts and
decay widths are incorporated in the Breit-Wigner spectral function of the
$\phi$ meson to calculate the production cross-section of $\phi$ in asymmetric
nuclear matter. Considerable modifications to the production cross-section are
observed at normal nuclear matter density, driven by the in-medium mass
reduction and the increase in the decay width of $\phi$ meson. The potential
experienced by $\phi$ meson in nuclear matter is used to study the possibility
of formation of the $\phi$ mesic bound state with atomic nuclei. We explore the
potential formation of $\phi$-mesic bound states in ${\rm{^{4}He}}$,
${\rm{^{12}C}}$, ${\rm{^{16}O}}$, ${\rm{^{40}Ca}}$, ${\rm{^{90}Zr}}$,
${\rm{^{197}Au}}$ and ${\rm{^{208}Pb}}$ nuclei by investigating their binding
energies and absorption widths based on the corresponding $\phi$-nucleus
potentials. Our study shows shallow bound states with the light nuclei and
deeply bound states in heavy nuclei. Among the investigated nuclei, a
particularly distinct signal for a $\phi$-mesic bound state is identified in
${\rm{^{16}O}}$, suggesting its potential experimental observability. The work
provides valuable insights into $\phi$ meson interactions in infinite nuclear
matter and the potential formation of exotic $\phi$-mesic nuclear states,
offering promising probes for strongly interacting matter in the upcoming
experiments at J-PARC, JLab, and ${\rm{\bar{P}}ANDA}$@FAIR physics program.; 50) Efficient Interactive 3D Multi-Object Removal; Object removal is of great significance to 3D scene understanding, essential
for applications in content filtering and scene editing. Current mainstream
methods primarily focus on removing individual objects, with a few methods
dedicated to eliminating an entire area or all objects of a certain category.
They however confront the challenge of insufficient granularity and flexibility
for real-world applications, where users demand tailored excision and
preservation of objects within defined zones. In addition, most of the current
methods require kinds of priors when addressing multi-view inpainting, which is
time-consuming. To address these limitations, we propose an efficient and
user-friendly pipeline for 3D multi-object removal, enabling users to flexibly
select areas and define objects for removal or preservation. Concretely, to
ensure object consistency and correspondence across multiple views, we propose
a novel mask matching and refinement module, which integrates homography-based
warping with high-confidence anchor points for segmentation. By leveraging the
IoU joint shape context distance loss, we enhance the accuracy of warped masks
and improve subsequent inpainting processes. Considering the current immaturity
of 3D multi-object removal, we provide a new evaluation dataset to bridge the
developmental void. Experimental results demonstrate that our method
significantly reduces computational costs, achieving processing speeds more
than 80% faster than state-of-the-art methods while maintaining equivalent or
higher reconstruction quality.; 51) ""It felt more real"": Investigating the User Experience of the MiWaves
  Personalizing JITAI Pilot Study; Cannabis use among emerging adults is increasing globally, posing significant
health risks and creating a need for effective interventions. We present an
exploratory analysis of the MiWaves pilot study, a digital intervention aimed
at supporting cannabis use reduction among emerging adults (ages 18-25). Our
findings indicate the potential of self-monitoring check-ins and trend
visualizations in fostering self-awareness and promoting behavioral reflection
in participants. MiWaves intervention message timing and frequency were also
generally well-received by the participants. The participants' perception of
effort were queried on intervention messages with different tasks, and our
findings suggest that messages with tasks like exploring links and typing in
responses are perceived as requiring more effort as compared to messages with
tasks involving reading and acknowledging. Finally, we discuss the findings and
limitations from this study and analysis, and their impact on informing future
iterations on MiWaves.; 52) Infinite State Model Checking by Learning Transitive Relations; We propose a new approach for proving safety of infinite state systems. It
extends the analyzed system by transitive relations until its diameter D
becomes finite, i.e., until constantly many steps suffice to cover all
reachable states, irrespective of the initial state. Then we can prove safety
by checking that no error state is reachable in D steps. To deduce transitive
relations, we use recurrence analysis. While recurrence analyses can usually
find conjunctive relations only, our approach also discovers disjunctive
relations by combining recurrence analysis with projections. An empirical
evaluation of the implementation of our approach in our tool LoAT shows that it
is highly competitive with the state of the art.; 53) Quantum contextuality of spin-1 massive particles; Contextuality is a fundamental property of quantum mechanics. Contrary to
entanglement, which can only exist in composite systems, contextuality is also
present for single entities. The case of a three-level system is of particular
interest because--in agreement with the Bell-Kochen-Specker theorem--it is the
simplest in which quantum contextuality is necessarily present. We verify that
the polarizations of spin-1 massive particles produced at collider experiments
indeed exhibit contextuality. To this purpose we consider $W$ gauge bosons
produced in top-quark decays, $J/\psi$ and $K^{*}(892)^0$ mesons created in
$B$-meson decays and $\phi$ mesons resulting from $\chi^0_c$ charmonium decays,
making use of the data collected and analyzed by the ATLAS, LHCb and BESIII
collaborations, respectively. The polarizations of all these four particles
show contextuality with a significance of more than $5\sigma$.; 54) Combinatorial Ricci Flow and Thurston's Triangulation Conjecture; Thurston's triangulation conjecture asserts that every hyperbolic 3-manifold
admits a geometric decomposition into ideal hyperbolic tetrahedra, a result
proven only for certain special 3-manifolds. This paper presents combinatorial
Ricci flow as a systematic and general approach to addressing Thurston's
triangulation conjecture, showing that the flow converges if and only if the
triangulation is geometric. First, we prove the rigidity of the most general
hyperbolic polyhedral 3-manifolds constructed by isometrically gluing partially
truncated and decorated hyperbolic tetrahedra, demonstrating that the metrics
are uniquely determined by cone angles modulo isometry and decoration changes.
Then, we demonstrate that combinatorial Ricci flow evolves polyhedral metrics
toward complete hyperbolic structures with geometric decompositions when
convergent. Conversely, the existence of a geometric triangulation guarantees
flow convergence.; 55) PromptArtisan: Multi-instruction Image Editing in Single Pass with
  Complete Attention Control; We present PromptArtisan, a groundbreaking approach to multi-instruction
image editing that achieves remarkable results in a single pass, eliminating
the need for time-consuming iterative refinement. Our method empowers users to
provide multiple editing instructions, each associated with a specific mask
within the image. This flexibility allows for complex edits involving mask
intersections or overlaps, enabling the realization of intricate and nuanced
image transformations. PromptArtisan leverages a pre-trained InstructPix2Pix
model in conjunction with a novel Complete Attention Control Mechanism (CACM).
This mechanism ensures precise adherence to user instructions, granting
fine-grained control over the editing process. Furthermore, our approach is
zero-shot, requiring no additional training, and boasts improved processing
complexity compared to traditional iterative methods. By seamlessly integrating
multi-instruction capabilities, single-pass efficiency, and complete attention
control, PromptArtisan unlocks new possibilities for creative and efficient
image editing workflows, catering to both novice and expert users alike.; 56) Deviance Detection and Regularity Sensitivity in Dissociated Neuronal
  Cultures; Understanding how neural networks process complex patterns of information is
crucial for advancing both neuroscience and artificial intelligence. To
investigate fundamental principles of neural computation, we studied
dissociated neuronal cultures, one of the most primitive living neural
networks, on high-resolution CMOS microelectrode arrays and tested whether the
dissociated culture exhibits regularity sensitivity beyond mere
stimulus-specific adaptation and deviance detection. In oddball electrical
stimulation paradigms, we confirmed that the neuronal culture produced mismatch
responses (MMRs) with true deviance detection beyond mere adaptation. These
MMRs were dependent on the N-methyl-D-aspartate (NMDA) receptors, similar to
mismatch negativity (MMN) in humans, which is known to have true deviance
detection properties. Crucially, we also showed sensitivity to the statistical
regularity of stimuli, a phenomenon previously observed only in intact brains:
the MMRs in a predictable, periodic sequence were smaller than those in a
commonly used sequence in which the appearance of the deviant stimulus was
random and unpredictable. These results challenge the traditional view that a
hierarchically structured neural network is required to process complex
temporal patterns, suggesting instead that deviant detection and regularity
sensitivity are inherent properties arising from the primitive neural network.
They also suggest new directions for the development of neuro-inspired
artificial intelligence systems, emphasizing the importance of incorporating
adaptive mechanisms and temporal dynamics in the design of neural networks.; 57) Maximal regularity estimates for the abstract Cauchy problems; In this work, we extend the Da Prato-Grisvard theory of maximal regularity
estimates for sectorial operators in interpolation spaces. Specifically, for
any generator $-A$ of an analytic semigroup on a Banach space $X$, we identify
the interpolation spaces between $X$ and the domain $D_{A}$ of $A$ in which the
part of $A$ satisfies certain maximal regularity estimates. We also establish
several new results concerning both homogeneous and inhomogeneous $L^1$-maximal
regularity estimates, extending and completing recent findings in the
literature. These results are motivated not only by applications to problems in
areas such as fluid mechanics but also by the intrinsic theoretical interest of
the subject. In particular, we address the optimal choice of data spaces for
the Cauchy problem associated with $A$, ensuring the existence of strong
solutions with global-in-time control of their derivatives. This control is
measured via the homogeneous parts of the interpolation norms in the spatial
variable and weighted Lebesgue norms over the time interval. Furthermore, we
characterize weighted $L^1$-estimates and establish their relationship with
unweighted estimates. Additionally, we reformulate the characterization
condition for $L^1$-maximal regularity due to Kalton and Portal in a priori
terms that do not rely on semigroup operators. Finally, we introduce a new
interpolation framework for $L^p$-maximal regularity estimates, where $p \in
(1, \infty)$, within interpolation spaces generated by non-classical
interpolation functors.; 58) Exploring Cosmological Implications of the Modified Raychaudhuri
  Equation in Non-Gravitating Vacuum Energy Theory; This article investigates the modified Raychaudhuri Equation (RE) in the
context of Non-Gravitating Vacuum Energy (NGVE) theory and its implications for
various cosmological characteristics. The equation is formulated based on the
NGVE framework, in which global scale invariance generates a unique geometry.
The newly developed geometry introduces a metric that is conformally connected
to the conventional metric, with the conformal factor dependent on scalar field
potentials. The cosmological study is carried out under the framework of a flat
Friedmann-Lema\^itre-Robertson-Walker (FLRW) universe. Assuming matter behaves
as an ideal fluid in the modified geometry, we formulate models for conditional
expansion, collapse, and steady state, governed by the scalar field ($\phi$).
In this context, the caustic solution and the focusing theorem are also
studied. Scalar field solutions for exponential and power-law scale factors are
also derived using NGVE theory's equations of motion. Finally, graphical
analysis is used to investigate the behavior of the interaction terms that
appear in the modified RE under these scale factors.; 59) Critical Mathematical Economics and the Model-theoretic Foundations of
  Controversies in Economic Policy; The aim of this article is to present elements and discuss the potential of a
research program at the intersection between mathematics and heterodox
economics, which we call Criticial Mathematical Economics (CME). We propose to
focus on the mathematical and model-theoretic foundations of controversies in
economic policy, and aim at providing an entrance to the literature and an
invitation to mathematicians that are potentially interested in such a project.
From our point of view, mathematics has been partly misused in mainstream
economics to justify `unregulated markets' before the financial crisis. We thus
identify two key parts of CME, which leads to a natural structure of this
article: The frst focusses on an analysis and critique of mathematical models
used in mainstream economics, like e.g. the Dynamic Stochastic General
Equilibrium (DSGE) in Macroeconomics and the so-called
""Sonnenschein-Mantel-Debreu""-Theorems. The aim of the second part is to improve
and extend heterodox models using ingredients from modern mathematics and
computer science, a method with strong relation to Complexity Economics. We
exemplify this idea by describing how methods from Non-Linear Dynamics have
been used in what could be called ""The Dynamical Systems approach to
Post-Keynesian Macroeconomics"", and also discuss (Pseudo-) Goodwin cycles and
possible Micro- and Mesofoundations. We conclude by giving an outlook in which
areas a collaboration between mathematicians and heterodox economists could be
most promising. The focus lies on the mathematical and model-theoretic
foundations of controversies in economic policy, and we discuss both existing
projects in such a direction as well as areas where new models for policy
advice are most needed from the perspective of the progressive political left.; 60) Discovery of Radio Recombination Lines from Proplyds in the Orion Nebula
  Cluster; We present new Atacama Large Millimeter/submillimeter Array observations
that, for the first time, detect hydrogen and helium radio recombination lines
from a protoplanetary disk. We imaged the Orion Nebula Cluster at 3.1 mm with a
spectral setup that covered the $n=42 \rightarrow 41$ transitions of hydrogen
(H41$\alpha$) and helium (He41$\alpha$). The unprecedented sensitivity of these
observations enables us to search for radio recombination lines toward the
positions of ${\sim}200$ protoplanetary disks. We detect H41$\alpha$ from 17
disks, all of which are HST-identified `proplyds.' The detected H41$\alpha$
emission is spatially coincident with the locations of proplyd ionization
fronts, indicating that proplyd H41$\alpha$ emission is produced by gas that
has been photoevaporated off the disk and ionized by UV radiation from massive
stars. We measure the fluxes and widths of the detected H41$\alpha$ lines and
find line fluxes of ${\sim}30-800$ mJy km s$^{-1}$ and line widths of
${\sim}30-90$ km s$^{-1}$. The derived line widths indicate that the broadening
of proplyd H41$\alpha$ emission is dominated by outflowing gas motions
associated with external photoevaporation. The derived line fluxes, when
compared with measurements of 3.1 mm free-free flux, imply that the ionization
fronts of H41$\alpha$-detected proplyds have electron temperatures of
${\sim}6,000-11,000$ K and electron densities of ${\sim}10^6-10^7$ cm$^{-3}$.
Finally, we detect He41$\alpha$ towards one H41$\alpha$-detected source and
find evidence that this system is helium-rich. Our study demonstrates that
radio recombination lines are readily detectable in ionized photoevaporating
disks, providing a new way to measure disk properties in clustered star-forming
regions.; 61) InstructAgent: Building User Controllable Recommender via LLM Agent; Traditional recommender systems usually take the user-platform paradigm,
where users are directly exposed under the control of the platform's
recommendation algorithms. However, the defect of recommendation algorithms may
put users in very vulnerable positions under this paradigm. First, many
sophisticated models are often designed with commercial objectives in mind,
focusing on the platform's benefits, which may hinder their ability to protect
and capture users' true interests. Second, these models are typically optimized
using data from all users, which may overlook individual user's preferences.
Due to these shortcomings, users may experience several disadvantages under the
traditional user-platform direct exposure paradigm, such as lack of control
over the recommender system, potential manipulation by the platform, echo
chamber effects, or lack of personalization for less active users due to the
dominance of active users during collaborative learning. Therefore, there is an
urgent need to develop a new paradigm to protect user interests and alleviate
these issues. Recently, some researchers have introduced LLM agents to simulate
user behaviors, these approaches primarily aim to optimize platform-side
performance, leaving core issues in recommender systems unresolved. To address
these limitations, we propose a new user-agent-platform paradigm, where agent
serves as the protective shield between user and recommender system that
enables indirect exposure. To this end, we first construct four recommendation
datasets, denoted as $\dataset$, along with user instructions for each record.; 62) Scalar fully-charm and bottom tetraquarks under extreme temperatures; Temperature dependences of masses and current couplings of the ground state
of the fully heavy tetraquarks $T_{4c}$ and $T_{4b}$, composed of charm $(c)$
and bottom $(b)$ quarks and antiquarks with spin-parities $J^{PC} = 0^{++}$ are
evaluated in the diquark antidiquark picture using Thermal QCD Sum Rules
including vacuum condensates up to dimension four. The calculated values for
$cc\bar{c}\bar{c}$ and $bb\bar{b}\bar{b}$ tetraquark states at $T=0$ align well
with the experimental data on the broad structures. Based on the numerical
analyses around the critical temperature, the mass of the $T_{4c}$ state
decreases by $8\%$ compared to its vacuum state, while for its b partner, this
percentage is approximately $3.3\%$. For the decay constants, the reductions
are approximately $71\%$ and $66.6\%$, respectively. The precise exploration of
tetraquark states awaits future scrutiny in upcoming experiments, including
Belle II, Super-B, PANDA, and LHCb.; 63) Pair Correlation of Zeros of the Riemann Zeta Function I: Proportions of
  Simple Zeros and Critical Zeros; Assuming the Riemann Hypothesis (RH), Montgomery proved a theorem in 1973
concerning the pair correlation of zeros of the Riemann zeta-function and
applied this to prove that at least $2/3$ of the zeros are simple. In earlier
work we showed how to remove RH from Montgomery's theorem and, in turn, obtain
results on simple zeros assuming conditions on the zeros that are weaker than
RH. Here we assume a more general condition, namely that all the zeros $\rho =
\beta +i\gamma$ with $T<\gamma\le 2T$ are in a narrow vertical box centered on
the critical line with width $\frac{b}{\log T}$. For simplicity, now assume
that $b\to 0$ as $T\to \infty$. Following Montgomery's method, we prove the
generalization of Montgomery's result that at least $2/3$ of zeros are simple,
and also the new result that at least $2/3$ of the zeros are on the critical
line. We also use the pair correlation method to prove that at least $1/3$ of
the zeros are both simple and on the critical line, a result already known
unconditionally. Our work thus shows that the pair correlation method can be
used to prove results not only on the vertical distribution of zeros but also
on their horizontal distribution.; 64) Calibrating the Instrumental Drift in MAROON-X using an Ensemble
  Analysis; MAROON-X is a state-of-the-art extreme precision radial velocity spectrograph
deployed on the 8.1-meter Gemini-N telescope on Maunakea, Hawai'i. Using a
stabilized Fabry-P\'erot etalon for wavelength and drift calibration, MAROON-X
has achieved a short-term precision of $\sim$\,30\,cm\,s$^{-1}$. However, due
to a long-term drift in the etalon (2.2\,cm\,s$^{-1}$ per day) and various
interruptions of the instrument baseline over the first few years of operation,
MAROON-X experiences RV offsets between observing runs several times larger
than the short-term precision during any individual run, which hinders the
detection of longer-period signals. In this study, we analyze RV measurements
of 11 targets that either exhibit small RV scatter or have signals that can be
precisely constrained using Keplerian or Gaussian Process models. Leveraging
this ensemble, we calibrate MAROON-X's run offsets for data collected between
September 2020 and early January 2024 to a precision of $\sim$0.5\,m\,s$^{-1}$.
When applying these calibrated offsets to HD 3651, a quiet star, we obtain
residual velocities with an RMS of $<$70\,cm\,s$^{-1}$ in both the Red and Blue
channels of MAROON-X over a baseline of 29 months. We also demonstrate the
sensitivity of MAROON-X data calibrated with these offsets through a series of
injection-recovery tests. Based on our findings, MAROON-X is capable of
detecting sub m\,s$^{-1}$ signals out to periods of more than 1,000 days.; 65) CAAT-EHR: Cross-Attentional Autoregressive Transformer for Multimodal
  Electronic Health Record Embeddings; Electronic health records (EHRs) provide a comprehensive source of
longitudinal patient data, encompassing structured modalities such as
laboratory results, imaging data, and vital signs, and unstructured clinical
notes. These datasets, after necessary preprocessing to clean and format the
data for analysis, often remain in their raw EHR form, representing numerical
or categorical values without further transformation into task-agnostic
embeddings. While such raw EHR data enables predictive modeling, its reliance
on manual feature engineering or downstream task-specific optimization limits
its utility for general-purpose applications. Deep learning (DL) techniques,
such as recurrent neural networks (RNNs) and Transformers, have facilitated
predictive tasks like disease progression and diagnosis prediction. However,
these methods often struggle to fully exploit the temporal and multimodal
dependencies inherent in EHR data due to their reliance on pre-processed but
untransformed raw EHR inputs. In this study, we introduce CAAT-EHR, a novel
architecture designed to bridge this gap by generating robust, task-agnostic
longitudinal embeddings from raw EHR data. CAAT-EHR leverages self- and
cross-attention mechanisms in its encoder to integrate temporal and contextual
relationships across multiple modalities, transforming the data into enriched
embeddings that capture complex dependencies. An autoregressive decoder
complements the encoder by predicting future time points data during
pre-training, ensuring that the resulting embeddings maintain temporal
consistency and alignment. CAAT-EHR eliminates the need for manual feature
engineering and enables seamless transferability across diverse downstream
tasks. Extensive evaluations on benchmark datasets, demonstrate the superiority
of CAAT-EHR-generated embeddings over pre-processed raw EHR data and other
baseline approaches.; 66) Combining Flow Matching and Transformers for Efficient Solution of
  Bayesian Inverse Problems; Solving Bayesian inverse problems efficiently remains a significant challenge
due to the complexity of posterior distributions and the computational cost of
traditional sampling methods. Given a series of observations and the forward
model, we want to recover the distribution of the parameters, conditioned on
observed experimental data. We show, that combining Conditional Flow Mathching
(CFM) with transformer-based architecture, we can efficiently sample from such
kind of distribution, conditioned on variable number of observations.; 67) Mean value theorems for rational exponential sums; We obtain finite field analogues of a series of recent results on various
mean value theorems for Weyl sums. Instead of the Vinogradov Mean Value
Theorem, our results rest on the classical argument of Mordell, combined with
several other ideas.; 68) Bridging Contrastive Learning and Domain Adaptation: Theoretical
  Perspective and Practical Application; This work studies the relationship between Contrastive Learning and Domain
Adaptation from a theoretical perspective. The two standard contrastive losses,
NT-Xent loss (Self-supervised) and Supervised Contrastive loss, are related to
the Class-wise Mean Maximum Discrepancy (CMMD), a dissimilarity measure widely
used for Domain Adaptation. Our work shows that minimizing the contrastive
losses decreases the CMMD and simultaneously improves class-separability,
laying the theoretical groundwork for the use of Contrastive Learning in the
context of Domain Adaptation. Due to the relevance of Domain Adaptation in
medical imaging, we focused the experiments on mammography images. Extensive
experiments on three mammography datasets - synthetic patches, clinical (real)
patches, and clinical (real) images - show improved Domain Adaptation,
class-separability, and classification performance, when minimizing the
Supervised Contrastive loss.; 69) Derivation of the Planck Units Based in a Membranes Model; In this study, the Planck units (mass, time and length) have only been
derived, explained and attributed a physical meaning when they were deduced
based on the concept of interacting membranes (membranes instead of strings of
string theory). For this purpose, a set of five assumptions were proposed: (a)
the existence of the interacting membranes; (b) the curvatures of the membranes
oscillate according to the classical wave equation; (c) the spatial period of
the wave that arise when the membranes oscillate is given by $\lambda =
{\xi}{\pi}/k$; (d) the membranes oscillate with wavelength given by de Broglie
relation and (e) $x=ct$ holds. The parameter $\xi$ determines the period of
oscillation of the given membranes. In deriving the Planck units in this work,
$\xi$ must take the value 2 and determines a period 2$\pi$, closely to minimum
value 1 or to fundamental period $\pi$, respectively. In this context, Planck
units must be fundamental. Moreover, the parameter $\xi$ was reported as a
unification parameter between the formulas for the Coulomb$^{\prime}$s law and
Newton$^{\prime}$s law of universal gravitation linking the forces of
microworld and macroworld. Depending on the value $\xi$ takes, one force or
another will be had. It is also shown that the potential $V = hc/{\xi}{\pi}x$
deduced from the above assumptions and which contributes to deduce the Planck
units, can be derived from Yukawa$^{\prime}$s equation. Hence, the present work
would be contributing to theoretical physics, since at the Planck scale
predictions of some theories like Standard Model, quantum field theory and
general relativity are not expected to be valid.; 70) Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language
  Model; Generalized few-shot 3D point cloud segmentation (GFS-PCS) adapts models to
new classes with few support samples while retaining base class segmentation.
Existing GFS-PCS methods enhance prototypes via interacting with support or
query features but remain limited by sparse knowledge from few-shot samples.
Meanwhile, 3D vision-language models (3D VLMs), generalizing across open-world
novel classes, contain rich but noisy novel class knowledge. In this work, we
introduce a GFS-PCS framework that synergizes dense but noisy pseudo-labels
from 3D VLMs with precise yet sparse few-shot samples to maximize the strengths
of both, named GFS-VL. Specifically, we present a prototype-guided pseudo-label
selection to filter low-quality regions, followed by an adaptive infilling
strategy that combines knowledge from pseudo-label contexts and few-shot
samples to adaptively label the filtered, unlabeled areas. Additionally, we
design a novel-base mix strategy to embed few-shot samples into training
scenes, preserving essential context for improved novel class learning.
Moreover, recognizing the limited diversity in current GFS-PCS benchmarks, we
introduce two challenging benchmarks with diverse novel classes for
comprehensive generalization evaluation. Experiments validate the effectiveness
of our framework across models and datasets. Our approach and benchmarks
provide a solid foundation for advancing GFS-PCS in the real world. The code is
at https://github.com/ZhaochongAn/GFS-VL; 71) Modeling Dynamic Hand-Object Interactions with Applications to
  Human-Robot Handovers; Humans frequently grasp, manipulate, and move objects. Interactive systems
assist humans in these tasks, enabling applications in Embodied AI, human-robot
interaction, and virtual reality. However, current methods in hand-object
synthesis often neglect dynamics and focus on generating static grasps. The
first part of this dissertation introduces dynamic grasp synthesis, where a
hand grasps and moves an object to a target pose. We approach this task using
physical simulation and reinforcement learning. We then extend this to bimanual
manipulation and articulated objects, requiring fine-grained coordination
between hands. In the second part of this dissertation, we study human-to-robot
handovers. We integrate captured human motion into simulation and introduce a
student-teacher framework that adapts to human behavior and transfers from sim
to real. To overcome data scarcity, we generate synthetic interactions,
increasing training diversity by 100x. Our user study finds no difference
between policies trained on synthetic vs. real motions.; 72) Rotating and non-linear magnetic-charged black hole with an anisotropic
  matter field; We present the solution of a non-linear magnetic-charged black hole with an
anisotropic matter field and further extend it to obtain the corresponding
rotating black hole solution using the modified Newman-Janis algorithm. The
event horizon and ergosphere of the rotating black hole are studied in terms of
the perspective of geometric properties, revealing that the rotating black hole
can have up to three horizons. The first law of thermodynamics and the
squared-mass formula for the rotating black hole are derived from a
thermodynamic perspective, based on which we obtain the thermodynamic
quantities and study the thermodynamic stability of the rotating black hole.
Additionally, we calculate the Penrose process for the rotating black hole,
indicating the influence of various black hole parameters on the maximal
efficiency of the Penrose process.; 73) A note on Strong Cosmic Censorship and its violation in
  Reissner-Nordstr\""om de Sitter black hole space-times; Penrose's Strong Cosmic Censorship conjecture safeguards determinism in
General Relativity. Within the initial value approach to General Relativity,
proof of Strong Cosmic Censorship preservation is predicated on the unique
evolution of the metric. For the Kerr-Newman family of black hole solutions,
this requires the inextendability of the metric past the Cauchy horizon, due to
the development of a ""blue-shift"" instability. Attempts to provide a rigorous
mathematical proof of Strong Cosmic Censorship has led to the formulation of
several Strong Cosmic Censorship conjectures of varying strengths, which seem
to be discussed rarely outside of the mathematical relativity literature. In
this note, we review some of the arguments for and against Strong Cosmic
Censorship preservation, with a focus on the Reissner-Nordstr\""om de Sitter
context, where the positive cosmological constant invites a ""red-shift"" effect
that competes against the ""blue-shift"". We study the consequent role of
quasinormal mode behaviour and illustrate the parameter space for which we
consistently observe violations of the Strong Cosmic Censorship conjecture
within Reissner-Nordstr\""om de Sitter black holes.; 74) Optimizing Wireless Resource Management and Synchronization in Digital
  Twin Networks; In this paper, we investigate an accurate synchronization between a physical
network and its digital network twin (DNT), which serves as a virtual
representation of the physical network. The considered network includes a set
of base stations (BSs) that must allocate its limited spectrum resources to
serve a set of users while also transmitting its partially observed physical
network information to a cloud server to generate the DNT. Since the DNT can
predict the physical network status based on its historical status, the BSs may
not need to send their physical network information at each time slot, allowing
them to conserve spectrum resources to serve the users. However, if the DNT
does not receive the physical network information of the BSs over a large time
period, the DNT's accuracy in representing the physical network may degrade. To
this end, each BS must decide when to send the physical network information to
the cloud server to update the DNT, while also determining the spectrum
resource allocation policy for both DNT synchronization and serving the users.
We formulate this resource allocation task as an optimization problem, aiming
to maximize the total data rate of all users while minimizing the
asynchronization between the physical network and the DNT. To address this
problem, we propose a method based on the GRUs and the value decomposition
network (VDN). Simulation results show that our GRU and VDN based algorithm
improves the weighted sum of data rates and the similarity between the status
of the DNT and the physical network by up to 28.96%, compared to a baseline
method combining GRU with the independent Q learning.; 75) WIggle Corrector Kit for NIRSpEc Data: WICKED; The point-spread function of the integral-field unit (IFU) mode of the JWST's
NIRSpec is heavily under-sampled, creating resampling noise seen as
low-frequency sinusoidal-like artifacts, or ""wiggles"". These artifacts in the
data are not corrected in the JWST data pipeline, and significantly impact the
science that can be achieved at a single-pixel level. We present WICKED (WIggle
Corrector Kit for NIRSpEc Data), a tool designed to empirically remove wiggles.
WICKED uses the Fast Fourier Transform to identify wiggle-affected spaxels
across the data cube. Spectra are modeled with a mix of integrated aperture and
annular templates, a power-law, and a second-degree polynomial. The method
works across all medium- and high-resolution NIRSpec gratings: F070LP, F100LP,
F170LP, and F290LP. WICKED can recover the true overall spectral shape up to a
factor of 3.5x better compared to uncorrected spectra. It recovers the
equivalent width of absorption lines within 5% of the true value-~3x better
than uncorrected spectra and ~2x better than other methods. WICKED
significantly improves kinematic measurements, recovering the line-of-sight
velocity (LOSV) within 1% of the true value -- more than 100x better than
uncorrected spectra at S/N ~40. As a case study, we applied WICKED to
G235H/F170LP IFU data of the elliptical galaxy NGC5128, finding good agreement
with previous studies. In wiggle-affected regions, the uncorrected spectrum
showed stellar LOSV and velocity dispersion differences compared to the
WICKED-cleaned spectrum, of ~17x and ~36x larger than the estimated
uncertainties, respectively. Wiggles in NIRSpec IFU data can introduce severe
biases in spectral shape, line measurements, and kinematics to values larger
than the typical uncertainties. WICKED provides a robust, user-friendly
solution, enabling precise single-pixel studies and maximizing JWST's
potential.; 76) French Onion Soup, Ipelets for Points and Polygons; There are many structures, both classical and modern, involving point-sets
and polygons whose deeper understanding can be facilitated through interactive
visualizations. The Ipe extensible drawing editor, developed by Otfried Cheong,
is a widely used software system for generating geometric figures. One of its
features is the capability to extend its functionality through programs called
Ipelets. In this media submission, we showcase a collection of new Ipelets that
construct a variety of geometric based structures based on point sets and
polygons. These include quadtrees, trapezoidal maps, beta skeletons, floating
bodies of convex polygons, onion graphs, fractals (Sierpi\'nski triangle and
carpet), simple polygon triangulations, and random point sets in simple
polygons. All of our Ipelets are programmed in Lua and are freely available.; 77) Efektywne energetycznie wielodost\k{e}powe przetwarzanie brzegowe w
  sieci 5G; Energy efficient Multi-access Edge Computing in 5G network; Multi-access edge computing is a technique that combines the use of
communication networks and remote computing resources. It allows to perform
complex computational tasks for devices with low computing power while
maintaining low latencies. However, it is important to effectively allocate the
computing tasks to individual nodes. The work will present how the multi-access
edge computing system can be integrated into the 5G network, as well as how
resources can be distributed between individual nodes to minimize energy
consumption. Some new degrees of freedom will be presented, which enable a
significant reduction in energy consumption compared to existing solutions for
independent optimization of the computation and communication parts.
  --
  Wielodost\k{e}powe przetwarzanie brzegowe jest technik\k{a}
{\l}\k{a}cz\k{a}c\k{a} wykorzystanie sieci komunikacyjnych i oddalonych
zasob\'ow obliczeniowych. Pozwala wykona\'c z{\l}o\.zone zadania obliczeniowe
na potrzeby urz\k{a}dze\'n o niewielkiej mocy obliczeniowej przy zachowaniu
niewielkich op\'o\'znie\'n. Istotne jest jednak efektywne zarz\k{a}dzanie
przydzia{\l}em zada\'n obliczeniowych do poszczeg\'olnych w\k{e}z{\l}\'ow. W
pracy przedstawiono jak system przetwarzania brzegowego mo\.ze by\'c
zintegrowany z sieci\k{a} 5G, a tak\.ze jak mo\.zna rozdzieli\'c zasoby
mi\k{e}dzy poszczeg\'olne w\k{e}z{\l}y, \.zeby zminimalizowa\'c zu\.zycie
energii. Przedstawiony zostanie szereg nowych stopni swobody, kt\'ore
umo\.zliwiaj\k{a} znaczne obni\.zenie zu\.zycia energii w stosunku do
istniej\k{a}cych rozwi\k{a}za\'n niezale\.znej optymalizacji cz\k{e}\'sci
obliczeniowej i komunikacyjnej.; 78) Thermal conductivity of 3C-SiC from configuration space sampling; Cubic silicon carbide phonon thermal conductivity has been calculated using
anharmonic phonon analysis. The atomic interaction model was built using
displacement-force data obtained with the High Efficiency Configuration Space
Sampling (HECSS) technique and density functional theory calculated forces. In
the new version of HECSS we replaced the Markov chain scheme of
Metropolis-Hastings Monte-Carlo with weighting of the final sampling according
to the target distribution. This increased the efficiency of the method and
allowed to use -- with appropriate weight -- all generated and ab-initio
evaluated samples. The quality of the proposed method is confirmed by the
accuracy with which the experimental results taken from the literature were
reproduced.; 79) Collect, Commit, Expand: Efficient CPQR-Based Column Selection for
  Extremely Wide Matrices; Column-pivoted QR (CPQR) factorization is a computational primitive used in
numerous applications that require selecting a small set of ``representative''
columns from a much larger matrix. These include applications in spectral
clustering, model-order reduction, low-rank approximation, and computational
quantum chemistry, where the matrix being factorized has a moderate number of
rows but an extremely large number of columns. We describe a modification of
the Golub-Businger algorithm which, for many matrices of this type, can perform
CPQR-based column selection much more efficiently. This algorithm, which we
call CCEQR, is based on a three-step ``collect, commit, expand'' strategy that
limits the number of columns being manipulated, while also transferring more
computational effort from level-2 BLAS to level-3. Unlike most CPQR algorithms
that exploit level-3 BLAS, CCEQR is deterministic, and provably recovers a
column permutation equivalent to the one computed by the Golub-Businger
algorithm. Tests on spectral clustering and Wannier basis localization problems
demonstrate that on appropriately structured problems, CCEQR can significantly
outperform GEQP3.; 80) Dual-component stellar assembly histories in local elliptical galaxies
  via MUSE; Elliptical galaxies often exhibit complex assembly histories, and are
presumed to typically form through a combination of rapid, early star formation
and subsequent accretion of material, often resulting from mergers with other
galaxies. To investigate theories of spheroidal galaxy formation, the objective
of this work is to analyse the star formation histories (SFHs) of a sample of
three isolated elliptical galaxies in the local Universe observed with MUSE at
$z<0.06$. With BUDDI, we decompose the integral field unit (IFU) datacubes into
two components with S\'ersic profiles, which roughly correspond to the two
phases of in-situ and ex-situ star formation. To constrain the mode of growth
in these galaxies, we derived the mass and light-weighted stellar ages and
metallicities, and created the 2D stellar population maps of each component
using pPXF. We reconstructed the mass and light-weighted SFHs to constrain the
contribution of different stellar populations to the mass and luminosity of the
components through cosmic time. Our results show that the ellipticals in this
sample have experienced an early and rapid phase of star formation either
through a rapid dissipative collapse or gas-rich major mergers concentrated in
the inner component, which contributes to $\sim50$% of the galaxy stellar mass.
The co-dominant outer component, however, had assembled the bulk of its stellar
mass shortly after the inner component did, through accretion via dry mergers
and possible gas accretion. This premise is supported by our observations of
the inner component being primarily composed of old and metal-rich stars. The
outer component has a combination of old and intermediate-age stars, with a
moderate spread in metallicities. These results are analysed through the lens
of the two-phase scenario, a framework developed over the years to explain the
formation histories of elliptical galaxies.; 81) Synergistic Effects of Knowledge Distillation and Structured Pruning for
  Self-Supervised Speech Models; Traditionally, Knowledge Distillation (KD) is used for model compression,
often leading to suboptimal performance. In this paper, we evaluate the impact
of combining KD loss with alternative pruning techniques, including Low-Rank
Factorization (LRF) and l0 regularization, on a conformer-based pre-trained
network under the paradigm of Self-Supervised Learning (SSL). We also propose a
strategy to jointly prune and train an RNN-T-based ASR model, demonstrating
that this approach yields superior performance compared to pruning a
pre-trained network first and then using it for ASR training. This approach led
to a significant reduction in word error rate: l0 and KD combination achieves
the best non-streaming performance, with a 8.9% Relative Word Error Rate (RWER)
improvement over the baseline, while LRF and KD combination yields the best
results for streaming ASR, improving RWER by 13.4%.; 82) Direct Expression for One-Loop Tensor Reduction with Lorentz Indices via
  Generating Function; In recent work, we derived a direct expression for one-loop tensor reduction
using generating functions and Feynman parametrization in projective space,
avoiding recursive relations. However, for practical applications, this
expression still presents two challenges: (1) While the final reduction
coefficients are expressed in terms of the dimension D and Mandelstam
variables, the given expression explicitly contains irrational functions; (2)
The expression involves an auxiliary vector R, which can be eliminated via
differentiation $\frac{\partial}{\partial R}$, but the presence of irrational
terms making differentiation cumbersome. (3) Most practical applications
require the tensor form with Lorentz indices.
  In this paper, we provide a rational form of the reduction coefficients with
Lorentz indices, free from recursion. Additionally, We provide a pure Wolfram
Mathematica implementation of the code. Our practical tests demonstrate that
this direct expression achieves significantly higher computational efficiency
compared to the traditional Passarino-Veltman (PV) reduction or other
recursion-based methods.; 83) HRAvatar: High-Quality and Relightable Gaussian Head Avatar; Reconstructing animatable and high-quality 3D head avatars from monocular
videos, especially with realistic relighting, is a valuable task. However, the
limited information from single-view input, combined with the complex head
poses and facial movements, makes this challenging. Previous methods achieve
real-time performance by combining 3D Gaussian Splatting with a parametric head
model, but the resulting head quality suffers from inaccurate face tracking and
limited expressiveness of the deformation model. These methods also fail to
produce realistic effects under novel lighting conditions. To address these
issues, we propose HRAvatar, a 3DGS-based method that reconstructs
high-fidelity, relightable 3D head avatars. HRAvatar reduces tracking errors
through end-to-end optimization and better captures individual facial
deformations using learnable blendshapes and learnable linear blend skinning.
Additionally, it decomposes head appearance into several physical properties
and incorporates physically-based shading to account for environmental
lighting. Extensive experiments demonstrate that HRAvatar not only reconstructs
superior-quality heads but also achieves realistic visual effects under varying
lighting conditions.; 84) Inference Scaling Reshapes AI Governance; The shift from scaling up the pre-training compute of AI systems to scaling
up their inference compute may have profound effects on AI governance. The
nature of these effects depends crucially on whether this new inference compute
will primarily be used during external deployment or as part of a more complex
training programme within the lab. Rapid scaling of inference-at-deployment
would: lower the importance of open-weight models (and of securing the weights
of closed models), reduce the impact of the first human-level models, change
the business model for frontier AI, reduce the need for power-intense data
centres, and derail the current paradigm of AI governance via training compute
thresholds. Rapid scaling of inference-during-training would have more
ambiguous effects that range from a revitalisation of pre-training scaling to a
form of recursive self-improvement via iterated distillation and amplification.; 85) A LSTM-Transformer Model for pulsation control of pVADs; Methods: A method of the pulsation for a pVAD is proposed (AP-pVAD Model).
AP-pVAD Model consists of two parts: NPQ Model and LSTM-Transformer Model.
(1)The NPQ Model determines the mathematical relationship between motor speed,
pressure, and flow rate for the pVAD. (2)The Attention module of Transformer
neural network is integrated into the LSTM neural network to form the new
LSTM-Transformer Model to predict the pulsation time characteristic points for
adjusting the motor speed of the pVAD. Results: The AP-pVAD Model is validated
in three hydraulic experiments and an animal experiment. (1)The pressure
provided by pVAD calculated with the NPQ Model has a maximum error of only 2.15
mmHg compared to the expected values. (2)The pulsation time characteristic
points predicted by the LSTM-Transformer Model shows a maximum prediction error
of 1.78ms, which is significantly lower than other methods. (3)The in-vivo test
of pVAD in animal experiment has significant improvements in aortic pressure.
Animals survive for over 27 hours after the initiation of pVAD operation.
Conclusion: (1)For a given pVAD, motor speed has a linear relationship with
pressure and a quadratic relationship with flow. (2)Deep learning can be used
to predict pulsation characteristic time points, with the LSTM-Transformer
Model demonstrating minimal prediction error and better robust performance
under conditions of limited dataset sizes, elevated noise levels, and diverse
hyperparameter combinations, demonstrating its feasibility and effectiveness.; 86) RobotIQ: Empowering Mobile Robots with Human-Level Planning for
  Real-World Execution; This paper introduces RobotIQ, a framework that empowers mobile robots with
human-level planning capabilities, enabling seamless communication via natural
language instructions through any Large Language Model. The proposed framework
is designed in the ROS architecture and aims to bridge the gap between humans
and robots, enabling robots to comprehend and execute user-expressed text or
voice commands. Our research encompasses a wide spectrum of robotic tasks,
ranging from fundamental logical, mathematical, and learning reasoning for
transferring knowledge in domains like navigation, manipulation, and object
localization, enabling the application of learned behaviors from simulated
environments to real-world operations. All encapsulated within a modular
crafted robot library suite of API-wise control functions, RobotIQ offers a
fully functional AI-ROS-based toolset that allows researchers to design and
develop their own robotic actions tailored to specific applications and robot
configurations. The effectiveness of the proposed system was tested and
validated both in simulated and real-world experiments focusing on a home
service scenario that included an assistive application designed for elderly
people. RobotIQ with an open-source, easy-to-use, and adaptable robotic library
suite for any robot can be found at https://github.com/emmarapt/RobotIQ.; 87) Is fitting error a reliable metric for assessing deformable motion
  correction in quantitative MRI?; Quantitative MR (qMR) can provide numerical values representing the physical
and chemical properties of the tissues. To collect a series of frames under
varying settings, retrospective motion correction is essential to align the
corresponding anatomical points or features. Under the assumption that the
misalignment makes the discrepancy between the corresponding features larger,
fitting error is a commonly used evaluation metric for motion correction in
qMR. This study evaluates the reliability of the fitting error metric in
cardiac diffusion tensor imaging (cDTI) after deformable registration. We found
that while fitting error correlates with the negative eigenvalues, the negative
Jacobian Determinant increases with broken cardiomyocytes, indicated by helix
angle gradient line profiles. Since fitting error measures the distance between
moved points and their re-rendered counterparts, the fitting parameter itself
may be adjusted due to poor registration. Therefore, fitting error in
deformable registration itself is a necessary but not sufficient metric and
should be combined with other metrics.; 88) Regret Analysis: a control perspective; Online learning and model reference adaptive control have many interesting
intersections. One area where they differ however is in how the algorithms are
analyzed and what objective or metric is used to discriminate ""good"" algorithms
from ""bad"" algorithms. In adaptive control there are usually two objectives: 1)
prove that all time varying parameters/states of the system are bounded, and 2)
that the instantaneous error between the adaptively controlled system and a
reference system converges to zero over time (or at least a compact set). For
online learning the performance of algorithms is often characterized by the
regret the algorithm incurs. Regret is defined as the cumulative loss (cost)
over time from the online algorithm minus the cumulative loss (cost) of the
single optimal fixed parameter choice in hindsight. Another significant
difference between the two areas of research is with regard to the assumptions
made in order to obtain said results. Adaptive control makes assumptions about
the input-output properties of the control problem and derives solutions for a
fixed error model or optimization task. In the online learning literature
results are derived for classes of loss functions (i.e. convex) while a priori
assuming certain signals are bounded. In this work we discuss these differences
in detail through the regret based analysis of gradient descent for convex
functions and the control based analysis of a streaming regression problem. We
close with a discussion about the newly defined paradigm of online adaptive
control.; 89) Enhancing the Product Quality of the Injection Process Using eXplainable
  Artificial Intelligence; The injection molding process is a traditional technique for making products
in various industries such as electronics and automobiles via solidifying
liquid resin into certain molds. Although the process is not related to
creating the main part of engines or semiconductors, this manufacturing
methodology sets the final form of the products. Re-cently, research has
continued to reduce the defect rate of the injection molding process. This
study proposes an optimal injection molding process control system to reduce
the defect rate of injection molding products with XAI (eXplainable Artificial
Intelligence) ap-proaches. Boosting algorithms (XGBoost and LightGBM) are used
as tree-based classifiers for predicting whether each product is normal or
defective. The main features to control the process for improving the product
are extracted by SHapley Additive exPlanations, while the individual
conditional expectation analyzes the optimal control range of these extracted
features. To validate the methodology presented in this work, the actual
injection molding AI manufacturing dataset provided by KAMP (Korea AI
Manufacturing Platform) is employed for the case study. The results reveal that
the defect rate decreases from 1.00% (Original defect rate) to 0.21% with
XGBoost and 0.13% with LightGBM, respectively.; 90) UAV-assisted Internet of Vehicles: A Framework Empowered by
  Reinforcement Learning and Blockchain; This paper addresses the challenges of selecting relay nodes and coordinating
among them in UAV-assisted Internet-of-Vehicles (IoV). The selection of UAV
relay nodes in IoV employs mechanisms executed either at centralized servers or
decentralized nodes, which have two main limitations: 1) the traceability of
the selection mechanism execution and 2) the coordination among the selected
UAVs, which is currently offered in a centralized manner and is not coupled
with the relay selection. Existing UAV coordination methods often rely on
optimization methods, which are not adaptable to different environment
complexities, or on centralized deep reinforcement learning, which lacks
scalability in multi-UAV settings. Overall, there is a need for a comprehensive
framework where relay selection and coordination are coupled and executed in a
transparent and trusted manner. This work proposes a framework empowered by
reinforcement learning and Blockchain for UAV-assisted IoV networks. It
consists of three main components: a two-sided UAV relay selection mechanism
for UAV-assisted IoV, a decentralized Multi-Agent Deep Reinforcement Learning
(MDRL) model for autonomous UAV coordination, and a Blockchain implementation
for transparency and traceability in the interactions between vehicles and
UAVs. The relay selection considers the two-sided preferences of vehicles and
UAVs based on the Quality-of-UAV (QoU) and the Quality-of-Vehicle (QoV). Upon
selection of relay UAVs, the decentralized coordination between them is enabled
through an MDRL model trained to control their mobility and maintain the
network coverage and connectivity using Proximal Policy Optimization (PPO). The
evaluation results demonstrate that the proposed selection and coordination
mechanisms improve the stability of the selected relays and maximize the
coverage and connectivity achieved by the UAVs.; 91) General Feature Extraction In SAR Target Classification: A Contrastive
  Learning Approach Across Sensor Types; The increased availability of SAR data has raised a growing interest in
applying deep learning algorithms. However, the limited availability of labeled
data poses a significant challenge for supervised training. This article
introduces a new method for classifying SAR data with minimal labeled images.
The method is based on a feature extractor Vit trained with contrastive
learning. It is trained on a dataset completely different from the one on which
classification is made. The effectiveness of the method is assessed through 2D
visualization using t-SNE for qualitative evaluation and k-NN classification
with a small number of labeled data for quantitative evaluation. Notably, our
results outperform a k-NN on data processed with PCA and a ResNet-34
specifically trained for the task, achieving a 95.9% accuracy on the MSTAR
dataset with just ten labeled images per class.; 92) Behavioural Predictors that Influence Digital Legacy Management
  Intentions among Individuals in South Africa; An emerging phenomenon, digital legacy management explores the management of
digital data individuals accumulate throughout their lifetime. With the
integration of digital systems and data into people's daily lives, it becomes
crucial to understand the intricacies of managing data to eventually form one's
digital legacy. This can be understood by investigating the significance of
behavioral predictors in shaping digital legacy management.
  The objective of this study is to explore how behavioral predictors influence
the intentions of individuals in South Africa towards managing their digital
legacy. This entailed:
  Investigating the impact of attitude, subjective norms, and perceived
behavioral control on these intentions. Exploring the perceived usefulness of
digital legacy management systems. Understanding the implications of response
cost and task-technology fit on individuals' inclinations towards digital
legacy planning. Data were collected (n = 203 valid responses) from South
African residents using an online survey and analyzed using partial least
squares structural equation analysis (PLS-SEM). Results indicate that
attitudes, peer opinions, personal resources, and skills are significant
positive influences on digital legacy management intention. Recognizing and
understanding these behavioral predictors is key when developing
region-specific and culturally sensitive digital legacy management tools,
awareness campaigns, and policies. Furthermore, it could pave the way for more
tailored strategies, ensuring effective transfer of post-mortem data, reducing
potential conflicts, and providing clarity when dealing with post-mortem data.; 93) HGO-YOLO: Advancing Anomaly Behavior Detection with Hierarchical
  Features and Lightweight Optimized Detection; Accurate and real-time object detection is crucial for anomaly behavior
detection, especially in scenarios constrained by hardware limitations, where
balancing accuracy and speed is essential for enhancing detection performance.
This study proposes a model called HGO-YOLO, which integrates the HGNetv2
architecture into YOLOv8. This combination expands the receptive field and
captures a wider range of features while simplifying model complexity through
GhostConv. We introduced a lightweight detection head, OptiConvDetect, which
utilizes parameter sharing to construct the detection head effectively.
Evaluation results show that the proposed algorithm achieves a mAP@0.5 of 87.4%
and a recall rate of 81.1%, with a model size of only 4.6 MB and a frame rate
of 56 FPS on the CPU. HGO-YOLO not only improves accuracy by 3.0% but also
reduces computational load by 51.69% (from 8.9 GFLOPs to 4.3 GFLOPs), while
increasing the frame rate by a factor of 1.7. Additionally, real-time tests
were conducted on Raspberry Pi4 and NVIDIA platforms. These results indicate
that the HGO-YOLO model demonstrates superior performance in anomaly behavior
detection.; 94) AI-Instruments: Embodying Prompts as Instruments to Abstract & Reflect
  Graphical Interface Commands as General-Purpose Tools; Chat-based prompts respond with verbose linear-sequential texts, making it
difficult to explore and refine ambiguous intents, back up and reinterpret, or
shift directions in creative AI-assisted design work. AI-Instruments instead
embody ""prompts"" as interface objects via three key principles: (1) Reification
of user-intent as reusable direct-manipulation instruments; (2) Reflection of
multiple interpretations of ambiguous user-intents (Reflection-in-intent) as
well as the range of AI-model responses (Reflection-in-response) to inform
design ""moves"" towards a desired result; and (3) Grounding to instantiate an
instrument from an example, result, or extrapolation directly from another
instrument. Further, AI-Instruments leverage LLM's to suggest, vary, and refine
new instruments, enabling a system that goes beyond hard-coded functionality by
generating its own instrumental controls from content. We demonstrate four
technology probes, applied to image generation, and qualitative insights from
twelve participants, showing how AI-Instruments address challenges of intent
formulation, steering via direct manipulation, and non-linear iterative
workflows to reflect and resolve ambiguous intents.; 95) The Value of Prediction in Identifying the Worst-Off; Machine learning is increasingly used in government programs to identify and
support the most vulnerable individuals, prioritizing assistance for those at
greatest risk over optimizing aggregate outcomes. This paper examines the
welfare impacts of prediction in equity-driven contexts, and how they compare
to other policy levers, such as expanding bureaucratic capacity. Through
mathematical models and a real-world case study on long-term unemployment
amongst German residents, we develop a comprehensive understanding of the
relative effectiveness of prediction in surfacing the worst-off. Our findings
provide clear analytical frameworks and practical, data-driven tools that
empower policymakers to make principled decisions when designing these systems.; 96) Universal machine learning interatomic potentials poised to supplant DFT
  in modeling general defects in metals and random alloys; Recent advances in machine learning, combined with the generation of
extensive density functional theory (DFT) datasets, have enabled the
development of universal machine learning interatomic potentials (uMLIPs).
These models offer broad applicability across the periodic table, achieving
first-principles accuracy at a fraction of the computational cost of
traditional DFT calculations. In this study, we demonstrate that
state-of-the-art pretrained uMLIPs can effectively replace DFT for accurately
modeling complex defects in a wide range of metals and alloys. Our
investigation spans diverse scenarios, including grain boundaries and general
defects in pure metals, defects in high-entropy alloys, hydrogen-alloy
interactions, and solute-defect interactions. Remarkably, the latest
EquiformerV2 models achieve DFT-level accuracy on comprehensive defect
datasets, with root mean square errors (RMSE) below 5 meV/atom for energies and
100 meV/{\AA} for forces, outperforming specialized machine learning potentials
such as moment tensor potential and atomic cluster expansion. We also present a
systematic analysis of accuracy versus computational cost and explore
uncertainty quantification for uMLIPs. A detailed case study of tungsten (W)
demonstrates that data on pure W alone is insufficient for modeling complex
defects in uMLIPs, underscoring the critical importance of advanced machine
learning architectures and diverse datasets, which include over 100 million
structures spanning all elements. These findings establish uMLIPs as a robust
alternative to DFT and a transformative tool for accelerating the discovery and
design of high-performance materials.; 97) Tarski Lower Bounds from Multi-Dimensional Herringbones; Tarski's theorem states that every monotone function from a complete lattice
to itself has a fixed point. We analyze the query complexity of finding such a
fixed point on the $k$-dimensional grid of side length $n$ under the $\leq$
relation. In this setting, there is an unknown monotone function $f:
\{0,1,\ldots, n-1\}^k \to \{0,1,\ldots, n-1\}^k$ and an algorithm must query a
vertex $v$ to learn $f(v)$. The goal is to find a fixed point of $f$ using as
few oracle queries as possible.
  We show that the randomized query complexity of this problem is $\Omega\left(
\frac{k \cdot \log^2{n}}{\log{k}} \right)$ for all $n,k \geq 2$. This unifies
and improves upon two prior results: a lower bound of $\Omega(\log^2{n})$ from
[EPRY 2019] and a lower bound of $\Omega\left( \frac{k \cdot
\log{n}}{\log{k}}\right)$ from [BPR 2024], respectively.; 98) Scaling Rich Style-Prompted Text-to-Speech Datasets; We introduce Paralinguistic Speech Captions (ParaSpeechCaps), a large-scale
dataset that annotates speech utterances with rich style captions. While rich
abstract tags (e.g. guttural, nasal, pained) have been explored in small-scale
human-annotated datasets, existing large-scale datasets only cover basic tags
(e.g. low-pitched, slow, loud). We combine off-the-shelf text and speech
embedders, classifiers and an audio language model to automatically scale rich
tag annotations for the first time. ParaSpeechCaps covers a total of 59 style
tags, including both speaker-level intrinsic tags and utterance-level
situational tags. It consists of 342 hours of human-labelled data (PSC-Base)
and 2427 hours of automatically annotated data (PSC-Scaled). We finetune
Parler-TTS, an open-source style-prompted TTS model, on ParaSpeechCaps, and
achieve improved style consistency (+7.9% Consistency MOS) and speech quality
(+15.5% Naturalness MOS) over the best performing baseline that combines
existing rich style tag datasets. We ablate several of our dataset design
choices to lay the foundation for future work in this space. Our dataset,
models and code are released at https://github.com/ajd12342/paraspeechcaps .; 99) Empirical Discovery of Multi-Scale Transfer of Information in Dynamical
  Systems; In this work, we quantify the timescales and information flow associated by
multiscale energy transfer in a weakly turbulent system through a novel new
interpretation of transfer entropy. Our goal is to provide a detailed
understanding of the nature of complex energy transfer in nonlinear dispersive
systems driven by wave mixing. Further, we present a modal decomposition method
based on the empirical wavelet transform that produces a relatively small
number of nearly decorrelated, scale separated modes. Using our method, we are
able to track multiscale energy transfer using only scalar time series
measurements of a weakly turbulent system. This points to our approach being of
broader applicability in real-world data coming from chaotic or turbulent
dynamical systems.; 100) Generalized Scattering Matrix Synthesis for Hybrid Systems with Multiple
  Scatterers and Antennas Using Independent Structure Simulations; This paper presents a unified formulation for calculating the generalized
scattering matrix (GS-matrix) of hybrid systems involving multiple scatterers
and antennas. The GS-matrix of the entire system is synthesized through the
scattering matrices and GS-matrices of each independent component, using the
addition theorem of vector spherical wavefunctions and fully matrix-based
operations. Since our formulation is applicable to general antenna-scatterer
hybrid systems, previous formulas for multiple scattering and antenna arrays
become special cases of our approach. This also establishes our formulation as
a universal domain decomposition method for analyzing the electromagnetic
performance of hybrid systems. We provide numerous numerical examples to
comprehensively demonstrate the capabilities and compatibility of the proposed
formulation, including its potential application in studying the effects of
structural rotation.",1.0,0.0
2412.00036,applied,2412.00036-pos1-2,"Quant GANs: deep generation of financial time series; Modeling financial time series by stochastic processes is a challenging task
and a central area of research in financial mathematics. As an alternative, we
introduce Quant GANs, a data-driven model which is inspired by the recent
success of generative adversarial networks (GANs). Quant GANs consist of a
generator and discriminator function, which utilize temporal convolutional
networks (TCNs) and thereby achieve to capture long-range dependencies such as
the presence of volatility clusters. The generator function is explicitly
constructed such that the induced stochastic process allows a transition to its
risk-neutral distribution. Our numerical results highlight that distributional
properties for small and large lags are in an excellent agreement and
dependence properties such as volatility clusters, leverage effects, and serial
autocorrelations can be generated by the generator function of Quant GANs,
demonstrably in high fidelity.",2412.00036-pos2-2,"On the Distribution of the Two-Sample Cramer-von Mises Criterion; The Cramer-von Mises $\omega^2$ criterion for testing that a sample, $x_1, \cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \begin{equation*}\tag{1}\omega^2 = \int^\infty_{-\infty} \lbrack F_N(x) - F(x)\rbrack^2 dF(x),\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\omega^2$, namely \begin{equation*}\tag{2} T NM/(N + M)\rbrack G_M(x)\rbrack^2 dH_{N+M}(x),\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\rbrack$. limiting $N\omega^2$ as $N \rightarrow \infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \infty, M \infty$, $N/M \lambda$, $\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4].",82,"['1', '2', '4', '5', '7', '14', '19', '20', '34', '62']","The best candidate paper is the first one, which utilizes machine learning interatomic potentials in modeling complex financial time series captured by Quant GANs. This candidate provides a novel application of machine learning techniques in finance, considering the need for high-fidelity stochastic processes in financial mathematics. The integration of universal machine learning interatomic potentials with Quant GANs could lead to improved modeling frameworks that not only enhance financial time series predictions but also apply across materials science, creating a unique intersection of finance and material modeling. The subsequent candidates are ranked based on their relevance and potential for collaboration with the main paper in terms of novelty and usefulness in addressing the complexities of financial time series modeling.","1) Universal machine learning interatomic potentials poised to supplant DFT
  in modeling general defects in metals and random alloys; Recent advances in machine learning, combined with the generation of
extensive density functional theory (DFT) datasets, have enabled the
development of universal machine learning interatomic potentials (uMLIPs).
These models offer broad applicability across the periodic table, achieving
first-principles accuracy at a fraction of the computational cost of
traditional DFT calculations. In this study, we demonstrate that
state-of-the-art pretrained uMLIPs can effectively replace DFT for accurately
modeling complex defects in a wide range of metals and alloys. Our
investigation spans diverse scenarios, including grain boundaries and general
defects in pure metals, defects in high-entropy alloys, hydrogen-alloy
interactions, and solute-defect interactions. Remarkably, the latest
EquiformerV2 models achieve DFT-level accuracy on comprehensive defect
datasets, with root mean square errors (RMSE) below 5 meV/atom for energies and
100 meV/{\AA} for forces, outperforming specialized machine learning potentials
such as moment tensor potential and atomic cluster expansion. We also present a
systematic analysis of accuracy versus computational cost and explore
uncertainty quantification for uMLIPs. A detailed case study of tungsten (W)
demonstrates that data on pure W alone is insufficient for modeling complex
defects in uMLIPs, underscoring the critical importance of advanced machine
learning architectures and diverse datasets, which include over 100 million
structures spanning all elements. These findings establish uMLIPs as a robust
alternative to DFT and a transformative tool for accelerating the discovery and
design of high-performance materials.; 2) Causal Inference on Outcomes Learned from Text; We propose a machine-learning tool that yields causal inference on text in
randomized trials. Based on a simple econometric framework in which text may
capture outcomes of interest, our procedure addresses three questions: First,
is the text affected by the treatment? Second, which outcomes is the effect on?
And third, how complete is our description of causal effects? To answer all
three questions, our approach uses large language models (LLMs) that suggest
systematic differences across two groups of text documents and then provides
valid inference based on costly validation. Specifically, we highlight the need
for sample splitting to allow for statistical validation of LLM outputs, as
well as the need for human labeling to validate substantive claims about how
documents differ across groups. We illustrate the tool in a proof-of-concept
application using abstracts of academic manuscripts.; 3) Dual-component stellar assembly histories in local elliptical galaxies
  via MUSE; Elliptical galaxies often exhibit complex assembly histories, and are
presumed to typically form through a combination of rapid, early star formation
and subsequent accretion of material, often resulting from mergers with other
galaxies. To investigate theories of spheroidal galaxy formation, the objective
of this work is to analyse the star formation histories (SFHs) of a sample of
three isolated elliptical galaxies in the local Universe observed with MUSE at
$z<0.06$. With BUDDI, we decompose the integral field unit (IFU) datacubes into
two components with S\'ersic profiles, which roughly correspond to the two
phases of in-situ and ex-situ star formation. To constrain the mode of growth
in these galaxies, we derived the mass and light-weighted stellar ages and
metallicities, and created the 2D stellar population maps of each component
using pPXF. We reconstructed the mass and light-weighted SFHs to constrain the
contribution of different stellar populations to the mass and luminosity of the
components through cosmic time. Our results show that the ellipticals in this
sample have experienced an early and rapid phase of star formation either
through a rapid dissipative collapse or gas-rich major mergers concentrated in
the inner component, which contributes to $\sim50$% of the galaxy stellar mass.
The co-dominant outer component, however, had assembled the bulk of its stellar
mass shortly after the inner component did, through accretion via dry mergers
and possible gas accretion. This premise is supported by our observations of
the inner component being primarily composed of old and metal-rich stars. The
outer component has a combination of old and intermediate-age stars, with a
moderate spread in metallicities. These results are analysed through the lens
of the two-phase scenario, a framework developed over the years to explain the
formation histories of elliptical galaxies.; 4) Efficient Interactive 3D Multi-Object Removal; Object removal is of great significance to 3D scene understanding, essential
for applications in content filtering and scene editing. Current mainstream
methods primarily focus on removing individual objects, with a few methods
dedicated to eliminating an entire area or all objects of a certain category.
They however confront the challenge of insufficient granularity and flexibility
for real-world applications, where users demand tailored excision and
preservation of objects within defined zones. In addition, most of the current
methods require kinds of priors when addressing multi-view inpainting, which is
time-consuming. To address these limitations, we propose an efficient and
user-friendly pipeline for 3D multi-object removal, enabling users to flexibly
select areas and define objects for removal or preservation. Concretely, to
ensure object consistency and correspondence across multiple views, we propose
a novel mask matching and refinement module, which integrates homography-based
warping with high-confidence anchor points for segmentation. By leveraging the
IoU joint shape context distance loss, we enhance the accuracy of warped masks
and improve subsequent inpainting processes. Considering the current immaturity
of 3D multi-object removal, we provide a new evaluation dataset to bridge the
developmental void. Experimental results demonstrate that our method
significantly reduces computational costs, achieving processing speeds more
than 80% faster than state-of-the-art methods while maintaining equivalent or
higher reconstruction quality.; 5) Infinite State Model Checking by Learning Transitive Relations; We propose a new approach for proving safety of infinite state systems. It
extends the analyzed system by transitive relations until its diameter D
becomes finite, i.e., until constantly many steps suffice to cover all
reachable states, irrespective of the initial state. Then we can prove safety
by checking that no error state is reachable in D steps. To deduce transitive
relations, we use recurrence analysis. While recurrence analyses can usually
find conjunctive relations only, our approach also discovers disjunctive
relations by combining recurrence analysis with projections. An empirical
evaluation of the implementation of our approach in our tool LoAT shows that it
is highly competitive with the state of the art.; 6) Symmetries and Anomalies of Hamiltonian Staggered Fermions; We review the shift and time reversal symmetries of Hamiltonian staggered
fermions and their connection to continuum symmetries concentrating in
particular on the case of massless fermions and (3+1) dimensions. We construct
operators using the staggered fields that implement these symmetries on finite
lattices. We show that the elementary shift symmetry of a single staggered
field depends on a $Z_4$ subgroup of an additional $U(1)$ phase symmetry and
anti-commutes with time reversal. This latter property implies that time
reversal symmetry will be broken if this phase symmetry is gauged - a mixed 't
Hooft anomaly. However, this anomaly can be canceled for multiples of four
staggered fields. Finally we observe that the naive continuum limit of the
minimal anomaly free lattice model has the symmetries and matter
representations of the Pati-Salam GUT.; 7) Regret Analysis: a control perspective; Online learning and model reference adaptive control have many interesting
intersections. One area where they differ however is in how the algorithms are
analyzed and what objective or metric is used to discriminate ""good"" algorithms
from ""bad"" algorithms. In adaptive control there are usually two objectives: 1)
prove that all time varying parameters/states of the system are bounded, and 2)
that the instantaneous error between the adaptively controlled system and a
reference system converges to zero over time (or at least a compact set). For
online learning the performance of algorithms is often characterized by the
regret the algorithm incurs. Regret is defined as the cumulative loss (cost)
over time from the online algorithm minus the cumulative loss (cost) of the
single optimal fixed parameter choice in hindsight. Another significant
difference between the two areas of research is with regard to the assumptions
made in order to obtain said results. Adaptive control makes assumptions about
the input-output properties of the control problem and derives solutions for a
fixed error model or optimization task. In the online learning literature
results are derived for classes of loss functions (i.e. convex) while a priori
assuming certain signals are bounded. In this work we discuss these differences
in detail through the regret based analysis of gradient descent for convex
functions and the control based analysis of a streaming regression problem. We
close with a discussion about the newly defined paradigm of online adaptive
control.; 8) Scaling Rich Style-Prompted Text-to-Speech Datasets; We introduce Paralinguistic Speech Captions (ParaSpeechCaps), a large-scale
dataset that annotates speech utterances with rich style captions. While rich
abstract tags (e.g. guttural, nasal, pained) have been explored in small-scale
human-annotated datasets, existing large-scale datasets only cover basic tags
(e.g. low-pitched, slow, loud). We combine off-the-shelf text and speech
embedders, classifiers and an audio language model to automatically scale rich
tag annotations for the first time. ParaSpeechCaps covers a total of 59 style
tags, including both speaker-level intrinsic tags and utterance-level
situational tags. It consists of 342 hours of human-labelled data (PSC-Base)
and 2427 hours of automatically annotated data (PSC-Scaled). We finetune
Parler-TTS, an open-source style-prompted TTS model, on ParaSpeechCaps, and
achieve improved style consistency (+7.9% Consistency MOS) and speech quality
(+15.5% Naturalness MOS) over the best performing baseline that combines
existing rich style tag datasets. We ablate several of our dataset design
choices to lay the foundation for future work in this space. Our dataset,
models and code are released at https://github.com/ajd12342/paraspeechcaps .; 9) Rolling Ahead Diffusion for Traffic Scene Simulation; Realistic driving simulation requires that NPCs not only mimic natural
driving behaviors but also react to the behavior of other simulated agents.
Recent developments in diffusion-based scenario generation focus on creating
diverse and realistic traffic scenarios by jointly modelling the motion of all
the agents in the scene. However, these traffic scenarios do not react when the
motion of agents deviates from their modelled trajectories. For example, the
ego-agent can be controlled by a stand along motion planner. To produce
reactive scenarios with joint scenario models, the model must regenerate the
scenario at each timestep based on new observations in a Model Predictive
Control (MPC) fashion. Although reactive, this method is time-consuming, as one
complete possible future for all NPCs is generated per simulation step.
Alternatively, one can utilize an autoregressive model (AR) to predict only the
immediate next-step future for all NPCs. Although faster, this method lacks the
capability for advanced planning. We present a rolling diffusion based traffic
scene generation model which mixes the benefits of both methods by predicting
the next step future and simultaneously predicting partially noised further
future steps at the same time. We show that such model is efficient compared to
diffusion model based AR, achieving a beneficial compromise between reactivity
and computational efficiency.; 10) Is fitting error a reliable metric for assessing deformable motion
  correction in quantitative MRI?; Quantitative MR (qMR) can provide numerical values representing the physical
and chemical properties of the tissues. To collect a series of frames under
varying settings, retrospective motion correction is essential to align the
corresponding anatomical points or features. Under the assumption that the
misalignment makes the discrepancy between the corresponding features larger,
fitting error is a commonly used evaluation metric for motion correction in
qMR. This study evaluates the reliability of the fitting error metric in
cardiac diffusion tensor imaging (cDTI) after deformable registration. We found
that while fitting error correlates with the negative eigenvalues, the negative
Jacobian Determinant increases with broken cardiomyocytes, indicated by helix
angle gradient line profiles. Since fitting error measures the distance between
moved points and their re-rendered counterparts, the fitting parameter itself
may be adjusted due to poor registration. Therefore, fitting error in
deformable registration itself is a necessary but not sufficient metric and
should be combined with other metrics.; 11) Inference Scaling Reshapes AI Governance; The shift from scaling up the pre-training compute of AI systems to scaling
up their inference compute may have profound effects on AI governance. The
nature of these effects depends crucially on whether this new inference compute
will primarily be used during external deployment or as part of a more complex
training programme within the lab. Rapid scaling of inference-at-deployment
would: lower the importance of open-weight models (and of securing the weights
of closed models), reduce the impact of the first human-level models, change
the business model for frontier AI, reduce the need for power-intense data
centres, and derail the current paradigm of AI governance via training compute
thresholds. Rapid scaling of inference-during-training would have more
ambiguous effects that range from a revitalisation of pre-training scaling to a
form of recursive self-improvement via iterated distillation and amplification.; 12) HGO-YOLO: Advancing Anomaly Behavior Detection with Hierarchical
  Features and Lightweight Optimized Detection; Accurate and real-time object detection is crucial for anomaly behavior
detection, especially in scenarios constrained by hardware limitations, where
balancing accuracy and speed is essential for enhancing detection performance.
This study proposes a model called HGO-YOLO, which integrates the HGNetv2
architecture into YOLOv8. This combination expands the receptive field and
captures a wider range of features while simplifying model complexity through
GhostConv. We introduced a lightweight detection head, OptiConvDetect, which
utilizes parameter sharing to construct the detection head effectively.
Evaluation results show that the proposed algorithm achieves a mAP@0.5 of 87.4%
and a recall rate of 81.1%, with a model size of only 4.6 MB and a frame rate
of 56 FPS on the CPU. HGO-YOLO not only improves accuracy by 3.0% but also
reduces computational load by 51.69% (from 8.9 GFLOPs to 4.3 GFLOPs), while
increasing the frame rate by a factor of 1.7. Additionally, real-time tests
were conducted on Raspberry Pi4 and NVIDIA platforms. These results indicate
that the HGO-YOLO model demonstrates superior performance in anomaly behavior
detection.; 13) Synergistic Effects of Knowledge Distillation and Structured Pruning for
  Self-Supervised Speech Models; Traditionally, Knowledge Distillation (KD) is used for model compression,
often leading to suboptimal performance. In this paper, we evaluate the impact
of combining KD loss with alternative pruning techniques, including Low-Rank
Factorization (LRF) and l0 regularization, on a conformer-based pre-trained
network under the paradigm of Self-Supervised Learning (SSL). We also propose a
strategy to jointly prune and train an RNN-T-based ASR model, demonstrating
that this approach yields superior performance compared to pruning a
pre-trained network first and then using it for ASR training. This approach led
to a significant reduction in word error rate: l0 and KD combination achieves
the best non-streaming performance, with a 8.9% Relative Word Error Rate (RWER)
improvement over the baseline, while LRF and KD combination yields the best
results for streaming ASR, improving RWER by 13.4%.; 14) Critical Mathematical Economics and the Model-theoretic Foundations of
  Controversies in Economic Policy; The aim of this article is to present elements and discuss the potential of a
research program at the intersection between mathematics and heterodox
economics, which we call Criticial Mathematical Economics (CME). We propose to
focus on the mathematical and model-theoretic foundations of controversies in
economic policy, and aim at providing an entrance to the literature and an
invitation to mathematicians that are potentially interested in such a project.
From our point of view, mathematics has been partly misused in mainstream
economics to justify `unregulated markets' before the financial crisis. We thus
identify two key parts of CME, which leads to a natural structure of this
article: The frst focusses on an analysis and critique of mathematical models
used in mainstream economics, like e.g. the Dynamic Stochastic General
Equilibrium (DSGE) in Macroeconomics and the so-called
""Sonnenschein-Mantel-Debreu""-Theorems. The aim of the second part is to improve
and extend heterodox models using ingredients from modern mathematics and
computer science, a method with strong relation to Complexity Economics. We
exemplify this idea by describing how methods from Non-Linear Dynamics have
been used in what could be called ""The Dynamical Systems approach to
Post-Keynesian Macroeconomics"", and also discuss (Pseudo-) Goodwin cycles and
possible Micro- and Mesofoundations. We conclude by giving an outlook in which
areas a collaboration between mathematicians and heterodox economists could be
most promising. The focus lies on the mathematical and model-theoretic
foundations of controversies in economic policy, and we discuss both existing
projects in such a direction as well as areas where new models for policy
advice are most needed from the perspective of the progressive political left.; 15) Exploring Cosmological Implications of the Modified Raychaudhuri
  Equation in Non-Gravitating Vacuum Energy Theory; This article investigates the modified Raychaudhuri Equation (RE) in the
context of Non-Gravitating Vacuum Energy (NGVE) theory and its implications for
various cosmological characteristics. The equation is formulated based on the
NGVE framework, in which global scale invariance generates a unique geometry.
The newly developed geometry introduces a metric that is conformally connected
to the conventional metric, with the conformal factor dependent on scalar field
potentials. The cosmological study is carried out under the framework of a flat
Friedmann-Lema\^itre-Robertson-Walker (FLRW) universe. Assuming matter behaves
as an ideal fluid in the modified geometry, we formulate models for conditional
expansion, collapse, and steady state, governed by the scalar field ($\phi$).
In this context, the caustic solution and the focusing theorem are also
studied. Scalar field solutions for exponential and power-law scale factors are
also derived using NGVE theory's equations of motion. Finally, graphical
analysis is used to investigate the behavior of the interaction terms that
appear in the modified RE under these scale factors.; 16) Quantum contextuality of spin-1 massive particles; Contextuality is a fundamental property of quantum mechanics. Contrary to
entanglement, which can only exist in composite systems, contextuality is also
present for single entities. The case of a three-level system is of particular
interest because--in agreement with the Bell-Kochen-Specker theorem--it is the
simplest in which quantum contextuality is necessarily present. We verify that
the polarizations of spin-1 massive particles produced at collider experiments
indeed exhibit contextuality. To this purpose we consider $W$ gauge bosons
produced in top-quark decays, $J/\psi$ and $K^{*}(892)^0$ mesons created in
$B$-meson decays and $\phi$ mesons resulting from $\chi^0_c$ charmonium decays,
making use of the data collected and analyzed by the ATLAS, LHCb and BESIII
collaborations, respectively. The polarizations of all these four particles
show contextuality with a significance of more than $5\sigma$.; 17) DDAT: Diffusion Policies Enforcing Dynamically Admissible Robot
  Trajectories; Diffusion models excel at creating images and videos thanks to their
multimodal generative capabilities. These same capabilities have made diffusion
models increasingly popular in robotics research, where they are used for
generating robot motion. However, the stochastic nature of diffusion models is
fundamentally at odds with the precise dynamical equations describing the
feasible motion of robots. Hence, generating dynamically admissible robot
trajectories is a challenge for diffusion models. To alleviate this issue, we
introduce DDAT: Diffusion policies for Dynamically Admissible Trajectories to
generate provably admissible trajectories of black-box robotic systems using
diffusion models. A sequence of states is a dynamically admissible trajectory
if each state of the sequence belongs to the reachable set of its predecessor
by the robot's equations of motion. To generate such trajectories, our
diffusion policies project their predictions onto a dynamically admissible
manifold during both training and inference to align the objective of the
denoiser neural network with the dynamical admissibility constraint. The
auto-regressive nature of these projections along with the black-box nature of
robot dynamics render these projections immensely challenging. We thus enforce
admissibility by iteratively sampling a polytopic under-approximation of the
reachable set of a state onto which we project its predicted successor, before
iterating this process with the projected successor. By producing accurate
trajectories, this projection eliminates the need for diffusion models to
continually replan, enabling one-shot long-horizon trajectory planning. We
demonstrate that our framework generates higher quality dynamically admissible
robot trajectories through extensive simulations on a quadcopter and various
MuJoCo environments, along with real-world experiments on a Unitree GO1 and
GO2.; 18) Rotating and non-linear magnetic-charged black hole with an anisotropic
  matter field; We present the solution of a non-linear magnetic-charged black hole with an
anisotropic matter field and further extend it to obtain the corresponding
rotating black hole solution using the modified Newman-Janis algorithm. The
event horizon and ergosphere of the rotating black hole are studied in terms of
the perspective of geometric properties, revealing that the rotating black hole
can have up to three horizons. The first law of thermodynamics and the
squared-mass formula for the rotating black hole are derived from a
thermodynamic perspective, based on which we obtain the thermodynamic
quantities and study the thermodynamic stability of the rotating black hole.
Additionally, we calculate the Penrose process for the rotating black hole,
indicating the influence of various black hole parameters on the maximal
efficiency of the Penrose process.; 19) Optimizing Wireless Resource Management and Synchronization in Digital
  Twin Networks; In this paper, we investigate an accurate synchronization between a physical
network and its digital network twin (DNT), which serves as a virtual
representation of the physical network. The considered network includes a set
of base stations (BSs) that must allocate its limited spectrum resources to
serve a set of users while also transmitting its partially observed physical
network information to a cloud server to generate the DNT. Since the DNT can
predict the physical network status based on its historical status, the BSs may
not need to send their physical network information at each time slot, allowing
them to conserve spectrum resources to serve the users. However, if the DNT
does not receive the physical network information of the BSs over a large time
period, the DNT's accuracy in representing the physical network may degrade. To
this end, each BS must decide when to send the physical network information to
the cloud server to update the DNT, while also determining the spectrum
resource allocation policy for both DNT synchronization and serving the users.
We formulate this resource allocation task as an optimization problem, aiming
to maximize the total data rate of all users while minimizing the
asynchronization between the physical network and the DNT. To address this
problem, we propose a method based on the GRUs and the value decomposition
network (VDN). Simulation results show that our GRU and VDN based algorithm
improves the weighted sum of data rates and the similarity between the status
of the DNT and the physical network by up to 28.96%, compared to a baseline
method combining GRU with the independent Q learning.; 20) A note on Strong Cosmic Censorship and its violation in
  Reissner-Nordstr\""om de Sitter black hole space-times; Penrose's Strong Cosmic Censorship conjecture safeguards determinism in
General Relativity. Within the initial value approach to General Relativity,
proof of Strong Cosmic Censorship preservation is predicated on the unique
evolution of the metric. For the Kerr-Newman family of black hole solutions,
this requires the inextendability of the metric past the Cauchy horizon, due to
the development of a ""blue-shift"" instability. Attempts to provide a rigorous
mathematical proof of Strong Cosmic Censorship has led to the formulation of
several Strong Cosmic Censorship conjectures of varying strengths, which seem
to be discussed rarely outside of the mathematical relativity literature. In
this note, we review some of the arguments for and against Strong Cosmic
Censorship preservation, with a focus on the Reissner-Nordstr\""om de Sitter
context, where the positive cosmological constant invites a ""red-shift"" effect
that competes against the ""blue-shift"". We study the consequent role of
quasinormal mode behaviour and illustrate the parameter space for which we
consistently observe violations of the Strong Cosmic Censorship conjecture
within Reissner-Nordstr\""om de Sitter black holes.; 21) Scalar fully-charm and bottom tetraquarks under extreme temperatures; Temperature dependences of masses and current couplings of the ground state
of the fully heavy tetraquarks $T_{4c}$ and $T_{4b}$, composed of charm $(c)$
and bottom $(b)$ quarks and antiquarks with spin-parities $J^{PC} = 0^{++}$ are
evaluated in the diquark antidiquark picture using Thermal QCD Sum Rules
including vacuum condensates up to dimension four. The calculated values for
$cc\bar{c}\bar{c}$ and $bb\bar{b}\bar{b}$ tetraquark states at $T=0$ align well
with the experimental data on the broad structures. Based on the numerical
analyses around the critical temperature, the mass of the $T_{4c}$ state
decreases by $8\%$ compared to its vacuum state, while for its b partner, this
percentage is approximately $3.3\%$. For the decay constants, the reductions
are approximately $71\%$ and $66.6\%$, respectively. The precise exploration of
tetraquark states awaits future scrutiny in upcoming experiments, including
Belle II, Super-B, PANDA, and LHCb.; 22) Generalized Scattering Matrix Synthesis for Hybrid Systems with Multiple
  Scatterers and Antennas Using Independent Structure Simulations; This paper presents a unified formulation for calculating the generalized
scattering matrix (GS-matrix) of hybrid systems involving multiple scatterers
and antennas. The GS-matrix of the entire system is synthesized through the
scattering matrices and GS-matrices of each independent component, using the
addition theorem of vector spherical wavefunctions and fully matrix-based
operations. Since our formulation is applicable to general antenna-scatterer
hybrid systems, previous formulas for multiple scattering and antenna arrays
become special cases of our approach. This also establishes our formulation as
a universal domain decomposition method for analyzing the electromagnetic
performance of hybrid systems. We provide numerous numerical examples to
comprehensively demonstrate the capabilities and compatibility of the proposed
formulation, including its potential application in studying the effects of
structural rotation.; 23) Towards an AI co-scientist; Scientific discovery relies on scientists generating novel hypotheses that
undergo rigorous experimental validation. To augment this process, we introduce
an AI co-scientist, a multi-agent system built on Gemini 2.0. The AI
co-scientist is intended to help uncover new, original knowledge and to
formulate demonstrably novel research hypotheses and proposals, building upon
prior evidence and aligned to scientist-provided research objectives and
guidance. The system's design incorporates a generate, debate, and evolve
approach to hypothesis generation, inspired by the scientific method and
accelerated by scaling test-time compute. Key contributions include: (1) a
multi-agent architecture with an asynchronous task execution framework for
flexible compute scaling; (2) a tournament evolution process for self-improving
hypotheses generation. Automated evaluations show continued benefits of
test-time compute, improving hypothesis quality. While general purpose, we
focus development and validation in three biomedical areas: drug repurposing,
novel target discovery, and explaining mechanisms of bacterial evolution and
anti-microbial resistance. For drug repurposing, the system proposes candidates
with promising validation findings, including candidates for acute myeloid
leukemia that show tumor inhibition in vitro at clinically applicable
concentrations. For novel target discovery, the AI co-scientist proposed new
epigenetic targets for liver fibrosis, validated by anti-fibrotic activity and
liver cell regeneration in human hepatic organoids. Finally, the AI
co-scientist recapitulated unpublished experimental results via a parallel in
silico discovery of a novel gene transfer mechanism in bacterial evolution.
These results, detailed in separate, co-timed reports, demonstrate the
potential to augment biomedical and scientific discovery and usher an era of AI
empowered scientists.; 24) PromptArtisan: Multi-instruction Image Editing in Single Pass with
  Complete Attention Control; We present PromptArtisan, a groundbreaking approach to multi-instruction
image editing that achieves remarkable results in a single pass, eliminating
the need for time-consuming iterative refinement. Our method empowers users to
provide multiple editing instructions, each associated with a specific mask
within the image. This flexibility allows for complex edits involving mask
intersections or overlaps, enabling the realization of intricate and nuanced
image transformations. PromptArtisan leverages a pre-trained InstructPix2Pix
model in conjunction with a novel Complete Attention Control Mechanism (CACM).
This mechanism ensures precise adherence to user instructions, granting
fine-grained control over the editing process. Furthermore, our approach is
zero-shot, requiring no additional training, and boasts improved processing
complexity compared to traditional iterative methods. By seamlessly integrating
multi-instruction capabilities, single-pass efficiency, and complete attention
control, PromptArtisan unlocks new possibilities for creative and efficient
image editing workflows, catering to both novice and expert users alike.; 25) Dislocation correlations in GaN epitaxial films revealed by EBSD and XRD; Correlations between dislocations in crystals reduce the elastic energy via
screening of the strain by the surrounding dislocations. We study the
correlations of threading dislocations in GaN epitaxial films with dislocation
densities of 5x10^8 cm^-2 and 1.8x10^10 cm^-2 by X-ray diffraction (XRD) in
reciprocal space and by high-resolution electron backscatter diffraction (EBSD)
in real space, where the strain is derived from a cross-correlation analysis of
the Kikuchi patterns. The measured XRD curves and EBSD strain and rotation maps
are compared with Monte Carlo simulations within one and the same model for the
dislocation distributions. The screening of the dislocation strains is provided
by creating pairs of dislocations with opposite Burgers vectors, with the mean
distance between dislocations in a pair equal to the screening distance. The
pairs overlap and cannot be distinguished as separate dipoles. The
EBSD-measured autocorrelation functions of the strain and rotation components
follow the expected logarithmic law for distances smaller than the screening
distances and become zero for larger distances, which is confirmed by the Monte
Carlo simulations. Screening distances of 2 \textmu m and 0.3 \textmu m are
obtained for the samples with low and high dislocation densities, respectively.
The dislocation strain is thus screened by only 4 neighboring dislocations.
High-resolution EBSD allows for a more precise determination of the screening
distances than from fits of the XRD curves. In addition, an anisotropic
resolution of the EBSD measurements is observed and quantified.; 26) Table as Thought: Exploring Structured Thoughts in LLM Reasoning; Large language models' reasoning abilities benefit from methods that organize
their thought processes, such as chain-of-thought prompting, which employs a
sequential structure to guide the reasoning process step-by-step. However,
existing approaches focus primarily on organizing the sequence of thoughts,
leaving structure in individual thought steps underexplored. To address this
gap, we propose Table as Thought, a framework inspired by cognitive
neuroscience theories on human thought. Table as Thought organizes reasoning
within a tabular schema, where rows represent sequential thought steps and
columns capture critical constraints and contextual information to enhance
reasoning. The reasoning process iteratively populates the table until
self-verification ensures completeness and correctness. Our experiments show
that Table as Thought excels in planning tasks and demonstrates a strong
potential for enhancing LLM performance in mathematical reasoning compared to
unstructured thought baselines. This work provides a novel exploration of
refining thought representation within LLMs, paving the way for advancements in
reasoning and AI cognition.; 27) Multiphoton fluorescence excitation with real intermediary states; We demonstrate fluorescence generation through a sequential multiphoton
process with real intermediary states. Our findings suggest new directions for
optical control of previously unexplored molecular excitation pathways.; 28) Feather-SQL: A Lightweight NL2SQL Framework with Dual-Model
  Collaboration Paradigm for Small Language Models; Natural Language to SQL (NL2SQL) has seen significant advancements with large
language models (LLMs). However, these models often depend on closed-source
systems and high computational resources, posing challenges in data privacy and
deployment. In contrast, small language models (SLMs) struggle with NL2SQL
tasks, exhibiting poor performance and incompatibility with existing
frameworks. To address these issues, we introduce Feather-SQL, a new
lightweight framework tailored for SLMs. Feather-SQL improves SQL executability
and accuracy through 1) schema pruning and linking, 2) multi-path and
multi-candidate generation. Additionally, we introduce the 1+1 Model
Collaboration Paradigm, which pairs a strong general-purpose chat model with a
fine-tuned SQL specialist, combining strong analytical reasoning with
high-precision SQL generation. Experimental results on BIRD demonstrate that
Feather-SQL improves NL2SQL performance on SLMs, with around 10% boost for
models without fine-tuning. The proposed paradigm raises the accuracy ceiling
of SLMs to 54.76%, highlighting its effectiveness.; 29) Connecting a Magnetized Disk to a Convective Low-mass Protostar: A
  Global Three-dimensional Model of Boundary Layer Accretion; In the early stages of star formation, boundary layer accretion, where
protostars accrete material from disks extending down to their surfaces, plays
a crucial role. Understanding how a magneto-rotational-instability (MRI)-active
disk connects to a protostar's surface remains a significant challenge. To
investigate the mechanisms of mass and angular momentum transfer, we develop a
global, three-dimensional magnetohydrodynamic model of boundary layer accretion
around a magnetized, convective low-mass protostar. Our results reveal that
angular momentum transport mechanisms transition significantly from the outer
MRI-active disk to the protostellar surface. Various mechanisms--MRI, spiral
shocks, coronal accretion, jets, and disk winds--contribute to angular momentum
transfer, resulting in three distinct disk structures: (1) the MRI-active disk,
(2) the transition layer, and (3) the boundary layer. The simulated protostar
is strongly magnetized due to the accumulation of the disk fields, wrapping by
disk toroidal fields, and stellar dynamo activity. Magnetic concentrations
analogous to starspots form on the protostar and interact with the rotating
disk gas to generate spiral shocks. These shocks play a key role in driving
accretion. These findings demonstrate the necessity of global MHD models for a
comprehensive understanding of angular momentum transport. Additionally, we
identify explosive events triggered by magnetic reconnection in both the
protostar and the disk atmosphere. We also find decretion flows in the disk
midplane, which may be important for the radial transport of refractory
materials, such as Calcium-Aluminium-rich Inclusions (CAIs) precursor gas, to
the outer disk.; 30) Bridging Contrastive Learning and Domain Adaptation: Theoretical
  Perspective and Practical Application; This work studies the relationship between Contrastive Learning and Domain
Adaptation from a theoretical perspective. The two standard contrastive losses,
NT-Xent loss (Self-supervised) and Supervised Contrastive loss, are related to
the Class-wise Mean Maximum Discrepancy (CMMD), a dissimilarity measure widely
used for Domain Adaptation. Our work shows that minimizing the contrastive
losses decreases the CMMD and simultaneously improves class-separability,
laying the theoretical groundwork for the use of Contrastive Learning in the
context of Domain Adaptation. Due to the relevance of Domain Adaptation in
medical imaging, we focused the experiments on mammography images. Extensive
experiments on three mammography datasets - synthetic patches, clinical (real)
patches, and clinical (real) images - show improved Domain Adaptation,
class-separability, and classification performance, when minimizing the
Supervised Contrastive loss.; 31) Quantum autoencoders for image classification; Classical machine learning often struggles with complex, high-dimensional
data. Quantum machine learning offers a potential solution, promising more
efficient processing. While the quantum convolutional neural network (QCNN), a
hybrid quantum-classical algorithm, is suitable for current noisy
intermediate-scale quantum-era hardware, its learning process relies heavily on
classical computation. Future large-scale, gate-based quantum computers could
unlock the full potential of quantum effects in machine learning. In contrast
to QCNNs, quantum autoencoders (QAEs) leverage classical optimization solely
for parameter tuning. Data compression and reconstruction are handled entirely
within quantum circuits, enabling purely quantum-based feature extraction. This
study introduces a novel image-classification approach using QAEs, achieving
classification without requiring additional qubits compared with conventional
QAE implementations. The quantum circuit structure significantly impacts
classification accuracy. Unlike hybrid methods such as QCNN, QAE-based
classification emphasizes quantum computation. Our experiments demonstrate high
accuracy in a four-class classification task, evaluating various quantum-gate
configurations to understand the impact of different parameterized quantum
circuit (ansatz) structures on classification performance. Our results reveal
that specific ansatz structures achieve superior accuracy, and we provide an
analysis of their effectiveness. Moreover, the proposed approach achieves
performance comparable to that of conventional machine-learning methods while
significantly reducing the number of parameters requiring optimization. These
findings indicate that QAEs can serve as efficient classification models with
fewer parameters and highlight the potential of utilizing quantum circuits for
complete end-to-end learning, a departure from hybrid approaches such as QCNN.; 32) Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language
  Model; Generalized few-shot 3D point cloud segmentation (GFS-PCS) adapts models to
new classes with few support samples while retaining base class segmentation.
Existing GFS-PCS methods enhance prototypes via interacting with support or
query features but remain limited by sparse knowledge from few-shot samples.
Meanwhile, 3D vision-language models (3D VLMs), generalizing across open-world
novel classes, contain rich but noisy novel class knowledge. In this work, we
introduce a GFS-PCS framework that synergizes dense but noisy pseudo-labels
from 3D VLMs with precise yet sparse few-shot samples to maximize the strengths
of both, named GFS-VL. Specifically, we present a prototype-guided pseudo-label
selection to filter low-quality regions, followed by an adaptive infilling
strategy that combines knowledge from pseudo-label contexts and few-shot
samples to adaptively label the filtered, unlabeled areas. Additionally, we
design a novel-base mix strategy to embed few-shot samples into training
scenes, preserving essential context for improved novel class learning.
Moreover, recognizing the limited diversity in current GFS-PCS benchmarks, we
introduce two challenging benchmarks with diverse novel classes for
comprehensive generalization evaluation. Experiments validate the effectiveness
of our framework across models and datasets. Our approach and benchmarks
provide a solid foundation for advancing GFS-PCS in the real world. The code is
at https://github.com/ZhaochongAn/GFS-VL; 33) Combining Flow Matching and Transformers for Efficient Solution of
  Bayesian Inverse Problems; Solving Bayesian inverse problems efficiently remains a significant challenge
due to the complexity of posterior distributions and the computational cost of
traditional sampling methods. Given a series of observations and the forward
model, we want to recover the distribution of the parameters, conditioned on
observed experimental data. We show, that combining Conditional Flow Mathching
(CFM) with transformer-based architecture, we can efficiently sample from such
kind of distribution, conditioned on variable number of observations.; 34) Efficiently Solving Discounted MDPs with Predictions on Transition
  Matrices; We study infinite-horizon Discounted Markov Decision Processes (DMDPs) under
a generative model. Motivated by the Algorithm with Advice framework
Mitzenmacher and Vassilvitskii 2022, we propose a novel framework to
investigate how a prediction on the transition matrix can enhance the sample
efficiency in solving DMDPs and improve sample complexity bounds. We focus on
the DMDPs with $N$ state-action pairs and discounted factor $\gamma$. Firstly,
we provide an impossibility result that, without prior knowledge of the
prediction accuracy, no sampling policy can compute an $\epsilon$-optimal
policy with a sample complexity bound better than $\tilde{O}((1-\gamma)^{-3}
N\epsilon^{-2})$, which matches the state-of-the-art minimax sample complexity
bound with no prediction. In complement, we propose an algorithm based on
minimax optimization techniques that leverages the prediction on the transition
matrix. Our algorithm achieves a sample complexity bound depending on the
prediction error, and the bound is uniformly better than
$\tilde{O}((1-\gamma)^{-4} N \epsilon^{-2})$, the previous best result derived
from convex optimization methods. These theoretical findings are further
supported by our numerical experiments.; 35) Discovery of Radio Recombination Lines from Proplyds in the Orion Nebula
  Cluster; We present new Atacama Large Millimeter/submillimeter Array observations
that, for the first time, detect hydrogen and helium radio recombination lines
from a protoplanetary disk. We imaged the Orion Nebula Cluster at 3.1 mm with a
spectral setup that covered the $n=42 \rightarrow 41$ transitions of hydrogen
(H41$\alpha$) and helium (He41$\alpha$). The unprecedented sensitivity of these
observations enables us to search for radio recombination lines toward the
positions of ${\sim}200$ protoplanetary disks. We detect H41$\alpha$ from 17
disks, all of which are HST-identified `proplyds.' The detected H41$\alpha$
emission is spatially coincident with the locations of proplyd ionization
fronts, indicating that proplyd H41$\alpha$ emission is produced by gas that
has been photoevaporated off the disk and ionized by UV radiation from massive
stars. We measure the fluxes and widths of the detected H41$\alpha$ lines and
find line fluxes of ${\sim}30-800$ mJy km s$^{-1}$ and line widths of
${\sim}30-90$ km s$^{-1}$. The derived line widths indicate that the broadening
of proplyd H41$\alpha$ emission is dominated by outflowing gas motions
associated with external photoevaporation. The derived line fluxes, when
compared with measurements of 3.1 mm free-free flux, imply that the ionization
fronts of H41$\alpha$-detected proplyds have electron temperatures of
${\sim}6,000-11,000$ K and electron densities of ${\sim}10^6-10^7$ cm$^{-3}$.
Finally, we detect He41$\alpha$ towards one H41$\alpha$-detected source and
find evidence that this system is helium-rich. Our study demonstrates that
radio recombination lines are readily detectable in ionized photoevaporating
disks, providing a new way to measure disk properties in clustered star-forming
regions.; 36) Combinatorial Ricci Flow and Thurston's Triangulation Conjecture; Thurston's triangulation conjecture asserts that every hyperbolic 3-manifold
admits a geometric decomposition into ideal hyperbolic tetrahedra, a result
proven only for certain special 3-manifolds. This paper presents combinatorial
Ricci flow as a systematic and general approach to addressing Thurston's
triangulation conjecture, showing that the flow converges if and only if the
triangulation is geometric. First, we prove the rigidity of the most general
hyperbolic polyhedral 3-manifolds constructed by isometrically gluing partially
truncated and decorated hyperbolic tetrahedra, demonstrating that the metrics
are uniquely determined by cone angles modulo isometry and decoration changes.
Then, we demonstrate that combinatorial Ricci flow evolves polyhedral metrics
toward complete hyperbolic structures with geometric decompositions when
convergent. Conversely, the existence of a geometric triangulation guarantees
flow convergence.; 37) ""It felt more real"": Investigating the User Experience of the MiWaves
  Personalizing JITAI Pilot Study; Cannabis use among emerging adults is increasing globally, posing significant
health risks and creating a need for effective interventions. We present an
exploratory analysis of the MiWaves pilot study, a digital intervention aimed
at supporting cannabis use reduction among emerging adults (ages 18-25). Our
findings indicate the potential of self-monitoring check-ins and trend
visualizations in fostering self-awareness and promoting behavioral reflection
in participants. MiWaves intervention message timing and frequency were also
generally well-received by the participants. The participants' perception of
effort were queried on intervention messages with different tasks, and our
findings suggest that messages with tasks like exploring links and typing in
responses are perceived as requiring more effort as compared to messages with
tasks involving reading and acknowledging. Finally, we discuss the findings and
limitations from this study and analysis, and their impact on informing future
iterations on MiWaves.; 38) Punctuation patterns in ""Finnegans Wake"" by James Joyce are largely
  translation-invariant; The complexity characteristics of texts written in natural languages are
significantly related to the rules of punctuation. In particular, the distances
between punctuation marks measured by the number of words quite universally
follow the family of Weibull distributions known from survival analyses.
However, the values of two parameters marking specific forms of these
distributions distinguish specific languages. This is such a strong constraint
that the punctuation distributions of texts translated from the original
language into another adopt quantitative characteristics of the target
language. All these changes take place within Weibull distributions such that
the corresponding hazard functions are always increasing. Recent previous
research shows that James Joyce's famous ""Finnegans Wake"" is subject to such
extreme distribution from the Weibull family that the corresponding hazard
function is clearly decreasing. At the same time, the distances of sentence
ending punctuation marks, determining the variability of sentence length, have
an almost perfect multifractal organization, so far to such an extent found
nowhere else in the literature. In the present contribution based on several
available translations (Dutch, French, German, Polish, Russian) of ""Finnegans
Wake"", it is shown that the punctuation characteristics of this work remain
largely translation invariant, contrary to the common cases. These observations
may constitute further evidence that ""Finnegans Wake"" is a translinguistic work
in this respect as well, in line with Joyce's original intention.; 39) TGV: Tabular Data-Guided Learning of Visual Cardiac Representations; Contrastive learning methods in computer vision typically rely on different
views of the same image to form pairs. However, in medical imaging, we often
seek to compare entire patients with different phenotypes rather than just
multiple augmentations of one scan. We propose harnessing clinically relevant
tabular data to identify distinct patient phenotypes and form more meaningful
pairs in a contrastive learning framework. Our method uses tabular attributes
to guide the training of visual representations, without requiring a joint
embedding space. We demonstrate its strength using short-axis cardiac MR images
and clinical attributes from the UK Biobank, where tabular data helps to more
effectively distinguish between patient subgroups. Evaluation on downstream
tasks, including fine-tuning and zero-shot prediction of cardiovascular artery
diseases and cardiac phenotypes, shows that incorporating tabular data yields
stronger visual representations than conventional methods that rely solely on
image augmentations or combined image-tabular embeddings. Furthermore, we
demonstrate that image encoders trained with tabular guidance are capable of
embedding demographic information in their representations, allowing them to
use insights from tabular data for unimodal predictions, making them
well-suited to real-world medical settings where extensive clinical annotations
may not be routinely available at inference time. The code will be available on
GitHub.; 40) Can LLM Agents Maintain a Persona in Discourse?; Large Language Models (LLMs) are widely used as conversational agents,
exploiting their capabilities in various sectors such as education, law,
medicine, and more. However, LLMs are often subjected to context-shifting
behaviour, resulting in a lack of consistent and interpretable
personality-aligned interactions. Adherence to psychological traits lacks
comprehensive analysis, especially in the case of dyadic (pairwise)
conversations. We examine this challenge from two viewpoints, initially using
two conversation agents to generate a discourse on a certain topic with an
assigned personality from the OCEAN framework (Openness, Conscientiousness,
Extraversion, Agreeableness, and Neuroticism) as High/Low for each trait. This
is followed by using multiple judge agents to infer the original traits
assigned to explore prediction consistency, inter-model agreement, and
alignment with the assigned personality. Our findings indicate that while LLMs
can be guided toward personality-driven dialogue, their ability to maintain
personality traits varies significantly depending on the combination of models
and discourse settings. These inconsistencies emphasise the challenges in
achieving stable and interpretable personality-aligned interactions in LLMs.; 41) French Onion Soup, Ipelets for Points and Polygons; There are many structures, both classical and modern, involving point-sets
and polygons whose deeper understanding can be facilitated through interactive
visualizations. The Ipe extensible drawing editor, developed by Otfried Cheong,
is a widely used software system for generating geometric figures. One of its
features is the capability to extend its functionality through programs called
Ipelets. In this media submission, we showcase a collection of new Ipelets that
construct a variety of geometric based structures based on point sets and
polygons. These include quadtrees, trapezoidal maps, beta skeletons, floating
bodies of convex polygons, onion graphs, fractals (Sierpi\'nski triangle and
carpet), simple polygon triangulations, and random point sets in simple
polygons. All of our Ipelets are programmed in Lua and are freely available.; 42) A Law Reasoning Benchmark for LLM with Tree-Organized Structures
  including Factum Probandum, Evidence and Experiences; While progress has been made in legal applications, law reasoning, crucial
for fair adjudication, remains unexplored. We propose a transparent law
reasoning schema enriched with hierarchical factum probandum, evidence, and
implicit experience, enabling public scrutiny and preventing bias. Inspired by
this schema, we introduce the challenging task, which takes a textual case
description and outputs a hierarchical structure justifying the final decision.
We also create the first crowd-sourced dataset for this task, enabling
comprehensive evaluation. Simultaneously, we propose an agent framework that
employs a comprehensive suite of legal analysis tools to address the challenge
task. This benchmark paves the way for transparent and accountable AI-assisted
law reasoning in the ``Intelligent Court''.; 43) A Comprehensive Experimentation Framework for Energy-Efficient Design of
  Cloud-Native Applications; Current approaches to designing energy-efficient applications typically rely
on measuring individual components using readily available local metrics, like
CPU utilization. However, these metrics fall short when applied to cloud-native
applications, which operate within the multi-tenant, shared environments of
distributed cloud providers. Assessing and optimizing the energy efficiency of
cloud-native applications requires consideration of the complex, layered nature
of modern cloud stacks.
  To address this need, we present a comprehensive, automated, and extensible
experimentation framework that enables developers to measure energy efficiency
across all relevant layers of a cloud-based application and evaluate associated
quality trade-offs. Our framework integrates a suite of service quality and
sustainability metrics, providing compatibility with any Kubernetes-based
application. We demonstrate the feasibility and effectiveness of this approach
through initial experimental results, comparing architectural design
alternatives for a widely used open-source cloud-native application.; 44) RAGO: Systematic Performance Optimization for Retrieval-Augmented
  Generation Serving; Retrieval-augmented generation (RAG), which combines large language models
(LLMs) with retrievals from external knowledge databases, is emerging as a
popular approach for reliable LLM serving. However, efficient RAG serving
remains an open challenge due to the rapid emergence of many RAG variants and
the substantial differences in workload characteristics across them. In this
paper, we make three fundamental contributions to advancing RAG serving. First,
we introduce RAGSchema, a structured abstraction that captures the wide range
of RAG algorithms, serving as a foundation for performance optimization.
Second, we analyze several representative RAG workloads with distinct
RAGSchema, revealing significant performance variability across these
workloads. Third, to address this variability and meet diverse performance
requirements, we propose RAGO (Retrieval-Augmented Generation Optimizer), a
system optimization framework for efficient RAG serving. Our evaluation shows
that RAGO achieves up to a 2x increase in QPS per chip and a 55% reduction in
time-to-first-token latency compared to RAG systems built on LLM-system
extensions.; 45) Collect, Commit, Expand: Efficient CPQR-Based Column Selection for
  Extremely Wide Matrices; Column-pivoted QR (CPQR) factorization is a computational primitive used in
numerous applications that require selecting a small set of ``representative''
columns from a much larger matrix. These include applications in spectral
clustering, model-order reduction, low-rank approximation, and computational
quantum chemistry, where the matrix being factorized has a moderate number of
rows but an extremely large number of columns. We describe a modification of
the Golub-Businger algorithm which, for many matrices of this type, can perform
CPQR-based column selection much more efficiently. This algorithm, which we
call CCEQR, is based on a three-step ``collect, commit, expand'' strategy that
limits the number of columns being manipulated, while also transferring more
computational effort from level-2 BLAS to level-3. Unlike most CPQR algorithms
that exploit level-3 BLAS, CCEQR is deterministic, and provably recovers a
column permutation equivalent to the one computed by the Golub-Businger
algorithm. Tests on spectral clustering and Wannier basis localization problems
demonstrate that on appropriately structured problems, CCEQR can significantly
outperform GEQP3.; 46) Integrated Multiphysics Modeling of a Piezoelectric Micropump; This paper presents an integrated multiphysics simulation approach of
piezoelectric micropumps. Micropumps and micro blowers are essential devices in
various cutting-edge industries like laboratory equipment, medical devices, and
fuel cells. A piezoelectric micropump involves complex physics including
microfluidics, flow-structure interaction, electricity, and piezoelectric
material. Hence, a comprehensive analysis of the interactions between different
physical phenomena, would be essential for the effective design and
optimization of these micropumps. Prior studies on piezoelectric micropump were
mainly focused on isolated physical aspects of these pumps, such as
piezoelectric mechanics, fluid dynamics, electrical properties, and also
fluid-structure Interactions. The present paper fills this gap by integrating
these aspects into a holistic simulation and design approach, introducing a new
methodology for micropump analysis. Advanced simulation and design tools like
COMSOL and SolidWorks were employed in accordance. A brief review of
piezoelectric materials, and an exploration of different types of micropumps
and their operating principles is discussed. Also, a comparison of various
piezoelectric materials, including their properties and applications is
investigated. Further, the paper discusses the simulation process of the
micropumps, using COMSOL software, and presents an in-depth analysis of the
simulation results. This structured approach provides a comprehensive
understanding of piezoelectric micropumps, from theoretical underpinnings to
practical design considerations. ..; 47) Direct Expression for One-Loop Tensor Reduction with Lorentz Indices via
  Generating Function; In recent work, we derived a direct expression for one-loop tensor reduction
using generating functions and Feynman parametrization in projective space,
avoiding recursive relations. However, for practical applications, this
expression still presents two challenges: (1) While the final reduction
coefficients are expressed in terms of the dimension D and Mandelstam
variables, the given expression explicitly contains irrational functions; (2)
The expression involves an auxiliary vector R, which can be eliminated via
differentiation $\frac{\partial}{\partial R}$, but the presence of irrational
terms making differentiation cumbersome. (3) Most practical applications
require the tensor form with Lorentz indices.
  In this paper, we provide a rational form of the reduction coefficients with
Lorentz indices, free from recursion. Additionally, We provide a pure Wolfram
Mathematica implementation of the code. Our practical tests demonstrate that
this direct expression achieves significantly higher computational efficiency
compared to the traditional Passarino-Veltman (PV) reduction or other
recursion-based methods.; 48) Gradient-Based Multi-Objective Deep Learning: Algorithms, Theories,
  Applications, and Beyond; Multi-objective optimization (MOO) in deep learning aims to simultaneously
optimize multiple conflicting objectives, a challenge frequently encountered in
areas like multi-task learning and multi-criteria learning. Recent advancements
in gradient-based MOO methods have enabled the discovery of diverse types of
solutions, ranging from a single balanced solution to finite or even infinite
Pareto sets, tailored to user needs. These developments have broad applications
across domains such as reinforcement learning, computer vision, recommendation
systems, and large language models. This survey provides the first
comprehensive review of gradient-based MOO in deep learning, covering
algorithms, theories, and practical applications. By unifying various
approaches and identifying critical challenges, it serves as a foundational
resource for driving innovation in this evolving field. A comprehensive list of
MOO algorithms in deep learning is available at
https://github.com/Baijiong-Lin/Awesome-Multi-Objective-Deep-Learning.; 49) Generation of reusable learning objects from digital medical
  collections: An analysis based on the MASMDOA framework; Learning Objects represent a widespread approach to structuring instructional
materials in a large variety of educational contexts. The main aim of this work
consists of analyzing from a qualitative point of view the process of
generating reusable learning objects (RLOs) followed by Clavy, a tool that can
be used to retrieve data from multiple medical knowledge sources and
reconfigure such sources in diverse multimedia-based structures and
organizations. From these organizations, Clavy is able to generate learning
objects which can be adapted to various instructional healthcare scenarios with
several types of user profiles and distinct learning requirements. Moreover,
Clavy provides the capability of exporting these learning objects through
educational standard specifications, which improves their reusability features.
The analysis insights highlight the importance of having a tool able to
transfer knowledge from the available digital medical collections to learning
objects that can be easily accessed by medical students and healthcare
practitioners through the most popular e-learning platforms.; 50) Maximal regularity estimates for the abstract Cauchy problems; In this work, we extend the Da Prato-Grisvard theory of maximal regularity
estimates for sectorial operators in interpolation spaces. Specifically, for
any generator $-A$ of an analytic semigroup on a Banach space $X$, we identify
the interpolation spaces between $X$ and the domain $D_{A}$ of $A$ in which the
part of $A$ satisfies certain maximal regularity estimates. We also establish
several new results concerning both homogeneous and inhomogeneous $L^1$-maximal
regularity estimates, extending and completing recent findings in the
literature. These results are motivated not only by applications to problems in
areas such as fluid mechanics but also by the intrinsic theoretical interest of
the subject. In particular, we address the optimal choice of data spaces for
the Cauchy problem associated with $A$, ensuring the existence of strong
solutions with global-in-time control of their derivatives. This control is
measured via the homogeneous parts of the interpolation norms in the spatial
variable and weighted Lebesgue norms over the time interval. Furthermore, we
characterize weighted $L^1$-estimates and establish their relationship with
unweighted estimates. Additionally, we reformulate the characterization
condition for $L^1$-maximal regularity due to Kalton and Portal in a priori
terms that do not rely on semigroup operators. Finally, we introduce a new
interpolation framework for $L^p$-maximal regularity estimates, where $p \in
(1, \infty)$, within interpolation spaces generated by non-classical
interpolation functors.; 51) HRAvatar: High-Quality and Relightable Gaussian Head Avatar; Reconstructing animatable and high-quality 3D head avatars from monocular
videos, especially with realistic relighting, is a valuable task. However, the
limited information from single-view input, combined with the complex head
poses and facial movements, makes this challenging. Previous methods achieve
real-time performance by combining 3D Gaussian Splatting with a parametric head
model, but the resulting head quality suffers from inaccurate face tracking and
limited expressiveness of the deformation model. These methods also fail to
produce realistic effects under novel lighting conditions. To address these
issues, we propose HRAvatar, a 3DGS-based method that reconstructs
high-fidelity, relightable 3D head avatars. HRAvatar reduces tracking errors
through end-to-end optimization and better captures individual facial
deformations using learnable blendshapes and learnable linear blend skinning.
Additionally, it decomposes head appearance into several physical properties
and incorporates physically-based shading to account for environmental
lighting. Extensive experiments demonstrate that HRAvatar not only reconstructs
superior-quality heads but also achieves realistic visual effects under varying
lighting conditions.; 52) Habitable Worlds Formed at Cosmic Dawn; Primordial supernovae were the first, great nucleosynthetic engines in the
Universe, forging the elements required for the later formation of planets and
life. Here we show that planetesimals, the precursors of terrestrial planets,
formed around low-mass stars in the debris of the first cosmic explosions 200
Myr after the Big Bang, before the first galaxies and far earlier than
previously thought. A dense core in one of these explosions collapsed to a
protoplanetary disk in which several Earth masses of planetesimals formed 0.46
- 1.66 AU from their parent 0.7 M$_{\odot}$ star, where equilibrium
temperatures varied from 269 K to 186 K, in water mass fractions that were only
a factor of a few less than in the Solar System today. Habitable worlds thus
formed among the first generation of stars in the Universe, before the advent
of the first galaxies.; 53) Deviance Detection and Regularity Sensitivity in Dissociated Neuronal
  Cultures; Understanding how neural networks process complex patterns of information is
crucial for advancing both neuroscience and artificial intelligence. To
investigate fundamental principles of neural computation, we studied
dissociated neuronal cultures, one of the most primitive living neural
networks, on high-resolution CMOS microelectrode arrays and tested whether the
dissociated culture exhibits regularity sensitivity beyond mere
stimulus-specific adaptation and deviance detection. In oddball electrical
stimulation paradigms, we confirmed that the neuronal culture produced mismatch
responses (MMRs) with true deviance detection beyond mere adaptation. These
MMRs were dependent on the N-methyl-D-aspartate (NMDA) receptors, similar to
mismatch negativity (MMN) in humans, which is known to have true deviance
detection properties. Crucially, we also showed sensitivity to the statistical
regularity of stimuli, a phenomenon previously observed only in intact brains:
the MMRs in a predictable, periodic sequence were smaller than those in a
commonly used sequence in which the appearance of the deviant stimulus was
random and unpredictable. These results challenge the traditional view that a
hierarchically structured neural network is required to process complex
temporal patterns, suggesting instead that deviant detection and regularity
sensitivity are inherent properties arising from the primitive neural network.
They also suggest new directions for the development of neuro-inspired
artificial intelligence systems, emphasizing the importance of incorporating
adaptive mechanisms and temporal dynamics in the design of neural networks.; 54) InstructAgent: Building User Controllable Recommender via LLM Agent; Traditional recommender systems usually take the user-platform paradigm,
where users are directly exposed under the control of the platform's
recommendation algorithms. However, the defect of recommendation algorithms may
put users in very vulnerable positions under this paradigm. First, many
sophisticated models are often designed with commercial objectives in mind,
focusing on the platform's benefits, which may hinder their ability to protect
and capture users' true interests. Second, these models are typically optimized
using data from all users, which may overlook individual user's preferences.
Due to these shortcomings, users may experience several disadvantages under the
traditional user-platform direct exposure paradigm, such as lack of control
over the recommender system, potential manipulation by the platform, echo
chamber effects, or lack of personalization for less active users due to the
dominance of active users during collaborative learning. Therefore, there is an
urgent need to develop a new paradigm to protect user interests and alleviate
these issues. Recently, some researchers have introduced LLM agents to simulate
user behaviors, these approaches primarily aim to optimize platform-side
performance, leaving core issues in recommender systems unresolved. To address
these limitations, we propose a new user-agent-platform paradigm, where agent
serves as the protective shield between user and recommender system that
enables indirect exposure. To this end, we first construct four recommendation
datasets, denoted as $\dataset$, along with user instructions for each record.; 55) Key Historical Experiments in Hadron Physics; The experimental observations that led to the quark structure of matter and
the development of hadron physics are reviewed with emphasis on the discoveries
of mesons and baryons, starting in the 1940s with the pion and kaon which
mediate the strong hadronic force. The evidence for an internal structure of
the hadrons consisting of two or three elementary spin 1/2 particles is
reviewed. The discoveries of hadrons made of the heavier charm and bottom
quarks are described. In 2003 more complex multi-quark hadrons began to emerge.
The subsequent developments beyond the early 2000s are covered in the Review of
Particle Physics (Phys. Rev. D 110 (2024) 030001). Given the very large number
of observed hadrons, the choice of key experiments is somewhat subjective.; 56) Entanglement entropy evolution during gravitational collapse; We investigate the dynamics of the ground state entanglement entropy for a
discretized scalar field propagating within the Oppenheimer-Snyder collapse
metric. Starting from a well-controlled initial configuration, we follow the
system as it evolves toward the formation of a horizon and, eventually, a
singularity. Our approach employs an Ermakov-like equation to determine the
time-dependent ground state of the field and calculates the resulting
entanglement entropy by tracing out the degrees of freedom inside a spherical
region within the matter sphere. We find that the entanglement entropy exhibits
nontrivial scaling and time dependence during collapse. Close to the horizon,
the entropy can deviate from the simple area law, reflecting the rapid changes
in geometry and field configuration. Although the model is idealized, these
results provide insights into the generation and scaling of entanglement in the
presence of realistic, dynamically evolving gravitational fields.; 57) De Finetti's problem with fixed transaction costs and regime switching; In this paper, we examine a modified version of de Finetti's optimal dividend
problem, incorporating fixed transaction costs and altering the surplus process
by introducing two-valued drift and two-valued volatility coefficients. This
modification aims to capture the transitions or adjustments in the company's
financial status. We identify the optimal dividend strategy, which maximizes
the expected total net dividend payments (after accounting for transaction
costs) until ruin, as a two-barrier impulsive dividend strategy. Notably, the
optimal strategy can be explicitly determined for almost all scenarios
involving different drifts and volatility coefficients. Our primary focus is on
exploring how changes in drift and volatility coefficients influence the
optimal dividend strategy.; 58) One-dimensional confined Rashba states in a two-dimensional
  Si$_{2}$Bi$_{2}$ induced by vacancy line defects; Advanced defect engineering techniques have enabled the creation of unique
quantum phases from pristine materials. One-dimensional (1D) atomic defects in
low-dimensional systems are particularly intriguing due to their distinct
quantum properties, such as 1D Rashba states that allow for the generation of
nondissipative spin currents, making them ideal for spintronic devices. Using
density-functional calculations and model-based symmetry analysis, we report
the emergence of 1D Rashba states in a two-dimensional Si$_{2}$Bi$_{2}$
monolayer (ML) with vacancy line defects (VLDs). We show that introducing VLDs
in the Si$_{2}$Bi$_{2}$ ML induces 1D confined defect states near the Fermi
level, which are strongly localized along the extended defect line. Notably, we
observed 1D Rashba spin-split bands in these defect states with significant
spin splitting originating mainly from the strong $p-p$ coupling orbitals
between Si and Bi atoms near the defect sites. These spin-split defect states
exhibit perfectly collinear spin polarization in momentum $\vec{k}$-space,
which is oriented perpendicularly to the VLD orientation. Moreover, using
$\vec{k}\cdot\vec{p}$ perturbation theory supplemented with symmetry analysis,
we show that the 1D Rashba states with collinear spin polarization are enforced
by the lowering of symmetry of the VLDs into the $C_{s}$ point group, which
retains the $M_{xz}$ mirror symmetry along with the 1D nature of the VLDs. The
observed 1D Rashba states in this system protect carriers against spin
decoherence and support an exceptionally long spin lifetime, which could be
promising for developing highly efficient spintronic devices.; 59) The cross-over from viscous to inertial lengthscales in rapidly-rotating
  convection; Convection is the main heat transport mechanism in the Earth's liquid core
and is thought to power the dynamo that generates the geomagnetic field. Core
convection is strongly constrained by rotation while being turbulent. Given the
difficulty in modelling these conditions, some key properties of core
convection are still debated, including the dominant energy-carrying
lengthscale. Different regimes of rapidly-rotating, unmagnetised, turbulent
convection exist depending on the importance of viscous and inertial forces in
the dynamics, and hence different theoretical predictions for the dominant flow
lengthscale have been proposed. Here we study the transition from
viscously-dominated to inertia-dominated regimes using numerical simulations in
spherical and planar geometries. We find that the cross-over occurs when the
inertial lengthscale approximately equals the viscous lengthscale. This
suggests that core convection in the absence of magnetic fields is dominated by
the inertial scale, which is hundred times larger than the viscous scale.; 60) A Multi-tiered Solution for Personalized Baggage Item Recommendations
  using FastText and Association Rule Mining; This paper introduces an intelligent baggage item recommendation system to
optimize packing for air travelers by providing tailored suggestions based on
specific travel needs and destinations. Using FastText word embeddings and
Association Rule Mining (ARM), the system ensures efficient luggage space
utilization, compliance with weight limits, and an enhanced travel experience.
The methodology comprises four phases: (1) data collection and preprocessing
with pre-trained FastText embeddings for text representation and similarity
scoring (2) a content-based recommendation system enriched by user search
history (3) application of ARM to user interactions to uncover meaningful item
associations and (4) integration of FastText and ARM for accurate, personalized
recommendations. Performance is evaluated using metrics such as coverage,
support, confidence, lift, leverage, and conviction. Results demonstrate the
system's effectiveness in providing relevant suggestions, improving customer
satisfaction, and simplifying the packing process. These insights advance
personalized recommendations, targeted marketing, and product optimization in
air travel and beyond.; 61) $\phi$ meson in nuclear matter and atomic nuclei; The properties (masses and decay widths) of the $\phi$ meson are investigated
in nuclear matter from the $\phi$ meson self-energy, using the tree-level $\phi
K\bar{K}$ Lagrangian, and, incorporating in-medium masses of (anti)kaons
calculated within the quark meson coupling (QMC) model. These mass shifts and
decay widths are incorporated in the Breit-Wigner spectral function of the
$\phi$ meson to calculate the production cross-section of $\phi$ in asymmetric
nuclear matter. Considerable modifications to the production cross-section are
observed at normal nuclear matter density, driven by the in-medium mass
reduction and the increase in the decay width of $\phi$ meson. The potential
experienced by $\phi$ meson in nuclear matter is used to study the possibility
of formation of the $\phi$ mesic bound state with atomic nuclei. We explore the
potential formation of $\phi$-mesic bound states in ${\rm{^{4}He}}$,
${\rm{^{12}C}}$, ${\rm{^{16}O}}$, ${\rm{^{40}Ca}}$, ${\rm{^{90}Zr}}$,
${\rm{^{197}Au}}$ and ${\rm{^{208}Pb}}$ nuclei by investigating their binding
energies and absorption widths based on the corresponding $\phi$-nucleus
potentials. Our study shows shallow bound states with the light nuclei and
deeply bound states in heavy nuclei. Among the investigated nuclei, a
particularly distinct signal for a $\phi$-mesic bound state is identified in
${\rm{^{16}O}}$, suggesting its potential experimental observability. The work
provides valuable insights into $\phi$ meson interactions in infinite nuclear
matter and the potential formation of exotic $\phi$-mesic nuclear states,
offering promising probes for strongly interacting matter in the upcoming
experiments at J-PARC, JLab, and ${\rm{\bar{P}}ANDA}$@FAIR physics program.; 62) Deep End-to-End Posterior ENergy (DEEPEN) for image recovery; Current end-to-end (E2E) and plug-and-play (PnP) image reconstruction
algorithms approximate the maximum a posteriori (MAP) estimate but cannot offer
sampling from the posterior distribution, like diffusion models. By contrast,
it is challenging for diffusion models to be trained in an E2E fashion. This
paper introduces a Deep End-to-End Posterior ENergy (DEEPEN) framework, which
enables MAP estimation as well as sampling. We learn the parameters of the
posterior, which is the sum of the data consistency error and the negative
log-prior distribution, using maximum likelihood optimization in an E2E
fashion. The proposed approach does not require algorithm unrolling, and hence
has a smaller computational and memory footprint than current E2E methods,
while it does not require contraction constraints typically needed by current
PnP methods. Our results demonstrate that DEEPEN offers improved performance
than current E2E and PnP models in the MAP setting, while it also offers faster
sampling compared to diffusion models. In addition, the learned energy-based
model is observed to be more robust to changes in image acquisition settings.; 63) Mathematical reasoning and the computer; Computers have already changed the way that humans do mathematics: they
enable us to compute efficiently. But will they soon be helping us to reason?
And will they one day start reasoning themselves? We give an overview of recent
developments in neural networks, computer theorem provers and large language
models.; 64) Enhancing the Product Quality of the Injection Process Using eXplainable
  Artificial Intelligence; The injection molding process is a traditional technique for making products
in various industries such as electronics and automobiles via solidifying
liquid resin into certain molds. Although the process is not related to
creating the main part of engines or semiconductors, this manufacturing
methodology sets the final form of the products. Re-cently, research has
continued to reduce the defect rate of the injection molding process. This
study proposes an optimal injection molding process control system to reduce
the defect rate of injection molding products with XAI (eXplainable Artificial
Intelligence) ap-proaches. Boosting algorithms (XGBoost and LightGBM) are used
as tree-based classifiers for predicting whether each product is normal or
defective. The main features to control the process for improving the product
are extracted by SHapley Additive exPlanations, while the individual
conditional expectation analyzes the optimal control range of these extracted
features. To validate the methodology presented in this work, the actual
injection molding AI manufacturing dataset provided by KAMP (Korea AI
Manufacturing Platform) is employed for the case study. The results reveal that
the defect rate decreases from 1.00% (Original defect rate) to 0.21% with
XGBoost and 0.13% with LightGBM, respectively.; 65) Study of long-term spectral evolution and X-ray and Gamma-ray
  correlation of blazars seen by HAWC; The HAWC Observatory collected 6 years of extensive data, providing an ideal
platform for long-term monitoring of blazars in the Very High Energy (VHE)
band, without bias towards specific flux states. HAWC continuously monitors
blazar activity at TeV energies, focusing on sources with a redshift of {z \lt
0.3}, based on the Third Fermi-LAT Catalog of High-Energy sources. We
specifically focused our analysis on Mrk 421 and Mrk 501, as they are the
brightest blazars observed by the HAWC Observatory. With a dataset of 2143
days, this work significantly extends the monitoring previously published,
which was based on 511 days of observation. By utilizing HAWC data for the VHE
{\gamma}-ray emission in the 300 GeV to 100 TeV energy range, in conjunction
with Swift-XRT data for the 0.3 to 10 keV X-ray emission, we aim to explore
potential correlations between these two bands. For Mrk 501, we found evidence
of a long-term correlation. Additionally, we identified a period in the light
curve where the flux was very low for more than two years. On the other hand,
our analysis of Mrk 421 measured a strong linear correlation for
quasi-simultaneous observations collected by HAWC and Swift-XRT. This result is
consistent with a linear dependence and a multiple-zone synchrotron
self-Compton model to explain the X-ray and the {\gamma}-ray emission. Finally,
as suggested by previous findings, we confirm a harder-when-brighter behavior
in the spectral evolution of the flux properties for Mrk 421. These findings
contribute to the understanding of blazar emissions and their underlying
mechanisms.; 66) Efektywne energetycznie wielodost\k{e}powe przetwarzanie brzegowe w
  sieci 5G; Energy efficient Multi-access Edge Computing in 5G network; Multi-access edge computing is a technique that combines the use of
communication networks and remote computing resources. It allows to perform
complex computational tasks for devices with low computing power while
maintaining low latencies. However, it is important to effectively allocate the
computing tasks to individual nodes. The work will present how the multi-access
edge computing system can be integrated into the 5G network, as well as how
resources can be distributed between individual nodes to minimize energy
consumption. Some new degrees of freedom will be presented, which enable a
significant reduction in energy consumption compared to existing solutions for
independent optimization of the computation and communication parts.
  --
  Wielodost\k{e}powe przetwarzanie brzegowe jest technik\k{a}
{\l}\k{a}cz\k{a}c\k{a} wykorzystanie sieci komunikacyjnych i oddalonych
zasob\'ow obliczeniowych. Pozwala wykona\'c z{\l}o\.zone zadania obliczeniowe
na potrzeby urz\k{a}dze\'n o niewielkiej mocy obliczeniowej przy zachowaniu
niewielkich op\'o\'znie\'n. Istotne jest jednak efektywne zarz\k{a}dzanie
przydzia{\l}em zada\'n obliczeniowych do poszczeg\'olnych w\k{e}z{\l}\'ow. W
pracy przedstawiono jak system przetwarzania brzegowego mo\.ze by\'c
zintegrowany z sieci\k{a} 5G, a tak\.ze jak mo\.zna rozdzieli\'c zasoby
mi\k{e}dzy poszczeg\'olne w\k{e}z{\l}y, \.zeby zminimalizowa\'c zu\.zycie
energii. Przedstawiony zostanie szereg nowych stopni swobody, kt\'ore
umo\.zliwiaj\k{a} znaczne obni\.zenie zu\.zycia energii w stosunku do
istniej\k{a}cych rozwi\k{a}za\'n niezale\.znej optymalizacji cz\k{e}\'sci
obliczeniowej i komunikacyjnej.; 67) Speaker Embedding Informed Audiovisual Active Speaker Detection for
  Egocentric Recordings; Audiovisual active speaker detection (ASD) addresses the task of determining
the speech activity of a candidate speaker given acoustic and visual data.
Typically, systems model the temporal correspondence of audiovisual cues, such
as the synchronisation between speech and lip movement. Recent work has
explored extending this paradigm by additionally leveraging speaker embeddings
extracted from candidate speaker reference speech. This paper proposes the
speaker comparison auxiliary network (SCAN) which uses speaker-specific
information from both reference speech and the candidate audio signal to
disambiguate challenging scenes when the visual signal is unresolvable.
Furthermore, an improved method for enrolling face-speaker libraries is
developed, which implements a self-supervised approach to video-based face
recognition. Fitting with the recent proliferation of wearable devices, this
work focuses on improving speaker-embedding-informed ASD in the context of
egocentric recordings, which can be characterised by acoustic noise and highly
dynamic scenes. SCAN is implemented with two well-established baselines, namely
TalkNet and Light-ASD; yielding a relative improvement in mAP of 14.5% and
10.3% on the Ego4D benchmark, respectively.; 68) Behavioural Predictors that Influence Digital Legacy Management
  Intentions among Individuals in South Africa; An emerging phenomenon, digital legacy management explores the management of
digital data individuals accumulate throughout their lifetime. With the
integration of digital systems and data into people's daily lives, it becomes
crucial to understand the intricacies of managing data to eventually form one's
digital legacy. This can be understood by investigating the significance of
behavioral predictors in shaping digital legacy management.
  The objective of this study is to explore how behavioral predictors influence
the intentions of individuals in South Africa towards managing their digital
legacy. This entailed:
  Investigating the impact of attitude, subjective norms, and perceived
behavioral control on these intentions. Exploring the perceived usefulness of
digital legacy management systems. Understanding the implications of response
cost and task-technology fit on individuals' inclinations towards digital
legacy planning. Data were collected (n = 203 valid responses) from South
African residents using an online survey and analyzed using partial least
squares structural equation analysis (PLS-SEM). Results indicate that
attitudes, peer opinions, personal resources, and skills are significant
positive influences on digital legacy management intention. Recognizing and
understanding these behavioral predictors is key when developing
region-specific and culturally sensitive digital legacy management tools,
awareness campaigns, and policies. Furthermore, it could pave the way for more
tailored strategies, ensuring effective transfer of post-mortem data, reducing
potential conflicts, and providing clarity when dealing with post-mortem data.; 69) External Large Foundation Model: How to Efficiently Serve Trillions of
  Parameters for Online Ads Recommendation; Ads recommendation is a prominent service of online advertising systems and
has been actively studied. Recent studies indicate that scaling-up and advanced
design of the recommendation model can bring significant performance
improvement. However, with a larger model scale, such prior studies have a
significantly increasing gap from industry as they often neglect two
fundamental challenges in industrial-scale applications. First, training and
inference budgets are restricted for the model to be served, exceeding which
may incur latency and impair user experience. Second, large-volume data arrive
in a streaming mode with data distributions dynamically shifting, as new
users/ads join and existing users/ads leave the system. We propose the External
Large Foundation Model (ExFM) framework to address the overlooked challenges.
Specifically, we develop external distillation and a data augmentation system
(DAS) to control the computational cost of training/inference while maintaining
high performance. We design the teacher in a way like a foundation model (FM)
that can serve multiple students as vertical models (VMs) to amortize its
building cost. We propose Auxiliary Head and Student Adapter to mitigate the
data distribution gap between FM and VMs caused by the streaming data issue.
Comprehensive experiments on internal industrial-scale applications and public
datasets demonstrate significant performance gain by ExFM.; 70) Pair Correlation of Zeros of the Riemann Zeta Function I: Proportions of
  Simple Zeros and Critical Zeros; Assuming the Riemann Hypothesis (RH), Montgomery proved a theorem in 1973
concerning the pair correlation of zeros of the Riemann zeta-function and
applied this to prove that at least $2/3$ of the zeros are simple. In earlier
work we showed how to remove RH from Montgomery's theorem and, in turn, obtain
results on simple zeros assuming conditions on the zeros that are weaker than
RH. Here we assume a more general condition, namely that all the zeros $\rho =
\beta +i\gamma$ with $T<\gamma\le 2T$ are in a narrow vertical box centered on
the critical line with width $\frac{b}{\log T}$. For simplicity, now assume
that $b\to 0$ as $T\to \infty$. Following Montgomery's method, we prove the
generalization of Montgomery's result that at least $2/3$ of zeros are simple,
and also the new result that at least $2/3$ of the zeros are on the critical
line. We also use the pair correlation method to prove that at least $1/3$ of
the zeros are both simple and on the critical line, a result already known
unconditionally. Our work thus shows that the pair correlation method can be
used to prove results not only on the vertical distribution of zeros but also
on their horizontal distribution.; 71) Generative Trajectory Stitching through Diffusion Composition; Effective trajectory stitching for long-horizon planning is a significant
challenge in robotic decision-making. While diffusion models have shown promise
in planning, they are limited to solving tasks similar to those seen in their
training data. We propose CompDiffuser, a novel generative approach that can
solve new tasks by learning to compositionally stitch together shorter
trajectory chunks from previously seen tasks. Our key insight is modeling the
trajectory distribution by subdividing it into overlapping chunks and learning
their conditional relationships through a single bidirectional diffusion model.
This allows information to propagate between segments during generation,
ensuring physically consistent connections. We conduct experiments on benchmark
tasks of various difficulties, covering different environment sizes, agent
state dimension, trajectory types, training data quality, and show that
CompDiffuser significantly outperforms existing methods.; 72) Posterior SBC: Simulation-Based Calibration Checking Conditional on Data; Simulation-based calibration checking (SBC) refers to the validation of an
inference algorithm and model implementation through repeated inference on data
simulated from a generative model. In the original and commonly used approach,
the generative model uses parameters drawn from the prior, and thus the
approach is testing whether the inference works for simulated data generated
with parameter values plausible under that prior. This approach is natural and
desirable when we want to test whether the inference works for a wide range of
datasets we might observe. However, after observing data, we are interested in
answering whether the inference works conditional on that particular data. In
this paper, we propose posterior SBC and demonstrate how it can be used to
validate the inference conditionally on observed data. We illustrate the
utility of posterior SBC in three case studies: (1) A simple multilevel model;
(2) a model that is governed by differential equations; and (3) a joint
integrative neuroscience model which is approximated via amortized Bayesian
inference with neural networks.; 73) Erasing Without Remembering: Safeguarding Knowledge Forgetting in Large
  Language Models; In this paper, we explore machine unlearning from a novel dimension, by
studying how to safeguard model unlearning in large language models (LLMs). Our
goal is to prevent unlearned models from recalling any related memory of the
targeted knowledge.We begin by uncovering a surprisingly simple yet overlooked
fact: existing methods typically erase only the exact expressions of the
targeted knowledge, leaving paraphrased or related information intact. To
rigorously measure such oversights, we introduce UGBench, the first benchmark
tailored for evaluating the generalisation performance across 13
state-of-the-art methods.UGBench reveals that unlearned models can still recall
paraphrased answers and retain target facts in intermediate layers. To address
this, we propose PERMU, a perturbation-based method that significantly enhances
the generalisation capabilities for safeguarding LLM unlearning.Experiments
demonstrate that PERMU delivers up to a 50.13% improvement in unlearning while
maintaining a 43.53% boost in robust generalisation. Our code can be found in
https://github.com/MaybeLizzy/UGBench.; 74) CAAT-EHR: Cross-Attentional Autoregressive Transformer for Multimodal
  Electronic Health Record Embeddings; Electronic health records (EHRs) provide a comprehensive source of
longitudinal patient data, encompassing structured modalities such as
laboratory results, imaging data, and vital signs, and unstructured clinical
notes. These datasets, after necessary preprocessing to clean and format the
data for analysis, often remain in their raw EHR form, representing numerical
or categorical values without further transformation into task-agnostic
embeddings. While such raw EHR data enables predictive modeling, its reliance
on manual feature engineering or downstream task-specific optimization limits
its utility for general-purpose applications. Deep learning (DL) techniques,
such as recurrent neural networks (RNNs) and Transformers, have facilitated
predictive tasks like disease progression and diagnosis prediction. However,
these methods often struggle to fully exploit the temporal and multimodal
dependencies inherent in EHR data due to their reliance on pre-processed but
untransformed raw EHR inputs. In this study, we introduce CAAT-EHR, a novel
architecture designed to bridge this gap by generating robust, task-agnostic
longitudinal embeddings from raw EHR data. CAAT-EHR leverages self- and
cross-attention mechanisms in its encoder to integrate temporal and contextual
relationships across multiple modalities, transforming the data into enriched
embeddings that capture complex dependencies. An autoregressive decoder
complements the encoder by predicting future time points data during
pre-training, ensuring that the resulting embeddings maintain temporal
consistency and alignment. CAAT-EHR eliminates the need for manual feature
engineering and enables seamless transferability across diverse downstream
tasks. Extensive evaluations on benchmark datasets, demonstrate the superiority
of CAAT-EHR-generated embeddings over pre-processed raw EHR data and other
baseline approaches.; 75) Tarski Lower Bounds from Multi-Dimensional Herringbones; Tarski's theorem states that every monotone function from a complete lattice
to itself has a fixed point. We analyze the query complexity of finding such a
fixed point on the $k$-dimensional grid of side length $n$ under the $\leq$
relation. In this setting, there is an unknown monotone function $f:
\{0,1,\ldots, n-1\}^k \to \{0,1,\ldots, n-1\}^k$ and an algorithm must query a
vertex $v$ to learn $f(v)$. The goal is to find a fixed point of $f$ using as
few oracle queries as possible.
  We show that the randomized query complexity of this problem is $\Omega\left(
\frac{k \cdot \log^2{n}}{\log{k}} \right)$ for all $n,k \geq 2$. This unifies
and improves upon two prior results: a lower bound of $\Omega(\log^2{n})$ from
[EPRY 2019] and a lower bound of $\Omega\left( \frac{k \cdot
\log{n}}{\log{k}}\right)$ from [BPR 2024], respectively.; 76) Derivation of the Planck Units Based in a Membranes Model; In this study, the Planck units (mass, time and length) have only been
derived, explained and attributed a physical meaning when they were deduced
based on the concept of interacting membranes (membranes instead of strings of
string theory). For this purpose, a set of five assumptions were proposed: (a)
the existence of the interacting membranes; (b) the curvatures of the membranes
oscillate according to the classical wave equation; (c) the spatial period of
the wave that arise when the membranes oscillate is given by $\lambda =
{\xi}{\pi}/k$; (d) the membranes oscillate with wavelength given by de Broglie
relation and (e) $x=ct$ holds. The parameter $\xi$ determines the period of
oscillation of the given membranes. In deriving the Planck units in this work,
$\xi$ must take the value 2 and determines a period 2$\pi$, closely to minimum
value 1 or to fundamental period $\pi$, respectively. In this context, Planck
units must be fundamental. Moreover, the parameter $\xi$ was reported as a
unification parameter between the formulas for the Coulomb$^{\prime}$s law and
Newton$^{\prime}$s law of universal gravitation linking the forces of
microworld and macroworld. Depending on the value $\xi$ takes, one force or
another will be had. It is also shown that the potential $V = hc/{\xi}{\pi}x$
deduced from the above assumptions and which contributes to deduce the Planck
units, can be derived from Yukawa$^{\prime}$s equation. Hence, the present work
would be contributing to theoretical physics, since at the Planck scale
predictions of some theories like Standard Model, quantum field theory and
general relativity are not expected to be valid.; 77) ACT-JEPA: Joint-Embedding Predictive Architecture Improves Policy
  Representation Learning; Learning efficient representations for decision-making policies is a
challenge in imitation learning (IL). Current IL methods require expert
demonstrations, which are expensive to collect. Consequently, they often have
underdeveloped world models. Self-supervised learning (SSL) offers an
alternative by allowing models to learn from diverse, unlabeled data, including
failures. However, SSL methods often operate in raw input space, making them
inefficient. In this work, we propose ACT-JEPA, a novel architecture that
integrates IL and SSL to enhance policy representations. We train a policy to
predict (1) action sequences and (2) abstract observation sequences. The first
objective uses action chunking to improve action prediction and reduce
compounding errors. The second objective extends this idea of chunking by
predicting abstract observation sequences. We utilize Joint-Embedding
Predictive Architecture to predict in abstract representation space, allowing
the model to filter out irrelevant details, improve efficiency, and develop a
robust world model. Our experiments show that ACT-JEPA improves the quality of
representations by learning temporal environment dynamics. Additionally, the
model's ability to predict abstract observation sequences results in
representations that effectively generalize to action sequence prediction.
ACT-JEPA performs on par with established baselines across a range of
decision-making tasks.; 78) 2DMCG:2DMambawith Change Flow Guidance for Change Detection in Remote
  Sensing; Remote sensing change detection (CD) has made significant advancements with
the adoption of Convolutional Neural Networks (CNNs) and Transformers. While
CNNs offer powerful feature extraction, they are constrained by receptive field
limitations, and Transformers suffer from quadratic complexity when processing
long sequences, restricting scalability. The Mamba architecture provides an
appealing alternative, offering linear complexity and high parallelism.
However, its inherent 1D processing structure causes a loss of spatial
information in 2D vision tasks. This paper addresses this limitation by
proposing an efficient framework based on a Vision Mamba variant that enhances
its ability to capture 2D spatial information while maintaining the linear
complexity characteristic of Mamba. The framework employs a 2DMamba encoder to
effectively learn global spatial contextual information from multi-temporal
images. For feature fusion, we introduce a 2D scan-based, channel-parallel
scanning strategy combined with a spatio-temporal feature fusion method, which
adeptly captures both local and global change information, alleviating spatial
discontinuity issues during fusion. In the decoding stage, we present a feature
change flow-based decoding method that improves the mapping of feature change
information from low-resolution to high-resolution feature maps, mitigating
feature shift and misalignment. Extensive experiments on benchmark datasets
such as LEVIR-CD+ and WHU-CD demonstrate the superior performance of our
framework compared to state-of-the-art methods, showcasing the potential of
Vision Mamba for efficient and accurate remote sensing change detection.; 79) UAV-assisted Internet of Vehicles: A Framework Empowered by
  Reinforcement Learning and Blockchain; This paper addresses the challenges of selecting relay nodes and coordinating
among them in UAV-assisted Internet-of-Vehicles (IoV). The selection of UAV
relay nodes in IoV employs mechanisms executed either at centralized servers or
decentralized nodes, which have two main limitations: 1) the traceability of
the selection mechanism execution and 2) the coordination among the selected
UAVs, which is currently offered in a centralized manner and is not coupled
with the relay selection. Existing UAV coordination methods often rely on
optimization methods, which are not adaptable to different environment
complexities, or on centralized deep reinforcement learning, which lacks
scalability in multi-UAV settings. Overall, there is a need for a comprehensive
framework where relay selection and coordination are coupled and executed in a
transparent and trusted manner. This work proposes a framework empowered by
reinforcement learning and Blockchain for UAV-assisted IoV networks. It
consists of three main components: a two-sided UAV relay selection mechanism
for UAV-assisted IoV, a decentralized Multi-Agent Deep Reinforcement Learning
(MDRL) model for autonomous UAV coordination, and a Blockchain implementation
for transparency and traceability in the interactions between vehicles and
UAVs. The relay selection considers the two-sided preferences of vehicles and
UAVs based on the Quality-of-UAV (QoU) and the Quality-of-Vehicle (QoV). Upon
selection of relay UAVs, the decentralized coordination between them is enabled
through an MDRL model trained to control their mobility and maintain the
network coverage and connectivity using Proximal Policy Optimization (PPO). The
evaluation results demonstrate that the proposed selection and coordination
mechanisms improve the stability of the selected relays and maximize the
coverage and connectivity achieved by the UAVs.; 80) RobotIQ: Empowering Mobile Robots with Human-Level Planning for
  Real-World Execution; This paper introduces RobotIQ, a framework that empowers mobile robots with
human-level planning capabilities, enabling seamless communication via natural
language instructions through any Large Language Model. The proposed framework
is designed in the ROS architecture and aims to bridge the gap between humans
and robots, enabling robots to comprehend and execute user-expressed text or
voice commands. Our research encompasses a wide spectrum of robotic tasks,
ranging from fundamental logical, mathematical, and learning reasoning for
transferring knowledge in domains like navigation, manipulation, and object
localization, enabling the application of learned behaviors from simulated
environments to real-world operations. All encapsulated within a modular
crafted robot library suite of API-wise control functions, RobotIQ offers a
fully functional AI-ROS-based toolset that allows researchers to design and
develop their own robotic actions tailored to specific applications and robot
configurations. The effectiveness of the proposed system was tested and
validated both in simulated and real-world experiments focusing on a home
service scenario that included an assistive application designed for elderly
people. RobotIQ with an open-source, easy-to-use, and adaptable robotic library
suite for any robot can be found at https://github.com/emmarapt/RobotIQ.; 81) Empirical Discovery of Multi-Scale Transfer of Information in Dynamical
  Systems; In this work, we quantify the timescales and information flow associated by
multiscale energy transfer in a weakly turbulent system through a novel new
interpretation of transfer entropy. Our goal is to provide a detailed
understanding of the nature of complex energy transfer in nonlinear dispersive
systems driven by wave mixing. Further, we present a modal decomposition method
based on the empirical wavelet transform that produces a relatively small
number of nearly decorrelated, scale separated modes. Using our method, we are
able to track multiscale energy transfer using only scalar time series
measurements of a weakly turbulent system. This points to our approach being of
broader applicability in real-world data coming from chaotic or turbulent
dynamical systems.; 82) On the Distribution of the Two-Sample Cramer-von Mises Criterion; The Cramer-von Mises $\omega^2$ criterion for testing that a sample, $x_1, \cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \begin{equation*}\tag{1}\omega^2 = \int^\infty_{-\infty} \lbrack F_N(x) - F(x)\rbrack^2 dF(x),\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\omega^2$, namely \begin{equation*}\tag{2} T NM/(N + M)\rbrack G_M(x)\rbrack^2 dH_{N+M}(x),\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\rbrack$. limiting $N\omega^2$ as $N \rightarrow \infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \infty, M \infty$, $N/M \lambda$, $\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4].; 83) AI-Instruments: Embodying Prompts as Instruments to Abstract & Reflect
  Graphical Interface Commands as General-Purpose Tools; Chat-based prompts respond with verbose linear-sequential texts, making it
difficult to explore and refine ambiguous intents, back up and reinterpret, or
shift directions in creative AI-assisted design work. AI-Instruments instead
embody ""prompts"" as interface objects via three key principles: (1) Reification
of user-intent as reusable direct-manipulation instruments; (2) Reflection of
multiple interpretations of ambiguous user-intents (Reflection-in-intent) as
well as the range of AI-model responses (Reflection-in-response) to inform
design ""moves"" towards a desired result; and (3) Grounding to instantiate an
instrument from an example, result, or extrapolation directly from another
instrument. Further, AI-Instruments leverage LLM's to suggest, vary, and refine
new instruments, enabling a system that goes beyond hard-coded functionality by
generating its own instrumental controls from content. We demonstrate four
technology probes, applied to image generation, and qualitative insights from
twelve participants, showing how AI-Instruments address challenges of intent
formulation, steering via direct manipulation, and non-linear iterative
workflows to reflect and resolve ambiguous intents.; 84) A LSTM-Transformer Model for pulsation control of pVADs; Methods: A method of the pulsation for a pVAD is proposed (AP-pVAD Model).
AP-pVAD Model consists of two parts: NPQ Model and LSTM-Transformer Model.
(1)The NPQ Model determines the mathematical relationship between motor speed,
pressure, and flow rate for the pVAD. (2)The Attention module of Transformer
neural network is integrated into the LSTM neural network to form the new
LSTM-Transformer Model to predict the pulsation time characteristic points for
adjusting the motor speed of the pVAD. Results: The AP-pVAD Model is validated
in three hydraulic experiments and an animal experiment. (1)The pressure
provided by pVAD calculated with the NPQ Model has a maximum error of only 2.15
mmHg compared to the expected values. (2)The pulsation time characteristic
points predicted by the LSTM-Transformer Model shows a maximum prediction error
of 1.78ms, which is significantly lower than other methods. (3)The in-vivo test
of pVAD in animal experiment has significant improvements in aortic pressure.
Animals survive for over 27 hours after the initiation of pVAD operation.
Conclusion: (1)For a given pVAD, motor speed has a linear relationship with
pressure and a quadratic relationship with flow. (2)Deep learning can be used
to predict pulsation characteristic time points, with the LSTM-Transformer
Model demonstrating minimal prediction error and better robust performance
under conditions of limited dataset sizes, elevated noise levels, and diverse
hyperparameter combinations, demonstrating its feasibility and effectiveness.; 85) AgroLLM: Connecting Farmers and Agricultural Practices through Large
  Language Models for Enhanced Knowledge Transfer and Practical Application; AgroLLM is an AI-powered chatbot designed to enhance knowledge-sharing and
education in agriculture using Large Language Models (LLMs) and a
Retrieval-Augmented Generation (RAG) framework. By using a comprehensive
open-source agricultural database, AgroLLM provides accurate, contextually
relevant responses while reducing incorrect information retrieval. The system
utilizes the FAISS vector database for efficient similarity searches, ensuring
rapid access to agricultural knowledge. A comparative study of three advanced
models: Gemini 1.5 Flash, ChatGPT-4o Mini, and Mistral-7B-Instruct-v0.2 was
conducted to evaluate performance across four key agricultural domains:
Agriculture and Life Sciences, Agricultural Management, Agriculture and
Forestry, and Agriculture Business. Key evaluation metrics included embedding
quality, search efficiency, and response relevance. Results indicated that
ChatGPT-4o Mini with RAG achieved the highest accuracy at 93%. Continuous
feedback mechanisms enhance response quality, making AgroLLM a benchmark
AI-driven educational tool for farmers, researchers, and professionals,
promoting informed decision-making and improved agricultural practices.; 86) A hybrid framework integrating classical computers and quantum annealers
  for optimisation of truss structures; This work proposes a hybrid framework combining classical computers with
quantum annealers for structural optimisation. At each optimisation iteration
of an iterative process, two minimisation problems are formulated one for the
underlying mechanical boundary value problem through the minimisation potential
energy principle and one for the minimisation problem to update the design
variables. Our hybrid approach leverages the strength of quantum computing to
solve these two minimisation problems at each step, thanks to the developed
quantum annealing-assisted sequential programming strategy introduced in
[Nguyen, Wu, Remacle, and Noels. A quantum annealing-sequential quadratic
programming assisted finite element simulation for non-linear and
history-dependent mechanical problems. European Journal of Mechanics-A/Solids
105 (2024): 105254]. The applicability of the proposed framework is
demonstrated through several case studies of truss optimisation, highlighting
its capability to perform optimisation with quantum computers. The proposed
framework offers a promising direction for future structural optimisation
applications, particularly in scenarios where the quantum computer could
resolve the size limitations of the classical computers due to problem
complexities.; 87) SWIFT: Mapping Sub-series with Wavelet Decomposition Improves Time
  Series Forecasting; In recent work on time-series prediction, Transformers and even large
language models have garnered significant attention due to their strong
capabilities in sequence modeling. However, in practical deployments,
time-series prediction often requires operation in resource-constrained
environments, such as edge devices, which are unable to handle the
computational overhead of large models. To address such scenarios, some
lightweight models have been proposed, but they exhibit poor performance on
non-stationary sequences. In this paper, we propose $\textit{SWIFT}$, a
lightweight model that is not only powerful, but also efficient in deployment
and inference for Long-term Time Series Forecasting (LTSF). Our model is based
on three key points: (i) Utilizing wavelet transform to perform lossless
downsampling of time series. (ii) Achieving cross-band information fusion with
a learnable filter. (iii) Using only one shared linear layer or one shallow MLP
for sub-series' mapping. We conduct comprehensive experiments, and the results
show that $\textit{SWIFT}$ achieves state-of-the-art (SOTA) performance on
multiple datasets, offering a promising method for edge computing and
deployment in this task. Moreover, it is noteworthy that the number of
parameters in $\textit{SWIFT-Linear}$ is only 25\% of what it would be with a
single-layer linear model for time-domain prediction. Our code is available at
https://github.com/LancelotXWX/SWIFT.; 88) Calibrating the Instrumental Drift in MAROON-X using an Ensemble
  Analysis; MAROON-X is a state-of-the-art extreme precision radial velocity spectrograph
deployed on the 8.1-meter Gemini-N telescope on Maunakea, Hawai'i. Using a
stabilized Fabry-P\'erot etalon for wavelength and drift calibration, MAROON-X
has achieved a short-term precision of $\sim$\,30\,cm\,s$^{-1}$. However, due
to a long-term drift in the etalon (2.2\,cm\,s$^{-1}$ per day) and various
interruptions of the instrument baseline over the first few years of operation,
MAROON-X experiences RV offsets between observing runs several times larger
than the short-term precision during any individual run, which hinders the
detection of longer-period signals. In this study, we analyze RV measurements
of 11 targets that either exhibit small RV scatter or have signals that can be
precisely constrained using Keplerian or Gaussian Process models. Leveraging
this ensemble, we calibrate MAROON-X's run offsets for data collected between
September 2020 and early January 2024 to a precision of $\sim$0.5\,m\,s$^{-1}$.
When applying these calibrated offsets to HD 3651, a quiet star, we obtain
residual velocities with an RMS of $<$70\,cm\,s$^{-1}$ in both the Red and Blue
channels of MAROON-X over a baseline of 29 months. We also demonstrate the
sensitivity of MAROON-X data calibrated with these offsets through a series of
injection-recovery tests. Based on our findings, MAROON-X is capable of
detecting sub m\,s$^{-1}$ signals out to periods of more than 1,000 days.; 89) Efficient space-time discretizations for tracking the boundaries of
  reachable sets; The reachable sets of nonlinear control systems can in general only be
numerically approximated, and are often very expensive to calculate. In this
paper, we propose an algorithm that tracks only the boundaries of the reachable
sets and that chooses the temporal and spatial discretizations in a non-uniform
way to reduce the computational complexity.; 90) General Feature Extraction In SAR Target Classification: A Contrastive
  Learning Approach Across Sensor Types; The increased availability of SAR data has raised a growing interest in
applying deep learning algorithms. However, the limited availability of labeled
data poses a significant challenge for supervised training. This article
introduces a new method for classifying SAR data with minimal labeled images.
The method is based on a feature extractor Vit trained with contrastive
learning. It is trained on a dataset completely different from the one on which
classification is made. The effectiveness of the method is assessed through 2D
visualization using t-SNE for qualitative evaluation and k-NN classification
with a small number of labeled data for quantitative evaluation. Notably, our
results outperform a k-NN on data processed with PCA and a ResNet-34
specifically trained for the task, achieving a 95.9% accuracy on the MSTAR
dataset with just ten labeled images per class.; 91) Leveraging Large Language Models For Scalable Vector Graphics
  Processing: A Review; In recent years, rapid advances in computer vision have significantly
improved the processing and generation of raster images. However, vector
graphics, which is essential in digital design, due to its scalability and ease
of editing, have been relatively understudied. Traditional vectorization
techniques, which are often used in vector generation, suffer from long
processing times and excessive output complexity, limiting their usability in
practical applications. The advent of large language models (LLMs) has opened
new possibilities for the generation, editing, and analysis of vector graphics,
particularly in the SVG format, which is inherently text-based and well-suited
for integration with LLMs.
  This paper provides a systematic review of existing LLM-based approaches for
SVG processing, categorizing them into three main tasks: generation, editing,
and understanding. We observe notable models such as IconShop, StrokeNUWA, and
StarVector, highlighting their strengths and limitations. Furthermore, we
analyze benchmark datasets designed for assessing SVG-related tasks, including
SVGEditBench, VGBench, and SGP-Bench, and conduct a series of experiments to
evaluate various LLMs in these domains. Our results demonstrate that for vector
graphics reasoning-enhanced models outperform standard LLMs, particularly in
generation and understanding tasks. Furthermore, our findings underscore the
need to develop more diverse and richly annotated datasets to further improve
LLM capabilities in vector graphics tasks.; 92) WIggle Corrector Kit for NIRSpEc Data: WICKED; The point-spread function of the integral-field unit (IFU) mode of the JWST's
NIRSpec is heavily under-sampled, creating resampling noise seen as
low-frequency sinusoidal-like artifacts, or ""wiggles"". These artifacts in the
data are not corrected in the JWST data pipeline, and significantly impact the
science that can be achieved at a single-pixel level. We present WICKED (WIggle
Corrector Kit for NIRSpEc Data), a tool designed to empirically remove wiggles.
WICKED uses the Fast Fourier Transform to identify wiggle-affected spaxels
across the data cube. Spectra are modeled with a mix of integrated aperture and
annular templates, a power-law, and a second-degree polynomial. The method
works across all medium- and high-resolution NIRSpec gratings: F070LP, F100LP,
F170LP, and F290LP. WICKED can recover the true overall spectral shape up to a
factor of 3.5x better compared to uncorrected spectra. It recovers the
equivalent width of absorption lines within 5% of the true value-~3x better
than uncorrected spectra and ~2x better than other methods. WICKED
significantly improves kinematic measurements, recovering the line-of-sight
velocity (LOSV) within 1% of the true value -- more than 100x better than
uncorrected spectra at S/N ~40. As a case study, we applied WICKED to
G235H/F170LP IFU data of the elliptical galaxy NGC5128, finding good agreement
with previous studies. In wiggle-affected regions, the uncorrected spectrum
showed stellar LOSV and velocity dispersion differences compared to the
WICKED-cleaned spectrum, of ~17x and ~36x larger than the estimated
uncertainties, respectively. Wiggles in NIRSpec IFU data can introduce severe
biases in spectral shape, line measurements, and kinematics to values larger
than the typical uncertainties. WICKED provides a robust, user-friendly
solution, enabling precise single-pixel studies and maximizing JWST's
potential.; 93) Simultaneous bifurcation of limit cycles for Piecewise Holomorphic
  systems; Let $\dot{z}=f(z)$ be a holomorphic differential equation with center at $p$.
In this paper we are concerned about studying the piecewise perturbation
systems $\dot{z}=f(z)+\epsilon R^\pm(z,\overline{z}),$ where
$R^\pm(z,\overline{z})$ are complex polynomials defined for
$\pm\operatorname{Im}(z)> 0.$ We provide an integral expression, similar to an
Abelian integral, for the period annulus of $p.$ The zeros of this integral
control the bifurcating limit cycles from the periodic orbits of this annular
region. This expression is given in terms of the conformal conjugation between
$\dot{z}=f(z)$ and its linearization $\dot{z}=f'(p)z$ at $p$. We use this
result to control the simultaneous bifurcation of limit cycles of the two
annular periods of $\dot{z}={\rm i} (z^2-1)/2$, after both complex and
holomorphic piecewise polynomial perturbations. In particular, as far as we
know, we provide the first proof of the existence of non nested limit cycles
for piecewise holomorphic systems.; 94) Discrete Lagrangian multiforms for ABS equations I: quad equations; Discrete Lagrangian multiform theory is a variational perspective on lattice
equations that are integrable in the sense of multidimensional consistency. The
Lagrangian multiforms for the equations of the ABS classification formed the
start of this theory, but the Lagrangian multiforms that are usually considered
in this context produce equations that are slightly weaker than the ABS
equations. In this work, we present alternative Lagrangian multiforms that have
Euler-Lagrange equations equivalent to the ABS equations.
  In addition, the treatment of the ABS Lagrangian multiforms in the existing
literature fails to acknowledge that the complex functions in their definitions
have branch cuts. The choice of branch affects both the existence of an
additive three-leg form for the ABS equations and the closure property of the
Lagrangian multiforms. We give counterexamples for both these properties, but
we recover them by including integer-valued fields, related to possible the
branch choices, in the action sums.; 95) Thermal conductivity of 3C-SiC from configuration space sampling; Cubic silicon carbide phonon thermal conductivity has been calculated using
anharmonic phonon analysis. The atomic interaction model was built using
displacement-force data obtained with the High Efficiency Configuration Space
Sampling (HECSS) technique and density functional theory calculated forces. In
the new version of HECSS we replaced the Markov chain scheme of
Metropolis-Hastings Monte-Carlo with weighting of the final sampling according
to the target distribution. This increased the efficiency of the method and
allowed to use -- with appropriate weight -- all generated and ab-initio
evaluated samples. The quality of the proposed method is confirmed by the
accuracy with which the experimental results taken from the literature were
reproduced.; 96) General Table Question Answering via Answer-Formula Joint Generation; Advanced table question answering (TableQA) methods prompt large language
models (LLMs) to generate answer text, SQL query, Python code, or custom
operations, which impressively improve the complex reasoning problems in the
TableQA task. However, these methods lack the versatility to cope with specific
question types or table structures. In contrast, the Spreadsheet Formula, the
widely-used and well-defined operation language for tabular data, has not been
thoroughly explored to solve TableQA. In this paper, we first attempt to use
Formula as the logical form for solving complex reasoning on the tables with
different structures. Specifically, we construct a large Formula-annotated
TableQA dataset \texttt{FromulaQA} from existing datasets. In addition, we
propose \texttt{TabAF}, a general table answering framework to solve multiple
types of tasks over multiple types of tables simultaneously. Unlike existing
methods, \texttt{TabAF} decodes answers and Formulas with a single LLM
backbone, demonstrating great versatility and generalization. \texttt{TabAF}
based on Llama3.1-70B achieves new state-of-the-art performance on the
WikiTableQuestion, HiTab and TabFact.; 97) Mean value theorems for rational exponential sums; We obtain finite field analogues of a series of recent results on various
mean value theorems for Weyl sums. Instead of the Vinogradov Mean Value
Theorem, our results rest on the classical argument of Mordell, combined with
several other ideas.; 98) Modeling Dynamic Hand-Object Interactions with Applications to
  Human-Robot Handovers; Humans frequently grasp, manipulate, and move objects. Interactive systems
assist humans in these tasks, enabling applications in Embodied AI, human-robot
interaction, and virtual reality. However, current methods in hand-object
synthesis often neglect dynamics and focus on generating static grasps. The
first part of this dissertation introduces dynamic grasp synthesis, where a
hand grasps and moves an object to a target pose. We approach this task using
physical simulation and reinforcement learning. We then extend this to bimanual
manipulation and articulated objects, requiring fine-grained coordination
between hands. In the second part of this dissertation, we study human-to-robot
handovers. We integrate captured human motion into simulation and introduce a
student-teacher framework that adapts to human behavior and transfers from sim
to real. To overcome data scarcity, we generate synthetic interactions,
increasing training diversity by 100x. Our user study finds no difference
between policies trained on synthetic vs. real motions.; 99) Stability of 2-class groups in the $\mathbb{Z}_2$-extension of certain
  real biquadratic fields; Greenberg's conjecture on the stability of $\ell$-class groups in the
cyclotomic $\mathbb{Z}_{\ell}$-extension of a real field has been proven for
various infinite families of real quadratic fields for the prime $\ell=2$. In
this work, we consider an infinite family of real biquadratic fields $K$. With
some extensive use of elementary group theoretic and class field theoretic
arguments, we investigate the $2$-class groups of the $n$-th layers $K_n$ of
the cyclotomic $\mathbb{Z}_2$-extension of $K$ and verify Greenberg's
conjecture. We also relate capitulation of ideal classes of certain
sub-extensions of $K_n$ to the relative sizes of the $2$-class groups.; 100) The Value of Prediction in Identifying the Worst-Off; Machine learning is increasingly used in government programs to identify and
support the most vulnerable individuals, prioritizing assistance for those at
greatest risk over optimizing aggregate outcomes. This paper examines the
welfare impacts of prediction in equity-driven contexts, and how they compare
to other policy levers, such as expanding bureaucratic capacity. Through
mathematical models and a real-world case study on long-term unemployment
amongst German residents, we develop a comprehensive understanding of the
relative effectiveness of prediction in surfacing the worst-off. Our findings
provide clear analytical frameworks and practical, data-driven tools that
empower policymakers to make principled decisions when designing these systems.",0.0,0.6309297535714575
2412.00036,applied,2412.00036-pos2-2,"On the Distribution of the Two-Sample Cramer-von Mises Criterion; The Cramer-von Mises $\omega^2$ criterion for testing that a sample, $x_1, \cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \begin{equation*}\tag{1}\omega^2 = \int^\infty_{-\infty} \lbrack F_N(x) - F(x)\rbrack^2 dF(x),\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\omega^2$, namely \begin{equation*}\tag{2} T NM/(N + M)\rbrack G_M(x)\rbrack^2 dH_{N+M}(x),\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\rbrack$. limiting $N\omega^2$ as $N \rightarrow \infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \infty, M \infty$, $N/M \lambda$, $\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4].",2412.00036-pos1-2,"Quant GANs: deep generation of financial time series; Modeling financial time series by stochastic processes is a challenging task
and a central area of research in financial mathematics. As an alternative, we
introduce Quant GANs, a data-driven model which is inspired by the recent
success of generative adversarial networks (GANs). Quant GANs consist of a
generator and discriminator function, which utilize temporal convolutional
networks (TCNs) and thereby achieve to capture long-range dependencies such as
the presence of volatility clusters. The generator function is explicitly
constructed such that the induced stochastic process allows a transition to its
risk-neutral distribution. Our numerical results highlight that distributional
properties for small and large lags are in an excellent agreement and
dependence properties such as volatility clusters, leverage effects, and serial
autocorrelations can be generated by the generator function of Quant GANs,
demonstrably in high fidelity.",43,"['1', '3', '2', '7', '4', '5', '9', '10', '6', '8']","The main paper discusses the two-sample Cramer-von Mises criterion for statistical testing, which revolves around understanding distributions and empirical measures. The best candidate, paper 1, focuses on a privacy-preserving AI model that can effectively analyze user data while maintaining secure statistical testing principles, making it the most relevant multidisciplinary connection. The subsequent candidates, while interesting, delve into different areas such as symbolic computation, large physics models, and other fields that diverge from the core statistical concepts found in the main paper. Hence, paper 1 aligns best in terms of integrating statistical methods with emerging AI technologies.","1) GOD model: Privacy Preserved AI School for Personal Assistant; Personal AI assistants (e.g., Apple Intelligence, Meta AI) offer proactive
recommendations that simplify everyday tasks, but their reliance on sensitive
user data raises concerns about privacy and trust. To address these challenges,
we introduce the Guardian of Data (GOD), a secure, privacy-preserving framework
for training and evaluating AI assistants directly on-device. Unlike
traditional benchmarks, the GOD model measures how well assistants can
anticipate user needs-such as suggesting gifts-while protecting user data and
autonomy. Functioning like an AI school, it addresses the cold start problem by
simulating user queries and employing a curriculum-based approach to refine the
performance of each assistant. Running within a Trusted Execution Environment
(TEE), it safeguards user data while applying reinforcement and imitation
learning to refine AI recommendations. A token-based incentive system
encourages users to share data securely, creating a data flywheel that drives
continuous improvement. Specifically, users mine with their data, and the
mining rate is determined by GOD's evaluation of how well their AI assistant
understands them across categories such as shopping, social interactions,
productivity, trading, and Web3. By integrating privacy, personalization, and
trust, the GOD model provides a scalable, responsible path for advancing
personal AI assistants. For community collaboration, part of the framework is
open-sourced at https://github.com/PIN-AI/God-Model.; 2) Symbolic Computations of the Two-Colored Diagrams for Central
  Configurations of the Planar N-vortex Problem; We apply the singular sequence method to investigate the finiteness problem
for stationary configurations of the planar N-vortex problem. The initial step
of the singular sequence method involves identifying all two-colored diagrams.
These diagrams represent potential scenarios where finiteness may fail. We
develop a symbolic computation algorithm to determine all two-colored diagrams
for central configurations of the planar N-vortex problem.; 3) Large Physics Models: Towards a collaborative approach with Large
  Language Models and Foundation Models; This paper explores ideas and provides a potential roadmap for the
development and evaluation of physics-specific large-scale AI models, which we
call Large Physics Models (LPMs). These models, based on foundation models such
as Large Language Models (LLMs) - trained on broad data - are tailored to
address the demands of physics research. LPMs can function independently or as
part of an integrated framework. This framework can incorporate specialized
tools, including symbolic reasoning modules for mathematical manipulations,
frameworks to analyse specific experimental and simulated data, and mechanisms
for synthesizing theories and scientific literature. We begin by examining
whether the physics community should actively develop and refine dedicated
models, rather than relying solely on commercial LLMs. We then outline how LPMs
can be realized through interdisciplinary collaboration among experts in
physics, computer science, and philosophy of science. To integrate these models
effectively, we identify three key pillars: Development, Evaluation, and
Philosophical Reflection. Development focuses on constructing models capable of
processing physics texts, mathematical formulations, and diverse physical data.
Evaluation assesses accuracy and reliability by testing and benchmarking.
Finally, Philosophical Reflection encompasses the analysis of broader
implications of LLMs in physics, including their potential to generate new
scientific understanding and what novel collaboration dynamics might arise in
research. Inspired by the organizational structure of experimental
collaborations in particle physics, we propose a similarly interdisciplinary
and collaborative approach to building and refining Large Physics Models. This
roadmap provides specific objectives, defines pathways to achieve them, and
identifies challenges that must be addressed to realise physics-specific large
scale AI models.; 4) The Spinning Blimp: Design and Control of a Novel Minimalist Aerial
  Vehicle Leveraging Rotational Dynamics and Locomotion; This paper presents the Spinning Blimp, a novel lighter-than-air (LTA) aerial
vehicle designed for low-energy stable flight. Utilizing an oblate spheroid
helium balloon for buoyancy, the vehicle achieves minimal energy consumption
while maintaining prolonged airborne states. The unique and low-cost design
employs a passively arranged wing coupled with a propeller to induce a spinning
behavior, providing inherent pendulum-like stabilization. We propose a control
strategy that takes advantage of the continuous revolving nature of the
spinning blimp to control translational motion. The cost-effectiveness of the
vehicle makes it highly suitable for a variety of applications, such as
patrolling, localization, air and turbulence monitoring, and domestic
surveillance. Experimental evaluations affirm the design's efficacy and
underscore its potential as a versatile and economically viable solution for
aerial applications.; 5) A Data-driven Investigation of Euphemistic Language: Comparing the usage
  of ""slave"" and ""servant"" in 19th century US newspapers; This study investigates the usage of ""slave"" and ""servant"" in the 19th
century US newspapers using computational methods. While both terms were used
to refer to enslaved African Americans, they were used in distinct ways. In the
Chronicling America corpus, we included possible OCR errors by using FastText
embedding and excluded text reprints to consider text reprint culture in the
19th century. Word2vec embedding was used to find semantically close words to
""slave"" and ""servant"" and log-odds ratio was calculated to identify
over-represented discourse words in the Southern and Northern newspapers. We
found that ""slave"" is associated with socio-economic, legal, and administrative
words, however, ""servant"" is linked to religious words in the Northern
newspapers while Southern newspapers associated ""servant"" with domestic and
familial words. We further found that slave discourse words in Southern
newspapers are more prevalent in Northern newspapers while servant discourse
words from each side are prevalent in their own region. This study contributes
to the understanding of how newspapers created different discourses around
enslaved African Americans in the 19th century US.; 6) Progressive Sparse Attention: Algorithm and System Co-design for
  Efficient Attention in LLM Serving; Processing long contexts has become a critical capability for modern large
language models (LLMs). However, serving long-context LLMs comes with
significant inference costs due to the high memory overhead of the key-value
(KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes)
to mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV
cache selection, which results in a trade-off between accuracy and efficiency.
A larger $k$ improves accuracy but decreases efficiency, while a smaller $k$
boosts efficiency but compromises accuracy. To overcome this trade-off, this
paper presents PSA, a $\underline{P}$rogressive $\underline{S}$parse
$\underline{A}$ttention mechanism that integrates algorithmic innovations with
system co-design to achieve both high inference accuracy and improved
efficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache
budget of different tokens and layers according to their real attention weight
distributions, rather than relying on a fixed budget $k$. This enables high
accuracy while minimizing KV cache usage. To further enhance execution
efficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU
interleaving and synchronization overhead during PSA computation. Additionally,
we implement unified GPU memory management that optimizes PSA's memory
utilization by accounting for uneven memory requirements across different model
layers. Extensive experimental results demonstrate that PSA reduces KV cache
usage for attention computation by up to 2.4$\times$ and 8.8$\times$, and
increases end-to-end serving throughput by up to 1.4$\times$ and 2.0$\times$,
compared to state-of-the-art DSAes and systems without sparse attention,
respectively.; 7) From Screens to Scenes: A Survey of Embodied AI in Healthcare; Healthcare systems worldwide face persistent challenges in efficiency,
accessibility, and personalization. Powered by modern AI technologies such as
multimodal large language models and world models, Embodied AI (EmAI)
represents a transformative frontier, offering enhanced autonomy and the
ability to interact with the physical world to address these challenges. As an
interdisciplinary and rapidly evolving research domain, ""EmAI in healthcare""
spans diverse fields such as algorithms, robotics, and biomedicine. This
complexity underscores the importance of timely reviews and analyses to track
advancements, address challenges, and foster cross-disciplinary collaboration.
In this paper, we provide a comprehensive overview of the ""brain"" of EmAI for
healthcare, wherein we introduce foundational AI algorithms for perception,
actuation, planning, and memory, and focus on presenting the healthcare
applications spanning clinical interventions, daily care & companionship,
infrastructure support, and biomedical research. Despite its promise, the
development of EmAI for healthcare is hindered by critical challenges such as
safety concerns, gaps between simulation platforms and real-world applications,
the absence of standardized benchmarks, and uneven progress across
interdisciplinary domains. We discuss the technical barriers and explore
ethical considerations, offering a forward-looking perspective on the future of
EmAI in healthcare. A hierarchical framework of intelligent levels for EmAI
systems is also introduced to guide further development. By providing
systematic insights, this work aims to inspire innovation and practical
applications, paving the way for a new era of intelligent, patient-centered
healthcare.; 8) Stitch-a-Recipe: Video Demonstration from Multistep Descriptions; When obtaining visual illustrations from text descriptions, today's methods
take a description with-a single text context caption, or an action
description-and retrieve or generate the matching visual context. However,
prior work does not permit visual illustration of multistep descriptions, e.g.
a cooking recipe composed of multiple steps. Furthermore, simply handling each
step description in isolation would result in an incoherent demonstration. We
propose Stitch-a-Recipe, a novel retrieval-based method to assemble a video
demonstration from a multistep description. The resulting video contains clips,
possibly from different sources, that accurately reflect all the step
descriptions, while being visually coherent. We formulate a training pipeline
that creates large-scale weakly supervised data containing diverse and novel
recipes and injects hard negatives that promote both correctness and coherence.
Validated on in-the-wild instructional videos, Stitch-a-Recipe achieves
state-of-the-art performance, with quantitative gains up to 24% as well as
dramatic wins in a human preference study.; 9) Utilizing AI and Machine Learning for Predictive Analysis of
  Post-Treatment Cancer Recurrence; In oncology, recurrence after treatment is one of the major challenges,
related to patients' survival and quality of life. Conventionally, prediction
of cancer relapse has always relied on clinical observation with statistical
model support, which almost fails to explain the complex, multifactorial nature
of tumor recurrence. This research explores how AI and ML models may increase
the accuracy and reliability of recurrence prediction in cancer. Therefore, AI
and ML create new opportunities not only for personalized medicine but also for
proactive management of patients through analyzing large volumes of data on
genetics, clinical manifestations, and treatment. The paper describes the
various AI and ML techniques for pattern identification and outcome prediction
in cancer patients using supervised and unsupervised learning. Clinical
implications provide an opportunity to review how early interventions could
happen and the design of treatment planning.; 10) One-Shot Dual-Arm Imitation Learning; We introduce One-Shot Dual-Arm Imitation Learning (ODIL), which enables
dual-arm robots to learn precise and coordinated everyday tasks from just a
single demonstration of the task. ODIL uses a new three-stage visual servoing
(3-VS) method for precise alignment between the end-effector and target object,
after which replay of the demonstration trajectory is sufficient to perform the
task. This is achieved without requiring prior task or object knowledge, or
additional data collection and training following the single demonstration.
Furthermore, we propose a new dual-arm coordination paradigm for learning
dual-arm tasks from a single demonstration. ODIL was tested on a real-world
dual-arm robot, demonstrating state-of-the-art performance across six precise
and coordinated tasks in both 4-DoF and 6-DoF settings, and showing robustness
in the presence of distractor objects and partial occlusions. Videos are
available at: https://www.robot-learning.uk/one-shot-dual-arm.; 11) Multimodal Graph Constrastive Learning and Prompt for ChartQA; ChartQA presents significant challenges due to the complex distribution of
chart elements and the implicit patterns embedded within the underlying data.
In this chapter, we have developed a joint multimodal scene graph for charts,
explicitly representing the relationships between chart elements and their
associated patterns.
  Our proposed multimodal scene graph consists of two components: a visual
graph and a textual graph, each designed to capture the structural and semantic
information within the chart. To unify representations across these different
modalities, we introduce a multimodal graph contrastive learning approach that
learns unified representations by maximizing similarity between nodes
representing the same object across multimodal graphs. The learned graph
representations can be seamlessly incorporated into a transformer decoder as a
soft prompt.
  Additionally, given the growing need for Multimodal Large Language Models
(MLLMs) in zero-shot scenarios, we have designed Chain-of-Thought (CoT) prompts
for MLLMs to reduce hallucinations. We tested both methods on public benchmarks
such as ChartQA, OpenCQA, and ChartX, demonstrating improved performance and
validating the effectiveness of our proposed methods.; 12) Weibull Processes in Network Degree Distributions; This study examines degree distributions in two large collaboration networks:
the Microsoft Academic Graph (1800-2020) and Internet Movie Database
(1900-2020), comprising $2.72 \times 10^8$ and $1.88 \times 10^6$ nodes
respectively. Statistical comparison using $\chi^2$ measures showed that
Weibull distributions fit the degree distributions better than power-law or
log-normal models, especially at later stages in the network evolution. The
Weibull shape parameters exhibit notable stability ($k \approx 0.8$-$1.0$ for
academic, $k \approx 0.9$-$1.1$ for entertainment collaborations) despite
orders of magnitude growth in network size. While early-stage networks display
approximate power-law scaling, mature networks develop characteristic
flattening in the low-degree region that Weibull distributions appear to
capture better. In the academic network, the cutoff between the flattened
region and power-law tail shows a gradual increase from $5$ to $9$ edges over
time, while the entertainment network maintains a distinctive degree structure
that may reflect storytelling and cast-size constraints. These patterns suggest
the possibility that collaboration network evolution might be influenced more
by constraint-based growth than by pure preferential attachment or
multiplicative processes.; 13) Solar prosumage under different pricing regimes: Interactions with the
  transmission grid; Solar prosumers, residential electricity consumers equipped with photovoltaic
(PV) systems and battery storage, are transforming electricity markets. Their
interactions with the transmission grid under varying tariff designs are not
yet fully understood. We explore the influence of different pricing regimes on
prosumer investment and dispatch decisions and their subsequent impact on the
transmission grid. Using an integrated modeling approach that combines two
open-source dispatch, investment and grid models, we simulate prosumage
behavior in Germany's electricity market under real-time pricing or
time-invariant pricing, as well as under zonal or nodal pricing. Our findings
show that zonal pricing favors prosumer investments, while time-invariant
pricing rather hinders it. In comparison, regional solar availability emerges
as a larger driver for rooftop PV investments. The impact of prosumer
strategies on grid congestion remains limited within the scope of our
model-setup, in which home batteries cannot be used for energy arbitrage.; 14) Excitability and oscillations of active droplets; In living cells, cycles of formation and dissolution of liquid droplets can
mediate biological functions such as DNA repair. However, the minimal
physicochemical prerequisite for such droplet oscillations remains elusive.
Here, we present a simple model composed of only two independent chemical
components with their diffusive and chemical fluxes governed by non-equilibrium
thermodynamics. There is turnover of fuel that maintains a chemical reaction
away from equilibrium, leading to active droplets. We find that a single active
droplet undergoes a pitchfork-bifurcation in the droplet volume upon increasing
the fueling strength. Strikingly, the active droplet becomes excitable upon
adding a further chemical reaction. For sufficient fueling, the system
undergoes self-sustained oscillations cycling between droplet formation and
dissolution. The minimal nature of our model suggests self-sustained active
droplets as functional modules for de novo life.; 15) KANITE: Kolmogorov-Arnold Networks for ITE estimation; We introduce KANITE, a framework leveraging Kolmogorov-Arnold Networks (KANs)
for Individual Treatment Effect (ITE) estimation under multiple treatments
setting in causal inference. By utilizing KAN's unique abilities to learn
univariate activation functions as opposed to learning linear weights by
Multi-Layer Perceptrons (MLPs), we improve the estimates of ITEs. The KANITE
framework comprises two key architectures: 1.Integral Probability Metric (IPM)
architecture: This employs an IPM loss in a specialized manner to effectively
align towards ITE estimation across multiple treatments. 2. Entropy Balancing
(EB) architecture: This uses weights for samples that are learned by optimizing
entropy subject to balancing the covariates across treatment groups. Extensive
evaluations on benchmark datasets demonstrate that KANITE outperforms
state-of-the-art algorithms in both $\epsilon_{\text{PEHE}}$ and
$\epsilon_{\text{ATE}}$ metrics. Our experiments highlight the advantages of
KANITE in achieving improved causal estimates, emphasizing the potential of
KANs to advance causal inference methodologies across diverse application
areas.; 16) Colorful Vertex Recoloring of Bipartite Graphs; In vertex recoloring, we are given $n$ vertices with their initial coloring,
and edges arrive in an online fashion. The algorithm must maintain a valid
coloring by recoloring vertices, at a cost. The problem abstracts a scenario of
job placement in machines (possibly in the cloud), where vertices represent
jobs, colors represent machines, and edges represent ``anti affinity''
(disengagement) constraints. Online recoloring is a hard problem. One family of
instances which is fairly well-understood is bipartite graphs, in which two
colors are sufficient to satisfy all constraints. In this case it is known that
the competitive ratio of vertex recoloring is $\Theta(\log n)$.
  We propose a generalization of the problem, which allows using additional
colors (possibly at a higher cost), to improve overall performance. We analyze
the simple case of bipartite graphs of bounded largest \emph{bond} (a bond of a
connected graph is an edge-cut that partitions the graph into two connected
components). First, we propose two algorithms. One exhibits a trade-off for the
uniform-cost case: given $\Omega(\log\beta)\le c\le O(\log n)$ colors, the
algorithm guarantees that its cost is at most $O(\frac{\log n}{c})$ times the
optimal offline cost for two colors, where $n$ is the number of vertices and
$\beta$ is the size of the largest bond. The other algorithm is for the case
where the additional colors come at a higher cost, $D>1$: given $\Delta$
additional colors, where $\Delta$ is the maximum degree in the graph, the
algorithm guarantees $O(\log D)$ competitiveness. As to lower bounds, we show
that if the cost of the extra colors is $D>1$, no (randomized) algorithm can
achieve a competitive ratio of $o(\log D)$. We also show that for bipartite
graphs of unbounded bond size, any deterministic online algorithm has
competitive ratio $\Omega(\min(D,\log n))$.; 17) Statistical estimation of a mean-field FitzHugh-Nagumo model; We consider an interacting system of particles with value in $\mathbb{R}^d
\times \mathbb{R}^d$, governed by transport and diffusion on the first
component, on that may serve as a representative model for kinetic models with
a degenerate component. In a first part, we control the fluctuations of the
empirical measure of the system around the solution of the corresponding
Vlasov-Fokker-Planck equation by proving a Bernstein concentration inequality,
extending a previous result of arXiv:2011.03762 in several directions. In a
second part, we study the nonparametric statistical estimation of the classical
solution of Vlasov-Fokker-Planck equation from the observation of the empirical
measure and prove an oracle inequality using the Goldenshluger-Lepski
methodology and we obtain minimax optimality. We then specialise on the
FitzHugh-Nagumo model for populations of neurons. We consider a version of the
model proposed in Mischler et al. arXiv:1503.00492 an optimally estimate the
$6$ parameters of the model by moment estimators.; 18) Standardised schema and taxonomy for AI incident databases in critical
  digital infrastructure; The rapid deployment of Artificial Intelligence (AI) in critical digital
infrastructure introduces significant risks, necessitating a robust framework
for systematically collecting AI incident data to prevent future incidents.
Existing databases lack the granularity as well as the standardized structure
required for consistent data collection and analysis, impeding effective
incident management. This work proposes a standardized schema and taxonomy for
AI incident databases, addressing these challenges by enabling detailed and
structured documentation of AI incidents across sectors. Key contributions
include developing a unified schema, introducing new fields such as incident
severity, causes, and harms caused, and proposing a taxonomy for classifying AI
incidents in critical digital infrastructure. The proposed solution facilitates
more effective incident data collection and analysis, thus supporting
evidence-based policymaking, enhancing industry safety measures, and promoting
transparency. This work lays the foundation for a coordinated global response
to AI incidents, ensuring trust, safety, and accountability in using AI across
regions.; 19) Improved Sublinear-time Moment Estimation using Weighted Sampling; In this work we study the {\it moment estimation} problem using weighted
sampling. Given sample access to a set $A$ with $n$ weighted elements, and a
parameter $t>0$, we estimate the $t$-th moment of $A$ given as $S_t=\sum_{a\in
A} w(a)^t$. For t=1, this is the sum estimation problem. The moment estimation
problem along with a number of its variants have been extensively studied in
streaming, sublinear and distributed communication models. Despite being well
studied, we don't yet have a complete understanding of the sample complexity of
the moment estimation problem in the sublinear model and in this work, we make
progress on this front. On the algorithmic side, our upper bounds match the
known upper bounds for the problem for $t>1$. To the best of our knowledge, no
sublinear algorithms were known for this problem for $0<t<1$. We design a
sublinear algorithm for this problem for $t>1/2$ and show that no sublinear
algorithms exist for $t\leq 1/2$. We prove a $\Omega(\frac{n^{1-1/t}\ln
1/\delta}{\epsilon^2})$ lower bound for moment estimation for $t>1$, and show
optimal sample complexity bound $\Theta(\frac{n^{1-1/t}\ln
1/\delta}{\epsilon^2})$ for moment estimation for $t\geq 2$. Hence, we obtain a
complete understanding of the sample complexity for moment estimation using
proportional sampling for $t\geq 2$. We also study the moment estimation
problem in the beyond worst-case analysis paradigm and identify a new {\it
moment-density} parameter of the input that characterizes the sample complexity
of the problem using proportional sampling and derive tight sample complexity
bounds with respect to that parameter. We also study the moment estimation
problem in the hybrid sampling framework in which one is given additional
access to a uniform sampling oracle and show that hybrid sampling framework
does not provide any additional gain over the proportional sampling oracle in
the worst case.; 20) Structures of Monoids Motivated by DNA Origami; We construct a class of monoids, called origami monoids, motivated by Jones
monoids and by strand organization in DNA origami structures. Two types of
basic building blocks of DNA origami closely associated with the graphical
representation of Jones monoids are identified and are taken as generators for
the origami monoid. Motivated by plausible modifications of the DNA origami
structures and the relations of the well studied Jones monoids, we then
identify a set of relations that characterize the origami monoid. These
relations expand the relations of the Jones monoids and include a new set of
relations called contextual commutation. With contextual commutation, certain
generators commute only when found within a given context. We prove that the
origami monoids are finite and propose a normal form representation of their
elements. We establish a correspondence between the Green's classes of the
origami monoid and the Green's classes of a direct product of Jones monoids.; 21) Real-time Monitoring of Economic Shocks using Company Websites; Understanding the effects of economic shocks on firms is critical for
analyzing economic growth and resilience. We introduce a Web-Based Affectedness
Indicator (WAI), a general-purpose tool for real-time monitoring of economic
disruptions across diverse contexts. By leveraging Large Language Model (LLM)
assisted classification and information extraction on texts from over five
million company websites, WAI quantifies the degree and nature of firms'
responses to external shocks. Using the COVID-19 pandemic as a specific
application, we show that WAI is highly correlated with pandemic containment
measures and reliably predicts firm performance. Unlike traditional data
sources, WAI provides timely firm-level information across industries and
geographies worldwide that would otherwise be unavailable due to institutional
and data availability constraints. This methodology offers significant
potential for monitoring and mitigating the impact of technological, political,
financial, health or environmental crises, and represents a transformative tool
for adaptive policy-making and economic resilience.; 22) Establishing tool support for a concept DSL; The quality of software products tends to correlate with the quality of the
abstractions adopted early in the design process. Acknowledging this tendency
has led to the development of various tools and methodologies for modeling
systems thoroughly before implementing them. However, creating effective
abstract models of domain problems is difficult, especially if the models are
also expected to exhibit qualities such as intuitiveness, being seamlessly
integrable with other models, or being easily translatable into code.
  This thesis describes Conceptual, a DSL for modeling the behavior of software
systems using self-contained and highly reusable units of functionally known as
concepts. The language's syntax and semantics are formalized based on previous
work. Additionally, the thesis proposes a strategy for mapping language
constructs from Conceptual into the Alloy modeling language. The suggested
strategy is then implemented with a simple compiler, allowing developers to
access and utilize Alloy's existing analysis tools for program reasoning.
  The utility and expressiveness of Conceptual is demonstrated qualitatively
through several practical case studies. Using the implemented compiler, a few
erroneous specifications are identified in the literature. Moreover, the thesis
establishes preliminary tool support in the Visual Studio Code IDE.; 23) Similar-mass versus diverse-mass planetary systems in wind-driven
  accretion discs; Many close-in multiple-planet systems show a peas-in-a-pod trend, where
neighbouring planets have similar sizes, masses, and orbital spacing. Others,
including the Solar System, have a more diverse size and mass distribution.
Classical planet formation models tend to produce the former rather than the
latter, and the origin of this difference remains unclear. Recent studies
suggest disc evolution is largely driven by magnetic winds rather than
viscosity alone. In such wind-driven accretion discs, the mass accretion rate
varies radially instead of being constant, as in classical viscous discs. We
investigate how the wind's efficiency in removing disc mass affects planet
formation and migration. We performed single-core planet formation simulations
via pebble accretion in wind-driven accretion discs. We varied wind efficiency
via the magnetic lever arm parameter $ \lambda $ and studied outcomes by
considering a range of initial disc masses and accretion timescales. Our
simulations show that higher $ \lambda $ discs, with less wind mass loss, lead
to faster formation and migration, generating similar-mass planetary systems.
Lower $ \lambda $ discs lead to slower formation and migration and more
diverse-mass planetary systems. A mass jump occurs for all $ \lambda $ cases
when planet formation and disc accretion timescales are comparable, but is
larger for lower $ \lambda $ discs. Super-Earth systems with cold Jupiters form
more frequently in metal-rich discs, consistent with observations. Our
simulations suggest similar-mass and diverse-mass systems are approximately
separated at $ \lambda \sim 2\text{--}3 $.; 24) Compactification of homology cells, Fujita's conjectures and the complex
  projective space; We show that a compact K\""ahler manifold $M$ containing a smooth connected
divisor $D$ such that $M \setminus D$ is a homology cell, e.g., contractible,
must be projective space with $D$ a hyperplane, provided $\dim M \not \equiv 3
\pmod 4$. This answers conjectures of Fujita in these dimensions.; 25) The action of the Morava stabilizer group on the coefficients of Morava
  E-theory at height 2; We calculate an explicit closed formula for the action of the height 2 full
Morava stabilizer group on the coefficient ring of height 2 Morava E-theory. In
particular, this yields an explicit, surprisingly simple closed formula for the
action of the automorphism group of a height 2 formal group law on its
Lubin-Tate deformation ring. The formula is of a combinatorial nature, given by
sums over certain labelled ordered rooted trees.; 26) Cryptocurrency Network Analysis; Cryptocurrency network analysis consists of applying the tools and methods of
social network analysis to transactional data issued from cryptocurrencies. The
main difference with most online social networks is that users do not exchange
textual content but instead value -- in systems designed mainly as
cryptocurrency, such as Bitcoin -- or digital items and services in more
permissive systems based on smart contracts such as Ethereum.; 27) Classification of pair symmetries in superconductors with unconventional
  magnetism; We consider superconductors with unconventional magnetism and investigate the
emergence of superconducting correlations by carrying out a full classification
of allowed Cooper pair symmetries. In particular, we focus on spin-singlet and
spin-triplet superconductors under the influence of $d$-wave altermagnets and
$p$-wave magnets. Under generic conditions, we find that unconventional magnets
not only drive a spin-singlet to spin-triplet conversion but also they transfer
their parity symmetry that induces superconducting correlations with higher
angular momentum. For instance, a conventional spin-singlet $s$-wave
superconductor with $d$-wave altermagnetism is able to host odd-frequency mixed
spin-triplet $d$-wave superconducting pair amplitudes, while when combining
with $p$-wave magnetism the emerging superconducting pairing acquires an
even-frequency mixed spin-triplet $p$-wave symmetry. We further demonstrate
that unconventional magnetism produces even more exotic superconducting
correlations in spin-singlet $d$-wave superconductors, where odd-frequency
mixed spin-triplet $g$-wave and even-frequency mixed spin-triplet $f$-wave pair
symmetries are possible in altermagnets and $p$-wave magnets, respectively. We
also discuss how these ideas generalize to spin-triplet $p$-wave
superconductors and also show how our results can be applied to unconventional
magnets with higher angular momentum, such as $f$-, $g$-, and $i$-wave. Our
results can help understand the emergent superconducting correlations due to
the interplay of unconventional magnetism and superconductivity.; 28) Cataclysmic Variables in Triples: Formation Models and New Discoveries; The formation of cataclysmic variables (CVs) has long been modeled as a
product of common envelope evolution (CEE) in isolated binaries. However, a
significant fraction of intermediate-mass stars -- the progenitors of the white
dwarfs (WDs) in CVs -- are in triples. We therefore investigate the importance
of triple star dynamics in CV formation. Using Gaia astrometry and existing CV
catalogs, we construct a sample of $\sim50$ CVs in hierarchical triples within
1 kpc of the Sun, containing main-sequence (MS) and WD tertiaries at
separations of 100 - 30,000 au. We infer that at least 10% of CVs host wide
tertiaries. To interpret this discovery, we evolve a population of 2000 triples
using detailed three-body simulations, 47 of which become CVs. We predict that
20% of CVs in triples form without ever experiencing CEE, where the WD and
donor are brought together by the eccentric Kozai-Lidov (EKL) mechanism after
the formation of the WD. These systems favor larger donor stars and longer
birth orbital periods (8-20 hrs) than typical CVs. Among systems that do
undergo CEE, about half would not have interacted without the presence of the
tertiary. Triple formation channels both with and without CEE require initially
wide inner orbits ($\gtrsim 1$ au), which in turn require larger tertiary
separations to be stable. Consistent with this prediction, we find that the
observed Gaia CV triples have wider separations on average than normal wide
binaries selected in the same way. Our work underscores the importance of
triples in shaping interacting binary populations including CVs, ultracompact
binaries, and low-mass X-ray binaries.; 29) Measuring the Impact of Technical Debt on Development Effort in Software
  Projects; Technical debt refers to the trade-offs between code quality and faster
delivery, impacting future development with increased complexity, bugs, and
costs. This study empirically analyzes the additional work effort caused by
technical debt in software projects, focusing on feature implementations. I
explore how delaying technical debt repayment through refactoring influences
long-term work effort. Using data from open-source and enterprise projects, I
correlate technical debt with practical work effort, drawing from issue
trackers and version control systems. Our goal is to provide a framework for
managing technical debt, aiding developers, project managers, and stakeholders
in understanding and mitigating its impact on productivity and costs.; 30) DCAMamba: Mamba-based Rapid Response DC Arc Fault Detection; In electrical equipment, even minor contact issues can lead to arc faults.
Traditional methods often struggle to balance the accuracy and rapid response
required for effective arc fault detection. To address this challenge, we
introduce DCAMamba, a novel framework for arc fault detection. Specifically,
DCAMamba is built upon a state-space model (SSM) and utilizes a hardware-aware
parallel algorithm, designed in a cyclic mode using the Mamba architecture. To
meet the dual demands of high accuracy and fast response in arc fault
detection, we have refined the original Mamba model and incorporated a Feature
Amplification Strategy (FAS), a simple yet effective method that enhances the
model's ability to interpret arc fault data. Experimental results show that
DCAMamba, with FAS, achieves a 12$\%$ improvement in accuracy over the original
Mamba, while maintaining an inference time of only 1.87 milliseconds. These
results highlight the significant potential of DCAMamba as a future backbone
for signal processing. Our code will be made open-source after peer review.; 31) Universal Efimov Scaling in the Rabi-Coupled Few-Body Spectrum; We investigate the behavior of the Efimov effect -- a universal quantum
few-body phenomenon -- in the presence of an external driving field.
Specifically, we consider up to three bosonic atoms, such as $^{133}$Cs,
interacting with a light atom, such as $^{6}$Li, where the latter has two
internal spin states $\{\uparrow, \downarrow\}$ that are Rabi coupled. Assuming
that only the spin-$\uparrow$ light atom interacts with the bosons, we find
that the Rabi drive transposes the entire Efimov spectrum such that the Efimov
trimers and tetramers are centered around the Rabi-shifted two-body scattering
resonance. Crucially, we show that the Rabi drive preserves the trimers'
discrete scaling symmetry, while universally shifting the Efimov three-body
parameter, leading to a log-periodic modulation in the spectrum as the Rabi
drive is varied. Our results suggest that Efimov physics can be conveniently
explored using an applied driving field, opening up the prospect of an
externally tunable three-body parameter.; 32) Degree of freedom count in linear gauge invariant PDE systems; Suppose a system of partial differential equations with constant coefficients
describes a classical field theory. Einstein proposed a definition of the
strength of such a theory and its degrees of freedom (DoF) based on the
asymptotic number of free Taylor series coefficients of bounded degree in the
general solution of the system. however, direct calculating the DoF is a
nontrivial task. Here, we apply commutative algebra methods to this problem.
  We begin by interpreting the matrix of the system as a linear map between
polynomial modules. First, we derive an explicit formula for the DoF as the
multiplicity of a certain extension module. Second, we prove (for homogeneous
and certain more general systems) another explicit formula for the DoF in terms
of gauge symmetries and identities. A notable consequence of this formula is
that two Hermitian conjugate systems have identical DoF.; 33) Zero-Shot End-to-End Relation Extraction in Chinese: A Comparative Study
  of Gemini, LLaMA and ChatGPT; This study investigates the performance of various large language models
(LLMs) on zero-shot end-to-end relation extraction (RE) in Chinese, a task that
integrates entity recognition and relation extraction without requiring
annotated data. While LLMs show promise for RE, most prior work focuses on
English or assumes pre-annotated entities, leaving their effectiveness in
Chinese RE largely unexplored. To bridge this gap, we evaluate ChatGPT, Gemini,
and LLaMA based on accuracy, efficiency, and adaptability. ChatGPT demonstrates
the highest overall performance, balancing precision and recall, while Gemini
achieves the fastest inference speed, making it suitable for real-time
applications. LLaMA underperforms in both accuracy and latency, highlighting
the need for further adaptation. Our findings provide insights into the
strengths and limitations of LLMs for zero-shot Chinese RE, shedding light on
trade-offs between accuracy and efficiency. This study serves as a foundation
for future research aimed at improving LLM adaptability to complex linguistic
tasks in Chinese NLP.; 34) PodAgent: A Comprehensive Framework for Podcast Generation; Existing Existing automatic audio generation methods struggle to generate
podcast-like audio programs effectively. The key challenges lie in in-depth
content generation, appropriate and expressive voice production. This paper
proposed PodAgent, a comprehensive framework for creating audio programs.
PodAgent 1) generates informative topic-discussion content by designing a
Host-Guest-Writer multi-agent collaboration system, 2) builds a voice pool for
suitable voice-role matching and 3) utilizes LLM-enhanced speech synthesis
method to generate expressive conversational speech. Given the absence of
standardized evaluation criteria for podcast-like audio generation, we
developed comprehensive assessment guidelines to effectively evaluate the
model's performance. Experimental results demonstrate PodAgent's effectiveness,
significantly surpassing direct GPT-4 generation in topic-discussion dialogue
content, achieving an 87.4% voice-matching accuracy, and producing more
expressive speech through LLM-guided synthesis. Demo page:
https://podcast-agent.github.io/demo/. Source code:
https://github.com/yujxx/PodAgent.; 35) Natural Reference Frames within Video Analysis; This study explores an alternative approach to video-based motion analysis
using natural reference frames rather than relying on manual alignment. We
demonstrate how the motion data itself can reveal optimal reference frames,
connecting fundamental physical principles with data analysis. We present three
classical mechanics experiments: uniform motion along a track, where the motion
direction naturally defines the axis; circular motion of a bicycle wheel, where
the center of rotation emerges as the reference origin; and projectile motion
from a juggling tutorial, where gravity determines the vertical direction. This
approach demonstrates how physical constraints and symmetries can naturally
emerge from experimental data, providing a viable approach for introductory
physics laboratories.; 36) AI-based Framework for Robust Model-Based Connector Mating in Robotic
  Wire Harness Installation; Despite the widespread adoption of industrial robots in automotive assembly,
wire harness installation remains a largely manual process, as it requires
precise and flexible manipulation. To address this challenge, we design a novel
AI-based framework that automates cable connector mating by integrating force
control with deep visuotactile learning. Our system optimizes
search-and-insertion strategies using first-order optimization over a
multimodal transformer architecture trained on visual, tactile, and
proprioceptive data. Additionally, we design a novel automated data collection
and optimization pipeline that minimizes the need for machine learning
expertise. The framework optimizes robot programs that run natively on standard
industrial controllers, permitting human experts to audit and certify them.
Experimental validations on a center console assembly task demonstrate
significant improvements in cycle times and robustness compared to conventional
robot programming approaches. Videos are available under
https://claudius-kienle.github.io/AppMuTT.; 37) Estimating Propensities of Selection for Big Datasets via Data
  Integration; Big data presents potential but unresolved value as a source for analysis and
inference. However,selection bias, present in many of these datasets, needs to
be accounted for so that appropriate inferences can be made on the target
population. One way of approaching the selection bias issue is to first
estimate the propensity of inclusion in the big dataset for each member of the
big dataset, and then to apply these propensities in an inverse probability
weighting approach to produce population estimates. In this paper, we provide
details of a new variant of existing propensity score estimation methods that
takes advantage of the ability to integrate the big data with a probability
sample. We compare the ability of this method to produce efficient inferences
for the target population with several alternative methods through an empirical
study.; 38) Enhancing Magnetic Coupling in MN4-Graphene via Strain Engineering; MN4-embedded graphene (MN4-G) layers, incorporating transition metal elements
(M), represent a class of experimentally accessible two-dimensional materials
with significant potential for stable nanoscale magnetization. In these
systems, magnetic exchange interactions are primarily governed by
Ruderman-Kittel-Kasuya-Yosida (RKKY) coupling, exhibiting an anomalously
prolonged decay of r to the power of (-n), where r is the M-M separation
distance and n is between 0.5 and 2. This study investigates the impact of
strain on the electronic and magnetic properties of MN4-G layers using
ab-initio density functional theory (DFT). A novel strain-engineering approach
is developed by applying controlled tension or compression to the layers. Our
findings reveal that strain significantly modulates the strength, amplitude,
and decay rate of the RKKY coupling. Notably, the CoN4-G layer demonstrates a
pronounced enhancement in RKKY coupling strength, oscillation amplitude, and
reduced decay rate under strain. Conversely, the CuN4-G layer exhibits distinct
behavior, maintaining decoupled spin chains and invariant electronic and
magnetic properties despite applied strain. This work underscores the
tunability of magnetic interactions in MN4-G layers via strain engineering,
providing insights into the design of strain-controlled magnetic materials for
next-generation spintronic applications.; 39) Nonlinear contractile response of actomyosin active gels to control
  signals; Biological systems tightly regulate their physiological state using control
signals. This includes the actomyosin cytoskeleton, a contractile active gel
that consumes chemical free energy to drive many examples of cellular
mechanical behavior. Upstream regulatory pathways activate or inhibit
actomyosin activity. However, the contractile response of the actomyosin
cytoskeleton to control signals remains poorly characterized. Here we employ
reconstituted actomyosin active gels and subject them to step and pulsatile
activation inputs. We find evidence for a nonlinear impulse response, which we
quantify via a transfer function $\delta \varepsilon / \delta g$ that relates
input free-energy pulses $\delta g$ to output strain pulses $\delta
\varepsilon$. We find a scaling relation $\delta \varepsilon / \delta g \sim
g^{-0.3}$. The negative sign of the exponent represents a decreased
effectiveness of a contracting gel in converting energy to strain. We ascribe
nonlinearity in our system to a density-dependent mechanism, which contrasts
strain-stiffening nonlinear responses to external stresses. Contractile
response to control signals is an essential step toward understanding how
information from mechanical signaling processes flow through actomyosin
networks in living, and likely also synthetic, cells.; 40) The Higgs Self-Coupling at FCC-ee; Single Higgs production at FCC-ee probes the Higgs self-coupling at
next-to-leading order (NLO). Extracting a bound requires a global analysis
accounting for other possible new physics contributions up to NLO. We determine
the FCC-ee sensitivity to Higgs self-coupling modifications
$\delta\kappa_\lambda$ within the Standard Model Effective Field Theory (SMEFT)
framework, including for the first time flavour, LEP, LHC, and FCC-ee
observables in a global analysis with all leading NLO effects via one-loop
renormalisation group evolution, as well as incorporating finite NLO
contributions to electroweak precision and $ZH$ observables. The global
sensitivity to $\delta\kappa_\lambda$ is estimated by marginalising over the
effects of all other operators, bringing flavour considerations to the fore. We
find that, under reasonable assumptions, FCC-ee sensitivity to
$\delta\kappa_\lambda$ can exceed that of the HL-LHC.; 41) SurgRIPE challenge: Benchmark of Surgical Robot Instrument Pose
  Estimation; Accurate instrument pose estimation is a crucial step towards the future of
robotic surgery, enabling applications such as autonomous surgical task
execution. Vision-based methods for surgical instrument pose estimation provide
a practical approach to tool tracking, but they often require markers to be
attached to the instruments. Recently, more research has focused on the
development of marker-less methods based on deep learning. However, acquiring
realistic surgical data, with ground truth instrument poses, required for deep
learning training, is challenging. To address the issues in surgical instrument
pose estimation, we introduce the Surgical Robot Instrument Pose Estimation
(SurgRIPE) challenge, hosted at the 26th International Conference on Medical
Image Computing and Computer-Assisted Intervention (MICCAI) in 2023. The
objectives of this challenge are: (1) to provide the surgical vision community
with realistic surgical video data paired with ground truth instrument poses,
and (2) to establish a benchmark for evaluating markerless pose estimation
methods. The challenge led to the development of several novel algorithms that
showcased improved accuracy and robustness over existing methods. The
performance evaluation study on the SurgRIPE dataset highlights the potential
of these advanced algorithms to be integrated into robotic surgery systems,
paving the way for more precise and autonomous surgical procedures. The
SurgRIPE challenge has successfully established a new benchmark for the field,
encouraging further research and development in surgical robot instrument pose
estimation.; 42) Connecting SPDE to SGMs; This paper investigates a Stochastic Partial Differential Equation (SPDE)
derived from the Fokker-Planck equation associated with Score-based Generative
Models. We modify the standard Fokker-Planck equation to better represent
practical SGMs and introduce noise to mitigate potential discretization issues.
The primary goal is to prove the existence and uniqueness of solutions for this
SPDE. This aspect requires careful consideration due to the time-dependent
operator and unbounded domain. To overcome these hurdles, we employ a
variational approach and introduce a novel space inspired by Ornstein-Uhlenbeck
operators. By demonstrating that this space and its subspace satisfy the
necessary assumptions, they establish the existence of a solution for the given
SPDE.; 43) Quant GANs: deep generation of financial time series; Modeling financial time series by stochastic processes is a challenging task
and a central area of research in financial mathematics. As an alternative, we
introduce Quant GANs, a data-driven model which is inspired by the recent
success of generative adversarial networks (GANs). Quant GANs consist of a
generator and discriminator function, which utilize temporal convolutional
networks (TCNs) and thereby achieve to capture long-range dependencies such as
the presence of volatility clusters. The generator function is explicitly
constructed such that the induced stochastic process allows a transition to its
risk-neutral distribution. Our numerical results highlight that distributional
properties for small and large lags are in an excellent agreement and
dependence properties such as volatility clusters, leverage effects, and serial
autocorrelations can be generated by the generator function of Quant GANs,
demonstrably in high fidelity.; 44) Statistical physics analysis of graph neural networks: Approaching
  optimality in the contextual stochastic block model; Graph neural networks (GNNs) are designed to process data associated with
graphs. They are finding an increasing range of applications; however, as with
other modern machine learning techniques, their theoretical understanding is
limited. GNNs can encounter difficulties in gathering information from nodes
that are far apart by iterated aggregation steps. This situation is partly
caused by so-called oversmoothing; and overcoming it is one of the practically
motivated challenges. We consider the situation where information is aggregated
by multiple steps of convolution, leading to graph convolutional networks
(GCNs). We analyze the generalization performance of a basic GCN, trained for
node classification on data generated by the contextual stochastic block model.
We predict its asymptotic performance by deriving the free energy of the
problem, using the replica method, in the high-dimensional limit. Calling depth
the number of convolutional steps, we show the importance of going to large
depth to approach the Bayes-optimality. We detail how the architecture of the
GCN has to scale with the depth to avoid oversmoothing. The resulting large
depth limit can be close to the Bayes-optimality and leads to a continuous GCN.
Technically, we tackle this continuous limit via an approach that resembles
dynamical mean-field theory (DMFT) with constraints at the initial and final
times. An expansion around large regularization allows us to solve the
corresponding equations for the performance of the deep GCN. This promising
tool may contribute to the analysis of further deep neural networks.; 45) Enhancement of Large Eddy Simulation for the prediction of an intake
  flow rig using sequential Data Assimilation; A Data Assimilation (DA) strategy based on an ensemble Kalman filter (EnKF)
is used to enhance the predictive capabilities of scale resolving numerical
tools for the analysis of flows exhibiting cyclic behaviour. More precisely, an
ensemble of numerical runs using Large Eddy Simulation (LES) for the
compressible steady-state flow rig is augmented via the integration of
high-fidelity data. This observation is in the form of instantaneous velocity
measurements, which are sampled at localized sensors in the physical domain.
Two objectives are targeted. The first one is the calibration of an unsteady
inlet condition suitable to capture the cyclic flow investigated. The second
one is the analysis of the synchronization of velocity field predicted by the
LES with the available observation. In order to reduce the computational costs
required for this analysis, a hyper-localization procedure (HLEnKF) is proposed
and it is integrated in the library CONES, tailored to perform fast online DA.
The proposed strategy performs a satisfactory calibration of the inlet
conditions, and its robustness is assessed using two different prior
distributions for the free parameters optimized in this task. DA state
estimation is efficient in obtaining accurate local synchronization of the
inferred velocity fields with the observed data. The modal analysis of the
kinetic energy of the flow field provides additional information on the quality
of the reconstruction of the velocity field, which shows improvements. Thus,
the HLEnKF shows promising features for the calibration and the synchronization
of scale-resolved turbulent flows, opening perspectives of applications for
complex phenomena using advanced tools such as digital twins.; 46) Confidence Improves Self-Consistency in LLMs; Self-consistency decoding enhances LLMs' performance on reasoning tasks by
sampling diverse reasoning paths and selecting the most frequent answer.
However, it is computationally expensive, as sampling many of these (lengthy)
paths is required to increase the chances that the correct answer emerges as
the most frequent one. To address this, we introduce Confidence-Informed
Self-Consistency (CISC). CISC performs a weighted majority vote based on
confidence scores obtained directly from the model. By prioritizing
high-confidence paths, it can identify the correct answer with a significantly
smaller sample size. When tested on nine models and four datasets, CISC
outperforms self-consistency in nearly all configurations, reducing the
required number of reasoning paths by over 40% on average. In addition, we
introduce the notion of within-question confidence evaluation, after showing
that standard evaluation methods are poor predictors of success in
distinguishing correct and incorrect answers to the same question. In fact, the
most calibrated confidence method proved to be the least effective for CISC.
Lastly, beyond these practical implications, our results and analyses show that
LLMs can effectively judge the correctness of their own outputs, contributing
to the ongoing debate on this topic.; 47) Does Hessian Data Improve the Performance of Machine Learning
  Potentials?; Integrating machine learning into reactive chemistry, materials discovery,
and drug design is revolutionizing the development of novel molecules and
materials. Machine Learning Interatomic Potentials (MLIPs) accurately predict
energies and forces at quantum chemistry levels, surpassing traditional
methods. Incorporating force fitting into MLIP training significantly improves
the representation of potential-energy surfaces (PES), enhancing model
transferability and reliability. This study introduces and evaluates
incorporating Hessian matrix training into MLIPs, capturing second-order
curvature information of PES. Our analysis specifically examines MLIPs trained
solely on stable molecular geometries, assessing their extrapolation
capabilities to non-equilibrium configurations. We show that integrating
Hessian information substantially improves MLIP performance in predicting
energies, forces, and Hessians for non-equilibrium structures. Hessian-trained
MLIPs notably enhance reaction pathway modeling, transition state
identification, and vibrational spectra accuracy, benefiting molecular dynamics
simulations and Nudged Elastic Band (NEB) calculations. By comparing models
trained with various combinations of energy, force, and Hessian data on a
small-molecule reactive dataset, we demonstrate Hessian inclusion leads to
improved accuracy in reaction modeling and vibrational analyses while
simultaneously reducing the total data needed for effective training. The
primary trade-off is increased computational expense, as Hessian training
demands more resources than conventional methods. Our results offer
comprehensive insights into the strengths and limitations of Hessian
integration in MLIP training, enabling practitioners in computational chemistry
to make informed decisions aligned with their research goals and available
computational resources.; 48) A Tensor-Train Decomposition based Compression of LLMs on Group Vector
  Systolic Accelerator; Large language models (LLMs) are both storage-intensive and
computation-intensive, posing significant challenges when deployed on
resource-constrained hardware. As linear layers in LLMs are mainly resource
consuming parts, this paper develops a tensor-train decomposition (TTD) for
LLMs with a further hardware implementation on FPGA. TTD compression is applied
to the linear layers in ChatGLM3-6B and LLaMA2-7B models with compression
ratios (CRs) for the whole network 1.94$\times$ and 1.60$\times$, respectively.
The compressed LLMs are further implemented on FPGA hardware within a highly
efficient group vector systolic array (GVSA) architecture, which has DSP-shared
parallel vector PEs for TTD inference, as well as optimized data communication
in the accelerator. Experimental results show that the corresponding TTD based
LLM accelerator implemented on FPGA achieves 1.45$\times$ and 1.57$\times$
reduction in first token delay for ChatGLM3-6B and LLaMA2-7B models,
respectively.; 49) The R-Vessel-X Project; 1) Objectives: This technical report presents a synthetic summary and the
principal outcomes of the project R-Vessel-X (""Robust vascular network
extraction and understanding within hepatic biomedical images"") funded by the
French Agence Nationale de la Recherche, and developed between 2019 and 2023.
2) Material and methods: We used datasets and tools publicly available such as
IRCAD, Bullitt or VascuSynth toobtain real or synthetic angiographic images.
The main contributions lie in the field of 3D angiographic image analysis:
filtering, segmentation, modeling and simulation, with a specific focus on the
liver. 3) Results: We paid a particular attention to open-source software
diffusion of the developed methods, by means of 3D Slicer plugins for the liver
anatomy segmentation (SlicerRVXLiverSegmentation) and vesselness filtering
(Slicer-RVXVesselnessFilters), and an online demo for the generation of
synthetic and realistic vessels in 2D and 3D (OpenCCO). 4) Conclusion: The
R-Vessel-X project provided extensive research outcomes, covering various
topics related to 3D angiographic image analysis, such as filtering,
segmentation, modeling and simulation. We also developed open-source and free
softwares so that the research communities in biomedical engineering can use
these results in their future research.; 50) Brain Controlled Wheelchair with Smart Feature; In Asia, many individuals with disabilities rely on wheelchairs for mobility.
However, some people, such as those who are fully disabled or paralyzed, cannot
use traditional wheelchairs despite having fully functioning cognitive
abilities. To address this issue, we propose the development of an electric
wheelchair that can be controlled using EEG signals and eye blinks. The project
utilizes a MindWave Mobile device and Arduino to enable seamless control.
Additionally, various sensors are incorporated to enhance the system's
reliability. An ultrasonic sensor helps avoid unexpected collisions, while a
smoke sensor detects hazardous smoke levels, triggering an automatic alert via
a short message to a designated person. Similarly, if the passenger falls from
the wheelchair, a notification will also be sent. The wheelchair's movement is
controlled via an Android application, with eye-blink detection serving as the
primary input method for navigation. This innovative design offers a
cost-effective solution, making it accessible for widespread use. By
integrating these advanced features, the system can be implemented on motorized
wheelchairs to better support individuals with disabilities and enhance their
independence.; 51) Ripples spreading across the Galactic disc: Interplay of direct and
  indirect effects of the Sagittarius dwarf impact; Gaia data have revealed vertically asymmetric phase-space structures in the
Milky Way (MW) disc, such as phase spirals, indicating vertical oscillations.
These oscillations exhibit two distinct modes: the bending mode and the
breathing mode, associated with one-arm and two-arm phase spirals,
respectively. This study aims to explore the excitation mechanisms of the
bending and breathing modes and their subsequent evolution in the MW disc,
focusing on the interplay between direct perturbations from the Sagittarius
dwarf galaxy and indirect contributions from tidally induced spiral arms. We
perform high-resolution $N$-body simulations to model the interaction between
an MW-like disc galaxy and a Sagittarius dwarf-like satellite. These
simulations resolve fine phase-space structures, enabling analysis of the
bending and breathing modes at both macroscopic (global bending and breathing
waves) and microscopic (local phase spirals) scales. Our simulations
demonstrate that the satellite's perturbation directly excites the bending mode
and induces spiral arms in the galactic disc. These spiral arms excite the
breathing mode, making it an indirect consequence of the satellite interaction.
Initially, the bending mode dominates, but it rapidly decays due to horizontal
mixing. In contrast, the breathing mode persists for a longer duration,
sustained by the spiral arms, leading to a transition from a bending-dominated
to a breathing-dominated state. This transition progresses faster in the inner
galaxy than in the outer regions. The simulations reproduce the one-arm phase
spiral observed in the solar neighbourhood and reveal two-arm phase spirals,
particularly in the inner galaxy, associated with spiral arm-induced breathing
modes. Our findings highlight the combined effects of direct satellite
perturbations and indirect spiral arm dynamics in shaping the vertical
structure of the MW disc.; 52) Unraveling phase transformation with phononic hyperbolicity using
  off-resonant terahertz light; Noncontacting and nondestructive control of geometric phase in conventional
semiconductors plays a pivotal role in various applications. In the current
work, we present a theoretical and computational investigation on terahertz
(THz) light-induced phase transformation of conventional binary semiconducting
compounds among different structures including rock-salt, zinc-blende,
wurtzite, and hexagonal phases. Using MgS and MgSe as prototypical examples, we
perform anharmonic phonon mediated calculations and reveal large contrasting
lattice contributed dielectric susceptibility in the THz regime. We then
construct a THz-induced phase diagram under intermediate temperature and reveal
rock-salt to hexagonal and then wurtzite structure transformations with
increasing light intensity. This does not require a high temperature
environment as observed in traditional experiments. The low energy barrier
suggests that the phase transition kinetics can be fast, and the stable room
temperature phonon dispersions guarantee their non-volatile nature.
Furthermore, we disclose the phononic hyperbolicity with strong anisotropic THz
susceptibility components, which serves as a natural hyperbolic material with
negative refractive index. Our work suggests the potential to realize
metastable hidden phases using noninvasive THz irradiation, which expands the
conventional pressure-temperature ($P-T$) phase diagram by adding light as an
additional control factor.; 53) Polarizabilities of low-lying states of silver; Assembly of ultracold polar molecules containing silver (Ag) from
laser-cooled atoms requires knowledge of the dynamic polarizabilities of Ag at
convenient laser wavelengths. We present calculations and analysis of the
energies and electric-dipole dc and ac polarizabilities of the low-lying states
of neutral Ag. Calculations of the properties of the 4d^{10}x states, where
x=5s,6s,7s,5p,6p,7p,5d,6d, and 4f, are performed using the linearized coupled
cluster single-double method. The properties of the 4d^9 5s^2 ^2D_{5/2,3/2}
states are obtained within the framework of configuration interaction with 11
and 17 electrons in the valence field. We analyze the different contributions
to the polarizabilities and estimate the uncertainties of our predictions.; 54) Domain Adaptation from Generated Multi-Weather Images for Unsupervised
  Maritime Object Classification; The classification and recognition of maritime objects are crucial for
enhancing maritime safety, monitoring, and intelligent sea environment
prediction. However, existing unsupervised methods for maritime object
classification often struggle with the long-tail data distributions in both
object categories and weather conditions. In this paper, we construct a dataset
named AIMO produced by large-scale generative models with diverse weather
conditions and balanced object categories, and collect a dataset named RMO with
real-world images where long-tail issue exists. We propose a novel domain
adaptation approach that leverages AIMO (source domain) to address the problem
of limited labeled data, unbalanced distribution and domain shift in RMO
(target domain), and enhance the generalization of source features with the
Vision-Language Models such as CLIP. Experimental results shows that the
proposed method significantly improves the classification accuracy,
particularly for samples within rare object categories and weather conditions.
Datasets and codes will be publicly available at
https://github.com/honoria0204/AIMO.; 55) Robust ab initio predictions for dimensionless ratios of E2 and radius
  observables. II. Estimation of E2 transition strengths by calibration to the
  charge radius; Converged results for E2 observables are notoriously challenging to obtain in
ab initio no-core configuration interaction (NCCI) approaches. Matrix elements
of the E2 operator are sensitive to the large-distance tails of the nuclear
wave function, which converge slowly in an oscillator basis expansion. Similar
convergence challenges beset ab initio prediction of the nuclear charge radius.
However, we exploit systematic correlations between the calculated E2 and
radius observables to yield meaningful predictions for relations among these
observables. In particular, we examine ab initio predictions for dimensionless
ratios of the form B(E2)/(e^2r^4), for nuclei throughout the p shell.
Meaningful predictions for E2 transition strengths may then be made by
calibrating to the ground-state charge radius, if experimentally known.; 56) Riemann surface foliations with non-discrete singular set; Let $\mathcal{F}$ be a singular Riemann surface foliation on a complex
manifold $M$, such that the singular set $E \subset M$ is non-discrete. We
study the behavior of the foliation near the singular set $E$, particularly
focusing on singular points that admit invariant submanifolds (locally) passing
through them. Our primary focus is on the singular points that are removable
singularities for some proper subfoliation. We classify singular points based
on the dimension of their invariant submanifold and, consequently, establish
that for hyperbolic foliations $\mathcal{F}$, the presence of such
singularities ensures the continuity of the leafwise Poincar\'{e} metric on $M
\setminus E$.; 57) Uniqueness of gauge covariant renormalisation of stochastic 3D
  Yang-Mills-Higgs; Local solutions to the 3D stochastic quantisation equations of
Yang-Mills-Higgs were constructed in (arXiv:2201.03487), and it was shown that,
in the limit of smooth mollifications, there exists a mass renormalisation of
the Yang-Mills field such that the solution is gauge covariant. In this paper
we prove uniqueness of the mass renormalisation that leads to gauge covariant
solutions. This strengthens the main result of (arXiv:2201.03487), and is
potentially important for the identification of the limit of other
approximations, such as lattice dynamics. Our proof relies on systematic
short-time expansions of singular stochastic PDEs and of regularised Wilson
loops. We also strengthen the recently introduced state spaces to allow finer
control on line integrals appearing in expansions of Wilson loops.; 58) Quantum Trojan Insertion: Controlled Activation for Covert Circuit
  Manipulation; Quantum computing has demonstrated superior efficiency compared to classical
computing. Quantum circuits are essential for implementing functions and
achieving correct computational outcomes. Quantum circuit compilers, which
translate high-level quantum operations into hardware-specific gates while
optimizing performance, serve as the interface between the quantum software
stack and physical quantum machines. However, untrusted compilers can introduce
malicious hardware Trojans into quantum circuits, altering their functionality
and leading to incorrect results. In the world of classical computing,
effective hardware Trojans are a critical threat to integrated circuits. This
process often involves stealthily inserting conditional logic gates that
activate under specific input conditions. In this paper, we propose a novel
advanced quantum Trojan that is controllable, allowing it to be activated or
deactivated under different circumstances. These Trojans remain dormant until
triggered by predefined input conditions, making detection challenging. Through
a series of benchmark experiments, we demonstrate the feasibility of this
method by evaluating the effectiveness of embedding controlled trojans in
quantum circuits and measuring their impact on circuit performance and
security.; 59) On the Representation Categories of Weak Hopf Algebras Arising from
  Levin-Wen Models; In their study of Levin-Wen models [Commun. Math. Phys. 313 (2012) 351-373],
Kitaev and Kong proposed a weak Hopf algebra associated with a unitary fusion
category $\mathcal{C}$ and a unitary left $\mathcal{C}$-module $\mathcal{M}$,
and sketched a proof that its representation category is monoidally equivalent
to the unitary $\mathcal{C}$-module functor category
$\mathrm{Fun}^{\mathrm{u}}_{\mathcal{C}}(\mathcal{M},\mathcal{M})^\mathrm{rev}$.
We give an independent proof of this result without the unitarity conditions.
In particular, viewing $\mathcal{C}$ as a left $\mathcal{C} \boxtimes
\mathcal{C}^{\mathrm{rev}}$-module, we obtain a quasi-triangular weak Hopf
algebra whose representation category is braided equivalent to the Drinfeld
center $\mathcal{Z}(\mathcal{C})$. In the appendix, we also compare this
quasi-triangular weak Hopf algebra with the tube algebra
$\mathrm{Tube}_{\mathcal{C}}$ of $\mathcal{C}$ when $\mathcal{C}$ is pivotal.
These two algebras are Morita equivalent by the well-known equivalence
$\mathrm{Rep}(\mathrm{Tube}_{\mathcal{C}})\cong\mathcal{Z}(\mathcal{C})$.
However, we show that in general there is no weak Hopf algebra structure on
$\mathrm{Tube}_{\mathcal{C}}$ such that the above equivalence is monoidal.; 60) Accurate AI-Driven Emergency Vehicle Location Tracking in Healthcare ITS
  Digital Twin; Creating a Digital Twin (DT) for Healthcare Intelligent Transportation
Systems (HITS) is a hot research trend focusing on enhancing HITS management,
particularly in emergencies where ambulance vehicles must arrive at the crash
scene on time and track their real-time location is crucial to the medical
authorities. Despite the claim of real-time representation, a temporal
misalignment persists between the physical and virtual domains, leading to
discrepancies in the ambulance's location representation. This study proposes
integrating AI predictive models, specifically Support Vector Regression (SVR)
and Deep Neural Networks (DNN), within a constructed mock DT data pipeline
framework to anticipate the medical vehicle's next location in the virtual
world. These models align virtual representations with their physical
counterparts, i.e., metaphorically offsetting the synchronization delay between
the two worlds. Trained meticulously on a historical geospatial dataset, SVR
and DNN exhibit exceptional prediction accuracy in MATLAB and Python
environments. Through various testing scenarios, we visually demonstrate the
efficacy of our methodology, showcasing SVR and DNN's key role in significantly
reducing the witnessed gap within the HITS's DT. This transformative approach
enhances real-time synchronization in emergency HITS by approximately 88% to
93%.; 61) Isometry groups of simply connected nonunimodular Lie groups of
  dimension four; For each left-invariant Riemannian metric on simply connected nonunimodular
Lie groups of dimension four, we determine the full group of isometries.; 62) Gotzmann's persistence theorem for Mori dream spaces; Gotzmann's persistence theorem provides a method for determining the Hilbert
polynomial of a subscheme of projective space by evaluating the Hilbert
function at only two points, irrespective of the dimension of the ambient
space. In arXiv:2405.02275 we established an analogue of Gotzmann's persistence
theorem for smooth projective toric varieties. We generalise our results to the
setting of Mori dream spaces, whose associated Cox rings are also finitely
generated. We also give an alternative, stronger, persistence result for points
in products of projective spaces.; 63) QCD sum rules study on weak decays $\Omega_c^0\to\Omega^- (\pi^+,
  \rho^+, l^+\nu_l)$; We study the weak decays of the charmed baryon $\Omega_c^0$ to the $\Omega^-$
baryon within the framework of QCD sum rules. A three-point correlation
function is defined for calculating eight form factors governing the
$\Omega_c^0 \to \Omega^-$ transition. The cutting rules are employed to extract
the double imaginary parts of the correlation functions, enabling their
expression in the form of dispersive integrals. These form factors are then
used to determine the branching fractions for the hadronic decays $\Omega_c^0
\to \Omega^- \pi^+$ and $\Omega_c^0 \to \Omega^- \rho^+$, as well as the
semi-leptonic decay $\Omega_c^0 \to \Omega^- \ell^+ \nu_\ell$. Our results are
compared with existing theoretical predictions and experimental data, providing
a comprehensive analysis of $\Omega_c^0$ decays and offering valuable insights
into the dynamics of charmed baryon decays.; 64) APB: Accelerating Distributed Long-Context Inference by Passing
  Compressed Context Blocks across GPUs; While long-context inference is crucial for advancing large language model
(LLM) applications, its prefill speed remains a significant bottleneck. Current
approaches, including sequence parallelism strategies and compute reduction
through approximate attention mechanisms, still fall short of delivering
optimal inference efficiency. This hinders scaling the inputs to longer
sequences and processing long-context queries in a timely manner. To address
this, we introduce APB, an efficient long-context inference framework that
leverages multi-host approximate attention to enhance prefill speed by reducing
compute and enhancing parallelism simultaneously. APB introduces a
communication mechanism for essential key-value pairs within a sequence
parallelism framework, enabling a faster inference speed while maintaining task
performance. We implement APB by incorporating a tailored FlashAttn kernel
alongside optimized distribution strategies, supporting diverse models and
parallelism configurations. APB achieves speedups of up to 9.2x, 4.2x, and 1.6x
compared with FlashAttn, RingAttn, and StarAttn, respectively, without any
observable task performance degradation. We provide the implementation and
experiment code of APB in https://github.com/thunlp/APB.; 65) Effects of correlated noise on the excitation of robust breathers in an
  ac-driven, lossy sine-Gordon system; Thermal noise and harmonic forcing have recently been shown to cooperatively
excite sine-Gordon breathers robust to dissipation. Such a phenomenon has been
found assuming a Gaussian noise source, delta-correlated both in time and
space. In light of the potential implications of this generation technique,
e.g., for the experimental observation of breathers in long Josephson
junctions, it is physically motivated to investigate the effects of more
realistic noise sources with finite correlation time and/or correlation length.
Here, breathers are demonstrated to still emerge under this broader class of
noise sources. The correlation time and the correlation length are found to
offer control over the probability of observing breathers, as well on the
typical timescale for their emergence. In particular, our results show that, as
compared to the thermal case, the temporal and spatial correlations in the
noise can lead to a larger breather-only occurrence frequency, i.e., the latter
quantity behaves nonmonotonically versus both the correlation time and the
correlation length. Overall, noise correlations represent a powerful tool for
controlling the excitation of the elusive breather modes in view of
experiments.; 66) Lattice calculation of the $D_s\mapsto X \ell \bar{\nu}_\ell$ inclusive
  decay rate: an overview; In this talks, on behalf of our collaboration we present an overview of our
first-principle lattice QCD calculation of the $D_s\mapsto X \ell
\bar{\nu}_\ell$ inclusive decay rate. Here we introduce the theoretical
background and focus on the methodological aspects of the calculation. A
detailed discussion of our results, including the comparison with the
corresponding experimental measurements will soon be presented in a forthcoming
publication.; 67) Temperatures of Robin Hood; Cumulative Games were introduced by Larsson, Meir, and Zick (2020) to bridge
some conceptual and technical gaps between Combinatorial Game Theory (CGT) and
Economic Game Theory. The partizan ruleset {\sc Robin Hood} is an instance of a
Cumulative Game, viz., {\sc Wealth Nim}. It is played on multiple heaps, each
associated with a pair of cumulations, interpreted here as wealth. Each player
chooses one of the heaps, removes tokens from that heap not exceeding their own
wealth, while simultaneously diminishing the other player's wealth by the same
amount. In CGT, the {\em temperature} of a {\em disjunctive sum} game component
is an estimate of the urgency of moving first in that component. It turns out
that most of the positions of {\sc Robin Hood} are {\em hot}. The temperature
of {\sc Robin Hood} on a single large heap shows a dichotomy in behavior
depending on the ratio of the wealths of the players. Interestingly, this
bifurcation is related to Pingala (Fibonacci) sequences and the Golden Ratio
$\phi$: when the ratio of the wealths lies in the interval $(\phi^{-1},\phi)$,
the temperature increases linearly with the heap size, and otherwise it remains
constant, and the mean values has a reciprocal property. It turns out that
despite {\sc Robin Hood} displaying high temperatures, playing in the hottest
component might be a sub-optimal strategy.; 68) Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and
  Refinement; The quality of Supervised Fine-Tuning (SFT) data plays a critical role in
enhancing the conversational capabilities of Large Language Models (LLMs).
However, as LLMs become more advanced, the availability of high-quality
human-annotated SFT data has become a significant bottleneck, necessitating a
greater reliance on synthetic training data. In this work, we introduce Condor,
a novel two-stage synthetic data generation framework that incorporates World
Knowledge Tree and Self-Reflection Refinement to produce high-quality SFT data
at scale. Our experimental results demonstrate that a base model fine-tuned on
only 20K Condor-generated samples achieves superior performance compared to
counterparts. The additional refinement stage in Condor further enables
iterative self-improvement for LLMs at various scales (up to 72B), validating
the effectiveness of our approach. Furthermore, our investigation into the
scaling for synthetic data in post-training reveals substantial unexplored
potential for performance improvements, opening promising avenues for future
research.; 69) Processing and Analyzing Real-World Driving Data: Insights on Trips,
  Scenarios, and Human Driving Behaviors; Analyzing large volumes of real-world driving data is essential for providing
meaningful and reliable insights into real-world trips, scenarios, and human
driving behaviors. To this end, we developed a multi-level data processing
approach that adds new information, segments data, and extracts desired
parameters. Leveraging a confidential but extensive dataset (over 1 million
km), this approach leads to three levels of in-depth analysis: trip, scenario,
and driving. The trip-level analysis explains representative properties
observed in real-world trips, while the scenario-level analysis focuses on
scenario conditions resulting from road events that reduce vehicle speed. The
driving-level analysis identifies the cause of driving regimes for specific
situations and characterizes typical human driving behaviors. Such analyses can
support the design of both trip- and scenario-based tests, the modeling of
human drivers, and the establishment of guidelines for connected and automated
vehicles.; 70) ADF22-WEB: Detection of a molecular gas reservoir in a massive quiescent
  galaxy located in a $z\approx3$ proto-cluster core; We present a study of the molecular gas reservoirs and dust contents in three
quiescent galaxies (QGs) located in the core of the $z=3.09$ SSA22
proto-cluster. Using the Atacama Large Millimeter/submillimeter Array (ALMA),
we detect CO(3--2) emission in one galaxy, ADF22-QG1, marking the first direct
detection of molecular gas in a quiescent galaxy from the early universe. The
detected galaxy, ADF22-QG1, has a molecular gas mass of log$M_{\rm
H_2}$/M$_\odot = 10.26 \pm 0.07$ assuming a CO-to-H$2$ conversion factor
$\alpha_{\rm CO} = 4.4$ (log$M_{\rm H_2}$/M$_\odot = 9.52 \pm 0.07$ for
$\alpha_{\rm CO} = 0.8$), corresponding to a gas mass fraction of $f_{\rm gas}
\approx 14\%$ (2.5\%). The gas-to-dust ratio $\delta _{\rm gdr}\gtrsim170$
($\delta_{\rm gdr}\gtrsim30$) for $\alpha_{\rm CO} = 4.4$ ($\alpha_{\rm CO}
=0.8$) is also derived for the first time for a QG at the epoch. For the other
two galaxies, ADF22-QG2 and ADF22-QG3, non detections of CO(3--2) emission
provide upper limits, $f_{\rm gas} \approx 17\%$ (3.1\%) and $f_{\rm gas}
\approx 13\%$ (2.4\%), respectively. The inferred gas-consumption history of
ADF22-QG1, based on its star-formation history, suggests that (i) dusty
star-forming galaxies (DSFGs) at $z = 4$--$6$ are plausible progenitors, and
(ii) the cessation of gas accretion from cosmic web filaments plays an
important role in their evolution to quenched systems. Furthermore, the
presence of a detectable molecular gas reservoir in ADF22-QG1 indicates that
additional mechanisms, such as morphological quenching, may be required to
fully explain its quiescent nature.; 71) 10 Years of Archival High-Resolution NIR Spectra: The Raw and Reduced
  IGRINS Spectral Archive (RRISA); The Immersion GRating INfrared Spectrometer (IGRINS) is a compact,
high-resolution (R~45,000) near-infrared spectrograph spanning 1.45 to 2.45 um
in a single exposure. We introduce the Raw and Reduced IGRINS Spectral Archive
(RRISA), which provides public data access for all non-proprietary IGRINS data
taken at McDonald Observatory's Harlan J. Smith Telescope, the Lowell Discovery
Telescope (formerly Discovery Channel Telescope), and Gemini South. RRISA
provides access to raw files, reduced data products, and cross-matched IGRINS
targets with the SIMBAD, 2MASS, Gaia DR3, APOGEE2 DR17, and PASTEL catalogs. We
also introduce version 3 of the IGRINS data reduction pipeline, IGRINS PLP v3,
which implements an improved cosmic ray correction, pattern noise removal, and
a new flexure correction that reduces telluric residuals. RRISA and supporting
information can be found at http://igrinscontact.github.io.; 72) Codimension one foliations on adjoint varieties; In this paper, we classify codimension one foliations on adjoint varieties
with most positive anti-canonical class. We show that on adjoint varieties with
Picard number one, these foliations are always induced by a pencil of
hyperplane sections with respect to their minimal embedding. For adjoint
varieties of Picard number two, there is more than one component of such
foliations, and we describe each of them. As a tool for understanding these
foliations, we introduce the concept of the degree of a foliation with respect
to a family of rational curves, which may be of independent interest.; 73) Phase-Sensitive Enhanced Absorption, Transmission and Slow Light in a
  Cross-cavity Magnomechanical System; We theoretically propose a scheme to explore the magnetically and
magnomechanically induced transparency phenomena in a cross-cavity
magnomechanical system, focusing on the role of relative phase and the
intensity of the two probing fields in enhancing the absorption and
transmission spectra and manipulating the group delay of the transmitted light.
Interestingly, the relative phase of the two probe fields could have
overwhelming effects on both the absorption spectrum and the group delay of the
output field. Tuning the relative phase and amplitude of the probe fields can
suppress or enhance the absorption and transmission spectra. The combined
effect of the magnon-photon and magnon-phonon couplings, along with relative
phase modulations, helps to switch the probe field's behavior from subluminal
to superluminal in the current system. The current study offers a
straightforward and practical approach, demonstrating the capability to employ
the relative phase for the modulation of microwave signals within the cavity
magnomechanical system, providing insights for the design of information
transduction and quantum sensing.; 74) Fast T2T: Optimization Consistency Speeds Up Diffusion-Based
  Training-to-Testing Solving for Combinatorial Optimization; Diffusion models have recently advanced Combinatorial Optimization (CO) as a
powerful backbone for neural solvers. However, their iterative sampling process
requiring denoising across multiple noise levels incurs substantial overhead.
We propose to learn direct mappings from different noise levels to the optimal
solution for a given instance, facilitating high-quality generation with
minimal shots. This is achieved through an optimization consistency training
protocol, which, for a given instance, minimizes the difference among samples
originating from varying generative trajectories and time steps relative to the
optimal solution. The proposed model enables fast single-step solution
generation while retaining the option of multi-step sampling to trade for
sampling quality, which offers a more effective and efficient alternative
backbone for neural solvers. In addition, within the training-to-testing (T2T)
framework, to bridge the gap between training on historical instances and
solving new instances, we introduce a novel consistency-based gradient search
scheme during the test stage, enabling more effective exploration of the
solution space learned during training. It is achieved by updating the latent
solution probabilities under objective gradient guidance during the alternation
of noise injection and denoising steps. We refer to this model as Fast T2T.
Extensive experiments on two popular tasks, the Traveling Salesman Problem
(TSP) and Maximal Independent Set (MIS), demonstrate the superiority of Fast
T2T regarding both solution quality and efficiency, even outperforming LKH
given limited time budgets. Notably, Fast T2T with merely one-step generation
and one-step gradient search can mostly outperform the SOTA diffusion-based
counterparts that require hundreds of steps, while achieving tens of times
speedup.; 75) Mixture of Experts for Recognizing Depression from Interview and Reading
  Tasks; Depression is a mental disorder and can cause a variety of symptoms,
including psychological, physical, and social. Speech has been proved an
objective marker for the early recognition of depression. For this reason, many
studies have been developed aiming to recognize depression through speech.
However, existing methods rely on the usage of only the spontaneous speech
neglecting information obtained via read speech, use transcripts which are
often difficult to obtain (manual) or come with high word-error rates
(automatic), and do not focus on input-conditional computation methods. To
resolve these limitations, this is the first study in depression recognition
task obtaining representations of both spontaneous and read speech, utilizing
multimodal fusion methods, and employing Mixture of Experts (MoE) models in a
single deep neural network. Specifically, we use audio files corresponding to
both interview and reading tasks and convert each audio file into log-Mel
spectrogram, delta, and delta-delta. Next, the image representations of the two
tasks pass through shared AlexNet models. The outputs of the AlexNet models are
given as input to a multimodal fusion method. The resulting vector is passed
through a MoE module. In this study, we employ three variants of MoE, namely
sparsely-gated MoE and multilinear MoE based on factorization. Findings suggest
that our proposed approach yields an Accuracy and F1-score of 87.00% and 86.66%
respectively on the Androids corpus.; 76) Limits to AI Growth: The Ecological and Social Consequences of Scaling; The accelerating development and deployment of AI technologies depend on the
continued ability to scale their infrastructure. This has implied increasing
amounts of monetary investment and natural resources. Frontier AI applications
have thus resulted in rising financial, environmental, and social costs. While
the factors that AI scaling depends on reach its limits, the push for its
accelerated advancement and entrenchment continues. In this paper, we provide a
holistic review of AI scaling using four lenses (technical, economic,
ecological, and social) and review the relationships between these lenses to
explore the dynamics of AI growth. We do so by drawing on system dynamics
concepts including archetypes such as ""limits to growth"" to model the dynamic
complexity of AI scaling and synthesize several perspectives. Our work maps out
the entangled relationships between the technical, economic, ecological and
social perspectives and the apparent limits to growth. The analysis explains
how industry's responses to external limits enables continued (but temporary)
scaling and how this benefits Big Tech while externalizing social and
environmental damages. To avoid an ""overshoot and collapse"" trajectory, we
advocate for realigning priorities and norms around scaling to prioritize
sustainable and mindful advancements.; 77) PHIBSS: Searching for Molecular Gas Outflows in Star-Forming Galaxies at
  $z =$ 0.5-2.6; We present an analysis of millimeter CO observations to search and quantify
signatures of molecular gas outflows. We exploit the large sample of $0.5 < z <
2.6$ galaxies observed as part of the PHIBSS1/2 surveys with the IRAM Plateau
de Bure interferometer, focusing on the 154 typical massive star-forming
galaxies with CO detections (mainly CO(3-2), but including also CO(2-1) and
CO(6-5)) at signal-to-noise (SNR) > 1.5 and available properties (stellar mass,
star formation rate, size) from ancillary data. None of the individual spectra
exhibit a compelling signature of CO outflow emission even at high SNR > 7. To
search for fainter outflow signatures, we carry out an analysis of stacked
spectra, including the full sample, as well as subsets, split in terms of
stellar mass, redshift, inclination, offset in star formation rate (SFR) from
the main sequence, and AGN activity. None of the physically motivated
subsamples show any outflow signature. We report a tentative detection in a
subset statistically designed to maximize outflow signatures. We derive upper
limits on molecular gas outflow rate and mass loading factors $\eta$ based on
our results and find $\eta \leq$ 2.2-35.4, depending on the subsample. Much
deeper CO data and observations of alternative tracers are needed to decisively
constrain the importance of cold molecular gas component of outflows relative
to other gas phases.; 78) Assessing zero-shot generalisation behaviour in graph-neural-network
  interatomic potentials; With the rapidly growing availability of machine-learned interatomic
potential (MLIP) models for chemistry, much current research focuses on the
development of generally applicable and ``foundational'' MLIPs. An important
question in this context is whether, and how well, such models can transfer
from one application domain to another. Here, we assess this transferability
for an MLIP model at the interface of materials and molecular chemistry.
Specifically, we study GO-MACE-23, a model designed for the extended covalent
network of graphene oxide, and quantify its zero-shot performance for small,
isolated molecules and chemical reactions outside its direct scope--in direct
comparison with a state-of-the-art model which has been trained in-domain. Our
work provides quantitative insight into the transfer and generalisation ability
of graph-neural-network potentials and, more generally, makes a step towards
the more widespread applicability of MLIPs in chemistry.; 79) A Transformer-Based Framework for Greek Sign Language Production using
  Extended Skeletal Motion Representations; Sign Languages are the primary form of communication for Deaf communities
across the world. To break the communication barriers between the Deaf and
Hard-of-Hearing and the hearing communities, it is imperative to build systems
capable of translating the spoken language into sign language and vice versa.
Building on insights from previous research, we propose a deep learning model
for Sign Language Production (SLP), which to our knowledge is the first attempt
on Greek SLP. We tackle this task by utilizing a transformer-based architecture
that enables the translation from text input to human pose keypoints, and the
opposite. We evaluate the effectiveness of the proposed pipeline on the Greek
SL dataset Elementary23, through a series of comparative analyses and ablation
studies. Our pipeline's components, which include data-driven gloss generation,
training through video to text translation and a scheduling algorithm for
teacher forcing - auto-regressive decoding seem to actively enhance the quality
of produced SL videos.; 80) Understanding the Uncertainty of LLM Explanations: A Perspective Based
  on Reasoning Topology; Understanding the uncertainty in large language model (LLM) explanations is
important for evaluating their faithfulness and reasoning consistency, and thus
provides insights into the reliability of LLM's output regarding a question. In
this work, we propose a novel framework that quantifies uncertainty in LLM
explanations through a reasoning topology perspective. By designing a
structural elicitation strategy, we guide the LLMs to frame the explanations of
an answer into a graph topology. This process decomposes the explanations into
the knowledge related sub-questions and topology-based reasoning structures,
which allows us to quantify uncertainty not only at the semantic level but also
from the reasoning path. It further brings convenience to assess knowledge
redundancy and provide interpretable insights into the reasoning process. Our
method offers a systematic way to interpret the LLM reasoning, analyze
limitations, and provide guidance for enhancing robustness and faithfulness.
This work pioneers the use of graph-structured uncertainty measurement in LLM
explanations and demonstrates the potential of topology-based quantification.; 81) Uniqueness of asymptotically conical shrinking gradient K\""ahler-Ricci
  solitons; We show that, up to biholomorphism, a given noncompact complex manifold only
admits one shrinking gradient K\""ahler-Ricci soliton with Ricci curvature
tending to zero at infinity. Our result does not require fixing the asymptotic
data of the metric, nor fixing the soliton vector field. The method used to
prove the uniqueness of the soliton vector field can be applied more widely,
for example to show that conical Calabi-Yau metrics on a given complex manifold
are unique up to biholomorphism. We also use it to prove that if two polarized
Fano fibrations, as introduced by Sun-Zhang, are biholomorphic and their
vertices agree, then they are isomorphic as algebraic varieties.; 82) Joint Communication and Radar Sensing for Terahertz Space-Air-Ground
  Integrated Networks (SAGIN); The transition from isolated systems to integrated solutions has driven the
development of space-air-ground integrated networks (SAGIN) as well as the
integration of communication and radar sensing functionalities. By leveraging
the unique properties of the Terahertz (THz) band, THz joint communication and
radar sensing (JCRS) supports high-speed communication and precise sensing,
addressing the growing demands of SAGIN for connectivity and environmental
awareness. However, most existing THz studies focus on terrestrial and static
scenarios, with limited consideration for the dynamic and non-terrestrial
environments of SAGIN. In this paper, the THz JCRS techniques for SAGIN are
comprehensively investigated. Specifically, propagation characteristics and
channel models of THz waves in non-terrestrial environments are analyzed. A
link capacity comparison with millimeter-wave, THz, and free-space optical
frequency bands is conducted to highlight the advantages of THz frequencies.
Moreover, novel JCRS waveform design strategies are presented to achieve mutual
merit of communication and radar sensing, while networking strategies are
developed to overcome challenges in SAGIN such as high mobility. Furthermore,
advancements in THz device technologies, including antennas and amplifiers, are
reviewed to assess their roles in enabling practical JCRS implementations.; 83) CoT-Drive: Efficient Motion Forecasting for Autonomous Driving with LLMs
  and Chain-of-Thought Prompting; Accurate motion forecasting is crucial for safe autonomous driving (AD). This
study proposes CoT-Drive, a novel approach that enhances motion forecasting by
leveraging large language models (LLMs) and a chain-of-thought (CoT) prompting
method. We introduce a teacher-student knowledge distillation strategy to
effectively transfer LLMs' advanced scene understanding capabilities to
lightweight language models (LMs), ensuring that CoT-Drive operates in
real-time on edge devices while maintaining comprehensive scene understanding
and generalization capabilities. By leveraging CoT prompting techniques for
LLMs without additional training, CoT-Drive generates semantic annotations that
significantly improve the understanding of complex traffic environments,
thereby boosting the accuracy and robustness of predictions. Additionally, we
present two new scene description datasets, Highway-Text and Urban-Text,
designed for fine-tuning lightweight LMs to generate context-specific semantic
annotations. Comprehensive evaluations of five real-world datasets demonstrate
that CoT-Drive outperforms existing models, highlighting its effectiveness and
efficiency in handling complex traffic scenarios. Overall, this study is the
first to consider the practical application of LLMs in this field. It pioneers
the training and use of a lightweight LLM surrogate for motion forecasting,
setting a new benchmark and showcasing the potential of integrating LLMs into
AD systems.; 84) Probing Dark Matter-Electron Interactions in the Cosmic Microwave
  Background Radiation; In this article, we consider Dark Matter (DM) interactions, and study the
same in the light of the Cosmic Microwave Background Radiation (CMBR) data. In
particular, we focus on the DM-electron interactions. Assuming that such
interactions are mediated by rather heavy mediators, we consider effective
operators describing the relevant interaction terms in the lagrangian. The
presence of such interaction terms lead to both DM annihilation and DM-electron
scattering (drag). We focus on operators which lead to velocity-independent DM
annihilation and DM-electron scattering cross-sections. Using the CMBR data, we
study the the implications of both of these effects imposing constraints on the
respective effective operators. This analysis underscores the importance of
taking both scattering and annihilation processes into consideration in the
study of DM interactions. We observe that the constraints on the DM
annihilation and scattering cross-sections can change, up to about 14% and 13%
respectively, for the benchmark scenarios we considered, depending on the mass
of DM, as compared to the scenario where only DM annihilation is accounted for.; 85) Towards Trustworthy Retrieval Augmented Generation for Large Language
  Models: A Survey; Retrieval-Augmented Generation (RAG) is an advanced technique designed to
address the challenges of Artificial Intelligence-Generated Content (AIGC). By
integrating context retrieval into content generation, RAG provides reliable
and up-to-date external knowledge, reduces hallucinations, and ensures relevant
context across a wide range of tasks. However, despite RAG's success and
potential, recent studies have shown that the RAG paradigm also introduces new
risks, including robustness issues, privacy concerns, adversarial attacks, and
accountability issues. Addressing these risks is critical for future
applications of RAG systems, as they directly impact their trustworthiness.
Although various methods have been developed to improve the trustworthiness of
RAG methods, there is a lack of a unified perspective and framework for
research in this topic. Thus, in this paper, we aim to address this gap by
providing a comprehensive roadmap for developing trustworthy RAG systems. We
place our discussion around five key perspectives: reliability, privacy,
safety, fairness, explainability, and accountability. For each perspective, we
present a general framework and taxonomy, offering a structured approach to
understanding the current challenges, evaluating existing solutions, and
identifying promising future research directions. To encourage broader adoption
and innovation, we also highlight the downstream applications where trustworthy
RAG systems have a significant impact.; 86) BS-Mamba for Black-Soil Area Detection On the Qinghai-Tibetan Plateau; Extremely degraded grassland on the Qinghai-Tibetan Plateau (QTP) presents a
significant environmental challenge due to overgrazing, climate change, and
rodent activity, which degrade vegetation cover and soil quality. These
extremely degraded grassland on QTP, commonly referred to as black-soil area,
require accurate assessment to guide effective restoration efforts. In this
paper, we present a newly created QTP black-soil dataset, annotated under
expert guidance. We introduce a novel neural network model, BS-Mamba,
specifically designed for the black-soil area detection using UAV remote
sensing imagery. The BS-Mamba model demonstrates higher accuracy in identifying
black-soil area across two independent test datasets than the state-of-the-art
models. This research contributes to grassland restoration by providing an
efficient method for assessing the extent of black-soil area on the QTP.; 87) PSF-4D: A Progressive Sampling Framework for View Consistent 4D Editing; Instruction-guided generative models, especially those using text-to-image
(T2I) and text-to-video (T2V) diffusion frameworks, have advanced the field of
content editing in recent years. To extend these capabilities to 4D scene, we
introduce a progressive sampling framework for 4D editing (PSF-4D) that ensures
temporal and multi-view consistency by intuitively controlling the noise
initialization during forward diffusion. For temporal coherence, we design a
correlated Gaussian noise structure that links frames over time, allowing each
frame to depend meaningfully on prior frames. Additionally, to ensure spatial
consistency across views, we implement a cross-view noise model, which uses
shared and independent noise components to balance commonalities and distinct
details among different views. To further enhance spatial coherence, PSF-4D
incorporates view-consistent iterative refinement, embedding view-aware
information into the denoising process to ensure aligned edits across frames
and views. Our approach enables high-quality 4D editing without relying on
external models, addressing key challenges in previous methods. Through
extensive evaluation on multiple benchmarks and multiple editing aspects (e.g.,
style transfer, multi-attribute editing, object removal, local editing, etc.),
we show the effectiveness of our proposed method. Experimental results
demonstrate that our proposed method outperforms state-of-the-art 4D editing
methods in diverse benchmarks.; 88) J-braid groups are torus necklace groups; We construct a family of links we call torus necklaces for which the link
groups are precisely the braid groups of generalised $J$-reflection groups.
Moreover, this correspondence exhibits the meridians of the aforementioned link
groups as braid reflections. In particular, this construction generalises to
all irreducible rank two complex reflection groups a well-known correspondence
between some rank two complex braid groups and some torus knot groups. In
addition, as abstract groups, we show that the family of link groups associated
to Seifert links coincides with the family of circular groups. This shows that
every time a link group has a non-trivial center, it is a Garside group.; 89) Characterization of Highly Robust Solutions in Multi-Objective
  Programming in Banach Spaces; This paper delves into the challenging issues in uncertain multi-objective
optimization, where uncertainty permeates nonsmooth nonconvex objective and
constraint functions. In this context, we investigate highly robust (weakly
efficient) solutions, a solution concept defined by efficiency across all
scenarios. Our exploration reveals important relationships between highly
robust solutions and other robustness notions, including set-based and
worst-case notions, as well as connections with proper and isolated efficiency.
Leveraging modern techniques from variational analysis, we establish necessary
and sufficient optimality conditions for these solutions. Moreover, we explore
the robustness of multi-objective optimization problems in the face of various
uncertain sets, such as ball, ellipsoidal, and polyhedral sets.; 90) CoInD: Enabling Logical Compositions in Diffusion Models; How can we learn generative models to sample data with arbitrary logical
compositions of statistically independent attributes? The prevailing solution
is to sample from distributions expressed as a composition of attributes'
conditional marginal distributions under the assumption that they are
statistically independent. This paper shows that standard conditional diffusion
models violate this assumption, even when all attribute compositions are
observed during training. And, this violation is significantly more severe when
only a subset of the compositions is observed. We propose CoInD to address this
problem. It explicitly enforces statistical independence between the
conditional marginal distributions by minimizing Fisher's divergence between
the joint and marginal distributions. The theoretical advantages of CoInD are
reflected in both qualitative and quantitative experiments, demonstrating a
significantly more faithful and controlled generation of samples for arbitrary
logical compositions of attributes. The benefit is more pronounced for
scenarios that current solutions relying on the assumption of conditionally
independent marginals struggle with, namely, logical compositions involving the
NOT operation and when only a subset of compositions are observed during
training.; 91) Strategic Learning with Local Explanations as Feedback; We investigate algorithmic decision problems where agents can respond
strategically to the decision maker's (DM) models. The demand for clear and
actionable explanations from DMs to (potentially strategic) agents continues to
rise. While prior work often treats explanations as full model disclosures,
explanations in practice might convey only partial information, which can lead
to misinterpretations and harmful responses. When full disclosure of the
predictive model is neither feasible nor desirable, a key open question is how
DMs can use explanations to maximise their utility without compromising agent
welfare. In this work, we explore well-known local and global explanation
methods, and establish a necessary condition to prevent explanations from
misleading agents into self-harming actions. Moreover, with conditional
homogeneity, we establish that action recommendation (AR)-based explanations
are sufficient for non-harmful responses, akin to the revelation principle in
information design. To operationalise AR-based explanations, we propose a
simple algorithm to jointly optimise the predictive model and AR policy to
balance DM outcomes with agent welfare. Our empirical results demonstrate the
benefits of this approach as a more refined strategy for safe and effective
partial model disclosure in algorithmic decision-making.; 92) In Shift and In Variance: Assessing the Robustness of HAR Deep Learning
  Models against Variability; Human Activity Recognition (HAR) using wearable inertial measurement unit
(IMU) sensors can revolutionize healthcare by enabling continual health
monitoring, disease prediction, and routine recognition. Despite the high
accuracy of Deep Learning (DL) HAR models, their robustness to real-world
variabilities remains untested, as they have primarily been trained and tested
on limited lab-confined data. In this study, we isolate subject, device,
position, and orientation variability to determine their effect on DL HAR
models and assess the robustness of these models in real-world conditions. We
evaluated the DL HAR models using the HARVAR and REALDISP datasets, providing a
comprehensive discussion on the impact of variability on data distribution
shifts and changes in model performance. Our experiments measured shifts in
data distribution using Maximum Mean Discrepancy (MMD) and observed DL model
performance drops due to variability. We concur that studied variabilities
affect DL HAR models differently, and there is an inverse relationship between
data distribution shifts and model performance. The compounding effect of
variability was analyzed, and the implications of variabilities in real-world
scenarios were highlighted. MMD proved an effective metric for calculating data
distribution shifts and explained the drop in performance due to variabilities
in HARVAR and REALDISP datasets. Combining our understanding of variability
with evaluating its effects will facilitate the development of more robust DL
HAR models and optimal training techniques. Allowing Future models to not only
be assessed based on their maximum F1 score but also on their ability to
generalize effectively; 93) Inductive Moment Matching; Diffusion models and Flow Matching generate high-quality samples but are slow
at inference, and distilling them into few-step models often leads to
instability and extensive tuning. To resolve these trade-offs, we propose
Inductive Moment Matching (IMM), a new class of generative models for one- or
few-step sampling with a single-stage training procedure. Unlike distillation,
IMM does not require pre-training initialization and optimization of two
networks; and unlike Consistency Models, IMM guarantees distribution-level
convergence and remains stable under various hyperparameters and standard model
architectures. IMM surpasses diffusion models on ImageNet-256x256 with 1.99 FID
using only 8 inference steps and achieves state-of-the-art 2-step FID of 1.98
on CIFAR-10 for a model trained from scratch.; 94) An ordinal analysis of CM and its extensions; In arXiv:0905.1675, Nik Weaver proposed a novel intuitionistic formal theory
of third-order arithmetic as a formalisation of his philosophical position
known as mathematical conceptualism. In this paper, we will construct a
realisability model from the partial combinatory algebra of
$\Sigma^1_1$-definable partial functions and use it to provide an ordinal
analysis of this formal theory. Additionally, we will examine possible
extensions to this system by adding well-ordering axioms, which are briefly
mentioned but never thoroughly studied in Weaver's work. We aim to use the
realisability arguments to discuss how much such extensions constitute an
increase from the original theory's proof-theoretic strength.; 95) Provable Ordering and Continuity in Vision-Language Pretraining for
  Generalizable Embodied Agents; Pre-training vision-language representations on human action videos has
emerged as a promising approach to reduce reliance on large-scale expert
demonstrations for training embodied agents. However, prior methods often
employ time contrastive learning based on goal-reaching heuristics,
progressively aligning language instructions from the initial to the final
frame. This overemphasis on future frames can result in erroneous
vision-language associations, as actions may terminate early or include
irrelevant moments in the end. To address this issue, we propose Action
Temporal Coherence Learning (AcTOL) to learn ordered and continuous
vision-language representations without rigid goal-based constraint. AcTOL
treats a video as a continuous trajectory where it (1) contrasts semantic
differences between frames to reflect their natural ordering, and (2) imposes a
local Brownian bridge constraint to ensure smooth transitions across
intermediate frames. Extensive imitation learning experiments across varying
numbers of demonstrations show that the pretrained features significantly
enhance downstream manipulation tasks by up to 49% with high robustness to
different linguistic styles of instructions, offering a viable pathway toward
generalized embodied agents. The source code is included in the supplementary
material for reference.; 96) Variations of Augmented Lagrangian for Robotic Multi-Contact Simulation; The multi-contact nonlinear complementarity problem (NCP) is a naturally
arising challenge in robotic simulations. Achieving high performance in terms
of both accuracy and efficiency remains a significant challenge, particularly
in scenarios involving intensive contacts and stiff interactions. In this
article, we introduce a new class of multi-contact NCP solvers based on the
theory of the Augmented Lagrangian (AL). We detail how the standard derivation
of AL in convex optimization can be adapted to handle multi-contact NCP through
the iteration of surrogate problem solutions and the subsequent update of
primal-dual variables. Specifically, we present two tailored variations of AL
for robotic simulations: the Cascaded Newton-based Augmented Lagrangian (CANAL)
and the Subsystem-based Alternating Direction Method of Multipliers (SubADMM).
We demonstrate how CANAL can manage multi-contact NCP in an accurate and robust
manner, while SubADMM offers superior computational speed, scalability, and
parallelizability for high degrees-of-freedom multibody systems with numerous
contacts. Our results showcase the effectiveness of the proposed solver
framework, illustrating its advantages in various robotic manipulation
scenarios.; 97) Bell and Mermin inequalities in Quantum Field Theory from vacuum
  projectors and Weyl operators; The use of the vacuum projector $|0 \rangle \langle 0| $ and of the unitary
Weyl operators enables us to construct a set of Hermitian dichotomic operators
in relativistic scalar Quantum Field Theory in Minkowski spacetime. Employing
test functions supported in diamond regions, both Bell and Mermin inequalities
are studied by means of a numerical setup. In addition to reporting expressive
violations of both inequalities, the cluster property is also checked.; 98) The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating
  Reward Hacking; This work identifies the Energy Loss Phenomenon in Reinforcement Learning
from Human Feedback (RLHF) and its connection to reward hacking. Specifically,
energy loss in the final layer of a Large Language Model (LLM) gradually
increases during the RL process, with an excessive increase in energy loss
characterizing reward hacking. Beyond empirical analysis, we further provide a
theoretical foundation by proving that, under mild conditions, the increased
energy loss reduces the upper bound of contextual relevance in LLMs, which is a
critical aspect of reward hacking as the reduced contextual relevance typically
indicates overfitting to reward model-favored patterns in RL. To address this
issue, we propose an Energy loss-aware PPO algorithm (EPPO) which penalizes the
increase in energy loss in the LLM's final layer during reward calculation to
prevent excessive energy loss, thereby mitigating reward hacking. We
theoretically show that EPPO can be conceptually interpreted as an
entropy-regularized RL algorithm, which provides deeper insights into its
effectiveness. Extensive experiments across various LLMs and tasks demonstrate
the commonality of the energy loss phenomenon, as well as the effectiveness of
EPPO in mitigating reward hacking and improving RLHF performance.; 99) Linear-time classical approximate optimization of cubic-lattice
  classical spin glasses; Computing low-energy configurations (i.e., approximate optimization) of
classical spin glasses is of relevance to both condensed matter and
combinatorial optimization. Recent theoretical work opens the possibility to
make its time complexity with quantum annealing generically polynomial, and
D-Wave experiments can now achieve approximate optimization of cubic-lattice
classical Ising spin glasses with $\sim$$10^4$ spins. It is therefore timely to
ask which short-range classical spin glasses are good candidates for
demonstrating quantum advantage in the time complexity of heuristic approximate
optimization. One intuition is to consider models with very rugged energy
landscapes in configuration space. However, here we provide evidence that
short-range classical spin glasses may be approximately optimized in linear
time and space with a very simple deterministic tensor-network heuristic
regardless of ruggedness. On the cubic lattice with up to
50$\times$50$\times$50 spins, we obtain energy errors of $\lesssim$3% for the
$\pm J$ model used in recent D-Wave experiments, and $\lesssim$5% for much more
rugged planted-solution instances. For cubic-lattice-Ising reductions of
unweighted Max-Cut on random 3-regular graphs with up to 300 vertices, we find
energy errors of $<$1% and approximation ratios of about 72-88%. These results
inform the search for quantum advantage and suggest an efficient classical
method for generating warm starts for other spin-glass optimization algorithms.
Our algorithm is amenable to massive parallelization and may also allow for
low-power, accelerated implementations with photonic matrix-multiplication
hardware.; 100) A structure-preserving parametric finite element method with optimal
  energy stability condition for anisotropic surface diffusion; We propose and analyze a structure-preserving parametric finite element
method (SP-PFEM) for the evolution of closed curves under anisotropic surface
diffusion with surface energy density $\hat{\gamma}(\theta)$. Our primary
theoretical contribution establishes that the condition
$3\hat{\gamma}(\theta)-\hat{\gamma}(\theta-\pi)\geq 0$ is both necessary and
sufficient for unconditional energy stability within the framework of local
energy estimates. The proposed method introduces a symmetric surface energy
matrix $\hat{\boldsymbol{Z}}_k(\theta)$ with a stabilizing function
$k(\theta)$, leading to a conservative weak formulation. Its fully
discretization via SP-PFEM rigorously preserves the two geometric structures:
enclosed area conservation and energy dissipation unconditionally under our
energy stability condition. Numerical results are reported to demonstrate the
efficiency and accuracy of the proposed method, along with its area
conservation and energy dissipation properties.",0.0,0.6240505200038379
2411.0064,applied,2411.0064-pos1-3,"The Llama 3 Herd of Models; Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",2411.0064-pos2-3,"Quantifying Variance in Evaluation Benchmarks; Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models.",60,"['2', '1', '3', '4', '5', '6', '7', '8', '9', '10']","The paper 'Large Physics Models: Towards a collaborative approach with Large Language Models and Foundation Models' best complements the Llama 3 paper by focusing on the integration of language models in specific domains such as physics, thus enhancing their applicability to a multidisciplinary framework. It provides a roadmap for how language models can be utilized in physics, fostering collaboration across disciplines and addressing the need for specialized models that can leverage the capabilities of Llama 3. This collaboration is novel and promising in addressing both theoretical and practical challenges in scientific research, making it highly useful for the targeted application of language models.","1) A structure-preserving parametric finite element method with optimal
  energy stability condition for anisotropic surface diffusion; We propose and analyze a structure-preserving parametric finite element
method (SP-PFEM) for the evolution of closed curves under anisotropic surface
diffusion with surface energy density $\hat{\gamma}(\theta)$. Our primary
theoretical contribution establishes that the condition
$3\hat{\gamma}(\theta)-\hat{\gamma}(\theta-\pi)\geq 0$ is both necessary and
sufficient for unconditional energy stability within the framework of local
energy estimates. The proposed method introduces a symmetric surface energy
matrix $\hat{\boldsymbol{Z}}_k(\theta)$ with a stabilizing function
$k(\theta)$, leading to a conservative weak formulation. Its fully
discretization via SP-PFEM rigorously preserves the two geometric structures:
enclosed area conservation and energy dissipation unconditionally under our
energy stability condition. Numerical results are reported to demonstrate the
efficiency and accuracy of the proposed method, along with its area
conservation and energy dissipation properties.; 2) Large Physics Models: Towards a collaborative approach with Large
  Language Models and Foundation Models; This paper explores ideas and provides a potential roadmap for the
development and evaluation of physics-specific large-scale AI models, which we
call Large Physics Models (LPMs). These models, based on foundation models such
as Large Language Models (LLMs) - trained on broad data - are tailored to
address the demands of physics research. LPMs can function independently or as
part of an integrated framework. This framework can incorporate specialized
tools, including symbolic reasoning modules for mathematical manipulations,
frameworks to analyse specific experimental and simulated data, and mechanisms
for synthesizing theories and scientific literature. We begin by examining
whether the physics community should actively develop and refine dedicated
models, rather than relying solely on commercial LLMs. We then outline how LPMs
can be realized through interdisciplinary collaboration among experts in
physics, computer science, and philosophy of science. To integrate these models
effectively, we identify three key pillars: Development, Evaluation, and
Philosophical Reflection. Development focuses on constructing models capable of
processing physics texts, mathematical formulations, and diverse physical data.
Evaluation assesses accuracy and reliability by testing and benchmarking.
Finally, Philosophical Reflection encompasses the analysis of broader
implications of LLMs in physics, including their potential to generate new
scientific understanding and what novel collaboration dynamics might arise in
research. Inspired by the organizational structure of experimental
collaborations in particle physics, we propose a similarly interdisciplinary
and collaborative approach to building and refining Large Physics Models. This
roadmap provides specific objectives, defines pathways to achieve them, and
identifies challenges that must be addressed to realise physics-specific large
scale AI models.; 3) Stitch-a-Recipe: Video Demonstration from Multistep Descriptions; When obtaining visual illustrations from text descriptions, today's methods
take a description with-a single text context caption, or an action
description-and retrieve or generate the matching visual context. However,
prior work does not permit visual illustration of multistep descriptions, e.g.
a cooking recipe composed of multiple steps. Furthermore, simply handling each
step description in isolation would result in an incoherent demonstration. We
propose Stitch-a-Recipe, a novel retrieval-based method to assemble a video
demonstration from a multistep description. The resulting video contains clips,
possibly from different sources, that accurately reflect all the step
descriptions, while being visually coherent. We formulate a training pipeline
that creates large-scale weakly supervised data containing diverse and novel
recipes and injects hard negatives that promote both correctness and coherence.
Validated on in-the-wild instructional videos, Stitch-a-Recipe achieves
state-of-the-art performance, with quantitative gains up to 24% as well as
dramatic wins in a human preference study.; 4) PodAgent: A Comprehensive Framework for Podcast Generation; Existing Existing automatic audio generation methods struggle to generate
podcast-like audio programs effectively. The key challenges lie in in-depth
content generation, appropriate and expressive voice production. This paper
proposed PodAgent, a comprehensive framework for creating audio programs.
PodAgent 1) generates informative topic-discussion content by designing a
Host-Guest-Writer multi-agent collaboration system, 2) builds a voice pool for
suitable voice-role matching and 3) utilizes LLM-enhanced speech synthesis
method to generate expressive conversational speech. Given the absence of
standardized evaluation criteria for podcast-like audio generation, we
developed comprehensive assessment guidelines to effectively evaluate the
model's performance. Experimental results demonstrate PodAgent's effectiveness,
significantly surpassing direct GPT-4 generation in topic-discussion dialogue
content, achieving an 87.4% voice-matching accuracy, and producing more
expressive speech through LLM-guided synthesis. Demo page:
https://podcast-agent.github.io/demo/. Source code:
https://github.com/yujxx/PodAgent.; 5) Gotzmann's persistence theorem for Mori dream spaces; Gotzmann's persistence theorem provides a method for determining the Hilbert
polynomial of a subscheme of projective space by evaluating the Hilbert
function at only two points, irrespective of the dimension of the ambient
space. In arXiv:2405.02275 we established an analogue of Gotzmann's persistence
theorem for smooth projective toric varieties. We generalise our results to the
setting of Mori dream spaces, whose associated Cox rings are also finitely
generated. We also give an alternative, stronger, persistence result for points
in products of projective spaces.; 6) Probing Dark Matter-Electron Interactions in the Cosmic Microwave
  Background Radiation; In this article, we consider Dark Matter (DM) interactions, and study the
same in the light of the Cosmic Microwave Background Radiation (CMBR) data. In
particular, we focus on the DM-electron interactions. Assuming that such
interactions are mediated by rather heavy mediators, we consider effective
operators describing the relevant interaction terms in the lagrangian. The
presence of such interaction terms lead to both DM annihilation and DM-electron
scattering (drag). We focus on operators which lead to velocity-independent DM
annihilation and DM-electron scattering cross-sections. Using the CMBR data, we
study the the implications of both of these effects imposing constraints on the
respective effective operators. This analysis underscores the importance of
taking both scattering and annihilation processes into consideration in the
study of DM interactions. We observe that the constraints on the DM
annihilation and scattering cross-sections can change, up to about 14% and 13%
respectively, for the benchmark scenarios we considered, depending on the mass
of DM, as compared to the scenario where only DM annihilation is accounted for.; 7) Assessing zero-shot generalisation behaviour in graph-neural-network
  interatomic potentials; With the rapidly growing availability of machine-learned interatomic
potential (MLIP) models for chemistry, much current research focuses on the
development of generally applicable and ``foundational'' MLIPs. An important
question in this context is whether, and how well, such models can transfer
from one application domain to another. Here, we assess this transferability
for an MLIP model at the interface of materials and molecular chemistry.
Specifically, we study GO-MACE-23, a model designed for the extended covalent
network of graphene oxide, and quantify its zero-shot performance for small,
isolated molecules and chemical reactions outside its direct scope--in direct
comparison with a state-of-the-art model which has been trained in-domain. Our
work provides quantitative insight into the transfer and generalisation ability
of graph-neural-network potentials and, more generally, makes a step towards
the more widespread applicability of MLIPs in chemistry.; 8) The R-Vessel-X Project; 1) Objectives: This technical report presents a synthetic summary and the
principal outcomes of the project R-Vessel-X (""Robust vascular network
extraction and understanding within hepatic biomedical images"") funded by the
French Agence Nationale de la Recherche, and developed between 2019 and 2023.
2) Material and methods: We used datasets and tools publicly available such as
IRCAD, Bullitt or VascuSynth toobtain real or synthetic angiographic images.
The main contributions lie in the field of 3D angiographic image analysis:
filtering, segmentation, modeling and simulation, with a specific focus on the
liver. 3) Results: We paid a particular attention to open-source software
diffusion of the developed methods, by means of 3D Slicer plugins for the liver
anatomy segmentation (SlicerRVXLiverSegmentation) and vesselness filtering
(Slicer-RVXVesselnessFilters), and an online demo for the generation of
synthetic and realistic vessels in 2D and 3D (OpenCCO). 4) Conclusion: The
R-Vessel-X project provided extensive research outcomes, covering various
topics related to 3D angiographic image analysis, such as filtering,
segmentation, modeling and simulation. We also developed open-source and free
softwares so that the research communities in biomedical engineering can use
these results in their future research.; 9) Nonlinear contractile response of actomyosin active gels to control
  signals; Biological systems tightly regulate their physiological state using control
signals. This includes the actomyosin cytoskeleton, a contractile active gel
that consumes chemical free energy to drive many examples of cellular
mechanical behavior. Upstream regulatory pathways activate or inhibit
actomyosin activity. However, the contractile response of the actomyosin
cytoskeleton to control signals remains poorly characterized. Here we employ
reconstituted actomyosin active gels and subject them to step and pulsatile
activation inputs. We find evidence for a nonlinear impulse response, which we
quantify via a transfer function $\delta \varepsilon / \delta g$ that relates
input free-energy pulses $\delta g$ to output strain pulses $\delta
\varepsilon$. We find a scaling relation $\delta \varepsilon / \delta g \sim
g^{-0.3}$. The negative sign of the exponent represents a decreased
effectiveness of a contracting gel in converting energy to strain. We ascribe
nonlinearity in our system to a density-dependent mechanism, which contrasts
strain-stiffening nonlinear responses to external stresses. Contractile
response to control signals is an essential step toward understanding how
information from mechanical signaling processes flow through actomyosin
networks in living, and likely also synthetic, cells.; 10) Lattice calculation of the $D_s\mapsto X \ell \bar{\nu}_\ell$ inclusive
  decay rate: an overview; In this talks, on behalf of our collaboration we present an overview of our
first-principle lattice QCD calculation of the $D_s\mapsto X \ell
\bar{\nu}_\ell$ inclusive decay rate. Here we introduce the theoretical
background and focus on the methodological aspects of the calculation. A
detailed discussion of our results, including the comparison with the
corresponding experimental measurements will soon be presented in a forthcoming
publication.; 11) Enhancing Magnetic Coupling in MN4-Graphene via Strain Engineering; MN4-embedded graphene (MN4-G) layers, incorporating transition metal elements
(M), represent a class of experimentally accessible two-dimensional materials
with significant potential for stable nanoscale magnetization. In these
systems, magnetic exchange interactions are primarily governed by
Ruderman-Kittel-Kasuya-Yosida (RKKY) coupling, exhibiting an anomalously
prolonged decay of r to the power of (-n), where r is the M-M separation
distance and n is between 0.5 and 2. This study investigates the impact of
strain on the electronic and magnetic properties of MN4-G layers using
ab-initio density functional theory (DFT). A novel strain-engineering approach
is developed by applying controlled tension or compression to the layers. Our
findings reveal that strain significantly modulates the strength, amplitude,
and decay rate of the RKKY coupling. Notably, the CoN4-G layer demonstrates a
pronounced enhancement in RKKY coupling strength, oscillation amplitude, and
reduced decay rate under strain. Conversely, the CuN4-G layer exhibits distinct
behavior, maintaining decoupled spin chains and invariant electronic and
magnetic properties despite applied strain. This work underscores the
tunability of magnetic interactions in MN4-G layers via strain engineering,
providing insights into the design of strain-controlled magnetic materials for
next-generation spintronic applications.; 12) QCD sum rules study on weak decays $\Omega_c^0\to\Omega^- (\pi^+,
  \rho^+, l^+\nu_l)$; We study the weak decays of the charmed baryon $\Omega_c^0$ to the $\Omega^-$
baryon within the framework of QCD sum rules. A three-point correlation
function is defined for calculating eight form factors governing the
$\Omega_c^0 \to \Omega^-$ transition. The cutting rules are employed to extract
the double imaginary parts of the correlation functions, enabling their
expression in the form of dispersive integrals. These form factors are then
used to determine the branching fractions for the hadronic decays $\Omega_c^0
\to \Omega^- \pi^+$ and $\Omega_c^0 \to \Omega^- \rho^+$, as well as the
semi-leptonic decay $\Omega_c^0 \to \Omega^- \ell^+ \nu_\ell$. Our results are
compared with existing theoretical predictions and experimental data, providing
a comprehensive analysis of $\Omega_c^0$ decays and offering valuable insights
into the dynamics of charmed baryon decays.; 13) CoInD: Enabling Logical Compositions in Diffusion Models; How can we learn generative models to sample data with arbitrary logical
compositions of statistically independent attributes? The prevailing solution
is to sample from distributions expressed as a composition of attributes'
conditional marginal distributions under the assumption that they are
statistically independent. This paper shows that standard conditional diffusion
models violate this assumption, even when all attribute compositions are
observed during training. And, this violation is significantly more severe when
only a subset of the compositions is observed. We propose CoInD to address this
problem. It explicitly enforces statistical independence between the
conditional marginal distributions by minimizing Fisher's divergence between
the joint and marginal distributions. The theoretical advantages of CoInD are
reflected in both qualitative and quantitative experiments, demonstrating a
significantly more faithful and controlled generation of samples for arbitrary
logical compositions of attributes. The benefit is more pronounced for
scenarios that current solutions relying on the assumption of conditionally
independent marginals struggle with, namely, logical compositions involving the
NOT operation and when only a subset of compositions are observed during
training.; 14) Mixture of Experts for Recognizing Depression from Interview and Reading
  Tasks; Depression is a mental disorder and can cause a variety of symptoms,
including psychological, physical, and social. Speech has been proved an
objective marker for the early recognition of depression. For this reason, many
studies have been developed aiming to recognize depression through speech.
However, existing methods rely on the usage of only the spontaneous speech
neglecting information obtained via read speech, use transcripts which are
often difficult to obtain (manual) or come with high word-error rates
(automatic), and do not focus on input-conditional computation methods. To
resolve these limitations, this is the first study in depression recognition
task obtaining representations of both spontaneous and read speech, utilizing
multimodal fusion methods, and employing Mixture of Experts (MoE) models in a
single deep neural network. Specifically, we use audio files corresponding to
both interview and reading tasks and convert each audio file into log-Mel
spectrogram, delta, and delta-delta. Next, the image representations of the two
tasks pass through shared AlexNet models. The outputs of the AlexNet models are
given as input to a multimodal fusion method. The resulting vector is passed
through a MoE module. In this study, we employ three variants of MoE, namely
sparsely-gated MoE and multilinear MoE based on factorization. Findings suggest
that our proposed approach yields an Accuracy and F1-score of 87.00% and 86.66%
respectively on the Androids corpus.; 15) Unraveling phase transformation with phononic hyperbolicity using
  off-resonant terahertz light; Noncontacting and nondestructive control of geometric phase in conventional
semiconductors plays a pivotal role in various applications. In the current
work, we present a theoretical and computational investigation on terahertz
(THz) light-induced phase transformation of conventional binary semiconducting
compounds among different structures including rock-salt, zinc-blende,
wurtzite, and hexagonal phases. Using MgS and MgSe as prototypical examples, we
perform anharmonic phonon mediated calculations and reveal large contrasting
lattice contributed dielectric susceptibility in the THz regime. We then
construct a THz-induced phase diagram under intermediate temperature and reveal
rock-salt to hexagonal and then wurtzite structure transformations with
increasing light intensity. This does not require a high temperature
environment as observed in traditional experiments. The low energy barrier
suggests that the phase transition kinetics can be fast, and the stable room
temperature phonon dispersions guarantee their non-volatile nature.
Furthermore, we disclose the phononic hyperbolicity with strong anisotropic THz
susceptibility components, which serves as a natural hyperbolic material with
negative refractive index. Our work suggests the potential to realize
metastable hidden phases using noninvasive THz irradiation, which expands the
conventional pressure-temperature ($P-T$) phase diagram by adding light as an
additional control factor.; 16) Inductive Moment Matching; Diffusion models and Flow Matching generate high-quality samples but are slow
at inference, and distilling them into few-step models often leads to
instability and extensive tuning. To resolve these trade-offs, we propose
Inductive Moment Matching (IMM), a new class of generative models for one- or
few-step sampling with a single-stage training procedure. Unlike distillation,
IMM does not require pre-training initialization and optimization of two
networks; and unlike Consistency Models, IMM guarantees distribution-level
convergence and remains stable under various hyperparameters and standard model
architectures. IMM surpasses diffusion models on ImageNet-256x256 with 1.99 FID
using only 8 inference steps and achieves state-of-the-art 2-step FID of 1.98
on CIFAR-10 for a model trained from scratch.; 17) Provable Ordering and Continuity in Vision-Language Pretraining for
  Generalizable Embodied Agents; Pre-training vision-language representations on human action videos has
emerged as a promising approach to reduce reliance on large-scale expert
demonstrations for training embodied agents. However, prior methods often
employ time contrastive learning based on goal-reaching heuristics,
progressively aligning language instructions from the initial to the final
frame. This overemphasis on future frames can result in erroneous
vision-language associations, as actions may terminate early or include
irrelevant moments in the end. To address this issue, we propose Action
Temporal Coherence Learning (AcTOL) to learn ordered and continuous
vision-language representations without rigid goal-based constraint. AcTOL
treats a video as a continuous trajectory where it (1) contrasts semantic
differences between frames to reflect their natural ordering, and (2) imposes a
local Brownian bridge constraint to ensure smooth transitions across
intermediate frames. Extensive imitation learning experiments across varying
numbers of demonstrations show that the pretrained features significantly
enhance downstream manipulation tasks by up to 49% with high robustness to
different linguistic styles of instructions, offering a viable pathway toward
generalized embodied agents. The source code is included in the supplementary
material for reference.; 18) ADF22-WEB: Detection of a molecular gas reservoir in a massive quiescent
  galaxy located in a $z\approx3$ proto-cluster core; We present a study of the molecular gas reservoirs and dust contents in three
quiescent galaxies (QGs) located in the core of the $z=3.09$ SSA22
proto-cluster. Using the Atacama Large Millimeter/submillimeter Array (ALMA),
we detect CO(3--2) emission in one galaxy, ADF22-QG1, marking the first direct
detection of molecular gas in a quiescent galaxy from the early universe. The
detected galaxy, ADF22-QG1, has a molecular gas mass of log$M_{\rm
H_2}$/M$_\odot = 10.26 \pm 0.07$ assuming a CO-to-H$2$ conversion factor
$\alpha_{\rm CO} = 4.4$ (log$M_{\rm H_2}$/M$_\odot = 9.52 \pm 0.07$ for
$\alpha_{\rm CO} = 0.8$), corresponding to a gas mass fraction of $f_{\rm gas}
\approx 14\%$ (2.5\%). The gas-to-dust ratio $\delta _{\rm gdr}\gtrsim170$
($\delta_{\rm gdr}\gtrsim30$) for $\alpha_{\rm CO} = 4.4$ ($\alpha_{\rm CO}
=0.8$) is also derived for the first time for a QG at the epoch. For the other
two galaxies, ADF22-QG2 and ADF22-QG3, non detections of CO(3--2) emission
provide upper limits, $f_{\rm gas} \approx 17\%$ (3.1\%) and $f_{\rm gas}
\approx 13\%$ (2.4\%), respectively. The inferred gas-consumption history of
ADF22-QG1, based on its star-formation history, suggests that (i) dusty
star-forming galaxies (DSFGs) at $z = 4$--$6$ are plausible progenitors, and
(ii) the cessation of gas accretion from cosmic web filaments plays an
important role in their evolution to quenched systems. Furthermore, the
presence of a detectable molecular gas reservoir in ADF22-QG1 indicates that
additional mechanisms, such as morphological quenching, may be required to
fully explain its quiescent nature.; 19) Characterization of Highly Robust Solutions in Multi-Objective
  Programming in Banach Spaces; This paper delves into the challenging issues in uncertain multi-objective
optimization, where uncertainty permeates nonsmooth nonconvex objective and
constraint functions. In this context, we investigate highly robust (weakly
efficient) solutions, a solution concept defined by efficiency across all
scenarios. Our exploration reveals important relationships between highly
robust solutions and other robustness notions, including set-based and
worst-case notions, as well as connections with proper and isolated efficiency.
Leveraging modern techniques from variational analysis, we establish necessary
and sufficient optimality conditions for these solutions. Moreover, we explore
the robustness of multi-objective optimization problems in the face of various
uncertain sets, such as ball, ellipsoidal, and polyhedral sets.; 20) Linear-time classical approximate optimization of cubic-lattice
  classical spin glasses; Computing low-energy configurations (i.e., approximate optimization) of
classical spin glasses is of relevance to both condensed matter and
combinatorial optimization. Recent theoretical work opens the possibility to
make its time complexity with quantum annealing generically polynomial, and
D-Wave experiments can now achieve approximate optimization of cubic-lattice
classical Ising spin glasses with $\sim$$10^4$ spins. It is therefore timely to
ask which short-range classical spin glasses are good candidates for
demonstrating quantum advantage in the time complexity of heuristic approximate
optimization. One intuition is to consider models with very rugged energy
landscapes in configuration space. However, here we provide evidence that
short-range classical spin glasses may be approximately optimized in linear
time and space with a very simple deterministic tensor-network heuristic
regardless of ruggedness. On the cubic lattice with up to
50$\times$50$\times$50 spins, we obtain energy errors of $\lesssim$3% for the
$\pm J$ model used in recent D-Wave experiments, and $\lesssim$5% for much more
rugged planted-solution instances. For cubic-lattice-Ising reductions of
unweighted Max-Cut on random 3-regular graphs with up to 300 vertices, we find
energy errors of $<$1% and approximation ratios of about 72-88%. These results
inform the search for quantum advantage and suggest an efficient classical
method for generating warm starts for other spin-glass optimization algorithms.
Our algorithm is amenable to massive parallelization and may also allow for
low-power, accelerated implementations with photonic matrix-multiplication
hardware.; 21) Variations of Augmented Lagrangian for Robotic Multi-Contact Simulation; The multi-contact nonlinear complementarity problem (NCP) is a naturally
arising challenge in robotic simulations. Achieving high performance in terms
of both accuracy and efficiency remains a significant challenge, particularly
in scenarios involving intensive contacts and stiff interactions. In this
article, we introduce a new class of multi-contact NCP solvers based on the
theory of the Augmented Lagrangian (AL). We detail how the standard derivation
of AL in convex optimization can be adapted to handle multi-contact NCP through
the iteration of surrogate problem solutions and the subsequent update of
primal-dual variables. Specifically, we present two tailored variations of AL
for robotic simulations: the Cascaded Newton-based Augmented Lagrangian (CANAL)
and the Subsystem-based Alternating Direction Method of Multipliers (SubADMM).
We demonstrate how CANAL can manage multi-contact NCP in an accurate and robust
manner, while SubADMM offers superior computational speed, scalability, and
parallelizability for high degrees-of-freedom multibody systems with numerous
contacts. Our results showcase the effectiveness of the proposed solver
framework, illustrating its advantages in various robotic manipulation
scenarios.; 22) Processing and Analyzing Real-World Driving Data: Insights on Trips,
  Scenarios, and Human Driving Behaviors; Analyzing large volumes of real-world driving data is essential for providing
meaningful and reliable insights into real-world trips, scenarios, and human
driving behaviors. To this end, we developed a multi-level data processing
approach that adds new information, segments data, and extracts desired
parameters. Leveraging a confidential but extensive dataset (over 1 million
km), this approach leads to three levels of in-depth analysis: trip, scenario,
and driving. The trip-level analysis explains representative properties
observed in real-world trips, while the scenario-level analysis focuses on
scenario conditions resulting from road events that reduce vehicle speed. The
driving-level analysis identifies the cause of driving regimes for specific
situations and characterizes typical human driving behaviors. Such analyses can
support the design of both trip- and scenario-based tests, the modeling of
human drivers, and the establishment of guidelines for connected and automated
vehicles.; 23) Standardised schema and taxonomy for AI incident databases in critical
  digital infrastructure; The rapid deployment of Artificial Intelligence (AI) in critical digital
infrastructure introduces significant risks, necessitating a robust framework
for systematically collecting AI incident data to prevent future incidents.
Existing databases lack the granularity as well as the standardized structure
required for consistent data collection and analysis, impeding effective
incident management. This work proposes a standardized schema and taxonomy for
AI incident databases, addressing these challenges by enabling detailed and
structured documentation of AI incidents across sectors. Key contributions
include developing a unified schema, introducing new fields such as incident
severity, causes, and harms caused, and proposing a taxonomy for classifying AI
incidents in critical digital infrastructure. The proposed solution facilitates
more effective incident data collection and analysis, thus supporting
evidence-based policymaking, enhancing industry safety measures, and promoting
transparency. This work lays the foundation for a coordinated global response
to AI incidents, ensuring trust, safety, and accountability in using AI across
regions.; 24) Fast T2T: Optimization Consistency Speeds Up Diffusion-Based
  Training-to-Testing Solving for Combinatorial Optimization; Diffusion models have recently advanced Combinatorial Optimization (CO) as a
powerful backbone for neural solvers. However, their iterative sampling process
requiring denoising across multiple noise levels incurs substantial overhead.
We propose to learn direct mappings from different noise levels to the optimal
solution for a given instance, facilitating high-quality generation with
minimal shots. This is achieved through an optimization consistency training
protocol, which, for a given instance, minimizes the difference among samples
originating from varying generative trajectories and time steps relative to the
optimal solution. The proposed model enables fast single-step solution
generation while retaining the option of multi-step sampling to trade for
sampling quality, which offers a more effective and efficient alternative
backbone for neural solvers. In addition, within the training-to-testing (T2T)
framework, to bridge the gap between training on historical instances and
solving new instances, we introduce a novel consistency-based gradient search
scheme during the test stage, enabling more effective exploration of the
solution space learned during training. It is achieved by updating the latent
solution probabilities under objective gradient guidance during the alternation
of noise injection and denoising steps. We refer to this model as Fast T2T.
Extensive experiments on two popular tasks, the Traveling Salesman Problem
(TSP) and Maximal Independent Set (MIS), demonstrate the superiority of Fast
T2T regarding both solution quality and efficiency, even outperforming LKH
given limited time budgets. Notably, Fast T2T with merely one-step generation
and one-step gradient search can mostly outperform the SOTA diffusion-based
counterparts that require hundreds of steps, while achieving tens of times
speedup.; 25) Measuring the Impact of Technical Debt on Development Effort in Software
  Projects; Technical debt refers to the trade-offs between code quality and faster
delivery, impacting future development with increased complexity, bugs, and
costs. This study empirically analyzes the additional work effort caused by
technical debt in software projects, focusing on feature implementations. I
explore how delaying technical debt repayment through refactoring influences
long-term work effort. Using data from open-source and enterprise projects, I
correlate technical debt with practical work effort, drawing from issue
trackers and version control systems. Our goal is to provide a framework for
managing technical debt, aiding developers, project managers, and stakeholders
in understanding and mitigating its impact on productivity and costs.; 26) KANITE: Kolmogorov-Arnold Networks for ITE estimation; We introduce KANITE, a framework leveraging Kolmogorov-Arnold Networks (KANs)
for Individual Treatment Effect (ITE) estimation under multiple treatments
setting in causal inference. By utilizing KAN's unique abilities to learn
univariate activation functions as opposed to learning linear weights by
Multi-Layer Perceptrons (MLPs), we improve the estimates of ITEs. The KANITE
framework comprises two key architectures: 1.Integral Probability Metric (IPM)
architecture: This employs an IPM loss in a specialized manner to effectively
align towards ITE estimation across multiple treatments. 2. Entropy Balancing
(EB) architecture: This uses weights for samples that are learned by optimizing
entropy subject to balancing the covariates across treatment groups. Extensive
evaluations on benchmark datasets demonstrate that KANITE outperforms
state-of-the-art algorithms in both $\epsilon_{\text{PEHE}}$ and
$\epsilon_{\text{ATE}}$ metrics. Our experiments highlight the advantages of
KANITE in achieving improved causal estimates, emphasizing the potential of
KANs to advance causal inference methodologies across diverse application
areas.; 27) A Tensor-Train Decomposition based Compression of LLMs on Group Vector
  Systolic Accelerator; Large language models (LLMs) are both storage-intensive and
computation-intensive, posing significant challenges when deployed on
resource-constrained hardware. As linear layers in LLMs are mainly resource
consuming parts, this paper develops a tensor-train decomposition (TTD) for
LLMs with a further hardware implementation on FPGA. TTD compression is applied
to the linear layers in ChatGLM3-6B and LLaMA2-7B models with compression
ratios (CRs) for the whole network 1.94$\times$ and 1.60$\times$, respectively.
The compressed LLMs are further implemented on FPGA hardware within a highly
efficient group vector systolic array (GVSA) architecture, which has DSP-shared
parallel vector PEs for TTD inference, as well as optimized data communication
in the accelerator. Experimental results show that the corresponding TTD based
LLM accelerator implemented on FPGA achieves 1.45$\times$ and 1.57$\times$
reduction in first token delay for ChatGLM3-6B and LLaMA2-7B models,
respectively.; 28) BS-Mamba for Black-Soil Area Detection On the Qinghai-Tibetan Plateau; Extremely degraded grassland on the Qinghai-Tibetan Plateau (QTP) presents a
significant environmental challenge due to overgrazing, climate change, and
rodent activity, which degrade vegetation cover and soil quality. These
extremely degraded grassland on QTP, commonly referred to as black-soil area,
require accurate assessment to guide effective restoration efforts. In this
paper, we present a newly created QTP black-soil dataset, annotated under
expert guidance. We introduce a novel neural network model, BS-Mamba,
specifically designed for the black-soil area detection using UAV remote
sensing imagery. The BS-Mamba model demonstrates higher accuracy in identifying
black-soil area across two independent test datasets than the state-of-the-art
models. This research contributes to grassland restoration by providing an
efficient method for assessing the extent of black-soil area on the QTP.; 29) Isometry groups of simply connected nonunimodular Lie groups of
  dimension four; For each left-invariant Riemannian metric on simply connected nonunimodular
Lie groups of dimension four, we determine the full group of isometries.; 30) Bell and Mermin inequalities in Quantum Field Theory from vacuum
  projectors and Weyl operators; The use of the vacuum projector $|0 \rangle \langle 0| $ and of the unitary
Weyl operators enables us to construct a set of Hermitian dichotomic operators
in relativistic scalar Quantum Field Theory in Minkowski spacetime. Employing
test functions supported in diamond regions, both Bell and Mermin inequalities
are studied by means of a numerical setup. In addition to reporting expressive
violations of both inequalities, the cluster property is also checked.; 31) Joint Communication and Radar Sensing for Terahertz Space-Air-Ground
  Integrated Networks (SAGIN); The transition from isolated systems to integrated solutions has driven the
development of space-air-ground integrated networks (SAGIN) as well as the
integration of communication and radar sensing functionalities. By leveraging
the unique properties of the Terahertz (THz) band, THz joint communication and
radar sensing (JCRS) supports high-speed communication and precise sensing,
addressing the growing demands of SAGIN for connectivity and environmental
awareness. However, most existing THz studies focus on terrestrial and static
scenarios, with limited consideration for the dynamic and non-terrestrial
environments of SAGIN. In this paper, the THz JCRS techniques for SAGIN are
comprehensively investigated. Specifically, propagation characteristics and
channel models of THz waves in non-terrestrial environments are analyzed. A
link capacity comparison with millimeter-wave, THz, and free-space optical
frequency bands is conducted to highlight the advantages of THz frequencies.
Moreover, novel JCRS waveform design strategies are presented to achieve mutual
merit of communication and radar sensing, while networking strategies are
developed to overcome challenges in SAGIN such as high mobility. Furthermore,
advancements in THz device technologies, including antennas and amplifiers, are
reviewed to assess their roles in enabling practical JCRS implementations.; 32) The Energy Loss Phenomenon in RLHF: A New Perspective on Mitigating
  Reward Hacking; This work identifies the Energy Loss Phenomenon in Reinforcement Learning
from Human Feedback (RLHF) and its connection to reward hacking. Specifically,
energy loss in the final layer of a Large Language Model (LLM) gradually
increases during the RL process, with an excessive increase in energy loss
characterizing reward hacking. Beyond empirical analysis, we further provide a
theoretical foundation by proving that, under mild conditions, the increased
energy loss reduces the upper bound of contextual relevance in LLMs, which is a
critical aspect of reward hacking as the reduced contextual relevance typically
indicates overfitting to reward model-favored patterns in RL. To address this
issue, we propose an Energy loss-aware PPO algorithm (EPPO) which penalizes the
increase in energy loss in the LLM's final layer during reward calculation to
prevent excessive energy loss, thereby mitigating reward hacking. We
theoretically show that EPPO can be conceptually interpreted as an
entropy-regularized RL algorithm, which provides deeper insights into its
effectiveness. Extensive experiments across various LLMs and tasks demonstrate
the commonality of the energy loss phenomenon, as well as the effectiveness of
EPPO in mitigating reward hacking and improving RLHF performance.; 33) Classification of pair symmetries in superconductors with unconventional
  magnetism; We consider superconductors with unconventional magnetism and investigate the
emergence of superconducting correlations by carrying out a full classification
of allowed Cooper pair symmetries. In particular, we focus on spin-singlet and
spin-triplet superconductors under the influence of $d$-wave altermagnets and
$p$-wave magnets. Under generic conditions, we find that unconventional magnets
not only drive a spin-singlet to spin-triplet conversion but also they transfer
their parity symmetry that induces superconducting correlations with higher
angular momentum. For instance, a conventional spin-singlet $s$-wave
superconductor with $d$-wave altermagnetism is able to host odd-frequency mixed
spin-triplet $d$-wave superconducting pair amplitudes, while when combining
with $p$-wave magnetism the emerging superconducting pairing acquires an
even-frequency mixed spin-triplet $p$-wave symmetry. We further demonstrate
that unconventional magnetism produces even more exotic superconducting
correlations in spin-singlet $d$-wave superconductors, where odd-frequency
mixed spin-triplet $g$-wave and even-frequency mixed spin-triplet $f$-wave pair
symmetries are possible in altermagnets and $p$-wave magnets, respectively. We
also discuss how these ideas generalize to spin-triplet $p$-wave
superconductors and also show how our results can be applied to unconventional
magnets with higher angular momentum, such as $f$-, $g$-, and $i$-wave. Our
results can help understand the emergent superconducting correlations due to
the interplay of unconventional magnetism and superconductivity.; 34) APB: Accelerating Distributed Long-Context Inference by Passing
  Compressed Context Blocks across GPUs; While long-context inference is crucial for advancing large language model
(LLM) applications, its prefill speed remains a significant bottleneck. Current
approaches, including sequence parallelism strategies and compute reduction
through approximate attention mechanisms, still fall short of delivering
optimal inference efficiency. This hinders scaling the inputs to longer
sequences and processing long-context queries in a timely manner. To address
this, we introduce APB, an efficient long-context inference framework that
leverages multi-host approximate attention to enhance prefill speed by reducing
compute and enhancing parallelism simultaneously. APB introduces a
communication mechanism for essential key-value pairs within a sequence
parallelism framework, enabling a faster inference speed while maintaining task
performance. We implement APB by incorporating a tailored FlashAttn kernel
alongside optimized distribution strategies, supporting diverse models and
parallelism configurations. APB achieves speedups of up to 9.2x, 4.2x, and 1.6x
compared with FlashAttn, RingAttn, and StarAttn, respectively, without any
observable task performance degradation. We provide the implementation and
experiment code of APB in https://github.com/thunlp/APB.; 35) CoT-Drive: Efficient Motion Forecasting for Autonomous Driving with LLMs
  and Chain-of-Thought Prompting; Accurate motion forecasting is crucial for safe autonomous driving (AD). This
study proposes CoT-Drive, a novel approach that enhances motion forecasting by
leveraging large language models (LLMs) and a chain-of-thought (CoT) prompting
method. We introduce a teacher-student knowledge distillation strategy to
effectively transfer LLMs' advanced scene understanding capabilities to
lightweight language models (LMs), ensuring that CoT-Drive operates in
real-time on edge devices while maintaining comprehensive scene understanding
and generalization capabilities. By leveraging CoT prompting techniques for
LLMs without additional training, CoT-Drive generates semantic annotations that
significantly improve the understanding of complex traffic environments,
thereby boosting the accuracy and robustness of predictions. Additionally, we
present two new scene description datasets, Highway-Text and Urban-Text,
designed for fine-tuning lightweight LMs to generate context-specific semantic
annotations. Comprehensive evaluations of five real-world datasets demonstrate
that CoT-Drive outperforms existing models, highlighting its effectiveness and
efficiency in handling complex traffic scenarios. Overall, this study is the
first to consider the practical application of LLMs in this field. It pioneers
the training and use of a lightweight LLM surrogate for motion forecasting,
setting a new benchmark and showcasing the potential of integrating LLMs into
AD systems.; 36) Uniqueness of gauge covariant renormalisation of stochastic 3D
  Yang-Mills-Higgs; Local solutions to the 3D stochastic quantisation equations of
Yang-Mills-Higgs were constructed in (arXiv:2201.03487), and it was shown that,
in the limit of smooth mollifications, there exists a mass renormalisation of
the Yang-Mills field such that the solution is gauge covariant. In this paper
we prove uniqueness of the mass renormalisation that leads to gauge covariant
solutions. This strengthens the main result of (arXiv:2201.03487), and is
potentially important for the identification of the limit of other
approximations, such as lattice dynamics. Our proof relies on systematic
short-time expansions of singular stochastic PDEs and of regularised Wilson
loops. We also strengthen the recently introduced state spaces to allow finer
control on line integrals appearing in expansions of Wilson loops.; 37) Colorful Vertex Recoloring of Bipartite Graphs; In vertex recoloring, we are given $n$ vertices with their initial coloring,
and edges arrive in an online fashion. The algorithm must maintain a valid
coloring by recoloring vertices, at a cost. The problem abstracts a scenario of
job placement in machines (possibly in the cloud), where vertices represent
jobs, colors represent machines, and edges represent ``anti affinity''
(disengagement) constraints. Online recoloring is a hard problem. One family of
instances which is fairly well-understood is bipartite graphs, in which two
colors are sufficient to satisfy all constraints. In this case it is known that
the competitive ratio of vertex recoloring is $\Theta(\log n)$.
  We propose a generalization of the problem, which allows using additional
colors (possibly at a higher cost), to improve overall performance. We analyze
the simple case of bipartite graphs of bounded largest \emph{bond} (a bond of a
connected graph is an edge-cut that partitions the graph into two connected
components). First, we propose two algorithms. One exhibits a trade-off for the
uniform-cost case: given $\Omega(\log\beta)\le c\le O(\log n)$ colors, the
algorithm guarantees that its cost is at most $O(\frac{\log n}{c})$ times the
optimal offline cost for two colors, where $n$ is the number of vertices and
$\beta$ is the size of the largest bond. The other algorithm is for the case
where the additional colors come at a higher cost, $D>1$: given $\Delta$
additional colors, where $\Delta$ is the maximum degree in the graph, the
algorithm guarantees $O(\log D)$ competitiveness. As to lower bounds, we show
that if the cost of the extra colors is $D>1$, no (randomized) algorithm can
achieve a competitive ratio of $o(\log D)$. We also show that for bipartite
graphs of unbounded bond size, any deterministic online algorithm has
competitive ratio $\Omega(\min(D,\log n))$.; 38) One-Shot Dual-Arm Imitation Learning; We introduce One-Shot Dual-Arm Imitation Learning (ODIL), which enables
dual-arm robots to learn precise and coordinated everyday tasks from just a
single demonstration of the task. ODIL uses a new three-stage visual servoing
(3-VS) method for precise alignment between the end-effector and target object,
after which replay of the demonstration trajectory is sufficient to perform the
task. This is achieved without requiring prior task or object knowledge, or
additional data collection and training following the single demonstration.
Furthermore, we propose a new dual-arm coordination paradigm for learning
dual-arm tasks from a single demonstration. ODIL was tested on a real-world
dual-arm robot, demonstrating state-of-the-art performance across six precise
and coordinated tasks in both 4-DoF and 6-DoF settings, and showing robustness
in the presence of distractor objects and partial occlusions. Videos are
available at: https://www.robot-learning.uk/one-shot-dual-arm.; 39) Domain Adaptation from Generated Multi-Weather Images for Unsupervised
  Maritime Object Classification; The classification and recognition of maritime objects are crucial for
enhancing maritime safety, monitoring, and intelligent sea environment
prediction. However, existing unsupervised methods for maritime object
classification often struggle with the long-tail data distributions in both
object categories and weather conditions. In this paper, we construct a dataset
named AIMO produced by large-scale generative models with diverse weather
conditions and balanced object categories, and collect a dataset named RMO with
real-world images where long-tail issue exists. We propose a novel domain
adaptation approach that leverages AIMO (source domain) to address the problem
of limited labeled data, unbalanced distribution and domain shift in RMO
(target domain), and enhance the generalization of source features with the
Vision-Language Models such as CLIP. Experimental results shows that the
proposed method significantly improves the classification accuracy,
particularly for samples within rare object categories and weather conditions.
Datasets and codes will be publicly available at
https://github.com/honoria0204/AIMO.; 40) Similar-mass versus diverse-mass planetary systems in wind-driven
  accretion discs; Many close-in multiple-planet systems show a peas-in-a-pod trend, where
neighbouring planets have similar sizes, masses, and orbital spacing. Others,
including the Solar System, have a more diverse size and mass distribution.
Classical planet formation models tend to produce the former rather than the
latter, and the origin of this difference remains unclear. Recent studies
suggest disc evolution is largely driven by magnetic winds rather than
viscosity alone. In such wind-driven accretion discs, the mass accretion rate
varies radially instead of being constant, as in classical viscous discs. We
investigate how the wind's efficiency in removing disc mass affects planet
formation and migration. We performed single-core planet formation simulations
via pebble accretion in wind-driven accretion discs. We varied wind efficiency
via the magnetic lever arm parameter $ \lambda $ and studied outcomes by
considering a range of initial disc masses and accretion timescales. Our
simulations show that higher $ \lambda $ discs, with less wind mass loss, lead
to faster formation and migration, generating similar-mass planetary systems.
Lower $ \lambda $ discs lead to slower formation and migration and more
diverse-mass planetary systems. A mass jump occurs for all $ \lambda $ cases
when planet formation and disc accretion timescales are comparable, but is
larger for lower $ \lambda $ discs. Super-Earth systems with cold Jupiters form
more frequently in metal-rich discs, consistent with observations. Our
simulations suggest similar-mass and diverse-mass systems are approximately
separated at $ \lambda \sim 2\text{--}3 $.; 41) Ripples spreading across the Galactic disc: Interplay of direct and
  indirect effects of the Sagittarius dwarf impact; Gaia data have revealed vertically asymmetric phase-space structures in the
Milky Way (MW) disc, such as phase spirals, indicating vertical oscillations.
These oscillations exhibit two distinct modes: the bending mode and the
breathing mode, associated with one-arm and two-arm phase spirals,
respectively. This study aims to explore the excitation mechanisms of the
bending and breathing modes and their subsequent evolution in the MW disc,
focusing on the interplay between direct perturbations from the Sagittarius
dwarf galaxy and indirect contributions from tidally induced spiral arms. We
perform high-resolution $N$-body simulations to model the interaction between
an MW-like disc galaxy and a Sagittarius dwarf-like satellite. These
simulations resolve fine phase-space structures, enabling analysis of the
bending and breathing modes at both macroscopic (global bending and breathing
waves) and microscopic (local phase spirals) scales. Our simulations
demonstrate that the satellite's perturbation directly excites the bending mode
and induces spiral arms in the galactic disc. These spiral arms excite the
breathing mode, making it an indirect consequence of the satellite interaction.
Initially, the bending mode dominates, but it rapidly decays due to horizontal
mixing. In contrast, the breathing mode persists for a longer duration,
sustained by the spiral arms, leading to a transition from a bending-dominated
to a breathing-dominated state. This transition progresses faster in the inner
galaxy than in the outer regions. The simulations reproduce the one-arm phase
spiral observed in the solar neighbourhood and reveal two-arm phase spirals,
particularly in the inner galaxy, associated with spiral arm-induced breathing
modes. Our findings highlight the combined effects of direct satellite
perturbations and indirect spiral arm dynamics in shaping the vertical
structure of the MW disc.; 42) Brain Controlled Wheelchair with Smart Feature; In Asia, many individuals with disabilities rely on wheelchairs for mobility.
However, some people, such as those who are fully disabled or paralyzed, cannot
use traditional wheelchairs despite having fully functioning cognitive
abilities. To address this issue, we propose the development of an electric
wheelchair that can be controlled using EEG signals and eye blinks. The project
utilizes a MindWave Mobile device and Arduino to enable seamless control.
Additionally, various sensors are incorporated to enhance the system's
reliability. An ultrasonic sensor helps avoid unexpected collisions, while a
smoke sensor detects hazardous smoke levels, triggering an automatic alert via
a short message to a designated person. Similarly, if the passenger falls from
the wheelchair, a notification will also be sent. The wheelchair's movement is
controlled via an Android application, with eye-blink detection serving as the
primary input method for navigation. This innovative design offers a
cost-effective solution, making it accessible for widespread use. By
integrating these advanced features, the system can be implemented on motorized
wheelchairs to better support individuals with disabilities and enhance their
independence.; 43) In Shift and In Variance: Assessing the Robustness of HAR Deep Learning
  Models against Variability; Human Activity Recognition (HAR) using wearable inertial measurement unit
(IMU) sensors can revolutionize healthcare by enabling continual health
monitoring, disease prediction, and routine recognition. Despite the high
accuracy of Deep Learning (DL) HAR models, their robustness to real-world
variabilities remains untested, as they have primarily been trained and tested
on limited lab-confined data. In this study, we isolate subject, device,
position, and orientation variability to determine their effect on DL HAR
models and assess the robustness of these models in real-world conditions. We
evaluated the DL HAR models using the HARVAR and REALDISP datasets, providing a
comprehensive discussion on the impact of variability on data distribution
shifts and changes in model performance. Our experiments measured shifts in
data distribution using Maximum Mean Discrepancy (MMD) and observed DL model
performance drops due to variability. We concur that studied variabilities
affect DL HAR models differently, and there is an inverse relationship between
data distribution shifts and model performance. The compounding effect of
variability was analyzed, and the implications of variabilities in real-world
scenarios were highlighted. MMD proved an effective metric for calculating data
distribution shifts and explained the drop in performance due to variabilities
in HARVAR and REALDISP datasets. Combining our understanding of variability
with evaluating its effects will facilitate the development of more robust DL
HAR models and optimal training techniques. Allowing Future models to not only
be assessed based on their maximum F1 score but also on their ability to
generalize effectively; 44) Connecting SPDE to SGMs; This paper investigates a Stochastic Partial Differential Equation (SPDE)
derived from the Fokker-Planck equation associated with Score-based Generative
Models. We modify the standard Fokker-Planck equation to better represent
practical SGMs and introduce noise to mitigate potential discretization issues.
The primary goal is to prove the existence and uniqueness of solutions for this
SPDE. This aspect requires careful consideration due to the time-dependent
operator and unbounded domain. To overcome these hurdles, we employ a
variational approach and introduce a novel space inspired by Ornstein-Uhlenbeck
operators. By demonstrating that this space and its subspace satisfy the
necessary assumptions, they establish the existence of a solution for the given
SPDE.; 45) Understanding the Uncertainty of LLM Explanations: A Perspective Based
  on Reasoning Topology; Understanding the uncertainty in large language model (LLM) explanations is
important for evaluating their faithfulness and reasoning consistency, and thus
provides insights into the reliability of LLM's output regarding a question. In
this work, we propose a novel framework that quantifies uncertainty in LLM
explanations through a reasoning topology perspective. By designing a
structural elicitation strategy, we guide the LLMs to frame the explanations of
an answer into a graph topology. This process decomposes the explanations into
the knowledge related sub-questions and topology-based reasoning structures,
which allows us to quantify uncertainty not only at the semantic level but also
from the reasoning path. It further brings convenience to assess knowledge
redundancy and provide interpretable insights into the reasoning process. Our
method offers a systematic way to interpret the LLM reasoning, analyze
limitations, and provide guidance for enhancing robustness and faithfulness.
This work pioneers the use of graph-structured uncertainty measurement in LLM
explanations and demonstrates the potential of topology-based quantification.; 46) Does Hessian Data Improve the Performance of Machine Learning
  Potentials?; Integrating machine learning into reactive chemistry, materials discovery,
and drug design is revolutionizing the development of novel molecules and
materials. Machine Learning Interatomic Potentials (MLIPs) accurately predict
energies and forces at quantum chemistry levels, surpassing traditional
methods. Incorporating force fitting into MLIP training significantly improves
the representation of potential-energy surfaces (PES), enhancing model
transferability and reliability. This study introduces and evaluates
incorporating Hessian matrix training into MLIPs, capturing second-order
curvature information of PES. Our analysis specifically examines MLIPs trained
solely on stable molecular geometries, assessing their extrapolation
capabilities to non-equilibrium configurations. We show that integrating
Hessian information substantially improves MLIP performance in predicting
energies, forces, and Hessians for non-equilibrium structures. Hessian-trained
MLIPs notably enhance reaction pathway modeling, transition state
identification, and vibrational spectra accuracy, benefiting molecular dynamics
simulations and Nudged Elastic Band (NEB) calculations. By comparing models
trained with various combinations of energy, force, and Hessian data on a
small-molecule reactive dataset, we demonstrate Hessian inclusion leads to
improved accuracy in reaction modeling and vibrational analyses while
simultaneously reducing the total data needed for effective training. The
primary trade-off is increased computational expense, as Hessian training
demands more resources than conventional methods. Our results offer
comprehensive insights into the strengths and limitations of Hessian
integration in MLIP training, enabling practitioners in computational chemistry
to make informed decisions aligned with their research goals and available
computational resources.; 47) Structures of Monoids Motivated by DNA Origami; We construct a class of monoids, called origami monoids, motivated by Jones
monoids and by strand organization in DNA origami structures. Two types of
basic building blocks of DNA origami closely associated with the graphical
representation of Jones monoids are identified and are taken as generators for
the origami monoid. Motivated by plausible modifications of the DNA origami
structures and the relations of the well studied Jones monoids, we then
identify a set of relations that characterize the origami monoid. These
relations expand the relations of the Jones monoids and include a new set of
relations called contextual commutation. With contextual commutation, certain
generators commute only when found within a given context. We prove that the
origami monoids are finite and propose a normal form representation of their
elements. We establish a correspondence between the Green's classes of the
origami monoid and the Green's classes of a direct product of Jones monoids.; 48) Limits to AI Growth: The Ecological and Social Consequences of Scaling; The accelerating development and deployment of AI technologies depend on the
continued ability to scale their infrastructure. This has implied increasing
amounts of monetary investment and natural resources. Frontier AI applications
have thus resulted in rising financial, environmental, and social costs. While
the factors that AI scaling depends on reach its limits, the push for its
accelerated advancement and entrenchment continues. In this paper, we provide a
holistic review of AI scaling using four lenses (technical, economic,
ecological, and social) and review the relationships between these lenses to
explore the dynamics of AI growth. We do so by drawing on system dynamics
concepts including archetypes such as ""limits to growth"" to model the dynamic
complexity of AI scaling and synthesize several perspectives. Our work maps out
the entangled relationships between the technical, economic, ecological and
social perspectives and the apparent limits to growth. The analysis explains
how industry's responses to external limits enables continued (but temporary)
scaling and how this benefits Big Tech while externalizing social and
environmental damages. To avoid an ""overshoot and collapse"" trajectory, we
advocate for realigning priorities and norms around scaling to prioritize
sustainable and mindful advancements.; 49) PSF-4D: A Progressive Sampling Framework for View Consistent 4D Editing; Instruction-guided generative models, especially those using text-to-image
(T2I) and text-to-video (T2V) diffusion frameworks, have advanced the field of
content editing in recent years. To extend these capabilities to 4D scene, we
introduce a progressive sampling framework for 4D editing (PSF-4D) that ensures
temporal and multi-view consistency by intuitively controlling the noise
initialization during forward diffusion. For temporal coherence, we design a
correlated Gaussian noise structure that links frames over time, allowing each
frame to depend meaningfully on prior frames. Additionally, to ensure spatial
consistency across views, we implement a cross-view noise model, which uses
shared and independent noise components to balance commonalities and distinct
details among different views. To further enhance spatial coherence, PSF-4D
incorporates view-consistent iterative refinement, embedding view-aware
information into the denoising process to ensure aligned edits across frames
and views. Our approach enables high-quality 4D editing without relying on
external models, addressing key challenges in previous methods. Through
extensive evaluation on multiple benchmarks and multiple editing aspects (e.g.,
style transfer, multi-attribute editing, object removal, local editing, etc.),
we show the effectiveness of our proposed method. Experimental results
demonstrate that our proposed method outperforms state-of-the-art 4D editing
methods in diverse benchmarks.; 50) Weibull Processes in Network Degree Distributions; This study examines degree distributions in two large collaboration networks:
the Microsoft Academic Graph (1800-2020) and Internet Movie Database
(1900-2020), comprising $2.72 \times 10^8$ and $1.88 \times 10^6$ nodes
respectively. Statistical comparison using $\chi^2$ measures showed that
Weibull distributions fit the degree distributions better than power-law or
log-normal models, especially at later stages in the network evolution. The
Weibull shape parameters exhibit notable stability ($k \approx 0.8$-$1.0$ for
academic, $k \approx 0.9$-$1.1$ for entertainment collaborations) despite
orders of magnitude growth in network size. While early-stage networks display
approximate power-law scaling, mature networks develop characteristic
flattening in the low-degree region that Weibull distributions appear to
capture better. In the academic network, the cutoff between the flattened
region and power-law tail shows a gradual increase from $5$ to $9$ edges over
time, while the entertainment network maintains a distinctive degree structure
that may reflect storytelling and cast-size constraints. These patterns suggest
the possibility that collaboration network evolution might be influenced more
by constraint-based growth than by pure preferential attachment or
multiplicative processes.; 51) The action of the Morava stabilizer group on the coefficients of Morava
  E-theory at height 2; We calculate an explicit closed formula for the action of the height 2 full
Morava stabilizer group on the coefficient ring of height 2 Morava E-theory. In
particular, this yields an explicit, surprisingly simple closed formula for the
action of the automorphism group of a height 2 formal group law on its
Lubin-Tate deformation ring. The formula is of a combinatorial nature, given by
sums over certain labelled ordered rooted trees.; 52) Cryptocurrency Network Analysis; Cryptocurrency network analysis consists of applying the tools and methods of
social network analysis to transactional data issued from cryptocurrencies. The
main difference with most online social networks is that users do not exchange
textual content but instead value -- in systems designed mainly as
cryptocurrency, such as Bitcoin -- or digital items and services in more
permissive systems based on smart contracts such as Ethereum.; 53) An ordinal analysis of CM and its extensions; In arXiv:0905.1675, Nik Weaver proposed a novel intuitionistic formal theory
of third-order arithmetic as a formalisation of his philosophical position
known as mathematical conceptualism. In this paper, we will construct a
realisability model from the partial combinatory algebra of
$\Sigma^1_1$-definable partial functions and use it to provide an ordinal
analysis of this formal theory. Additionally, we will examine possible
extensions to this system by adding well-ordering axioms, which are briefly
mentioned but never thoroughly studied in Weaver's work. We aim to use the
realisability arguments to discuss how much such extensions constitute an
increase from the original theory's proof-theoretic strength.; 54) PHIBSS: Searching for Molecular Gas Outflows in Star-Forming Galaxies at
  $z =$ 0.5-2.6; We present an analysis of millimeter CO observations to search and quantify
signatures of molecular gas outflows. We exploit the large sample of $0.5 < z <
2.6$ galaxies observed as part of the PHIBSS1/2 surveys with the IRAM Plateau
de Bure interferometer, focusing on the 154 typical massive star-forming
galaxies with CO detections (mainly CO(3-2), but including also CO(2-1) and
CO(6-5)) at signal-to-noise (SNR) > 1.5 and available properties (stellar mass,
star formation rate, size) from ancillary data. None of the individual spectra
exhibit a compelling signature of CO outflow emission even at high SNR > 7. To
search for fainter outflow signatures, we carry out an analysis of stacked
spectra, including the full sample, as well as subsets, split in terms of
stellar mass, redshift, inclination, offset in star formation rate (SFR) from
the main sequence, and AGN activity. None of the physically motivated
subsamples show any outflow signature. We report a tentative detection in a
subset statistically designed to maximize outflow signatures. We derive upper
limits on molecular gas outflow rate and mass loading factors $\eta$ based on
our results and find $\eta \leq$ 2.2-35.4, depending on the subsample. Much
deeper CO data and observations of alternative tracers are needed to decisively
constrain the importance of cold molecular gas component of outflows relative
to other gas phases.; 55) Uniqueness of asymptotically conical shrinking gradient K\""ahler-Ricci
  solitons; We show that, up to biholomorphism, a given noncompact complex manifold only
admits one shrinking gradient K\""ahler-Ricci soliton with Ricci curvature
tending to zero at infinity. Our result does not require fixing the asymptotic
data of the metric, nor fixing the soliton vector field. The method used to
prove the uniqueness of the soliton vector field can be applied more widely,
for example to show that conical Calabi-Yau metrics on a given complex manifold
are unique up to biholomorphism. We also use it to prove that if two polarized
Fano fibrations, as introduced by Sun-Zhang, are biholomorphic and their
vertices agree, then they are isomorphic as algebraic varieties.; 56) Universal Efimov Scaling in the Rabi-Coupled Few-Body Spectrum; We investigate the behavior of the Efimov effect -- a universal quantum
few-body phenomenon -- in the presence of an external driving field.
Specifically, we consider up to three bosonic atoms, such as $^{133}$Cs,
interacting with a light atom, such as $^{6}$Li, where the latter has two
internal spin states $\{\uparrow, \downarrow\}$ that are Rabi coupled. Assuming
that only the spin-$\uparrow$ light atom interacts with the bosons, we find
that the Rabi drive transposes the entire Efimov spectrum such that the Efimov
trimers and tetramers are centered around the Rabi-shifted two-body scattering
resonance. Crucially, we show that the Rabi drive preserves the trimers'
discrete scaling symmetry, while universally shifting the Efimov three-body
parameter, leading to a log-periodic modulation in the spectrum as the Rabi
drive is varied. Our results suggest that Efimov physics can be conveniently
explored using an applied driving field, opening up the prospect of an
externally tunable three-body parameter.; 57) Strategic Learning with Local Explanations as Feedback; We investigate algorithmic decision problems where agents can respond
strategically to the decision maker's (DM) models. The demand for clear and
actionable explanations from DMs to (potentially strategic) agents continues to
rise. While prior work often treats explanations as full model disclosures,
explanations in practice might convey only partial information, which can lead
to misinterpretations and harmful responses. When full disclosure of the
predictive model is neither feasible nor desirable, a key open question is how
DMs can use explanations to maximise their utility without compromising agent
welfare. In this work, we explore well-known local and global explanation
methods, and establish a necessary condition to prevent explanations from
misleading agents into self-harming actions. Moreover, with conditional
homogeneity, we establish that action recommendation (AR)-based explanations
are sufficient for non-harmful responses, akin to the revelation principle in
information design. To operationalise AR-based explanations, we propose a
simple algorithm to jointly optimise the predictive model and AR policy to
balance DM outcomes with agent welfare. Our empirical results demonstrate the
benefits of this approach as a more refined strategy for safe and effective
partial model disclosure in algorithmic decision-making.; 58) Compactification of homology cells, Fujita's conjectures and the complex
  projective space; We show that a compact K\""ahler manifold $M$ containing a smooth connected
divisor $D$ such that $M \setminus D$ is a homology cell, e.g., contractible,
must be projective space with $D$ a hyperplane, provided $\dim M \not \equiv 3
\pmod 4$. This answers conjectures of Fujita in these dimensions.; 59) GOD model: Privacy Preserved AI School for Personal Assistant; Personal AI assistants (e.g., Apple Intelligence, Meta AI) offer proactive
recommendations that simplify everyday tasks, but their reliance on sensitive
user data raises concerns about privacy and trust. To address these challenges,
we introduce the Guardian of Data (GOD), a secure, privacy-preserving framework
for training and evaluating AI assistants directly on-device. Unlike
traditional benchmarks, the GOD model measures how well assistants can
anticipate user needs-such as suggesting gifts-while protecting user data and
autonomy. Functioning like an AI school, it addresses the cold start problem by
simulating user queries and employing a curriculum-based approach to refine the
performance of each assistant. Running within a Trusted Execution Environment
(TEE), it safeguards user data while applying reinforcement and imitation
learning to refine AI recommendations. A token-based incentive system
encourages users to share data securely, creating a data flywheel that drives
continuous improvement. Specifically, users mine with their data, and the
mining rate is determined by GOD's evaluation of how well their AI assistant
understands them across categories such as shopping, social interactions,
productivity, trading, and Web3. By integrating privacy, personalization, and
trust, the GOD model provides a scalable, responsible path for advancing
personal AI assistants. For community collaboration, part of the framework is
open-sourced at https://github.com/PIN-AI/God-Model.; 60) Quantifying Variance in Evaluation Benchmarks; Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models.; 61) Quantum Trojan Insertion: Controlled Activation for Covert Circuit
  Manipulation; Quantum computing has demonstrated superior efficiency compared to classical
computing. Quantum circuits are essential for implementing functions and
achieving correct computational outcomes. Quantum circuit compilers, which
translate high-level quantum operations into hardware-specific gates while
optimizing performance, serve as the interface between the quantum software
stack and physical quantum machines. However, untrusted compilers can introduce
malicious hardware Trojans into quantum circuits, altering their functionality
and leading to incorrect results. In the world of classical computing,
effective hardware Trojans are a critical threat to integrated circuits. This
process often involves stealthily inserting conditional logic gates that
activate under specific input conditions. In this paper, we propose a novel
advanced quantum Trojan that is controllable, allowing it to be activated or
deactivated under different circumstances. These Trojans remain dormant until
triggered by predefined input conditions, making detection challenging. Through
a series of benchmark experiments, we demonstrate the feasibility of this
method by evaluating the effectiveness of embedding controlled trojans in
quantum circuits and measuring their impact on circuit performance and
security.; 62) Estimating Propensities of Selection for Big Datasets via Data
  Integration; Big data presents potential but unresolved value as a source for analysis and
inference. However,selection bias, present in many of these datasets, needs to
be accounted for so that appropriate inferences can be made on the target
population. One way of approaching the selection bias issue is to first
estimate the propensity of inclusion in the big dataset for each member of the
big dataset, and then to apply these propensities in an inverse probability
weighting approach to produce population estimates. In this paper, we provide
details of a new variant of existing propensity score estimation methods that
takes advantage of the ability to integrate the big data with a probability
sample. We compare the ability of this method to produce efficient inferences
for the target population with several alternative methods through an empirical
study.; 63) Effects of correlated noise on the excitation of robust breathers in an
  ac-driven, lossy sine-Gordon system; Thermal noise and harmonic forcing have recently been shown to cooperatively
excite sine-Gordon breathers robust to dissipation. Such a phenomenon has been
found assuming a Gaussian noise source, delta-correlated both in time and
space. In light of the potential implications of this generation technique,
e.g., for the experimental observation of breathers in long Josephson
junctions, it is physically motivated to investigate the effects of more
realistic noise sources with finite correlation time and/or correlation length.
Here, breathers are demonstrated to still emerge under this broader class of
noise sources. The correlation time and the correlation length are found to
offer control over the probability of observing breathers, as well on the
typical timescale for their emergence. In particular, our results show that, as
compared to the thermal case, the temporal and spatial correlations in the
noise can lead to a larger breather-only occurrence frequency, i.e., the latter
quantity behaves nonmonotonically versus both the correlation time and the
correlation length. Overall, noise correlations represent a powerful tool for
controlling the excitation of the elusive breather modes in view of
experiments.; 64) SurgRIPE challenge: Benchmark of Surgical Robot Instrument Pose
  Estimation; Accurate instrument pose estimation is a crucial step towards the future of
robotic surgery, enabling applications such as autonomous surgical task
execution. Vision-based methods for surgical instrument pose estimation provide
a practical approach to tool tracking, but they often require markers to be
attached to the instruments. Recently, more research has focused on the
development of marker-less methods based on deep learning. However, acquiring
realistic surgical data, with ground truth instrument poses, required for deep
learning training, is challenging. To address the issues in surgical instrument
pose estimation, we introduce the Surgical Robot Instrument Pose Estimation
(SurgRIPE) challenge, hosted at the 26th International Conference on Medical
Image Computing and Computer-Assisted Intervention (MICCAI) in 2023. The
objectives of this challenge are: (1) to provide the surgical vision community
with realistic surgical video data paired with ground truth instrument poses,
and (2) to establish a benchmark for evaluating markerless pose estimation
methods. The challenge led to the development of several novel algorithms that
showcased improved accuracy and robustness over existing methods. The
performance evaluation study on the SurgRIPE dataset highlights the potential
of these advanced algorithms to be integrated into robotic surgery systems,
paving the way for more precise and autonomous surgical procedures. The
SurgRIPE challenge has successfully established a new benchmark for the field,
encouraging further research and development in surgical robot instrument pose
estimation.; 65) Progressive Sparse Attention: Algorithm and System Co-design for
  Efficient Attention in LLM Serving; Processing long contexts has become a critical capability for modern large
language models (LLMs). However, serving long-context LLMs comes with
significant inference costs due to the high memory overhead of the key-value
(KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes)
to mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV
cache selection, which results in a trade-off between accuracy and efficiency.
A larger $k$ improves accuracy but decreases efficiency, while a smaller $k$
boosts efficiency but compromises accuracy. To overcome this trade-off, this
paper presents PSA, a $\underline{P}$rogressive $\underline{S}$parse
$\underline{A}$ttention mechanism that integrates algorithmic innovations with
system co-design to achieve both high inference accuracy and improved
efficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache
budget of different tokens and layers according to their real attention weight
distributions, rather than relying on a fixed budget $k$. This enables high
accuracy while minimizing KV cache usage. To further enhance execution
efficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU
interleaving and synchronization overhead during PSA computation. Additionally,
we implement unified GPU memory management that optimizes PSA's memory
utilization by accounting for uneven memory requirements across different model
layers. Extensive experimental results demonstrate that PSA reduces KV cache
usage for attention computation by up to 2.4$\times$ and 8.8$\times$, and
increases end-to-end serving throughput by up to 1.4$\times$ and 2.0$\times$,
compared to state-of-the-art DSAes and systems without sparse attention,
respectively.; 66) Real-time Monitoring of Economic Shocks using Company Websites; Understanding the effects of economic shocks on firms is critical for
analyzing economic growth and resilience. We introduce a Web-Based Affectedness
Indicator (WAI), a general-purpose tool for real-time monitoring of economic
disruptions across diverse contexts. By leveraging Large Language Model (LLM)
assisted classification and information extraction on texts from over five
million company websites, WAI quantifies the degree and nature of firms'
responses to external shocks. Using the COVID-19 pandemic as a specific
application, we show that WAI is highly correlated with pandemic containment
measures and reliably predicts firm performance. Unlike traditional data
sources, WAI provides timely firm-level information across industries and
geographies worldwide that would otherwise be unavailable due to institutional
and data availability constraints. This methodology offers significant
potential for monitoring and mitigating the impact of technological, political,
financial, health or environmental crises, and represents a transformative tool
for adaptive policy-making and economic resilience.; 67) The Spinning Blimp: Design and Control of a Novel Minimalist Aerial
  Vehicle Leveraging Rotational Dynamics and Locomotion; This paper presents the Spinning Blimp, a novel lighter-than-air (LTA) aerial
vehicle designed for low-energy stable flight. Utilizing an oblate spheroid
helium balloon for buoyancy, the vehicle achieves minimal energy consumption
while maintaining prolonged airborne states. The unique and low-cost design
employs a passively arranged wing coupled with a propeller to induce a spinning
behavior, providing inherent pendulum-like stabilization. We propose a control
strategy that takes advantage of the continuous revolving nature of the
spinning blimp to control translational motion. The cost-effectiveness of the
vehicle makes it highly suitable for a variety of applications, such as
patrolling, localization, air and turbulence monitoring, and domestic
surveillance. Experimental evaluations affirm the design's efficacy and
underscore its potential as a versatile and economically viable solution for
aerial applications.; 68) Excitability and oscillations of active droplets; In living cells, cycles of formation and dissolution of liquid droplets can
mediate biological functions such as DNA repair. However, the minimal
physicochemical prerequisite for such droplet oscillations remains elusive.
Here, we present a simple model composed of only two independent chemical
components with their diffusive and chemical fluxes governed by non-equilibrium
thermodynamics. There is turnover of fuel that maintains a chemical reaction
away from equilibrium, leading to active droplets. We find that a single active
droplet undergoes a pitchfork-bifurcation in the droplet volume upon increasing
the fueling strength. Strikingly, the active droplet becomes excitable upon
adding a further chemical reaction. For sufficient fueling, the system
undergoes self-sustained oscillations cycling between droplet formation and
dissolution. The minimal nature of our model suggests self-sustained active
droplets as functional modules for de novo life.; 69) On the Representation Categories of Weak Hopf Algebras Arising from
  Levin-Wen Models; In their study of Levin-Wen models [Commun. Math. Phys. 313 (2012) 351-373],
Kitaev and Kong proposed a weak Hopf algebra associated with a unitary fusion
category $\mathcal{C}$ and a unitary left $\mathcal{C}$-module $\mathcal{M}$,
and sketched a proof that its representation category is monoidally equivalent
to the unitary $\mathcal{C}$-module functor category
$\mathrm{Fun}^{\mathrm{u}}_{\mathcal{C}}(\mathcal{M},\mathcal{M})^\mathrm{rev}$.
We give an independent proof of this result without the unitarity conditions.
In particular, viewing $\mathcal{C}$ as a left $\mathcal{C} \boxtimes
\mathcal{C}^{\mathrm{rev}}$-module, we obtain a quasi-triangular weak Hopf
algebra whose representation category is braided equivalent to the Drinfeld
center $\mathcal{Z}(\mathcal{C})$. In the appendix, we also compare this
quasi-triangular weak Hopf algebra with the tube algebra
$\mathrm{Tube}_{\mathcal{C}}$ of $\mathcal{C}$ when $\mathcal{C}$ is pivotal.
These two algebras are Morita equivalent by the well-known equivalence
$\mathrm{Rep}(\mathrm{Tube}_{\mathcal{C}})\cong\mathcal{Z}(\mathcal{C})$.
However, we show that in general there is no weak Hopf algebra structure on
$\mathrm{Tube}_{\mathcal{C}}$ such that the above equivalence is monoidal.; 70) Towards Trustworthy Retrieval Augmented Generation for Large Language
  Models: A Survey; Retrieval-Augmented Generation (RAG) is an advanced technique designed to
address the challenges of Artificial Intelligence-Generated Content (AIGC). By
integrating context retrieval into content generation, RAG provides reliable
and up-to-date external knowledge, reduces hallucinations, and ensures relevant
context across a wide range of tasks. However, despite RAG's success and
potential, recent studies have shown that the RAG paradigm also introduces new
risks, including robustness issues, privacy concerns, adversarial attacks, and
accountability issues. Addressing these risks is critical for future
applications of RAG systems, as they directly impact their trustworthiness.
Although various methods have been developed to improve the trustworthiness of
RAG methods, there is a lack of a unified perspective and framework for
research in this topic. Thus, in this paper, we aim to address this gap by
providing a comprehensive roadmap for developing trustworthy RAG systems. We
place our discussion around five key perspectives: reliability, privacy,
safety, fairness, explainability, and accountability. For each perspective, we
present a general framework and taxonomy, offering a structured approach to
understanding the current challenges, evaluating existing solutions, and
identifying promising future research directions. To encourage broader adoption
and innovation, we also highlight the downstream applications where trustworthy
RAG systems have a significant impact.; 71) Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and
  Refinement; The quality of Supervised Fine-Tuning (SFT) data plays a critical role in
enhancing the conversational capabilities of Large Language Models (LLMs).
However, as LLMs become more advanced, the availability of high-quality
human-annotated SFT data has become a significant bottleneck, necessitating a
greater reliance on synthetic training data. In this work, we introduce Condor,
a novel two-stage synthetic data generation framework that incorporates World
Knowledge Tree and Self-Reflection Refinement to produce high-quality SFT data
at scale. Our experimental results demonstrate that a base model fine-tuned on
only 20K Condor-generated samples achieves superior performance compared to
counterparts. The additional refinement stage in Condor further enables
iterative self-improvement for LLMs at various scales (up to 72B), validating
the effectiveness of our approach. Furthermore, our investigation into the
scaling for synthetic data in post-training reveals substantial unexplored
potential for performance improvements, opening promising avenues for future
research.; 72) Establishing tool support for a concept DSL; The quality of software products tends to correlate with the quality of the
abstractions adopted early in the design process. Acknowledging this tendency
has led to the development of various tools and methodologies for modeling
systems thoroughly before implementing them. However, creating effective
abstract models of domain problems is difficult, especially if the models are
also expected to exhibit qualities such as intuitiveness, being seamlessly
integrable with other models, or being easily translatable into code.
  This thesis describes Conceptual, a DSL for modeling the behavior of software
systems using self-contained and highly reusable units of functionally known as
concepts. The language's syntax and semantics are formalized based on previous
work. Additionally, the thesis proposes a strategy for mapping language
constructs from Conceptual into the Alloy modeling language. The suggested
strategy is then implemented with a simple compiler, allowing developers to
access and utilize Alloy's existing analysis tools for program reasoning.
  The utility and expressiveness of Conceptual is demonstrated qualitatively
through several practical case studies. Using the implemented compiler, a few
erroneous specifications are identified in the literature. Moreover, the thesis
establishes preliminary tool support in the Visual Studio Code IDE.; 73) Confidence Improves Self-Consistency in LLMs; Self-consistency decoding enhances LLMs' performance on reasoning tasks by
sampling diverse reasoning paths and selecting the most frequent answer.
However, it is computationally expensive, as sampling many of these (lengthy)
paths is required to increase the chances that the correct answer emerges as
the most frequent one. To address this, we introduce Confidence-Informed
Self-Consistency (CISC). CISC performs a weighted majority vote based on
confidence scores obtained directly from the model. By prioritizing
high-confidence paths, it can identify the correct answer with a significantly
smaller sample size. When tested on nine models and four datasets, CISC
outperforms self-consistency in nearly all configurations, reducing the
required number of reasoning paths by over 40% on average. In addition, we
introduce the notion of within-question confidence evaluation, after showing
that standard evaluation methods are poor predictors of success in
distinguishing correct and incorrect answers to the same question. In fact, the
most calibrated confidence method proved to be the least effective for CISC.
Lastly, beyond these practical implications, our results and analyses show that
LLMs can effectively judge the correctness of their own outputs, contributing
to the ongoing debate on this topic.; 74) Multimodal Graph Constrastive Learning and Prompt for ChartQA; ChartQA presents significant challenges due to the complex distribution of
chart elements and the implicit patterns embedded within the underlying data.
In this chapter, we have developed a joint multimodal scene graph for charts,
explicitly representing the relationships between chart elements and their
associated patterns.
  Our proposed multimodal scene graph consists of two components: a visual
graph and a textual graph, each designed to capture the structural and semantic
information within the chart. To unify representations across these different
modalities, we introduce a multimodal graph contrastive learning approach that
learns unified representations by maximizing similarity between nodes
representing the same object across multimodal graphs. The learned graph
representations can be seamlessly incorporated into a transformer decoder as a
soft prompt.
  Additionally, given the growing need for Multimodal Large Language Models
(MLLMs) in zero-shot scenarios, we have designed Chain-of-Thought (CoT) prompts
for MLLMs to reduce hallucinations. We tested both methods on public benchmarks
such as ChartQA, OpenCQA, and ChartX, demonstrating improved performance and
validating the effectiveness of our proposed methods.; 75) Polarizabilities of low-lying states of silver; Assembly of ultracold polar molecules containing silver (Ag) from
laser-cooled atoms requires knowledge of the dynamic polarizabilities of Ag at
convenient laser wavelengths. We present calculations and analysis of the
energies and electric-dipole dc and ac polarizabilities of the low-lying states
of neutral Ag. Calculations of the properties of the 4d^{10}x states, where
x=5s,6s,7s,5p,6p,7p,5d,6d, and 4f, are performed using the linearized coupled
cluster single-double method. The properties of the 4d^9 5s^2 ^2D_{5/2,3/2}
states are obtained within the framework of configuration interaction with 11
and 17 electrons in the valence field. We analyze the different contributions
to the polarizabilities and estimate the uncertainties of our predictions.; 76) Natural Reference Frames within Video Analysis; This study explores an alternative approach to video-based motion analysis
using natural reference frames rather than relying on manual alignment. We
demonstrate how the motion data itself can reveal optimal reference frames,
connecting fundamental physical principles with data analysis. We present three
classical mechanics experiments: uniform motion along a track, where the motion
direction naturally defines the axis; circular motion of a bicycle wheel, where
the center of rotation emerges as the reference origin; and projectile motion
from a juggling tutorial, where gravity determines the vertical direction. This
approach demonstrates how physical constraints and symmetries can naturally
emerge from experimental data, providing a viable approach for introductory
physics laboratories.; 77) Degree of freedom count in linear gauge invariant PDE systems; Suppose a system of partial differential equations with constant coefficients
describes a classical field theory. Einstein proposed a definition of the
strength of such a theory and its degrees of freedom (DoF) based on the
asymptotic number of free Taylor series coefficients of bounded degree in the
general solution of the system. however, direct calculating the DoF is a
nontrivial task. Here, we apply commutative algebra methods to this problem.
  We begin by interpreting the matrix of the system as a linear map between
polynomial modules. First, we derive an explicit formula for the DoF as the
multiplicity of a certain extension module. Second, we prove (for homogeneous
and certain more general systems) another explicit formula for the DoF in terms
of gauge symmetries and identities. A notable consequence of this formula is
that two Hermitian conjugate systems have identical DoF.; 78) DCAMamba: Mamba-based Rapid Response DC Arc Fault Detection; In electrical equipment, even minor contact issues can lead to arc faults.
Traditional methods often struggle to balance the accuracy and rapid response
required for effective arc fault detection. To address this challenge, we
introduce DCAMamba, a novel framework for arc fault detection. Specifically,
DCAMamba is built upon a state-space model (SSM) and utilizes a hardware-aware
parallel algorithm, designed in a cyclic mode using the Mamba architecture. To
meet the dual demands of high accuracy and fast response in arc fault
detection, we have refined the original Mamba model and incorporated a Feature
Amplification Strategy (FAS), a simple yet effective method that enhances the
model's ability to interpret arc fault data. Experimental results show that
DCAMamba, with FAS, achieves a 12$\%$ improvement in accuracy over the original
Mamba, while maintaining an inference time of only 1.87 milliseconds. These
results highlight the significant potential of DCAMamba as a future backbone
for signal processing. Our code will be made open-source after peer review.; 79) Symbolic Computations of the Two-Colored Diagrams for Central
  Configurations of the Planar N-vortex Problem; We apply the singular sequence method to investigate the finiteness problem
for stationary configurations of the planar N-vortex problem. The initial step
of the singular sequence method involves identifying all two-colored diagrams.
These diagrams represent potential scenarios where finiteness may fail. We
develop a symbolic computation algorithm to determine all two-colored diagrams
for central configurations of the planar N-vortex problem.; 80) Improved Sublinear-time Moment Estimation using Weighted Sampling; In this work we study the {\it moment estimation} problem using weighted
sampling. Given sample access to a set $A$ with $n$ weighted elements, and a
parameter $t>0$, we estimate the $t$-th moment of $A$ given as $S_t=\sum_{a\in
A} w(a)^t$. For t=1, this is the sum estimation problem. The moment estimation
problem along with a number of its variants have been extensively studied in
streaming, sublinear and distributed communication models. Despite being well
studied, we don't yet have a complete understanding of the sample complexity of
the moment estimation problem in the sublinear model and in this work, we make
progress on this front. On the algorithmic side, our upper bounds match the
known upper bounds for the problem for $t>1$. To the best of our knowledge, no
sublinear algorithms were known for this problem for $0<t<1$. We design a
sublinear algorithm for this problem for $t>1/2$ and show that no sublinear
algorithms exist for $t\leq 1/2$. We prove a $\Omega(\frac{n^{1-1/t}\ln
1/\delta}{\epsilon^2})$ lower bound for moment estimation for $t>1$, and show
optimal sample complexity bound $\Theta(\frac{n^{1-1/t}\ln
1/\delta}{\epsilon^2})$ for moment estimation for $t\geq 2$. Hence, we obtain a
complete understanding of the sample complexity for moment estimation using
proportional sampling for $t\geq 2$. We also study the moment estimation
problem in the beyond worst-case analysis paradigm and identify a new {\it
moment-density} parameter of the input that characterizes the sample complexity
of the problem using proportional sampling and derive tight sample complexity
bounds with respect to that parameter. We also study the moment estimation
problem in the hybrid sampling framework in which one is given additional
access to a uniform sampling oracle and show that hybrid sampling framework
does not provide any additional gain over the proportional sampling oracle in
the worst case.; 81) Statistical estimation of a mean-field FitzHugh-Nagumo model; We consider an interacting system of particles with value in $\mathbb{R}^d
\times \mathbb{R}^d$, governed by transport and diffusion on the first
component, on that may serve as a representative model for kinetic models with
a degenerate component. In a first part, we control the fluctuations of the
empirical measure of the system around the solution of the corresponding
Vlasov-Fokker-Planck equation by proving a Bernstein concentration inequality,
extending a previous result of arXiv:2011.03762 in several directions. In a
second part, we study the nonparametric statistical estimation of the classical
solution of Vlasov-Fokker-Planck equation from the observation of the empirical
measure and prove an oracle inequality using the Goldenshluger-Lepski
methodology and we obtain minimax optimality. We then specialise on the
FitzHugh-Nagumo model for populations of neurons. We consider a version of the
model proposed in Mischler et al. arXiv:1503.00492 an optimally estimate the
$6$ parameters of the model by moment estimators.; 82) J-braid groups are torus necklace groups; We construct a family of links we call torus necklaces for which the link
groups are precisely the braid groups of generalised $J$-reflection groups.
Moreover, this correspondence exhibits the meridians of the aforementioned link
groups as braid reflections. In particular, this construction generalises to
all irreducible rank two complex reflection groups a well-known correspondence
between some rank two complex braid groups and some torus knot groups. In
addition, as abstract groups, we show that the family of link groups associated
to Seifert links coincides with the family of circular groups. This shows that
every time a link group has a non-trivial center, it is a Garside group.; 83) Statistical physics analysis of graph neural networks: Approaching
  optimality in the contextual stochastic block model; Graph neural networks (GNNs) are designed to process data associated with
graphs. They are finding an increasing range of applications; however, as with
other modern machine learning techniques, their theoretical understanding is
limited. GNNs can encounter difficulties in gathering information from nodes
that are far apart by iterated aggregation steps. This situation is partly
caused by so-called oversmoothing; and overcoming it is one of the practically
motivated challenges. We consider the situation where information is aggregated
by multiple steps of convolution, leading to graph convolutional networks
(GCNs). We analyze the generalization performance of a basic GCN, trained for
node classification on data generated by the contextual stochastic block model.
We predict its asymptotic performance by deriving the free energy of the
problem, using the replica method, in the high-dimensional limit. Calling depth
the number of convolutional steps, we show the importance of going to large
depth to approach the Bayes-optimality. We detail how the architecture of the
GCN has to scale with the depth to avoid oversmoothing. The resulting large
depth limit can be close to the Bayes-optimality and leads to a continuous GCN.
Technically, we tackle this continuous limit via an approach that resembles
dynamical mean-field theory (DMFT) with constraints at the initial and final
times. An expansion around large regularization allows us to solve the
corresponding equations for the performance of the deep GCN. This promising
tool may contribute to the analysis of further deep neural networks.; 84) 10 Years of Archival High-Resolution NIR Spectra: The Raw and Reduced
  IGRINS Spectral Archive (RRISA); The Immersion GRating INfrared Spectrometer (IGRINS) is a compact,
high-resolution (R~45,000) near-infrared spectrograph spanning 1.45 to 2.45 um
in a single exposure. We introduce the Raw and Reduced IGRINS Spectral Archive
(RRISA), which provides public data access for all non-proprietary IGRINS data
taken at McDonald Observatory's Harlan J. Smith Telescope, the Lowell Discovery
Telescope (formerly Discovery Channel Telescope), and Gemini South. RRISA
provides access to raw files, reduced data products, and cross-matched IGRINS
targets with the SIMBAD, 2MASS, Gaia DR3, APOGEE2 DR17, and PASTEL catalogs. We
also introduce version 3 of the IGRINS data reduction pipeline, IGRINS PLP v3,
which implements an improved cosmic ray correction, pattern noise removal, and
a new flexure correction that reduces telluric residuals. RRISA and supporting
information can be found at http://igrinscontact.github.io.; 85) Accurate AI-Driven Emergency Vehicle Location Tracking in Healthcare ITS
  Digital Twin; Creating a Digital Twin (DT) for Healthcare Intelligent Transportation
Systems (HITS) is a hot research trend focusing on enhancing HITS management,
particularly in emergencies where ambulance vehicles must arrive at the crash
scene on time and track their real-time location is crucial to the medical
authorities. Despite the claim of real-time representation, a temporal
misalignment persists between the physical and virtual domains, leading to
discrepancies in the ambulance's location representation. This study proposes
integrating AI predictive models, specifically Support Vector Regression (SVR)
and Deep Neural Networks (DNN), within a constructed mock DT data pipeline
framework to anticipate the medical vehicle's next location in the virtual
world. These models align virtual representations with their physical
counterparts, i.e., metaphorically offsetting the synchronization delay between
the two worlds. Trained meticulously on a historical geospatial dataset, SVR
and DNN exhibit exceptional prediction accuracy in MATLAB and Python
environments. Through various testing scenarios, we visually demonstrate the
efficacy of our methodology, showcasing SVR and DNN's key role in significantly
reducing the witnessed gap within the HITS's DT. This transformative approach
enhances real-time synchronization in emergency HITS by approximately 88% to
93%.; 86) Zero-Shot End-to-End Relation Extraction in Chinese: A Comparative Study
  of Gemini, LLaMA and ChatGPT; This study investigates the performance of various large language models
(LLMs) on zero-shot end-to-end relation extraction (RE) in Chinese, a task that
integrates entity recognition and relation extraction without requiring
annotated data. While LLMs show promise for RE, most prior work focuses on
English or assumes pre-annotated entities, leaving their effectiveness in
Chinese RE largely unexplored. To bridge this gap, we evaluate ChatGPT, Gemini,
and LLaMA based on accuracy, efficiency, and adaptability. ChatGPT demonstrates
the highest overall performance, balancing precision and recall, while Gemini
achieves the fastest inference speed, making it suitable for real-time
applications. LLaMA underperforms in both accuracy and latency, highlighting
the need for further adaptation. Our findings provide insights into the
strengths and limitations of LLMs for zero-shot Chinese RE, shedding light on
trade-offs between accuracy and efficiency. This study serves as a foundation
for future research aimed at improving LLM adaptability to complex linguistic
tasks in Chinese NLP.; 87) The Higgs Self-Coupling at FCC-ee; Single Higgs production at FCC-ee probes the Higgs self-coupling at
next-to-leading order (NLO). Extracting a bound requires a global analysis
accounting for other possible new physics contributions up to NLO. We determine
the FCC-ee sensitivity to Higgs self-coupling modifications
$\delta\kappa_\lambda$ within the Standard Model Effective Field Theory (SMEFT)
framework, including for the first time flavour, LEP, LHC, and FCC-ee
observables in a global analysis with all leading NLO effects via one-loop
renormalisation group evolution, as well as incorporating finite NLO
contributions to electroweak precision and $ZH$ observables. The global
sensitivity to $\delta\kappa_\lambda$ is estimated by marginalising over the
effects of all other operators, bringing flavour considerations to the fore. We
find that, under reasonable assumptions, FCC-ee sensitivity to
$\delta\kappa_\lambda$ can exceed that of the HL-LHC.; 88) AI-based Framework for Robust Model-Based Connector Mating in Robotic
  Wire Harness Installation; Despite the widespread adoption of industrial robots in automotive assembly,
wire harness installation remains a largely manual process, as it requires
precise and flexible manipulation. To address this challenge, we design a novel
AI-based framework that automates cable connector mating by integrating force
control with deep visuotactile learning. Our system optimizes
search-and-insertion strategies using first-order optimization over a
multimodal transformer architecture trained on visual, tactile, and
proprioceptive data. Additionally, we design a novel automated data collection
and optimization pipeline that minimizes the need for machine learning
expertise. The framework optimizes robot programs that run natively on standard
industrial controllers, permitting human experts to audit and certify them.
Experimental validations on a center console assembly task demonstrate
significant improvements in cycle times and robustness compared to conventional
robot programming approaches. Videos are available under
https://claudius-kienle.github.io/AppMuTT.; 89) Solar prosumage under different pricing regimes: Interactions with the
  transmission grid; Solar prosumers, residential electricity consumers equipped with photovoltaic
(PV) systems and battery storage, are transforming electricity markets. Their
interactions with the transmission grid under varying tariff designs are not
yet fully understood. We explore the influence of different pricing regimes on
prosumer investment and dispatch decisions and their subsequent impact on the
transmission grid. Using an integrated modeling approach that combines two
open-source dispatch, investment and grid models, we simulate prosumage
behavior in Germany's electricity market under real-time pricing or
time-invariant pricing, as well as under zonal or nodal pricing. Our findings
show that zonal pricing favors prosumer investments, while time-invariant
pricing rather hinders it. In comparison, regional solar availability emerges
as a larger driver for rooftop PV investments. The impact of prosumer
strategies on grid congestion remains limited within the scope of our
model-setup, in which home batteries cannot be used for energy arbitrage.; 90) Codimension one foliations on adjoint varieties; In this paper, we classify codimension one foliations on adjoint varieties
with most positive anti-canonical class. We show that on adjoint varieties with
Picard number one, these foliations are always induced by a pencil of
hyperplane sections with respect to their minimal embedding. For adjoint
varieties of Picard number two, there is more than one component of such
foliations, and we describe each of them. As a tool for understanding these
foliations, we introduce the concept of the degree of a foliation with respect
to a family of rational curves, which may be of independent interest.; 91) Cataclysmic Variables in Triples: Formation Models and New Discoveries; The formation of cataclysmic variables (CVs) has long been modeled as a
product of common envelope evolution (CEE) in isolated binaries. However, a
significant fraction of intermediate-mass stars -- the progenitors of the white
dwarfs (WDs) in CVs -- are in triples. We therefore investigate the importance
of triple star dynamics in CV formation. Using Gaia astrometry and existing CV
catalogs, we construct a sample of $\sim50$ CVs in hierarchical triples within
1 kpc of the Sun, containing main-sequence (MS) and WD tertiaries at
separations of 100 - 30,000 au. We infer that at least 10% of CVs host wide
tertiaries. To interpret this discovery, we evolve a population of 2000 triples
using detailed three-body simulations, 47 of which become CVs. We predict that
20% of CVs in triples form without ever experiencing CEE, where the WD and
donor are brought together by the eccentric Kozai-Lidov (EKL) mechanism after
the formation of the WD. These systems favor larger donor stars and longer
birth orbital periods (8-20 hrs) than typical CVs. Among systems that do
undergo CEE, about half would not have interacted without the presence of the
tertiary. Triple formation channels both with and without CEE require initially
wide inner orbits ($\gtrsim 1$ au), which in turn require larger tertiary
separations to be stable. Consistent with this prediction, we find that the
observed Gaia CV triples have wider separations on average than normal wide
binaries selected in the same way. Our work underscores the importance of
triples in shaping interacting binary populations including CVs, ultracompact
binaries, and low-mass X-ray binaries.; 92) Robust ab initio predictions for dimensionless ratios of E2 and radius
  observables. II. Estimation of E2 transition strengths by calibration to the
  charge radius; Converged results for E2 observables are notoriously challenging to obtain in
ab initio no-core configuration interaction (NCCI) approaches. Matrix elements
of the E2 operator are sensitive to the large-distance tails of the nuclear
wave function, which converge slowly in an oscillator basis expansion. Similar
convergence challenges beset ab initio prediction of the nuclear charge radius.
However, we exploit systematic correlations between the calculated E2 and
radius observables to yield meaningful predictions for relations among these
observables. In particular, we examine ab initio predictions for dimensionless
ratios of the form B(E2)/(e^2r^4), for nuclei throughout the p shell.
Meaningful predictions for E2 transition strengths may then be made by
calibrating to the ground-state charge radius, if experimentally known.; 93) Utilizing AI and Machine Learning for Predictive Analysis of
  Post-Treatment Cancer Recurrence; In oncology, recurrence after treatment is one of the major challenges,
related to patients' survival and quality of life. Conventionally, prediction
of cancer relapse has always relied on clinical observation with statistical
model support, which almost fails to explain the complex, multifactorial nature
of tumor recurrence. This research explores how AI and ML models may increase
the accuracy and reliability of recurrence prediction in cancer. Therefore, AI
and ML create new opportunities not only for personalized medicine but also for
proactive management of patients through analyzing large volumes of data on
genetics, clinical manifestations, and treatment. The paper describes the
various AI and ML techniques for pattern identification and outcome prediction
in cancer patients using supervised and unsupervised learning. Clinical
implications provide an opportunity to review how early interventions could
happen and the design of treatment planning.; 94) From Screens to Scenes: A Survey of Embodied AI in Healthcare; Healthcare systems worldwide face persistent challenges in efficiency,
accessibility, and personalization. Powered by modern AI technologies such as
multimodal large language models and world models, Embodied AI (EmAI)
represents a transformative frontier, offering enhanced autonomy and the
ability to interact with the physical world to address these challenges. As an
interdisciplinary and rapidly evolving research domain, ""EmAI in healthcare""
spans diverse fields such as algorithms, robotics, and biomedicine. This
complexity underscores the importance of timely reviews and analyses to track
advancements, address challenges, and foster cross-disciplinary collaboration.
In this paper, we provide a comprehensive overview of the ""brain"" of EmAI for
healthcare, wherein we introduce foundational AI algorithms for perception,
actuation, planning, and memory, and focus on presenting the healthcare
applications spanning clinical interventions, daily care & companionship,
infrastructure support, and biomedical research. Despite its promise, the
development of EmAI for healthcare is hindered by critical challenges such as
safety concerns, gaps between simulation platforms and real-world applications,
the absence of standardized benchmarks, and uneven progress across
interdisciplinary domains. We discuss the technical barriers and explore
ethical considerations, offering a forward-looking perspective on the future of
EmAI in healthcare. A hierarchical framework of intelligent levels for EmAI
systems is also introduced to guide further development. By providing
systematic insights, this work aims to inspire innovation and practical
applications, paving the way for a new era of intelligent, patient-centered
healthcare.; 95) Enhancement of Large Eddy Simulation for the prediction of an intake
  flow rig using sequential Data Assimilation; A Data Assimilation (DA) strategy based on an ensemble Kalman filter (EnKF)
is used to enhance the predictive capabilities of scale resolving numerical
tools for the analysis of flows exhibiting cyclic behaviour. More precisely, an
ensemble of numerical runs using Large Eddy Simulation (LES) for the
compressible steady-state flow rig is augmented via the integration of
high-fidelity data. This observation is in the form of instantaneous velocity
measurements, which are sampled at localized sensors in the physical domain.
Two objectives are targeted. The first one is the calibration of an unsteady
inlet condition suitable to capture the cyclic flow investigated. The second
one is the analysis of the synchronization of velocity field predicted by the
LES with the available observation. In order to reduce the computational costs
required for this analysis, a hyper-localization procedure (HLEnKF) is proposed
and it is integrated in the library CONES, tailored to perform fast online DA.
The proposed strategy performs a satisfactory calibration of the inlet
conditions, and its robustness is assessed using two different prior
distributions for the free parameters optimized in this task. DA state
estimation is efficient in obtaining accurate local synchronization of the
inferred velocity fields with the observed data. The modal analysis of the
kinetic energy of the flow field provides additional information on the quality
of the reconstruction of the velocity field, which shows improvements. Thus,
the HLEnKF shows promising features for the calibration and the synchronization
of scale-resolved turbulent flows, opening perspectives of applications for
complex phenomena using advanced tools such as digital twins.; 96) Riemann surface foliations with non-discrete singular set; Let $\mathcal{F}$ be a singular Riemann surface foliation on a complex
manifold $M$, such that the singular set $E \subset M$ is non-discrete. We
study the behavior of the foliation near the singular set $E$, particularly
focusing on singular points that admit invariant submanifolds (locally) passing
through them. Our primary focus is on the singular points that are removable
singularities for some proper subfoliation. We classify singular points based
on the dimension of their invariant submanifold and, consequently, establish
that for hyperbolic foliations $\mathcal{F}$, the presence of such
singularities ensures the continuity of the leafwise Poincar\'{e} metric on $M
\setminus E$.; 97) A Data-driven Investigation of Euphemistic Language: Comparing the usage
  of ""slave"" and ""servant"" in 19th century US newspapers; This study investigates the usage of ""slave"" and ""servant"" in the 19th
century US newspapers using computational methods. While both terms were used
to refer to enslaved African Americans, they were used in distinct ways. In the
Chronicling America corpus, we included possible OCR errors by using FastText
embedding and excluded text reprints to consider text reprint culture in the
19th century. Word2vec embedding was used to find semantically close words to
""slave"" and ""servant"" and log-odds ratio was calculated to identify
over-represented discourse words in the Southern and Northern newspapers. We
found that ""slave"" is associated with socio-economic, legal, and administrative
words, however, ""servant"" is linked to religious words in the Northern
newspapers while Southern newspapers associated ""servant"" with domestic and
familial words. We further found that slave discourse words in Southern
newspapers are more prevalent in Northern newspapers while servant discourse
words from each side are prevalent in their own region. This study contributes
to the understanding of how newspapers created different discourses around
enslaved African Americans in the 19th century US.; 98) A Transformer-Based Framework for Greek Sign Language Production using
  Extended Skeletal Motion Representations; Sign Languages are the primary form of communication for Deaf communities
across the world. To break the communication barriers between the Deaf and
Hard-of-Hearing and the hearing communities, it is imperative to build systems
capable of translating the spoken language into sign language and vice versa.
Building on insights from previous research, we propose a deep learning model
for Sign Language Production (SLP), which to our knowledge is the first attempt
on Greek SLP. We tackle this task by utilizing a transformer-based architecture
that enables the translation from text input to human pose keypoints, and the
opposite. We evaluate the effectiveness of the proposed pipeline on the Greek
SL dataset Elementary23, through a series of comparative analyses and ablation
studies. Our pipeline's components, which include data-driven gloss generation,
training through video to text translation and a scheduling algorithm for
teacher forcing - auto-regressive decoding seem to actively enhance the quality
of produced SL videos.; 99) Temperatures of Robin Hood; Cumulative Games were introduced by Larsson, Meir, and Zick (2020) to bridge
some conceptual and technical gaps between Combinatorial Game Theory (CGT) and
Economic Game Theory. The partizan ruleset {\sc Robin Hood} is an instance of a
Cumulative Game, viz., {\sc Wealth Nim}. It is played on multiple heaps, each
associated with a pair of cumulations, interpreted here as wealth. Each player
chooses one of the heaps, removes tokens from that heap not exceeding their own
wealth, while simultaneously diminishing the other player's wealth by the same
amount. In CGT, the {\em temperature} of a {\em disjunctive sum} game component
is an estimate of the urgency of moving first in that component. It turns out
that most of the positions of {\sc Robin Hood} are {\em hot}. The temperature
of {\sc Robin Hood} on a single large heap shows a dichotomy in behavior
depending on the ratio of the wealths of the players. Interestingly, this
bifurcation is related to Pingala (Fibonacci) sequences and the Golden Ratio
$\phi$: when the ratio of the wealths lies in the interval $(\phi^{-1},\phi)$,
the temperature increases linearly with the heap size, and otherwise it remains
constant, and the mean values has a reciprocal property. It turns out that
despite {\sc Robin Hood} displaying high temperatures, playing in the hottest
component might be a sub-optimal strategy.; 100) Phase-Sensitive Enhanced Absorption, Transmission and Slow Light in a
  Cross-cavity Magnomechanical System; We theoretically propose a scheme to explore the magnetically and
magnomechanically induced transparency phenomena in a cross-cavity
magnomechanical system, focusing on the role of relative phase and the
intensity of the two probing fields in enhancing the absorption and
transmission spectra and manipulating the group delay of the transmitted light.
Interestingly, the relative phase of the two probe fields could have
overwhelming effects on both the absorption spectrum and the group delay of the
output field. Tuning the relative phase and amplitude of the probe fields can
suppress or enhance the absorption and transmission spectra. The combined
effect of the magnon-photon and magnon-phonon couplings, along with relative
phase modulations, helps to switch the probe field's behavior from subluminal
to superluminal in the current system. The current study offers a
straightforward and practical approach, demonstrating the capability to employ
the relative phase for the modulation of microwave signals within the cavity
magnomechanical system, providing insights for the design of information
transduction and quantum sensing.",0.0,0.3562071871080222
2411.0064,applied,2411.0064-pos2-3,"Quantifying Variance in Evaluation Benchmarks; Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models.",2411.0064-pos1-3,"The Llama 3 Herd of Models; Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",41,"['1', '2', '3', '6', '7', '36', '41', '57', '62', '64']","The main paper discusses variance in evaluation benchmarks for large language models (LLMs) and the importance of accurately measuring performance. The first candidate paper, 'Entropic force and bouncing behaviour in kappa-Minkowski space-time,' explores fundamental physics concepts that can potentially guide innovative modeling approaches for understanding the performance metrics discussed in the main paper. This synergy between physics and LLM evaluation could lead to new methods for quantifying performance variance, making it a strong multidisciplinary pairing. The subsequent papers explore various methods and applications but do not align as closely with the framework of LLM performance measurement and variance reduction. The second paper on 'A new proof of superadditivity and of the density conjecture for Activated Random Walks on the line' is mathematically interesting but does not directly apply to the context of LLMs. The rest of the papers follow similarly in their lack of direct relevance to the core themes of the main paper.","1) Entropic force and bouncing behaviour in $\kappa$-Minkowski space-time; We generalise the entropic force description of gravity into
$\kappa$-Minkowski space-time and derive the $\kappa$-deformed corrections to
the Newton's gravitational force. Using this we show the appearance of
logarithmic correction as the first order $\kappa$-deformed correction term to
Bekenstein-Hawking entropy. Further we also derive the $\kappa$-deformed
Friedmann equations and study the evolution of scale factor in
$\kappa$-deformed space-time. Finally we show that the $\kappa$-deformation
parameter avoids the initial singularity in early universe by providing a
bounce behaviour for the case of spatially flat and closed universe.; 2) A new proof of superadditivity and of the density conjecture for
  Activated Random Walks on the line; In two recent works, Hoffman, Johnson and Junge proved the density
conjecture, the hockey stick conjecture and the ball conjecture for Activated
Random Walks in dimension 1, showing an equality between several different
definitions of the critical density of the model. This establishes a kind of
self-organized criticality, that was originally predicted for the Abelian
Sandpile Model. The proof of Hoffman, Johnson and Junge uses a comparison with
a percolation process, which exhibits a superadditivity property. In the
present note, we revisit their argument by providing a new proof of
superadditivity directly for Activated Random Walks, without relying on a
percolation process. The proof relies on a simple comparison between the
stabilization of two neighbouring segments and that of their union. We then
explain how this superaddivity property implies the three mentioned
conjectures. Yet, so far it does not seem that this approach yields as much
information as does the percolation technology developed by Hoffman, Johnson
and Junge, which yields an exponential concentration bound on the stationary
density, whereas the superadditivity property alone only ensures an exponential
bound on the lower tail.; 3) Analysis of harmonic average method for interface problems with
  discontinuous solutions and fluxes; Harmonic average method has been widely utilized to deal with heterogeneous
coefficients in solving differential equations. One remarkable advantage of the
harmonic averaging method is that no derivative of the coefficient is needed.
Furthermore, the coefficient matrix of the finite difference equations is an
M-matrix which guarantees the stability of the algorithm. It has been
numerically observed but not theoretically proved that the method produces
second order pointwise accuracy when the solution and flux are continuous even
if the coefficient has finite discontinuities for which the method is
inconsistent ($O(1)$ in the local truncation errors). It has been believed that
there are some fortunate error cancellations. The harmonic average method does
not converge when the solution or the flux has finite discontinuities. In this
paper, not only we rigorously prove the second order convergence of the
harmonic averaging method for one-dimensional interface problem when the
coefficient has a finite discontinuities and the solution and the flux are
continuous, but also proposed an {\em improved harmonic average method} that is
also second order accurate (in the $L^{\infty}$ norm), which allows
discontinuous solutions and fluxes along with the discontinuous coefficients.
The key in the convergence proof is the construction of the Green's function.
The proof shows how the error cancellations occur in a subtle way. Numerical
experiments in both 1D and 2D confirmed the theoretical proof of the improved
harmonic average method.; 4) Action accessible and weakly action representable varieties of algebras; The main goal of this article is to investigate the relationship between
action accessibility and weak action representability in the context of
varieties of non-associative algebras over a field. Specifically, using an
argument of J. R. A. Gray in the setting of groups, we prove that the varieties
of $k$-nilpotent Lie algebras ($k \geq 3$) and the varieties of $n$-solvable
Lie algebras ($n \geq 2$) do not form weakly action representable categories.
These are the first known examples of action accessible varieties of
non-associative algebras that fail to be weakly action representable,
establishing that a subvariety of a (weakly) action representable variety of
non-associative algebras needs not be weakly action representable. Eventually,
we refine J. R. A. Gray's result by proving that the varieties of $k$-nilpotent
groups ($k \geq 3$) and that of $2$-solvable groups are not weakly action
representable.; 5) Tensor Learning and Compression of N-phonon Interactions; Phonon interactions from lattice anharmonicity govern thermal properties and
heat transport in materials. These interactions are described by n-th order
interatomic force constants (n-IFCs), which can be viewed as high-dimensional
tensors correlating the motion of n atoms, or equivalently encoding n-phonon
scattering processes in momentum space. Here, we introduce a tensor
decomposition to efficiently compress n-IFCs for arbitrary order n. Using
tensor learning, we find optimal low-rank approximations of n-IFCs by solving
the resulting optimization problem. Our approach reveals the inherent low
dimensionality of phonon-phonon interactions and allows compression of the 3
and 4-IFC tensors by factors of up to $10^3-10^4$ while retaining high accuracy
in calculations of phonon scattering rates and thermal conductivity.
Calculations of thermal conductivity using the compressed n-IFCs achieve a
speed-up by nearly three orders of magnitude with >98% accuracy relative to the
reference uncompressed solution. These calculations include both 3- and
4-phonon scattering and are shown for a diverse range of materials (Si, HgTe,
MgO, and TiNiSn). In addition to accelerating state-of-the-art thermal
transport calculations, the method shown here paves the way for modeling
strongly anharmonic materials and higher-order phonon interactions.; 6) Conflicts of Interest in Published NLP Research 2000-2024; Natural Language Processing research is increasingly reliant on large scale
data and computational power. Many achievements in the past decade resulted
from collaborations with the tech industry. But an increasing entanglement of
academic research and industry interests leads to conflicts of interest. We
assessed published NLP research from 2000-2024 and labeled author affiliations
as academic or industry-affiliated to measure conflicts of interest. Overall
27.65% of the papers contained at least one industry-affiliated author. That
figure increased substantially with more than 1 in 3 papers having a conflict
of interest in 2024. We identify top-tier venues (ACL, EMNLP) as main drivers
for that effect. The paper closes with a discussion and a simple, concrete
suggestion for the future.; 7) Enhancing the Accuracy and Comprehensibility in Architectural Tactics
  Detection via Small Model-Augmented Prompt Engineering; Architectural tactics (ATs), as the concrete implementation of architectural
decisions in code, address non-functional requirements of software systems. Due
to the implicit nature of architectural knowledge in code implementation,
developers may risk inadvertently altering or removing these tactics during
code modifications or optimizations. Such unintended changes can trigger
architectural erosion, gradually undermining the system's original design.
While many researchers have proposed machine learning-based methods to improve
the accuracy of detecting ATs in code, the black-box nature and the required
architectural domain knowledge pose significant challenges for developers in
verifying the results. Effective verification requires not only accurate
detection results but also interpretable explanations that enhance their
comprehensibility. However, this is a critical gap in current research. Large
language models (LLMs) can generate easily interpretable ATs detection comments
if they have domain knowledge. Fine-tuning LLMs to acquire domain knowledge
faces challenges such as catastrophic forgetting and hardware constraints.
Thus, we propose Prmt4TD, a small model-augmented prompting framework to
enhance the accuracy and comprehensibility of ATs detection. Combining
fine-tuned small models with In-Context Learning can also reduce fine-tuning
costs while equipping the LLM with additional domain knowledge. Prmt4TD can
leverage the remarkable processing and reasoning capabilities of LLMs to
generate easily interpretable ATs detection results. Our evaluation results
demonstrate that Prmt4TD achieves accuracy (\emph{F1-score}) improvement of
13\%-23\% on the ATs balanced dataset and enhances the comprehensibility of the
detection results.; 8) Negative correlations in Ising models of credit risk; We analyze a subclass of Ising models in the context of credit risk, focusing
on Dandelion models when the correlations $\rho$ between the central node and
each non-central node are negative. We establish the possible range of values
for $\rho$ and derive an explicit formula linking the correlation between any
pair of non-central nodes to $\rho$. The paper concludes with a simulation
study.; 9) Resonant Drag Instabilities for Polydisperse Dust, I. The Acoustic
  Resonant Drag Instability; Dust grains embedded in gas flow give rise to a class of hydrodynamic
instabilities that can occur whenever there exists a relative velocity between
gas and dust. These instabilities have predominantly been studied for single
grain sizes, for which a strong interaction can be found between drifting dust
and a travelling gas wave, leading to fast-growing perturbations (growth rates
$\propto \sqrt{\mu}$) even at small dust-to-gas ratios $\mu$. They are called
resonant drag instabilities. We focus on the acoustic resonant drag
instability, which is potentially important in AGB star outflows, around
supernova remnants and star clusters in starburst galaxies. We study the
acoustic resonant drag instability, taking into account a continuous spectrum
of grain sizes, to determine whether it survives in the polydisperse regime and
how the resulting growth rates compare to the monodisperse case. We solve the
linear equations for a polydisperse fluid for the acoustic drag instability,
focusing on small dust-to-gas ratios. Size distributions of realistic width
turn the fast-growing perturbations $\propto \sqrt{\mu}$ of the monodisperse
limit into slower growing perturbations $\propto \mu$ due to the fact that the
backreaction on the gas involves an integration over the resonance.
Furthermore, the large wave numbers that grow fastest in the monodisperse
regime are stabilized by a size distribution, severely limiting the growth
rates in the polydisperse regime. The acoustic resonant drag instability turns
from a singularly perturbed problem in $\mu$ in the monodisperse limit into a
regular perturbation for a sufficiently wide size distribution. It can still
grow exponentially in the polydisperse regime, but at a slower pace compared to
the single size case.; 10) Geometric properties versus particle motion in the Fang-Wang spacetime; In this work, we explore general relativistic effects and geometric
properties of the Fan-Wang spacetime, one of the simplest regular solutions
that can be obtained in nonlinear electrodynamics. In particular, we
investigate the motion of test particles, the capture cross-section of neutral
massive and massless particles, such as neutrinos and photons, and the
gravitational redshift. Additionally, using a perturbative approach, we derive
analytical expressions for the perihelion shift and gravitational deflection of
massless particles. By identifying the one-parameter corrections to the
Schwarzschild spacetime, induced by the magnetic charge contained in the
Fang-Wang metric, we show that this spacetime can be falsified, since it
modifies classical general relativity predictions even at the local level.
Moreover, we argue that these modifications could be experimentally tested with
advanced observational instrumentation.; 11) Variational Tail Bounds for Norms of Random Vectors and Matrices; We propose a variational tail bound for norms of random vectors under moment
assumptions on their one-dimensional marginals. We also propose a simplified
version of the bound that parametrizes the ``aggregating'' distribution in the
proposed variational bound by considering a certain pushforward of the Gaussian
distribution. Furthermore, we show that the proposed method recovers some of
the well-known bounds on norms of Gaussian random vectors, as well as a recent
concentration inequality for the spectral norm of sum of independent and
identically distributed positive semidefinite matrices.; 12) Dirac Operators on Orbifold Resolutions: Uniform Elliptic Theory; Dirac operators on Riemannian spaces play a central role in various branches
of mathematics, encoding rich geometric and topological data. They appear as
deformation operators in moduli problems, including those associated with
special holonomy metrics, gauge theory instantons, and calibrated submanifolds.
This paper investigates the behaviour of families of Dirac operators as the
underlying Riemannian spaces degenerate to a Riemannian orbifold in the so
called adiabatic limit. Specifically, we focus on relating the kernels and
cokernels of the family of Dirac operators to adiabatic data and establish
uniform bounds on their right inverse. These results provide a crucial analytic
foundation for gluing problems on orbifold resolutions, without relying on the
intricate iterated edge calculus developed by Mazzeo, Melrose, Schulze and
others. This work paves the way for forthcoming studies, where these techniques
will be used to address open problems in special holonomy geometry, for example
the construction of compact $\mathrm{G}_2$- and $\mathrm{Spin}(7)$-manifolds,
gauge theory, and calibrated geometry. The methods developed here have broader
implications for the study of singular Riemannian spaces and their analytic
properties.; 13) Superheavy Supersymmetric Dark Matter for the origin of KM3NeT
  Ultra-High Energy signal; We propose an explanation for the recently reported ultra-high-energy
neutrino signal at KM3NeT, which lacks an identifiable astrophysical source.
While decaying dark matter in the Galactic Center is a natural candidate, the
observed arrival direction strongly suggests an extragalactic origin. We
introduce a multicomponent dark matter scenario in which the components are
part of a supermultiplet, with supersymmetry ensuring a nearly degenerate mass
spectrum among the fields. This setup allows a heavy component to decay into a
lighter one, producing a boosted neutrino spectrum with energy $E_\nu \sim 100$
PeV, determined by the mass difference. The heavy-to-light decay occurs at a
cosmological redshift of $z \sim \text{a few}$ or higher, leading to an
isotropic directional distribution of the signal.; 14) Flavor Constraints in a Generational Three Higgs Doublet Model; We propose a Three Higgs Doublet Model (3HDM) that goes beyond natural flavor
conservation and in which each of the three Higgs doublets couples mainly to a
single generation of fermions via non-standard Yukawa structures. A hierarchy
in the vacuum expectation values of the three Higgs doublets can partially
address the SM flavor puzzle. In light of the experimentally observed $125$ GeV
Higgs boson, we primarily work within a 3HDM alignment limit such that a
Standard Model-like Higgs is recovered. In order to reproduce the observed CKM
mixing among quarks, the neutral Higgs bosons of the theory necessarily mediate
flavor changing neutral currents at the tree level. We consider constraints
from neutral kaon, $B$ meson, and $D$ meson mixing as well as from the rare
leptonic decays $B_s/B^0/K_L\rightarrow\mu^+\mu^-/e^+e^-$. We identify regions
of parameter space in which the new physics Higgs bosons can be as light as a
TeV or even lighter.; 15) Event Constrained Programming; In this paper, we present event constraints as a new modeling paradigm that
generalizes joint chance constraints from stochastic optimization to (1)
enforce a constraint on the probability of satisfying a set of constraints
aggregated via application-specific logic (constituting an event) and (2) to be
applied to general infinite-dimensional optimization (InfiniteOpt) problems
(i.e., time, space, and/or uncertainty domains). This new constraint class
offers significant modeling flexibility in posing InfiniteOpt constraints that
are enforced over a certain portion of their domain (e.g., to a certain
probability level), but can be challenging to reformulate/solve due to
difficulties in representing arbitrary logical conditions and specifying a
probabilistic measure on a collection of constraints. To address these
challenges, we derive a generalized disjunctive programming (GDP)
representation of event constrained optimization problems, which readily
enables us to pose logical event conditions in a standard form and allows us to
draw from a suite of GDP solution strategies that leverage the special
structure of this problem class. We also extend several approximation
techniques from the chance constraint literature to provide a means to
reformulate certain event constraints without the use of binary variables. We
illustrate these findings with case studies in stochastic optimal power flow,
dynamic disease control, and optimal 2D diffusion.; 16) Asymptotic solutions of the boundary value problems for the singularly
  perturbed differential algebraic equations with a turning point; This paper deals with the boundary value problems for the singularly
perturbed differential-algebraic system of equations. The case of turning
points has been studied. The sufficient conditions for existence and uniqueness
of the solution of the boundary value problems for DAEs have been found. The
technique of constructing the asymptotic solutions has been developed; 17) Exploration of VLMs for Driver Monitoring Systems Applications; In recent years, we have witnessed significant progress in emerging deep
learning models, particularly Large Language Models (LLMs) and Vision-Language
Models (VLMs). These models have demonstrated promising results, indicating a
new era of Artificial Intelligence (AI) that surpasses previous methodologies.
Their extensive knowledge and zero-shot capabilities suggest a paradigm shift
in developing deep learning solutions, moving from data capturing and algorithm
training to just writing appropriate prompts. While the application of these
technologies has been explored across various industries, including automotive,
there is a notable gap in the scientific literature regarding their use in
Driver Monitoring Systems (DMS). This paper presents our initial approach to
implementing VLMs in this domain, utilising the Driver Monitoring Dataset to
evaluate their performance and discussing their advantages and challenges when
implemented in real-world scenarios.; 18) Video Summarisation with Incident and Context Information using
  Generative AI; The proliferation of video content production has led to vast amounts of
data, posing substantial challenges in terms of analysis efficiency and
resource utilization. Addressing this issue calls for the development of robust
video analysis tools. This paper proposes a novel approach leveraging
Generative Artificial Intelligence (GenAI) to facilitate streamlined video
analysis. Our tool aims to deliver tailored textual summaries of user-defined
queries, offering a focused insight amidst extensive video datasets. Unlike
conventional frameworks that offer generic summaries or limited action
recognition, our method harnesses the power of GenAI to distil relevant
information, enhancing analysis precision and efficiency. Employing YOLO-V8 for
object detection and Gemini for comprehensive video and text analysis, our
solution achieves heightened contextual accuracy. By combining YOLO with
Gemini, our approach furnishes textual summaries extracted from extensive CCTV
footage, enabling users to swiftly navigate and verify pertinent events without
the need for exhaustive manual review. The quantitative evaluation revealed a
similarity of 72.8%, while the qualitative assessment rated an accuracy of 85%,
demonstrating the capability of the proposed method.; 19) COSINT-Agent: A Knowledge-Driven Multimodal Agent for Chinese Open
  Source Intelligence; Open Source Intelligence (OSINT) requires the integration and reasoning of
diverse multimodal data, presenting significant challenges in deriving
actionable insights. Traditional approaches, including multimodal large
language models (MLLMs), often struggle to infer complex contextual
relationships or deliver comprehensive intelligence from unstructured data
sources. In this paper, we introduce COSINT-Agent, a knowledge-driven
multimodal agent tailored to address the challenges of OSINT in the Chinese
domain. COSINT-Agent seamlessly integrates the perceptual capabilities of
fine-tuned MLLMs with the structured reasoning power of the Entity-Event-Scene
Knowledge Graph (EES-KG). Central to COSINT-Agent is the innovative EES-Match
framework, which bridges COSINT-MLLM and EES-KG, enabling systematic
extraction, reasoning, and contextualization of multimodal insights. This
integration facilitates precise entity recognition, event interpretation, and
context retrieval, effectively transforming raw multimodal data into actionable
intelligence. Extensive experiments validate the superior performance of
COSINT-Agent across core OSINT tasks, including entity recognition, EES
generation, and context matching. These results underscore its potential as a
robust and scalable solution for advancing automated multimodal reasoning and
enhancing the effectiveness of OSINT methodologies.; 20) Evolving the Computational Notebook: A Two-Dimensional Canvas for
  Enhanced Human-AI Interaction; Computational notebooks, while essential for data science, are limited by
their one-dimensional interface, which poorly aligns with non-linear developer
workflows and complicates collaboration and human-AI interaction. In this work,
we focus on features of Computational Canvas, a novel two-dimensional interface
that evolves notebooks to enhance data analysis and AI-assisted development
within integrated development environments (IDEs). We present vital features,
including freely arrangeable code cells, separate environments, and improved
output management. These features are designed to facilitate intuitive
organization, visual exploration, and natural collaboration with other users
and AI agents. We also show the implementation of Computational Canvas with
designed features as a Visual Studio Code plugin. By shifting from linear to
two-dimensional spatial interfaces, we aim to significantly boost developers'
productivity in data exploration, experimentation, and AI-assisted development,
addressing the current limitations of traditional notebooks and fostering more
flexible, collaborative data science workflows.; 21) Analysis of intramolecular modes of liquid water in two-dimensional
  spectroscopy: a classical hierarchical equations of motion approach; Two-dimensional (2D) vibrational spectroscopy is a powerful means of
investigating the structure and dynamics of complex molecules in condensed
phases. However, even in theory, analysis of 2D spectra resulting from complex
inter- and intra-molecular motions using only molecular dynamics methods is not
easy. This is because molecular motions comprise complex multiple modes, and
peaks broaden and overlap owing to various relaxation processes and
inhomogeneous broadening. On the basis of an anharmonic multimode Brownian
oscillator model with nonlinear system-bath coupling, we have developed an
approach that simulates 2D spectra, taking into account arbitrary modes of
intermolecular and intramolecular vibrations simultaneously. Although only
two-mode quantum calculations are feasible with this model, owing to high
computational costs, here we restrict ourselves to the classical case and
perform three-mode calculations. We demonstrate the applicability of our method
by calculating 2D correlation infrared spectra of water for symmetric
stretching, antisymmetric stretching, and bending modes. The quantum effects of
these results are deduced by comparing 2D quantum spectra previously obtained
for two intramolecular modes with those obtained using our classical approach
under the same physical conditions. The results show that the 2D spectra
calculated by separating the stretching modes into symmetric and asymmetric
modes provide better descriptions of peak profiles, such as the splitting of
cross-peaks.; 22) Revisiting Continuous p-Hub Location Problems with the L1 Metric; Motivated by emerging urban applications in commercial, public sector, and
humanitarian logistics, we revisit continuous $p$-hub location problems in
which several facilities must be located in a continuous space such that the
expected minimum Manhattan travel distance from a random service provider to a
random customer through exactly one hub facility is minimized. In this paper,
we begin by deriving closed-form results for a one-dimensional case and
two-dimensional cases with up to two hubs. Subsequently, a simulation-based
approximation method is proposed for more complex two-dimensional scenarios
with more than two hubs. Moreover, an extended problem with multiple service
providers is analyzed to reflect real-life service settings. Finally, we apply
our model and approximation method using publicly available data as a case
study to optimize the deployment of public-access automated external
defibrillators in Virginia Beach.; 23) Dirac fermions under imaginary rotation; In the present study, we investigate the properties of an ensemble of free
Dirac fermions, at finite inverse temperature $\beta$ and finite chemical
potential $\mu$, undergoing rigid rotation with an imaginary angular velocity
$\Omega_I$. Our purpose is to establish the analytical structure of such
states, as well as the prospects (and dangers) of extrapolating results
obtained under imaginary rotation to the case of real rotation. We show that in
the thermodynamic limit, the state of the system is akin to a stationary system
with modified inverse temperature $\beta_q = q\beta$ and the same chemical
potential, where $q$ is the denominator of the irreducible fraction $\nu =
\beta \Omega_I / 2\pi = p/q$. The temperature of the system becomes a fractal
function of the rotation parameter, as in the case of the scalar field. The
chemical potential breaks the fractalization of fermions. We also compute the
thermodynamic potential $\Phi$ and associated thermodynamic functions, showing
that they also exhibit fractal behavior. Finally, we evaluate the axial and
helical flux through the transverse plane, generated through the vortical
effects, and show that they diverge in the thermodynamic limit, in the case
when $\nu = 1/q$ and $q \to \infty$.; 24) Will Systems of LLM Agents Cooperate: An Investigation into a Social
  Dilemma; As autonomous agents become more prevalent, understanding their collective
behaviour in strategic interactions is crucial. This study investigates the
emergent cooperative tendencies of systems of Large Language Model (LLM) agents
in a social dilemma. Unlike previous research where LLMs output individual
actions, we prompt state-of-the-art LLMs to generate complete strategies for
iterated Prisoner's Dilemma. Using evolutionary game theory, we simulate
populations of agents with different strategic dispositions (aggressive,
cooperative, or neutral) and observe their evolutionary dynamics. Our findings
reveal that different LLMs exhibit distinct biases affecting the relative
success of aggressive versus cooperative strategies. This research provides
insights into the potential long-term behaviour of systems of deployed
LLM-based autonomous agents and highlights the importance of carefully
considering the strategic environments in which they operate.; 25) Lower bounds for Ramsey numbers of bounded degree hypergraphs; We prove that, for all $k \ge 3,$ and any integers $\Delta, n$ with $n \ge
\Delta,$ there exists a $k$-uniform hypergraph on $n$ vertices with maximum
degree at most $\Delta$ whose $4$-color Ramsey number is at least
$\mathrm{tw}_k(c_k \sqrt{\Delta}) \cdot n$, for some constant $c_k > 0$, where
$\mathrm{tw}_k$ denotes the tower function. This is tight up to the power of
$\Delta$ on top of the tower and extends a result of Graham, R\""{o}dl and
Ruci\'{n}ski for graphs.; 26) ExposNet: A Deep Learning Framework for EMF Exposure Prediction in
  Complex Urban Environments; The prediction of the electric field (E-field) plays a crucial role in
monitoring radiofrequency electromagnetic field (RF-EMF) exposure induced by
cellular networks. In this paper, a deep learning framework is proposed to
predict E-field levels in complex urban environments. First, the measurement
campaign and publicly accessible databases used to construct the training
dataset are introduced, with a detailed explanation provided on how these
datasets are formulated and integrated to enhance their suitability for
Convolutional Neural Networks (CNNs)-based models. Then, the proposed model,
ExposNet, is presented, and its network architecture and workflow are
thoroughly explained. Two variations of the network structure are proposed, and
extensive experimental analyses are conducted, demonstrating that ExposNet
achieves good prediction accuracy with both configurations. Furthermore, the
generalization capability of the model is evaluated. The overall results
indicate that, despite being trained and tested on real-world measurements, the
model performs well and achieves better accuracy compared to previous studies.; 27) Real-time edge dynamics of non-Hermitian lattices; We derive the asymptotic forms of the Green's function at the open edges of
general non-Hermitian band systems in all dimensions in the long-time limit,
using a modified saddle-point approximation and the analytic continuation of
the momentum. The edge dynamics is determined by the ""dominant saddle point"", a
complex momentum, which, contrary to previous conjectures, may lie outside the
generalized Brillouin zone. From this result, we obtain the effective edge
Hamiltonians that evidently, as demonstrated by extensive numerical
simulations, characterize the dynamics on the edges, and can be probed in
real-time experiments or spectroscopies.; 28) Boundary behaviour of the Fefferman--Szeg\""o metric in strictly
  pseudoconvex domains; We study the boundary behaviour of the Fefferman--Szeg\""o metric and several
associated invariants in a $C^\infty$-smoothly bounded strictly pseudoconvex
domain.; 29) Nonadiabatic quantum kinetic equations and Dirac-Heisenberg-Wigner
  formalism for Schwinger pair production in time-varying electric fields with
  multiple components; The nonadiabatic quantum kinetic equations and Dirac-Heisenberg-Wigner
formalism for Schwinger pair production in a spatially uniform and time-varying
electric field with multiple components are derived and proven to be
equivalent. The relation between nonadiabatic and adiabatic quantum kinetic
equations is also established. By analyzing the time evolution of the
distribution functions of particles created in a circularly polarized Gaussian
pulse field with a subcycle structure, it is found that the nonadiabatic and
adiabatic distribution functions are the same after the field, with a
sufficient number of oscillation cycles, fades away. However, during the
presence of the field, the two distribution functions typically differ.
Nonetheless, the time evolution characteristics of the nonadiabatic and
adiabatic momentum distributions are similar. For instance, the number of
spirals is one less than the number of photons absorbed in both cases.
Furthermore, for a rapidly oscillating electric field, the nonadiabatic quantum
kinetic approaches may provide a more meaningful description of pair production
at intermediate times. These findings deepen our understanding of the
nonadiabatic quantum kinetic approaches and their application in pair
production.; 30) A distribution related to Farey sequences -- I; We study some arithmetical properties of Farey sequences by the method
introduced by F.Boca, C.Cobeli and A.Zaharescu (2001). Let $\Phi_{Q}$ be the
classical Farey sequence of order $Q$. Having the fixed integers $D\geqslant 2$
and $0\leqslant c\leqslant D-1$, we colour to the red the fractions in
$\Phi_{Q}$ with denominators $\equiv c \pmod D$. Consider the gaps in
$\Phi_{Q}$ with coloured endpoints, that do not contain the fractions $a/q$
with $q\equiv c \pmod D$ inside. The question is to find the limit proportions
$\nu(r;D,c)$ (as $Q\to +\infty$) of such gaps with precisely $r$ fractions
inside in the whole set of the gaps under considering ($r = 0,1,2,3,\ldots$).
In fact, the expression for this proportion can be derived from the general
result obtained by C.Cobeli, M.V\^{a}j\^{a}itu and A.Zaharescu (2014). However,
such formula expresses $\nu(r;D,c)$ in the terms of areas of some polygons
related to a special geometrical transform. In the present paper, we obtain an
explicit formulas for $\nu(r;D,c)$ for the cases $D = 2, 3$ and $c=0$.; 31) Forecasting Open-Weight AI Model Growth on HuggingFace; As the open-weight AI landscape continues to proliferate-with model
development, significant investment, and user interest-it becomes increasingly
important to predict which models will ultimately drive innovation and shape AI
ecosystems. Building on parallels with citation dynamics in scientific
literature, we propose a framework to quantify how an open-weight model's
influence evolves. Specifically, we adapt the model introduced by Wang et al.
for scientific citations, using three key parameters-immediacy, longevity, and
relative fitness-to track the cumulative number of fine-tuned models of an
open-weight model. Our findings reveal that this citation-style approach can
effectively capture the diverse trajectories of open-weight model adoption,
with most models fitting well and outliers indicating unique patterns or abrupt
jumps in usage.; 32) Using curved meshes to derive a priori error estimates for a linear
  elasticity problem with Robin boundary conditions; This work concerns the numerical analysis of the linear elasticity problem
with a Robin boundary condition on a smooth domain. A finite element
discretization is presented using high-order curved meshes in order to
accurately discretize the physical domain. The primary objective is to conduct
a detailed error analysis for the elasticity problem using the vector lift
operator, which maps vector-valued functions from the mesh domain to the
physical domain. Error estimates are established, both in terms of the finite
element approximation error and the geometric error, respectively associated to
the finite element degree and to the mesh order. These theoretical a priori
error estimates are validated by numerical experiments in 2D and 3D.; 33) Target Tracking using Robust Sensor Motion Control; We consider the problem of tracking moving targets using mobile wireless
sensors (of possibly different types). This is a joint estimation and control
problem in which a tracking system must take into account both target and
sensor dynamics. We make minimal assumptions about the target dynamics, namely
only that their accelerations are bounded. We develop a control law that
determines the sensor motion control signals so as to maximize target
resolvability as the target dynamics evolve. The method is given a tractable
formulation that is amenable to an efficient search method and is evaluated in
a series of experiments involving both round-trip time based ranging and
Doppler frequency shift measurements; 34) Automated Market Makers: Toward More Profitable Liquidity Provisioning
  Strategies; To trade tokens in cryptoeconomic systems, automated market makers (AMMs)
typically rely on liquidity providers (LPs) that deposit tokens in exchange for
rewards. To profit from such rewards, LPs must use effective liquidity
provisioning strategies. However, LPs lack guidance for developing such
strategies, which often leads them to financial losses. We developed a
measurement model based on impermanent loss to analyze the influences of key
parameters (i.e., liquidity pool type, position duration, position range size,
and position size) of liquidity provisioning strategies on LPs' returns. To
reveal the influences of those key parameters on LPs' profits, we used the
measurement model to analyze 700 days of historical liquidity provision data of
Uniswap v3. By uncovering the influences of key parameters of liquidity
provisioning strategies on profitability, this work supports LPs in developing
more profitable strategies.; 35) Incoherent horizontal emittance growth due to the interplay of beam-beam
  and longitudinal wakefield in crab-waist colliders; In this paper, we investigate quadrupolar sychrobetatron resonances caused by
beam-beam collisions and their interplay with longitudinal wakefields in the
context of crab-waist colliders. We present a comprehensive theoretical review
of the established theory of sychrobetatron resonances and extend the formalism
to explore horizontal sychrobetatron resonances specific to crab-waist
colliders. As a case study, we examine incoherent horizontal emittance growth
at the SuperKEKB and demonstrate through simulations that the interplay between
beam-beam and longitudinal wakefields leads to a horizontal blowup of the bunch
size and that the study of the dynamics can be reduced to the
horizontal-longitudinal plane, independent of the motion in the vertical
dimension. We present extensive simulation results using the codes BBWS,
PyHEADTAIL and Xsuite, connect our analytical findings with these findings, and
propose strategies to mitigate horizontal blowup.; 36) MAP: Evaluation and Multi-Agent Enhancement of Large Language Models for
  Inpatient Pathways; Inpatient pathways demand complex clinical decision-making based on
comprehensive patient information, posing critical challenges for clinicians.
Despite advancements in large language models (LLMs) in medical applications,
limited research focused on artificial intelligence (AI) inpatient pathways
systems, due to the lack of large-scale inpatient datasets. Moreover, existing
medical benchmarks typically concentrated on medical question-answering and
examinations, ignoring the multifaceted nature of clinical decision-making in
inpatient settings. To address these gaps, we first developed the Inpatient
Pathway Decision Support (IPDS) benchmark from the MIMIC-IV database,
encompassing 51,274 cases across nine triage departments and 17 major disease
categories alongside 16 standardized treatment options. Then, we proposed the
Multi-Agent Inpatient Pathways (MAP) framework to accomplish inpatient pathways
with three clinical agents, including a triage agent managing the patient
admission, a diagnosis agent serving as the primary decision maker at the
department, and a treatment agent providing treatment plans. Additionally, our
MAP framework includes a chief agent overseeing the inpatient pathways to guide
and promote these three clinician agents. Extensive experiments showed our MAP
improved the diagnosis accuracy by 25.10% compared to the state-of-the-art LLM
HuatuoGPT2-13B. It is worth noting that our MAP demonstrated significant
clinical compliance, outperforming three board-certified clinicians by 10%-12%,
establishing a foundation for inpatient pathways systems.; 37) Experimental observation of Dirac exceptional point; The energy level degeneracies, also known as exceptional points (EPs), are
crucial for comprehending emerging phenomena in materials and enabling
innovative functionalities for devices. Since EPs were proposed over half a
century age, only two types of EPs have been experimentally discovered,
revealing intriguing phases of materials such as Dirac and Weyl semimetals.
These discoveries have showcased numerous exotic topological properties and
novel applications, such as unidirectional energy transfer. Here we report the
observation of a novel type of EP, named the Dirac EP, utilizing a
nitrogen-vacancy center in diamond. Two of the eigenvalues are measured to be
degenerate at the Dirac EP and remain real in its vicinity. This exotic band
topology associated with the Dirac EP enables the preservation of the symmetry
when passing through, and makes it possible to achieve adiabatic evolution in
non-Hermitian systems. We examined the degeneracy between the two eigenstates
by quantum state tomography, confirming that the degenerate point is a Dirac EP
rather than a Hermitian degeneracy. Our research of the distinct type of EP
contributes a fresh perspective on dynamics in non-Hermitian systems and is
potentially valuable for applications in quantum control in non-Hermitian
systems and the study of the topological properties of EP.; 38) Martian atmospheric disturbances from orbital images and surface
  pressure at Jezero Crater, Mars, during Martian Year 36; We present a study of atmospheric disturbances at Jezero Crater, Mars, using
ground-based measurements of surface pressure by the Perseverance rover in
combination with orbital images from the Mars Express and Mars Reconnaissance
Orbiter missions. The study starts at Ls $\sim$ 13.3{\deg} in MY36 (March 6th,
2021) and extends up to Ls $\sim$ 30.3{\deg} in MY37 (February 28th, 2023). We
focus on the characterization of the major atmospheric phenomena at synoptic
and planetary-scales. These are the thermal tides (measured up to the sixth
component), long-period pressure oscillations (periods > 1 sol), the Aphelion
Cloud Belt, and the occasional development of regional dust storms over Jezero.
We present the seasonal evolution of the amplitudes and phases of the thermal
tides and their relation with the atmospheric dust content (optical depth).
Three regional dust storms and one polar storm extending over Jezero produced
an increase in the diurnal and semidiurnal amplitudes but resulted in inverse
responses in their phases. We show that the primary regular wave activity is
due to baroclinic disturbances with periods of 2-4 sols and amplitudes $\sim$
1-15 Pa increasing with dust content, in good agreement with theoretical
predictions by model calculations. The spacecraft images show a number of
arc-shaped, spiral and irregular cyclonic vortices, traced by dust and clouds
at the edge of the North Polar Cap, that could be behind some of the pressure
oscillations measured at Jezero.; 39) Actuation mechanisms in twisted and coiled polymer actuators using
  finite element model; Twisted and coiled polymer actuators (TCPAs) offer the advantages of large
stroke and large specific work as compared to other actuators. There have been
extensive experimental investigations towards understanding their actuation
response, however, a computational model with full material description is not
utilized to probe into the underlying mechanisms responsible for their large
actuation. In this work, we develop a three-dimensional finite element model
that includes the physics of the fabrication process to simulate the actuation
of TCPA under various loading and boundary conditions. The model is validated
against the experimental data and used to explore the factors responsible for
actuation under free and isobaric conditions. The model captures the physics of
the angle of twist in the fiber and the distinction between the homochiral and
heterochiral nature of TCPA actuation response. The simulations show that the
anisotropy in the thermal expansion coefficient (CTE) matrix plays a major role
in large actuation irrespective of the anisotropy or isotropy in the elasticity
tensor. We further investigate the extent of anisotropy in thermal expansion
and the parametric studies show that the key for TCPA actuation is the absolute
value of mismatch in thermal expansion even if the material has positive or
negative CTE in both directions of the fiber. Furthermore, we propose a new
shell-core composite-based TCPA concept by combining the epoxy and hollow Nylon
tubes to suppress the creep in TCPA. The results show that the volume fraction
of epoxy-core can be tuned to attain a desired actuation while offering a
stiffer and creep-resistant response. This framework provides a wider
application for probing various kinds of TCPAs and enhancing their actuation
performance.; 40) A reduction theorem for non-vanishing of Hochschild cohomology of block
  algebras and Happel's property; We show that any $p$-block algebra ($p$ is a prime) of a finite group with
realizable fusion system satisfies Happel's property. We obtain a reduction
theorem for the non-vanishing of the first Hochschild cohomology of block
algebras with non-trivial defect groups. Along the way we investigate this
problem for the blocks of some simple finite groups.; 41) The Llama 3 Herd of Models; Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.; 42) Genome evolution in an endangered freshwater mussel; Nearly neutral theory predicts that evolutionary processes will differ in
small populations compared to large populations, a key point of concern for
endangered species. The nearly-neutral threshold, the span of neutral
variation, and the adaptive potential from new mutations all differ depending
on N_e. To determine how genomes respond in small populations, we have created
a reference genome for a US federally endangered IUCN Red List freshwater
mussel, Elliptio spinosa, and compare it to genetic variation for a common and
successful relative, Elliptio crassidens. We find higher rates of background
duplication rates in E. spinosa consistent with proposed theories of duplicate
gene accumulation according to nearly-neutral processes. Along with these
changes we observe fewer cases of adaptive gene family amplification in this
endangered species. However, TE content is not consistent with nearly-neutral
theory. We observe substantially less recent TE proliferation in the endangered
species with over 500 Mb of newly copied TEs in Elliptio crassidens. These
results suggest a more complex interplay between TEs and duplicate genes than
previously proposed for small populations. They further suggest that TEs and
duplications require greater attention in surveys of genomic health for
endangered species.; 43) A Bregman ADMM for Bethe variational problem; In this work, we propose a novel Bregman ADMM with nonlinear dual update to
solve the Bethe variational problem (BVP), a key optimization formulation in
graphical models and statistical physics. Our algorithm provides rigorous
convergence guarantees, even if the objective function of BVP is non-convex and
non-Lipschitz continuous on the boundary. A central result of our analysis is
proving that the entries in local minima of BVP are strictly positive,
effectively resolving non-smoothness issues caused by zero entries. Beyond
theoretical guarantees, the algorithm possesses high level of separability and
parallelizability to achieve highly efficient subproblem computation. Our
Bregman ADMM can be easily extended to solve the quantum Bethe variational
problem. Numerical experiments are conducted to validate the effectiveness and
robustness of the proposed method. Based on this research, we have released an
open-source package of the proposed method at
https://github.com/TTYmath/BADMM-BVP.; 44) Modeling Changes in Individuals' Cognitive Self-Esteem With and Without
  Access To Search Tools; Search engines, as cognitive partners, reshape how individuals evaluate their
cognitive abilities. This study examines how search tool access influences
cognitive self-esteem (CSE)-users' self-perception of cognitive abilities --
through the lens of transactive memory systems. Using a within-subject design
with 164 participants, we found that CSE significantly inflates when users have
access to search tools, driven by cognitive offloading. Participants with lower
initial CSE exhibited greater shifts, highlighting individual differences.
Search self-efficacy mediated the relationship between prior search experience
and CSE, emphasizing the role of users' past interactions. These findings
reveal opportunities for search engine design: interfaces that promote
awareness of cognitive offloading and foster self-reflection can support
accurate metacognitive evaluations, reducing overreliance on external tools.
This research contributes to HCI by demonstrating how interactive systems shape
cognitive self-perception, offering actionable insights for designing
human-centered tools that balance user confidence and cognitive independence.; 45) Pseudo-spectra of multivariate inhomogeneous spatial point processes; In this article, we propose a spectral method for multivariate inhomogeneous
spatial point processes. A key ingredient is utilizing the asymptotic behavior
of the periodogram. The periodogram is an asymptotically unbiased estimator of
the spectrum of a second-order stationary point process. By extending this
property, we show that under inhomogeneity, the expectation of the periodogram
also converges to a matrix-valued function, which we refer to as the
pseudo-spectrum. The pseudo-spectrum shares similar properties with the
spectrum of stationary processes and can be interpreted using local parameters.
We derive a consistent estimator of the pseudo-spectrum through kernel
smoothing and propose two bandwidth selection methods. The performance and
utility of our frequency domain methods are illustrated through simulation
studies and a real data analysis of rainforest data.; 46) Energy-Efficient Flat Precoding for MIMO Systems; This paper addresses the suboptimal energy efficiency of conventional digital
precoding schemes in multiple-input multiple-output (MIMO) systems. Through an
analysis of the power amplifier (PA) output power distribution associated with
conventional precoders, it is observed that these power distributions can be
quite uneven, resulting in large PA backoff (thus low efficiency) and high
power consumption. To tackle this issue, we propose a novel approach called
flat precoding, which aims to control the flatness of the power distribution
within a desired interval. In addition to reducing PA power consumption, flat
precoding offers the advantage of requiring smaller saturation levels for PAs,
which reduces the size of PAs and lowers the cost. To incorporate the concept
of flat power distribution into precoding design, we introduce a new
lower-bound per-antenna power constraint alongside the conventional sum power
constraint and the upper-bound per-antenna power constraint. By adjusting the
lower-bound and upper-bound values, we can effectively control the level of
flatness in the power distribution. We then seek to find a flat precoder that
satisfies these three sets of constraints while maximizing the weighted sum
rate (WSR). In particular, we develop efficient algorithms to design weighted
minimum mean squared error (WMMSE) and zero-forcing (ZF)-type precoders with
controllable flatness features that maximize WSR. Numerical results demonstrate
that complete flat precoding approaches, where the power distribution is a
straight line, achieve the best trade-off between spectral efficiency and
energy efficiency for existing PA technologies. We also show that the proposed
ZF and WMMSE precoding methods can approach the performance of their
conventional counterparts with only the sum power constraint, while
significantly reducing PA size and power consumption.; 47) The Gordon-Litherland pairing and its many applications; Gordon and Litherland's paper $\textit{On the Signature of a link}$
introduced a bilinear form that simultaneously unifies both the quadratic forms
of Trotter and Goeritz. This remarkable pairing of combinatorics and topology
has had widespread application in low-dimensional topology. In this expository
note, we give a picture proof (via Kirby diagrams) of their main result and
discuss the numerous ways their theorem has been put to good use.; 48) Economic Censorship Games in Fraud Proofs; Optimistic rollups rely on fraud proofs -- interactive protocols executed on
Ethereum to resolve conflicting claims about the rollup's state -- to scale
Ethereum securely.
  To mitigate against potential censorship of protocol moves, fraud proofs
grant participants a significant time window, known as the challenge period, to
ensure their moves are processed on chain. Major optimistic rollups today set
this period at roughly one week, mainly to guard against strong censorship that
undermines Ethereum's own crypto-economic security. However, other forms of
censorship are possible, and their implication on optimistic rollup security is
not well understood.
  This paper considers economic censorship attacks, where an attacker censors
the defender's transactions by bribing block proposers. At each step, the
attacker can either censor the defender -- depleting the defender's time
allowance at the cost of the bribe -- or allow the current transaction through
while conserving funds for future censorship.
  We analyze three game theoretic models of these dynamics and determine the
challenge period length required to ensure the defender's success, as a
function of the number of required protocol moves and the players' available
budgets.; 49) Prime Identification and Composite Filtering Using GM-(n+1) Sequences; This paper presents a distinctive prime detection approach. This method use
GM-(n+1) sequences to effectively eliminate complex numbers. The sequences,
which consist of odd a number of (n+1), exclude all components except for the
initial prime integer. Only the first prime number is presented. This research
proposes an approach using this model to identify exceptional candidates and
examine their distribution. This study examines the interconnections among the
laws of division, basic gaps, and their applications in analytical procedures.
Computer studies may provide a novel perspective on the theory of prime
numbers, demonstrating the effectiveness of this approach in refining the
search space for primes.; 50) Unified Enhancement of the Generalization and Robustness of Language
  Models via Bi-Stage Optimization; Neural network language models (LMs) are confronted with significant
challenges in generalization and robustness. Currently, many studies focus on
improving either generalization or robustness in isolation, without methods
addressing both aspects simultaneously, which presents a significant challenge
in developing LMs that are both robust and generalized. In this paper, we
propose a bi-stage optimization framework to uniformly enhance both the
generalization and robustness of LMs, termed UEGR. Specifically, during the
forward propagation stage, we enrich the output probability distributions of
adversarial samples by adaptive dropout to generate diverse sub models, and
incorporate JS divergence and adversarial losses of these output distributions
to reinforce output stability. During backward propagation stage, we compute
parameter saliency scores and selectively update only the most critical
parameters to minimize unnecessary deviations and consolidate the model's
resilience. Theoretical analysis shows that our framework includes gradient
regularization to limit the model's sensitivity to input perturbations and
selective parameter updates to flatten the loss landscape, thus improving both
generalization and robustness. The experimental results show that our method
significantly improves the generalization and robustness of LMs compared to
other existing methods across 13 publicly available language datasets,
achieving state-of-the-art (SOTA) performance.; 51) Policy iteration for nonconvex viscous Hamilton--Jacobi equations; We study the convergence rates of policy iteration (PI) for nonconvex viscous
Hamilton--Jacobi equations using a discrete space-time scheme, where both space
and time variables are discretized. We analyze the case with an uncontrolled
diffusion term, which corresponds to a possibly degenerate viscous
Hamilton--Jacobi equation. We first obtain an exponential convergent result of
PI for the discrete space-time schemes. We then investigate the discretization
error.; 52) Self-consistent scenario for jet and stellar explosion in collapsar:
  General relativistic magnetohydrodynamics simulation with dynamo; A resistive magnetohydrodynamics simulation with a dynamo term is performed
for modeling the collapsar in full general relativity. As an initial condition,
a spinning black hole and infalling stellar matter are modeled based on a
stellar evolution result, superimposing a weak toroidal magnetic field. After
the growth of a massive torus around the black hole, the magnetic field is
amplified in it, developing poloidal fields via dynamo. In an early stage of
the torus growth, magnetic fluxes that fall to the vicinity of the central
black hole are swallowed by the black hole and global poloidal magnetic fields
that can be the source of the Blandford-Znajek mechanism are not developed.
However, in a later stage in which the ram pressure of the infalling matter
becomes weak, the magnetic field amplified by the black hole spin via the
winding becomes large enough to expel the infalling matter by the magnetic
pressure, and subsequently, a global poloidal magnetic field that penetrates
the black hole is established, launching a jet along the spin axis by the
Blandford-Znajek mechanism with the luminosity suitable for explaining typical
long gamma-ray bursts. Together with the jet launch, the effectively viscous
effect in the inner region of the torus and the magnetocentrifugal effect drive
the stellar explosion with the explosion energy comparable to typical or
powerful supernovae. We also find large amounts of synthesized $^{56}$Ni and Zn
associated with the stellar explosion. In the presence of jet launching,
$r$-process elements are weakly synthesized. The numerical results of the
explosion energy, ejecta mass, and $^{56}$Ni mass are in a good agreement with
those for observed broad-lined type Ic supernovae. Our result illustrates a
self-consistent scenario for the gamma-ray-burst-associated broad-lined type Ic
supernovae.; 53) Decentralized Online Ensembles of Gaussian Processes for Multi-Agent
  Systems; Flexible and scalable decentralized learning solutions are fundamentally
important in the application of multi-agent systems. While several recent
approaches introduce (ensembles of) kernel machines in the distributed setting,
Bayesian solutions are much more limited. We introduce a fully decentralized,
asymptotically exact solution to computing the random feature approximation of
Gaussian processes. We further address the choice of hyperparameters by
introducing an ensembling scheme for Bayesian multiple kernel learning based on
online Bayesian model averaging. The resulting algorithm is tested against
Bayesian and frequentist methods on simulated and real-world datasets.; 54) WaveMax: Radar Waveform Design via Convex Maximization of FrFT Phase
  Retrieval; The ambiguity function (AF) is a critical tool in radar waveform design,
representing the two-dimensional correlation between a transmitted signal and
its time-delayed, frequency-shifted version. Obtaining a radar signal to match
a specified AF magnitude is a bi-variate variant of the well-known phase
retrieval problem. Prior approaches to this problem were either limited to a
few classes of waveforms or lacked a computable procedure to estimate the
signal. Our recent work provided a framework for solving this problem for both
band- and time-limited signals using non-convex optimization. In this paper, we
introduce a novel approach WaveMax that formulates waveform recovery as a
convex optimization problem by relying on the fractional Fourier transform
(FrFT)-based AF. We exploit the fact that AF of the FrFT of the original signal
is equivalent to a rotation of the original AF. In particular, we reconstruct
the radar signal by solving a low-rank minimization problem, which approximates
the waveform using the leading eigenvector of a matrix derived from the AF. Our
theoretical analysis shows that unique waveform reconstruction is achievable
with a sample size no more than three times the signal frequencies or time
samples. Numerical experiments validate the efficacy of WaveMax in recovering
signals from noiseless and noisy AF, including scenarios with randomly and
uniformly sampled sparse data.; 55) Water transport on finite graphs; Consider a simple finite graph and its nodes to represent identical water
barrels (containing different amounts of water) on a level plane. Each edge
corresponds to a (locked, water-filled) pipe connecting two barrels below the
plane. We fix one node $v$ and consider the optimization problem relating to
the maximum value to which the level in $v$ can be raised without pumps, i.e.
by opening/closing pipes in a suitable order. This fairly natural optimization
problem originated from the analysis of an opinion formation process and proved
to be not only sufficiently intricate in order to be of independent interest,
but also difficult from an algorithmic point of view.; 56) Physics-based Machine Learning for Computational Fracture Mechanics; This study introduces a physics-based machine learning framework for modeling
both brittle and ductile fractures. Unlike physics-informed neural networks,
which solve partial differential equations by embedding physical laws as soft
constraints in loss functions and enforcing boundary conditions via collocation
points, our framework integrates physical principles, such as the governing
equations and constraints, directly into the neural network architecture. This
approach eliminates the dependency on problem-specific retraining for new
boundary value problems, ensuring adaptability and consistency. By embedding
constitutive behavior into the network's foundational design, our method
represents a significant step toward unifying material modeling with machine
learning for computational fracture mechanics. Specifically, a feedforward
neural network is designed to embed physical laws within its architecture,
ensuring thermodynamic consistency. Building on this foundation, synthetic
datasets generated from finite element-based phase-field simulations are
employed to train the proposed framework, focusing on capturing the homogeneous
responses of brittle and ductile fractures. Detailed analyses are performed on
the stored elastic energy and the dissipated work due to plasticity and
fracture, demonstrating the capability of the framework to predict essential
fracture features. The proposed physics-based machine learning framework
overcomes the shortcomings of classical machine learning models, which rely
heavily on large datasets and lack guarantees of physical principles. By
leveraging its physics-integrated design, the physics-based machine learning
framework demonstrates exceptional performance in predicting key properties of
brittle and ductile fractures with limited training data.; 57) Chain-of-Rank: Enhancing Large Language Models for Domain-Specific RAG
  in Edge Device; Retrieval-augmented generation (RAG) with large language models (LLMs) is
especially valuable in specialized domains, where precision is critical. To
more specialize the LLMs into a target domain, domain-specific RAG has recently
been developed by allowing the LLM to access the target domain early via
finetuning. The domain-specific RAG makes more sense in resource-constrained
environments like edge devices, as they should perform a specific task (e.g.
personalization) reliably using only small-scale LLMs. While the
domain-specific RAG is well-aligned with edge devices in this respect, it often
relies on widely-used reasoning techniques like chain-of-thought (CoT). The
reasoning step is useful to understand the given external knowledge, and yet it
is computationally expensive and difficult for small-scale LLMs to learn it.
Tackling this, we propose the Chain of Rank (CoR) which shifts the focus from
intricate lengthy reasoning to simple ranking of the reliability of input
external documents. Then, CoR reduces computational complexity while
maintaining high accuracy, making it particularly suited for
resource-constrained environments. We attain the state-of-the-art (SOTA)
results in benchmarks, and analyze its efficacy.; 58) Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth
  Approach; We study a novel language model architecture that is capable of scaling
test-time computation by implicitly reasoning in latent space. Our model works
by iterating a recurrent block, thereby unrolling to arbitrary depth at
test-time. This stands in contrast to mainstream reasoning models that scale up
compute by producing more tokens. Unlike approaches based on chain-of-thought,
our approach does not require any specialized training data, can work with
small context windows, and can capture types of reasoning that are not easily
represented in words. We scale a proof-of-concept model to 3.5 billion
parameters and 800 billion tokens. We show that the resulting model can improve
its performance on reasoning benchmarks, sometimes dramatically, up to a
computation load equivalent to 50 billion parameters.; 59) Optical RIS-enabled Multiple Access Communications; In this paper, we identify optical reconfigurable intelligent surfaces
(ORISs) as key enablers of next-generation free-space optical (FSO) multiple
access systems. By leveraging their beam steering and beam splitting
capabilities, ORISs are able to effectively address line-of-sight (LoS)
constraints, while enabling multi-user connectivity. We consider an
ORIS-assisted non-orthogonal multiple access (NOMA) system model consisting of
a single transmitter (Tx) and two receivers (Rxs). We derive novel analytical
expressions to characterize the statistical particularities of the Tx-ORIS-Rx
communication channel. Building upon the aforementioned expressions, we
investigate the outage performance of the Rxs by deriving exact analytical
expressions for the outage probability (OP) of each Rx. To provide deeper
insights into the impact of various system parameters and physical conditions
on the outage performance of each Rx, we conduct a high signal-to-noise ratio
(SNR) analysis, that returns asymptotic expressions for the Rxs OPs at the
high-SNR regime. Monte Carlo simulations validate the analysis, demonstrate the
effectiveness of ORIS-enabled NOMA under a variety of configurations and
physical scenarios, and showcase its superiority over its orthogonal-based
counterpart.; 60) Optimizing Ansatz Design in Quantum Generative Adversarial Networks
  Using Large Language Models; We present a novel approach for improving the design of ansatzes in Quantum
Generative Adversarial Networks (qGANs) by leveraging Large Language Models
(LLMs). By combining the strengths of LLMs with qGANs, our approach iteratively
refines ansatz structures to improve accuracy while reducing circuit depth and
the number of parameters. This study paves the way for further exploration in
AI-driven quantum algorithm design. The flexibility of our proposed workflow
extends to other quantum variational algorithms, providing a general framework
for optimizing quantum circuits in a variety of quantum computing tasks.; 61) Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion
  Transformers via In-Context Reflection; The predominant approach to advancing text-to-image generation has been
training-time scaling, where larger models are trained on more data using
greater computational resources. While effective, this approach is
computationally expensive, leading to growing interest in inference-time
scaling to improve performance. Currently, inference-time scaling for
text-to-image diffusion models is largely limited to best-of-N sampling, where
multiple images are generated per prompt and a selection model chooses the best
output. Inspired by the recent success of reasoning models like DeepSeek-R1 in
the language domain, we introduce an alternative to naive best-of-N sampling by
equipping text-to-image Diffusion Transformers with in-context reflection
capabilities. We propose Reflect-DiT, a method that enables Diffusion
Transformers to refine their generations using in-context examples of
previously generated images alongside textual feedback describing necessary
improvements. Instead of passively relying on random sampling and hoping for a
better result in a future generation, Reflect-DiT explicitly tailors its
generations to address specific aspects requiring enhancement. Experimental
results demonstrate that Reflect-DiT improves performance on the GenEval
benchmark (+0.19) using SANA-1.0-1.6B as a base model. Additionally, it
achieves a new state-of-the-art score of 0.81 on GenEval while generating only
20 samples per prompt, surpassing the previous best score of 0.80, which was
obtained using a significantly larger model (SANA-1.5-4.8B) with 2048 samples
under the best-of-N approach.; 62) No LLM is Free From Bias: A Comprehensive Study of Bias Evaluation in
  Large Language models; Advancements in Large Language Models (LLMs) have increased the performance
of different natural language understanding as well as generation tasks.
Although LLMs have breached the state-of-the-art performance in various tasks,
they often reflect different forms of bias present in the training data. In the
light of this perceived limitation, we provide a unified evaluation of
benchmarks using a set of representative LLMs that cover different forms of
biases starting from physical characteristics to socio-economic categories.
Moreover, we propose five prompting approaches to carry out the bias detection
task across different aspects of bias. Further, we formulate three research
questions to gain valuable insight in detecting biases in LLMs using different
approaches and evaluation metrics across benchmarks. The results indicate that
each of the selected LLMs suffer from one or the other form of bias with the
LLaMA3.1-8B model being the least biased. Finally, we conclude the paper with
the identification of key challenges and possible future directions.; 63) PUGS: Zero-shot Physical Understanding with Gaussian Splatting; Current robotic systems can understand the categories and poses of objects
well. But understanding physical properties like mass, friction, and hardness,
in the wild, remains challenging. We propose a new method that reconstructs 3D
objects using the Gaussian splatting representation and predicts various
physical properties in a zero-shot manner. We propose two techniques during the
reconstruction phase: a geometry-aware regularization loss function to improve
the shape quality and a region-aware feature contrastive loss function to
promote region affinity. Two other new techniques are designed during
inference: a feature-based property propagation module and a volume integration
module tailored for the Gaussian representation. Our framework is named as
zero-shot physical understanding with Gaussian splatting, or PUGS. PUGS
achieves new state-of-the-art results on the standard benchmark of ABO-500 mass
prediction. We provide extensive quantitative ablations and qualitative
visualization to demonstrate the mechanism of our designs. We show the proposed
methodology can help address challenging real-world grasping tasks. Our codes,
data, and models are available at https://github.com/EverNorif/PUGS; 64) Revolutionizing Healthcare Record Management: Secure Documentation
  Storage and Access through Advanced Blockchain Solutions; Integrating blockchain technology into healthcare systems presents a
transformative approach to documenting, storing, and accessing electronic
health records (EHRs). This research introduces a novel blockchain-based EHR
system designed to significantly enhance security, scalability, and
accessibility compared to existing solutions. Current systems primarily utilize
SHA-256 for security and either IPFS or centralized storage, which, while
effective, have limitations in providing comprehensive data integrity and
security. The proposed system leverages a hybrid security algorithm combining
Argon2 and AES and integrates a hybrid storage and consensus mechanism
utilizing IPFS and PBFT. This multifaceted approach ensures robust encryption,
efficient consensus, and high fault tolerance. Furthermore, the system
incorporates Multi-Factor Authentication (MFA) to safeguard against
unauthorized access. It utilizes advanced blockchain tools like MetaMask,
Ganache, and Truffle to facilitate seamless interaction with the decentralized
network. Simulation results demonstrate that this system offers superior
protection against data breaches and enhances operational efficiency.
Specifically, the proposed hybrid model substantially improves data integrity,
consensus efficiency, fault tolerance, data availability, latency, bandwidth
utilization, throughput, memory usage, and CPU usage across various healthcare
applications. To validate the performance and security of the proposed system,
comprehensive analyses were conducted using real-world healthcare scenarios.
The findings highlight the significant advantages of the blockchain-based EHR
system, emphasizing its potential to revolutionize healthcare data management
by ensuring secure, reliable, and efficient handling of sensitive medical
information.; 65) Large Negative Magnetoresistance in Antiferromagnetic Gd2Se3; Rare earth chalcogenides provide a great platform to study exotic quantum
phenomena such as superconductivity and charge density waves. Among various
interesting properties, the coupling between magnetism and electronic transport
has attracted significant attention. Here, we report the investigation of such
coupling in {alpha}-Gd2Se3 single crystals through magnetic, calorimetric, and
transport property measurements. {alpha}-Gd2Se3 is found to display an
antiferromagnetic ground state below 11 K with metamagnetic spin-flop
transitions. The magnetic fluctuations remain strong above the transition
temperature. Transport measurements reveal an overall metallic transport
behavior with a large negative magnetoresistance of ~ 65% near the magnetic
transition temperature, together with positive MR near the field-induced
spin-flop transitions, which can be understood in terms of the suppression of
spin scattering by the magnetic field.; 66) A Louder Gravitational Wave Bang from a Fast-Expanding Universe; A strong first-order phase transition in a dark sector may produce all or
part of the low-frequency gravitational wave signal recently reported by the
NANOGrav Collaboration and other pulsar timing arrays. Here we point out, with
a simple toy model, that even if the amplitude of the gravitational wave
background from the dark phase transition is insufficient to match the NANOGrav
signal, a modified expansion rate at early times may considerably enhance the
gravitational wave signal. In particular, a faster-than-standard expansion
rate, triggered, for instance, by the presence of one or more additional
sources of energy density redshifting with higher powers of temperatures than
radiation, boosts upper limits on the gravitational wave signal from
first-order cosmological phase transitions, enlarging the slate of possible
dark sector scenarios matching the NANOGrav signal.; 67) A Multimodal Physics-Informed Neural Network Approach for Mean Radiant
  Temperature Modeling; Outdoor thermal comfort is a critical determinant of urban livability,
particularly in hot desert climates where extreme heat poses challenges to
public health, energy consumption, and urban planning. Mean Radiant Temperature
($T_{mrt}$) is a key parameter for evaluating outdoor thermal comfort,
especially in urban environments where radiation dynamics significantly impact
human thermal exposure. Traditional methods of estimating $T_{mrt}$ rely on
field measurements and computational simulations, both of which are resource
intensive. This study introduces a Physics-Informed Neural Network (PINN)
approach that integrates shortwave and longwave radiation modeling with deep
learning techniques. By leveraging a multimodal dataset that includes
meteorological data, built environment characteristics, and fisheye
image-derived shading information, our model enhances predictive accuracy while
maintaining physical consistency. Our experimental results demonstrate that the
proposed PINN framework outperforms conventional deep learning models, with the
best-performing configurations achieving an RMSE of 3.50 and an $R^2$ of 0.88.
This approach highlights the potential of physics-informed machine learning in
bridging the gap between computational modeling and real-world applications,
offering a scalable and interpretable solution for urban thermal comfort
assessments.; 68) A Case Study on Model Checking and Runtime Verification for Awkernel; In operating system development, concurrency poses significant challenges. It
is difficult for humans to manually review concurrent behaviors or to write
test cases covering all possible executions, often resulting in critical bugs.
Preemption in schedulers serves as a typical example. This paper proposes a
development method for concurrent software, such as schedulers. Our method
incorporates model checking as an aid for tracing code, simplifying the
analysis of concurrent behavior; we refer to this as model checking-assisted
code review. While this approach aids in tracing behaviors, the accuracy of the
results is limited because of the semantics gap between the modeling language
and the programming language. Therefore, we also introduce runtime verification
to address this limitation in model checking-assisted code review. We applied
our approach to a real-world operating system, Awkernel, as a case study. This
new operating system, currently under development for autonomous driving, is
designed for preemptive task execution using asynchronous functions in Rust.
After implementing our method, we identified several bugs that are difficult to
detect through manual reviews or simple tests.; 69) Superconducting LaPtH$_{ 6 }$ with triatomic hydrogen units; To veryfy ""hot supreconductivity"" recently proposed in lanthanum
hydride-based compounds, we explored thermodynamically stable and
superconducting phases in the lanthanum (La)-platinum (Pt)-hydrogen (H) ternary
system at 20 GPa using an evolutionary construction scheme of a
formation-enthalpy convex hull, universal neural network potential
calculations, and density functional theory calculations. Although we found no
evidence of the hot superconductivity in this ternary system, we predicted a
unique compound, LaPtH$_{ 6 }$, which has equilateral triangular H$_{ 3 }$
units nearly forming a two-dimensional kagome lattice between La and Pt layers
and shows the superconductivity at 18.67 K. This structure is dynamically
stable from ambient pressure to at least 200 GPa and the superconducting
critical temperature increases from 13.51 to 40.63 K.; 70) ProjectTest: A Project-level LLM Unit Test Generation Benchmark and
  Impact of Error Fixing Mechanisms; Unit test generation has become a promising and important use case of LLMs.
However, existing evaluation benchmarks for assessing LLM unit test generation
capabilities focus on function- or class-level code rather than more practical
and challenging project-level codebases. To address such limitation, we propose
ProjectTest, a project-level benchmark for unit test generation covering
Python, Java, and JavaScript. ProjectTest features 20 moderate-sized and
high-quality projects per language. We evaluate nine frontier LLMs on
ProjectTest and the results show that all frontier LLMs tested exhibit moderate
performance on ProjectTest on Python and Java, highlighting the difficulty of
ProjectTest. We also conduct a thorough error analysis, which shows that even
frontier LLMs, such as Claude-3.5-Sonnet, have significant basic yet critical
errors, including compilation and cascade errors. Motivated by this
observation, we further evaluate all frontier LLMs under manual error-fixing
and self-error-fixing scenarios to assess their potential when equipped with
error-fixing mechanisms. Our code and dataset is available at
\href{https://github.com/YiboWANG214/ProjectTest}{ProjectTest}.; 71) Towards Zero Touch Networks: Cross-Layer Automated Security Solutions
  for 6G Wireless Networks; The transition from 5G to 6G mobile networks necessitates network automation
to meet the escalating demands for high data rates, ultra-low latency, and
integrated technology. Recently, Zero-Touch Networks (ZTNs), driven by
Artificial Intelligence (AI) and Machine Learning (ML), are designed to
automate the entire lifecycle of network operations with minimal human
intervention, presenting a promising solution for enhancing automation in 5G/6G
networks. However, the implementation of ZTNs brings forth the need for
autonomous and robust cybersecurity solutions, as ZTNs rely heavily on
automation. AI/ML algorithms are widely used to develop cybersecurity
mechanisms, but require substantial specialized expertise and encounter model
drift issues, posing significant challenges in developing autonomous
cybersecurity measures. Therefore, this paper proposes an automated security
framework targeting Physical Layer Authentication (PLA) and Cross-Layer
Intrusion Detection Systems (CLIDS) to address security concerns at multiple
Internet protocol layers. The proposed framework employs drift-adaptive online
learning techniques and a novel enhanced Successive Halving (SH)-based
Automated ML (AutoML) method to automatically generate optimized ML models for
dynamic networking environments. Experimental results illustrate that the
proposed framework achieves high performance on the public Radio Frequency (RF)
fingerprinting and the Canadian Institute for CICIDS2017 datasets, showcasing
its effectiveness in addressing PLA and CLIDS tasks within dynamic and complex
networking environments. Furthermore, the paper explores open challenges and
research directions in the 5G/6G cybersecurity domain. This framework
represents a significant advancement towards fully autonomous and secure 6G
networks, paving the way for future innovations in network automation and
cybersecurity.; 72) Magnetic Equivariant K-theory; We present the fundamental properties of the K-theory groups of complex
vector bundles endowed with actions of magnetic groups. In this work we show
that the magnetic equivariant K-theory groups define an equivariant cohomology
theory, we determine its coefficients, we show Bott's, Thom's and the degree
shift isomorphism, we present the Atiyah-Hirzeburh spectral sequence, and we
explicitly calculate two magnetic equivariant K-theory groups in order to
showcase its applicability. These magnetic equivariant K-theory groups are
relevant in condensed matter physics since they provide topological invariants
of gapped Hamiltonians in magnetic crystals.; 73) Learning A Zero-shot Occupancy Network from Vision Foundation Models via
  Self-supervised Adaptation; Estimating the 3D world from 2D monocular images is a fundamental yet
challenging task due to the labour-intensive nature of 3D annotations. To
simplify label acquisition, this work proposes a novel approach that bridges 2D
vision foundation models (VFMs) with 3D tasks by decoupling 3D supervision into
an ensemble of image-level primitives, e.g., semantic and geometric components.
As a key motivator, we leverage the zero-shot capabilities of vision-language
models for image semantics. However, due to the notorious ill-posed problem -
multiple distinct 3D scenes can produce identical 2D projections, directly
inferring metric depth from a monocular image in a zero-shot manner is
unsuitable. In contrast, 2D VFMs provide promising sources of relative depth,
which theoretically aligns with metric depth when properly scaled and offset.
Thus, we adapt the relative depth derived from VFMs into metric depth by
optimising the scale and offset using temporal consistency, also known as novel
view synthesis, without access to ground-truth metric depth. Consequently, we
project the semantics into 3D space using the reconstructed metric depth,
thereby providing 3D supervision. Extensive experiments on nuScenes and
SemanticKITTI demonstrate the effectiveness of our framework. For instance, the
proposed method surpasses the current state-of-the-art by 3.34% mIoU on
nuScenes for voxel occupancy prediction.; 74) Sparse Binary Representation Learning for Knowledge Tracing; Knowledge tracing (KT) models aim to predict students' future performance
based on their historical interactions. Most existing KT models rely
exclusively on human-defined knowledge concepts (KCs) associated with
exercises. As a result, the effectiveness of these models is highly dependent
on the quality and completeness of the predefined KCs. Human errors in labeling
and the cost of covering all potential underlying KCs can limit model
performance.
  In this paper, we propose a KT model, Sparse Binary Representation KT
(SBRKT), that generates new KC labels, referred to as auxiliary KCs, which can
augment the predefined KCs to address the limitations of relying solely on
human-defined KCs. These are learned through a binary vector representation,
where each bit indicates the presence (one) or absence (zero) of an auxiliary
KC. The resulting discrete representation allows these auxiliary KCs to be
utilized in training any KT model that incorporates KCs. Unlike pre-trained
dense embeddings, which are limited to models designed to accept such vectors,
our discrete representations are compatible with both classical models, such as
Bayesian Knowledge Tracing (BKT), and modern deep learning approaches.
  To generate this discrete representation, SBRKT employs a binarization method
that learns a sparse representation, fully trainable via stochastic gradient
descent. Additionally, SBRKT incorporates a recurrent neural network (RNN) to
capture temporal dynamics and predict future student responses by effectively
combining the auxiliary and predefined KCs. Experimental results demonstrate
that SBRKT outperforms the tested baselines on several datasets and achieves
competitive performance on others. Furthermore, incorporating the learned
auxiliary KCs consistently enhances the performance of BKT across all tested
datasets.; 75) Privacy Protection in Prosumer Energy Management Based on Federated
  Learning; With the booming development of prosumers, there is an urgent need for a
prosumer energy management system to take full advantage of the flexibility of
prosumers and take into account the interests of other parties. However,
building such a system will undoubtedly reveal users' privacy. In this paper,
by solving the non-independent and identical distribution of data (Non-IID)
problem in federated learning with federated cluster average(FedClusAvg)
algorithm, prosumers' information can efficiently participate in the
intelligent decision making of the system without revealing privacy. In the
proposed FedClusAvg algorithm, each client performs cluster stratified sampling
and multiple iterations. Then, the average weight of the parameters of the
sub-server is determined according to the degree of deviation of the parameter
from the average parameter. Finally, the sub-server multiple local iterations
and updates, and then upload to the main server. The advantages of FedClusAvg
algorithm are the following two parts. First, the accuracy of the model in the
case of Non-IID is improved through the method of clustering and parameter
weighted average. Second, local multiple iterations and three-tier framework
can effectively reduce communication rounds.; 76) RewardSDS: Aligning Score Distillation via Reward-Weighted Sampling; Score Distillation Sampling (SDS) has emerged as an effective technique for
leveraging 2D diffusion priors for tasks such as text-to-3D generation. While
powerful, SDS struggles with achieving fine-grained alignment to user intent.
To overcome this, we introduce RewardSDS, a novel approach that weights noise
samples based on alignment scores from a reward model, producing a weighted SDS
loss. This loss prioritizes gradients from noise samples that yield aligned
high-reward output. Our approach is broadly applicable and can extend SDS-based
methods. In particular, we demonstrate its applicability to Variational Score
Distillation (VSD) by introducing RewardVSD. We evaluate RewardSDS and
RewardVSD on text-to-image, 2D editing, and text-to-3D generation tasks,
showing significant improvements over SDS and VSD on a diverse set of metrics
measuring generation quality and alignment to desired reward models, enabling
state-of-the-art performance. Project page is available at
https://itaychachy.github.io/reward-sds/.; 77) Gaussian basis set approach to one-loop self-energy; We report a method for the evaluation of the one-loop self-energy, to all
orders in the external binding field, using a Gaussian basis set expansion.
This choice of basis is motivated by its widespread use in molecular
calculations. For a one-electron atom, our results show excellent agreement
with those obtained using the exact Dirac--Coulomb wave functions. The
developed method can be of interest for high-precision studies of heavy
few-electron molecular systems, where the rigorous computation of QED
corrections is currently a formidable task.; 78) The null condition in elastodynamics leads to non-uniqueness; We consider the Cauchy problem for the system of elastodynamic equations in
two dimensions. Specifically, we focus on materials characterized by a null
condition imposed on the quadratic part of the nonlinearity. We can construct
non-zero weak solutions $u \in C^1([0, T] \times \mathbb{T}^2)$ that emanate
from zero initial data. The proof relies on the convex integration scheme. By
exploiting the characteristic double wave speeds of the equations, we construct
a new class of building blocks. This work extends the application of convex
integration techniques to hyperbolic systems with a null condition and reveals
the rich solution structure in nonlinear elastodynamics.; 79) Isolated attosecond free-electron laser based on a sub-cycle driver from
  hollow capillary fibers; The attosecond light source provides an advanced tool for investigating
electron motion using time-resolved-spectroscopy techniques. Isolated
attosecond pulses, especially, will significantly advance the study of electron
dynamics. However, achieving high-intensity isolated attosecond pulses is still
challenging at the present stage. In this paper, we propose a novel scheme for
generating high-intensity, isolated attosecond soft X-ray free-electron lasers
(FELs) using a mid-infrared (MIR) sub-cycle modulation laser from gas-filled
hollow capillary fibers (HCFs). The multi-cycle MIR pulses are first compressed
to sub-cycle using a helium-filled HCF with decreasing pressure gradient due to
soliton self-compression effect. By utilizing such sub-cycle MIR laser pulse to
modulate the electron beam, we can obtain a quasi-isolated current peak, which
can then produce an isolated FEL pulse with high signal-to-noise ratio (SNR),
naturally synchronizing with the sub-cycle MIR laser pulse. Numerical
simulations have been carried out, including the sub-cycle pulse generation,
electron beam modulation and FEL radiation processes. The simulation results
indicate that an isolated attosecond pulse with wavelength of 1 nm, peak power
of ~28 GW, pulse duration of ~600 attoseconds and SNR of ~96.4% can be
generated by our proposed method. The numerical results demonstrated here pave
a new way for generating the high-intensity isolated attosecond soft X-ray
pulse, which may have many applications in nonlinear spectroscopy and
atomic-site electronic process.; 80) GraphSense: Graph Embedding Based Code Suggestion Framework; Code suggestions have become an integral part of IDEs and developers use code
suggestions generated by IDEs all the time. These code suggestions are mostly
for calling a method of an object or for using a function of a library and not
for possible next line of the code. GPT based models are too slow or resource
intensive for real-time code suggestions in local environments. As a solution
to this GraphSense was introduced which provide code suggestions with minimum
amount of resource usage in real-time.; 81) The Role, Trends, and Applications of Machine Learning in Undersea
  Communication: A Bangladesh Perspective; The rapid evolution of machine learning (ML) has brought about groundbreaking
developments in numerous industries, not the least of which is in the area of
undersea communication. This domain is critical for applications like ocean
exploration, environmental monitoring, resource management, and national
security. Bangladesh, a maritime nation with abundant resources in the Bay of
Bengal, can harness the immense potential of ML to tackle the unprecedented
challenges associated with underwater communication. Beyond that, environmental
conditions are unique to the region: in addition to signal attenuation,
multipath propagation, noise interference, and limited bandwidth. In this
study, we address the necessity to bring ML into communication via undersea; it
investigates the latest technologies under the domain of ML in that respect,
such as deep learning and reinforcement learning, especially concentrating on
Bangladesh scenarios in the sense of implementation. This paper offers a
contextualized regional perspective by incorporating region-specific needs,
case studies, and recent research to propose a roadmap for deploying ML-driven
solutions to improve safety at sea, promote sustainable resource use, and
enhance disaster response systems. This research ultimately highlights the
promise of ML-powered solutions for transforming undersea communication,
leading to more efficient and cost-effective technologies that subsequently
contribute to both economic growth and environmental sustainability.; 82) Low-cost Microfluidic Testbed for Molecular Communications with
  Integrated Hydrodynamic Gating and Screen-printed Sensors; Molecular Communications (MC), transferring information via chemical signals,
holds promise for transformative healthcare applications within the Internet of
Bio-Nano Things (IoBNT) framework. Despite promising advances toward practical
MC systems, progress has been constrained by experimental testbeds that are
costly, difficult to customize, and require labor-intensive fabrication. Here,
we address these challenges by introducing a low-cost ($\sim$\$1 per unit),
rapidly fabricated ($<$1 hour), and highly customizable microfluidic testbed
that integrates hydrodynamic gating and screen-printed potentiometric sensors.
This platform enables precise spatiotemporal control over chemical signals and
supports reconfigurable channel architectures along with on-demand sensor
functionalization. As a proof of concept, we demonstrate a pH-based MC system
combining a polyaniline (PANI)-functionalized sensor for real-time signal
detection with a programmable hydrodynamic gating architecture, patterned in a
double-sided adhesive tape, as the transmitter. By dynamically mixing
phosphate-buffered saline (PBS) with an acidic solution (pH 3), the testbed
reliably generates pH-encoded pulses. Experimental results confirm robust
control over pulse amplitude and pulse width, enabling the simulation of
end-to-end MC scenarios with 4-ary concentration shift keying (CSK) modulation.
By combining affordability and rapid prototyping without compromising
customizability, this platform is poised to accelerate the translation of MC
concepts into practical IoBNT applications.; 83) Analysis of the weak lensing mass-richness relation of redMaPPer
  clusters in the LSST DESC DC2 simulations; Cluster scaling relations are key ingredients in cluster abundance-based
cosmological studies. In optical cluster cosmology, weak gravitational lensing
has proven to be a powerful tool to constrain the cluster mass-richness
relation. This work is conducted as part of the Dark Energy Science
Collaboration (DESC), which aims to analyze the Legacy Survey of Space and Time
(LSST) of Vera C. Rubin Observatory, starting in 2026. Weak lensing-inferred
cluster properties, such as mass, suffer from several sources of bias. In this
paper, we aim to test the impact of modeling choices and observational
systematics in cluster lensing on the inference of the mass-richness relation.
We constrain the mass-richness relation of 3,600 clusters detected by the
redMaPPer algorithm in the cosmoDC2 extra-galactic mock catalog (covering $440$
deg$^2$) of the LSST DESC DC2 simulation, using number count measurements and
stacked weak lensing profiles in several intervals of richness ($20 \leq
\lambda \leq 200$) and redshift ($0.2 \leq z \leq 1$). By modeling the mean of
the scaling relation as $\langle \ln \lambda|M_{\rm 200c}, z\rangle =
\ln\lambda_0 + \mu_z\log[(1+z)/(1+0.5)] + \mu_m[\log_{10}(M_{\rm 200c}) -
14.3]$, our baseline constraints are $\ln\lambda_0 = 3.37\pm 0.03$, $\mu_z =
0.08\pm 0.07$ and $\mu_m = 2.18 \pm 0.07$. We have found that, for a LSST-like
source galaxy density, our constraints are robust to a change in
concentration-mass relation and dark matter density profile modeling choices,
when source redshifts and shapes are perfectly known. We have found that
photometric redshift uncertainties can introduce bias at the $1\sigma$ level,
which can be mitigated by an overall correcting factor, fitted jointly with
scaling parameters. We find that including positive shear-richness covariance
in the fit shifts the results by up to 0.5$\sigma$.; 84) Superlubric sliding ferroelectricity; Sliding ferroelectricity may emerge in many van der Waals
bilayers/multilayers and the low switching barriers render ultrafast data
writing with low energy cost. We note that such barriers are still much higher
compared with structural superlubricity, and in this paper we propose a type of
superlubric sliding ferroelectricity in homobilayers separated by a different
layer that leads to unprecedented low switching barriers due to incommensurate
interfaces. For example, the switching barrier of 3R bilayer MoS2 will be
respectively reduced by around 2 or 1 order of magnitudes if they are separated
by a graphene or BN monolayer, and the required voltage for switching can be
about 1 order of magnitude lower. Such superlubric sliding ferroelectricity
widely exists in various similar sandwich trilayer systems where the
polarizations stem from symmetry breaking in across-layer stacking
configurations, and with ultralow barriers of superlubric sliding, their
performances for various applications are greatly enhanced compared with
homobilayer sliding ferroelectrics.; 85) Indoor Light and Heat Estimation from a Single Panorama; This paper presents a novel application for directly estimating indoor light
and heat maps from captured indoor-outdoor High Dynamic Range (HDR) panoramas.
In our image-based rendering method, the indoor panorama is used to estimate
the 3D room layout, while the corresponding outdoor panorama serves as an
environment map to infer spatially-varying light and material properties. We
establish a connection between indoor light transport and heat transport and
implement transient heat simulation to generate indoor heat panoramas. The
sensitivity analysis of various thermal parameters is conducted, and the
resulting heat maps are compared with the images captured by the thermal camera
in real-world scenarios. This digital application enables automatic indoor
light and heat estimation without manual inputs and cumbersome field
measurements.; 86) New Limits on Ultralight Axionlike Dark Matter; New limits on the axion-nucleon coupling over the axion mass region $10^{-24}
\leq m_a \leq 5 \times 10^{-21}$ eV are derived by reanalyzing data from
laboratory measurements on Lorentz and $CPT$ violation. These results establish
the first laboratory constraints on the axion-nucleon coupling for axion masses
below $10^{-22}$ eV. For $10^{-22} \leq m_a \leq 5 \times 10^{-21}$ eV, the
results improve upon previous laboratory limits by more than 3 orders of
magnitude, exceeding for the first time the astrophysical limits from supernova
SN1987A cooling. For the axion mass range of interest corresponding to ultralow
frequencies, the crucial initial phase of the axion field is considered.
Furthermore, the obtained limits are nearly equivalent to those projected for a
recently proposed experiment employing high-intensity neutron beams at the
European Spallation Source [Phys. Rev. Lett. 133, 181001 (2024)]. For an
alternative type of axion-nucleon interaction, the quadratic wind coupling, the
constraints exceed the current best results by approximately 2 orders of
magnitude.; 87) WorldPose: A World Cup Dataset for Global 3D Human Pose Estimation; We present WorldPose, a novel dataset for advancing research in multi-person
global pose estimation in the wild, featuring footage from the 2022 FIFA World
Cup. While previous datasets have primarily focused on local poses, often
limited to a single person or in constrained, indoor settings, the
infrastructure deployed for this sporting event allows access to multiple fixed
and moving cameras in different stadiums. We exploit the static multi-view
setup of HD cameras to recover the 3D player poses and motions with
unprecedented accuracy given capture areas of more than 1.75 acres. We then
leverage the captured players' motions and field markings to calibrate a moving
broadcasting camera. The resulting dataset comprises more than 80 sequences
with approx 2.5 million 3D poses and a total traveling distance of over 120 km.
Subsequently, we conduct an in-depth analysis of the SOTA methods for global
pose estimation. Our experiments demonstrate that WorldPose challenges existing
multi-person techniques, supporting the potential for new research in this area
and others, such as sports analysis. All pose annotations (in SMPL format),
broadcasting camera parameters and footage will be released for academic
research purposes.; 88) Density-Matrix Embedding Based Multi-Configurational Perturbation Theory
  Approach to Single-Ion Magnets; Multi-configurational wave-function theory (MC-WFT) that combines complete
active space self-consistent field (CASSCF) approach with subsequent state
interaction (SI) treatment of spin-orbit coupling (SOC), abbreviated as
CASSCF-SO, plays important roles in microscopic understanding of single-ion
magnets (SIMs) with different central transition metal or lanthanide ions and
various coordination environments, but its application to SIMs with complex
structure is severely limited due to its highly demanding computational cost.
Density-matrix embedding theory (DMET) provides a systematic and mathematically
rigorous framework to combine low-level mean field approaches like Hartree-Fock
and high-level MC-WFT methods like CASSCF-SO, which is particularly promising
to SIMs. As a continuation of our previous work on DMET+CASSCF for $3d$ SIMs
(Ai, Sun, and Jiang, J. Phys. Chem. Lett. 2022, 13, 10627), we extend the
methodology by considering dynamic correlation on top of CASSCF using the
second-order $n$-electron valence perturbation theory (NEVPT2) in the DMET
framework, abbreviated as DMET+NEVPT2, and benchmark the accuracy of this
approach to molecular magnetic anisotropy in a set of typical transition metal
complexes. We found that DMET+NEVPT2 can give the results very close to
all-electron treatment, and can be systematically improved for higher accuracy
by expanding the region treated as the central cluster, while the computation
cost is dramatically reduced due to the reduction of the number of orbitals by
DMET construction. Our findings suggest that DMET is capable of accounting for
most of the dynamic correlation that is important for magnetic anisotropy in
typical SIMs, and can be useful for further high-accuracy spin-phonon study and
high-throughput computations.; 89) An ordinal analysis of CM and its extensions; In arXiv:0905.1675, Nik Weaver proposed a novel intuitionistic formal theory
of third-order arithmetic as a formalisation of his philosophical position
known as mathematical conceptualism. In this paper, we will construct a
realisability model from the partial combinatory algebra of
$\Sigma^1_1$-definable partial functions and use it to provide an ordinal
analysis of this formal theory. Additionally, we will examine possible
extensions to this system by adding well-ordering axioms, which are briefly
mentioned but never thoroughly studied in Weaver's work. We aim to use the
realisability arguments to discuss how much such extensions constitute an
increase from the original theory's proof-theoretic strength.; 90) HEISIR: Hierarchical Expansion of Inverted Semantic Indexing for
  Training-free Retrieval of Conversational Data using LLMs; The growth of conversational AI services has increased demand for effective
information retrieval from dialogue data. However, existing methods often face
challenges in capturing semantic intent or require extensive labeling and
fine-tuning. This paper introduces HEISIR (Hierarchical Expansion of Inverted
Semantic Indexing for Retrieval), a novel framework that enhances semantic
understanding in conversational data retrieval through optimized data
ingestion, eliminating the need for resource-intensive labeling or model
adaptation. HEISIR implements a two-step process: (1) Hierarchical Triplets
Formulation and (2) Adjunct Augmentation, creating semantic indices consisting
of Subject-Verb-Object-Adjunct (SVOA) quadruplets. This structured
representation effectively captures the underlying semantic information from
dialogue content. HEISIR achieves high retrieval performance while maintaining
low latency during the actual retrieval process. Our experimental results
demonstrate that HEISIR outperforms fine-tuned models across various embedding
types and language models. Beyond improving retrieval capabilities, HEISIR also
offers opportunities for intent and topic analysis in conversational data,
providing a versatile solution for dialogue systems.; 91) L-shell Photoionisation Cross Sections in the S^{+}, S^{2+}, S^{3+}
  Isonuclear Sequence; We present absolute L-shell photoionisation cross sections for the S+, S2+,
S3+ions. The cross sections were obtained using the monochromatised photon beam
delivered by the SOLEIL synchrotron source coupled with an ion beam extracted
from an electron cyclotron resonance source (ECRIS) in the merged dual-beam
configuration. The cross sections for single, double and triple ionisation were
measured and combined to generate total photoionisation cross sections. For
each of the S+, S2+, S3+ ions, the photon energy regions corresponding to the
excitation and ionisation of a 2p or a 2s electron (175-230 eV) were
investigated. The experimental results are interpreted with the help of
multiconfigurational Dirac-Fock (MCDF) and Breit-Pauli R-Matrix (BPRM) or Dirac
R-Matrix (DARC) theoretical calculations. The former generates photoabsorption
cross sections from eigenenergies and eigenfunctions obtained by solving
variationally the multiconfiguration Dirac Hamiltonian while the latter
calculate cross sections for photon scattering by atoms. The cross sectional
spectra feature rich resonance structures with narrow natural widths (typically
less than 100 meV) due to 2p to nd excitations below and up to the 2p
thresholds. This behaviour is consistent with the large number of inner-shell
states based on correlation and spin-orbit mixed configurations having three
open subshells. Strong and wide (typically 1 eV) Rydberg series of resonances
due to 2s to np excitations dominate above the 2p threshold.; 92) International AI Safety Report; The first International AI Safety Report comprehensively synthesizes the
current evidence on the capabilities, risks, and safety of advanced AI systems.
The report was mandated by the nations attending the AI Safety Summit in
Bletchley, UK. Thirty nations, the UN, the OECD, and the EU each nominated a
representative to the report's Expert Advisory Panel. A total of 100 AI experts
contributed, representing diverse perspectives and disciplines. Led by the
report's Chair, these independent experts collectively had full discretion over
the report's content.; 93) Performance Trade-offs of High Order Meshless Approximation on
  Distributed Memory Systems; Meshless methods approximate operators in a specific node as a weighted sum
of values in its neighbours. Higher order approximations of derivatives provide
more accurate solutions with better convergence characteristics, but they come
at the cost of including more neighbours. On the accuracy-per-compute time
basis we know that increasing the approximation order is beneficial for a
shared memory computer, but there is additional communication overhead when
problems become too large and we have to resort to distributed memory systems.
Meshless nodes are divided between systems in spatially coherent subdomains
with approximations at their edges requiring neighbouring value exchange.
Performance optimization is then a balancing act between minimizing the
required number of communicated neighbours by lowering the approximation order
or increasing it to enable faster convergence. We use the radial basis
function-generated finite difference method (RBF-FD) to approximate the
derivatives that we use to solve the Poisson equation with an explicit
iterative scheme. Inter-system communication is provided by Open MPI, while
OpenMP is used for intra-system parallelisation. We perform the analysis on a
homogenous CPU-based cluster where we examine the behaviour and attempt to
determine the optimal parameterisation with the goal of minimizing the
computational time to reach a desired accuracy.; 94) Formalising Propositional Information via Implication Hypergraphs; This work introduces a framework for quantifying the information content of
logical propositions through the use of implication hypergraphs. We posit that
a proposition's informativeness is primarily determined by its relationships
with other propositions -- specifically, the extent to which it implies or
derives other propositions. To formalize this notion, we develop a framework
based on implication hypergraphs, that seeks to capture these relationships.
Within this framework, we define propositional information, derive some key
properties, and illustrate the concept through examples. While the approach is
broadly applicable, mathematical propositions emerge as an ideal domain for its
application due to their inherently rich and interconnected structure. We
provide several examples to illustrate this and subsequently discuss the
limitations of the framework, along with suggestions for potential refinements.; 95) CRSet: Non-Interactive Verifiable Credential Revocation with Metadata
  Privacy for Issuers and Everyone Else; Like any digital certificate, Verifiable Credentials (VCs) require a way to
revoke them in case of an error or key compromise. Existing solutions for VC
revocation, most prominently Bitstring Status List, are not viable for many use
cases since they leak the issuer's behavior, which in turn leaks internal
business metrics. For instance, exact staff fluctuation through issuance and
revocation of employee IDs. We introduce CRSet, a revocation mechanism that
allows an issuer to encode revocation information for years worth of VCs as a
Bloom filter cascade. Padding is used to provide deniability for issuer
metrics. Issuers periodically publish this filter cascade on a decentralized
storage system. Relying Parties (RPs) can download it to perform any number of
revocation checks locally. Compared to existing solutions, CRSet protects the
metadata of subject, RPs, and issuer equally. At the same time, it is
non-interactive, making it work with wallet devices having limited hardware
power and drop-in compatible with existing VC exchange protocols and wallet
applications. We present a prototype using the Ethereum blockchain as
decentralized storage. The recently introduced blob-carrying transactions,
enabling cheaper data writes, allow us to write each CRSet directly to the
chain. We built software for issuers and RPs that we successfully tested
end-to-end with an existing publicly available wallet agents and the OpenID for
Verifiable Credentials protocols. Storage and bandwidth costs paid by issuers
and RP are higher than for Bitstring Status List, but still manageable at
around 1 MB for an issuer issuing hundreds of thousands of VCs annually and
covering decades.; 96) PriFFT: Privacy-preserving Federated Fine-tuning of Large Language
  Models via Function Secret Sharing; Fine-tuning large language models (LLMs) raises privacy concerns due to the
risk of exposing sensitive training data. Federated learning (FL) mitigates
this risk by keeping training samples on local devices, but recent studies show
that adversaries can still infer private information from model updates in FL.
Additionally, LLM parameters are typically shared publicly during federated
fine-tuning, while developers are often reluctant to disclose these parameters,
posing further security challenges. Inspired by the above problems, we propose
PriFFT, a privacy-preserving federated fine-tuning mechanism, to protect both
the model updates and parameters. In PriFFT, clients and the server share model
inputs and parameters by secret sharing, performing secure fine-tuning on
shared values without accessing plaintext data. Due to considerable LLM
parameters, privacy-preserving federated fine-tuning invokes complex secure
calculations and requires substantial communication and computation resources.
To optimize the efficiency of privacy-preserving federated fine-tuning of LLMs,
we introduce function secret-sharing protocols for various operations,
including reciprocal calculation, tensor products, natural exponentiation,
softmax, hyperbolic tangent, and dropout. The proposed protocols achieve up to
4.02X speed improvement and reduce 7.19X communication overhead compared to the
implementation based on existing secret sharing methods. Besides, PriFFT
achieves a 2.23X speed improvement and reduces 4.08X communication overhead in
privacy-preserving fine-tuning without accuracy drop compared to the existing
secret sharing methods.; 97) Beyond Homes scaling: disorder, the Planckian bound and a new
  universality; Beginning with high-$T_c$ cuprate materials, it has been observed that many
superconductors exhibit so-called ""Homes scaling"", in which the
zero-temperature superfluid density, $\rho_{s0}$, is proportional to the
product of the normal-state dc conductivity and the superconducting transition
temperature, $\sigma_\mathrm{dc} T_c$. For conventional, s-wave
superconductors, such scaling has been shown to be a natural consequence of
elastic-scattering disorder, not only in the extreme dirty limit but across a
broad range of scattering parameters. Here we show that when an analogous
calculation is carried out for elastic scattering in d-wave superconductors, a
stark contrast emerges, with $\rho_{s0} \propto \left(\sigma_\mathrm{dc} T_c
\right)^2$ in the dirty limit, in apparent violation of Homes scaling. Within a
simple approximate Migdal--Eliashberg treatment of inelastic scattering, we
show how Homes scaling is recovered. The normal-state behavior of near
optimally doped cuprates is dominated by inelastic scattering, but significant
deviations from Homes scaling occur for disorder-dominated cuprate systems,
such as underdoped YBCO and overdoped LSCO, and in very clean materials with
little inelastic scattering, such as Sr$_2$RuO$_4$. We present a revised
analysis where both axes of the original Homes scaling plot are normalized by
the Drude plasma weight, $\omega_{p,D}^2$, and show that new universal scaling
emerges, in which the superfluid fractions of dirty s-wave and dirty d-wave
superconductors coalesce to a single point at which normal-state scattering is
occurring at the Planckian bound. The combined result is a new tool for
classifying superconductors in terms of order parameter symmetry, as well as
scattering strength and character. Although our model starts from a
Fermi-liquid assumption it describes underdoped cuprates surprisingly well.; 98) Towards Experience Replay for Class-Incremental Learning in Fully-Binary
  Networks; Binary Neural Networks (BNNs) are a promising approach to enable Artificial
Neural Network (ANN) implementation on ultra-low power edge devices. Such
devices may compute data in highly dynamic environments, in which the classes
targeted for inference can evolve or even novel classes may arise, requiring
continual learning. Class Incremental Learning (CIL) is a common type of
continual learning for classification problems, that has been scarcely
addressed in the context of BNNs. Furthermore, most of existing BNNs models are
not fully binary, as they require several real-valued network layers, at the
input, the output, and for batch normalization. This paper goes a step further,
enabling class incremental learning in Fully-Binarized NNs (FBNNs) through four
main contributions. We firstly revisit the FBNN design and its training
procedure that is suitable to CIL. Secondly, we explore loss balancing, a
method to trade-off the performance of past and current classes. Thirdly, we
propose a semi-supervised method to pre-train the feature extractor of the FBNN
for transferable representations. Fourthly, two conventional CIL methods, \ie,
Latent and Native replay, are thoroughly compared. These contributions are
exemplified first on the CIFAR100 dataset, before being scaled up to address
the CORE50 continual learning benchmark. The final results based on our 3Mb
FBNN on CORE50 exhibit at par and better performance than conventional
real-valued larger NN models.; 99) Natural Language Processing and Deep Learning Models to Classify Phase
  of Flight in Aviation Safety Occurrences; The air transport system recognizes the criticality of safety, as even minor
anomalies can have severe consequences. Reporting accidents and incidents play
a vital role in identifying their causes and proposing safety recommendations.
However, the narratives describing pre-accident events are presented in
unstructured text that is not easily understood by computer systems.
Classifying and categorizing safety occurrences based on these narratives can
support informed decision-making by aviation industry stakeholders. In this
study, researchers applied natural language processing (NLP) and artificial
intelligence (AI) models to process text narratives to classify the flight
phases of safety occurrences. The classification performance of two deep
learning models, ResNet and sRNN was evaluated, using an initial dataset of
27,000 safety occurrence reports from the NTSB. The results demonstrated good
performance, with both models achieving an accuracy exceeding 68%, well above
the random guess rate of 14% for a seven-class classification problem. The
models also exhibited high precision, recall, and F1 scores. The sRNN model
greatly outperformed the simplified ResNet model architecture used in this
study. These findings indicate that NLP and deep learning models can infer the
flight phase from raw text narratives, enabling effective analysis of safety
occurrences.; 100) Towards Trustworthy Retrieval Augmented Generation for Large Language
  Models: A Survey; Retrieval-Augmented Generation (RAG) is an advanced technique designed to
address the challenges of Artificial Intelligence-Generated Content (AIGC). By
integrating context retrieval into content generation, RAG provides reliable
and up-to-date external knowledge, reduces hallucinations, and ensures relevant
context across a wide range of tasks. However, despite RAG's success and
potential, recent studies have shown that the RAG paradigm also introduces new
risks, including robustness issues, privacy concerns, adversarial attacks, and
accountability issues. Addressing these risks is critical for future
applications of RAG systems, as they directly impact their trustworthiness.
Although various methods have been developed to improve the trustworthiness of
RAG methods, there is a lack of a unified perspective and framework for
research in this topic. Thus, in this paper, we aim to address this gap by
providing a comprehensive roadmap for developing trustworthy RAG systems. We
place our discussion around five key perspectives: reliability, privacy,
safety, fairness, explainability, and accountability. For each perspective, we
present a general framework and taxonomy, offering a structured approach to
understanding the current challenges, evaluating existing solutions, and
identifying promising future research directions. To encourage broader adoption
and innovation, we also highlight the downstream applications where trustworthy
RAG systems have a significant impact.",0.14285714285714285,1.0
2411.00609,applied,2411.00609-pos1-4,"Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype
  Identification Using a Novel AUROC Loss Function for Convolutional Neural
  Networks; Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016).",2411.00609-pos2-4,"Pediatric low-grade glioma: State-of-the-art and ongoing challenges; Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%–40% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence.",74,"['1', '5', '13', '10', '3', '4', '17', '21', '2', '6']","The first candidate paper, 'Will Systems of LLM Agents Cooperate: An Investigation into a Social Dilemma', is highly relevant as it explores collective behavior in AI agents, which can be applied to enhance the non-invasive diagnostic capabilities of CNNs for pediatric low-grade neuroepithelial tumors by integrating cooperative learning mechanisms. This aligns with the goal of improving subtype identification through advanced computational techniques. The subsequent papers on multimodal systems, knowledge-driven intelligence, and AI applications in healthcare further support the integration of AI in medical diagnostics, enhancing the overall multidisciplinary approach of the research idea.","1) Will Systems of LLM Agents Cooperate: An Investigation into a Social
  Dilemma; As autonomous agents become more prevalent, understanding their collective
behaviour in strategic interactions is crucial. This study investigates the
emergent cooperative tendencies of systems of Large Language Model (LLM) agents
in a social dilemma. Unlike previous research where LLMs output individual
actions, we prompt state-of-the-art LLMs to generate complete strategies for
iterated Prisoner's Dilemma. Using evolutionary game theory, we simulate
populations of agents with different strategic dispositions (aggressive,
cooperative, or neutral) and observe their evolutionary dynamics. Our findings
reveal that different LLMs exhibit distinct biases affecting the relative
success of aggressive versus cooperative strategies. This research provides
insights into the potential long-term behaviour of systems of deployed
LLM-based autonomous agents and highlights the importance of carefully
considering the strategic environments in which they operate.; 2) Martian atmospheric disturbances from orbital images and surface
  pressure at Jezero Crater, Mars, during Martian Year 36; We present a study of atmospheric disturbances at Jezero Crater, Mars, using
ground-based measurements of surface pressure by the Perseverance rover in
combination with orbital images from the Mars Express and Mars Reconnaissance
Orbiter missions. The study starts at Ls $\sim$ 13.3{\deg} in MY36 (March 6th,
2021) and extends up to Ls $\sim$ 30.3{\deg} in MY37 (February 28th, 2023). We
focus on the characterization of the major atmospheric phenomena at synoptic
and planetary-scales. These are the thermal tides (measured up to the sixth
component), long-period pressure oscillations (periods > 1 sol), the Aphelion
Cloud Belt, and the occasional development of regional dust storms over Jezero.
We present the seasonal evolution of the amplitudes and phases of the thermal
tides and their relation with the atmospheric dust content (optical depth).
Three regional dust storms and one polar storm extending over Jezero produced
an increase in the diurnal and semidiurnal amplitudes but resulted in inverse
responses in their phases. We show that the primary regular wave activity is
due to baroclinic disturbances with periods of 2-4 sols and amplitudes $\sim$
1-15 Pa increasing with dust content, in good agreement with theoretical
predictions by model calculations. The spacecraft images show a number of
arc-shaped, spiral and irregular cyclonic vortices, traced by dust and clouds
at the edge of the North Polar Cap, that could be behind some of the pressure
oscillations measured at Jezero.; 3) The null condition in elastodynamics leads to non-uniqueness; We consider the Cauchy problem for the system of elastodynamic equations in
two dimensions. Specifically, we focus on materials characterized by a null
condition imposed on the quadratic part of the nonlinearity. We can construct
non-zero weak solutions $u \in C^1([0, T] \times \mathbb{T}^2)$ that emanate
from zero initial data. The proof relies on the convex integration scheme. By
exploiting the characteristic double wave speeds of the equations, we construct
a new class of building blocks. This work extends the application of convex
integration techniques to hyperbolic systems with a null condition and reveals
the rich solution structure in nonlinear elastodynamics.; 4) Incoherent horizontal emittance growth due to the interplay of beam-beam
  and longitudinal wakefield in crab-waist colliders; In this paper, we investigate quadrupolar sychrobetatron resonances caused by
beam-beam collisions and their interplay with longitudinal wakefields in the
context of crab-waist colliders. We present a comprehensive theoretical review
of the established theory of sychrobetatron resonances and extend the formalism
to explore horizontal sychrobetatron resonances specific to crab-waist
colliders. As a case study, we examine incoherent horizontal emittance growth
at the SuperKEKB and demonstrate through simulations that the interplay between
beam-beam and longitudinal wakefields leads to a horizontal blowup of the bunch
size and that the study of the dynamics can be reduced to the
horizontal-longitudinal plane, independent of the motion in the vertical
dimension. We present extensive simulation results using the codes BBWS,
PyHEADTAIL and Xsuite, connect our analytical findings with these findings, and
propose strategies to mitigate horizontal blowup.; 5) COSINT-Agent: A Knowledge-Driven Multimodal Agent for Chinese Open
  Source Intelligence; Open Source Intelligence (OSINT) requires the integration and reasoning of
diverse multimodal data, presenting significant challenges in deriving
actionable insights. Traditional approaches, including multimodal large
language models (MLLMs), often struggle to infer complex contextual
relationships or deliver comprehensive intelligence from unstructured data
sources. In this paper, we introduce COSINT-Agent, a knowledge-driven
multimodal agent tailored to address the challenges of OSINT in the Chinese
domain. COSINT-Agent seamlessly integrates the perceptual capabilities of
fine-tuned MLLMs with the structured reasoning power of the Entity-Event-Scene
Knowledge Graph (EES-KG). Central to COSINT-Agent is the innovative EES-Match
framework, which bridges COSINT-MLLM and EES-KG, enabling systematic
extraction, reasoning, and contextualization of multimodal insights. This
integration facilitates precise entity recognition, event interpretation, and
context retrieval, effectively transforming raw multimodal data into actionable
intelligence. Extensive experiments validate the superior performance of
COSINT-Agent across core OSINT tasks, including entity recognition, EES
generation, and context matching. These results underscore its potential as a
robust and scalable solution for advancing automated multimodal reasoning and
enhancing the effectiveness of OSINT methodologies.; 6) New Limits on Ultralight Axionlike Dark Matter; New limits on the axion-nucleon coupling over the axion mass region $10^{-24}
\leq m_a \leq 5 \times 10^{-21}$ eV are derived by reanalyzing data from
laboratory measurements on Lorentz and $CPT$ violation. These results establish
the first laboratory constraints on the axion-nucleon coupling for axion masses
below $10^{-22}$ eV. For $10^{-22} \leq m_a \leq 5 \times 10^{-21}$ eV, the
results improve upon previous laboratory limits by more than 3 orders of
magnitude, exceeding for the first time the astrophysical limits from supernova
SN1987A cooling. For the axion mass range of interest corresponding to ultralow
frequencies, the crucial initial phase of the axion field is considered.
Furthermore, the obtained limits are nearly equivalent to those projected for a
recently proposed experiment employing high-intensity neutron beams at the
European Spallation Source [Phys. Rev. Lett. 133, 181001 (2024)]. For an
alternative type of axion-nucleon interaction, the quadratic wind coupling, the
constraints exceed the current best results by approximately 2 orders of
magnitude.; 7) A Case Study on Model Checking and Runtime Verification for Awkernel; In operating system development, concurrency poses significant challenges. It
is difficult for humans to manually review concurrent behaviors or to write
test cases covering all possible executions, often resulting in critical bugs.
Preemption in schedulers serves as a typical example. This paper proposes a
development method for concurrent software, such as schedulers. Our method
incorporates model checking as an aid for tracing code, simplifying the
analysis of concurrent behavior; we refer to this as model checking-assisted
code review. While this approach aids in tracing behaviors, the accuracy of the
results is limited because of the semantics gap between the modeling language
and the programming language. Therefore, we also introduce runtime verification
to address this limitation in model checking-assisted code review. We applied
our approach to a real-world operating system, Awkernel, as a case study. This
new operating system, currently under development for autonomous driving, is
designed for preemptive task execution using asynchronous functions in Rust.
After implementing our method, we identified several bugs that are difficult to
detect through manual reviews or simple tests.; 8) Magnetic Equivariant K-theory; We present the fundamental properties of the K-theory groups of complex
vector bundles endowed with actions of magnetic groups. In this work we show
that the magnetic equivariant K-theory groups define an equivariant cohomology
theory, we determine its coefficients, we show Bott's, Thom's and the degree
shift isomorphism, we present the Atiyah-Hirzeburh spectral sequence, and we
explicitly calculate two magnetic equivariant K-theory groups in order to
showcase its applicability. These magnetic equivariant K-theory groups are
relevant in condensed matter physics since they provide topological invariants
of gapped Hamiltonians in magnetic crystals.; 9) A Bregman ADMM for Bethe variational problem; In this work, we propose a novel Bregman ADMM with nonlinear dual update to
solve the Bethe variational problem (BVP), a key optimization formulation in
graphical models and statistical physics. Our algorithm provides rigorous
convergence guarantees, even if the objective function of BVP is non-convex and
non-Lipschitz continuous on the boundary. A central result of our analysis is
proving that the entries in local minima of BVP are strictly positive,
effectively resolving non-smoothness issues caused by zero entries. Beyond
theoretical guarantees, the algorithm possesses high level of separability and
parallelizability to achieve highly efficient subproblem computation. Our
Bregman ADMM can be easily extended to solve the quantum Bethe variational
problem. Numerical experiments are conducted to validate the effectiveness and
robustness of the proposed method. Based on this research, we have released an
open-source package of the proposed method at
https://github.com/TTYmath/BADMM-BVP.; 10) Revolutionizing Healthcare Record Management: Secure Documentation
  Storage and Access through Advanced Blockchain Solutions; Integrating blockchain technology into healthcare systems presents a
transformative approach to documenting, storing, and accessing electronic
health records (EHRs). This research introduces a novel blockchain-based EHR
system designed to significantly enhance security, scalability, and
accessibility compared to existing solutions. Current systems primarily utilize
SHA-256 for security and either IPFS or centralized storage, which, while
effective, have limitations in providing comprehensive data integrity and
security. The proposed system leverages a hybrid security algorithm combining
Argon2 and AES and integrates a hybrid storage and consensus mechanism
utilizing IPFS and PBFT. This multifaceted approach ensures robust encryption,
efficient consensus, and high fault tolerance. Furthermore, the system
incorporates Multi-Factor Authentication (MFA) to safeguard against
unauthorized access. It utilizes advanced blockchain tools like MetaMask,
Ganache, and Truffle to facilitate seamless interaction with the decentralized
network. Simulation results demonstrate that this system offers superior
protection against data breaches and enhances operational efficiency.
Specifically, the proposed hybrid model substantially improves data integrity,
consensus efficiency, fault tolerance, data availability, latency, bandwidth
utilization, throughput, memory usage, and CPU usage across various healthcare
applications. To validate the performance and security of the proposed system,
comprehensive analyses were conducted using real-world healthcare scenarios.
The findings highlight the significant advantages of the blockchain-based EHR
system, emphasizing its potential to revolutionize healthcare data management
by ensuring secure, reliable, and efficient handling of sensitive medical
information.; 11) The Gordon-Litherland pairing and its many applications; Gordon and Litherland's paper $\textit{On the Signature of a link}$
introduced a bilinear form that simultaneously unifies both the quadratic forms
of Trotter and Goeritz. This remarkable pairing of combinatorics and topology
has had widespread application in low-dimensional topology. In this expository
note, we give a picture proof (via Kirby diagrams) of their main result and
discuss the numerous ways their theorem has been put to good use.; 12) Decentralized Online Ensembles of Gaussian Processes for Multi-Agent
  Systems; Flexible and scalable decentralized learning solutions are fundamentally
important in the application of multi-agent systems. While several recent
approaches introduce (ensembles of) kernel machines in the distributed setting,
Bayesian solutions are much more limited. We introduce a fully decentralized,
asymptotically exact solution to computing the random feature approximation of
Gaussian processes. We further address the choice of hyperparameters by
introducing an ensembling scheme for Bayesian multiple kernel learning based on
online Bayesian model averaging. The resulting algorithm is tested against
Bayesian and frequentist methods on simulated and real-world datasets.; 13) MAP: Evaluation and Multi-Agent Enhancement of Large Language Models for
  Inpatient Pathways; Inpatient pathways demand complex clinical decision-making based on
comprehensive patient information, posing critical challenges for clinicians.
Despite advancements in large language models (LLMs) in medical applications,
limited research focused on artificial intelligence (AI) inpatient pathways
systems, due to the lack of large-scale inpatient datasets. Moreover, existing
medical benchmarks typically concentrated on medical question-answering and
examinations, ignoring the multifaceted nature of clinical decision-making in
inpatient settings. To address these gaps, we first developed the Inpatient
Pathway Decision Support (IPDS) benchmark from the MIMIC-IV database,
encompassing 51,274 cases across nine triage departments and 17 major disease
categories alongside 16 standardized treatment options. Then, we proposed the
Multi-Agent Inpatient Pathways (MAP) framework to accomplish inpatient pathways
with three clinical agents, including a triage agent managing the patient
admission, a diagnosis agent serving as the primary decision maker at the
department, and a treatment agent providing treatment plans. Additionally, our
MAP framework includes a chief agent overseeing the inpatient pathways to guide
and promote these three clinician agents. Extensive experiments showed our MAP
improved the diagnosis accuracy by 25.10% compared to the state-of-the-art LLM
HuatuoGPT2-13B. It is worth noting that our MAP demonstrated significant
clinical compliance, outperforming three board-certified clinicians by 10%-12%,
establishing a foundation for inpatient pathways systems.; 14) Energy-Efficient Flat Precoding for MIMO Systems; This paper addresses the suboptimal energy efficiency of conventional digital
precoding schemes in multiple-input multiple-output (MIMO) systems. Through an
analysis of the power amplifier (PA) output power distribution associated with
conventional precoders, it is observed that these power distributions can be
quite uneven, resulting in large PA backoff (thus low efficiency) and high
power consumption. To tackle this issue, we propose a novel approach called
flat precoding, which aims to control the flatness of the power distribution
within a desired interval. In addition to reducing PA power consumption, flat
precoding offers the advantage of requiring smaller saturation levels for PAs,
which reduces the size of PAs and lowers the cost. To incorporate the concept
of flat power distribution into precoding design, we introduce a new
lower-bound per-antenna power constraint alongside the conventional sum power
constraint and the upper-bound per-antenna power constraint. By adjusting the
lower-bound and upper-bound values, we can effectively control the level of
flatness in the power distribution. We then seek to find a flat precoder that
satisfies these three sets of constraints while maximizing the weighted sum
rate (WSR). In particular, we develop efficient algorithms to design weighted
minimum mean squared error (WMMSE) and zero-forcing (ZF)-type precoders with
controllable flatness features that maximize WSR. Numerical results demonstrate
that complete flat precoding approaches, where the power distribution is a
straight line, achieve the best trade-off between spectral efficiency and
energy efficiency for existing PA technologies. We also show that the proposed
ZF and WMMSE precoding methods can approach the performance of their
conventional counterparts with only the sum power constraint, while
significantly reducing PA size and power consumption.; 15) Pseudo-spectra of multivariate inhomogeneous spatial point processes; In this article, we propose a spectral method for multivariate inhomogeneous
spatial point processes. A key ingredient is utilizing the asymptotic behavior
of the periodogram. The periodogram is an asymptotically unbiased estimator of
the spectrum of a second-order stationary point process. By extending this
property, we show that under inhomogeneity, the expectation of the periodogram
also converges to a matrix-valued function, which we refer to as the
pseudo-spectrum. The pseudo-spectrum shares similar properties with the
spectrum of stationary processes and can be interpreted using local parameters.
We derive a consistent estimator of the pseudo-spectrum through kernel
smoothing and propose two bandwidth selection methods. The performance and
utility of our frequency domain methods are illustrated through simulation
studies and a real data analysis of rainforest data.; 16) Conflicts of Interest in Published NLP Research 2000-2024; Natural Language Processing research is increasingly reliant on large scale
data and computational power. Many achievements in the past decade resulted
from collaborations with the tech industry. But an increasing entanglement of
academic research and industry interests leads to conflicts of interest. We
assessed published NLP research from 2000-2024 and labeled author affiliations
as academic or industry-affiliated to measure conflicts of interest. Overall
27.65% of the papers contained at least one industry-affiliated author. That
figure increased substantially with more than 1 in 3 papers having a conflict
of interest in 2024. We identify top-tier venues (ACL, EMNLP) as main drivers
for that effect. The paper closes with a discussion and a simple, concrete
suggestion for the future.; 17) Optimizing Ansatz Design in Quantum Generative Adversarial Networks
  Using Large Language Models; We present a novel approach for improving the design of ansatzes in Quantum
Generative Adversarial Networks (qGANs) by leveraging Large Language Models
(LLMs). By combining the strengths of LLMs with qGANs, our approach iteratively
refines ansatz structures to improve accuracy while reducing circuit depth and
the number of parameters. This study paves the way for further exploration in
AI-driven quantum algorithm design. The flexibility of our proposed workflow
extends to other quantum variational algorithms, providing a general framework
for optimizing quantum circuits in a variety of quantum computing tasks.; 18) Natural Language Processing and Deep Learning Models to Classify Phase
  of Flight in Aviation Safety Occurrences; The air transport system recognizes the criticality of safety, as even minor
anomalies can have severe consequences. Reporting accidents and incidents play
a vital role in identifying their causes and proposing safety recommendations.
However, the narratives describing pre-accident events are presented in
unstructured text that is not easily understood by computer systems.
Classifying and categorizing safety occurrences based on these narratives can
support informed decision-making by aviation industry stakeholders. In this
study, researchers applied natural language processing (NLP) and artificial
intelligence (AI) models to process text narratives to classify the flight
phases of safety occurrences. The classification performance of two deep
learning models, ResNet and sRNN was evaluated, using an initial dataset of
27,000 safety occurrence reports from the NTSB. The results demonstrated good
performance, with both models achieving an accuracy exceeding 68%, well above
the random guess rate of 14% for a seven-class classification problem. The
models also exhibited high precision, recall, and F1 scores. The sRNN model
greatly outperformed the simplified ResNet model architecture used in this
study. These findings indicate that NLP and deep learning models can infer the
flight phase from raw text narratives, enabling effective analysis of safety
occurrences.; 19) Analysis of the weak lensing mass-richness relation of redMaPPer
  clusters in the LSST DESC DC2 simulations; Cluster scaling relations are key ingredients in cluster abundance-based
cosmological studies. In optical cluster cosmology, weak gravitational lensing
has proven to be a powerful tool to constrain the cluster mass-richness
relation. This work is conducted as part of the Dark Energy Science
Collaboration (DESC), which aims to analyze the Legacy Survey of Space and Time
(LSST) of Vera C. Rubin Observatory, starting in 2026. Weak lensing-inferred
cluster properties, such as mass, suffer from several sources of bias. In this
paper, we aim to test the impact of modeling choices and observational
systematics in cluster lensing on the inference of the mass-richness relation.
We constrain the mass-richness relation of 3,600 clusters detected by the
redMaPPer algorithm in the cosmoDC2 extra-galactic mock catalog (covering $440$
deg$^2$) of the LSST DESC DC2 simulation, using number count measurements and
stacked weak lensing profiles in several intervals of richness ($20 \leq
\lambda \leq 200$) and redshift ($0.2 \leq z \leq 1$). By modeling the mean of
the scaling relation as $\langle \ln \lambda|M_{\rm 200c}, z\rangle =
\ln\lambda_0 + \mu_z\log[(1+z)/(1+0.5)] + \mu_m[\log_{10}(M_{\rm 200c}) -
14.3]$, our baseline constraints are $\ln\lambda_0 = 3.37\pm 0.03$, $\mu_z =
0.08\pm 0.07$ and $\mu_m = 2.18 \pm 0.07$. We have found that, for a LSST-like
source galaxy density, our constraints are robust to a change in
concentration-mass relation and dark matter density profile modeling choices,
when source redshifts and shapes are perfectly known. We have found that
photometric redshift uncertainties can introduce bias at the $1\sigma$ level,
which can be mitigated by an overall correcting factor, fitted jointly with
scaling parameters. We find that including positive shear-richness covariance
in the fit shifts the results by up to 0.5$\sigma$.; 20) Analysis of harmonic average method for interface problems with
  discontinuous solutions and fluxes; Harmonic average method has been widely utilized to deal with heterogeneous
coefficients in solving differential equations. One remarkable advantage of the
harmonic averaging method is that no derivative of the coefficient is needed.
Furthermore, the coefficient matrix of the finite difference equations is an
M-matrix which guarantees the stability of the algorithm. It has been
numerically observed but not theoretically proved that the method produces
second order pointwise accuracy when the solution and flux are continuous even
if the coefficient has finite discontinuities for which the method is
inconsistent ($O(1)$ in the local truncation errors). It has been believed that
there are some fortunate error cancellations. The harmonic average method does
not converge when the solution or the flux has finite discontinuities. In this
paper, not only we rigorously prove the second order convergence of the
harmonic averaging method for one-dimensional interface problem when the
coefficient has a finite discontinuities and the solution and the flux are
continuous, but also proposed an {\em improved harmonic average method} that is
also second order accurate (in the $L^{\infty}$ norm), which allows
discontinuous solutions and fluxes along with the discontinuous coefficients.
The key in the convergence proof is the construction of the Green's function.
The proof shows how the error cancellations occur in a subtle way. Numerical
experiments in both 1D and 2D confirmed the theoretical proof of the improved
harmonic average method.; 21) Unified Enhancement of the Generalization and Robustness of Language
  Models via Bi-Stage Optimization; Neural network language models (LMs) are confronted with significant
challenges in generalization and robustness. Currently, many studies focus on
improving either generalization or robustness in isolation, without methods
addressing both aspects simultaneously, which presents a significant challenge
in developing LMs that are both robust and generalized. In this paper, we
propose a bi-stage optimization framework to uniformly enhance both the
generalization and robustness of LMs, termed UEGR. Specifically, during the
forward propagation stage, we enrich the output probability distributions of
adversarial samples by adaptive dropout to generate diverse sub models, and
incorporate JS divergence and adversarial losses of these output distributions
to reinforce output stability. During backward propagation stage, we compute
parameter saliency scores and selectively update only the most critical
parameters to minimize unnecessary deviations and consolidate the model's
resilience. Theoretical analysis shows that our framework includes gradient
regularization to limit the model's sensitivity to input perturbations and
selective parameter updates to flatten the loss landscape, thus improving both
generalization and robustness. The experimental results show that our method
significantly improves the generalization and robustness of LMs compared to
other existing methods across 13 publicly available language datasets,
achieving state-of-the-art (SOTA) performance.; 22) Prime Identification and Composite Filtering Using GM-(n+1) Sequences; This paper presents a distinctive prime detection approach. This method use
GM-(n+1) sequences to effectively eliminate complex numbers. The sequences,
which consist of odd a number of (n+1), exclude all components except for the
initial prime integer. Only the first prime number is presented. This research
proposes an approach using this model to identify exceptional candidates and
examine their distribution. This study examines the interconnections among the
laws of division, basic gaps, and their applications in analytical procedures.
Computer studies may provide a novel perspective on the theory of prime
numbers, demonstrating the effectiveness of this approach in refining the
search space for primes.; 23) Asymptotic solutions of the boundary value problems for the singularly
  perturbed differential algebraic equations with a turning point; This paper deals with the boundary value problems for the singularly
perturbed differential-algebraic system of equations. The case of turning
points has been studied. The sufficient conditions for existence and uniqueness
of the solution of the boundary value problems for DAEs have been found. The
technique of constructing the asymptotic solutions has been developed; 24) ExposNet: A Deep Learning Framework for EMF Exposure Prediction in
  Complex Urban Environments; The prediction of the electric field (E-field) plays a crucial role in
monitoring radiofrequency electromagnetic field (RF-EMF) exposure induced by
cellular networks. In this paper, a deep learning framework is proposed to
predict E-field levels in complex urban environments. First, the measurement
campaign and publicly accessible databases used to construct the training
dataset are introduced, with a detailed explanation provided on how these
datasets are formulated and integrated to enhance their suitability for
Convolutional Neural Networks (CNNs)-based models. Then, the proposed model,
ExposNet, is presented, and its network architecture and workflow are
thoroughly explained. Two variations of the network structure are proposed, and
extensive experimental analyses are conducted, demonstrating that ExposNet
achieves good prediction accuracy with both configurations. Furthermore, the
generalization capability of the model is evaluated. The overall results
indicate that, despite being trained and tested on real-world measurements, the
model performs well and achieves better accuracy compared to previous studies.; 25) Exploration of VLMs for Driver Monitoring Systems Applications; In recent years, we have witnessed significant progress in emerging deep
learning models, particularly Large Language Models (LLMs) and Vision-Language
Models (VLMs). These models have demonstrated promising results, indicating a
new era of Artificial Intelligence (AI) that surpasses previous methodologies.
Their extensive knowledge and zero-shot capabilities suggest a paradigm shift
in developing deep learning solutions, moving from data capturing and algorithm
training to just writing appropriate prompts. While the application of these
technologies has been explored across various industries, including automotive,
there is a notable gap in the scientific literature regarding their use in
Driver Monitoring Systems (DMS). This paper presents our initial approach to
implementing VLMs in this domain, utilising the Driver Monitoring Dataset to
evaluate their performance and discussing their advantages and challenges when
implemented in real-world scenarios.; 26) PUGS: Zero-shot Physical Understanding with Gaussian Splatting; Current robotic systems can understand the categories and poses of objects
well. But understanding physical properties like mass, friction, and hardness,
in the wild, remains challenging. We propose a new method that reconstructs 3D
objects using the Gaussian splatting representation and predicts various
physical properties in a zero-shot manner. We propose two techniques during the
reconstruction phase: a geometry-aware regularization loss function to improve
the shape quality and a region-aware feature contrastive loss function to
promote region affinity. Two other new techniques are designed during
inference: a feature-based property propagation module and a volume integration
module tailored for the Gaussian representation. Our framework is named as
zero-shot physical understanding with Gaussian splatting, or PUGS. PUGS
achieves new state-of-the-art results on the standard benchmark of ABO-500 mass
prediction. We provide extensive quantitative ablations and qualitative
visualization to demonstrate the mechanism of our designs. We show the proposed
methodology can help address challenging real-world grasping tasks. Our codes,
data, and models are available at https://github.com/EverNorif/PUGS; 27) Nonadiabatic quantum kinetic equations and Dirac-Heisenberg-Wigner
  formalism for Schwinger pair production in time-varying electric fields with
  multiple components; The nonadiabatic quantum kinetic equations and Dirac-Heisenberg-Wigner
formalism for Schwinger pair production in a spatially uniform and time-varying
electric field with multiple components are derived and proven to be
equivalent. The relation between nonadiabatic and adiabatic quantum kinetic
equations is also established. By analyzing the time evolution of the
distribution functions of particles created in a circularly polarized Gaussian
pulse field with a subcycle structure, it is found that the nonadiabatic and
adiabatic distribution functions are the same after the field, with a
sufficient number of oscillation cycles, fades away. However, during the
presence of the field, the two distribution functions typically differ.
Nonetheless, the time evolution characteristics of the nonadiabatic and
adiabatic momentum distributions are similar. For instance, the number of
spirals is one less than the number of photons absorbed in both cases.
Furthermore, for a rapidly oscillating electric field, the nonadiabatic quantum
kinetic approaches may provide a more meaningful description of pair production
at intermediate times. These findings deepen our understanding of the
nonadiabatic quantum kinetic approaches and their application in pair
production.; 28) A Multimodal Physics-Informed Neural Network Approach for Mean Radiant
  Temperature Modeling; Outdoor thermal comfort is a critical determinant of urban livability,
particularly in hot desert climates where extreme heat poses challenges to
public health, energy consumption, and urban planning. Mean Radiant Temperature
($T_{mrt}$) is a key parameter for evaluating outdoor thermal comfort,
especially in urban environments where radiation dynamics significantly impact
human thermal exposure. Traditional methods of estimating $T_{mrt}$ rely on
field measurements and computational simulations, both of which are resource
intensive. This study introduces a Physics-Informed Neural Network (PINN)
approach that integrates shortwave and longwave radiation modeling with deep
learning techniques. By leveraging a multimodal dataset that includes
meteorological data, built environment characteristics, and fisheye
image-derived shading information, our model enhances predictive accuracy while
maintaining physical consistency. Our experimental results demonstrate that the
proposed PINN framework outperforms conventional deep learning models, with the
best-performing configurations achieving an RMSE of 3.50 and an $R^2$ of 0.88.
This approach highlights the potential of physics-informed machine learning in
bridging the gap between computational modeling and real-world applications,
offering a scalable and interpretable solution for urban thermal comfort
assessments.; 29) A new proof of superadditivity and of the density conjecture for
  Activated Random Walks on the line; In two recent works, Hoffman, Johnson and Junge proved the density
conjecture, the hockey stick conjecture and the ball conjecture for Activated
Random Walks in dimension 1, showing an equality between several different
definitions of the critical density of the model. This establishes a kind of
self-organized criticality, that was originally predicted for the Abelian
Sandpile Model. The proof of Hoffman, Johnson and Junge uses a comparison with
a percolation process, which exhibits a superadditivity property. In the
present note, we revisit their argument by providing a new proof of
superadditivity directly for Activated Random Walks, without relying on a
percolation process. The proof relies on a simple comparison between the
stabilization of two neighbouring segments and that of their union. We then
explain how this superaddivity property implies the three mentioned
conjectures. Yet, so far it does not seem that this approach yields as much
information as does the percolation technology developed by Hoffman, Johnson
and Junge, which yields an exponential concentration bound on the stationary
density, whereas the superadditivity property alone only ensures an exponential
bound on the lower tail.; 30) Low-cost Microfluidic Testbed for Molecular Communications with
  Integrated Hydrodynamic Gating and Screen-printed Sensors; Molecular Communications (MC), transferring information via chemical signals,
holds promise for transformative healthcare applications within the Internet of
Bio-Nano Things (IoBNT) framework. Despite promising advances toward practical
MC systems, progress has been constrained by experimental testbeds that are
costly, difficult to customize, and require labor-intensive fabrication. Here,
we address these challenges by introducing a low-cost ($\sim$\$1 per unit),
rapidly fabricated ($<$1 hour), and highly customizable microfluidic testbed
that integrates hydrodynamic gating and screen-printed potentiometric sensors.
This platform enables precise spatiotemporal control over chemical signals and
supports reconfigurable channel architectures along with on-demand sensor
functionalization. As a proof of concept, we demonstrate a pH-based MC system
combining a polyaniline (PANI)-functionalized sensor for real-time signal
detection with a programmable hydrodynamic gating architecture, patterned in a
double-sided adhesive tape, as the transmitter. By dynamically mixing
phosphate-buffered saline (PBS) with an acidic solution (pH 3), the testbed
reliably generates pH-encoded pulses. Experimental results confirm robust
control over pulse amplitude and pulse width, enabling the simulation of
end-to-end MC scenarios with 4-ary concentration shift keying (CSK) modulation.
By combining affordability and rapid prototyping without compromising
customizability, this platform is poised to accelerate the translation of MC
concepts into practical IoBNT applications.; 31) Target Tracking using Robust Sensor Motion Control; We consider the problem of tracking moving targets using mobile wireless
sensors (of possibly different types). This is a joint estimation and control
problem in which a tracking system must take into account both target and
sensor dynamics. We make minimal assumptions about the target dynamics, namely
only that their accelerations are bounded. We develop a control law that
determines the sensor motion control signals so as to maximize target
resolvability as the target dynamics evolve. The method is given a tractable
formulation that is amenable to an efficient search method and is evaluated in
a series of experiments involving both round-trip time based ranging and
Doppler frequency shift measurements; 32) Economic Censorship Games in Fraud Proofs; Optimistic rollups rely on fraud proofs -- interactive protocols executed on
Ethereum to resolve conflicting claims about the rollup's state -- to scale
Ethereum securely.
  To mitigate against potential censorship of protocol moves, fraud proofs
grant participants a significant time window, known as the challenge period, to
ensure their moves are processed on chain. Major optimistic rollups today set
this period at roughly one week, mainly to guard against strong censorship that
undermines Ethereum's own crypto-economic security. However, other forms of
censorship are possible, and their implication on optimistic rollup security is
not well understood.
  This paper considers economic censorship attacks, where an attacker censors
the defender's transactions by bribing block proposers. At each step, the
attacker can either censor the defender -- depleting the defender's time
allowance at the cost of the bribe -- or allow the current transaction through
while conserving funds for future censorship.
  We analyze three game theoretic models of these dynamics and determine the
challenge period length required to ensure the defender's success, as a
function of the number of required protocol moves and the players' available
budgets.; 33) An ordinal analysis of CM and its extensions; In arXiv:0905.1675, Nik Weaver proposed a novel intuitionistic formal theory
of third-order arithmetic as a formalisation of his philosophical position
known as mathematical conceptualism. In this paper, we will construct a
realisability model from the partial combinatory algebra of
$\Sigma^1_1$-definable partial functions and use it to provide an ordinal
analysis of this formal theory. Additionally, we will examine possible
extensions to this system by adding well-ordering axioms, which are briefly
mentioned but never thoroughly studied in Weaver's work. We aim to use the
realisability arguments to discuss how much such extensions constitute an
increase from the original theory's proof-theoretic strength.; 34) Sparse Binary Representation Learning for Knowledge Tracing; Knowledge tracing (KT) models aim to predict students' future performance
based on their historical interactions. Most existing KT models rely
exclusively on human-defined knowledge concepts (KCs) associated with
exercises. As a result, the effectiveness of these models is highly dependent
on the quality and completeness of the predefined KCs. Human errors in labeling
and the cost of covering all potential underlying KCs can limit model
performance.
  In this paper, we propose a KT model, Sparse Binary Representation KT
(SBRKT), that generates new KC labels, referred to as auxiliary KCs, which can
augment the predefined KCs to address the limitations of relying solely on
human-defined KCs. These are learned through a binary vector representation,
where each bit indicates the presence (one) or absence (zero) of an auxiliary
KC. The resulting discrete representation allows these auxiliary KCs to be
utilized in training any KT model that incorporates KCs. Unlike pre-trained
dense embeddings, which are limited to models designed to accept such vectors,
our discrete representations are compatible with both classical models, such as
Bayesian Knowledge Tracing (BKT), and modern deep learning approaches.
  To generate this discrete representation, SBRKT employs a binarization method
that learns a sparse representation, fully trainable via stochastic gradient
descent. Additionally, SBRKT incorporates a recurrent neural network (RNN) to
capture temporal dynamics and predict future student responses by effectively
combining the auxiliary and predefined KCs. Experimental results demonstrate
that SBRKT outperforms the tested baselines on several datasets and achieves
competitive performance on others. Furthermore, incorporating the learned
auxiliary KCs consistently enhances the performance of BKT across all tested
datasets.; 35) Lower bounds for Ramsey numbers of bounded degree hypergraphs; We prove that, for all $k \ge 3,$ and any integers $\Delta, n$ with $n \ge
\Delta,$ there exists a $k$-uniform hypergraph on $n$ vertices with maximum
degree at most $\Delta$ whose $4$-color Ramsey number is at least
$\mathrm{tw}_k(c_k \sqrt{\Delta}) \cdot n$, for some constant $c_k > 0$, where
$\mathrm{tw}_k$ denotes the tower function. This is tight up to the power of
$\Delta$ on top of the tower and extends a result of Graham, R\""{o}dl and
Ruci\'{n}ski for graphs.; 36) Gaussian basis set approach to one-loop self-energy; We report a method for the evaluation of the one-loop self-energy, to all
orders in the external binding field, using a Gaussian basis set expansion.
This choice of basis is motivated by its widespread use in molecular
calculations. For a one-electron atom, our results show excellent agreement
with those obtained using the exact Dirac--Coulomb wave functions. The
developed method can be of interest for high-precision studies of heavy
few-electron molecular systems, where the rigorous computation of QED
corrections is currently a formidable task.; 37) Flavor Constraints in a Generational Three Higgs Doublet Model; We propose a Three Higgs Doublet Model (3HDM) that goes beyond natural flavor
conservation and in which each of the three Higgs doublets couples mainly to a
single generation of fermions via non-standard Yukawa structures. A hierarchy
in the vacuum expectation values of the three Higgs doublets can partially
address the SM flavor puzzle. In light of the experimentally observed $125$ GeV
Higgs boson, we primarily work within a 3HDM alignment limit such that a
Standard Model-like Higgs is recovered. In order to reproduce the observed CKM
mixing among quarks, the neutral Higgs bosons of the theory necessarily mediate
flavor changing neutral currents at the tree level. We consider constraints
from neutral kaon, $B$ meson, and $D$ meson mixing as well as from the rare
leptonic decays $B_s/B^0/K_L\rightarrow\mu^+\mu^-/e^+e^-$. We identify regions
of parameter space in which the new physics Higgs bosons can be as light as a
TeV or even lighter.; 38) Resonant Drag Instabilities for Polydisperse Dust, I. The Acoustic
  Resonant Drag Instability; Dust grains embedded in gas flow give rise to a class of hydrodynamic
instabilities that can occur whenever there exists a relative velocity between
gas and dust. These instabilities have predominantly been studied for single
grain sizes, for which a strong interaction can be found between drifting dust
and a travelling gas wave, leading to fast-growing perturbations (growth rates
$\propto \sqrt{\mu}$) even at small dust-to-gas ratios $\mu$. They are called
resonant drag instabilities. We focus on the acoustic resonant drag
instability, which is potentially important in AGB star outflows, around
supernova remnants and star clusters in starburst galaxies. We study the
acoustic resonant drag instability, taking into account a continuous spectrum
of grain sizes, to determine whether it survives in the polydisperse regime and
how the resulting growth rates compare to the monodisperse case. We solve the
linear equations for a polydisperse fluid for the acoustic drag instability,
focusing on small dust-to-gas ratios. Size distributions of realistic width
turn the fast-growing perturbations $\propto \sqrt{\mu}$ of the monodisperse
limit into slower growing perturbations $\propto \mu$ due to the fact that the
backreaction on the gas involves an integration over the resonance.
Furthermore, the large wave numbers that grow fastest in the monodisperse
regime are stabilized by a size distribution, severely limiting the growth
rates in the polydisperse regime. The acoustic resonant drag instability turns
from a singularly perturbed problem in $\mu$ in the monodisperse limit into a
regular perturbation for a sufficiently wide size distribution. It can still
grow exponentially in the polydisperse regime, but at a slower pace compared to
the single size case.; 39) Revisiting Continuous p-Hub Location Problems with the L1 Metric; Motivated by emerging urban applications in commercial, public sector, and
humanitarian logistics, we revisit continuous $p$-hub location problems in
which several facilities must be located in a continuous space such that the
expected minimum Manhattan travel distance from a random service provider to a
random customer through exactly one hub facility is minimized. In this paper,
we begin by deriving closed-form results for a one-dimensional case and
two-dimensional cases with up to two hubs. Subsequently, a simulation-based
approximation method is proposed for more complex two-dimensional scenarios
with more than two hubs. Moreover, an extended problem with multiple service
providers is analyzed to reflect real-life service settings. Finally, we apply
our model and approximation method using publicly available data as a case
study to optimize the deployment of public-access automated external
defibrillators in Virginia Beach.; 40) Density-Matrix Embedding Based Multi-Configurational Perturbation Theory
  Approach to Single-Ion Magnets; Multi-configurational wave-function theory (MC-WFT) that combines complete
active space self-consistent field (CASSCF) approach with subsequent state
interaction (SI) treatment of spin-orbit coupling (SOC), abbreviated as
CASSCF-SO, plays important roles in microscopic understanding of single-ion
magnets (SIMs) with different central transition metal or lanthanide ions and
various coordination environments, but its application to SIMs with complex
structure is severely limited due to its highly demanding computational cost.
Density-matrix embedding theory (DMET) provides a systematic and mathematically
rigorous framework to combine low-level mean field approaches like Hartree-Fock
and high-level MC-WFT methods like CASSCF-SO, which is particularly promising
to SIMs. As a continuation of our previous work on DMET+CASSCF for $3d$ SIMs
(Ai, Sun, and Jiang, J. Phys. Chem. Lett. 2022, 13, 10627), we extend the
methodology by considering dynamic correlation on top of CASSCF using the
second-order $n$-electron valence perturbation theory (NEVPT2) in the DMET
framework, abbreviated as DMET+NEVPT2, and benchmark the accuracy of this
approach to molecular magnetic anisotropy in a set of typical transition metal
complexes. We found that DMET+NEVPT2 can give the results very close to
all-electron treatment, and can be systematically improved for higher accuracy
by expanding the region treated as the central cluster, while the computation
cost is dramatically reduced due to the reduction of the number of orbitals by
DMET construction. Our findings suggest that DMET is capable of accounting for
most of the dynamic correlation that is important for magnetic anisotropy in
typical SIMs, and can be useful for further high-accuracy spin-phonon study and
high-throughput computations.; 41) A reduction theorem for non-vanishing of Hochschild cohomology of block
  algebras and Happel's property; We show that any $p$-block algebra ($p$ is a prime) of a finite group with
realizable fusion system satisfies Happel's property. We obtain a reduction
theorem for the non-vanishing of the first Hochschild cohomology of block
algebras with non-trivial defect groups. Along the way we investigate this
problem for the blocks of some simple finite groups.; 42) Genome evolution in an endangered freshwater mussel; Nearly neutral theory predicts that evolutionary processes will differ in
small populations compared to large populations, a key point of concern for
endangered species. The nearly-neutral threshold, the span of neutral
variation, and the adaptive potential from new mutations all differ depending
on N_e. To determine how genomes respond in small populations, we have created
a reference genome for a US federally endangered IUCN Red List freshwater
mussel, Elliptio spinosa, and compare it to genetic variation for a common and
successful relative, Elliptio crassidens. We find higher rates of background
duplication rates in E. spinosa consistent with proposed theories of duplicate
gene accumulation according to nearly-neutral processes. Along with these
changes we observe fewer cases of adaptive gene family amplification in this
endangered species. However, TE content is not consistent with nearly-neutral
theory. We observe substantially less recent TE proliferation in the endangered
species with over 500 Mb of newly copied TEs in Elliptio crassidens. These
results suggest a more complex interplay between TEs and duplicate genes than
previously proposed for small populations. They further suggest that TEs and
duplications require greater attention in surveys of genomic health for
endangered species.; 43) The Role, Trends, and Applications of Machine Learning in Undersea
  Communication: A Bangladesh Perspective; The rapid evolution of machine learning (ML) has brought about groundbreaking
developments in numerous industries, not the least of which is in the area of
undersea communication. This domain is critical for applications like ocean
exploration, environmental monitoring, resource management, and national
security. Bangladesh, a maritime nation with abundant resources in the Bay of
Bengal, can harness the immense potential of ML to tackle the unprecedented
challenges associated with underwater communication. Beyond that, environmental
conditions are unique to the region: in addition to signal attenuation,
multipath propagation, noise interference, and limited bandwidth. In this
study, we address the necessity to bring ML into communication via undersea; it
investigates the latest technologies under the domain of ML in that respect,
such as deep learning and reinforcement learning, especially concentrating on
Bangladesh scenarios in the sense of implementation. This paper offers a
contextualized regional perspective by incorporating region-specific needs,
case studies, and recent research to propose a roadmap for deploying ML-driven
solutions to improve safety at sea, promote sustainable resource use, and
enhance disaster response systems. This research ultimately highlights the
promise of ML-powered solutions for transforming undersea communication,
leading to more efficient and cost-effective technologies that subsequently
contribute to both economic growth and environmental sustainability.; 44) Towards Zero Touch Networks: Cross-Layer Automated Security Solutions
  for 6G Wireless Networks; The transition from 5G to 6G mobile networks necessitates network automation
to meet the escalating demands for high data rates, ultra-low latency, and
integrated technology. Recently, Zero-Touch Networks (ZTNs), driven by
Artificial Intelligence (AI) and Machine Learning (ML), are designed to
automate the entire lifecycle of network operations with minimal human
intervention, presenting a promising solution for enhancing automation in 5G/6G
networks. However, the implementation of ZTNs brings forth the need for
autonomous and robust cybersecurity solutions, as ZTNs rely heavily on
automation. AI/ML algorithms are widely used to develop cybersecurity
mechanisms, but require substantial specialized expertise and encounter model
drift issues, posing significant challenges in developing autonomous
cybersecurity measures. Therefore, this paper proposes an automated security
framework targeting Physical Layer Authentication (PLA) and Cross-Layer
Intrusion Detection Systems (CLIDS) to address security concerns at multiple
Internet protocol layers. The proposed framework employs drift-adaptive online
learning techniques and a novel enhanced Successive Halving (SH)-based
Automated ML (AutoML) method to automatically generate optimized ML models for
dynamic networking environments. Experimental results illustrate that the
proposed framework achieves high performance on the public Radio Frequency (RF)
fingerprinting and the Canadian Institute for CICIDS2017 datasets, showcasing
its effectiveness in addressing PLA and CLIDS tasks within dynamic and complex
networking environments. Furthermore, the paper explores open challenges and
research directions in the 5G/6G cybersecurity domain. This framework
represents a significant advancement towards fully autonomous and secure 6G
networks, paving the way for future innovations in network automation and
cybersecurity.; 45) RewardSDS: Aligning Score Distillation via Reward-Weighted Sampling; Score Distillation Sampling (SDS) has emerged as an effective technique for
leveraging 2D diffusion priors for tasks such as text-to-3D generation. While
powerful, SDS struggles with achieving fine-grained alignment to user intent.
To overcome this, we introduce RewardSDS, a novel approach that weights noise
samples based on alignment scores from a reward model, producing a weighted SDS
loss. This loss prioritizes gradients from noise samples that yield aligned
high-reward output. Our approach is broadly applicable and can extend SDS-based
methods. In particular, we demonstrate its applicability to Variational Score
Distillation (VSD) by introducing RewardVSD. We evaluate RewardSDS and
RewardVSD on text-to-image, 2D editing, and text-to-3D generation tasks,
showing significant improvements over SDS and VSD on a diverse set of metrics
measuring generation quality and alignment to desired reward models, enabling
state-of-the-art performance. Project page is available at
https://itaychachy.github.io/reward-sds/.; 46) International AI Safety Report; The first International AI Safety Report comprehensively synthesizes the
current evidence on the capabilities, risks, and safety of advanced AI systems.
The report was mandated by the nations attending the AI Safety Summit in
Bletchley, UK. Thirty nations, the UN, the OECD, and the EU each nominated a
representative to the report's Expert Advisory Panel. A total of 100 AI experts
contributed, representing diverse perspectives and disciplines. Led by the
report's Chair, these independent experts collectively had full discretion over
the report's content.; 47) Learning A Zero-shot Occupancy Network from Vision Foundation Models via
  Self-supervised Adaptation; Estimating the 3D world from 2D monocular images is a fundamental yet
challenging task due to the labour-intensive nature of 3D annotations. To
simplify label acquisition, this work proposes a novel approach that bridges 2D
vision foundation models (VFMs) with 3D tasks by decoupling 3D supervision into
an ensemble of image-level primitives, e.g., semantic and geometric components.
As a key motivator, we leverage the zero-shot capabilities of vision-language
models for image semantics. However, due to the notorious ill-posed problem -
multiple distinct 3D scenes can produce identical 2D projections, directly
inferring metric depth from a monocular image in a zero-shot manner is
unsuitable. In contrast, 2D VFMs provide promising sources of relative depth,
which theoretically aligns with metric depth when properly scaled and offset.
Thus, we adapt the relative depth derived from VFMs into metric depth by
optimising the scale and offset using temporal consistency, also known as novel
view synthesis, without access to ground-truth metric depth. Consequently, we
project the semantics into 3D space using the reconstructed metric depth,
thereby providing 3D supervision. Extensive experiments on nuScenes and
SemanticKITTI demonstrate the effectiveness of our framework. For instance, the
proposed method surpasses the current state-of-the-art by 3.34% mIoU on
nuScenes for voxel occupancy prediction.; 48) Event Constrained Programming; In this paper, we present event constraints as a new modeling paradigm that
generalizes joint chance constraints from stochastic optimization to (1)
enforce a constraint on the probability of satisfying a set of constraints
aggregated via application-specific logic (constituting an event) and (2) to be
applied to general infinite-dimensional optimization (InfiniteOpt) problems
(i.e., time, space, and/or uncertainty domains). This new constraint class
offers significant modeling flexibility in posing InfiniteOpt constraints that
are enforced over a certain portion of their domain (e.g., to a certain
probability level), but can be challenging to reformulate/solve due to
difficulties in representing arbitrary logical conditions and specifying a
probabilistic measure on a collection of constraints. To address these
challenges, we derive a generalized disjunctive programming (GDP)
representation of event constrained optimization problems, which readily
enables us to pose logical event conditions in a standard form and allows us to
draw from a suite of GDP solution strategies that leverage the special
structure of this problem class. We also extend several approximation
techniques from the chance constraint literature to provide a means to
reformulate certain event constraints without the use of binary variables. We
illustrate these findings with case studies in stochastic optimal power flow,
dynamic disease control, and optimal 2D diffusion.; 49) Beyond Homes scaling: disorder, the Planckian bound and a new
  universality; Beginning with high-$T_c$ cuprate materials, it has been observed that many
superconductors exhibit so-called ""Homes scaling"", in which the
zero-temperature superfluid density, $\rho_{s0}$, is proportional to the
product of the normal-state dc conductivity and the superconducting transition
temperature, $\sigma_\mathrm{dc} T_c$. For conventional, s-wave
superconductors, such scaling has been shown to be a natural consequence of
elastic-scattering disorder, not only in the extreme dirty limit but across a
broad range of scattering parameters. Here we show that when an analogous
calculation is carried out for elastic scattering in d-wave superconductors, a
stark contrast emerges, with $\rho_{s0} \propto \left(\sigma_\mathrm{dc} T_c
\right)^2$ in the dirty limit, in apparent violation of Homes scaling. Within a
simple approximate Migdal--Eliashberg treatment of inelastic scattering, we
show how Homes scaling is recovered. The normal-state behavior of near
optimally doped cuprates is dominated by inelastic scattering, but significant
deviations from Homes scaling occur for disorder-dominated cuprate systems,
such as underdoped YBCO and overdoped LSCO, and in very clean materials with
little inelastic scattering, such as Sr$_2$RuO$_4$. We present a revised
analysis where both axes of the original Homes scaling plot are normalized by
the Drude plasma weight, $\omega_{p,D}^2$, and show that new universal scaling
emerges, in which the superfluid fractions of dirty s-wave and dirty d-wave
superconductors coalesce to a single point at which normal-state scattering is
occurring at the Planckian bound. The combined result is a new tool for
classifying superconductors in terms of order parameter symmetry, as well as
scattering strength and character. Although our model starts from a
Fermi-liquid assumption it describes underdoped cuprates surprisingly well.; 50) Variational Tail Bounds for Norms of Random Vectors and Matrices; We propose a variational tail bound for norms of random vectors under moment
assumptions on their one-dimensional marginals. We also propose a simplified
version of the bound that parametrizes the ``aggregating'' distribution in the
proposed variational bound by considering a certain pushforward of the Gaussian
distribution. Furthermore, we show that the proposed method recovers some of
the well-known bounds on norms of Gaussian random vectors, as well as a recent
concentration inequality for the spectral norm of sum of independent and
identically distributed positive semidefinite matrices.; 51) Video Summarisation with Incident and Context Information using
  Generative AI; The proliferation of video content production has led to vast amounts of
data, posing substantial challenges in terms of analysis efficiency and
resource utilization. Addressing this issue calls for the development of robust
video analysis tools. This paper proposes a novel approach leveraging
Generative Artificial Intelligence (GenAI) to facilitate streamlined video
analysis. Our tool aims to deliver tailored textual summaries of user-defined
queries, offering a focused insight amidst extensive video datasets. Unlike
conventional frameworks that offer generic summaries or limited action
recognition, our method harnesses the power of GenAI to distil relevant
information, enhancing analysis precision and efficiency. Employing YOLO-V8 for
object detection and Gemini for comprehensive video and text analysis, our
solution achieves heightened contextual accuracy. By combining YOLO with
Gemini, our approach furnishes textual summaries extracted from extensive CCTV
footage, enabling users to swiftly navigate and verify pertinent events without
the need for exhaustive manual review. The quantitative evaluation revealed a
similarity of 72.8%, while the qualitative assessment rated an accuracy of 85%,
demonstrating the capability of the proposed method.; 52) Self-consistent scenario for jet and stellar explosion in collapsar:
  General relativistic magnetohydrodynamics simulation with dynamo; A resistive magnetohydrodynamics simulation with a dynamo term is performed
for modeling the collapsar in full general relativity. As an initial condition,
a spinning black hole and infalling stellar matter are modeled based on a
stellar evolution result, superimposing a weak toroidal magnetic field. After
the growth of a massive torus around the black hole, the magnetic field is
amplified in it, developing poloidal fields via dynamo. In an early stage of
the torus growth, magnetic fluxes that fall to the vicinity of the central
black hole are swallowed by the black hole and global poloidal magnetic fields
that can be the source of the Blandford-Znajek mechanism are not developed.
However, in a later stage in which the ram pressure of the infalling matter
becomes weak, the magnetic field amplified by the black hole spin via the
winding becomes large enough to expel the infalling matter by the magnetic
pressure, and subsequently, a global poloidal magnetic field that penetrates
the black hole is established, launching a jet along the spin axis by the
Blandford-Znajek mechanism with the luminosity suitable for explaining typical
long gamma-ray bursts. Together with the jet launch, the effectively viscous
effect in the inner region of the torus and the magnetocentrifugal effect drive
the stellar explosion with the explosion energy comparable to typical or
powerful supernovae. We also find large amounts of synthesized $^{56}$Ni and Zn
associated with the stellar explosion. In the presence of jet launching,
$r$-process elements are weakly synthesized. The numerical results of the
explosion energy, ejecta mass, and $^{56}$Ni mass are in a good agreement with
those for observed broad-lined type Ic supernovae. Our result illustrates a
self-consistent scenario for the gamma-ray-burst-associated broad-lined type Ic
supernovae.; 53) Dirac fermions under imaginary rotation; In the present study, we investigate the properties of an ensemble of free
Dirac fermions, at finite inverse temperature $\beta$ and finite chemical
potential $\mu$, undergoing rigid rotation with an imaginary angular velocity
$\Omega_I$. Our purpose is to establish the analytical structure of such
states, as well as the prospects (and dangers) of extrapolating results
obtained under imaginary rotation to the case of real rotation. We show that in
the thermodynamic limit, the state of the system is akin to a stationary system
with modified inverse temperature $\beta_q = q\beta$ and the same chemical
potential, where $q$ is the denominator of the irreducible fraction $\nu =
\beta \Omega_I / 2\pi = p/q$. The temperature of the system becomes a fractal
function of the rotation parameter, as in the case of the scalar field. The
chemical potential breaks the fractalization of fermions. We also compute the
thermodynamic potential $\Phi$ and associated thermodynamic functions, showing
that they also exhibit fractal behavior. Finally, we evaluate the axial and
helical flux through the transverse plane, generated through the vortical
effects, and show that they diverge in the thermodynamic limit, in the case
when $\nu = 1/q$ and $q \to \infty$.; 54) A Louder Gravitational Wave Bang from a Fast-Expanding Universe; A strong first-order phase transition in a dark sector may produce all or
part of the low-frequency gravitational wave signal recently reported by the
NANOGrav Collaboration and other pulsar timing arrays. Here we point out, with
a simple toy model, that even if the amplitude of the gravitational wave
background from the dark phase transition is insufficient to match the NANOGrav
signal, a modified expansion rate at early times may considerably enhance the
gravitational wave signal. In particular, a faster-than-standard expansion
rate, triggered, for instance, by the presence of one or more additional
sources of energy density redshifting with higher powers of temperatures than
radiation, boosts upper limits on the gravitational wave signal from
first-order cosmological phase transitions, enlarging the slate of possible
dark sector scenarios matching the NANOGrav signal.; 55) Geometric properties versus particle motion in the Fang-Wang spacetime; In this work, we explore general relativistic effects and geometric
properties of the Fan-Wang spacetime, one of the simplest regular solutions
that can be obtained in nonlinear electrodynamics. In particular, we
investigate the motion of test particles, the capture cross-section of neutral
massive and massless particles, such as neutrinos and photons, and the
gravitational redshift. Additionally, using a perturbative approach, we derive
analytical expressions for the perihelion shift and gravitational deflection of
massless particles. By identifying the one-parameter corrections to the
Schwarzschild spacetime, induced by the magnetic charge contained in the
Fang-Wang metric, we show that this spacetime can be falsified, since it
modifies classical general relativity predictions even at the local level.
Moreover, we argue that these modifications could be experimentally tested with
advanced observational instrumentation.; 56) Optical RIS-enabled Multiple Access Communications; In this paper, we identify optical reconfigurable intelligent surfaces
(ORISs) as key enablers of next-generation free-space optical (FSO) multiple
access systems. By leveraging their beam steering and beam splitting
capabilities, ORISs are able to effectively address line-of-sight (LoS)
constraints, while enabling multi-user connectivity. We consider an
ORIS-assisted non-orthogonal multiple access (NOMA) system model consisting of
a single transmitter (Tx) and two receivers (Rxs). We derive novel analytical
expressions to characterize the statistical particularities of the Tx-ORIS-Rx
communication channel. Building upon the aforementioned expressions, we
investigate the outage performance of the Rxs by deriving exact analytical
expressions for the outage probability (OP) of each Rx. To provide deeper
insights into the impact of various system parameters and physical conditions
on the outage performance of each Rx, we conduct a high signal-to-noise ratio
(SNR) analysis, that returns asymptotic expressions for the Rxs OPs at the
high-SNR regime. Monte Carlo simulations validate the analysis, demonstrate the
effectiveness of ORIS-enabled NOMA under a variety of configurations and
physical scenarios, and showcase its superiority over its orthogonal-based
counterpart.; 57) PriFFT: Privacy-preserving Federated Fine-tuning of Large Language
  Models via Function Secret Sharing; Fine-tuning large language models (LLMs) raises privacy concerns due to the
risk of exposing sensitive training data. Federated learning (FL) mitigates
this risk by keeping training samples on local devices, but recent studies show
that adversaries can still infer private information from model updates in FL.
Additionally, LLM parameters are typically shared publicly during federated
fine-tuning, while developers are often reluctant to disclose these parameters,
posing further security challenges. Inspired by the above problems, we propose
PriFFT, a privacy-preserving federated fine-tuning mechanism, to protect both
the model updates and parameters. In PriFFT, clients and the server share model
inputs and parameters by secret sharing, performing secure fine-tuning on
shared values without accessing plaintext data. Due to considerable LLM
parameters, privacy-preserving federated fine-tuning invokes complex secure
calculations and requires substantial communication and computation resources.
To optimize the efficiency of privacy-preserving federated fine-tuning of LLMs,
we introduce function secret-sharing protocols for various operations,
including reciprocal calculation, tensor products, natural exponentiation,
softmax, hyperbolic tangent, and dropout. The proposed protocols achieve up to
4.02X speed improvement and reduce 7.19X communication overhead compared to the
implementation based on existing secret sharing methods. Besides, PriFFT
achieves a 2.23X speed improvement and reduces 4.08X communication overhead in
privacy-preserving fine-tuning without accuracy drop compared to the existing
secret sharing methods.; 58) Evolving the Computational Notebook: A Two-Dimensional Canvas for
  Enhanced Human-AI Interaction; Computational notebooks, while essential for data science, are limited by
their one-dimensional interface, which poorly aligns with non-linear developer
workflows and complicates collaboration and human-AI interaction. In this work,
we focus on features of Computational Canvas, a novel two-dimensional interface
that evolves notebooks to enhance data analysis and AI-assisted development
within integrated development environments (IDEs). We present vital features,
including freely arrangeable code cells, separate environments, and improved
output management. These features are designed to facilitate intuitive
organization, visual exploration, and natural collaboration with other users
and AI agents. We also show the implementation of Computational Canvas with
designed features as a Visual Studio Code plugin. By shifting from linear to
two-dimensional spatial interfaces, we aim to significantly boost developers'
productivity in data exploration, experimentation, and AI-assisted development,
addressing the current limitations of traditional notebooks and fostering more
flexible, collaborative data science workflows.; 59) Automated Market Makers: Toward More Profitable Liquidity Provisioning
  Strategies; To trade tokens in cryptoeconomic systems, automated market makers (AMMs)
typically rely on liquidity providers (LPs) that deposit tokens in exchange for
rewards. To profit from such rewards, LPs must use effective liquidity
provisioning strategies. However, LPs lack guidance for developing such
strategies, which often leads them to financial losses. We developed a
measurement model based on impermanent loss to analyze the influences of key
parameters (i.e., liquidity pool type, position duration, position range size,
and position size) of liquidity provisioning strategies on LPs' returns. To
reveal the influences of those key parameters on LPs' profits, we used the
measurement model to analyze 700 days of historical liquidity provision data of
Uniswap v3. By uncovering the influences of key parameters of liquidity
provisioning strategies on profitability, this work supports LPs in developing
more profitable strategies.; 60) WaveMax: Radar Waveform Design via Convex Maximization of FrFT Phase
  Retrieval; The ambiguity function (AF) is a critical tool in radar waveform design,
representing the two-dimensional correlation between a transmitted signal and
its time-delayed, frequency-shifted version. Obtaining a radar signal to match
a specified AF magnitude is a bi-variate variant of the well-known phase
retrieval problem. Prior approaches to this problem were either limited to a
few classes of waveforms or lacked a computable procedure to estimate the
signal. Our recent work provided a framework for solving this problem for both
band- and time-limited signals using non-convex optimization. In this paper, we
introduce a novel approach WaveMax that formulates waveform recovery as a
convex optimization problem by relying on the fractional Fourier transform
(FrFT)-based AF. We exploit the fact that AF of the FrFT of the original signal
is equivalent to a rotation of the original AF. In particular, we reconstruct
the radar signal by solving a low-rank minimization problem, which approximates
the waveform using the leading eigenvector of a matrix derived from the AF. Our
theoretical analysis shows that unique waveform reconstruction is achievable
with a sample size no more than three times the signal frequencies or time
samples. Numerical experiments validate the efficacy of WaveMax in recovering
signals from noiseless and noisy AF, including scenarios with randomly and
uniformly sampled sparse data.; 61) Boundary behaviour of the Fefferman--Szeg\""o metric in strictly
  pseudoconvex domains; We study the boundary behaviour of the Fefferman--Szeg\""o metric and several
associated invariants in a $C^\infty$-smoothly bounded strictly pseudoconvex
domain.; 62) Analysis of intramolecular modes of liquid water in two-dimensional
  spectroscopy: a classical hierarchical equations of motion approach; Two-dimensional (2D) vibrational spectroscopy is a powerful means of
investigating the structure and dynamics of complex molecules in condensed
phases. However, even in theory, analysis of 2D spectra resulting from complex
inter- and intra-molecular motions using only molecular dynamics methods is not
easy. This is because molecular motions comprise complex multiple modes, and
peaks broaden and overlap owing to various relaxation processes and
inhomogeneous broadening. On the basis of an anharmonic multimode Brownian
oscillator model with nonlinear system-bath coupling, we have developed an
approach that simulates 2D spectra, taking into account arbitrary modes of
intermolecular and intramolecular vibrations simultaneously. Although only
two-mode quantum calculations are feasible with this model, owing to high
computational costs, here we restrict ourselves to the classical case and
perform three-mode calculations. We demonstrate the applicability of our method
by calculating 2D correlation infrared spectra of water for symmetric
stretching, antisymmetric stretching, and bending modes. The quantum effects of
these results are deduced by comparing 2D quantum spectra previously obtained
for two intramolecular modes with those obtained using our classical approach
under the same physical conditions. The results show that the 2D spectra
calculated by separating the stretching modes into symmetric and asymmetric
modes provide better descriptions of peak profiles, such as the splitting of
cross-peaks.; 63) Tensor Learning and Compression of N-phonon Interactions; Phonon interactions from lattice anharmonicity govern thermal properties and
heat transport in materials. These interactions are described by n-th order
interatomic force constants (n-IFCs), which can be viewed as high-dimensional
tensors correlating the motion of n atoms, or equivalently encoding n-phonon
scattering processes in momentum space. Here, we introduce a tensor
decomposition to efficiently compress n-IFCs for arbitrary order n. Using
tensor learning, we find optimal low-rank approximations of n-IFCs by solving
the resulting optimization problem. Our approach reveals the inherent low
dimensionality of phonon-phonon interactions and allows compression of the 3
and 4-IFC tensors by factors of up to $10^3-10^4$ while retaining high accuracy
in calculations of phonon scattering rates and thermal conductivity.
Calculations of thermal conductivity using the compressed n-IFCs achieve a
speed-up by nearly three orders of magnitude with >98% accuracy relative to the
reference uncompressed solution. These calculations include both 3- and
4-phonon scattering and are shown for a diverse range of materials (Si, HgTe,
MgO, and TiNiSn). In addition to accelerating state-of-the-art thermal
transport calculations, the method shown here paves the way for modeling
strongly anharmonic materials and higher-order phonon interactions.; 64) ProjectTest: A Project-level LLM Unit Test Generation Benchmark and
  Impact of Error Fixing Mechanisms; Unit test generation has become a promising and important use case of LLMs.
However, existing evaluation benchmarks for assessing LLM unit test generation
capabilities focus on function- or class-level code rather than more practical
and challenging project-level codebases. To address such limitation, we propose
ProjectTest, a project-level benchmark for unit test generation covering
Python, Java, and JavaScript. ProjectTest features 20 moderate-sized and
high-quality projects per language. We evaluate nine frontier LLMs on
ProjectTest and the results show that all frontier LLMs tested exhibit moderate
performance on ProjectTest on Python and Java, highlighting the difficulty of
ProjectTest. We also conduct a thorough error analysis, which shows that even
frontier LLMs, such as Claude-3.5-Sonnet, have significant basic yet critical
errors, including compilation and cascade errors. Motivated by this
observation, we further evaluate all frontier LLMs under manual error-fixing
and self-error-fixing scenarios to assess their potential when equipped with
error-fixing mechanisms. Our code and dataset is available at
\href{https://github.com/YiboWANG214/ProjectTest}{ProjectTest}.; 65) Indoor Light and Heat Estimation from a Single Panorama; This paper presents a novel application for directly estimating indoor light
and heat maps from captured indoor-outdoor High Dynamic Range (HDR) panoramas.
In our image-based rendering method, the indoor panorama is used to estimate
the 3D room layout, while the corresponding outdoor panorama serves as an
environment map to infer spatially-varying light and material properties. We
establish a connection between indoor light transport and heat transport and
implement transient heat simulation to generate indoor heat panoramas. The
sensitivity analysis of various thermal parameters is conducted, and the
resulting heat maps are compared with the images captured by the thermal camera
in real-world scenarios. This digital application enables automatic indoor
light and heat estimation without manual inputs and cumbersome field
measurements.; 66) Superconducting LaPtH$_{ 6 }$ with triatomic hydrogen units; To veryfy ""hot supreconductivity"" recently proposed in lanthanum
hydride-based compounds, we explored thermodynamically stable and
superconducting phases in the lanthanum (La)-platinum (Pt)-hydrogen (H) ternary
system at 20 GPa using an evolutionary construction scheme of a
formation-enthalpy convex hull, universal neural network potential
calculations, and density functional theory calculations. Although we found no
evidence of the hot superconductivity in this ternary system, we predicted a
unique compound, LaPtH$_{ 6 }$, which has equilateral triangular H$_{ 3 }$
units nearly forming a two-dimensional kagome lattice between La and Pt layers
and shows the superconductivity at 18.67 K. This structure is dynamically
stable from ambient pressure to at least 200 GPa and the superconducting
critical temperature increases from 13.51 to 40.63 K.; 67) Negative correlations in Ising models of credit risk; We analyze a subclass of Ising models in the context of credit risk, focusing
on Dandelion models when the correlations $\rho$ between the central node and
each non-central node are negative. We establish the possible range of values
for $\rho$ and derive an explicit formula linking the correlation between any
pair of non-central nodes to $\rho$. The paper concludes with a simulation
study.; 68) Formalising Propositional Information via Implication Hypergraphs; This work introduces a framework for quantifying the information content of
logical propositions through the use of implication hypergraphs. We posit that
a proposition's informativeness is primarily determined by its relationships
with other propositions -- specifically, the extent to which it implies or
derives other propositions. To formalize this notion, we develop a framework
based on implication hypergraphs, that seeks to capture these relationships.
Within this framework, we define propositional information, derive some key
properties, and illustrate the concept through examples. While the approach is
broadly applicable, mathematical propositions emerge as an ideal domain for its
application due to their inherently rich and interconnected structure. We
provide several examples to illustrate this and subsequently discuss the
limitations of the framework, along with suggestions for potential refinements.; 69) A distribution related to Farey sequences -- I; We study some arithmetical properties of Farey sequences by the method
introduced by F.Boca, C.Cobeli and A.Zaharescu (2001). Let $\Phi_{Q}$ be the
classical Farey sequence of order $Q$. Having the fixed integers $D\geqslant 2$
and $0\leqslant c\leqslant D-1$, we colour to the red the fractions in
$\Phi_{Q}$ with denominators $\equiv c \pmod D$. Consider the gaps in
$\Phi_{Q}$ with coloured endpoints, that do not contain the fractions $a/q$
with $q\equiv c \pmod D$ inside. The question is to find the limit proportions
$\nu(r;D,c)$ (as $Q\to +\infty$) of such gaps with precisely $r$ fractions
inside in the whole set of the gaps under considering ($r = 0,1,2,3,\ldots$).
In fact, the expression for this proportion can be derived from the general
result obtained by C.Cobeli, M.V\^{a}j\^{a}itu and A.Zaharescu (2014). However,
such formula expresses $\nu(r;D,c)$ in the terms of areas of some polygons
related to a special geometrical transform. In the present paper, we obtain an
explicit formulas for $\nu(r;D,c)$ for the cases $D = 2, 3$ and $c=0$.; 70) Towards Trustworthy Retrieval Augmented Generation for Large Language
  Models: A Survey; Retrieval-Augmented Generation (RAG) is an advanced technique designed to
address the challenges of Artificial Intelligence-Generated Content (AIGC). By
integrating context retrieval into content generation, RAG provides reliable
and up-to-date external knowledge, reduces hallucinations, and ensures relevant
context across a wide range of tasks. However, despite RAG's success and
potential, recent studies have shown that the RAG paradigm also introduces new
risks, including robustness issues, privacy concerns, adversarial attacks, and
accountability issues. Addressing these risks is critical for future
applications of RAG systems, as they directly impact their trustworthiness.
Although various methods have been developed to improve the trustworthiness of
RAG methods, there is a lack of a unified perspective and framework for
research in this topic. Thus, in this paper, we aim to address this gap by
providing a comprehensive roadmap for developing trustworthy RAG systems. We
place our discussion around five key perspectives: reliability, privacy,
safety, fairness, explainability, and accountability. For each perspective, we
present a general framework and taxonomy, offering a structured approach to
understanding the current challenges, evaluating existing solutions, and
identifying promising future research directions. To encourage broader adoption
and innovation, we also highlight the downstream applications where trustworthy
RAG systems have a significant impact.; 71) Large Negative Magnetoresistance in Antiferromagnetic Gd2Se3; Rare earth chalcogenides provide a great platform to study exotic quantum
phenomena such as superconductivity and charge density waves. Among various
interesting properties, the coupling between magnetism and electronic transport
has attracted significant attention. Here, we report the investigation of such
coupling in {alpha}-Gd2Se3 single crystals through magnetic, calorimetric, and
transport property measurements. {alpha}-Gd2Se3 is found to display an
antiferromagnetic ground state below 11 K with metamagnetic spin-flop
transitions. The magnetic fluctuations remain strong above the transition
temperature. Transport measurements reveal an overall metallic transport
behavior with a large negative magnetoresistance of ~ 65% near the magnetic
transition temperature, together with positive MR near the field-induced
spin-flop transitions, which can be understood in terms of the suppression of
spin scattering by the magnetic field.; 72) HEISIR: Hierarchical Expansion of Inverted Semantic Indexing for
  Training-free Retrieval of Conversational Data using LLMs; The growth of conversational AI services has increased demand for effective
information retrieval from dialogue data. However, existing methods often face
challenges in capturing semantic intent or require extensive labeling and
fine-tuning. This paper introduces HEISIR (Hierarchical Expansion of Inverted
Semantic Indexing for Retrieval), a novel framework that enhances semantic
understanding in conversational data retrieval through optimized data
ingestion, eliminating the need for resource-intensive labeling or model
adaptation. HEISIR implements a two-step process: (1) Hierarchical Triplets
Formulation and (2) Adjunct Augmentation, creating semantic indices consisting
of Subject-Verb-Object-Adjunct (SVOA) quadruplets. This structured
representation effectively captures the underlying semantic information from
dialogue content. HEISIR achieves high retrieval performance while maintaining
low latency during the actual retrieval process. Our experimental results
demonstrate that HEISIR outperforms fine-tuned models across various embedding
types and language models. Beyond improving retrieval capabilities, HEISIR also
offers opportunities for intent and topic analysis in conversational data,
providing a versatile solution for dialogue systems.; 73) Chain-of-Rank: Enhancing Large Language Models for Domain-Specific RAG
  in Edge Device; Retrieval-augmented generation (RAG) with large language models (LLMs) is
especially valuable in specialized domains, where precision is critical. To
more specialize the LLMs into a target domain, domain-specific RAG has recently
been developed by allowing the LLM to access the target domain early via
finetuning. The domain-specific RAG makes more sense in resource-constrained
environments like edge devices, as they should perform a specific task (e.g.
personalization) reliably using only small-scale LLMs. While the
domain-specific RAG is well-aligned with edge devices in this respect, it often
relies on widely-used reasoning techniques like chain-of-thought (CoT). The
reasoning step is useful to understand the given external knowledge, and yet it
is computationally expensive and difficult for small-scale LLMs to learn it.
Tackling this, we propose the Chain of Rank (CoR) which shifts the focus from
intricate lengthy reasoning to simple ranking of the reliability of input
external documents. Then, CoR reduces computational complexity while
maintaining high accuracy, making it particularly suited for
resource-constrained environments. We attain the state-of-the-art (SOTA)
results in benchmarks, and analyze its efficacy.; 74) Pediatric low-grade glioma: State-of-the-art and ongoing challenges; Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%–40% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence.; 75) Actuation mechanisms in twisted and coiled polymer actuators using
  finite element model; Twisted and coiled polymer actuators (TCPAs) offer the advantages of large
stroke and large specific work as compared to other actuators. There have been
extensive experimental investigations towards understanding their actuation
response, however, a computational model with full material description is not
utilized to probe into the underlying mechanisms responsible for their large
actuation. In this work, we develop a three-dimensional finite element model
that includes the physics of the fabrication process to simulate the actuation
of TCPA under various loading and boundary conditions. The model is validated
against the experimental data and used to explore the factors responsible for
actuation under free and isobaric conditions. The model captures the physics of
the angle of twist in the fiber and the distinction between the homochiral and
heterochiral nature of TCPA actuation response. The simulations show that the
anisotropy in the thermal expansion coefficient (CTE) matrix plays a major role
in large actuation irrespective of the anisotropy or isotropy in the elasticity
tensor. We further investigate the extent of anisotropy in thermal expansion
and the parametric studies show that the key for TCPA actuation is the absolute
value of mismatch in thermal expansion even if the material has positive or
negative CTE in both directions of the fiber. Furthermore, we propose a new
shell-core composite-based TCPA concept by combining the epoxy and hollow Nylon
tubes to suppress the creep in TCPA. The results show that the volume fraction
of epoxy-core can be tuned to attain a desired actuation while offering a
stiffer and creep-resistant response. This framework provides a wider
application for probing various kinds of TCPAs and enhancing their actuation
performance.; 76) Dirac Operators on Orbifold Resolutions: Uniform Elliptic Theory; Dirac operators on Riemannian spaces play a central role in various branches
of mathematics, encoding rich geometric and topological data. They appear as
deformation operators in moduli problems, including those associated with
special holonomy metrics, gauge theory instantons, and calibrated submanifolds.
This paper investigates the behaviour of families of Dirac operators as the
underlying Riemannian spaces degenerate to a Riemannian orbifold in the so
called adiabatic limit. Specifically, we focus on relating the kernels and
cokernels of the family of Dirac operators to adiabatic data and establish
uniform bounds on their right inverse. These results provide a crucial analytic
foundation for gluing problems on orbifold resolutions, without relying on the
intricate iterated edge calculus developed by Mazzeo, Melrose, Schulze and
others. This work paves the way for forthcoming studies, where these techniques
will be used to address open problems in special holonomy geometry, for example
the construction of compact $\mathrm{G}_2$- and $\mathrm{Spin}(7)$-manifolds,
gauge theory, and calibrated geometry. The methods developed here have broader
implications for the study of singular Riemannian spaces and their analytic
properties.; 77) Forecasting Open-Weight AI Model Growth on HuggingFace; As the open-weight AI landscape continues to proliferate-with model
development, significant investment, and user interest-it becomes increasingly
important to predict which models will ultimately drive innovation and shape AI
ecosystems. Building on parallels with citation dynamics in scientific
literature, we propose a framework to quantify how an open-weight model's
influence evolves. Specifically, we adapt the model introduced by Wang et al.
for scientific citations, using three key parameters-immediacy, longevity, and
relative fitness-to track the cumulative number of fine-tuned models of an
open-weight model. Our findings reveal that this citation-style approach can
effectively capture the diverse trajectories of open-weight model adoption,
with most models fitting well and outliers indicating unique patterns or abrupt
jumps in usage.; 78) Experimental observation of Dirac exceptional point; The energy level degeneracies, also known as exceptional points (EPs), are
crucial for comprehending emerging phenomena in materials and enabling
innovative functionalities for devices. Since EPs were proposed over half a
century age, only two types of EPs have been experimentally discovered,
revealing intriguing phases of materials such as Dirac and Weyl semimetals.
These discoveries have showcased numerous exotic topological properties and
novel applications, such as unidirectional energy transfer. Here we report the
observation of a novel type of EP, named the Dirac EP, utilizing a
nitrogen-vacancy center in diamond. Two of the eigenvalues are measured to be
degenerate at the Dirac EP and remain real in its vicinity. This exotic band
topology associated with the Dirac EP enables the preservation of the symmetry
when passing through, and makes it possible to achieve adiabatic evolution in
non-Hermitian systems. We examined the degeneracy between the two eigenstates
by quantum state tomography, confirming that the degenerate point is a Dirac EP
rather than a Hermitian degeneracy. Our research of the distinct type of EP
contributes a fresh perspective on dynamics in non-Hermitian systems and is
potentially valuable for applications in quantum control in non-Hermitian
systems and the study of the topological properties of EP.; 79) Entropic force and bouncing behaviour in $\kappa$-Minkowski space-time; We generalise the entropic force description of gravity into
$\kappa$-Minkowski space-time and derive the $\kappa$-deformed corrections to
the Newton's gravitational force. Using this we show the appearance of
logarithmic correction as the first order $\kappa$-deformed correction term to
Bekenstein-Hawking entropy. Further we also derive the $\kappa$-deformed
Friedmann equations and study the evolution of scale factor in
$\kappa$-deformed space-time. Finally we show that the $\kappa$-deformation
parameter avoids the initial singularity in early universe by providing a
bounce behaviour for the case of spatially flat and closed universe.; 80) GraphSense: Graph Embedding Based Code Suggestion Framework; Code suggestions have become an integral part of IDEs and developers use code
suggestions generated by IDEs all the time. These code suggestions are mostly
for calling a method of an object or for using a function of a library and not
for possible next line of the code. GPT based models are too slow or resource
intensive for real-time code suggestions in local environments. As a solution
to this GraphSense was introduced which provide code suggestions with minimum
amount of resource usage in real-time.; 81) Superheavy Supersymmetric Dark Matter for the origin of KM3NeT
  Ultra-High Energy signal; We propose an explanation for the recently reported ultra-high-energy
neutrino signal at KM3NeT, which lacks an identifiable astrophysical source.
While decaying dark matter in the Galactic Center is a natural candidate, the
observed arrival direction strongly suggests an extragalactic origin. We
introduce a multicomponent dark matter scenario in which the components are
part of a supermultiplet, with supersymmetry ensuring a nearly degenerate mass
spectrum among the fields. This setup allows a heavy component to decay into a
lighter one, producing a boosted neutrino spectrum with energy $E_\nu \sim 100$
PeV, determined by the mass difference. The heavy-to-light decay occurs at a
cosmological redshift of $z \sim \text{a few}$ or higher, leading to an
isotropic directional distribution of the signal.; 82) WorldPose: A World Cup Dataset for Global 3D Human Pose Estimation; We present WorldPose, a novel dataset for advancing research in multi-person
global pose estimation in the wild, featuring footage from the 2022 FIFA World
Cup. While previous datasets have primarily focused on local poses, often
limited to a single person or in constrained, indoor settings, the
infrastructure deployed for this sporting event allows access to multiple fixed
and moving cameras in different stadiums. We exploit the static multi-view
setup of HD cameras to recover the 3D player poses and motions with
unprecedented accuracy given capture areas of more than 1.75 acres. We then
leverage the captured players' motions and field markings to calibrate a moving
broadcasting camera. The resulting dataset comprises more than 80 sequences
with approx 2.5 million 3D poses and a total traveling distance of over 120 km.
Subsequently, we conduct an in-depth analysis of the SOTA methods for global
pose estimation. Our experiments demonstrate that WorldPose challenges existing
multi-person techniques, supporting the potential for new research in this area
and others, such as sports analysis. All pose annotations (in SMPL format),
broadcasting camera parameters and footage will be released for academic
research purposes.; 83) Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth
  Approach; We study a novel language model architecture that is capable of scaling
test-time computation by implicitly reasoning in latent space. Our model works
by iterating a recurrent block, thereby unrolling to arbitrary depth at
test-time. This stands in contrast to mainstream reasoning models that scale up
compute by producing more tokens. Unlike approaches based on chain-of-thought,
our approach does not require any specialized training data, can work with
small context windows, and can capture types of reasoning that are not easily
represented in words. We scale a proof-of-concept model to 3.5 billion
parameters and 800 billion tokens. We show that the resulting model can improve
its performance on reasoning benchmarks, sometimes dramatically, up to a
computation load equivalent to 50 billion parameters.; 84) L-shell Photoionisation Cross Sections in the S^{+}, S^{2+}, S^{3+}
  Isonuclear Sequence; We present absolute L-shell photoionisation cross sections for the S+, S2+,
S3+ions. The cross sections were obtained using the monochromatised photon beam
delivered by the SOLEIL synchrotron source coupled with an ion beam extracted
from an electron cyclotron resonance source (ECRIS) in the merged dual-beam
configuration. The cross sections for single, double and triple ionisation were
measured and combined to generate total photoionisation cross sections. For
each of the S+, S2+, S3+ ions, the photon energy regions corresponding to the
excitation and ionisation of a 2p or a 2s electron (175-230 eV) were
investigated. The experimental results are interpreted with the help of
multiconfigurational Dirac-Fock (MCDF) and Breit-Pauli R-Matrix (BPRM) or Dirac
R-Matrix (DARC) theoretical calculations. The former generates photoabsorption
cross sections from eigenenergies and eigenfunctions obtained by solving
variationally the multiconfiguration Dirac Hamiltonian while the latter
calculate cross sections for photon scattering by atoms. The cross sectional
spectra feature rich resonance structures with narrow natural widths (typically
less than 100 meV) due to 2p to nd excitations below and up to the 2p
thresholds. This behaviour is consistent with the large number of inner-shell
states based on correlation and spin-orbit mixed configurations having three
open subshells. Strong and wide (typically 1 eV) Rydberg series of resonances
due to 2s to np excitations dominate above the 2p threshold.; 85) Water transport on finite graphs; Consider a simple finite graph and its nodes to represent identical water
barrels (containing different amounts of water) on a level plane. Each edge
corresponds to a (locked, water-filled) pipe connecting two barrels below the
plane. We fix one node $v$ and consider the optimization problem relating to
the maximum value to which the level in $v$ can be raised without pumps, i.e.
by opening/closing pipes in a suitable order. This fairly natural optimization
problem originated from the analysis of an opinion formation process and proved
to be not only sufficiently intricate in order to be of independent interest,
but also difficult from an algorithmic point of view.; 86) Performance Trade-offs of High Order Meshless Approximation on
  Distributed Memory Systems; Meshless methods approximate operators in a specific node as a weighted sum
of values in its neighbours. Higher order approximations of derivatives provide
more accurate solutions with better convergence characteristics, but they come
at the cost of including more neighbours. On the accuracy-per-compute time
basis we know that increasing the approximation order is beneficial for a
shared memory computer, but there is additional communication overhead when
problems become too large and we have to resort to distributed memory systems.
Meshless nodes are divided between systems in spatially coherent subdomains
with approximations at their edges requiring neighbouring value exchange.
Performance optimization is then a balancing act between minimizing the
required number of communicated neighbours by lowering the approximation order
or increasing it to enable faster convergence. We use the radial basis
function-generated finite difference method (RBF-FD) to approximate the
derivatives that we use to solve the Poisson equation with an explicit
iterative scheme. Inter-system communication is provided by Open MPI, while
OpenMP is used for intra-system parallelisation. We perform the analysis on a
homogenous CPU-based cluster where we examine the behaviour and attempt to
determine the optimal parameterisation with the goal of minimizing the
computational time to reach a desired accuracy.; 87) Using curved meshes to derive a priori error estimates for a linear
  elasticity problem with Robin boundary conditions; This work concerns the numerical analysis of the linear elasticity problem
with a Robin boundary condition on a smooth domain. A finite element
discretization is presented using high-order curved meshes in order to
accurately discretize the physical domain. The primary objective is to conduct
a detailed error analysis for the elasticity problem using the vector lift
operator, which maps vector-valued functions from the mesh domain to the
physical domain. Error estimates are established, both in terms of the finite
element approximation error and the geometric error, respectively associated to
the finite element degree and to the mesh order. These theoretical a priori
error estimates are validated by numerical experiments in 2D and 3D.; 88) CRSet: Non-Interactive Verifiable Credential Revocation with Metadata
  Privacy for Issuers and Everyone Else; Like any digital certificate, Verifiable Credentials (VCs) require a way to
revoke them in case of an error or key compromise. Existing solutions for VC
revocation, most prominently Bitstring Status List, are not viable for many use
cases since they leak the issuer's behavior, which in turn leaks internal
business metrics. For instance, exact staff fluctuation through issuance and
revocation of employee IDs. We introduce CRSet, a revocation mechanism that
allows an issuer to encode revocation information for years worth of VCs as a
Bloom filter cascade. Padding is used to provide deniability for issuer
metrics. Issuers periodically publish this filter cascade on a decentralized
storage system. Relying Parties (RPs) can download it to perform any number of
revocation checks locally. Compared to existing solutions, CRSet protects the
metadata of subject, RPs, and issuer equally. At the same time, it is
non-interactive, making it work with wallet devices having limited hardware
power and drop-in compatible with existing VC exchange protocols and wallet
applications. We present a prototype using the Ethereum blockchain as
decentralized storage. The recently introduced blob-carrying transactions,
enabling cheaper data writes, allow us to write each CRSet directly to the
chain. We built software for issuers and RPs that we successfully tested
end-to-end with an existing publicly available wallet agents and the OpenID for
Verifiable Credentials protocols. Storage and bandwidth costs paid by issuers
and RP are higher than for Bitstring Status List, but still manageable at
around 1 MB for an issuer issuing hundreds of thousands of VCs annually and
covering decades.; 89) Action accessible and weakly action representable varieties of algebras; The main goal of this article is to investigate the relationship between
action accessibility and weak action representability in the context of
varieties of non-associative algebras over a field. Specifically, using an
argument of J. R. A. Gray in the setting of groups, we prove that the varieties
of $k$-nilpotent Lie algebras ($k \geq 3$) and the varieties of $n$-solvable
Lie algebras ($n \geq 2$) do not form weakly action representable categories.
These are the first known examples of action accessible varieties of
non-associative algebras that fail to be weakly action representable,
establishing that a subvariety of a (weakly) action representable variety of
non-associative algebras needs not be weakly action representable. Eventually,
we refine J. R. A. Gray's result by proving that the varieties of $k$-nilpotent
groups ($k \geq 3$) and that of $2$-solvable groups are not weakly action
representable.; 90) Physics-based Machine Learning for Computational Fracture Mechanics; This study introduces a physics-based machine learning framework for modeling
both brittle and ductile fractures. Unlike physics-informed neural networks,
which solve partial differential equations by embedding physical laws as soft
constraints in loss functions and enforcing boundary conditions via collocation
points, our framework integrates physical principles, such as the governing
equations and constraints, directly into the neural network architecture. This
approach eliminates the dependency on problem-specific retraining for new
boundary value problems, ensuring adaptability and consistency. By embedding
constitutive behavior into the network's foundational design, our method
represents a significant step toward unifying material modeling with machine
learning for computational fracture mechanics. Specifically, a feedforward
neural network is designed to embed physical laws within its architecture,
ensuring thermodynamic consistency. Building on this foundation, synthetic
datasets generated from finite element-based phase-field simulations are
employed to train the proposed framework, focusing on capturing the homogeneous
responses of brittle and ductile fractures. Detailed analyses are performed on
the stored elastic energy and the dissipated work due to plasticity and
fracture, demonstrating the capability of the framework to predict essential
fracture features. The proposed physics-based machine learning framework
overcomes the shortcomings of classical machine learning models, which rely
heavily on large datasets and lack guarantees of physical principles. By
leveraging its physics-integrated design, the physics-based machine learning
framework demonstrates exceptional performance in predicting key properties of
brittle and ductile fractures with limited training data.; 91) Real-time edge dynamics of non-Hermitian lattices; We derive the asymptotic forms of the Green's function at the open edges of
general non-Hermitian band systems in all dimensions in the long-time limit,
using a modified saddle-point approximation and the analytic continuation of
the momentum. The edge dynamics is determined by the ""dominant saddle point"", a
complex momentum, which, contrary to previous conjectures, may lie outside the
generalized Brillouin zone. From this result, we obtain the effective edge
Hamiltonians that evidently, as demonstrated by extensive numerical
simulations, characterize the dynamics on the edges, and can be probed in
real-time experiments or spectroscopies.; 92) Superlubric sliding ferroelectricity; Sliding ferroelectricity may emerge in many van der Waals
bilayers/multilayers and the low switching barriers render ultrafast data
writing with low energy cost. We note that such barriers are still much higher
compared with structural superlubricity, and in this paper we propose a type of
superlubric sliding ferroelectricity in homobilayers separated by a different
layer that leads to unprecedented low switching barriers due to incommensurate
interfaces. For example, the switching barrier of 3R bilayer MoS2 will be
respectively reduced by around 2 or 1 order of magnitudes if they are separated
by a graphene or BN monolayer, and the required voltage for switching can be
about 1 order of magnitude lower. Such superlubric sliding ferroelectricity
widely exists in various similar sandwich trilayer systems where the
polarizations stem from symmetry breaking in across-layer stacking
configurations, and with ultralow barriers of superlubric sliding, their
performances for various applications are greatly enhanced compared with
homobilayer sliding ferroelectrics.; 93) Modeling Changes in Individuals' Cognitive Self-Esteem With and Without
  Access To Search Tools; Search engines, as cognitive partners, reshape how individuals evaluate their
cognitive abilities. This study examines how search tool access influences
cognitive self-esteem (CSE)-users' self-perception of cognitive abilities --
through the lens of transactive memory systems. Using a within-subject design
with 164 participants, we found that CSE significantly inflates when users have
access to search tools, driven by cognitive offloading. Participants with lower
initial CSE exhibited greater shifts, highlighting individual differences.
Search self-efficacy mediated the relationship between prior search experience
and CSE, emphasizing the role of users' past interactions. These findings
reveal opportunities for search engine design: interfaces that promote
awareness of cognitive offloading and foster self-reflection can support
accurate metacognitive evaluations, reducing overreliance on external tools.
This research contributes to HCI by demonstrating how interactive systems shape
cognitive self-perception, offering actionable insights for designing
human-centered tools that balance user confidence and cognitive independence.; 94) Towards Experience Replay for Class-Incremental Learning in Fully-Binary
  Networks; Binary Neural Networks (BNNs) are a promising approach to enable Artificial
Neural Network (ANN) implementation on ultra-low power edge devices. Such
devices may compute data in highly dynamic environments, in which the classes
targeted for inference can evolve or even novel classes may arise, requiring
continual learning. Class Incremental Learning (CIL) is a common type of
continual learning for classification problems, that has been scarcely
addressed in the context of BNNs. Furthermore, most of existing BNNs models are
not fully binary, as they require several real-valued network layers, at the
input, the output, and for batch normalization. This paper goes a step further,
enabling class incremental learning in Fully-Binarized NNs (FBNNs) through four
main contributions. We firstly revisit the FBNN design and its training
procedure that is suitable to CIL. Secondly, we explore loss balancing, a
method to trade-off the performance of past and current classes. Thirdly, we
propose a semi-supervised method to pre-train the feature extractor of the FBNN
for transferable representations. Fourthly, two conventional CIL methods, \ie,
Latent and Native replay, are thoroughly compared. These contributions are
exemplified first on the CIFAR100 dataset, before being scaled up to address
the CORE50 continual learning benchmark. The final results based on our 3Mb
FBNN on CORE50 exhibit at par and better performance than conventional
real-valued larger NN models.; 95) Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion
  Transformers via In-Context Reflection; The predominant approach to advancing text-to-image generation has been
training-time scaling, where larger models are trained on more data using
greater computational resources. While effective, this approach is
computationally expensive, leading to growing interest in inference-time
scaling to improve performance. Currently, inference-time scaling for
text-to-image diffusion models is largely limited to best-of-N sampling, where
multiple images are generated per prompt and a selection model chooses the best
output. Inspired by the recent success of reasoning models like DeepSeek-R1 in
the language domain, we introduce an alternative to naive best-of-N sampling by
equipping text-to-image Diffusion Transformers with in-context reflection
capabilities. We propose Reflect-DiT, a method that enables Diffusion
Transformers to refine their generations using in-context examples of
previously generated images alongside textual feedback describing necessary
improvements. Instead of passively relying on random sampling and hoping for a
better result in a future generation, Reflect-DiT explicitly tailors its
generations to address specific aspects requiring enhancement. Experimental
results demonstrate that Reflect-DiT improves performance on the GenEval
benchmark (+0.19) using SANA-1.0-1.6B as a base model. Additionally, it
achieves a new state-of-the-art score of 0.81 on GenEval while generating only
20 samples per prompt, surpassing the previous best score of 0.80, which was
obtained using a significantly larger model (SANA-1.5-4.8B) with 2048 samples
under the best-of-N approach.; 96) Policy iteration for nonconvex viscous Hamilton--Jacobi equations; We study the convergence rates of policy iteration (PI) for nonconvex viscous
Hamilton--Jacobi equations using a discrete space-time scheme, where both space
and time variables are discretized. We analyze the case with an uncontrolled
diffusion term, which corresponds to a possibly degenerate viscous
Hamilton--Jacobi equation. We first obtain an exponential convergent result of
PI for the discrete space-time schemes. We then investigate the discretization
error.; 97) Privacy Protection in Prosumer Energy Management Based on Federated
  Learning; With the booming development of prosumers, there is an urgent need for a
prosumer energy management system to take full advantage of the flexibility of
prosumers and take into account the interests of other parties. However,
building such a system will undoubtedly reveal users' privacy. In this paper,
by solving the non-independent and identical distribution of data (Non-IID)
problem in federated learning with federated cluster average(FedClusAvg)
algorithm, prosumers' information can efficiently participate in the
intelligent decision making of the system without revealing privacy. In the
proposed FedClusAvg algorithm, each client performs cluster stratified sampling
and multiple iterations. Then, the average weight of the parameters of the
sub-server is determined according to the degree of deviation of the parameter
from the average parameter. Finally, the sub-server multiple local iterations
and updates, and then upload to the main server. The advantages of FedClusAvg
algorithm are the following two parts. First, the accuracy of the model in the
case of Non-IID is improved through the method of clustering and parameter
weighted average. Second, local multiple iterations and three-tier framework
can effectively reduce communication rounds.; 98) Isolated attosecond free-electron laser based on a sub-cycle driver from
  hollow capillary fibers; The attosecond light source provides an advanced tool for investigating
electron motion using time-resolved-spectroscopy techniques. Isolated
attosecond pulses, especially, will significantly advance the study of electron
dynamics. However, achieving high-intensity isolated attosecond pulses is still
challenging at the present stage. In this paper, we propose a novel scheme for
generating high-intensity, isolated attosecond soft X-ray free-electron lasers
(FELs) using a mid-infrared (MIR) sub-cycle modulation laser from gas-filled
hollow capillary fibers (HCFs). The multi-cycle MIR pulses are first compressed
to sub-cycle using a helium-filled HCF with decreasing pressure gradient due to
soliton self-compression effect. By utilizing such sub-cycle MIR laser pulse to
modulate the electron beam, we can obtain a quasi-isolated current peak, which
can then produce an isolated FEL pulse with high signal-to-noise ratio (SNR),
naturally synchronizing with the sub-cycle MIR laser pulse. Numerical
simulations have been carried out, including the sub-cycle pulse generation,
electron beam modulation and FEL radiation processes. The simulation results
indicate that an isolated attosecond pulse with wavelength of 1 nm, peak power
of ~28 GW, pulse duration of ~600 attoseconds and SNR of ~96.4% can be
generated by our proposed method. The numerical results demonstrated here pave
a new way for generating the high-intensity isolated attosecond soft X-ray
pulse, which may have many applications in nonlinear spectroscopy and
atomic-site electronic process.; 99) Enhancing the Accuracy and Comprehensibility in Architectural Tactics
  Detection via Small Model-Augmented Prompt Engineering; Architectural tactics (ATs), as the concrete implementation of architectural
decisions in code, address non-functional requirements of software systems. Due
to the implicit nature of architectural knowledge in code implementation,
developers may risk inadvertently altering or removing these tactics during
code modifications or optimizations. Such unintended changes can trigger
architectural erosion, gradually undermining the system's original design.
While many researchers have proposed machine learning-based methods to improve
the accuracy of detecting ATs in code, the black-box nature and the required
architectural domain knowledge pose significant challenges for developers in
verifying the results. Effective verification requires not only accurate
detection results but also interpretable explanations that enhance their
comprehensibility. However, this is a critical gap in current research. Large
language models (LLMs) can generate easily interpretable ATs detection comments
if they have domain knowledge. Fine-tuning LLMs to acquire domain knowledge
faces challenges such as catastrophic forgetting and hardware constraints.
Thus, we propose Prmt4TD, a small model-augmented prompting framework to
enhance the accuracy and comprehensibility of ATs detection. Combining
fine-tuned small models with In-Context Learning can also reduce fine-tuning
costs while equipping the LLM with additional domain knowledge. Prmt4TD can
leverage the remarkable processing and reasoning capabilities of LLMs to
generate easily interpretable ATs detection results. Our evaluation results
demonstrate that Prmt4TD achieves accuracy (\emph{F1-score}) improvement of
13\%-23\% on the ATs balanced dataset and enhances the comprehensibility of the
detection results.; 100) No LLM is Free From Bias: A Comprehensive Study of Bias Evaluation in
  Large Language models; Advancements in Large Language Models (LLMs) have increased the performance
of different natural language understanding as well as generation tasks.
Although LLMs have breached the state-of-the-art performance in various tasks,
they often reflect different forms of bias present in the training data. In the
light of this perceived limitation, we provide a unified evaluation of
benchmarks using a set of representative LLMs that cover different forms of
biases starting from physical characteristics to socio-economic categories.
Moreover, we propose five prompting approaches to carry out the bias detection
task across different aspects of bias. Further, we formulate three research
questions to gain valuable insight in detecting biases in LLMs using different
approaches and evaluation metrics across benchmarks. The results indicate that
each of the selected LLMs suffer from one or the other form of bias with the
LLaMA3.1-8B model being the least biased. Finally, we conclude the paper with
the identification of key challenges and possible future directions.",0.0,0.3562071871080222
2411.00609,applied,2411.00609-pos2-4,"Pediatric low-grade glioma: State-of-the-art and ongoing challenges; Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%–40% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence.",2411.00609-pos1-4,"Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype
  Identification Using a Novel AUROC Loss Function for Convolutional Neural
  Networks; Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016).",13,"['13', '46', '12', '80', '84', '92', '67', '61', '30', '71']","The first candidate paper, 'Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype Identification Using a Novel AUROC Loss Function for Convolutional Neural Networks,' aligns best with the main paper on pediatric low-grade glioma (pLGG) as it directly addresses the specific challenge of identifying molecular subtypes in pediatric tumors, which is crucial for treatment planning. This paper combines the insights from the main paper on the diagnosis and treatment of pLGG with advanced deep learning techniques, thus forming a multidisciplinary approach that is both novel and useful. The subsequent papers also relate to healthcare and technology, but none match the direct relevance and specificity of the first candidate in addressing the challenges outlined in the main paper.","1) On the Role of Pre-trained Embeddings in Binary Code Analysis; Deep learning has enabled remarkable progress in binary code analysis. In
particular, pre-trained embeddings of assembly code have become a gold standard
for solving analysis tasks, such as measuring code similarity or recognizing
functions. These embeddings are capable of learning a vector representation
from unlabeled code. In contrast to natural language processing, however, label
information is not scarce for many tasks in binary code analysis. For example,
labeled training data for function boundaries, optimization levels, and
argument types can be easily derived from debug information provided by a
compiler. Consequently, the main motivation of embeddings does not transfer
directly to binary code analysis.
  In this paper, we explore the role of pre-trained embeddings from a critical
perspective. To this end, we systematically evaluate recent embeddings for
assembly code on five downstream tasks using a corpus of 1.2 million functions
from the Debian distribution. We observe that several embeddings perform
similarly when sufficient labeled data is available, and that differences
reported in prior work are hardly noticeable. Surprisingly, we find that
end-to-end learning without pre-training performs best on average, which calls
into question the need for specialized embeddings. By varying the amount of
labeled data, we eventually derive guidelines for when embeddings offer
advantages and when end-to-end learning is preferable for binary code analysis.; 2) ""In order that"" -- a data driven study of symptoms and causes of
  obsolescence; The paper is an empirical case study of grammatical obsolescence in progress.
The main studied variable is the purpose subordinator in order that, which is
shown to be steadily decreasing in the frequency of use starting from the
beginning of the twentieth century. This work applies a data-driven approach
for the investigation and description of obsolescence, recently developed by
the Rudnicka (2019). The methodology combines philological analysis with
statistical methods used on data acquired from mega-corpora. Moving from the
description of possible symptoms of obsolescence to different causes for it,
the paper aims at presenting a comprehensive account of the studied phenomenon.
Interestingly, a very significant role in the decline of in order that can be
ascribed to the so-called higher-order processes, understood as processes
influencing the constructional level from above. Two kinds of higher-order
processes are shown to play an important role, namely i) an
externally-motivated higher-order process exemplified by the drastic
socio-cultural changes of the 19th and 20th centuries; ii) an
internally-motivated higher-order processes instantiated by the rise of the
to-infinitive (rise of infinite clauses).; 3) Approximate isometries of Hilbert spaces; We improve the Hyers-Ulam stability result for isometries of real Hilbert
spaces by removing the surjectivity assumption.; 4) The chiral SYK model in three-dimensional holography; A celebrated realization of the holographic principle posits an approximate
duality between the $(0+1)$-dimensional quantum mechanical SYK model and
two-dimensional Jackiw-Teitelboim gravity, mediated by the Schwarzian action as
an effective low energy theory common to both systems. We here propose a
generalization of this correspondence to one dimension higher. Starting from
different microscopic realizations of effectively chiral $(1+1)$-dimensional
generalizations of the SYK model, we derive a reduction to the
Alekseev-Shatashvilli (AS)-action, a minimal extension of the Schwarzian action
which has been proposed as the effective boundary action of three-dimensional
gravity. In the bulk, we show how the same action describes fluctuations around
the Euclidean BTZ black hole configuration, the dominant stationary solution of
three-dimensional gravity. These two constructions allow us to match bulk and
boundary coupling constants, and to compute observables. Specifically, we apply
semiclassical techniques inspired by condensed matter physics to the
computation of out-of-time-order correlation functions (OTOCs), demonstrating
maximal chaos in the chiral SYK chain and its gravity dual.; 5) Liquidity provision of utility indifference type in decentralized
  exchanges; We present a mathematical formulation of liquidity provision in decentralized
exchanges. We focus on constant function market makers of utility indifference
type, which include constant product market makers with concentrated liquidity
as a special case. First, we examine no-arbitrage conditions for a liquidity
pool and compute an optimal arbitrage strategy when there is an external liquid
market. Second, we show that liquidity provision suffers from impermanent loss
unless a transaction fee is levied under the general framework with
concentrated liquidity. Third, we establish the well-definedness of
arbitrage-free reserve processes of a liquidity pool in continuous-time and
show that there is no loss-versus-rebalancing under a nonzero fee if the
external market price is continuous. We then argue that liquidity provision by
multiple liquidity providers can be understood as liquidity provision by a
representative liquidity provider, meaning that the analysis boils down to that
for a single liquidity provider. Last, but not least, we give an answer to the
fundamental question in which sense the very construction of constant function
market makers with concentrated liquidity in the popular platform Uniswap v3 is
optimal.; 6) On Persistently Resetting Learning Integrators: A Framework For
  Model-Free Feedback Optimization; We study a novel class of algorithms for solving model-free feedback
optimization problems in dynamical systems. The key novelty is the introduction
of \emph{persistent resetting learning integrators} (PRLI), which are
integrators that are reset at the same frequency at which the plant is dithered
using exploratory signals for model-free optimization. It is shown that PRLIs
can serve as core mechanisms for real-time gradient estimation in online
feedback-optimization tasks where only cost function measurements are
available. In particular, unlike existing approaches based on approximation
theory, such as averaging or finite-differences, PRLIs can produce global
real-time gradient estimates of cost functions, with uniformly bounded
perturbations of arbitrarily small magnitude. In this sense, PRLIs function as
robust \emph{hybrid} ""Oracles"" suitable for interconnection with discrete-time
optimization algorithms that optimize the performance of continuous-time
dynamical plants in closed-loop operation. Compared to existing methods, PRLIs
yield \emph{global} stability properties for a broad class of cost functions,
surpassing the local or semi-global guarantees offered by traditional
approaches based on perturbation and approximation theory. The proposed
framework naturally bridges physical systems, modeled as continuous-time plants
where continuous exploration is essential, with digital algorithms, represented
as discrete-time optimization methods. The main results are illustrated using
different numerical examples.; 7) Metarecycling in Physics Education in Basic Education within the Context
  of the UN Sustainable Development Goals through Technological Research; The rapid advancement of digital technologies in the first quarter of the
21st century has introduced significant transformations in various fields, such
as communication, healthcare, and education. However, it has also led to an
increase in the use and disposal of electronic devices, resulting in
environmental challenges related to Waste Electrical and Electronic Equipment
(WEEE), also known as e-waste. This phenomenon is observed in schools, where
the integration and renewal of equipment have become essential for the
development and implementation of new teaching strategies. Based on a
technological research project, we present how students from a public school in
S\~ao Paulo's countryside conducted e-waste reuse processes, applying the
principles of metarecycling and physics knowledge to build a portable battery
(power bank) and a smartphone charger powered by a dynamo attached to a
bicycle. The appropriation of the relationships between science, technology,
and social aspects was facilitated by validating the chargers through
characterization tests of the charging time provided by the power bank and the
bicycle-installed device under riding conditions. Educational actions within
the community, involving concepts of sustainability, clean energy, and health
benefits through physical exercise, were guided by the United Nations
Sustainable Development Goals (SDGs).; 8) RM-PoT: Reformulating Mathematical Problems and Solving via Program of
  Thoughts; Recently, substantial advancements have been made in training language models
to carry out step-by-step reasoning for solving intricate numerical reasoning
tasks. Beyond the methods used to solve these problems, the structure and
formulation of the problems themselves also play a crucial role in determining
the performance of large language models. We observe that even small changes in
the surface form of mathematical problems can have a profound impact on both
the answer distribution and solve rate. This highlights the vulnerability of
LLMs to surface-level variations, revealing its limited robustness when
reasoning through complex problems. In this paper, we propose RM-PoT, a
three-stage framework that integrates problem reformulation (RM), code-aided
reasoning (PoT), and domain-aware few-shot learning to address these
limitations. Our approach first reformulates the input problem into diverse
surface forms to reduce structural bias, then retrieves five semantically
aligned examples from a pre-constructed domain-specific question bank to
provide contextual guidance, and finally generates executable Python code for
precise computation.; 9) DPO-Shift: Shifting the Distribution of Direct Preference Optimization; Direct Preference Optimization (DPO) and its variants have become
increasingly popular for aligning language models with human preferences. These
methods aim to teach models to better distinguish between chosen (or preferred)
and rejected (or dispreferred) responses. However, prior research has
identified that the probability of chosen responses often decreases during
training, and this phenomenon is known as likelihood displacement. To tackle
this challenge, in this work we introduce \method to controllably shift the
distribution of the chosen probability. Then, we show that \method exhibits a
fundamental trade-off between improving the chosen probability and sacrificing
the reward margin, as supported by both theoretical analysis and experimental
validation. Furthermore, we demonstrate the superiority of \method over DPO on
downstream tasks such as MT-Bench and a designed win rate experiment. We
believe this study shows that the likelihood displacement issue of DPO can be
effectively mitigated with a simple, theoretically grounded solution. Our code
is available at https://github.com/Meaquadddd/DPO-Shift.; 10) Joint Delay-Doppler Estimation using OFDMA Payloads for Integrated
  Sensing and Communications; The use of future communication systems for sensing offers the potential for
a number of new applications. In this paper, we show that leveraging user data
payloads in multi-node Orthogonal Frequency Division Multiple Access (OFDMA)
networks for estimating target delay and Doppler-shift parameters can yield a
significant advantage in SNR and addressable bandwidth. However, gaps in the
frequency-time resources, reference signal boosting and amplitude modulation
schemes introduce challenges for estimation at the sensing receiver.
  In this work, we propose a joint delay and Doppler-shift model-based
estimator designed to address these challenges. Furthermore, we demonstrate
that incorporating knowledge of the device model into the estimation procedure
helps mitigate the effects of the non-ideal radar ambiguity function caused by
amplitude-modulated user payloads and sparse reference signals. Simulation
results demonstrate that the estimator achieves the theoretical lower bound on
estimation variance.; 11) Block Diffusion: Interpolating Between Autoregressive and Diffusion
  Language Models; Diffusion language models offer unique benefits over autoregressive models
due to their potential for parallelized generation and controllability, yet
they lag in likelihood modeling and are limited to fixed-length generation. In
this work, we introduce a class of block diffusion language models that
interpolate between discrete denoising diffusion and autoregressive models.
Block diffusion overcomes key limitations of both approaches by supporting
flexible-length generation and improving inference efficiency with KV caching
and parallel token sampling. We propose a recipe for building effective block
diffusion models that includes an efficient training algorithm, estimators of
gradient variance, and data-driven noise schedules to minimize the variance.
Block diffusion sets a new state-of-the-art performance among diffusion models
on language modeling benchmarks and enables generation of arbitrary-length
sequences. We provide the code, along with the model weights and blog post on
the project page: https://m-arriola.com/bd3lms/; 12) BrainNet-MoE: Brain-Inspired Mixture-of-Experts Learning for
  Neurological Disease Identification; The Lewy body dementia (LBD) is the second most common neurodegenerative
dementia after Alzheimer's disease (AD). Early differentiation between AD and
LBD is crucial because they require different treatment approaches, but this is
challenging due to significant clinical overlap, heterogeneity, complex
pathogenesis, and the rarity of LBD. While recent advances in artificial
intelligence (AI) demonstrate powerful learning capabilities and offer new hope
for accurate diagnosis, existing methods primarily focus on designing
""neural-level networks"". Our work represents a pioneering effort in modeling
system-level artificial neural network called BrainNet-MoE for brain modeling
and diagnosing. Inspired by the brain's hierarchical organization of bottom-up
sensory integration and top-down control, we design a set of disease-specific
expert groups to process brain sub-network under different condition, A disease
gate mechanism guides the specializa-tion of expert groups, while a transformer
layer enables communication be-tween all sub-networks, generating a
comprehensive whole-brain represen-tation for downstream disease
classification. Experimental results show superior classification accuracy with
interpretable insights into how brain sub-networks contribute to different
neurodegenerative conditions.; 13) Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype
  Identification Using a Novel AUROC Loss Function for Convolutional Neural
  Networks; Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016).; 14) Can KAN CANs? Input-convex Kolmogorov-Arnold Networks (KANs) as
  hyperelastic constitutive artificial neural networks (CANs); Traditional constitutive models rely on hand-crafted parametric forms with
limited expressivity and generalizability, while neural network-based models
can capture complex material behavior but often lack interpretability. To
balance these trade-offs, we present Input-Convex Kolmogorov-Arnold Networks
(ICKANs) for learning polyconvex hyperelastic constitutive laws. ICKANs
leverage the Kolmogorov-Arnold representation, decomposing the model into
compositions of trainable univariate spline-based activation functions for rich
expressivity. We introduce trainable input-convex splines within the KAN
architecture, ensuring physically admissible polyconvex hyperelastic models.
The resulting models are both compact and interpretable, enabling explicit
extraction of analytical constitutive relationships through an input-convex
symbolic regression techinque. Through unsupervised training on full-field
strain data and limited global force measurements, ICKANs accurately capture
nonlinear stress-strain behavior across diverse strain states. Finite element
simulations of unseen geometries with trained ICKAN hyperelastic constitutive
models confirm the framework's robustness and generalization capability.; 15) Parental Guidance: Efficient Lifelong Learning through Evolutionary
  Distillation; Developing robotic agents that can perform well in diverse environments while
showing a variety of behaviors is a key challenge in AI and robotics.
Traditional reinforcement learning (RL) methods often create agents that
specialize in narrow tasks, limiting their adaptability and diversity. To
overcome this, we propose a preliminary, evolution-inspired framework that
includes a reproduction module, similar to natural species reproduction,
balancing diversity and specialization. By integrating RL, imitation learning
(IL), and a coevolutionary agent-terrain curriculum, our system evolves agents
continuously through complex tasks. This approach promotes adaptability,
inheritance of useful traits, and continual learning. Agents not only refine
inherited skills but also surpass their predecessors. Our initial experiments
show that this method improves exploration efficiency and supports open-ended
learning, offering a scalable solution where sparse reward coupled with diverse
terrain environments induces a multi-task setting.; 16) System Message Generation for User Preferences using Open-Source Models; System messages play a crucial role in interactions with large language
models (LLMs), often serving as prompts to initiate conversations. Through
system messages, users can assign specific roles, perform intended tasks,
incorporate background information, specify various output formats and
communication styles. Despite such versatility, publicly available data are
often lack system messages and subject to strict license constraints in the
industry field. Manual labeling of publicly available data with system messages
that align with user instructions demands significant resources. In view of
such challenges, our work introduces SysGen, a pipeline for generating system
messages with better aligned assistant responses from the supervised
fine-tuning dataset without system messages. Training on SysGen data has
demonstrated substantial improvements in the alignment of model responses with
system messages and user instructions, as demonstrated across various
open-source models on the Multifacet benchmark, while maintaining minimal
impact on other unseen benchmarks such as Open LLM Leaderboard 2. Our
qualitative analysis highlights the importance of diverse system messages to
ensure better adaptability across different contexts.; 17) CoDa-4DGS: Dynamic Gaussian Splatting with Context and Deformation
  Awareness for Autonomous Driving; Dynamic scene rendering opens new avenues in autonomous driving by enabling
closed-loop simulations with photorealistic data, which is crucial for
validating end-to-end algorithms. However, the complex and highly dynamic
nature of traffic environments presents significant challenges in accurately
rendering these scenes. In this paper, we introduce a novel 4D Gaussian
Splatting (4DGS) approach, which incorporates context and temporal deformation
awareness to improve dynamic scene rendering. Specifically, we employ a 2D
semantic segmentation foundation model to self-supervise the 4D semantic
features of Gaussians, ensuring meaningful contextual embedding.
Simultaneously, we track the temporal deformation of each Gaussian across
adjacent frames. By aggregating and encoding both semantic and temporal
deformation features, each Gaussian is equipped with cues for potential
deformation compensation within 3D space, facilitating a more precise
representation of dynamic scenes. Experimental results show that our method
improves 4DGS's ability to capture fine details in dynamic scene rendering for
autonomous driving and outperforms other self-supervised methods in 4D
reconstruction and novel view synthesis. Furthermore, CoDa-4DGS deforms
semantic features with each Gaussian, enabling broader applications.; 18) Pricing is All You Need to Improve Traffic Routing; We investigate the design of pricing policies that enhance driver adherence
to route guidance, ensuring effective routing control. The major novelty lies
in that we adopt a Markov chain to model drivers' compliance rates conditioned
on both traffic states and tolls. By formulating the managed traffic network as
a nonlinear stochastic dynamical system, we can quantify in a more realistic
way the impacts of driver route choices and thus determine appropriate tolls.
Specially, we focus on a network comprised of one corridor and one local
street. We assume that a reasonable routing policy is specified in advance.
However, drivers could be reluctant to be detoured. Thus a fixed toll is set on
the corridor to give drivers incentives to choose the local street. We evaluate
the effectiveness of the given routing and pricing policies via stability
analysis. We suggest using the stability and instability conditions to
establish lower and upper bounds on throughput. This allows us to select
suitable tolls that maximize these bounds.; 19) Learning to Optimize Joint Chance-constrained Power Dispatch Problems; The ever-increasing integration of stochastic renewable energy sources into
power systems operation is making the supply-demand balance more challenging.
While joint chance-constrained methods are equipped to model these complexities
and uncertainties, solving these models using the traditional iterative solvers
is time-consuming and can hinder real-time implementation. To overcome the
shortcomings of today's solvers, we propose a fast, scalable, and explainable
machine learning-based optimization proxy. Our solution, called Learning to
Optimize the Optimization of Joint Chance-Constrained Problems (LOOP-JCCP), is
iteration-free and solves the underlying problem in a single-shot. Our model
uses a polyhedral reformulation of the original problem to manage constraint
violations and ensure solution feasibility across various scenarios through
customizable probability settings. To this end, we build on our recent
deterministic solution (LOOP-LC 2.0) by incorporating a set aggregator module
to handle uncertain sample sets of varying sizes and complexities. Our results
verify the feasibility of our near-optimal solutions for joint
chance-constrained power dispatch scenarios. Additionally, our feasibility
guarantees increase the transparency and interpretability of our method, which
is essential for operators to trust the outcomes. We showcase the effectiveness
of our model in solving the stochastic energy management problem of Virtual
Power Plants (VPPs). Our numerical findings complement our theoretical
justifications and demonstrate great flexibility in parameter tuning,
adaptability to diverse datasets, and increased computational speed.; 20) Geometric and topological rigidity of pinched submanifolds II; We continue the study of the geometry and topology of compact submanifolds of
arbitrary codimension in space forms satisfying a certain pinching condition
involving the length of the second fundamental form and the mean curvature. Our
primary focus is on four-dimensional submanifolds, where, to our surprise, both
the results obtained and the methods employed differ significantly and are
notably more intricate compared to those in higher dimensions. This study
heavily relies on concepts from four-dimensional geometry, the geometry of
Riemannian manifolds with nonnegative isotropic curvature, and the Bochner
technique, each playing a crucial role. The results are sharp and extend
previous results by several authors, without imposing any further assumption on
either the mean curvature or the fundamental group of the submanifold.; 21) Diverse Inference and Verification for Advanced Reasoning; Reasoning LLMs such as OpenAI o1, o3 and DeepSeek R1 have made significant
progress in mathematics and coding, yet find challenging advanced tasks such as
International Mathematical Olympiad (IMO) combinatorics problems, Abstraction
and Reasoning Corpus (ARC) puzzles, and Humanity's Last Exam (HLE) questions.
We use a diverse inference approach that combines multiple models and methods
at test time. We find that verifying mathematics and code problems, and
rejection sampling on other problems is simple and effective. We automatically
verify correctness of solutions to IMO problems by Lean, and ARC puzzles by
code, and find that best-of-N effectively answers HLE questions. Our approach
increases answer accuracy on IMO combinatorics problems from 33.3% to 77.8%,
accuracy on HLE questions from 8% to 37%, and solves 80% of ARC puzzles that
948 humans could not and 26.5% of ARC puzzles that o3 high compute does not.
Test-time simulations, reinforcement learning, and meta-learning with inference
feedback improve generalization by adapting agent graph representations and
varying prompts, code, and datasets. Our approach is reliable, robust, and
scalable, and in the spirit of reproducible research, we will make it publicly
available upon publication.; 22) R2LDM: An Efficient 4D Radar Super-Resolution Framework Leveraging
  Diffusion Model; We introduce R2LDM, an innovative approach for generating dense and accurate
4D radar point clouds, guided by corresponding LiDAR point clouds. Instead of
utilizing range images or bird's eye view (BEV) images, we represent both LiDAR
and 4D radar point clouds using voxel features, which more effectively capture
3D shape information. Subsequently, we propose the Latent Voxel Diffusion Model
(LVDM), which performs the diffusion process in the latent space. Additionally,
a novel Latent Point Cloud Reconstruction (LPCR) module is utilized to
reconstruct point clouds from high-dimensional latent voxel features. As a
result, R2LDM effectively generates LiDAR-like point clouds from paired raw
radar data. We evaluate our approach on two different datasets, and the
experimental results demonstrate that our model achieves 6- to 10-fold
densification of radar point clouds, outperforming state-of-the-art baselines
in 4D radar point cloud super-resolution. Furthermore, the enhanced radar point
clouds generated by our method significantly improve downstream tasks,
achieving up to 31.7% improvement in point cloud registration recall rate and
24.9% improvement in object detection accuracy.; 23) Dissipation and particle acceleration in astrophysical jets with
  velocity and magnetic shear: Interaction of Kelvin-Helmholtz and Drift-Kink
  Instabilities; We present 2D particle-in-cell simulations of a magnetized, collisionless,
relativistic pair plasma subjected to combined velocity and magnetic-field
shear, a scenario typical for astrophysical black-hole jet-wind boundaries. We
create conditions where only the Kelvin-Helmholtz (KH) and Drift-Kink (DK)
instabilities can develop, while tearing modes are forbidden. We find that DKI
can effectively disrupt the cats-eye vortices generated by KHI, creating a
turbulent shear layer on the DK timescale. This interplay leads to a
significant enhancement of dissipation over cases with only velocity shear or
only magnetic shear. Moreover, we observe efficient nonthermal particle
acceleration caused by the alignment of the instability-driven electric fields
with Speiser-like motion of particles close to the shear interface. This study
highlights the sensitivity of dissipation to multiple simultaneous
instabilities, thus providing a strong motivation for further studies of their
nonlinear interaction at the kinetic level.; 24) The algebraic structure of Dyson--Schwinger equations with multiple
  insertion places; We give combinatorially controlled series solutions to Dyson--Schwinger
equations with multiple insertion places using tubings of rooted trees and
investigate the algebraic relation between such solutions and the
renormalization group equation.; 25) Galvanic molecular intercalation; The intercalation of molecular species between the layers of van der Waals
(vdW) materials has recently emerged as a powerful approach to combine the
remarkable electronic and magnetic properties of vdW materials with the
chemical flexibility of organic molecules. However, the full transformative
potential of molecular intercalation remains underexplored, largely due to the
lack of simple, broadly applicable methods that preserve high crystalline
quality down to the few-layer limit. Here, we introduce a simple galvanic
approach to intercalate different molecules into various vdW materials under
ambient conditions, leveraging the low reduction potential of selected metals
to enable a spontaneous molecular insertion. We employ our method, which is
particularly well-suited for the in-situ intercalation of few-layer-thick
crystals, to intercalate nine vdW materials, including magnets and
superconductors, with molecules ranging from conventional alkylammonium ions to
metallorganic and bio-inspired chiral cations. Notably, intercalation leads to
a molecule-dependent enhancement of the superconducting transition in 2H-TaS2,
reaching a critical temperature of 4.7 K, higher than TaS2 monolayers.
Additionally, RuCl3 exhibits an unprecedented transition from antiferromagnetic
to ferrimagnetic ordering upon intercalation with cobaltocenium. These results
establish our approach as a versatile technique for engineering atomically thin
quantum materials and heterostructures, unlocking the transformative effects of
molecular intercalation.; 26) High-accuracy evaluation of non-thermal magnetic states beyond spin-wave
  theory: applications to higher-energy states; We present an approximation scheme based on selective Hilbert space
truncation for characterizing non-thermal states of magnetic systems beyond
spin-wave theory. We study applications to states that are inaccessible through
linear spin-wave theory, such as multi-magnon states and higher-energy states.
Our approach is based on the existence of an exact representation of spin
operators in terms of finite-order polynomials of bosonic operators. It can be
applied to systems with and without a magnetically ordered ground state. The
approximation exactly diagonalizes the bosonic Hamiltonian restricted to
particular boson occupation subspaces, improving the conventional linear
spin-wave approach and exponentially reducing the computing time relative to
exact diagonalization schemes. As a test case, we apply the approach to a
prototypical one-dimensional model - an XXZ spin chain with an applied magnetic
field and antisymmetric exchange coupling. Here the antisymmetric coupling
introduces a continuous parameter to tune the system away from its exactly
solvable limit. We find excellent agreement between numerically exact
eigenstates and eigenvalues and those found via the approximation scheme. Our
approach applies not just to higher lying states but also to boson bound
states, which could make them more accessible to theoretical predictions for
comparison with experiment.; 27) Too Little, Too Late: Moderation of Misinformation around the
  Russo-Ukrainian Conflict; In this study, we examine the role of Twitter as a first line of defense
against misinformation by tracking the public engagement with, and the
platforms response to, 500 tweets concerning the RussoUkrainian conflict which
were identified as misinformation. Using a realtime sample of 543 475 of their
retweets, we find that users who geolocate themselves in the U.S. both produce
and consume the largest portion of misinformation, however accounts claiming to
be in Ukraine are the second largest source. At the time of writing, 84% of
these tweets were still available on the platform, especially those having an
anti-Russia narrative. For those that did receive some sanctions, the
retweeting rate has already stabilized, pointing to ineffectiveness of the
measures to stem their spread. These findings point to the need for a change in
the existing anti-misinformation system ecosystem. We propose several design
and research guidelines for its possible improvement.; 28) Koel-TTS: Enhancing LLM based Speech Generation with Preference
  Alignment and Classifier Free Guidance; While autoregressive speech token generation models produce speech with
remarkable variety and naturalness, their inherent lack of controllability
often results in issues such as hallucinations and undesired vocalizations that
do not conform to conditioning inputs. We introduce Koel-TTS, a suite of
enhanced encoder-decoder Transformer TTS models that address these challenges
by incorporating preference alignment techniques guided by automatic speech
recognition and speaker verification models. Additionally, we incorporate
classifier-free guidance to further improve synthesis adherence to the
transcript and reference speaker audio. Our experiments demonstrate that these
optimizations significantly enhance target speaker similarity, intelligibility,
and naturalness of synthesized speech. Notably, Koel-TTS directly maps text and
context audio to acoustic tokens, and on the aforementioned metrics,
outperforms state-of-the-art TTS models, despite being trained on a
significantly smaller dataset. Audio samples and demos are available on our
website.; 29) An Adaptive Collocation Point Strategy For Physics Informed Neural
  Networks via the QR Discrete Empirical Interpolation Method; Physics-informed neural networks (PINNs) have gained significant attention
for solving forward and inverse problems related to partial differential
equations (PDEs). While advancements in loss functions and network
architectures have improved PINN accuracy, the impact of collocation point
sampling on their performance remains underexplored. Fixed sampling methods,
such as uniform random sampling and equispaced grids, can fail to capture
critical regions with high solution gradients, limiting their effectiveness for
complex PDEs. Adaptive methods, inspired by adaptive mesh refinement from
traditional numerical methods, address this by dynamically updating collocation
points during training but may overlook residual dynamics between updates,
potentially losing valuable information. To overcome this limitation, we
propose an adaptive collocation point selection strategy utilizing the QR
Discrete Empirical Interpolation Method (QR-DEIM), a reduced-order modeling
technique for efficiently approximating nonlinear functions. Our results on
benchmark PDEs, including the wave, Allen-Cahn, and Burgers' equations,
demonstrate that our QR-DEIM-based approach improves PINN accuracy compared to
existing methods, offering a promising direction for adaptive collocation point
strategies.; 30) 2-Coherent Internal Models of Homotopical Type Theory; The program of internal type theory seeks to develop the categorical model
theory of dependent type theory using the language of dependent type theory
itself. In the present work we study internal homotopical type theory by
relaxing the notion of a category with families (cwf) to that of a wild, or
precoherent higher cwf, and determine coherence conditions that suffice to
recover properties expected of models of dependent type theory. The result is a
definition of a split 2-coherent wild cwf, which admits as instances both the
syntax and the ""standard model"" given by a universe type. This allows us to
give a straightforward internalization of the notion of a 2-coherent reflection
of homotopical type theory in itself: namely as a 2-coherent wild cwf morphism
from the syntax to the standard model. Our theory also easily specializes to
give definitions of ""low-dimensional"" higher cwfs, and conjecturally includes
the container higher model as a further instance.; 31) Comparison of near-field light intensities: plasmon nanofocusing vs
  localized plasmon resonance; The localized surface plasmon resonance of metallic nanostructures produces
strongly localized and enhanced near-field light, significantly contributing to
nanophotonics research and applications. Plasmon nanofocusing represents
another method for generating near-field light through the propagation and
condensation of plasmons on tapered plasmonic structures. In both methods, the
intensity of near-field light is a critical aspect for many applications. In
this study, we numerically inspect and compare the intensities of near-field
light generated by either localized plasmon resonance or plasmon nanofocusing.
To account for the light-induced changes in the optical properties of plasmonic
structures, which in turn influence the near-field light intensity, we couple
electromagnetic and thermal calculations to consider in a fully self-consistent
manner the effects of the incident light and the light-induced temperature rise
within the metal. A gold nanorod and a cone were adopted for exciting the
localized plasmon resonance and plasmon nanofocusing, respectively. We find
that plasmon nanofocusing generates approximately 1.5 times as strong
near-field light as localized plasmon resonance. Our research provides a
necessary foundation for generating near-field light, which is crucial for
advancing the applications of near-field optics.; 32) HarmonySet: A Comprehensive Dataset for Understanding Video-Music
  Semantic Alignment and Temporal Synchronization; This paper introduces HarmonySet, a comprehensive dataset designed to advance
video-music understanding. HarmonySet consists of 48,328 diverse video-music
pairs, annotated with detailed information on rhythmic synchronization,
emotional alignment, thematic coherence, and cultural relevance. We propose a
multi-step human-machine collaborative framework for efficient annotation,
combining human insights with machine-generated descriptions to identify key
transitions and assess alignment across multiple dimensions. Additionally, we
introduce a novel evaluation framework with tasks and metrics to assess the
multi-dimensional alignment of video and music, including rhythm, emotion,
theme, and cultural context. Our extensive experiments demonstrate that
HarmonySet, along with the proposed evaluation framework, significantly
improves the ability of multimodal models to capture and analyze the intricate
relationships between video and music.; 33) SVGS-DSGAT: An IoT-Enabled Innovation in Underwater Robotic Object
  Detection Technology; With the advancement of Internet of Things (IoT) technology, underwater
target detection and tracking have become increasingly important for ocean
monitoring and resource management. Existing methods often fall short in
handling high-noise and low-contrast images in complex underwater environments,
lacking precision and robustness. This paper introduces a novel SVGS-DSGAT
model that combines GraphSage, SVAM, and DSGAT modules, enhancing feature
extraction and target detection capabilities through graph neural networks and
attention mechanisms. The model integrates IoT technology to facilitate
real-time data collection and processing, optimizing resource allocation and
model responsiveness. Experimental results demonstrate that the SVGS-DSGAT
model achieves an mAP of 40.8% on the URPC 2020 dataset and 41.5% on the
SeaDronesSee dataset, significantly outperforming existing mainstream models.
This IoT-enhanced approach not only excels in high-noise and complex
backgrounds but also improves the overall efficiency and scalability of the
system. This research provides an effective IoT solution for underwater target
detection technology, offering significant practical application value and
broad development prospects.; 34) PersonaBench: Evaluating AI Models on Understanding Personal Information
  through Accessing (Synthetic) Private User Data; Personalization is critical in AI assistants, particularly in the context of
private AI models that work with individual users. A key scenario in this
domain involves enabling AI models to access and interpret a user's private
data (e.g., conversation history, user-AI interactions, app usage) to
understand personal details such as biographical information, preferences, and
social connections. However, due to the sensitive nature of such data, there
are no publicly available datasets that allow us to assess an AI model's
ability to understand users through direct access to personal information.
  To address this gap, we introduce a synthetic data generation pipeline that
creates diverse, realistic user profiles and private documents simulating human
activities. Leveraging this synthetic data, we present PersonaBench, a
benchmark designed to evaluate AI models' performance in understanding personal
information derived from simulated private user data.
  We evaluate Retrieval-Augmented Generation (RAG) pipelines using questions
directly related to a user's personal information, supported by the relevant
private documents provided to the models. Our results reveal that current
retrieval-augmented AI models struggle to answer private questions by
extracting personal information from user documents, highlighting the need for
improved methodologies to enhance personalization capabilities in AI.; 35) Comparison of the detector response and calibration function of metallic
  microcalorimeters for X-ray photons and external electrons; Metallic microcalorimeters (MMCs) are cryogenic single-particle detectors
that rely on a calorimetric detection principle. Due to their excellent energy
resolution, close-to-ideal linear detector response, fast signal rise time and
the potential for \SI{100}{\%} quantum efficiency, MMCs outperform conventional
detectors by several orders of magnitude in resolution. These attributes make
them particularly interesting for a broad spectrum of applications, including a
next-generation neutrino mass experiment based on the measurement of the
tritium beta-decay spectrum, with an objective of achieving a sensitivity
surpassing that of the pioneering KATRIN experiment. However, although MMCs
have been used in measurements of photons and heavy ions with great success, no
information is currently available on the interaction between MMCs and external
light charged particles such as electrons. This work aims to provide such
missing information and to demonstrate that MMC-based detectors are suitable
for high-resolution spectroscopy of external electron sources. Particularly, we
present the first-ever measurements of external electrons using a metallic
microcalorimeter, comprehensively discuss the characteristics of the signal
shape and the calibration function and give a direct comparison between
well-defined conversion electron and X-ray photon signals from the same
$^{83}$Rb/$^{83m}$Kr source.; 36) Bridging Writing Manner Gap in Visual Instruction Tuning by Creating
  LLM-aligned Instructions; In the realm of Large Multi-modal Models (LMMs), the instruction quality
during the visual instruction tuning stage significantly influences the
performance of modality alignment. In this paper, we assess the instruction
quality from a unique perspective termed \textbf{Writing Manner}, which
encompasses the selection of vocabulary, grammar and sentence structure to
convey specific semantics. We argue that there exists a substantial writing
manner gap between the visual instructions and the base Large Language Models
(LLMs) within LMMs. This gap forces the pre-trained base LLMs to deviate from
their original writing styles, leading to capability degradation of both base
LLMs and LMMs. To bridge the writing manner gap while preserving the original
semantics, we propose directly leveraging the base LLM to align the writing
manner of soft-format visual instructions with that of the base LLM itself,
resulting in novel LLM-aligned instructions. The manual writing manner
evaluation results demonstrate that our approach successfully minimizes the
writing manner gap. By utilizing LLM-aligned instructions, the baseline models
LLaVA-7B and QwenVL demonstrate enhanced resistance to hallucinations and
non-trivial comprehensive improvements across all $15$ visual and language
benchmarks.; 37) State preparation with parallel-sequential circuits; We introduce parallel-sequential (PS) circuits, a family of quantum circuit
layouts that interpolate between brickwall and sequential circuits, which
introduces control parameters governing the ratio of over the amount of
entanglement and the maximum correlation distance they can express. We provide
numerical evidence that PS circuits can efficiently prepare many-body ground
states in one dimension. On noisy devices, characterized through both idling
errors and two-qubit gate errors, we show that in a wide parameter regime, PS
circuits outperform brickwall, sequential, and log-depth circuits from [Malz,
Styliaris, Wei, Cirac, PRL 132, 040404 (2024)]. Additionally, we demonstrate
that properly chosen noisy random PS circuits suppress error proliferation and,
when employed as a variational ansatz, exhibit superior trainability.; 38) Yang-Lee Zeros of 2D Nearest-Neighbor Antiferromagnetic Ising Models: A
  Numerical Linked Cluster Expansion Study; We study Yang-Lee zeros in the thermodynamic limit of the 2D nearest-neighbor
antiferromagnetic Ising model on square and triangular lattices. We employ the
Numerical Linked Cluster Expansion (NLCE) equipped with Exact Enumeration (EE)
of the partition function to estimate the Laplacian of the free energy, which
is proportional to the zeros density. Using a modified NLCE, where the
expansion can be carried directly on the Yang-Lee zeros of the involved
clusters, we estimate the density of Yang-Lee zeros in the thermodynamic limit.
NLCE gives significantly more zeros than EE in the complex field plane
providing more insights on how the root curves look in the thermodynamic limit.
For the square lattice at $T \ll T_c$, the results suggest that two vertical
lines at $\pm h_c(T)$ in the complex field plane (i.e two concentric circles in
the complex fugacity plane) are the thermodynamic root curves. A similar
picture is expected for the triangular lattice for phase transitions at large
values of magnetic field while further study is needed for phase transitions at
smaller values of magnetic field. The convergence of the NLCE and (EE)
calculations of the partition function to the thermodynamic limit is studied in
both lattices and the temperature-field phase diagram is obtained from Yang-Lee
zeros using both methods. This NLCE-based approach will facilitate the study of
different types of phase transitions using Yang-Lee zeros in future research.; 39) Equivariant formality of the little disks operad; The little $n$-disks operad is $SO(n)$ and $O(n)$-equivariantly formal over
the rationals. Equivalently, the oriented and unoriented framed little disks
operads are rationally formal as $\infty$-operads.; 40) Adiabatic charge transport in extended SSH models; We explore the topological properties of extended SSH models, considering
four sub-lattices in a unit cell and second-nearest-neighbor intercell hopping
for SSH4 and SSH long-range (SSHLR) models, respectively. The additional tuning
parameters cause the SSH4 (SSHLR) model to host chiral symmetry protected two
(two and four) zero-energy modes producing a richer phase diagram that we
characterize by momentum space, periodic-bulk and open-bulk real space winding
numbers. We introduce time to study charge transport in the periodically driven
SSH4 and SSHLR models under the adiabatic limit. We remarkably find that the
whole parameter space turned topological for a certain choice of the remaining
parameters leading to always finite quantized value of pumped charge at the end
of a complete cycle. Considering time as another variable, we characterize
these new phases of the driven models by momentum space Chern number,
periodic-bulk and open-bulk real space Bott index. We also investigate the time
evolution of pumped charge for these models and connect it with the intriguing
windings of the mid-gap energy levels with time. Interestingly, the maximum
value of Chern number or Bott index for the driven models is more than that of
the winding number associated with the static model indicating the fact that
there exist more zero-energy modes during the full course of a driving cycle
compared to the underlying static models. We further extend our study to the
quantum metric where the fluctuations in the above quantity can identify the
presence of a topological phase boundary.; 41) Phantom: Subject-consistent video generation via cross-modal alignment; The continuous development of foundational models for video generation is
evolving into various applications, with subject-consistent video generation
still in the exploratory stage. We refer to this as Subject-to-Video, which
extracts subject elements from reference images and generates
subject-consistent video through textual instructions. We believe that the
essence of subject-to-video lies in balancing the dual-modal prompts of text
and image, thereby deeply and simultaneously aligning both text and visual
content. To this end, we propose Phantom, a unified video generation framework
for both single and multi-subject references. Building on existing
text-to-video and image-to-video architectures, we redesign the joint
text-image injection model and drive it to learn cross-modal alignment via
text-image-video triplet data. In particular, we emphasize subject consistency
in human generation, covering existing ID-preserving video generation while
offering enhanced advantages. The project homepage is here
https://phantom-video.github.io/Phantom/.; 42) The $z \gtrsim 9$ galaxy UV luminosity function from the JWST Advanced
  Deep Extragalactic Survey: insights into early galaxy evolution and
  reionization; The high-redshift UV luminosity function provides important insights into the
evolution of early galaxies. JWST has revealed an unexpectedly large population
of bright ($M_\mathrm{UV} \lesssim -20$) galaxies at $z\gtrsim10$, implying
fundamental changes in the star forming properties of galaxies at increasingly
early times. However, constraining the fainter population ($M_\mathrm{UV}
\gtrsim -18$) has been more challenging. In this work, we present the
$z\gtrsim9$ UV luminosity function from the JWST Advanced Deep Extragalactic
Survey. We calculate the UV luminosity function from several hundred
$z\gtrsim9$ galaxy candidates that reach UV luminosities of
$M_\mathrm{UV}\sim-17$ in redshift bins of $z\sim9-12$ (309 candidates) and
$z\sim12-16$ (63 candidates). We search for candidates at $z\sim16-22.5$ and
find none. We also estimate the $z\sim14-16$ luminosity function from the
$z\geq14$ subset of the $z\sim12-16$ sample. Consistent with other
measurements, we find an excess of bright galaxies that is in tension with many
theoretical models, especially at $z\gtrsim12$. However, we also find high
number densities at $-18\lesssim M_\mathrm{UV} \lesssim-17$, suggesting that
there is a larger population of faint galaxies than expected, as well as bright
ones. From our parametric fits for the luminosity function, we find steep faint
end slopes of $-2.5\lesssim\alpha\lesssim-2.3$, suggesting a large population
of faint ($M_\mathrm{UV} \gtrsim -17$) galaxies. Combined, the high
normalization and steep faint end slope of the luminosity function could imply
that the reionization process is appreciably underway as early as $z=10$.; 43) Relativistic model of spontaneous wave-function localization induced by
  nonHermitian colored noise; We propose a relativistic model of spontaneous wave-function collapse, based
on a random nonHermitian action where the fermion density operator is coupled
to a universal colored noise. Upon quantization, the wave function obeys a
nonlinear stochastic differential equation that respects statistical Lorentz
symmetry. The localization mechanism is driven by the colored noise, derived
from the d'Alembert equation using generalized stochastic calculus in
1+3-dimensional spacetime. We analytically determine the noise-induced
localization length, which decreases as the size of the observable universe
increases.; 44) Aportes para el cumplimiento del Reglamento (UE) 2024/1689 en rob\'otica
  y sistemas aut\'onomos; Cybersecurity in robotics stands out as a key aspect within Regulation (EU)
2024/1689, also known as the Artificial Intelligence Act, which establishes
specific guidelines for intelligent and automated systems. A fundamental
distinction in this regulatory framework is the difference between robots with
Artificial Intelligence (AI) and those that operate through automation systems
without AI, since the former are subject to stricter security requirements due
to their learning and autonomy capabilities. This work analyzes cybersecurity
tools applicable to advanced robotic systems, with special emphasis on the
protection of knowledge bases in cognitive architectures. Furthermore, a list
of basic tools is proposed to guarantee the security, integrity, and resilience
of these systems, and a practical case is presented, focused on the analysis of
robot knowledge management, where ten evaluation criteria are defined to ensure
compliance with the regulation and reduce risks in human-robot interaction
(HRI) environments.; 45) Large Language Models for Knowledge Graph Embedding Techniques, Methods,
  and Challenges: A Survey; Large Language Models (LLMs) have attracted a lot of attention in various
fields due to their superior performance, aiming to train hundreds of millions
or more parameters on large amounts of text data to understand and generate
natural language. As the superior performance of LLMs becomes apparent, they
are increasingly being applied to knowledge graph embedding (KGE) related tasks
to improve the processing results. As a deep learning model in the field of
Natural Language Processing (NLP), it learns a large amount of textual data to
predict the next word or generate content related to a given text. However,
LLMs have recently been invoked to varying degrees in different types of KGE
related scenarios such as multi-modal KGE and open KGE according to their task
characteristics. In this paper, we investigate a wide range of approaches for
performing LLMs-related tasks in different types of KGE scenarios. To better
compare the various approaches, we summarize each KGE scenario in a
classification. In addition to the categorization methods, we provide a tabular
overview of the methods and their source code links for a more direct
comparison. In the article we also discuss the applications in which the
methods are mainly used and suggest several forward-looking directions for the
development of this new research area.; 46) Explainability and AI Confidence in Clinical Decision Support Systems:
  Effects on Trust, Diagnostic Performance, and Cognitive Load in Breast Cancer
  Care; Artificial Intelligence (AI) has demonstrated potential in healthcare,
particularly in enhancing diagnostic accuracy and decision-making through
Clinical Decision Support Systems (CDSSs). However, the successful
implementation of these systems relies on user trust and reliance, which can be
influenced by explainable AI. This study explores the impact of varying
explainability levels on clinicians trust, cognitive load, and diagnostic
performance in breast cancer detection. Utilizing an interrupted time series
design, we conducted a web-based experiment involving 28 healthcare
professionals. The results revealed that high confidence scores substantially
increased trust but also led to overreliance, reducing diagnostic accuracy. In
contrast, low confidence scores decreased trust and agreement while increasing
diagnosis duration, reflecting more cautious behavior. Some explainability
features influenced cognitive load by increasing stress levels. Additionally,
demographic factors such as age, gender, and professional role shaped
participants' perceptions and interactions with the system. This study provides
valuable insights into how explainability impact clinicians' behavior and
decision-making. The findings highlight the importance of designing AI-driven
CDSSs that balance transparency, usability, and cognitive demands to foster
trust and improve integration into clinical workflows.; 47) Dream-IF: Dynamic Relative EnhAnceMent for Image Fusion; Image fusion aims to integrate comprehensive information from images acquired
through multiple sources. However, images captured by diverse sensors often
encounter various degradations that can negatively affect fusion quality.
Traditional fusion methods generally treat image enhancement and fusion as
separate processes, overlooking the inherent correlation between them; notably,
the dominant regions in one modality of a fused image often indicate areas
where the other modality might benefit from enhancement. Inspired by this
observation, we introduce the concept of dominant regions for image enhancement
and present a Dynamic Relative EnhAnceMent framework for Image Fusion
(Dream-IF). This framework quantifies the relative dominance of each modality
across different layers and leverages this information to facilitate reciprocal
cross-modal enhancement. By integrating the relative dominance derived from
image fusion, our approach supports not only image restoration but also a
broader range of image enhancement applications. Furthermore, we employ
prompt-based encoding to capture degradation-specific details, which
dynamically steer the restoration process and promote coordinated enhancement
in both multi-modal image fusion and image enhancement scenarios. Extensive
experimental results demonstrate that Dream-IF consistently outperforms its
counterparts.; 48) Spike-and-Slab Posterior Sampling in High Dimensions; Posterior sampling with the spike-and-slab prior [MB88], a popular multimodal
distribution used to model uncertainty in variable selection, is considered the
theoretical gold standard method for Bayesian sparse linear regression [CPS09,
Roc18]. However, designing provable algorithms for performing this sampling
task is notoriously challenging. Existing posterior samplers for Bayesian
sparse variable selection tasks either require strong assumptions about the
signal-to-noise ratio (SNR) [YWJ16], only work when the measurement count grows
at least linearly in the dimension [MW24], or rely on heuristic approximations
to the posterior. We give the first provable algorithms for spike-and-slab
posterior sampling that apply for any SNR, and use a measurement count
sublinear in the problem dimension. Concretely, assume we are given a
measurement matrix $\mathbf{X} \in \mathbb{R}^{n\times d}$ and noisy
observations $\mathbf{y} = \mathbf{X}\mathbf{\theta}^\star + \mathbf{\xi}$ of a
signal $\mathbf{\theta}^\star$ drawn from a spike-and-slab prior $\pi$ with a
Gaussian diffuse density and expected sparsity k, where $\mathbf{\xi} \sim
\mathcal{N}(\mathbb{0}_n, \sigma^2\mathbf{I}_n)$. We give a polynomial-time
high-accuracy sampler for the posterior $\pi(\cdot \mid \mathbf{X},
\mathbf{y})$, for any SNR $\sigma^{-1}$ > 0, as long as $n \geq k^3 \cdot
\text{polylog}(d)$ and $X$ is drawn from a matrix ensemble satisfying the
restricted isometry property. We further give a sampler that runs in
near-linear time $\approx nd$ in the same setting, as long as $n \geq k^5 \cdot
\text{polylog}(d)$. To demonstrate the flexibility of our framework, we extend
our result to spike-and-slab posterior sampling with Laplace diffuse densities,
achieving similar guarantees when $\sigma = O(\frac{1}{k})$ is bounded.; 49) Strands Rocq: Why is a Security Protocol Correct, Mechanically?; Strand spaces are a formal framework for symbolic protocol verification that
allows for pen-and-paper proofs of security. While extremely insightful,
pen-and-paper proofs are error-prone, and it is hard to gain confidence on
their correctness. To overcome this problem, we developed StrandsRocq, a full
mechanization of the strand spaces in Coq (soon to be renamed Rocq). The
mechanization was designed to be faithful to the original pen-and-paper
development, and it was engineered to be modular and extensible. StrandsRocq
incorporates new original proof techniques, a novel notion of maximal
penetrator that enables protocol compositionality, and a set of Coq tactics
tailored to the domain, facilitating proof automation and reuse, and
simplifying the work of protocol analysts. To demonstrate the versatility of
our approach, we modelled and analyzed a family of authentication protocols,
drawing inspiration from ISO/IEC 9798-2 two-pass authentication, the classical
Needham-Schroeder-Lowe protocol, as well as a recently-proposed static analysis
for a key management API. The analyses in StrandsRocq confirmed the high degree
of proof reuse, and enabled us to distill the minimal requirements for protocol
security. Through mechanization, we identified and addressed several issues in
the original proofs and we were able to significantly improve the precision of
the static analysis for the key management API. Moreover, we were able to
leverage the novel notion of maximal penetrator to provide a compositional
proof of security for two simple authentication protocols.; 50) Sketch Disaggregation Across Time and Space; Streaming analytics are essential in a large range of applications, including
databases, networking, and machine learning. To optimize performance,
practitioners are increasingly offloading such analytics to network nodes such
as switches. However, resources such as fast SRAM memory available at switches
are limited, not uniform, and may serve other functionalities as well (e.g.,
firewall). Moreover, resource availability can also change over time due to the
dynamic demands of in-network applications.
  In this paper, we propose a new approach to disaggregating data structures
over time and space, leveraging any residual resource available at network
nodes. We focus on sketches, which are fundamental for summarizing data for
streaming analytics while providing beneficial space-accuracy tradeoffs. Our
idea is to break sketches into multiple `fragments' that are placed at
different network nodes. The fragments cover different time periods and are of
varying sizes, and are combined to form a network-wide view of the underlying
traffic. We apply our solution to three popular sketches (namely, Count Sketch,
Count-Min Sketch, and UnivMon) and demonstrate we can achieve approximately a
75% memory size reduction for the same error for many queries, or a near
order-of-magnitude error reduction if memory is kept unchanged.; 51) The quantromon: A qubit-resonator system with orthogonal qubit and
  readout modes; The measurement of a superconducting qubit is implemented by coupling it to a
resonator. The common choice is transverse coupling, which, in the dispersive
approximation, introduces an interaction term which enables the measurement.
This cross-Kerr term provides a qubit-state dependent dispersive shift in the
resonator frequency with the device parameters chosen carefully to get
sufficient signal while minimizing Purcell decay of the qubit. We introduce a
two-mode circuit, nicknamed quantromon, with two orthogonal modes implementing
a qubit and a resonator. Unlike before, where the coupling term emerges as a
perturbative expansion, the quantromon has intrinsic cross-Kerr coupling by
design. Our experiments implemented in a hybrid 2D-3D cQED architecture
demonstrate some unique features of the quantromon like weak dependence of the
dispersive shift on the qubit-resonator detuning and intrinsic Purcell
protection. In a tunable qubit-frequency device, we show that the dispersive
shift ($2\chi/2\pi$) changes by only $0.8$ MHz while the qubit-resonator
detuning ($\Delta/2\pi$) is varied between $0.398$ GHz - $3.288$ GHz. We also
demonstrate Purcell protection in a second device where we tune the
orthogonality between the two modes. Finally, we demonstrate a single-shot
readout fidelity of $98.3\%$ without using a parametric amplifier which is
comparable to the state-of-the-art and suggests a potential simplification of
the measurement circuitry for scaling up quantum processors.; 52) Asking for Help Enables Safety Guarantees Without Sacrificing
  Effectiveness; Most reinforcement learning algorithms with regret guarantees rely on a
critical assumption: that all errors are recoverable. Recent work by Plaut et
al. discarded this assumption and presented algorithms that avoid ""catastrophe""
(i.e., irreparable errors) by asking for help. However, they provided only
safety guarantees and did not consider reward maximization. We prove that any
algorithm that avoids catastrophe in their setting also guarantees high reward
(i.e., sublinear regret) in any Markov Decision Process (MDP), including MDPs
with irreversible costs. This constitutes the first no-regret guarantee for
general MDPs. More broadly, our result may be the first formal proof that it is
possible for an agent to obtain high reward while becoming self-sufficient in
an unknown, unbounded, and high-stakes environment without causing catastrophe
or requiring resets.; 53) DynASyn: Multi-Subject Personalization Enabling Dynamic Action Synthesis; Recent advances in text-to-image diffusion models spurred research on
personalization, i.e., a customized image synthesis, of subjects within
reference images. Although existing personalization methods are able to alter
the subjects' positions or to personalize multiple subjects simultaneously,
they often struggle to modify the behaviors of subjects or their dynamic
interactions. The difficulty is attributable to overfitting to reference
images, which worsens if only a single reference image is available. We propose
DynASyn, an effective multi-subject personalization from a single reference
image addressing these challenges. DynASyn preserves the subject identity in
the personalization process by aligning concept-based priors with subject
appearances and actions. This is achieved by regularizing the attention maps
between the subject token and images through concept-based priors. In addition,
we propose concept-based prompt-and-image augmentation for an enhanced
trade-off between identity preservation and action diversity. We adopt an
SDE-based editing guided by augmented prompts to generate diverse appearances
and actions while maintaining identity consistency in the augmented images.
Experiments show that DynASyn is capable of synthesizing highly realistic
images of subjects with novel contexts and dynamic interactions with the
surroundings, and outperforms baseline methods in both quantitative and
qualitative aspects.; 54) Byzantine Consensus in the Random Asynchronous Model; We propose a novel relaxation of the classic asynchronous network model,
called the random asynchronous model, which removes adversarial message
scheduling while preserving unbounded message delays and Byzantine faults.
Instead of an adversary dictating message order, delivery follows a random
schedule. We analyze Byzantine consensus at different resilience thresholds
($n=3f+1$, $n=2f+1$, and $n=f+2$) and show that our relaxation allows consensus
with probabilistic guarantees which are impossible in the standard asynchronous
model or even the partially synchronous model. We complement these protocols
with corresponding impossibility results, establishing the limits of consensus
in the random asynchronous model.; 55) Target Detection in OFDM-ISAC Systems: A Multipath Exploitation Approach; This paper investigates the potential of multipath exploitation for enhancing
target detection in orthogonal frequency division multiplexing (OFDM)-based
integrated sensing and communication (ISAC) systems. The study aims to improve
target detection performance by harnessing the diversity gain in the
delay-Doppler domain. We propose a weighted generalized likelihood ratio test
(GLRT) detector that effectively leverages the multipath propagation between
the base station (BS) and the target. To further enhance detection accuracy, a
joint optimization framework is developed for subcarrier power allocation at
the transmitter and weight coefficients of the GLRT detector. The objective is
to maximize the probability of target detection while satisfying constraints on
total transmit power and the communication receiver's signal-to-noise ratio
(SNR). An iterative algorithm based on the majorization-minimization (MM)
method is employed to address the resulting non-convex optimization problem.
Simulation results demonstrate the efficacy of the proposed algorithm and
confirm the benefits of multipath exploitation for target detection in
OFDM-ISAC systems under multipath-rich environments.; 56) Idiom Detection in Sorani Kurdish Texts; Idiom detection using Natural Language Processing (NLP) is the computerized
process of recognizing figurative expressions within a text that convey
meanings beyond the literal interpretation of the words. While idiom detection
has seen significant progress across various languages, the Kurdish language
faces a considerable research gap in this area despite the importance of idioms
in tasks like machine translation and sentiment analysis. This study addresses
idiom detection in Sorani Kurdish by approaching it as a text classification
task using deep learning techniques. To tackle this, we developed a dataset
containing 10,580 sentences embedding 101 Sorani Kurdish idioms across diverse
contexts. Using this dataset, we developed and evaluated three deep learning
models: KuBERT-based transformer sequence classification, a Recurrent
Convolutional Neural Network (RCNN), and a BiLSTM model with an attention
mechanism. The evaluations revealed that the transformer model, the fine-tuned
BERT, consistently outperformed the others, achieving nearly 99% accuracy while
the RCNN achieved 96.5% and the BiLSTM 80%. These results highlight the
effectiveness of Transformer-based architectures in low-resource languages like
Kurdish. This research provides a dataset, three optimized models, and insights
into idiom detection, laying a foundation for advancing Kurdish NLP.; 57) State-of-the-Art Transformer Models for Image Super-Resolution:
  Techniques, Challenges, and Applications; Image Super-Resolution (SR) aims to recover a high-resolution image from its
low-resolution counterpart, which has been affected by a specific degradation
process. This is achieved by enhancing detail and visual quality. Recent
advancements in transformer-based methods have remolded image super-resolution
by enabling high-quality reconstructions surpassing previous deep-learning
approaches like CNN and GAN-based. This effectively addresses the limitations
of previous methods, such as limited receptive fields, poor global context
capture, and challenges in high-frequency detail recovery. Additionally, the
paper reviews recent trends and advancements in transformer-based SR models,
exploring various innovative techniques and architectures that combine
transformers with traditional networks to balance global and local contexts.
These neoteric methods are critically analyzed, revealing promising yet
unexplored gaps and potential directions for future research. Several
visualizations of models and techniques are included to foster a holistic
understanding of recent trends. This work seeks to offer a structured roadmap
for researchers at the forefront of deep learning, specifically exploring the
impact of transformers on super-resolution techniques.; 58) Photon-assisted stochastic resonance in nanojunctions; We study stochastic resonance in molecular junctions driven by a
periodically-varying external field. This is done using the time-dependent
Landauer-B{\""u}ttiker formalism, which follows from exact analytical solutions
to the Kadanoff-Baym equations describing the molecular junction subject to an
arbitrary time-dependent bias. We focus on a double quantum dot nanojunction
and compare the effects of the temperature with the fluctuating bias in the
statically-driven case. We then consider the combined effect of AC-driving and
white noise fluctuations on the rectified current through the nanojunction, and
find a stochastic resonance effect, where at certain driving conditions the
bias fluctuations enhance the current signal. The study is then extended to
include the color noise in the applied bias, so that the combined effect of the
color noise correlation time and driving frequency on stochastic resonance is
investigated. We thereby demonstrate that photon-assisted transport can be
optimized by a suitably tuned environment.; 59) Enhancing Health Information Retrieval with RAG by Prioritizing Topical
  Relevance and Factual Accuracy; The exponential surge in online health information, coupled with its
increasing use by non-experts, highlights the pressing need for advanced Health
Information Retrieval models that consider not only topical relevance but also
the factual accuracy of the retrieved information, given the potential risks
associated with health misinformation. To this aim, this paper introduces a
solution driven by Retrieval-Augmented Generation (RAG), which leverages the
capabilities of generative Large Language Models (LLMs) to enhance the
retrieval of health-related documents grounded in scientific evidence. In
particular, we propose a three-stage model: in the first stage, the user's
query is employed to retrieve topically relevant passages with associated
references from a knowledge base constituted by scientific literature. In the
second stage, these passages, alongside the initial query, are processed by
LLMs to generate a contextually relevant rich text (GenText). In the last
stage, the documents to be retrieved are evaluated and ranked both from the
point of view of topical relevance and factual accuracy by means of their
comparison with GenText, either through stance detection or semantic
similarity. In addition to calculating factual accuracy, GenText can offer a
layer of explainability for it, aiding users in understanding the reasoning
behind the retrieval. Experimental evaluation of our model on benchmark
datasets and against baseline models demonstrates its effectiveness in
enhancing the retrieval of both topically relevant and factually accurate
health information, thus presenting a significant step forward in the health
misinformation mitigation problem.; 60) Einstein Constants and Smooth Topology; It was first shown in (Catanese-LeBrun 1997) that certain high-dimensional
smooth closed manifolds admit pairs of Einstein metrics with Ricci curvatures
of opposite sign. After reviewing subsequent progress that has been made on
this topic, we then prove various related results, with the ultimate goal of
stimulating further research on associated questions.; 61) Mitigating Hallucinations on Object Attributes using Multiview Images
  and Negative Instructions; Current popular Large Vision-Language Models (LVLMs) are suffering from
Hallucinations on Object Attributes (HoOA), leading to incorrect determination
of fine-grained attributes in the input images. Leveraging significant
advancements in 3D generation from a single image, this paper proposes a novel
method to mitigate HoOA in LVLMs. This method utilizes multiview images sampled
from generated 3D representations as visual prompts for LVLMs, thereby
providing more visual information from other viewpoints. Furthermore, we
observe the input order of multiple multiview images significantly affects the
performance of LVLMs. Consequently, we have devised Multiview Image Augmented
VLM (MIAVLM), incorporating a Multiview Attributes Perceiver (MAP) submodule
capable of simultaneously eliminating the influence of input image order and
aligning visual information from multiview images with Large Language Models
(LLMs). Besides, we designed and employed negative instructions to mitigate
LVLMs' bias towards ``Yes"" responses. Comprehensive experiments demonstrate the
effectiveness of our method.; 62) Star-Forming Nuclear Clusters in Dwarf Galaxies Mimicking AGN Signatures
  in the Mid-Infrared; Effectively finding and identifying active galactic nuclei (AGNs) in dwarf
galaxies is an important step in studying black hole formation and evolution.
In this work, we examine four mid-IR-selected AGN candidates in dwarf galaxies
with stellar masses between $M_\star \sim 10^8 - 10^9 M_\odot$ , and find that
the galaxies are host to nuclear star clusters (NSCs) that are notably rare in
how young and massive they are. We perform photometric measurements on the
central star clusters in our target galaxies galaxies using Hubble Space
Telescope optical and near-IR imaging and compare their observed properties to
models of stellar population evolution. We find that these galaxies are host to
very massive ($\sim10^7 M_\odot$), extremely young ($\lesssim 8$ Myr), dusty
($0.6 \lesssim \mathrm{A_v} \lesssim 1.8$) nuclear star clusters. Our results
indicate that these galactic nuclei have ongoing star-formation, are still at
least partially obscured by clouds of gas and dust, and are most likely
producing the extremely red AGN-like mid-IR colors. Moreover, prior work has
shown that these galaxies do not exhibit X-ray or optical AGN signatures.
Therefore, we recommend caution when using mid-IR color-color diagnostics for
AGN selection in dwarf galaxies, since, as directly exemplified in this sample,
they can be contaminated by massive star clusters with ongoing star formation.; 63) Decoding Interpretable Logic Rules from Neural Networks; As deep neural networks continue to excel across various domains, their
black-box nature has raised concerns about transparency and trust. In
particular, interpretability has become increasingly essential for applications
that demand high safety and knowledge rigor, such as drug discovery, autonomous
driving, and genomics. However, progress in understanding even the simplest
deep neural networks - such as fully connected networks - has been limited,
despite their role as foundational elements in state-of-the-art models like
ResNet and Transformer. In this paper, we address this challenge by introducing
NeuroLogic, a novel approach for decoding interpretable logic rules from neural
networks. NeuroLogic leverages neural activation patterns to capture the
model's critical decision-making processes, translating them into logical rules
represented by hidden predicates. Thanks to its flexible design in the
grounding phase, NeuroLogic can be adapted to a wide range of neural networks.
For simple fully connected neural networks, hidden predicates can be grounded
in certain split patterns of original input features to derive
decision-tree-like rules. For large, complex vision neural networks, NeuroLogic
grounds hidden predicates into high-level visual concepts that are
understandable to humans. Our empirical study demonstrates that NeuroLogic can
extract global and interpretable rules from state-of-the-art models such as
ResNet, a task at which existing work struggles. We believe NeuroLogic can help
pave the way for understanding the black-box nature of neural networks.; 64) Federated Quantum-Train Long Short-Term Memory for Gravitational Wave
  Signal; We present Federated QT-LSTM, a novel framework that combines the
Quantum-Train (QT) methodology with Long Short-Term Memory (LSTM) networks in a
federated learning setup. By leveraging quantum neural networks (QNNs) to
generate classical LSTM model parameters during training, the framework
effectively addresses challenges in model compression, scalability, and
computational efficiency. Importantly, Federated QT-LSTM eliminates the
reliance on quantum devices during inference, making it practical for
real-world applications. Experiments on simulated gravitational wave (GW)
signal datasets demonstrate the framework's superior performance compared to
baseline models, including LSTM and QLSTM, achieving lower training and testing
losses while significantly reducing the number of trainable parameters. The
results also reveal that deeper QT layers enhance model expressiveness for
complex tasks, highlighting the adaptability of the framework. Federated
QT-LSTM provides a scalable and efficient solution for privacy-preserving
distributed learning, showcasing the potential of quantum-inspired techniques
in advancing time-series prediction and signal reconstruction tasks.; 65) Photon-ALP beam propagation from Mrk 501; The very high energy (VHE, E $>$ $100 \mathrm~{GeV}$) $\gamma$-ray
observations offer a possibility of indirectly detecting the presence of
axion-like particles (ALPs). The paper focuses on detecting photon-ALP
oscillations on $\gamma$-ray spectra from distant sources in astrophysical
magnetic fields. Strong evidence indicates that: (1) the photon-ALP
oscillations can effectively decrease the photon absorption at energies of
several tens of TeV -- caused by the extragalactic background light (EBL) -- to
a level able to explain better the observational data; (2) the impact of
magnetic-field models in photon-ALP beams crossing several magnetized media is
significant. We revisit the expected signature for the photon-ALP oscillation
effects on $\gamma-\gamma $ absorption in the TeV spectra of Mrk 501. The
result issues that the photon-ALP beam propagation with mass
$\mathrm{m_a}\sim10^{-10} eV$ and two-photon coupling constant
$\begin{aligned}g_{a\gamma}\sim0.417\times10^{-11}GeV^{-1}\end{aligned}$
crossing reasonable magnetic field scenarios considered here can roughly
reproduce the observed TeV $\gamma$-ray spectra for Mrk 501.; 66) Federated Learning for Anomaly Detection in Energy Consumption Data:
  Assessing the Vulnerability to Adversarial Attacks; Anomaly detection is crucial in the energy sector to identify irregular
patterns indicating equipment failures, energy theft, or other issues. Machine
learning techniques for anomaly detection have achieved great success, but are
typically centralized, involving sharing local data with a central server which
raises privacy and security concerns. Federated Learning (FL) has been gaining
popularity as it enables distributed learning without sharing local data.
However, FL depends on neural networks, which are vulnerable to adversarial
attacks that manipulate data, leading models to make erroneous predictions.
While adversarial attacks have been explored in the image domain, they remain
largely unexplored in time series problems, especially in the energy domain.
Moreover, the effect of adversarial attacks in the FL setting is also mostly
unknown. This paper assesses the vulnerability of FL-based anomaly detection in
energy data to adversarial attacks. Specifically, two state-of-the-art models,
Long Short Term Memory (LSTM) and Transformers, are used to detect anomalies in
an FL setting, and two white-box attack methods, Fast Gradient Sign Method
(FGSM) and Projected Gradient Descent (PGD), are employed to perturb the data.
The results show that FL is more sensitive to PGD attacks than to FGSM attacks,
attributed to PGD's iterative nature, resulting in an accuracy drop of over 10%
even with naive, weaker attacks. Moreover, FL is more affected by these attacks
than centralized learning, highlighting the need for defense mechanisms in FL.; 67) RAPID: Retrieval-Augmented Parallel Inference Drafting for Text-Based
  Video Event Retrieval; Retrieving events from videos using text queries has become increasingly
challenging due to the rapid growth of multimedia content. Existing methods for
text-based video event retrieval often focus heavily on object-level
descriptions, overlooking the crucial role of contextual information. This
limitation is especially apparent when queries lack sufficient context, such as
missing location details or ambiguous background elements. To address these
challenges, we propose a novel system called RAPID (Retrieval-Augmented
Parallel Inference Drafting), which leverages advancements in Large Language
Models (LLMs) and prompt-based learning to semantically correct and enrich user
queries with relevant contextual information. These enriched queries are then
processed through parallel retrieval, followed by an evaluation step to select
the most relevant results based on their alignment with the original query.
Through extensive experiments on our custom-developed dataset, we demonstrate
that RAPID significantly outperforms traditional retrieval methods,
particularly for contextually incomplete queries. Our system was validated for
both speed and accuracy through participation in the Ho Chi Minh City AI
Challenge 2024, where it successfully retrieved events from over 300 hours of
video. Further evaluation comparing RAPID with the baseline proposed by the
competition organizers demonstrated its superior effectiveness, highlighting
the strength and robustness of our approach.; 68) Phase Stability Analysis of Volume-preserving Algorithms for Accurate
  Single Particle Orbit Simulations in Tokamak Plasmas; Second-order Volume-preserving algorithms (VPAs) for simulating charged
particle motion in electromagnetic fields have been generalized to a rotating
angle formulation by using the matrix decomposition methods. Based on this
method, the phase stability of this class of VPAs has been analyzed by using
the Discrete Fourier Transformations (DFT) technique. It is found that two
prominent VPAs, namely the $G_h^2$ and the Boris algorithm, exhibit optimal
phase precision for high-frequency (gyro motion) and low-frequency dynamics
(transit/bounce motion), respectively. These findings have been empirically
verified through numerical experiments. The insights gained from this study
enable the selection of an appropriate VPA for practical simulations based on
the characteristic frequencies of specific physics problems, which can
substantially enhance numerical accuracy and improve computational efficiency
for long-term simulations.; 69) GRiNS: A Python Library for Simulating Gene Regulatory Network Dynamics; The emergent dynamics of complex gene regulatory networks govern various
cellular processes. However, understanding these dynamics is challenging due to
the difficulty of parameterizing the computational models for these networks,
especially as the network size increases. Here, we introduce a simulation
library, Gene Regulatory Interaction Network Simulator (GRiNS), to address
these challenges. GRiNS integrates popular parameter-agnostic simulation
frameworks, RACIPE and Boolean Ising formalism, into a single Python library
capable of leveraging GPU acceleration for efficient and scalable simulations.
GRiNS extends the ordinary differential equations (ODE) based RACIPE framework
with a more modular design, allowing users to choose parameters, initial
conditions, and time-series outputs for greater customisability and accuracy in
simulations. For large networks, where ODE-based simulation formalisms do not
scale well, GRiNS implements Boolean Ising formalism, providing a simplified,
coarse-grained alternative, significantly reducing the computational cost while
capturing key dynamical behaviours of large regulatory networks. The
documentation and installation instructions for GRiNS can be found at
https://moltenecdysone09.github.io/GRiNS/.; 70) Polyhedra Encoding Transformers: Enhancing Diffusion MRI Analysis Beyond
  Voxel and Volumetric Embedding; Diffusion-weighted Magnetic Resonance Imaging (dMRI) is an essential tool in
neuroimaging. It is arguably the sole noninvasive technique for examining the
microstructural properties and structural connectivity of the brain. Recent
years have seen the emergence of machine learning and data-driven approaches
that enhance the speed, accuracy, and consistency of dMRI data analysis.
However, traditional deep learning models often fell short, as they typically
utilize pixel-level or volumetric patch-level embeddings similar to those used
in structural MRI, and do not account for the unique distribution of various
gradient encodings. In this paper, we propose a novel method called Polyhedra
Encoding Transformer (PE-Transformer) for dMRI, designed specifically to handle
spherical signals. Our approach involves projecting an icosahedral polygon onto
a unit sphere to resample signals from predetermined directions. These
resampled signals are then transformed into embeddings, which are processed by
a transformer encoder that incorporates orientational information reflective of
the icosahedral structure. Through experimental validation with various
gradient encoding protocols, our method demonstrates superior accuracy in
estimating multi-compartment models and Fiber Orientation Distributions (FOD),
outperforming both conventional CNN architectures and standard transformers.; 71) Electrons and phonons in pentacene, insights from comparison between
  experiment and simulations; We have computed the vibrational pattern and the electron-phonon coupling at
several q-points in the Brillouin Zone for the three known pentacene
polymorphs. After having verified that they effectively correspond to three
different structures, we revisit the assignment of the experimental Raman
modes. Finally, using a pool of post-processing tools, we present the phonon
spectra and dispersions, confirming previous indications about the effect of
phonon dispersion and charge mobility. In addition, we consider in an effective
way the coexistence of more polymorphs and the possible role of defects and
energy disorder.; 72) Savaal: Scalable Concept-Driven Question Generation to Enhance Human
  Learning; Assessing and enhancing human learning through question-answering is vital,
yet automating this process remains challenging. While large language models
(LLMs) excel at summarization and query responses, their ability to generate
meaningful questions for learners is underexplored.
  We propose Savaal, a scalable question-generation system with three
objectives: (i) scalability, enabling question generation from hundreds of
pages of text (ii) depth of understanding, producing questions beyond factual
recall to test conceptual reasoning, and (iii) domain-independence,
automatically generating questions across diverse knowledge areas. Instead of
providing an LLM with large documents as context, Savaal improves results with
a three-stage processing pipeline. Our evaluation with 76 human experts on 71
papers and PhD dissertations shows that Savaal generates questions that better
test depth of understanding by 6.5X for dissertations and 1.5X for papers
compared to a direct-prompting LLM baseline. Notably, as document length
increases, Savaal's advantages in higher question quality and lower cost become
more pronounced.; 73) CrossView-GS: Cross-view Gaussian Splatting For Large-scale Scene
  Reconstruction; 3D Gaussian Splatting (3DGS) has emerged as a prominent method for scene
representation and reconstruction, leveraging densely distributed Gaussian
primitives to enable real-time rendering of high-resolution images. While
existing 3DGS methods perform well in scenes with minor view variation, large
view changes in cross-view scenes pose optimization challenges for these
methods. To address these issues, we propose a novel cross-view Gaussian
Splatting method for large-scale scene reconstruction, based on dual-branch
fusion. Our method independently reconstructs models from aerial and ground
views as two independent branches to establish the baselines of Gaussian
distribution, providing reliable priors for cross-view reconstruction during
both initialization and densification. Specifically, a gradient-aware
regularization strategy is introduced to mitigate smoothing issues caused by
significant view disparities. Additionally, a unique Gaussian supplementation
strategy is utilized to incorporate complementary information of dual-branch
into the cross-view model. Extensive experiments on benchmark datasets
demonstrate that our method achieves superior performance in novel view
synthesis compared to state-of-the-art methods.; 74) CSSDM Ontology to Enable Continuity of Care Data Interoperability; The rapid advancement of digital technologies and recent global pandemic
scenarios have led to a growing focus on how these technologies can enhance
healthcare service delivery and workflow to address crises. Action plans that
consolidate existing digital transformation programs are being reviewed to
establish core infrastructure and foundations for sustainable healthcare
solutions. Reforming health and social care to personalize home care, for
example, can help avoid treatment in overcrowded acute hospital settings and
improve the experiences and outcomes for both healthcare professionals and
service users. In this information-intensive domain, addressing the
interoperability challenge through standards-based roadmaps is crucial for
enabling effective connections between health and social care services. This
approach facilitates safe and trustworthy data workflows between different
healthcare system providers. In this paper, we present a methodology for
extracting, transforming, and loading data through a semi-automated process
using a Common Semantic Standardized Data Model (CSSDM) to create personalized
healthcare knowledge graph (KG). The CSSDM is grounded in the formal ontology
of ISO 13940 ContSys and incorporates FHIR-based specifications to support
structural attributes for generating KGs. We propose that the CSSDM facilitates
data harmonization and linking, offering an alternative approach to
interoperability. This approach promotes a novel form of collaboration between
companies developing health information systems and cloud-enabled health
services. Consequently, it provides multiple stakeholders with access to
high-quality data and information sharing.; 75) Out-of-plane displacement of quantum color centers in monolayer h-BN; Color centers exhibiting deep-level states within the wide bandgap h-BN
monolayer possess substantial potential for quantum applications. Uncovering
precise geometric characteristics at the atomic scale is crucial for
understanding defect performance. In this study, first-principles calculations
were performed on the most extensively investigated CBVN and NBVN color centers
in h-BN, focusing on the out-of-plane displacement and their specific impacts
on electronic, vibrational, and emission properties. We demonstrate the
competition between the {\sigma}*-like antibonding state and the {\pi}-like
bonding state, which determines the out-of-plane displacement. The overall
effect of vibronic coupling on geometry is elucidated using a pseudo
Jahn-Teller model. Local vibrational analysis reveals a series of distinct
quasi-local phonon modes that could serve as fingerprints for experimental
identification of specific point defects. The critical effects of out-of-plane
displacement during the quantum emission process are carefully elucidated to
answer the distinct observations in experiments, and these revelations are
universal in quantum point defects in other layered materials.; 76) Development of an Adaptive Sliding Mode Controller using Neural Networks
  for Trajectory Tracking of a Cylindrical Manipulator; Cylindrical manipulators are extensively used in industrial automation,
especially in emerging technologies like 3D printing, which represents a
significant future trend. However, controlling the trajectory of nonlinear
models with system uncertainties remains a critical challenge, often leading to
reduced accuracy and reliability. To address this, the study develops an
Adaptive Sliding Mode Controller (ASMC) integrated with Neural Networks (NNs)
to improve trajectory tracking for cylindrical manipulators. The ASMC leverages
the robustness of sliding mode control and the adaptability of neural networks
to handle uncertainties and dynamic variations effectively. Simulation results
validate that the proposed ASMC-NN achieves high trajectory tracking accuracy,
fast response time, and enhanced reliability, making it a promising solution
for applications in 3D printing and beyond.; 77) Utility-inspired Reward Transformations Improve Reinforcement Learning
  Training of Language Models; Current methods that train large language models (LLMs) with reinforcement
learning feedback, often resort to averaging outputs of multiple rewards
functions during training. This overlooks crucial aspects of individual reward
dimensions and inter-reward dependencies that can lead to sub-optimal outcomes
in generations. In this work, we show how linear aggregation of rewards
exhibits some vulnerabilities that can lead to undesired properties of
generated text. We then propose a transformation of reward functions inspired
by economic theory of utility functions (specifically Inada conditions), that
enhances sensitivity to low reward values while diminishing sensitivity to
already high values. We compare our approach to the existing baseline methods
that linearly aggregate rewards and show how the Inada-inspired reward feedback
is superior to traditional weighted averaging. We quantitatively and
qualitatively analyse the difference in the methods, and see that models
trained with Inada-transformations score as more helpful while being less
harmful.; 78) VLM-E2E: Enhancing End-to-End Autonomous Driving with Multimodal Driver
  Attention Fusion; Human drivers adeptly navigate complex scenarios by utilizing rich
attentional semantics, but the current autonomous systems struggle to replicate
this ability, as they often lose critical semantic information when converting
2D observations into 3D space. In this sense, it hinders their effective
deployment in dynamic and complex environments. Leveraging the superior scene
understanding and reasoning abilities of Vision-Language Models (VLMs), we
propose VLM-E2E, a novel framework that uses the VLMs to enhance training by
providing attentional cues. Our method integrates textual representations into
Bird's-Eye-View (BEV) features for semantic supervision, which enables the
model to learn richer feature representations that explicitly capture the
driver's attentional semantics. By focusing on attentional semantics, VLM-E2E
better aligns with human-like driving behavior, which is critical for
navigating dynamic and complex environments. Furthermore, we introduce a
BEV-Text learnable weighted fusion strategy to address the issue of modality
importance imbalance in fusing multimodal information. This approach
dynamically balances the contributions of BEV and text features, ensuring that
the complementary information from visual and textual modality is effectively
utilized. By explicitly addressing the imbalance in multimodal fusion, our
method facilitates a more holistic and robust representation of driving
environments. We evaluate VLM-E2E on the nuScenes dataset and demonstrate its
superiority over state-of-the-art approaches, showcasing significant
improvements in performance.; 79) Recurrence relations for degenerate Bell and Dowling polynomials via
  Boson operators; Spivey found a recurrence relation for the Bell numbers by using
combinatorial method. The aim of this paper is to derive Spivey's type
recurrence relations for the degenerate Bell polynomials and the degenerate
Dowling polynomials by using the boson annihilation and creation operators
satisfying the commutation relation aa+-a+a=1.
  In addition, we derive a Spivey's type recurrence relation for the r-Dowling
polynomials.; 80) SFC-GAN: A Generative Adversarial Network for Brain Functional and
  Structural Connectome Translation; Modern brain imaging technologies have enabled the detailed reconstruction of
human brain connectomes, capturing structural connectivity (SC) from diffusion
MRI and functional connectivity (FC) from functional MRI. Understanding the
intricate relationships between SC and FC is vital for gaining deeper insights
into the brain's functional and organizational mechanisms. However, obtaining
both SC and FC modalities simultaneously remains challenging, hindering
comprehensive analyses. Existing deep generative models typically focus on
synthesizing a single modality or unidirectional translation between FC and SC,
thereby missing the potential benefits of bi-directional translation,
especially in scenarios where only one connectome is available. Therefore, we
propose Structural-Functional Connectivity GAN (SFC-GAN), a novel framework for
bidirectional translation between SC and FC. This approach leverages the
CycleGAN architecture, incorporating convolutional layers to effectively
capture the spatial structures of brain connectomes. To preserve the
topological integrity of these connectomes, we employ a structure-preserving
loss that guides the model in capturing both global and local connectome
patterns while maintaining symmetry. Our framework demonstrates superior
performance in translating between SC and FC, outperforming baseline models in
similarity and graph property evaluations compared to ground truth data, each
translated modality can be effectively utilized for downstream classification.; 81) On H-Intersecting Graph Families and Counting of Homomorphisms; This work derives an upper bound on the maximum cardinality of a family of
graphs on a fixed number of vertices, in which the intersection of every two
graphs in that family contains a subgraph that is isomorphic to a specified
graph H. Such families are referred to as H-intersecting graph families. The
bound is derived using the combinatorial version of Shearer's lemma, and it
forms a nontrivial extension of the bound derived by Chung, Graham, Frankl, and
Shearer (1986), where H is specialized to a triangle. The derived bound is
expressed in terms of the chromatic number of H, while a relaxed version,
formulated using the Lov\'{a}sz $\vartheta$-function of the complement of H,
offers reduced computational complexity. Additionally, a probabilistic version
of Shearer's lemma, combined with properties of the Shannon entropy, are
employed to establish bounds related to the enumeration of graph homomorphisms,
providing further insights into the interplay between combinatorial structures
and information-theoretic principles.; 82) MuST: Multi-Head Skill Transformer for Long-Horizon Dexterous
  Manipulation with Skill Progress; Robot picking and packing tasks require dexterous manipulation skills, such
as rearranging objects to establish a good grasping pose, or placing and
pushing items to achieve tight packing. These tasks are challenging for robots
due to the complexity and variability of the required actions. To tackle the
difficulty of learning and executing long-horizon tasks, we propose a novel
framework called the Multi-Head Skill Transformer (MuST). This model is
designed to learn and sequentially chain together multiple motion primitives
(skills), enabling robots to perform complex sequences of actions effectively.
MuST introduces a ""progress value"" for each skill, guiding the robot on which
skill to execute next and ensuring smooth transitions between skills.
Additionally, our model is capable of expanding its skill set and managing
various sequences of sub-tasks efficiently. Extensive experiments in both
simulated and real-world environments demonstrate that MuST significantly
enhances the robot's ability to perform long-horizon dexterous manipulation
tasks.; 83) Local probe evidence supporting altermagnetism in Co$_{1/4}$NbSe$_2$; Muon spin rotation (${\mu}$SR), combined with muon stopping site and local
field analysis, was used to investigate the magnetic properties of cobalt
intercalated 2H-NbSe$_2$ (Co$_{1/4}$NbSe$_2$). Co$_{1/4}$NbSe$_2$ is predicted
to be an altermagnet, and whilst neutron diffraction has proposed its magnetic
structure, microscopic details such as the magnetic volume fraction remain
unclear. Therefore, a local probe investigation of its magnetism is essential.
Here, we report the determination of the magnetically ordered volume fraction,
ordered moment size, and magnetic structure. Our results reveal a sharp
second-order transition to a full-volume-fraction, homogeneous magnetic order
below $T_\mathrm{N}~=~168~$K. The moments are aligned antiparallel along the
$c$-axis, consistent with neutron diffraction and altermagnetism. ${\mu}$SR
reveals that the state remains stable under a $c$-axis magnetic field up to
$0.78~$T, with magnetisation measurements suggesting this robust regimes
extends to at least $5$~T. Within the time resolution of ${\mu}$SR, no
precursor slow altermagnetic fluctuations were detected above $T_\mathrm{N}$,
which is important for interpreting the band splitting in the paramagnetic
state reported by photoemission studies. These findings support altermagnetism
in Co$_{1/4}$NbSe$_2$ and motivate further experiments to explore the
tunability of its magnetic and electronic structure.; 84) LLMs for Drug-Drug Interaction Prediction: A Comprehensive Comparison; The increasing volume of drug combinations in modern therapeutic regimens
needs reliable methods for predicting drug-drug interactions (DDIs). While
Large Language Models (LLMs) have revolutionized various domains, their
potential in pharmaceutical research, particularly in DDI prediction, remains
largely unexplored. This study thoroughly investigates LLMs' capabilities in
predicting DDIs by uniquely processing molecular structures (SMILES), target
organisms, and gene interaction data as raw text input from the latest DrugBank
dataset. We evaluated 18 different LLMs, including proprietary models (GPT-4,
Claude, Gemini) and open-source variants (from 1.5B to 72B parameters), first
assessing their zero-shot capabilities in DDI prediction. We then fine-tuned
selected models (GPT-4, Phi-3.5 2.7B, Qwen-2.5 3B, Gemma-2 9B, and Deepseek R1
distilled Qwen 1.5B) to optimize their performance. Our comprehensive
evaluation framework included validation across 13 external DDI datasets,
comparing against traditional approaches such as l2-regularized logistic
regression. Fine-tuned LLMs demonstrated superior performance, with Phi-3.5
2.7B achieving a sensitivity of 0.978 in DDI prediction, with an accuracy of
0.919 on balanced datasets (50% positive, 50% negative cases). This result
represents an improvement over both zero-shot predictions and state-of-the-art
machine-learning methods used for DDI prediction. Our analysis reveals that
LLMs can effectively capture complex molecular interaction patterns and cases
where drug pairs target common genes, making them valuable tools for practical
applications in pharmaceutical research and clinical settings.; 85) Commonsense Reasoning-Aided Autonomous Vehicle Systems; Autonomous Vehicle (AV) systems have been developed with a strong reliance on
machine learning techniques. While machine learning approaches, such as deep
learning, are extremely effective at tasks that involve observation and
classification, they struggle when it comes to performing higher level
reasoning about situations on the road. This research involves incorporating
commonsense reasoning models that use image data to improve AV systems. This
will allow AV systems to perform more accurate reasoning while also making them
more adjustable, explainable, and ethical. This paper will discuss the findings
so far and motivate its direction going forward.; 86) A technical review of multi-omics data integration methods: from
  classical statistical to deep generative approaches; The rapid advancement of high-throughput sequencing and other assay
technologies has resulted in the generation of large and complex multi-omics
datasets, offering unprecedented opportunities for advancing precision medicine
strategies. However, multi-omics data integration presents significant
challenges due to the high dimensionality, heterogeneity, experimental gaps,
and frequency of missing values across data types. Computational methods have
been developed to address these issues, employing statistical and machine
learning approaches to uncover complex biological patterns and provide deeper
insights into our understanding of disease mechanisms. Here, we comprehensively
review state-of-the-art multi-omics data integration methods with a focus on
deep generative models, particularly variational autoencoders (VAEs) that have
been widely used for data imputation and augmentation, joint embedding
creation, and batch effect correction. We explore the technical aspects of loss
functions and regularisation techniques including adversarial training,
disentanglement and contrastive learning. Moreover, we discuss recent
advancements in foundation models and the integration of emerging data
modalities, while describing the current limitations and outlining future
directions for enhancing multi-modal methodologies in biomedical research.; 87) Arab Spring's Impact on Science through the Lens of Scholarly Attention,
  Funding, and Migration; The Arab Spring is a major socio-political movement that reshaped democratic
aspirations in the Middle East and North Africa, attracting global attention
through news, social media, and academic discourse. However, its consequences
on the academic landscape in the region are still unclear. Here, we conduct the
first study of scholarly attention toward 10 target countries affected by the
Arab Spring by analyzing more than 25 million articles published from 2002 to
2019. We find that changes in scholarly attention were unevenly distributed
among target countries, with Egypt attracting the most attention. We reveal
strong correlations between increases in scholarly attention given by source
countries and increases in funding for research about the target countries as
well as increased immigration of scholars from the affected region to them.
Notably, our analysis reveals Saudi Arabia's positioning as a key player, among
Western nations, that shapes research in the region.; 88) Symbol Resolution MatRs: Make it Fast and Observable with Stable Linking; Dynamic linking is the standard mechanism for using external dependencies
since it enables code reuse, streamlines software updates, and reduces
disk/network use. Dynamic linking waits until runtime to calculate an
application's relocation mapping, i.e., the mapping between each externally
referenced symbol in the application to the dependency that provides the
symbol. Unfortunately, it comes with two downsides. First, dynamic linking
limits the performance of current systems since it can take seconds to
calculate a relocation mapping for a large program. Second, dynamic linking
limits the dependency management of applications since it prevents a developer
from accurately observing a relocation mapping except at runtime.
  This paper makes the key insight that the benefits conventionally attributed
to dynamic linking: code reuse, streamlined software updates, and reduced
disk/network use are actually benefits of shared libraries. Thus, we present
stable linking, a new mechanism for using dependencies that uses shared
libraries to retain their benefits but eliminates the downsides of dynamic
linking. Stable linking separates a system's state into management times; when
the system can be modified, and epochs when it cannot. Stable linking
calculates each application's relocation mapping at the beginning of each
epoch, allows developers to inspect the relocation mapping during the epoch,
and reuses the mapping for subsequent executions in the epoch. We design and
build MatR, the first stable linker. We use MatR in three workloads and show
that it improves upon dynamic linking performance by a factor of 2.19 on
average. Additionally, we use the system in three vignettes, or case-studies,
that illustrate the system's improvements to dependency management.; 89) Masked Latent Prediction and Classification for Self-Supervised Audio
  Representation Learning; Recently, self-supervised learning methods based on masked latent prediction
have proven to encode input data into powerful representations. However, during
training, the learned latent space can be further transformed to extract
higher-level information that could be more suited for downstream
classification tasks. Therefore, we propose a new method: MAsked latenT
Prediction And Classification (MATPAC), which is trained with two pretext tasks
solved jointly. As in previous work, the first pretext task is a masked latent
prediction task, ensuring a robust input representation in the latent space.
The second one is unsupervised classification, which utilises the latent
representations of the first pretext task to match probability distributions
between a teacher and a student. We validate the MATPAC method by comparing it
to other state-of-the-art proposals and conducting ablations studies. MATPAC
reaches state-of-the-art self-supervised learning results on reference audio
classification datasets such as OpenMIC, GTZAN, ESC-50 and US8K and outperforms
comparable supervised methods results for musical auto-tagging on
Magna-tag-a-tune.; 90) Selective band interaction and long-range hopping in a structured
  environment with giant atoms; Giant atoms, which couple to the environment at multiple discrete points,
exhibit various nontrivial phenomena in quantum optics due to their nonlocal
couplings. In this study, we propose a one-dimensional cross-stitch ladder
lattice featuring both a dispersive band and a flat band. By modulating the
relative phase between the coupling points, the giant atom selectively
interacts with either band. First, we analyze the scenario where the dispersive
and flat bands intersect at two points, and the atomic frequency lies within
the band. Unlike the small atom, which simultaneously interacts with both
bands, a single giant atom with a controllable phase interacts exclusively with
the dispersive or flat band. Second, in the bandgap regime, where two atoms
interact through bound-state overlaps manifesting as dipole-dipole
interactions, we demonstrate that giant atoms enable deterministic long-range
hopping and energy exchange with higher fidelity compared to small atoms. These
findings provide promising applications in quantum information processing,
offering enhanced controllability and selectivity for quantum systems and
devices.; 91) Data Sharing, Privacy and Security Considerations in the Energy Sector:
  A Review from Technical Landscape to Regulatory Specifications; Decarbonization, decentralization and digitalization are the three key
elements driving the twin energy transition. The energy system is evolving to a
more data driven ecosystem, leading to the need of communication and storage of
large amount of data of different resolution from the prosumers and other
stakeholders in the energy ecosystem. While the energy system is certainly
advancing, this paradigm shift is bringing in new privacy and security issues
related to collection, processing and storage of data - not only from the
technical dimension, but also from the regulatory perspective. Understanding
data privacy and security in the evolving energy system, regarding regulatory
compliance, is an immature field of research. Contextualized knowledge of how
related issues are regulated is still in its infancy, and the practical and
technical basis for the regulatory framework for data privacy and security is
not clear. To fill this gap, this paper conducts a comprehensive review of the
data-related issues for the energy system by integrating both technical and
regulatory dimensions. We start by reviewing open-access data, data
communication and data-processing techniques for the energy system, and use it
as the basis to connect the analysis of data-related issues from the integrated
perspective. We classify the issues into three categories: (i) data-sharing
among energy end users and stakeholders (ii) privacy of end users, and (iii)
cyber security, and then explore these issues from a regulatory perspective. We
analyze the evolution of related regulations, and introduce the relevant
regulatory initiatives for the categorized issues in terms of regulatory
definitions, concepts, principles, rights and obligations in the context of
energy systems. Finally, we provide reflections on the gaps that still exist,
and guidelines for regulatory frameworks for a truly participatory energy
system.; 92) BioSerenity-E1: a self-supervised EEG model for medical applications; Electroencephalography (EEG) serves as an essential diagnostic tool in
neurology; however, its accurate manual interpretation is a time-intensive
process that demands highly specialized expertise, which remains relatively
scarce and not consistently accessible. To address these limitations, the
implementation of automated pre-screening and analysis systems for EEG data
holds considerable promise. Advances in self-supervised learning made it
possible to pre-train complex deep learning architectures on large volumes of
unlabeled EEG data to learn generalizable representations, that can later be
used to enhance performance on multiple tasks while needing less downstream
data. In the present paper, we introduce BioSerenity-E1, the first of a family
of self-supervised foundation models for clinical EEG applications that
combines spectral tokenization with masked prediction to achieve
state-of-the-art performance across relevant diagnostic tasks. The two-phase
self-supervised pretraining framework initially acquires compressed EEG
representations via a transformer-based VQ-VAE architecture designed to
reconstruct log-multitaper spectral projections, then implements extensive (70%
block) masked token prediction to force the model to learn complex
spatiotemporal dependencies in EEG signals. BioSerenity-E1 achieves strong
performance across three clinical tasks, either in line or above
state-of-the-art methods: seizure detection (AUROC = 0.926, Sensitivity =
0.909), normal/abnormal classification (AUPRC = 0.970 on proprietary data;
0.910 on TUH-Abnormal), and multiclass pathology differentiation on unbalanced
data (Weighted F1 = 0.730). The utility of BioSerenity-E1 is further confirmed
in low-data regimes scenarios, showing clear improvements in AUPRC (from +2% to
17%) when trained on less than 10% of the available data.; 93) Simultaneous Angle-of-Arrival Sensing and Anomalous Deflection with
  Aperiodically Loaded Patch Arrays; We propose and numerically demonstrate a reconfigurable patch antenna array
that enables simultaneous incident wave sensing and anomalous reflection
without prior knowledge of the propagation environment. We acquire anomalous
reflection by suppressing parasitic scattering through accurate and efficient
optimization of induced load currents and by varying impedances of reactive
loads. By mitigating parasitic scattering lobes, we demonstrate the feasibility
of accurately detecting the incoming illumination angle via the spatial Fourier
transform of the optimized load current distribution, facilitated by tunable
reactive loads. This approach eliminates the need for additional RF chains,
pre-computed data, or calibration measurements. The developed strategy, which
integrates arithmetic load optimization with angle-of-arrival sensing, is
applicable to general finite-size arrays.; 94) Observational constraints on vector-like dark energy; The canonical cosmological model to explain the recent acceleration of the
universe relies on a cosmological constant, and most dynamical dark energy and
modified gravity model alternatives are based on scalar fields. Still, further
alternatives are possible. One of these involves vector fields: under certain
conditions, they can lead to accelerating universes while preserving
large-scale homogeneity and isotropy. We report quantitative observational
constraints on a model previously proposed by Armend\'ariz-Pic\'on and known as
the cosmic triad. We consider several subclasses of the model, which
generically is a parametric extension of the canonical $\Lambda$CDM model, as
well as two possible choices of the triad's potential. Our analysis shows that
any deviations from this limit are constrained to be small. In particular the
preferred present-day values of the matter density and the dark energy equation
of state are fully consistent with those obtained, for the same datasets, in
flat $\Lambda$CDM and $w_0$CDM. The constraints mildly depend on the priors on
the dark energy equation of state, specifically on whether phantom values
thereof are allowed, while the choice of potential does not play a significant
role since any such potential is constrained to be relatively flat.; 95) Highly efficient exciton-exciton annihilation in single conjugated
  polymer chains; The number of excitons that conjugated polymers can support at any one time
underpins their optoelectronic performance in light emitting diodes and as
laser gain media, as it sets a natural limit on exciton density. Here we have
measured the time-resolved photon statistics of single chains of polyfluorene
to extract the absolute number of independent emitting sites present and its
time dependence. We find that after 100 ps each chain can only support 1 or 2
independent excitons, and that even at the earliest times this number rises
only to 4, suggesting a high degree of electronic coupling between chromophores
that facilitates efficient exciton-exciton annihilation. In circumstances where
a low density of low-energy sites is present, annihilation between them still
dominates. The results indicate that achieving high exciton densities in
conjugated polymers is difficult, and in applications where it is desirable new
strategies should be devised to control exciton-exciton annihilation.; 96) Out-of-Distribution Detection on Graphs: A Survey; Graph machine learning has witnessed rapid growth, driving advancements
across diverse domains. However, the in-distribution assumption, where training
and testing data share the same distribution, often breaks in real-world
scenarios, leading to degraded model performance under distribution shifts.
This challenge has catalyzed interest in graph out-of-distribution (GOOD)
detection, which focuses on identifying graph data that deviates from the
distribution seen during training, thereby enhancing model robustness. In this
paper, we provide a rigorous definition of GOOD detection and systematically
categorize existing methods into four types: enhancement-based,
reconstruction-based, information propagation-based, and classification-based
approaches. We analyze the principles and mechanisms of each approach and
clarify the distinctions between GOOD detection and related fields, such as
graph anomaly detection, outlier detection, and GOOD generalization. Beyond
methodology, we discuss practical applications and theoretical foundations,
highlighting the unique challenges posed by graph data. Finally, we discuss the
primary challenges and propose future directions to advance this emerging
field. The repository of this survey is available at
https://github.com/ca1man-2022/Awesome-GOOD-Detection.; 97) Orthogonal projections of hypercubes; Projections of hypercubes have been applied to visualize high-dimensional
binary state spaces in various scientific fields. Conventional methods for
projecting hypercubes, however, face practical difficulties. Manual methods
require nontrivial adjustments of the projection basis, while
optimization-based algorithms limit the interpretability and reproducibility of
the resulting plots. These limitations motivate us to explore theoretically
analyzable projection algorithms such as principal component analysis (PCA).
Here, we investigate the mathematical properties of PCA-projected hypercubes.
Our numerical and analytical results show that PCA effectively captures
polarized distributions within the hypercubic state space. This property
enables the assessment of the asymptotic distribution of projected vertices and
error bounds, which characterize the performance of PCA in the projected space.
We demonstrate the application of PCA to visualize the hypercubic energy
landscapes of Ising spin systems. By adding projected hypercubic edges, these
visualizations reveal pathways of correlated spin flips. Our work provides a
better understanding of how PCA discovers hidden patterns in high-dimensional
binary data.; 98) On Noncommutative Quantum Mechanics and the Black-Scholes Model; Two novel and direct quantum mechanical representations of the Black-Scholes
model are constructed based on the (Wick-rotated) quantization of two specific
mechanical systems. The quantum setup is achieved by means of the associated
Laplace-Beltrami operator (one for each model), and not by merely applying the
usual naive rule. Additionally, the clear identification of the geometric
content of the underlying classical framework is exploited in order to arrive
at a noncommutative quantum mechanics generalization of the Black-Scholes
model. We also consider a system consisting of two degrees of freedom whose
(Wick-rotated) quantization leads to a model which can be seen as related to
the Merton-Garman family. This model is also generalized via noncommutative
quantum mechanics.; 99) Causally Aligned Curriculum Learning; A pervasive challenge in Reinforcement Learning (RL) is the ""curse of
dimensionality"" which is the exponential growth in the state-action space when
optimizing a high-dimensional target task. The framework of curriculum learning
trains the agent in a curriculum composed of a sequence of related and more
manageable source tasks. The expectation is that when some optimal decision
rules are shared across source tasks and the target task, the agent could more
quickly pick up the necessary skills to behave optimally in the environment,
thus accelerating the learning process. However, this critical assumption of
invariant optimal decision rules does not necessarily hold in many practical
applications, specifically when the underlying environment contains unobserved
confounders. This paper studies the problem of curriculum RL through causal
lenses. We derive a sufficient graphical condition characterizing causally
aligned source tasks, i.e., the invariance of optimal decision rules holds. We
further develop an efficient algorithm to generate a causally aligned
curriculum, provided with qualitative causal knowledge of the target task.
Finally, we validate our proposed methodology through experiments in discrete
and continuous confounded tasks with pixel observations.; 100) PROTOCALC, A W-band polarized calibrator for CMB Telescopes: application
  to Simons Observatory and CLASS; Current- and next-generation Cosmic Microwave Background (CMB) experiments
will measure polarization anisotropies with unprecedented sensitivities. The
need for high precision in these measurements underscores the importance of
gaining a comprehensive understanding of instrument properties, with a
particular emphasis on the study of the beam properties and, in particular,
their polarization characteristics, and the measurement of the polarization
angle. In this context, a major challenge lies in the scarcity of millimeter
polarized astrophysical sources with sufficient brightness and calibration
knowledge to meet the stringent accuracy requirements of future CMB missions.
This led to the development of a drone-borne calibration source designed for
frequency band centered on approximately 90 GHz band, matching a commonly used
channel in ground based CMB measurements. The PROTOtype CALibrator for
Cosmology, PROTOCALC, has undergone thorough in-lab testing, and its properties
have been subsequently modeled through simulation software integrated into the
standard Simons Observatory (SO) analysis pipeline. Moreover, the PROTOCALC
system has been tested in the field, having been deployed twice on calibration
campaigns with CMB telescopes in the Atacama desert. The data collected
constrain the roll angle of the source with a statistical accuracy of
$0.045^\circ$.",1.0,0.0
2411.00726,applied,2411.00726-pos1-5,"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",2411.00726-pos2-5,"Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema; This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection",10,"['1', '2', '9', '33', '16', '45', '12', '70', '25', '56']","The first candidate paper 'RM-PoT: Reformulating Mathematical Problems and Solving via Program of Thoughts' aligns well with the main paper, as both explore the application of transformer models in different domains - the first in numerical reasoning and the second in image recognition. The innovative approach of reformulating problems in RM-PoT can potentially enhance the effectiveness of transformer models in computer vision tasks outlined in the main paper. This intersection provides a novel avenue for research in adapting transformer architectures for various data types beyond traditional image inputs. The subsequent candidates align progressively less with the core themes of transdisciplinary innovation stemming from the main paper, mainly because they focus on specialized applications or domains that are less relevant to image processing or transformer adaptations.","1) RM-PoT: Reformulating Mathematical Problems and Solving via Program of
  Thoughts; Recently, substantial advancements have been made in training language models
to carry out step-by-step reasoning for solving intricate numerical reasoning
tasks. Beyond the methods used to solve these problems, the structure and
formulation of the problems themselves also play a crucial role in determining
the performance of large language models. We observe that even small changes in
the surface form of mathematical problems can have a profound impact on both
the answer distribution and solve rate. This highlights the vulnerability of
LLMs to surface-level variations, revealing its limited robustness when
reasoning through complex problems. In this paper, we propose RM-PoT, a
three-stage framework that integrates problem reformulation (RM), code-aided
reasoning (PoT), and domain-aware few-shot learning to address these
limitations. Our approach first reformulates the input problem into diverse
surface forms to reduce structural bias, then retrieves five semantically
aligned examples from a pre-constructed domain-specific question bank to
provide contextual guidance, and finally generates executable Python code for
precise computation.; 2) Enhancing Health Information Retrieval with RAG by Prioritizing Topical
  Relevance and Factual Accuracy; The exponential surge in online health information, coupled with its
increasing use by non-experts, highlights the pressing need for advanced Health
Information Retrieval models that consider not only topical relevance but also
the factual accuracy of the retrieved information, given the potential risks
associated with health misinformation. To this aim, this paper introduces a
solution driven by Retrieval-Augmented Generation (RAG), which leverages the
capabilities of generative Large Language Models (LLMs) to enhance the
retrieval of health-related documents grounded in scientific evidence. In
particular, we propose a three-stage model: in the first stage, the user's
query is employed to retrieve topically relevant passages with associated
references from a knowledge base constituted by scientific literature. In the
second stage, these passages, alongside the initial query, are processed by
LLMs to generate a contextually relevant rich text (GenText). In the last
stage, the documents to be retrieved are evaluated and ranked both from the
point of view of topical relevance and factual accuracy by means of their
comparison with GenText, either through stance detection or semantic
similarity. In addition to calculating factual accuracy, GenText can offer a
layer of explainability for it, aiding users in understanding the reasoning
behind the retrieval. Experimental evaluation of our model on benchmark
datasets and against baseline models demonstrates its effectiveness in
enhancing the retrieval of both topically relevant and factually accurate
health information, thus presenting a significant step forward in the health
misinformation mitigation problem.; 3) On Noncommutative Quantum Mechanics and the Black-Scholes Model; Two novel and direct quantum mechanical representations of the Black-Scholes
model are constructed based on the (Wick-rotated) quantization of two specific
mechanical systems. The quantum setup is achieved by means of the associated
Laplace-Beltrami operator (one for each model), and not by merely applying the
usual naive rule. Additionally, the clear identification of the geometric
content of the underlying classical framework is exploited in order to arrive
at a noncommutative quantum mechanics generalization of the Black-Scholes
model. We also consider a system consisting of two degrees of freedom whose
(Wick-rotated) quantization leads to a model which can be seen as related to
the Merton-Garman family. This model is also generalized via noncommutative
quantum mechanics.; 4) Aportes para el cumplimiento del Reglamento (UE) 2024/1689 en rob\'otica
  y sistemas aut\'onomos; Cybersecurity in robotics stands out as a key aspect within Regulation (EU)
2024/1689, also known as the Artificial Intelligence Act, which establishes
specific guidelines for intelligent and automated systems. A fundamental
distinction in this regulatory framework is the difference between robots with
Artificial Intelligence (AI) and those that operate through automation systems
without AI, since the former are subject to stricter security requirements due
to their learning and autonomy capabilities. This work analyzes cybersecurity
tools applicable to advanced robotic systems, with special emphasis on the
protection of knowledge bases in cognitive architectures. Furthermore, a list
of basic tools is proposed to guarantee the security, integrity, and resilience
of these systems, and a practical case is presented, focused on the analysis of
robot knowledge management, where ten evaluation criteria are defined to ensure
compliance with the regulation and reduce risks in human-robot interaction
(HRI) environments.; 5) Star-Forming Nuclear Clusters in Dwarf Galaxies Mimicking AGN Signatures
  in the Mid-Infrared; Effectively finding and identifying active galactic nuclei (AGNs) in dwarf
galaxies is an important step in studying black hole formation and evolution.
In this work, we examine four mid-IR-selected AGN candidates in dwarf galaxies
with stellar masses between $M_\star \sim 10^8 - 10^9 M_\odot$ , and find that
the galaxies are host to nuclear star clusters (NSCs) that are notably rare in
how young and massive they are. We perform photometric measurements on the
central star clusters in our target galaxies galaxies using Hubble Space
Telescope optical and near-IR imaging and compare their observed properties to
models of stellar population evolution. We find that these galaxies are host to
very massive ($\sim10^7 M_\odot$), extremely young ($\lesssim 8$ Myr), dusty
($0.6 \lesssim \mathrm{A_v} \lesssim 1.8$) nuclear star clusters. Our results
indicate that these galactic nuclei have ongoing star-formation, are still at
least partially obscured by clouds of gas and dust, and are most likely
producing the extremely red AGN-like mid-IR colors. Moreover, prior work has
shown that these galaxies do not exhibit X-ray or optical AGN signatures.
Therefore, we recommend caution when using mid-IR color-color diagnostics for
AGN selection in dwarf galaxies, since, as directly exemplified in this sample,
they can be contaminated by massive star clusters with ongoing star formation.; 6) Selective band interaction and long-range hopping in a structured
  environment with giant atoms; Giant atoms, which couple to the environment at multiple discrete points,
exhibit various nontrivial phenomena in quantum optics due to their nonlocal
couplings. In this study, we propose a one-dimensional cross-stitch ladder
lattice featuring both a dispersive band and a flat band. By modulating the
relative phase between the coupling points, the giant atom selectively
interacts with either band. First, we analyze the scenario where the dispersive
and flat bands intersect at two points, and the atomic frequency lies within
the band. Unlike the small atom, which simultaneously interacts with both
bands, a single giant atom with a controllable phase interacts exclusively with
the dispersive or flat band. Second, in the bandgap regime, where two atoms
interact through bound-state overlaps manifesting as dipole-dipole
interactions, we demonstrate that giant atoms enable deterministic long-range
hopping and energy exchange with higher fidelity compared to small atoms. These
findings provide promising applications in quantum information processing,
offering enhanced controllability and selectivity for quantum systems and
devices.; 7) The algebraic structure of Dyson--Schwinger equations with multiple
  insertion places; We give combinatorially controlled series solutions to Dyson--Schwinger
equations with multiple insertion places using tubings of rooted trees and
investigate the algebraic relation between such solutions and the
renormalization group equation.; 8) Idiom Detection in Sorani Kurdish Texts; Idiom detection using Natural Language Processing (NLP) is the computerized
process of recognizing figurative expressions within a text that convey
meanings beyond the literal interpretation of the words. While idiom detection
has seen significant progress across various languages, the Kurdish language
faces a considerable research gap in this area despite the importance of idioms
in tasks like machine translation and sentiment analysis. This study addresses
idiom detection in Sorani Kurdish by approaching it as a text classification
task using deep learning techniques. To tackle this, we developed a dataset
containing 10,580 sentences embedding 101 Sorani Kurdish idioms across diverse
contexts. Using this dataset, we developed and evaluated three deep learning
models: KuBERT-based transformer sequence classification, a Recurrent
Convolutional Neural Network (RCNN), and a BiLSTM model with an attention
mechanism. The evaluations revealed that the transformer model, the fine-tuned
BERT, consistently outperformed the others, achieving nearly 99% accuracy while
the RCNN achieved 96.5% and the BiLSTM 80%. These results highlight the
effectiveness of Transformer-based architectures in low-resource languages like
Kurdish. This research provides a dataset, three optimized models, and insights
into idiom detection, laying a foundation for advancing Kurdish NLP.; 9) Block Diffusion: Interpolating Between Autoregressive and Diffusion
  Language Models; Diffusion language models offer unique benefits over autoregressive models
due to their potential for parallelized generation and controllability, yet
they lag in likelihood modeling and are limited to fixed-length generation. In
this work, we introduce a class of block diffusion language models that
interpolate between discrete denoising diffusion and autoregressive models.
Block diffusion overcomes key limitations of both approaches by supporting
flexible-length generation and improving inference efficiency with KV caching
and parallel token sampling. We propose a recipe for building effective block
diffusion models that includes an efficient training algorithm, estimators of
gradient variance, and data-driven noise schedules to minimize the variance.
Block diffusion sets a new state-of-the-art performance among diffusion models
on language modeling benchmarks and enables generation of arbitrary-length
sequences. We provide the code, along with the model weights and blog post on
the project page: https://m-arriola.com/bd3lms/; 10) Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema; This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection; 11) The $z \gtrsim 9$ galaxy UV luminosity function from the JWST Advanced
  Deep Extragalactic Survey: insights into early galaxy evolution and
  reionization; The high-redshift UV luminosity function provides important insights into the
evolution of early galaxies. JWST has revealed an unexpectedly large population
of bright ($M_\mathrm{UV} \lesssim -20$) galaxies at $z\gtrsim10$, implying
fundamental changes in the star forming properties of galaxies at increasingly
early times. However, constraining the fainter population ($M_\mathrm{UV}
\gtrsim -18$) has been more challenging. In this work, we present the
$z\gtrsim9$ UV luminosity function from the JWST Advanced Deep Extragalactic
Survey. We calculate the UV luminosity function from several hundred
$z\gtrsim9$ galaxy candidates that reach UV luminosities of
$M_\mathrm{UV}\sim-17$ in redshift bins of $z\sim9-12$ (309 candidates) and
$z\sim12-16$ (63 candidates). We search for candidates at $z\sim16-22.5$ and
find none. We also estimate the $z\sim14-16$ luminosity function from the
$z\geq14$ subset of the $z\sim12-16$ sample. Consistent with other
measurements, we find an excess of bright galaxies that is in tension with many
theoretical models, especially at $z\gtrsim12$. However, we also find high
number densities at $-18\lesssim M_\mathrm{UV} \lesssim-17$, suggesting that
there is a larger population of faint galaxies than expected, as well as bright
ones. From our parametric fits for the luminosity function, we find steep faint
end slopes of $-2.5\lesssim\alpha\lesssim-2.3$, suggesting a large population
of faint ($M_\mathrm{UV} \gtrsim -17$) galaxies. Combined, the high
normalization and steep faint end slope of the luminosity function could imply
that the reionization process is appreciably underway as early as $z=10$.; 12) R2LDM: An Efficient 4D Radar Super-Resolution Framework Leveraging
  Diffusion Model; We introduce R2LDM, an innovative approach for generating dense and accurate
4D radar point clouds, guided by corresponding LiDAR point clouds. Instead of
utilizing range images or bird's eye view (BEV) images, we represent both LiDAR
and 4D radar point clouds using voxel features, which more effectively capture
3D shape information. Subsequently, we propose the Latent Voxel Diffusion Model
(LVDM), which performs the diffusion process in the latent space. Additionally,
a novel Latent Point Cloud Reconstruction (LPCR) module is utilized to
reconstruct point clouds from high-dimensional latent voxel features. As a
result, R2LDM effectively generates LiDAR-like point clouds from paired raw
radar data. We evaluate our approach on two different datasets, and the
experimental results demonstrate that our model achieves 6- to 10-fold
densification of radar point clouds, outperforming state-of-the-art baselines
in 4D radar point cloud super-resolution. Furthermore, the enhanced radar point
clouds generated by our method significantly improve downstream tasks,
achieving up to 31.7% improvement in point cloud registration recall rate and
24.9% improvement in object detection accuracy.; 13) Parental Guidance: Efficient Lifelong Learning through Evolutionary
  Distillation; Developing robotic agents that can perform well in diverse environments while
showing a variety of behaviors is a key challenge in AI and robotics.
Traditional reinforcement learning (RL) methods often create agents that
specialize in narrow tasks, limiting their adaptability and diversity. To
overcome this, we propose a preliminary, evolution-inspired framework that
includes a reproduction module, similar to natural species reproduction,
balancing diversity and specialization. By integrating RL, imitation learning
(IL), and a coevolutionary agent-terrain curriculum, our system evolves agents
continuously through complex tasks. This approach promotes adaptability,
inheritance of useful traits, and continual learning. Agents not only refine
inherited skills but also surpass their predecessors. Our initial experiments
show that this method improves exploration efficiency and supports open-ended
learning, offering a scalable solution where sparse reward coupled with diverse
terrain environments induces a multi-task setting.; 14) Sketch Disaggregation Across Time and Space; Streaming analytics are essential in a large range of applications, including
databases, networking, and machine learning. To optimize performance,
practitioners are increasingly offloading such analytics to network nodes such
as switches. However, resources such as fast SRAM memory available at switches
are limited, not uniform, and may serve other functionalities as well (e.g.,
firewall). Moreover, resource availability can also change over time due to the
dynamic demands of in-network applications.
  In this paper, we propose a new approach to disaggregating data structures
over time and space, leveraging any residual resource available at network
nodes. We focus on sketches, which are fundamental for summarizing data for
streaming analytics while providing beneficial space-accuracy tradeoffs. Our
idea is to break sketches into multiple `fragments' that are placed at
different network nodes. The fragments cover different time periods and are of
varying sizes, and are combined to form a network-wide view of the underlying
traffic. We apply our solution to three popular sketches (namely, Count Sketch,
Count-Min Sketch, and UnivMon) and demonstrate we can achieve approximately a
75% memory size reduction for the same error for many queries, or a near
order-of-magnitude error reduction if memory is kept unchanged.; 15) HarmonySet: A Comprehensive Dataset for Understanding Video-Music
  Semantic Alignment and Temporal Synchronization; This paper introduces HarmonySet, a comprehensive dataset designed to advance
video-music understanding. HarmonySet consists of 48,328 diverse video-music
pairs, annotated with detailed information on rhythmic synchronization,
emotional alignment, thematic coherence, and cultural relevance. We propose a
multi-step human-machine collaborative framework for efficient annotation,
combining human insights with machine-generated descriptions to identify key
transitions and assess alignment across multiple dimensions. Additionally, we
introduce a novel evaluation framework with tasks and metrics to assess the
multi-dimensional alignment of video and music, including rhythm, emotion,
theme, and cultural context. Our extensive experiments demonstrate that
HarmonySet, along with the proposed evaluation framework, significantly
improves the ability of multimodal models to capture and analyze the intricate
relationships between video and music.; 16) Mitigating Hallucinations on Object Attributes using Multiview Images
  and Negative Instructions; Current popular Large Vision-Language Models (LVLMs) are suffering from
Hallucinations on Object Attributes (HoOA), leading to incorrect determination
of fine-grained attributes in the input images. Leveraging significant
advancements in 3D generation from a single image, this paper proposes a novel
method to mitigate HoOA in LVLMs. This method utilizes multiview images sampled
from generated 3D representations as visual prompts for LVLMs, thereby
providing more visual information from other viewpoints. Furthermore, we
observe the input order of multiple multiview images significantly affects the
performance of LVLMs. Consequently, we have devised Multiview Image Augmented
VLM (MIAVLM), incorporating a Multiview Attributes Perceiver (MAP) submodule
capable of simultaneously eliminating the influence of input image order and
aligning visual information from multiview images with Large Language Models
(LLMs). Besides, we designed and employed negative instructions to mitigate
LVLMs' bias towards ``Yes"" responses. Comprehensive experiments demonstrate the
effectiveness of our method.; 17) Masked Latent Prediction and Classification for Self-Supervised Audio
  Representation Learning; Recently, self-supervised learning methods based on masked latent prediction
have proven to encode input data into powerful representations. However, during
training, the learned latent space can be further transformed to extract
higher-level information that could be more suited for downstream
classification tasks. Therefore, we propose a new method: MAsked latenT
Prediction And Classification (MATPAC), which is trained with two pretext tasks
solved jointly. As in previous work, the first pretext task is a masked latent
prediction task, ensuring a robust input representation in the latent space.
The second one is unsupervised classification, which utilises the latent
representations of the first pretext task to match probability distributions
between a teacher and a student. We validate the MATPAC method by comparing it
to other state-of-the-art proposals and conducting ablations studies. MATPAC
reaches state-of-the-art self-supervised learning results on reference audio
classification datasets such as OpenMIC, GTZAN, ESC-50 and US8K and outperforms
comparable supervised methods results for musical auto-tagging on
Magna-tag-a-tune.; 18) Learning to Optimize Joint Chance-constrained Power Dispatch Problems; The ever-increasing integration of stochastic renewable energy sources into
power systems operation is making the supply-demand balance more challenging.
While joint chance-constrained methods are equipped to model these complexities
and uncertainties, solving these models using the traditional iterative solvers
is time-consuming and can hinder real-time implementation. To overcome the
shortcomings of today's solvers, we propose a fast, scalable, and explainable
machine learning-based optimization proxy. Our solution, called Learning to
Optimize the Optimization of Joint Chance-Constrained Problems (LOOP-JCCP), is
iteration-free and solves the underlying problem in a single-shot. Our model
uses a polyhedral reformulation of the original problem to manage constraint
violations and ensure solution feasibility across various scenarios through
customizable probability settings. To this end, we build on our recent
deterministic solution (LOOP-LC 2.0) by incorporating a set aggregator module
to handle uncertain sample sets of varying sizes and complexities. Our results
verify the feasibility of our near-optimal solutions for joint
chance-constrained power dispatch scenarios. Additionally, our feasibility
guarantees increase the transparency and interpretability of our method, which
is essential for operators to trust the outcomes. We showcase the effectiveness
of our model in solving the stochastic energy management problem of Virtual
Power Plants (VPPs). Our numerical findings complement our theoretical
justifications and demonstrate great flexibility in parameter tuning,
adaptability to diverse datasets, and increased computational speed.; 19) Equivariant formality of the little disks operad; The little $n$-disks operad is $SO(n)$ and $O(n)$-equivariantly formal over
the rationals. Equivalently, the oriented and unoriented framed little disks
operads are rationally formal as $\infty$-operads.; 20) Galvanic molecular intercalation; The intercalation of molecular species between the layers of van der Waals
(vdW) materials has recently emerged as a powerful approach to combine the
remarkable electronic and magnetic properties of vdW materials with the
chemical flexibility of organic molecules. However, the full transformative
potential of molecular intercalation remains underexplored, largely due to the
lack of simple, broadly applicable methods that preserve high crystalline
quality down to the few-layer limit. Here, we introduce a simple galvanic
approach to intercalate different molecules into various vdW materials under
ambient conditions, leveraging the low reduction potential of selected metals
to enable a spontaneous molecular insertion. We employ our method, which is
particularly well-suited for the in-situ intercalation of few-layer-thick
crystals, to intercalate nine vdW materials, including magnets and
superconductors, with molecules ranging from conventional alkylammonium ions to
metallorganic and bio-inspired chiral cations. Notably, intercalation leads to
a molecule-dependent enhancement of the superconducting transition in 2H-TaS2,
reaching a critical temperature of 4.7 K, higher than TaS2 monolayers.
Additionally, RuCl3 exhibits an unprecedented transition from antiferromagnetic
to ferrimagnetic ordering upon intercalation with cobaltocenium. These results
establish our approach as a versatile technique for engineering atomically thin
quantum materials and heterostructures, unlocking the transformative effects of
molecular intercalation.; 21) Data Sharing, Privacy and Security Considerations in the Energy Sector:
  A Review from Technical Landscape to Regulatory Specifications; Decarbonization, decentralization and digitalization are the three key
elements driving the twin energy transition. The energy system is evolving to a
more data driven ecosystem, leading to the need of communication and storage of
large amount of data of different resolution from the prosumers and other
stakeholders in the energy ecosystem. While the energy system is certainly
advancing, this paradigm shift is bringing in new privacy and security issues
related to collection, processing and storage of data - not only from the
technical dimension, but also from the regulatory perspective. Understanding
data privacy and security in the evolving energy system, regarding regulatory
compliance, is an immature field of research. Contextualized knowledge of how
related issues are regulated is still in its infancy, and the practical and
technical basis for the regulatory framework for data privacy and security is
not clear. To fill this gap, this paper conducts a comprehensive review of the
data-related issues for the energy system by integrating both technical and
regulatory dimensions. We start by reviewing open-access data, data
communication and data-processing techniques for the energy system, and use it
as the basis to connect the analysis of data-related issues from the integrated
perspective. We classify the issues into three categories: (i) data-sharing
among energy end users and stakeholders (ii) privacy of end users, and (iii)
cyber security, and then explore these issues from a regulatory perspective. We
analyze the evolution of related regulations, and introduce the relevant
regulatory initiatives for the categorized issues in terms of regulatory
definitions, concepts, principles, rights and obligations in the context of
energy systems. Finally, we provide reflections on the gaps that still exist,
and guidelines for regulatory frameworks for a truly participatory energy
system.; 22) Photon-ALP beam propagation from Mrk 501; The very high energy (VHE, E $>$ $100 \mathrm~{GeV}$) $\gamma$-ray
observations offer a possibility of indirectly detecting the presence of
axion-like particles (ALPs). The paper focuses on detecting photon-ALP
oscillations on $\gamma$-ray spectra from distant sources in astrophysical
magnetic fields. Strong evidence indicates that: (1) the photon-ALP
oscillations can effectively decrease the photon absorption at energies of
several tens of TeV -- caused by the extragalactic background light (EBL) -- to
a level able to explain better the observational data; (2) the impact of
magnetic-field models in photon-ALP beams crossing several magnetized media is
significant. We revisit the expected signature for the photon-ALP oscillation
effects on $\gamma-\gamma $ absorption in the TeV spectra of Mrk 501. The
result issues that the photon-ALP beam propagation with mass
$\mathrm{m_a}\sim10^{-10} eV$ and two-photon coupling constant
$\begin{aligned}g_{a\gamma}\sim0.417\times10^{-11}GeV^{-1}\end{aligned}$
crossing reasonable magnetic field scenarios considered here can roughly
reproduce the observed TeV $\gamma$-ray spectra for Mrk 501.; 23) Dream-IF: Dynamic Relative EnhAnceMent for Image Fusion; Image fusion aims to integrate comprehensive information from images acquired
through multiple sources. However, images captured by diverse sensors often
encounter various degradations that can negatively affect fusion quality.
Traditional fusion methods generally treat image enhancement and fusion as
separate processes, overlooking the inherent correlation between them; notably,
the dominant regions in one modality of a fused image often indicate areas
where the other modality might benefit from enhancement. Inspired by this
observation, we introduce the concept of dominant regions for image enhancement
and present a Dynamic Relative EnhAnceMent framework for Image Fusion
(Dream-IF). This framework quantifies the relative dominance of each modality
across different layers and leverages this information to facilitate reciprocal
cross-modal enhancement. By integrating the relative dominance derived from
image fusion, our approach supports not only image restoration but also a
broader range of image enhancement applications. Furthermore, we employ
prompt-based encoding to capture degradation-specific details, which
dynamically steer the restoration process and promote coordinated enhancement
in both multi-modal image fusion and image enhancement scenarios. Extensive
experimental results demonstrate that Dream-IF consistently outperforms its
counterparts.; 24) Local probe evidence supporting altermagnetism in Co$_{1/4}$NbSe$_2$; Muon spin rotation (${\mu}$SR), combined with muon stopping site and local
field analysis, was used to investigate the magnetic properties of cobalt
intercalated 2H-NbSe$_2$ (Co$_{1/4}$NbSe$_2$). Co$_{1/4}$NbSe$_2$ is predicted
to be an altermagnet, and whilst neutron diffraction has proposed its magnetic
structure, microscopic details such as the magnetic volume fraction remain
unclear. Therefore, a local probe investigation of its magnetism is essential.
Here, we report the determination of the magnetically ordered volume fraction,
ordered moment size, and magnetic structure. Our results reveal a sharp
second-order transition to a full-volume-fraction, homogeneous magnetic order
below $T_\mathrm{N}~=~168~$K. The moments are aligned antiparallel along the
$c$-axis, consistent with neutron diffraction and altermagnetism. ${\mu}$SR
reveals that the state remains stable under a $c$-axis magnetic field up to
$0.78~$T, with magnetisation measurements suggesting this robust regimes
extends to at least $5$~T. Within the time resolution of ${\mu}$SR, no
precursor slow altermagnetic fluctuations were detected above $T_\mathrm{N}$,
which is important for interpreting the band splitting in the paramagnetic
state reported by photoemission studies. These findings support altermagnetism
in Co$_{1/4}$NbSe$_2$ and motivate further experiments to explore the
tunability of its magnetic and electronic structure.; 25) LLMs for Drug-Drug Interaction Prediction: A Comprehensive Comparison; The increasing volume of drug combinations in modern therapeutic regimens
needs reliable methods for predicting drug-drug interactions (DDIs). While
Large Language Models (LLMs) have revolutionized various domains, their
potential in pharmaceutical research, particularly in DDI prediction, remains
largely unexplored. This study thoroughly investigates LLMs' capabilities in
predicting DDIs by uniquely processing molecular structures (SMILES), target
organisms, and gene interaction data as raw text input from the latest DrugBank
dataset. We evaluated 18 different LLMs, including proprietary models (GPT-4,
Claude, Gemini) and open-source variants (from 1.5B to 72B parameters), first
assessing their zero-shot capabilities in DDI prediction. We then fine-tuned
selected models (GPT-4, Phi-3.5 2.7B, Qwen-2.5 3B, Gemma-2 9B, and Deepseek R1
distilled Qwen 1.5B) to optimize their performance. Our comprehensive
evaluation framework included validation across 13 external DDI datasets,
comparing against traditional approaches such as l2-regularized logistic
regression. Fine-tuned LLMs demonstrated superior performance, with Phi-3.5
2.7B achieving a sensitivity of 0.978 in DDI prediction, with an accuracy of
0.919 on balanced datasets (50% positive, 50% negative cases). This result
represents an improvement over both zero-shot predictions and state-of-the-art
machine-learning methods used for DDI prediction. Our analysis reveals that
LLMs can effectively capture complex molecular interaction patterns and cases
where drug pairs target common genes, making them valuable tools for practical
applications in pharmaceutical research and clinical settings.; 26) Phase Stability Analysis of Volume-preserving Algorithms for Accurate
  Single Particle Orbit Simulations in Tokamak Plasmas; Second-order Volume-preserving algorithms (VPAs) for simulating charged
particle motion in electromagnetic fields have been generalized to a rotating
angle formulation by using the matrix decomposition methods. Based on this
method, the phase stability of this class of VPAs has been analyzed by using
the Discrete Fourier Transformations (DFT) technique. It is found that two
prominent VPAs, namely the $G_h^2$ and the Boris algorithm, exhibit optimal
phase precision for high-frequency (gyro motion) and low-frequency dynamics
(transit/bounce motion), respectively. These findings have been empirically
verified through numerical experiments. The insights gained from this study
enable the selection of an appropriate VPA for practical simulations based on
the characteristic frequencies of specific physics problems, which can
substantially enhance numerical accuracy and improve computational efficiency
for long-term simulations.; 27) An Adaptive Collocation Point Strategy For Physics Informed Neural
  Networks via the QR Discrete Empirical Interpolation Method; Physics-informed neural networks (PINNs) have gained significant attention
for solving forward and inverse problems related to partial differential
equations (PDEs). While advancements in loss functions and network
architectures have improved PINN accuracy, the impact of collocation point
sampling on their performance remains underexplored. Fixed sampling methods,
such as uniform random sampling and equispaced grids, can fail to capture
critical regions with high solution gradients, limiting their effectiveness for
complex PDEs. Adaptive methods, inspired by adaptive mesh refinement from
traditional numerical methods, address this by dynamically updating collocation
points during training but may overlook residual dynamics between updates,
potentially losing valuable information. To overcome this limitation, we
propose an adaptive collocation point selection strategy utilizing the QR
Discrete Empirical Interpolation Method (QR-DEIM), a reduced-order modeling
technique for efficiently approximating nonlinear functions. Our results on
benchmark PDEs, including the wave, Allen-Cahn, and Burgers' equations,
demonstrate that our QR-DEIM-based approach improves PINN accuracy compared to
existing methods, offering a promising direction for adaptive collocation point
strategies.; 28) Simultaneous Angle-of-Arrival Sensing and Anomalous Deflection with
  Aperiodically Loaded Patch Arrays; We propose and numerically demonstrate a reconfigurable patch antenna array
that enables simultaneous incident wave sensing and anomalous reflection
without prior knowledge of the propagation environment. We acquire anomalous
reflection by suppressing parasitic scattering through accurate and efficient
optimization of induced load currents and by varying impedances of reactive
loads. By mitigating parasitic scattering lobes, we demonstrate the feasibility
of accurately detecting the incoming illumination angle via the spatial Fourier
transform of the optimized load current distribution, facilitated by tunable
reactive loads. This approach eliminates the need for additional RF chains,
pre-computed data, or calibration measurements. The developed strategy, which
integrates arithmetic load optimization with angle-of-arrival sensing, is
applicable to general finite-size arrays.; 29) Geometric and topological rigidity of pinched submanifolds II; We continue the study of the geometry and topology of compact submanifolds of
arbitrary codimension in space forms satisfying a certain pinching condition
involving the length of the second fundamental form and the mean curvature. Our
primary focus is on four-dimensional submanifolds, where, to our surprise, both
the results obtained and the methods employed differ significantly and are
notably more intricate compared to those in higher dimensions. This study
heavily relies on concepts from four-dimensional geometry, the geometry of
Riemannian manifolds with nonnegative isotropic curvature, and the Bochner
technique, each playing a crucial role. The results are sharp and extend
previous results by several authors, without imposing any further assumption on
either the mean curvature or the fundamental group of the submanifold.; 30) Target Detection in OFDM-ISAC Systems: A Multipath Exploitation Approach; This paper investigates the potential of multipath exploitation for enhancing
target detection in orthogonal frequency division multiplexing (OFDM)-based
integrated sensing and communication (ISAC) systems. The study aims to improve
target detection performance by harnessing the diversity gain in the
delay-Doppler domain. We propose a weighted generalized likelihood ratio test
(GLRT) detector that effectively leverages the multipath propagation between
the base station (BS) and the target. To further enhance detection accuracy, a
joint optimization framework is developed for subcarrier power allocation at
the transmitter and weight coefficients of the GLRT detector. The objective is
to maximize the probability of target detection while satisfying constraints on
total transmit power and the communication receiver's signal-to-noise ratio
(SNR). An iterative algorithm based on the majorization-minimization (MM)
method is employed to address the resulting non-convex optimization problem.
Simulation results demonstrate the efficacy of the proposed algorithm and
confirm the benefits of multipath exploitation for target detection in
OFDM-ISAC systems under multipath-rich environments.; 31) On the Role of Pre-trained Embeddings in Binary Code Analysis; Deep learning has enabled remarkable progress in binary code analysis. In
particular, pre-trained embeddings of assembly code have become a gold standard
for solving analysis tasks, such as measuring code similarity or recognizing
functions. These embeddings are capable of learning a vector representation
from unlabeled code. In contrast to natural language processing, however, label
information is not scarce for many tasks in binary code analysis. For example,
labeled training data for function boundaries, optimization levels, and
argument types can be easily derived from debug information provided by a
compiler. Consequently, the main motivation of embeddings does not transfer
directly to binary code analysis.
  In this paper, we explore the role of pre-trained embeddings from a critical
perspective. To this end, we systematically evaluate recent embeddings for
assembly code on five downstream tasks using a corpus of 1.2 million functions
from the Debian distribution. We observe that several embeddings perform
similarly when sufficient labeled data is available, and that differences
reported in prior work are hardly noticeable. Surprisingly, we find that
end-to-end learning without pre-training performs best on average, which calls
into question the need for specialized embeddings. By varying the amount of
labeled data, we eventually derive guidelines for when embeddings offer
advantages and when end-to-end learning is preferable for binary code analysis.; 32) System Message Generation for User Preferences using Open-Source Models; System messages play a crucial role in interactions with large language
models (LLMs), often serving as prompts to initiate conversations. Through
system messages, users can assign specific roles, perform intended tasks,
incorporate background information, specify various output formats and
communication styles. Despite such versatility, publicly available data are
often lack system messages and subject to strict license constraints in the
industry field. Manual labeling of publicly available data with system messages
that align with user instructions demands significant resources. In view of
such challenges, our work introduces SysGen, a pipeline for generating system
messages with better aligned assistant responses from the supervised
fine-tuning dataset without system messages. Training on SysGen data has
demonstrated substantial improvements in the alignment of model responses with
system messages and user instructions, as demonstrated across various
open-source models on the Multifacet benchmark, while maintaining minimal
impact on other unseen benchmarks such as Open LLM Leaderboard 2. Our
qualitative analysis highlights the importance of diverse system messages to
ensure better adaptability across different contexts.; 33) State-of-the-Art Transformer Models for Image Super-Resolution:
  Techniques, Challenges, and Applications; Image Super-Resolution (SR) aims to recover a high-resolution image from its
low-resolution counterpart, which has been affected by a specific degradation
process. This is achieved by enhancing detail and visual quality. Recent
advancements in transformer-based methods have remolded image super-resolution
by enabling high-quality reconstructions surpassing previous deep-learning
approaches like CNN and GAN-based. This effectively addresses the limitations
of previous methods, such as limited receptive fields, poor global context
capture, and challenges in high-frequency detail recovery. Additionally, the
paper reviews recent trends and advancements in transformer-based SR models,
exploring various innovative techniques and architectures that combine
transformers with traditional networks to balance global and local contexts.
These neoteric methods are critically analyzed, revealing promising yet
unexplored gaps and potential directions for future research. Several
visualizations of models and techniques are included to foster a holistic
understanding of recent trends. This work seeks to offer a structured roadmap
for researchers at the forefront of deep learning, specifically exploring the
impact of transformers on super-resolution techniques.; 34) Liquidity provision of utility indifference type in decentralized
  exchanges; We present a mathematical formulation of liquidity provision in decentralized
exchanges. We focus on constant function market makers of utility indifference
type, which include constant product market makers with concentrated liquidity
as a special case. First, we examine no-arbitrage conditions for a liquidity
pool and compute an optimal arbitrage strategy when there is an external liquid
market. Second, we show that liquidity provision suffers from impermanent loss
unless a transaction fee is levied under the general framework with
concentrated liquidity. Third, we establish the well-definedness of
arbitrage-free reserve processes of a liquidity pool in continuous-time and
show that there is no loss-versus-rebalancing under a nonzero fee if the
external market price is continuous. We then argue that liquidity provision by
multiple liquidity providers can be understood as liquidity provision by a
representative liquidity provider, meaning that the analysis boils down to that
for a single liquidity provider. Last, but not least, we give an answer to the
fundamental question in which sense the very construction of constant function
market makers with concentrated liquidity in the popular platform Uniswap v3 is
optimal.; 35) On H-Intersecting Graph Families and Counting of Homomorphisms; This work derives an upper bound on the maximum cardinality of a family of
graphs on a fixed number of vertices, in which the intersection of every two
graphs in that family contains a subgraph that is isomorphic to a specified
graph H. Such families are referred to as H-intersecting graph families. The
bound is derived using the combinatorial version of Shearer's lemma, and it
forms a nontrivial extension of the bound derived by Chung, Graham, Frankl, and
Shearer (1986), where H is specialized to a triangle. The derived bound is
expressed in terms of the chromatic number of H, while a relaxed version,
formulated using the Lov\'{a}sz $\vartheta$-function of the complement of H,
offers reduced computational complexity. Additionally, a probabilistic version
of Shearer's lemma, combined with properties of the Shannon entropy, are
employed to establish bounds related to the enumeration of graph homomorphisms,
providing further insights into the interplay between combinatorial structures
and information-theoretic principles.; 36) Pricing is All You Need to Improve Traffic Routing; We investigate the design of pricing policies that enhance driver adherence
to route guidance, ensuring effective routing control. The major novelty lies
in that we adopt a Markov chain to model drivers' compliance rates conditioned
on both traffic states and tolls. By formulating the managed traffic network as
a nonlinear stochastic dynamical system, we can quantify in a more realistic
way the impacts of driver route choices and thus determine appropriate tolls.
Specially, we focus on a network comprised of one corridor and one local
street. We assume that a reasonable routing policy is specified in advance.
However, drivers could be reluctant to be detoured. Thus a fixed toll is set on
the corridor to give drivers incentives to choose the local street. We evaluate
the effectiveness of the given routing and pricing policies via stability
analysis. We suggest using the stability and instability conditions to
establish lower and upper bounds on throughput. This allows us to select
suitable tolls that maximize these bounds.; 37) Savaal: Scalable Concept-Driven Question Generation to Enhance Human
  Learning; Assessing and enhancing human learning through question-answering is vital,
yet automating this process remains challenging. While large language models
(LLMs) excel at summarization and query responses, their ability to generate
meaningful questions for learners is underexplored.
  We propose Savaal, a scalable question-generation system with three
objectives: (i) scalability, enabling question generation from hundreds of
pages of text (ii) depth of understanding, producing questions beyond factual
recall to test conceptual reasoning, and (iii) domain-independence,
automatically generating questions across diverse knowledge areas. Instead of
providing an LLM with large documents as context, Savaal improves results with
a three-stage processing pipeline. Our evaluation with 76 human experts on 71
papers and PhD dissertations shows that Savaal generates questions that better
test depth of understanding by 6.5X for dissertations and 1.5X for papers
compared to a direct-prompting LLM baseline. Notably, as document length
increases, Savaal's advantages in higher question quality and lower cost become
more pronounced.; 38) Strands Rocq: Why is a Security Protocol Correct, Mechanically?; Strand spaces are a formal framework for symbolic protocol verification that
allows for pen-and-paper proofs of security. While extremely insightful,
pen-and-paper proofs are error-prone, and it is hard to gain confidence on
their correctness. To overcome this problem, we developed StrandsRocq, a full
mechanization of the strand spaces in Coq (soon to be renamed Rocq). The
mechanization was designed to be faithful to the original pen-and-paper
development, and it was engineered to be modular and extensible. StrandsRocq
incorporates new original proof techniques, a novel notion of maximal
penetrator that enables protocol compositionality, and a set of Coq tactics
tailored to the domain, facilitating proof automation and reuse, and
simplifying the work of protocol analysts. To demonstrate the versatility of
our approach, we modelled and analyzed a family of authentication protocols,
drawing inspiration from ISO/IEC 9798-2 two-pass authentication, the classical
Needham-Schroeder-Lowe protocol, as well as a recently-proposed static analysis
for a key management API. The analyses in StrandsRocq confirmed the high degree
of proof reuse, and enabled us to distill the minimal requirements for protocol
security. Through mechanization, we identified and addressed several issues in
the original proofs and we were able to significantly improve the precision of
the static analysis for the key management API. Moreover, we were able to
leverage the novel notion of maximal penetrator to provide a compositional
proof of security for two simple authentication protocols.; 39) GRiNS: A Python Library for Simulating Gene Regulatory Network Dynamics; The emergent dynamics of complex gene regulatory networks govern various
cellular processes. However, understanding these dynamics is challenging due to
the difficulty of parameterizing the computational models for these networks,
especially as the network size increases. Here, we introduce a simulation
library, Gene Regulatory Interaction Network Simulator (GRiNS), to address
these challenges. GRiNS integrates popular parameter-agnostic simulation
frameworks, RACIPE and Boolean Ising formalism, into a single Python library
capable of leveraging GPU acceleration for efficient and scalable simulations.
GRiNS extends the ordinary differential equations (ODE) based RACIPE framework
with a more modular design, allowing users to choose parameters, initial
conditions, and time-series outputs for greater customisability and accuracy in
simulations. For large networks, where ODE-based simulation formalisms do not
scale well, GRiNS implements Boolean Ising formalism, providing a simplified,
coarse-grained alternative, significantly reducing the computational cost while
capturing key dynamical behaviours of large regulatory networks. The
documentation and installation instructions for GRiNS can be found at
https://moltenecdysone09.github.io/GRiNS/.; 40) Large Language Models for Knowledge Graph Embedding Techniques, Methods,
  and Challenges: A Survey; Large Language Models (LLMs) have attracted a lot of attention in various
fields due to their superior performance, aiming to train hundreds of millions
or more parameters on large amounts of text data to understand and generate
natural language. As the superior performance of LLMs becomes apparent, they
are increasingly being applied to knowledge graph embedding (KGE) related tasks
to improve the processing results. As a deep learning model in the field of
Natural Language Processing (NLP), it learns a large amount of textual data to
predict the next word or generate content related to a given text. However,
LLMs have recently been invoked to varying degrees in different types of KGE
related scenarios such as multi-modal KGE and open KGE according to their task
characteristics. In this paper, we investigate a wide range of approaches for
performing LLMs-related tasks in different types of KGE scenarios. To better
compare the various approaches, we summarize each KGE scenario in a
classification. In addition to the categorization methods, we provide a tabular
overview of the methods and their source code links for a more direct
comparison. In the article we also discuss the applications in which the
methods are mainly used and suggest several forward-looking directions for the
development of this new research area.; 41) Dissipation and particle acceleration in astrophysical jets with
  velocity and magnetic shear: Interaction of Kelvin-Helmholtz and Drift-Kink
  Instabilities; We present 2D particle-in-cell simulations of a magnetized, collisionless,
relativistic pair plasma subjected to combined velocity and magnetic-field
shear, a scenario typical for astrophysical black-hole jet-wind boundaries. We
create conditions where only the Kelvin-Helmholtz (KH) and Drift-Kink (DK)
instabilities can develop, while tearing modes are forbidden. We find that DKI
can effectively disrupt the cats-eye vortices generated by KHI, creating a
turbulent shear layer on the DK timescale. This interplay leads to a
significant enhancement of dissipation over cases with only velocity shear or
only magnetic shear. Moreover, we observe efficient nonthermal particle
acceleration caused by the alignment of the instability-driven electric fields
with Speiser-like motion of particles close to the shear interface. This study
highlights the sensitivity of dissipation to multiple simultaneous
instabilities, thus providing a strong motivation for further studies of their
nonlinear interaction at the kinetic level.; 42) Utility-inspired Reward Transformations Improve Reinforcement Learning
  Training of Language Models; Current methods that train large language models (LLMs) with reinforcement
learning feedback, often resort to averaging outputs of multiple rewards
functions during training. This overlooks crucial aspects of individual reward
dimensions and inter-reward dependencies that can lead to sub-optimal outcomes
in generations. In this work, we show how linear aggregation of rewards
exhibits some vulnerabilities that can lead to undesired properties of
generated text. We then propose a transformation of reward functions inspired
by economic theory of utility functions (specifically Inada conditions), that
enhances sensitivity to low reward values while diminishing sensitivity to
already high values. We compare our approach to the existing baseline methods
that linearly aggregate rewards and show how the Inada-inspired reward feedback
is superior to traditional weighted averaging. We quantitatively and
qualitatively analyse the difference in the methods, and see that models
trained with Inada-transformations score as more helpful while being less
harmful.; 43) Electrons and phonons in pentacene, insights from comparison between
  experiment and simulations; We have computed the vibrational pattern and the electron-phonon coupling at
several q-points in the Brillouin Zone for the three known pentacene
polymorphs. After having verified that they effectively correspond to three
different structures, we revisit the assignment of the experimental Raman
modes. Finally, using a pool of post-processing tools, we present the phonon
spectra and dispersions, confirming previous indications about the effect of
phonon dispersion and charge mobility. In addition, we consider in an effective
way the coexistence of more polymorphs and the possible role of defects and
energy disorder.; 44) Bridging Writing Manner Gap in Visual Instruction Tuning by Creating
  LLM-aligned Instructions; In the realm of Large Multi-modal Models (LMMs), the instruction quality
during the visual instruction tuning stage significantly influences the
performance of modality alignment. In this paper, we assess the instruction
quality from a unique perspective termed \textbf{Writing Manner}, which
encompasses the selection of vocabulary, grammar and sentence structure to
convey specific semantics. We argue that there exists a substantial writing
manner gap between the visual instructions and the base Large Language Models
(LLMs) within LMMs. This gap forces the pre-trained base LLMs to deviate from
their original writing styles, leading to capability degradation of both base
LLMs and LMMs. To bridge the writing manner gap while preserving the original
semantics, we propose directly leveraging the base LLM to align the writing
manner of soft-format visual instructions with that of the base LLM itself,
resulting in novel LLM-aligned instructions. The manual writing manner
evaluation results demonstrate that our approach successfully minimizes the
writing manner gap. By utilizing LLM-aligned instructions, the baseline models
LLaVA-7B and QwenVL demonstrate enhanced resistance to hallucinations and
non-trivial comprehensive improvements across all $15$ visual and language
benchmarks.; 45) BioSerenity-E1: a self-supervised EEG model for medical applications; Electroencephalography (EEG) serves as an essential diagnostic tool in
neurology; however, its accurate manual interpretation is a time-intensive
process that demands highly specialized expertise, which remains relatively
scarce and not consistently accessible. To address these limitations, the
implementation of automated pre-screening and analysis systems for EEG data
holds considerable promise. Advances in self-supervised learning made it
possible to pre-train complex deep learning architectures on large volumes of
unlabeled EEG data to learn generalizable representations, that can later be
used to enhance performance on multiple tasks while needing less downstream
data. In the present paper, we introduce BioSerenity-E1, the first of a family
of self-supervised foundation models for clinical EEG applications that
combines spectral tokenization with masked prediction to achieve
state-of-the-art performance across relevant diagnostic tasks. The two-phase
self-supervised pretraining framework initially acquires compressed EEG
representations via a transformer-based VQ-VAE architecture designed to
reconstruct log-multitaper spectral projections, then implements extensive (70%
block) masked token prediction to force the model to learn complex
spatiotemporal dependencies in EEG signals. BioSerenity-E1 achieves strong
performance across three clinical tasks, either in line or above
state-of-the-art methods: seizure detection (AUROC = 0.926, Sensitivity =
0.909), normal/abnormal classification (AUPRC = 0.970 on proprietary data;
0.910 on TUH-Abnormal), and multiclass pathology differentiation on unbalanced
data (Weighted F1 = 0.730). The utility of BioSerenity-E1 is further confirmed
in low-data regimes scenarios, showing clear improvements in AUPRC (from +2% to
17%) when trained on less than 10% of the available data.; 46) CoDa-4DGS: Dynamic Gaussian Splatting with Context and Deformation
  Awareness for Autonomous Driving; Dynamic scene rendering opens new avenues in autonomous driving by enabling
closed-loop simulations with photorealistic data, which is crucial for
validating end-to-end algorithms. However, the complex and highly dynamic
nature of traffic environments presents significant challenges in accurately
rendering these scenes. In this paper, we introduce a novel 4D Gaussian
Splatting (4DGS) approach, which incorporates context and temporal deformation
awareness to improve dynamic scene rendering. Specifically, we employ a 2D
semantic segmentation foundation model to self-supervise the 4D semantic
features of Gaussians, ensuring meaningful contextual embedding.
Simultaneously, we track the temporal deformation of each Gaussian across
adjacent frames. By aggregating and encoding both semantic and temporal
deformation features, each Gaussian is equipped with cues for potential
deformation compensation within 3D space, facilitating a more precise
representation of dynamic scenes. Experimental results show that our method
improves 4DGS's ability to capture fine details in dynamic scene rendering for
autonomous driving and outperforms other self-supervised methods in 4D
reconstruction and novel view synthesis. Furthermore, CoDa-4DGS deforms
semantic features with each Gaussian, enabling broader applications.; 47) DynASyn: Multi-Subject Personalization Enabling Dynamic Action Synthesis; Recent advances in text-to-image diffusion models spurred research on
personalization, i.e., a customized image synthesis, of subjects within
reference images. Although existing personalization methods are able to alter
the subjects' positions or to personalize multiple subjects simultaneously,
they often struggle to modify the behaviors of subjects or their dynamic
interactions. The difficulty is attributable to overfitting to reference
images, which worsens if only a single reference image is available. We propose
DynASyn, an effective multi-subject personalization from a single reference
image addressing these challenges. DynASyn preserves the subject identity in
the personalization process by aligning concept-based priors with subject
appearances and actions. This is achieved by regularizing the attention maps
between the subject token and images through concept-based priors. In addition,
we propose concept-based prompt-and-image augmentation for an enhanced
trade-off between identity preservation and action diversity. We adopt an
SDE-based editing guided by augmented prompts to generate diverse appearances
and actions while maintaining identity consistency in the augmented images.
Experiments show that DynASyn is capable of synthesizing highly realistic
images of subjects with novel contexts and dynamic interactions with the
surroundings, and outperforms baseline methods in both quantitative and
qualitative aspects.; 48) PersonaBench: Evaluating AI Models on Understanding Personal Information
  through Accessing (Synthetic) Private User Data; Personalization is critical in AI assistants, particularly in the context of
private AI models that work with individual users. A key scenario in this
domain involves enabling AI models to access and interpret a user's private
data (e.g., conversation history, user-AI interactions, app usage) to
understand personal details such as biographical information, preferences, and
social connections. However, due to the sensitive nature of such data, there
are no publicly available datasets that allow us to assess an AI model's
ability to understand users through direct access to personal information.
  To address this gap, we introduce a synthetic data generation pipeline that
creates diverse, realistic user profiles and private documents simulating human
activities. Leveraging this synthetic data, we present PersonaBench, a
benchmark designed to evaluate AI models' performance in understanding personal
information derived from simulated private user data.
  We evaluate Retrieval-Augmented Generation (RAG) pipelines using questions
directly related to a user's personal information, supported by the relevant
private documents provided to the models. Our results reveal that current
retrieval-augmented AI models struggle to answer private questions by
extracting personal information from user documents, highlighting the need for
improved methodologies to enhance personalization capabilities in AI.; 49) A technical review of multi-omics data integration methods: from
  classical statistical to deep generative approaches; The rapid advancement of high-throughput sequencing and other assay
technologies has resulted in the generation of large and complex multi-omics
datasets, offering unprecedented opportunities for advancing precision medicine
strategies. However, multi-omics data integration presents significant
challenges due to the high dimensionality, heterogeneity, experimental gaps,
and frequency of missing values across data types. Computational methods have
been developed to address these issues, employing statistical and machine
learning approaches to uncover complex biological patterns and provide deeper
insights into our understanding of disease mechanisms. Here, we comprehensively
review state-of-the-art multi-omics data integration methods with a focus on
deep generative models, particularly variational autoencoders (VAEs) that have
been widely used for data imputation and augmentation, joint embedding
creation, and batch effect correction. We explore the technical aspects of loss
functions and regularisation techniques including adversarial training,
disentanglement and contrastive learning. Moreover, we discuss recent
advancements in foundation models and the integration of emerging data
modalities, while describing the current limitations and outlining future
directions for enhancing multi-modal methodologies in biomedical research.; 50) Arab Spring's Impact on Science through the Lens of Scholarly Attention,
  Funding, and Migration; The Arab Spring is a major socio-political movement that reshaped democratic
aspirations in the Middle East and North Africa, attracting global attention
through news, social media, and academic discourse. However, its consequences
on the academic landscape in the region are still unclear. Here, we conduct the
first study of scholarly attention toward 10 target countries affected by the
Arab Spring by analyzing more than 25 million articles published from 2002 to
2019. We find that changes in scholarly attention were unevenly distributed
among target countries, with Egypt attracting the most attention. We reveal
strong correlations between increases in scholarly attention given by source
countries and increases in funding for research about the target countries as
well as increased immigration of scholars from the affected region to them.
Notably, our analysis reveals Saudi Arabia's positioning as a key player, among
Western nations, that shapes research in the region.; 51) Can KAN CANs? Input-convex Kolmogorov-Arnold Networks (KANs) as
  hyperelastic constitutive artificial neural networks (CANs); Traditional constitutive models rely on hand-crafted parametric forms with
limited expressivity and generalizability, while neural network-based models
can capture complex material behavior but often lack interpretability. To
balance these trade-offs, we present Input-Convex Kolmogorov-Arnold Networks
(ICKANs) for learning polyconvex hyperelastic constitutive laws. ICKANs
leverage the Kolmogorov-Arnold representation, decomposing the model into
compositions of trainable univariate spline-based activation functions for rich
expressivity. We introduce trainable input-convex splines within the KAN
architecture, ensuring physically admissible polyconvex hyperelastic models.
The resulting models are both compact and interpretable, enabling explicit
extraction of analytical constitutive relationships through an input-convex
symbolic regression techinque. Through unsupervised training on full-field
strain data and limited global force measurements, ICKANs accurately capture
nonlinear stress-strain behavior across diverse strain states. Finite element
simulations of unseen geometries with trained ICKAN hyperelastic constitutive
models confirm the framework's robustness and generalization capability.; 52) Metarecycling in Physics Education in Basic Education within the Context
  of the UN Sustainable Development Goals through Technological Research; The rapid advancement of digital technologies in the first quarter of the
21st century has introduced significant transformations in various fields, such
as communication, healthcare, and education. However, it has also led to an
increase in the use and disposal of electronic devices, resulting in
environmental challenges related to Waste Electrical and Electronic Equipment
(WEEE), also known as e-waste. This phenomenon is observed in schools, where
the integration and renewal of equipment have become essential for the
development and implementation of new teaching strategies. Based on a
technological research project, we present how students from a public school in
S\~ao Paulo's countryside conducted e-waste reuse processes, applying the
principles of metarecycling and physics knowledge to build a portable battery
(power bank) and a smartphone charger powered by a dynamo attached to a
bicycle. The appropriation of the relationships between science, technology,
and social aspects was facilitated by validating the chargers through
characterization tests of the charging time provided by the power bank and the
bicycle-installed device under riding conditions. Educational actions within
the community, involving concepts of sustainability, clean energy, and health
benefits through physical exercise, were guided by the United Nations
Sustainable Development Goals (SDGs).; 53) Federated Learning for Anomaly Detection in Energy Consumption Data:
  Assessing the Vulnerability to Adversarial Attacks; Anomaly detection is crucial in the energy sector to identify irregular
patterns indicating equipment failures, energy theft, or other issues. Machine
learning techniques for anomaly detection have achieved great success, but are
typically centralized, involving sharing local data with a central server which
raises privacy and security concerns. Federated Learning (FL) has been gaining
popularity as it enables distributed learning without sharing local data.
However, FL depends on neural networks, which are vulnerable to adversarial
attacks that manipulate data, leading models to make erroneous predictions.
While adversarial attacks have been explored in the image domain, they remain
largely unexplored in time series problems, especially in the energy domain.
Moreover, the effect of adversarial attacks in the FL setting is also mostly
unknown. This paper assesses the vulnerability of FL-based anomaly detection in
energy data to adversarial attacks. Specifically, two state-of-the-art models,
Long Short Term Memory (LSTM) and Transformers, are used to detect anomalies in
an FL setting, and two white-box attack methods, Fast Gradient Sign Method
(FGSM) and Projected Gradient Descent (PGD), are employed to perturb the data.
The results show that FL is more sensitive to PGD attacks than to FGSM attacks,
attributed to PGD's iterative nature, resulting in an accuracy drop of over 10%
even with naive, weaker attacks. Moreover, FL is more affected by these attacks
than centralized learning, highlighting the need for defense mechanisms in FL.; 54) SFC-GAN: A Generative Adversarial Network for Brain Functional and
  Structural Connectome Translation; Modern brain imaging technologies have enabled the detailed reconstruction of
human brain connectomes, capturing structural connectivity (SC) from diffusion
MRI and functional connectivity (FC) from functional MRI. Understanding the
intricate relationships between SC and FC is vital for gaining deeper insights
into the brain's functional and organizational mechanisms. However, obtaining
both SC and FC modalities simultaneously remains challenging, hindering
comprehensive analyses. Existing deep generative models typically focus on
synthesizing a single modality or unidirectional translation between FC and SC,
thereby missing the potential benefits of bi-directional translation,
especially in scenarios where only one connectome is available. Therefore, we
propose Structural-Functional Connectivity GAN (SFC-GAN), a novel framework for
bidirectional translation between SC and FC. This approach leverages the
CycleGAN architecture, incorporating convolutional layers to effectively
capture the spatial structures of brain connectomes. To preserve the
topological integrity of these connectomes, we employ a structure-preserving
loss that guides the model in capturing both global and local connectome
patterns while maintaining symmetry. Our framework demonstrates superior
performance in translating between SC and FC, outperforming baseline models in
similarity and graph property evaluations compared to ground truth data, each
translated modality can be effectively utilized for downstream classification.; 55) PROTOCALC, A W-band polarized calibrator for CMB Telescopes: application
  to Simons Observatory and CLASS; Current- and next-generation Cosmic Microwave Background (CMB) experiments
will measure polarization anisotropies with unprecedented sensitivities. The
need for high precision in these measurements underscores the importance of
gaining a comprehensive understanding of instrument properties, with a
particular emphasis on the study of the beam properties and, in particular,
their polarization characteristics, and the measurement of the polarization
angle. In this context, a major challenge lies in the scarcity of millimeter
polarized astrophysical sources with sufficient brightness and calibration
knowledge to meet the stringent accuracy requirements of future CMB missions.
This led to the development of a drone-borne calibration source designed for
frequency band centered on approximately 90 GHz band, matching a commonly used
channel in ground based CMB measurements. The PROTOtype CALibrator for
Cosmology, PROTOCALC, has undergone thorough in-lab testing, and its properties
have been subsequently modeled through simulation software integrated into the
standard Simons Observatory (SO) analysis pipeline. Moreover, the PROTOCALC
system has been tested in the field, having been deployed twice on calibration
campaigns with CMB telescopes in the Atacama desert. The data collected
constrain the roll angle of the source with a statistical accuracy of
$0.045^\circ$.; 56) Out-of-Distribution Detection on Graphs: A Survey; Graph machine learning has witnessed rapid growth, driving advancements
across diverse domains. However, the in-distribution assumption, where training
and testing data share the same distribution, often breaks in real-world
scenarios, leading to degraded model performance under distribution shifts.
This challenge has catalyzed interest in graph out-of-distribution (GOOD)
detection, which focuses on identifying graph data that deviates from the
distribution seen during training, thereby enhancing model robustness. In this
paper, we provide a rigorous definition of GOOD detection and systematically
categorize existing methods into four types: enhancement-based,
reconstruction-based, information propagation-based, and classification-based
approaches. We analyze the principles and mechanisms of each approach and
clarify the distinctions between GOOD detection and related fields, such as
graph anomaly detection, outlier detection, and GOOD generalization. Beyond
methodology, we discuss practical applications and theoretical foundations,
highlighting the unique challenges posed by graph data. Finally, we discuss the
primary challenges and propose future directions to advance this emerging
field. The repository of this survey is available at
https://github.com/ca1man-2022/Awesome-GOOD-Detection.; 57) Federated Quantum-Train Long Short-Term Memory for Gravitational Wave
  Signal; We present Federated QT-LSTM, a novel framework that combines the
Quantum-Train (QT) methodology with Long Short-Term Memory (LSTM) networks in a
federated learning setup. By leveraging quantum neural networks (QNNs) to
generate classical LSTM model parameters during training, the framework
effectively addresses challenges in model compression, scalability, and
computational efficiency. Importantly, Federated QT-LSTM eliminates the
reliance on quantum devices during inference, making it practical for
real-world applications. Experiments on simulated gravitational wave (GW)
signal datasets demonstrate the framework's superior performance compared to
baseline models, including LSTM and QLSTM, achieving lower training and testing
losses while significantly reducing the number of trainable parameters. The
results also reveal that deeper QT layers enhance model expressiveness for
complex tasks, highlighting the adaptability of the framework. Federated
QT-LSTM provides a scalable and efficient solution for privacy-preserving
distributed learning, showcasing the potential of quantum-inspired techniques
in advancing time-series prediction and signal reconstruction tasks.; 58) The quantromon: A qubit-resonator system with orthogonal qubit and
  readout modes; The measurement of a superconducting qubit is implemented by coupling it to a
resonator. The common choice is transverse coupling, which, in the dispersive
approximation, introduces an interaction term which enables the measurement.
This cross-Kerr term provides a qubit-state dependent dispersive shift in the
resonator frequency with the device parameters chosen carefully to get
sufficient signal while minimizing Purcell decay of the qubit. We introduce a
two-mode circuit, nicknamed quantromon, with two orthogonal modes implementing
a qubit and a resonator. Unlike before, where the coupling term emerges as a
perturbative expansion, the quantromon has intrinsic cross-Kerr coupling by
design. Our experiments implemented in a hybrid 2D-3D cQED architecture
demonstrate some unique features of the quantromon like weak dependence of the
dispersive shift on the qubit-resonator detuning and intrinsic Purcell
protection. In a tunable qubit-frequency device, we show that the dispersive
shift ($2\chi/2\pi$) changes by only $0.8$ MHz while the qubit-resonator
detuning ($\Delta/2\pi$) is varied between $0.398$ GHz - $3.288$ GHz. We also
demonstrate Purcell protection in a second device where we tune the
orthogonality between the two modes. Finally, we demonstrate a single-shot
readout fidelity of $98.3\%$ without using a parametric amplifier which is
comparable to the state-of-the-art and suggests a potential simplification of
the measurement circuitry for scaling up quantum processors.; 59) DPO-Shift: Shifting the Distribution of Direct Preference Optimization; Direct Preference Optimization (DPO) and its variants have become
increasingly popular for aligning language models with human preferences. These
methods aim to teach models to better distinguish between chosen (or preferred)
and rejected (or dispreferred) responses. However, prior research has
identified that the probability of chosen responses often decreases during
training, and this phenomenon is known as likelihood displacement. To tackle
this challenge, in this work we introduce \method to controllably shift the
distribution of the chosen probability. Then, we show that \method exhibits a
fundamental trade-off between improving the chosen probability and sacrificing
the reward margin, as supported by both theoretical analysis and experimental
validation. Furthermore, we demonstrate the superiority of \method over DPO on
downstream tasks such as MT-Bench and a designed win rate experiment. We
believe this study shows that the likelihood displacement issue of DPO can be
effectively mitigated with a simple, theoretically grounded solution. Our code
is available at https://github.com/Meaquadddd/DPO-Shift.; 60) The chiral SYK model in three-dimensional holography; A celebrated realization of the holographic principle posits an approximate
duality between the $(0+1)$-dimensional quantum mechanical SYK model and
two-dimensional Jackiw-Teitelboim gravity, mediated by the Schwarzian action as
an effective low energy theory common to both systems. We here propose a
generalization of this correspondence to one dimension higher. Starting from
different microscopic realizations of effectively chiral $(1+1)$-dimensional
generalizations of the SYK model, we derive a reduction to the
Alekseev-Shatashvilli (AS)-action, a minimal extension of the Schwarzian action
which has been proposed as the effective boundary action of three-dimensional
gravity. In the bulk, we show how the same action describes fluctuations around
the Euclidean BTZ black hole configuration, the dominant stationary solution of
three-dimensional gravity. These two constructions allow us to match bulk and
boundary coupling constants, and to compute observables. Specifically, we apply
semiclassical techniques inspired by condensed matter physics to the
computation of out-of-time-order correlation functions (OTOCs), demonstrating
maximal chaos in the chiral SYK chain and its gravity dual.; 61) Phantom: Subject-consistent video generation via cross-modal alignment; The continuous development of foundational models for video generation is
evolving into various applications, with subject-consistent video generation
still in the exploratory stage. We refer to this as Subject-to-Video, which
extracts subject elements from reference images and generates
subject-consistent video through textual instructions. We believe that the
essence of subject-to-video lies in balancing the dual-modal prompts of text
and image, thereby deeply and simultaneously aligning both text and visual
content. To this end, we propose Phantom, a unified video generation framework
for both single and multi-subject references. Building on existing
text-to-video and image-to-video architectures, we redesign the joint
text-image injection model and drive it to learn cross-modal alignment via
text-image-video triplet data. In particular, we emphasize subject consistency
in human generation, covering existing ID-preserving video generation while
offering enhanced advantages. The project homepage is here
https://phantom-video.github.io/Phantom/.; 62) BrainNet-MoE: Brain-Inspired Mixture-of-Experts Learning for
  Neurological Disease Identification; The Lewy body dementia (LBD) is the second most common neurodegenerative
dementia after Alzheimer's disease (AD). Early differentiation between AD and
LBD is crucial because they require different treatment approaches, but this is
challenging due to significant clinical overlap, heterogeneity, complex
pathogenesis, and the rarity of LBD. While recent advances in artificial
intelligence (AI) demonstrate powerful learning capabilities and offer new hope
for accurate diagnosis, existing methods primarily focus on designing
""neural-level networks"". Our work represents a pioneering effort in modeling
system-level artificial neural network called BrainNet-MoE for brain modeling
and diagnosing. Inspired by the brain's hierarchical organization of bottom-up
sensory integration and top-down control, we design a set of disease-specific
expert groups to process brain sub-network under different condition, A disease
gate mechanism guides the specializa-tion of expert groups, while a transformer
layer enables communication be-tween all sub-networks, generating a
comprehensive whole-brain represen-tation for downstream disease
classification. Experimental results show superior classification accuracy with
interpretable insights into how brain sub-networks contribute to different
neurodegenerative conditions.; 63) Recurrence relations for degenerate Bell and Dowling polynomials via
  Boson operators; Spivey found a recurrence relation for the Bell numbers by using
combinatorial method. The aim of this paper is to derive Spivey's type
recurrence relations for the degenerate Bell polynomials and the degenerate
Dowling polynomials by using the boson annihilation and creation operators
satisfying the commutation relation aa+-a+a=1.
  In addition, we derive a Spivey's type recurrence relation for the r-Dowling
polynomials.; 64) Causally Aligned Curriculum Learning; A pervasive challenge in Reinforcement Learning (RL) is the ""curse of
dimensionality"" which is the exponential growth in the state-action space when
optimizing a high-dimensional target task. The framework of curriculum learning
trains the agent in a curriculum composed of a sequence of related and more
manageable source tasks. The expectation is that when some optimal decision
rules are shared across source tasks and the target task, the agent could more
quickly pick up the necessary skills to behave optimally in the environment,
thus accelerating the learning process. However, this critical assumption of
invariant optimal decision rules does not necessarily hold in many practical
applications, specifically when the underlying environment contains unobserved
confounders. This paper studies the problem of curriculum RL through causal
lenses. We derive a sufficient graphical condition characterizing causally
aligned source tasks, i.e., the invariance of optimal decision rules holds. We
further develop an efficient algorithm to generate a causally aligned
curriculum, provided with qualitative causal knowledge of the target task.
Finally, we validate our proposed methodology through experiments in discrete
and continuous confounded tasks with pixel observations.; 65) Explainability and AI Confidence in Clinical Decision Support Systems:
  Effects on Trust, Diagnostic Performance, and Cognitive Load in Breast Cancer
  Care; Artificial Intelligence (AI) has demonstrated potential in healthcare,
particularly in enhancing diagnostic accuracy and decision-making through
Clinical Decision Support Systems (CDSSs). However, the successful
implementation of these systems relies on user trust and reliance, which can be
influenced by explainable AI. This study explores the impact of varying
explainability levels on clinicians trust, cognitive load, and diagnostic
performance in breast cancer detection. Utilizing an interrupted time series
design, we conducted a web-based experiment involving 28 healthcare
professionals. The results revealed that high confidence scores substantially
increased trust but also led to overreliance, reducing diagnostic accuracy. In
contrast, low confidence scores decreased trust and agreement while increasing
diagnosis duration, reflecting more cautious behavior. Some explainability
features influenced cognitive load by increasing stress levels. Additionally,
demographic factors such as age, gender, and professional role shaped
participants' perceptions and interactions with the system. This study provides
valuable insights into how explainability impact clinicians' behavior and
decision-making. The findings highlight the importance of designing AI-driven
CDSSs that balance transparency, usability, and cognitive demands to foster
trust and improve integration into clinical workflows.; 66) Relativistic model of spontaneous wave-function localization induced by
  nonHermitian colored noise; We propose a relativistic model of spontaneous wave-function collapse, based
on a random nonHermitian action where the fermion density operator is coupled
to a universal colored noise. Upon quantization, the wave function obeys a
nonlinear stochastic differential equation that respects statistical Lorentz
symmetry. The localization mechanism is driven by the colored noise, derived
from the d'Alembert equation using generalized stochastic calculus in
1+3-dimensional spacetime. We analytically determine the noise-induced
localization length, which decreases as the size of the observable universe
increases.; 67) CrossView-GS: Cross-view Gaussian Splatting For Large-scale Scene
  Reconstruction; 3D Gaussian Splatting (3DGS) has emerged as a prominent method for scene
representation and reconstruction, leveraging densely distributed Gaussian
primitives to enable real-time rendering of high-resolution images. While
existing 3DGS methods perform well in scenes with minor view variation, large
view changes in cross-view scenes pose optimization challenges for these
methods. To address these issues, we propose a novel cross-view Gaussian
Splatting method for large-scale scene reconstruction, based on dual-branch
fusion. Our method independently reconstructs models from aerial and ground
views as two independent branches to establish the baselines of Gaussian
distribution, providing reliable priors for cross-view reconstruction during
both initialization and densification. Specifically, a gradient-aware
regularization strategy is introduced to mitigate smoothing issues caused by
significant view disparities. Additionally, a unique Gaussian supplementation
strategy is utilized to incorporate complementary information of dual-branch
into the cross-view model. Extensive experiments on benchmark datasets
demonstrate that our method achieves superior performance in novel view
synthesis compared to state-of-the-art methods.; 68) SVGS-DSGAT: An IoT-Enabled Innovation in Underwater Robotic Object
  Detection Technology; With the advancement of Internet of Things (IoT) technology, underwater
target detection and tracking have become increasingly important for ocean
monitoring and resource management. Existing methods often fall short in
handling high-noise and low-contrast images in complex underwater environments,
lacking precision and robustness. This paper introduces a novel SVGS-DSGAT
model that combines GraphSage, SVAM, and DSGAT modules, enhancing feature
extraction and target detection capabilities through graph neural networks and
attention mechanisms. The model integrates IoT technology to facilitate
real-time data collection and processing, optimizing resource allocation and
model responsiveness. Experimental results demonstrate that the SVGS-DSGAT
model achieves an mAP of 40.8% on the URPC 2020 dataset and 41.5% on the
SeaDronesSee dataset, significantly outperforming existing mainstream models.
This IoT-enhanced approach not only excels in high-noise and complex
backgrounds but also improves the overall efficiency and scalability of the
system. This research provides an effective IoT solution for underwater target
detection technology, offering significant practical application value and
broad development prospects.; 69) CSSDM Ontology to Enable Continuity of Care Data Interoperability; The rapid advancement of digital technologies and recent global pandemic
scenarios have led to a growing focus on how these technologies can enhance
healthcare service delivery and workflow to address crises. Action plans that
consolidate existing digital transformation programs are being reviewed to
establish core infrastructure and foundations for sustainable healthcare
solutions. Reforming health and social care to personalize home care, for
example, can help avoid treatment in overcrowded acute hospital settings and
improve the experiences and outcomes for both healthcare professionals and
service users. In this information-intensive domain, addressing the
interoperability challenge through standards-based roadmaps is crucial for
enabling effective connections between health and social care services. This
approach facilitates safe and trustworthy data workflows between different
healthcare system providers. In this paper, we present a methodology for
extracting, transforming, and loading data through a semi-automated process
using a Common Semantic Standardized Data Model (CSSDM) to create personalized
healthcare knowledge graph (KG). The CSSDM is grounded in the formal ontology
of ISO 13940 ContSys and incorporates FHIR-based specifications to support
structural attributes for generating KGs. We propose that the CSSDM facilitates
data harmonization and linking, offering an alternative approach to
interoperability. This approach promotes a novel form of collaboration between
companies developing health information systems and cloud-enabled health
services. Consequently, it provides multiple stakeholders with access to
high-quality data and information sharing.; 70) Commonsense Reasoning-Aided Autonomous Vehicle Systems; Autonomous Vehicle (AV) systems have been developed with a strong reliance on
machine learning techniques. While machine learning approaches, such as deep
learning, are extremely effective at tasks that involve observation and
classification, they struggle when it comes to performing higher level
reasoning about situations on the road. This research involves incorporating
commonsense reasoning models that use image data to improve AV systems. This
will allow AV systems to perform more accurate reasoning while also making them
more adjustable, explainable, and ethical. This paper will discuss the findings
so far and motivate its direction going forward.; 71) Photon-assisted stochastic resonance in nanojunctions; We study stochastic resonance in molecular junctions driven by a
periodically-varying external field. This is done using the time-dependent
Landauer-B{\""u}ttiker formalism, which follows from exact analytical solutions
to the Kadanoff-Baym equations describing the molecular junction subject to an
arbitrary time-dependent bias. We focus on a double quantum dot nanojunction
and compare the effects of the temperature with the fluctuating bias in the
statically-driven case. We then consider the combined effect of AC-driving and
white noise fluctuations on the rectified current through the nanojunction, and
find a stochastic resonance effect, where at certain driving conditions the
bias fluctuations enhance the current signal. The study is then extended to
include the color noise in the applied bias, so that the combined effect of the
color noise correlation time and driving frequency on stochastic resonance is
investigated. We thereby demonstrate that photon-assisted transport can be
optimized by a suitably tuned environment.; 72) Approximate isometries of Hilbert spaces; We improve the Hyers-Ulam stability result for isometries of real Hilbert
spaces by removing the surjectivity assumption.; 73) Spike-and-Slab Posterior Sampling in High Dimensions; Posterior sampling with the spike-and-slab prior [MB88], a popular multimodal
distribution used to model uncertainty in variable selection, is considered the
theoretical gold standard method for Bayesian sparse linear regression [CPS09,
Roc18]. However, designing provable algorithms for performing this sampling
task is notoriously challenging. Existing posterior samplers for Bayesian
sparse variable selection tasks either require strong assumptions about the
signal-to-noise ratio (SNR) [YWJ16], only work when the measurement count grows
at least linearly in the dimension [MW24], or rely on heuristic approximations
to the posterior. We give the first provable algorithms for spike-and-slab
posterior sampling that apply for any SNR, and use a measurement count
sublinear in the problem dimension. Concretely, assume we are given a
measurement matrix $\mathbf{X} \in \mathbb{R}^{n\times d}$ and noisy
observations $\mathbf{y} = \mathbf{X}\mathbf{\theta}^\star + \mathbf{\xi}$ of a
signal $\mathbf{\theta}^\star$ drawn from a spike-and-slab prior $\pi$ with a
Gaussian diffuse density and expected sparsity k, where $\mathbf{\xi} \sim
\mathcal{N}(\mathbb{0}_n, \sigma^2\mathbf{I}_n)$. We give a polynomial-time
high-accuracy sampler for the posterior $\pi(\cdot \mid \mathbf{X},
\mathbf{y})$, for any SNR $\sigma^{-1}$ > 0, as long as $n \geq k^3 \cdot
\text{polylog}(d)$ and $X$ is drawn from a matrix ensemble satisfying the
restricted isometry property. We further give a sampler that runs in
near-linear time $\approx nd$ in the same setting, as long as $n \geq k^5 \cdot
\text{polylog}(d)$. To demonstrate the flexibility of our framework, we extend
our result to spike-and-slab posterior sampling with Laplace diffuse densities,
achieving similar guarantees when $\sigma = O(\frac{1}{k})$ is bounded.; 74) Yang-Lee Zeros of 2D Nearest-Neighbor Antiferromagnetic Ising Models: A
  Numerical Linked Cluster Expansion Study; We study Yang-Lee zeros in the thermodynamic limit of the 2D nearest-neighbor
antiferromagnetic Ising model on square and triangular lattices. We employ the
Numerical Linked Cluster Expansion (NLCE) equipped with Exact Enumeration (EE)
of the partition function to estimate the Laplacian of the free energy, which
is proportional to the zeros density. Using a modified NLCE, where the
expansion can be carried directly on the Yang-Lee zeros of the involved
clusters, we estimate the density of Yang-Lee zeros in the thermodynamic limit.
NLCE gives significantly more zeros than EE in the complex field plane
providing more insights on how the root curves look in the thermodynamic limit.
For the square lattice at $T \ll T_c$, the results suggest that two vertical
lines at $\pm h_c(T)$ in the complex field plane (i.e two concentric circles in
the complex fugacity plane) are the thermodynamic root curves. A similar
picture is expected for the triangular lattice for phase transitions at large
values of magnetic field while further study is needed for phase transitions at
smaller values of magnetic field. The convergence of the NLCE and (EE)
calculations of the partition function to the thermodynamic limit is studied in
both lattices and the temperature-field phase diagram is obtained from Yang-Lee
zeros using both methods. This NLCE-based approach will facilitate the study of
different types of phase transitions using Yang-Lee zeros in future research.; 75) RAPID: Retrieval-Augmented Parallel Inference Drafting for Text-Based
  Video Event Retrieval; Retrieving events from videos using text queries has become increasingly
challenging due to the rapid growth of multimedia content. Existing methods for
text-based video event retrieval often focus heavily on object-level
descriptions, overlooking the crucial role of contextual information. This
limitation is especially apparent when queries lack sufficient context, such as
missing location details or ambiguous background elements. To address these
challenges, we propose a novel system called RAPID (Retrieval-Augmented
Parallel Inference Drafting), which leverages advancements in Large Language
Models (LLMs) and prompt-based learning to semantically correct and enrich user
queries with relevant contextual information. These enriched queries are then
processed through parallel retrieval, followed by an evaluation step to select
the most relevant results based on their alignment with the original query.
Through extensive experiments on our custom-developed dataset, we demonstrate
that RAPID significantly outperforms traditional retrieval methods,
particularly for contextually incomplete queries. Our system was validated for
both speed and accuracy through participation in the Ho Chi Minh City AI
Challenge 2024, where it successfully retrieved events from over 300 hours of
video. Further evaluation comparing RAPID with the baseline proposed by the
competition organizers demonstrated its superior effectiveness, highlighting
the strength and robustness of our approach.; 76) Observational constraints on vector-like dark energy; The canonical cosmological model to explain the recent acceleration of the
universe relies on a cosmological constant, and most dynamical dark energy and
modified gravity model alternatives are based on scalar fields. Still, further
alternatives are possible. One of these involves vector fields: under certain
conditions, they can lead to accelerating universes while preserving
large-scale homogeneity and isotropy. We report quantitative observational
constraints on a model previously proposed by Armend\'ariz-Pic\'on and known as
the cosmic triad. We consider several subclasses of the model, which
generically is a parametric extension of the canonical $\Lambda$CDM model, as
well as two possible choices of the triad's potential. Our analysis shows that
any deviations from this limit are constrained to be small. In particular the
preferred present-day values of the matter density and the dark energy equation
of state are fully consistent with those obtained, for the same datasets, in
flat $\Lambda$CDM and $w_0$CDM. The constraints mildly depend on the priors on
the dark energy equation of state, specifically on whether phantom values
thereof are allowed, while the choice of potential does not play a significant
role since any such potential is constrained to be relatively flat.; 77) Byzantine Consensus in the Random Asynchronous Model; We propose a novel relaxation of the classic asynchronous network model,
called the random asynchronous model, which removes adversarial message
scheduling while preserving unbounded message delays and Byzantine faults.
Instead of an adversary dictating message order, delivery follows a random
schedule. We analyze Byzantine consensus at different resilience thresholds
($n=3f+1$, $n=2f+1$, and $n=f+2$) and show that our relaxation allows consensus
with probabilistic guarantees which are impossible in the standard asynchronous
model or even the partially synchronous model. We complement these protocols
with corresponding impossibility results, establishing the limits of consensus
in the random asynchronous model.; 78) Diverse Inference and Verification for Advanced Reasoning; Reasoning LLMs such as OpenAI o1, o3 and DeepSeek R1 have made significant
progress in mathematics and coding, yet find challenging advanced tasks such as
International Mathematical Olympiad (IMO) combinatorics problems, Abstraction
and Reasoning Corpus (ARC) puzzles, and Humanity's Last Exam (HLE) questions.
We use a diverse inference approach that combines multiple models and methods
at test time. We find that verifying mathematics and code problems, and
rejection sampling on other problems is simple and effective. We automatically
verify correctness of solutions to IMO problems by Lean, and ARC puzzles by
code, and find that best-of-N effectively answers HLE questions. Our approach
increases answer accuracy on IMO combinatorics problems from 33.3% to 77.8%,
accuracy on HLE questions from 8% to 37%, and solves 80% of ARC puzzles that
948 humans could not and 26.5% of ARC puzzles that o3 high compute does not.
Test-time simulations, reinforcement learning, and meta-learning with inference
feedback improve generalization by adapting agent graph representations and
varying prompts, code, and datasets. Our approach is reliable, robust, and
scalable, and in the spirit of reproducible research, we will make it publicly
available upon publication.; 79) Comparison of the detector response and calibration function of metallic
  microcalorimeters for X-ray photons and external electrons; Metallic microcalorimeters (MMCs) are cryogenic single-particle detectors
that rely on a calorimetric detection principle. Due to their excellent energy
resolution, close-to-ideal linear detector response, fast signal rise time and
the potential for \SI{100}{\%} quantum efficiency, MMCs outperform conventional
detectors by several orders of magnitude in resolution. These attributes make
them particularly interesting for a broad spectrum of applications, including a
next-generation neutrino mass experiment based on the measurement of the
tritium beta-decay spectrum, with an objective of achieving a sensitivity
surpassing that of the pioneering KATRIN experiment. However, although MMCs
have been used in measurements of photons and heavy ions with great success, no
information is currently available on the interaction between MMCs and external
light charged particles such as electrons. This work aims to provide such
missing information and to demonstrate that MMC-based detectors are suitable
for high-resolution spectroscopy of external electron sources. Particularly, we
present the first-ever measurements of external electrons using a metallic
microcalorimeter, comprehensively discuss the characteristics of the signal
shape and the calibration function and give a direct comparison between
well-defined conversion electron and X-ray photon signals from the same
$^{83}$Rb/$^{83m}$Kr source.; 80) 2-Coherent Internal Models of Homotopical Type Theory; The program of internal type theory seeks to develop the categorical model
theory of dependent type theory using the language of dependent type theory
itself. In the present work we study internal homotopical type theory by
relaxing the notion of a category with families (cwf) to that of a wild, or
precoherent higher cwf, and determine coherence conditions that suffice to
recover properties expected of models of dependent type theory. The result is a
definition of a split 2-coherent wild cwf, which admits as instances both the
syntax and the ""standard model"" given by a universe type. This allows us to
give a straightforward internalization of the notion of a 2-coherent reflection
of homotopical type theory in itself: namely as a 2-coherent wild cwf morphism
from the syntax to the standard model. Our theory also easily specializes to
give definitions of ""low-dimensional"" higher cwfs, and conjecturally includes
the container higher model as a further instance.; 81) Highly efficient exciton-exciton annihilation in single conjugated
  polymer chains; The number of excitons that conjugated polymers can support at any one time
underpins their optoelectronic performance in light emitting diodes and as
laser gain media, as it sets a natural limit on exciton density. Here we have
measured the time-resolved photon statistics of single chains of polyfluorene
to extract the absolute number of independent emitting sites present and its
time dependence. We find that after 100 ps each chain can only support 1 or 2
independent excitons, and that even at the earliest times this number rises
only to 4, suggesting a high degree of electronic coupling between chromophores
that facilitates efficient exciton-exciton annihilation. In circumstances where
a low density of low-energy sites is present, annihilation between them still
dominates. The results indicate that achieving high exciton densities in
conjugated polymers is difficult, and in applications where it is desirable new
strategies should be devised to control exciton-exciton annihilation.; 82) Einstein Constants and Smooth Topology; It was first shown in (Catanese-LeBrun 1997) that certain high-dimensional
smooth closed manifolds admit pairs of Einstein metrics with Ricci curvatures
of opposite sign. After reviewing subsequent progress that has been made on
this topic, we then prove various related results, with the ultimate goal of
stimulating further research on associated questions.; 83) Koel-TTS: Enhancing LLM based Speech Generation with Preference
  Alignment and Classifier Free Guidance; While autoregressive speech token generation models produce speech with
remarkable variety and naturalness, their inherent lack of controllability
often results in issues such as hallucinations and undesired vocalizations that
do not conform to conditioning inputs. We introduce Koel-TTS, a suite of
enhanced encoder-decoder Transformer TTS models that address these challenges
by incorporating preference alignment techniques guided by automatic speech
recognition and speaker verification models. Additionally, we incorporate
classifier-free guidance to further improve synthesis adherence to the
transcript and reference speaker audio. Our experiments demonstrate that these
optimizations significantly enhance target speaker similarity, intelligibility,
and naturalness of synthesized speech. Notably, Koel-TTS directly maps text and
context audio to acoustic tokens, and on the aforementioned metrics,
outperforms state-of-the-art TTS models, despite being trained on a
significantly smaller dataset. Audio samples and demos are available on our
website.; 84) Out-of-plane displacement of quantum color centers in monolayer h-BN; Color centers exhibiting deep-level states within the wide bandgap h-BN
monolayer possess substantial potential for quantum applications. Uncovering
precise geometric characteristics at the atomic scale is crucial for
understanding defect performance. In this study, first-principles calculations
were performed on the most extensively investigated CBVN and NBVN color centers
in h-BN, focusing on the out-of-plane displacement and their specific impacts
on electronic, vibrational, and emission properties. We demonstrate the
competition between the {\sigma}*-like antibonding state and the {\pi}-like
bonding state, which determines the out-of-plane displacement. The overall
effect of vibronic coupling on geometry is elucidated using a pseudo
Jahn-Teller model. Local vibrational analysis reveals a series of distinct
quasi-local phonon modes that could serve as fingerprints for experimental
identification of specific point defects. The critical effects of out-of-plane
displacement during the quantum emission process are carefully elucidated to
answer the distinct observations in experiments, and these revelations are
universal in quantum point defects in other layered materials.; 85) Comparison of near-field light intensities: plasmon nanofocusing vs
  localized plasmon resonance; The localized surface plasmon resonance of metallic nanostructures produces
strongly localized and enhanced near-field light, significantly contributing to
nanophotonics research and applications. Plasmon nanofocusing represents
another method for generating near-field light through the propagation and
condensation of plasmons on tapered plasmonic structures. In both methods, the
intensity of near-field light is a critical aspect for many applications. In
this study, we numerically inspect and compare the intensities of near-field
light generated by either localized plasmon resonance or plasmon nanofocusing.
To account for the light-induced changes in the optical properties of plasmonic
structures, which in turn influence the near-field light intensity, we couple
electromagnetic and thermal calculations to consider in a fully self-consistent
manner the effects of the incident light and the light-induced temperature rise
within the metal. A gold nanorod and a cone were adopted for exciting the
localized plasmon resonance and plasmon nanofocusing, respectively. We find
that plasmon nanofocusing generates approximately 1.5 times as strong
near-field light as localized plasmon resonance. Our research provides a
necessary foundation for generating near-field light, which is crucial for
advancing the applications of near-field optics.; 86) On Persistently Resetting Learning Integrators: A Framework For
  Model-Free Feedback Optimization; We study a novel class of algorithms for solving model-free feedback
optimization problems in dynamical systems. The key novelty is the introduction
of \emph{persistent resetting learning integrators} (PRLI), which are
integrators that are reset at the same frequency at which the plant is dithered
using exploratory signals for model-free optimization. It is shown that PRLIs
can serve as core mechanisms for real-time gradient estimation in online
feedback-optimization tasks where only cost function measurements are
available. In particular, unlike existing approaches based on approximation
theory, such as averaging or finite-differences, PRLIs can produce global
real-time gradient estimates of cost functions, with uniformly bounded
perturbations of arbitrarily small magnitude. In this sense, PRLIs function as
robust \emph{hybrid} ""Oracles"" suitable for interconnection with discrete-time
optimization algorithms that optimize the performance of continuous-time
dynamical plants in closed-loop operation. Compared to existing methods, PRLIs
yield \emph{global} stability properties for a broad class of cost functions,
surpassing the local or semi-global guarantees offered by traditional
approaches based on perturbation and approximation theory. The proposed
framework naturally bridges physical systems, modeled as continuous-time plants
where continuous exploration is essential, with digital algorithms, represented
as discrete-time optimization methods. The main results are illustrated using
different numerical examples.; 87) Adiabatic charge transport in extended SSH models; We explore the topological properties of extended SSH models, considering
four sub-lattices in a unit cell and second-nearest-neighbor intercell hopping
for SSH4 and SSH long-range (SSHLR) models, respectively. The additional tuning
parameters cause the SSH4 (SSHLR) model to host chiral symmetry protected two
(two and four) zero-energy modes producing a richer phase diagram that we
characterize by momentum space, periodic-bulk and open-bulk real space winding
numbers. We introduce time to study charge transport in the periodically driven
SSH4 and SSHLR models under the adiabatic limit. We remarkably find that the
whole parameter space turned topological for a certain choice of the remaining
parameters leading to always finite quantized value of pumped charge at the end
of a complete cycle. Considering time as another variable, we characterize
these new phases of the driven models by momentum space Chern number,
periodic-bulk and open-bulk real space Bott index. We also investigate the time
evolution of pumped charge for these models and connect it with the intriguing
windings of the mid-gap energy levels with time. Interestingly, the maximum
value of Chern number or Bott index for the driven models is more than that of
the winding number associated with the static model indicating the fact that
there exist more zero-energy modes during the full course of a driving cycle
compared to the underlying static models. We further extend our study to the
quantum metric where the fluctuations in the above quantity can identify the
presence of a topological phase boundary.; 88) Joint Delay-Doppler Estimation using OFDMA Payloads for Integrated
  Sensing and Communications; The use of future communication systems for sensing offers the potential for
a number of new applications. In this paper, we show that leveraging user data
payloads in multi-node Orthogonal Frequency Division Multiple Access (OFDMA)
networks for estimating target delay and Doppler-shift parameters can yield a
significant advantage in SNR and addressable bandwidth. However, gaps in the
frequency-time resources, reference signal boosting and amplitude modulation
schemes introduce challenges for estimation at the sensing receiver.
  In this work, we propose a joint delay and Doppler-shift model-based
estimator designed to address these challenges. Furthermore, we demonstrate
that incorporating knowledge of the device model into the estimation procedure
helps mitigate the effects of the non-ideal radar ambiguity function caused by
amplitude-modulated user payloads and sparse reference signals. Simulation
results demonstrate that the estimator achieves the theoretical lower bound on
estimation variance.; 89) High-accuracy evaluation of non-thermal magnetic states beyond spin-wave
  theory: applications to higher-energy states; We present an approximation scheme based on selective Hilbert space
truncation for characterizing non-thermal states of magnetic systems beyond
spin-wave theory. We study applications to states that are inaccessible through
linear spin-wave theory, such as multi-magnon states and higher-energy states.
Our approach is based on the existence of an exact representation of spin
operators in terms of finite-order polynomials of bosonic operators. It can be
applied to systems with and without a magnetically ordered ground state. The
approximation exactly diagonalizes the bosonic Hamiltonian restricted to
particular boson occupation subspaces, improving the conventional linear
spin-wave approach and exponentially reducing the computing time relative to
exact diagonalization schemes. As a test case, we apply the approach to a
prototypical one-dimensional model - an XXZ spin chain with an applied magnetic
field and antisymmetric exchange coupling. Here the antisymmetric coupling
introduces a continuous parameter to tune the system away from its exactly
solvable limit. We find excellent agreement between numerically exact
eigenstates and eigenvalues and those found via the approximation scheme. Our
approach applies not just to higher lying states but also to boson bound
states, which could make them more accessible to theoretical predictions for
comparison with experiment.; 90) Orthogonal projections of hypercubes; Projections of hypercubes have been applied to visualize high-dimensional
binary state spaces in various scientific fields. Conventional methods for
projecting hypercubes, however, face practical difficulties. Manual methods
require nontrivial adjustments of the projection basis, while
optimization-based algorithms limit the interpretability and reproducibility of
the resulting plots. These limitations motivate us to explore theoretically
analyzable projection algorithms such as principal component analysis (PCA).
Here, we investigate the mathematical properties of PCA-projected hypercubes.
Our numerical and analytical results show that PCA effectively captures
polarized distributions within the hypercubic state space. This property
enables the assessment of the asymptotic distribution of projected vertices and
error bounds, which characterize the performance of PCA in the projected space.
We demonstrate the application of PCA to visualize the hypercubic energy
landscapes of Ising spin systems. By adding projected hypercubic edges, these
visualizations reveal pathways of correlated spin flips. Our work provides a
better understanding of how PCA discovers hidden patterns in high-dimensional
binary data.; 91) VLM-E2E: Enhancing End-to-End Autonomous Driving with Multimodal Driver
  Attention Fusion; Human drivers adeptly navigate complex scenarios by utilizing rich
attentional semantics, but the current autonomous systems struggle to replicate
this ability, as they often lose critical semantic information when converting
2D observations into 3D space. In this sense, it hinders their effective
deployment in dynamic and complex environments. Leveraging the superior scene
understanding and reasoning abilities of Vision-Language Models (VLMs), we
propose VLM-E2E, a novel framework that uses the VLMs to enhance training by
providing attentional cues. Our method integrates textual representations into
Bird's-Eye-View (BEV) features for semantic supervision, which enables the
model to learn richer feature representations that explicitly capture the
driver's attentional semantics. By focusing on attentional semantics, VLM-E2E
better aligns with human-like driving behavior, which is critical for
navigating dynamic and complex environments. Furthermore, we introduce a
BEV-Text learnable weighted fusion strategy to address the issue of modality
importance imbalance in fusing multimodal information. This approach
dynamically balances the contributions of BEV and text features, ensuring that
the complementary information from visual and textual modality is effectively
utilized. By explicitly addressing the imbalance in multimodal fusion, our
method facilitates a more holistic and robust representation of driving
environments. We evaluate VLM-E2E on the nuScenes dataset and demonstrate its
superiority over state-of-the-art approaches, showcasing significant
improvements in performance.; 92) State preparation with parallel-sequential circuits; We introduce parallel-sequential (PS) circuits, a family of quantum circuit
layouts that interpolate between brickwall and sequential circuits, which
introduces control parameters governing the ratio of over the amount of
entanglement and the maximum correlation distance they can express. We provide
numerical evidence that PS circuits can efficiently prepare many-body ground
states in one dimension. On noisy devices, characterized through both idling
errors and two-qubit gate errors, we show that in a wide parameter regime, PS
circuits outperform brickwall, sequential, and log-depth circuits from [Malz,
Styliaris, Wei, Cirac, PRL 132, 040404 (2024)]. Additionally, we demonstrate
that properly chosen noisy random PS circuits suppress error proliferation and,
when employed as a variational ansatz, exhibit superior trainability.; 93) MuST: Multi-Head Skill Transformer for Long-Horizon Dexterous
  Manipulation with Skill Progress; Robot picking and packing tasks require dexterous manipulation skills, such
as rearranging objects to establish a good grasping pose, or placing and
pushing items to achieve tight packing. These tasks are challenging for robots
due to the complexity and variability of the required actions. To tackle the
difficulty of learning and executing long-horizon tasks, we propose a novel
framework called the Multi-Head Skill Transformer (MuST). This model is
designed to learn and sequentially chain together multiple motion primitives
(skills), enabling robots to perform complex sequences of actions effectively.
MuST introduces a ""progress value"" for each skill, guiding the robot on which
skill to execute next and ensuring smooth transitions between skills.
Additionally, our model is capable of expanding its skill set and managing
various sequences of sub-tasks efficiently. Extensive experiments in both
simulated and real-world environments demonstrate that MuST significantly
enhances the robot's ability to perform long-horizon dexterous manipulation
tasks.; 94) Asking for Help Enables Safety Guarantees Without Sacrificing
  Effectiveness; Most reinforcement learning algorithms with regret guarantees rely on a
critical assumption: that all errors are recoverable. Recent work by Plaut et
al. discarded this assumption and presented algorithms that avoid ""catastrophe""
(i.e., irreparable errors) by asking for help. However, they provided only
safety guarantees and did not consider reward maximization. We prove that any
algorithm that avoids catastrophe in their setting also guarantees high reward
(i.e., sublinear regret) in any Markov Decision Process (MDP), including MDPs
with irreversible costs. This constitutes the first no-regret guarantee for
general MDPs. More broadly, our result may be the first formal proof that it is
possible for an agent to obtain high reward while becoming self-sufficient in
an unknown, unbounded, and high-stakes environment without causing catastrophe
or requiring resets.; 95) Development of an Adaptive Sliding Mode Controller using Neural Networks
  for Trajectory Tracking of a Cylindrical Manipulator; Cylindrical manipulators are extensively used in industrial automation,
especially in emerging technologies like 3D printing, which represents a
significant future trend. However, controlling the trajectory of nonlinear
models with system uncertainties remains a critical challenge, often leading to
reduced accuracy and reliability. To address this, the study develops an
Adaptive Sliding Mode Controller (ASMC) integrated with Neural Networks (NNs)
to improve trajectory tracking for cylindrical manipulators. The ASMC leverages
the robustness of sliding mode control and the adaptability of neural networks
to handle uncertainties and dynamic variations effectively. Simulation results
validate that the proposed ASMC-NN achieves high trajectory tracking accuracy,
fast response time, and enhanced reliability, making it a promising solution
for applications in 3D printing and beyond.; 96) Polyhedra Encoding Transformers: Enhancing Diffusion MRI Analysis Beyond
  Voxel and Volumetric Embedding; Diffusion-weighted Magnetic Resonance Imaging (dMRI) is an essential tool in
neuroimaging. It is arguably the sole noninvasive technique for examining the
microstructural properties and structural connectivity of the brain. Recent
years have seen the emergence of machine learning and data-driven approaches
that enhance the speed, accuracy, and consistency of dMRI data analysis.
However, traditional deep learning models often fell short, as they typically
utilize pixel-level or volumetric patch-level embeddings similar to those used
in structural MRI, and do not account for the unique distribution of various
gradient encodings. In this paper, we propose a novel method called Polyhedra
Encoding Transformer (PE-Transformer) for dMRI, designed specifically to handle
spherical signals. Our approach involves projecting an icosahedral polygon onto
a unit sphere to resample signals from predetermined directions. These
resampled signals are then transformed into embeddings, which are processed by
a transformer encoder that incorporates orientational information reflective of
the icosahedral structure. Through experimental validation with various
gradient encoding protocols, our method demonstrates superior accuracy in
estimating multi-compartment models and Fiber Orientation Distributions (FOD),
outperforming both conventional CNN architectures and standard transformers.; 97) Decoding Interpretable Logic Rules from Neural Networks; As deep neural networks continue to excel across various domains, their
black-box nature has raised concerns about transparency and trust. In
particular, interpretability has become increasingly essential for applications
that demand high safety and knowledge rigor, such as drug discovery, autonomous
driving, and genomics. However, progress in understanding even the simplest
deep neural networks - such as fully connected networks - has been limited,
despite their role as foundational elements in state-of-the-art models like
ResNet and Transformer. In this paper, we address this challenge by introducing
NeuroLogic, a novel approach for decoding interpretable logic rules from neural
networks. NeuroLogic leverages neural activation patterns to capture the
model's critical decision-making processes, translating them into logical rules
represented by hidden predicates. Thanks to its flexible design in the
grounding phase, NeuroLogic can be adapted to a wide range of neural networks.
For simple fully connected neural networks, hidden predicates can be grounded
in certain split patterns of original input features to derive
decision-tree-like rules. For large, complex vision neural networks, NeuroLogic
grounds hidden predicates into high-level visual concepts that are
understandable to humans. Our empirical study demonstrates that NeuroLogic can
extract global and interpretable rules from state-of-the-art models such as
ResNet, a task at which existing work struggles. We believe NeuroLogic can help
pave the way for understanding the black-box nature of neural networks.; 98) Too Little, Too Late: Moderation of Misinformation around the
  Russo-Ukrainian Conflict; In this study, we examine the role of Twitter as a first line of defense
against misinformation by tracking the public engagement with, and the
platforms response to, 500 tweets concerning the RussoUkrainian conflict which
were identified as misinformation. Using a realtime sample of 543 475 of their
retweets, we find that users who geolocate themselves in the U.S. both produce
and consume the largest portion of misinformation, however accounts claiming to
be in Ukraine are the second largest source. At the time of writing, 84% of
these tweets were still available on the platform, especially those having an
anti-Russia narrative. For those that did receive some sanctions, the
retweeting rate has already stabilized, pointing to ineffectiveness of the
measures to stem their spread. These findings point to the need for a change in
the existing anti-misinformation system ecosystem. We propose several design
and research guidelines for its possible improvement.; 99) Symbol Resolution MatRs: Make it Fast and Observable with Stable Linking; Dynamic linking is the standard mechanism for using external dependencies
since it enables code reuse, streamlines software updates, and reduces
disk/network use. Dynamic linking waits until runtime to calculate an
application's relocation mapping, i.e., the mapping between each externally
referenced symbol in the application to the dependency that provides the
symbol. Unfortunately, it comes with two downsides. First, dynamic linking
limits the performance of current systems since it can take seconds to
calculate a relocation mapping for a large program. Second, dynamic linking
limits the dependency management of applications since it prevents a developer
from accurately observing a relocation mapping except at runtime.
  This paper makes the key insight that the benefits conventionally attributed
to dynamic linking: code reuse, streamlined software updates, and reduced
disk/network use are actually benefits of shared libraries. Thus, we present
stable linking, a new mechanism for using dependencies that uses shared
libraries to retain their benefits but eliminates the downsides of dynamic
linking. Stable linking separates a system's state into management times; when
the system can be modified, and epochs when it cannot. Stable linking
calculates each application's relocation mapping at the beginning of each
epoch, allows developers to inspect the relocation mapping during the epoch,
and reuses the mapping for subsequent executions in the epoch. We design and
build MatR, the first stable linker. We use MatR in three workloads and show
that it improves upon dynamic linking performance by a factor of 2.19 on
average. Additionally, we use the system in three vignettes, or case-studies,
that illustrate the system's improvements to dependency management.; 100) ""In order that"" -- a data driven study of symptoms and causes of
  obsolescence; The paper is an empirical case study of grammatical obsolescence in progress.
The main studied variable is the purpose subordinator in order that, which is
shown to be steadily decreasing in the frequency of use starting from the
beginning of the twentieth century. This work applies a data-driven approach
for the investigation and description of obsolescence, recently developed by
the Rudnicka (2019). The methodology combines philological analysis with
statistical methods used on data acquired from mega-corpora. Moving from the
description of possible symptoms of obsolescence to different causes for it,
the paper aims at presenting a comprehensive account of the studied phenomenon.
Interestingly, a very significant role in the decline of in order that can be
ascribed to the so-called higher-order processes, understood as processes
influencing the constructional level from above. Two kinds of higher-order
processes are shown to play an important role, namely i) an
externally-motivated higher-order process exemplified by the drastic
socio-cultural changes of the 19th and 20th centuries; ii) an
internally-motivated higher-order processes instantiated by the rise of the
to-infinitive (rise of infinite clauses).",0.0,1.0
2411.00726,applied,2411.00726-pos2-5,"Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema; This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection",2411.00726-pos1-5,"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",92,"['1', '3', '2', '4', '5', '6', '10', '9', '30', '14']","The best candidate paper to form a multidisciplinary research idea with the main paper on retinal vasculature and thickness in macular edema is the paper on 'Polynomial-Time Approximability of Constrained Reinforcement Learning'. This candidate introduces advanced computational methods that could be applied to analyze retinal imaging data more efficiently, thus enhancing the understanding of the complex relationships between retinal vasculature and thickness in the context of macular edema. The integration of machine learning and computational optimization with medical imaging presents a novel and useful approach to addressing retinal health issues.","1) Polynomial-Time Approximability of Constrained Reinforcement Learning; We study the computational complexity of approximating general constrained
Markov decision processes. Our primary contribution is the design of a
polynomial time $(0,\epsilon)$-additive bicriteria approximation algorithm for
finding optimal constrained policies across a broad class of recursively
computable constraints, including almost-sure, chance, expectation, and their
anytime variants. Matching lower bounds imply our approximation guarantees are
optimal so long as $P \neq NP$. The generality of our approach results in
answers to several long-standing open complexity questions in the constrained
reinforcement learning literature. Specifically, we are the first to prove
polynomial-time approximability for the following settings: policies under
chance constraints, deterministic policies under multiple expectation
constraints, policies under non-homogeneous constraints (i.e., constraints of
different types), and policies under constraints for continuous-state
processes.; 2) Ultrafast neuromorphic computing with nanophotonic optical parametric
  oscillators; Over the past decade, artificial intelligence (AI) has led to disruptive
advancements in fundamental sciences and everyday technologies. Among various
machine learning algorithms, deep neural networks have become instrumental in
revealing complex patterns in large datasets with key applications in computer
vision, natural language processing, and predictive analytics. On-chip photonic
neural networks offer a promising platform that leverage high bandwidths and
low propagation losses associated with optical signals to perform analog
computations for deep learning. However, nanophotonic circuits are yet to
achieve the required linear and nonlinear operations simultaneously in an
all-optical and ultrafast fashion. Here, we report an ultrafast nanophotonic
neuromorphic processor using an optical parametric oscillator (OPO) fabricated
on thin-film lithium niobate (TFLN). The input data is used to modulate the
optical pulses synchronously pumping the OPO. The consequent signal pulses
generated by the OPO are coupled to one another via the nonlinear delayed
dynamics of the OPO, thus forming the internal nodes of a deep recurrent neural
network. We use such a nonlinearly coupled OPO network for chaotic time series
prediction, nonlinear error correction in a noisy communication channel, as
well as noisy waveform classification and achieve accuracies exceeding 93% at
an operating clock rate of ~ 10 GHz. Our OPO network is capable of achieving
sub-nanosecond latencies, a timescale comparable to a single clock cycle in
state-of-the-art digital electronic processors. By circumventing the need for
optical-electronic-optical (OEO) conversions, our ultrafast nanophotonic neural
network paves the way for the next generation of compact all-optical
neuromorphic processors with ultralow latencies and high energy efficiencies.; 3) Safe Gradient Flow for Bilevel Optimization; Bilevel optimization is a key framework in hierarchical decision-making,
where one problem is embedded within the constraints of another. In this work,
we propose a control-theoretic approach to solving bilevel optimization
problems. Our method consists of two components: a gradient flow mechanism to
minimize the upper-level objective and a safety filter to enforce the
constraints imposed by the lower-level problem. Together, these components form
a safe gradient flow that solves the bilevel problem in a single loop. To
improve scalability with respect to the lower-level problem's dimensions, we
introduce a relaxed formulation and design a compact variant of the safe
gradient flow. This variant minimizes the upper-level objective while ensuring
the lower-level decision variable remains within a user-defined suboptimality.
Using Lyapunov analysis, we establish convergence guarantees for the dynamics,
proving that they converge to a neighborhood of the optimal solution. Numerical
experiments further validate the effectiveness of the proposed approaches. Our
contributions provide both theoretical insights and practical tools for
efficiently solving bilevel optimization problems.; 4) Superconductivity near an Ising nematic quantum critical point in two
  dimensions; Near a two-dimensional Ising-type nematic quantum critical point, the quantum
fluctuations of the nematic order parameter are coupled to the electrons,
leading to non-Fermi liquid behavior and unconventional superconductivity. The
interplay between these two effects has been extensively studied through the
Eliashberg equations for the superconducting gap. However, previous studies
often rely on various approximations that may introduce uncertainties in the
results. Here, we revisit this problem without these approximations and examine
how their removal changes the outcomes. We numerically solve four
self-consistent Eliashberg integral equations of the mass renormalization
$A_{1}(p)$, the chemical potential renormalization $A_{2}(p)$, the pairing
function $\Phi(p)$, and the nematic self-energy (polarization) function
$\Pi(q)$ using the iteration method. Our calculations retain the explicit
non-linearity and the full momentum dependence of these equations. We find that
discarding some commonly used approximations allows for a more accurate
determination of the superconducting gap $\Delta = \Phi/A_{1}$ and the critical
temperature $T_{c}$. The Eliashberg equations have two different convergent gap
solutions: an extended $s$-wave gap and a $d_{x^{2}-y^{2}}$-wave gap. The
latter is fragile, whereas the former is robust against small perturbations.; 5) ISAC MIMO Systems with OTFS Waveforms and Virtual Arrays; A novel Integrated Sensing-Communication (ISAC) system is proposed that can
accommodate high mobility scenarios while making efficient use of bandwidth for
both communication and sensing. The system comprises a monostatic
multiple-input multiple-output (MIMO) radar that transmits orthogonal time
frequency space (OTFS) waveforms. Bandwidth efficiency is achieved by making
Doppler-delay (DD) domain bins available for shared use by the transmit
antennas. For maximum communication rate, all DD-domain bins are used as
shared, but in this case, the target resolution is limited by the aperture of
the receive array. A low-complexity method is proposed for obtaining coarse
estimates of the radar targets parameters in that case. A novel approach is
also proposed to construct a virtual array (VA) for achieving a target
resolution higher than that allowed by the receive array. The VA is formed by
enforcing zeros on certain time-frequency (TF) domain bins, thereby creating
private bins assigned to specific transmit antennas. The TF signals received on
these private bins are orthogonal, enabling the synthesis of a VA. When
combined with coarse target estimates, this approach provides high-accuracy
target estimation. To preserve DD-domain information, the introduction of
private bins requires reducing the number of DD-domain symbols, resulting in a
trade-off between communication rate and sensing performance. However, even a
small number of private bins is sufficient to achieve significant sensing gains
with minimal communication rate loss. The proposed system is robust to Doppler
frequency shifts that arise in high mobility scenarios.; 6) Tollmien-Schlichting waves near neutral stable curve; In this paper, we study the linear stability of boundary layer flows over a
flat plate. Tollmien, Schlichting, Lin et al. found that there exists a neutral
curve, which consists of two branches: lower branch $\alpha_{low}(Re)$ and
upper branch $\alpha_{up}(Re)$. Here, $\alpha$ is the wave number and $Re$ is
the Reynolds number. For any $\alpha\in(\alpha_{low},\alpha_{up})$, there exist
unstable modes known as Tollmien-Schlichting (T-S) waves to the linearized
Navier-Stokes system. These waves play a key role during the early stage of
boundary layer transition. In a breakthrough work (Duke math Jour, 165(2016)),
Grenier, Guo, and Nguyen provided a rigorous construction of the unstable T-S
waves. In this paper, we confirm the existence of the neutral stable curve. To
achieve this, we develop a more delicate method for solving the Orr-Sommerfeld
equation by borrowing some ideas from the triple-deck theory. This approach
allows us to construct the T-S waves in a neighborhood of the neutral curve.; 7) Low-dispersive phase-modulated rapid scanning interferometry; Time-domain interferometry is an important principle in Fourier transform
(FT) and nonlinear femto- to attosecond spectroscopy. To optimize the
resolution and sensitivity of this approach, various interferometer
stabilization schemes have been developed. Among them, acousto-optical phase
modulation (AOPM) of the interferometer arms combined with phase-synchronous
lock-in detection has proven as a particular sensitive technique. However, the
acousto-optical modulators (AOMs), required for this technique, introduce
several disadvantages. Here, we demonstrate an alternative phase modulation
scheme which omits AOMs, termed PM scheme here. As a benchmark, we directly
compare the performance between the PM and the AOPM scheme in a linear FT
spectroscopy experiment and find comparable sensitivity in both approaches.; 8) A threshold for Poisson behavior of non-stationary product measures; Let $\gamma_{n}= O (\log^{-c}n)$ and let $\nu$ be the infinite product
measure whose $n$-th marginal is Bernoulli$(1/2+\gamma_{n})$. We show that
$c=1/2$ is the threshold, above which $\nu$-almost every point is simply
Poisson generic in the sense of Peres-Weiss, and below which this can fail.
This provides a range in which $\nu$ is singular with respect to the uniform
product measure, but $\nu$-almost every point is simply Poisson generic.; 9) Transparent NLP: Using RAG and LLM Alignment for Privacy Q&A; The transparency principle of the General Data Protection Regulation (GDPR)
requires data processing information to be clear, precise, and accessible.
While language models show promise in this context, their probabilistic nature
complicates truthfulness and comprehensibility.
  This paper examines state-of-the-art Retrieval Augmented Generation (RAG)
systems enhanced with alignment techniques to fulfill GDPR obligations. We
evaluate RAG systems incorporating an alignment module like Rewindable
Auto-regressive Inference (RAIN) and our proposed multidimensional extension,
MultiRAIN, using a Privacy Q&A dataset. Responses are optimized for preciseness
and comprehensibility and are assessed through 21 metrics, including
deterministic and large language model-based evaluations.
  Our results show that RAG systems with an alignment module outperform
baseline RAG systems on most metrics, though none fully match human answers.
Principal component analysis of the results reveals complex interactions
between metrics, highlighting the need to refine metrics. This study provides a
foundation for integrating advanced natural language processing systems into
legal compliance frameworks.; 10) Extracting Symbolic Sequences from Visual Representations via
  Self-Supervised Learning; This paper explores the potential of abstracting complex visual information
into discrete, structured symbolic sequences using self-supervised learning
(SSL). Inspired by how language abstracts and organizes information to enable
better reasoning and generalization, we propose a novel approach for generating
symbolic representations from visual data. To learn these sequences, we extend
the DINO framework to handle visual and symbolic information. Initial
experiments suggest that the generated symbolic sequences capture a meaningful
level of abstraction, though further refinement is required. An advantage of
our method is its interpretability: the sequences are produced by a decoder
transformer using cross-attention, allowing attention maps to be linked to
specific symbols and offering insight into how these representations correspond
to image regions. This approach lays the foundation for creating interpretable
symbolic representations with potential applications in high-level scene
understanding.; 11) Meshless Super-Resolution of Scattered Data via constrained RBFs and
  KNN-Driven Densification; We propose a novel meshless method to achieve super-resolution from scattered
data obtained from sparse, randomly-positioned sensors such as the particle
tracers of particle tracking velocimetry. The method combines K-Nearest
Neighbor Particle Tracking Velocimetry (KNN-PTV, Tirelli et al. 2023) with
meshless Proper Orthogonal Decomposition (meshless POD, Tirelli et al. 2025)
and constrained Radial Basis Function regression (c-RBFs, Sperotto et al.
2022). The main idea is to use KNN-PTV to enhance the spatial resolution of
flow fields by blending data from \textit{locally similar} flow regions
available in the time series. This \textit{similarity} is assessed in terms of
statistical coherency with leading features, identified by meshless POD
directly on the scattered data without the need to first interpolate onto a
grid, but instead relying on RBFs to compute all the relevant inner products.
Lastly, the proposed approach uses the c-RBF on the denser scattered
distributions to derive an analytical representation of the flow fields that
incorporates physical constraints. This combination is meshless because it does
not require the definition of a grid at any step of the calculation, thus
providing flexibility in handling complex geometries. The algorithm is
validated on 3D measurements of a jet flow in air. The assessment covers three
key aspects: statistics, spectra, and modal analysis. The proposed method is
evaluated against standard Particle Image Velocimetry, KNN-PTV, and c-RBFs. The
results demonstrate improved accuracy, with an average error on the order of
11%, compared to 13-14% for the other methods. Additionally, the proposed
method achieves an increase in the cutoff frequency of approximately 3-4/D,
compared to the values observed in the competing approaches. Furthermore, it
shows nearly half the errors in low-order reconstructions.; 12) Generating Causally Compliant Counterfactual Explanations using ASP; This research is focused on generating achievable counterfactual
explanations. Given a negative outcome computed by a machine learning model or
a decision system, the novel CoGS approach generates (i) a counterfactual
solution that represents a positive outcome and (ii) a path that will take us
from the negative outcome to the positive one, where each node in the path
represents a change in an attribute (feature) value. CoGS computes paths that
respect the causal constraints among features. Thus, the counterfactuals
computed by CoGS are realistic. CoGS utilizes rule-based machine learning
algorithms to model causal dependencies between features. The paper discusses
the current status of the research and the preliminary results obtained.; 13) PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning; Large language models demonstrate remarkable capabilities across various
domains, especially mathematics and logic reasoning. However, current
evaluations overlook physics-based reasoning - a complex task requiring physics
theorems and constraints. We present PhysReason, a 1,200-problem benchmark
comprising knowledge-based (25%) and reasoning-based (75%) problems, where the
latter are divided into three difficulty levels (easy, medium, hard). Notably,
problems require an average of 8.1 solution steps, with hard requiring 15.6,
reflecting the complexity of physics-based reasoning. We propose the Physics
Solution Auto Scoring Framework, incorporating efficient answer-level and
comprehensive step-level evaluations. Top-performing models like Deepseek-R1,
Gemini-2.0-Flash-Thinking, and o3-mini-high achieve less than 60% on
answer-level evaluation, with performance dropping from knowledge questions
(75.11%) to hard problems (31.95%). Through step-level evaluation, we
identified four key bottlenecks: Physics Theorem Application, Physics Process
Understanding, Calculation, and Physics Condition Analysis. These findings
position PhysReason as a novel and comprehensive benchmark for evaluating
physics-based reasoning capabilities in large language models. Our code and
data will be published at https:/dxzxy12138.github.io/PhysReason.; 14) Evaluating the Efficacy and Safety of Stereotactic Arrhythmia
  Radioablation in Ventricular Tachycardia: A Comprehensive Systematic Review
  and Meta-Analysis; Purpose: Stereotactic arrhythmia radioablation (STAR) has emerged as a
promising non-invasive treatment for refractory ventricular tachycardia (VT),
offering a novel alternative for patients who are poor candidates for catheter
ablation. This systematic review and meta-analysis evaluates the safety,
efficacy, and technical aspects of STAR across preclinical studies, case
reports, case series, and clinical trials. Methods and Materials: A systematic
review identified 80 studies published between 2015 and 2024, including 12
preclinical studies, 47 case reports, 15 case series, and 6 clinical trials.
Data on patient demographics, treatment parameters, and clinical outcomes were
extracted. Meta-analyses were performed for pooled mortality rates, VT burden
reduction, and acute toxicities, with subgroup analyses exploring
cardiomyopathy type, age, left ventricular ejection fraction (LVEF), and
treatment modality. Results: The pooled 6- and 12-month mortality rates were
16% (95% CI: 11-21%) and 32% (95% CI: 26-39%), respectively. VT burden
reduction at 6 months was 75% (95% CI: 73-77%), with significant heterogeneity
(I^2 = 98.8%). Grade 3+ acute toxicities were observed in 7% (95% CI: 4-11%),
with pneumonitis being the most common. Subgroup analyses showed comparable
outcomes between LINAC- and CyberKnife-based treatments, with minor differences
based on patient characteristics and cardiomyopathy type. Conclusions: STAR
demonstrates significant potential in reducing VT burden and improving patient
outcomes. While favorable acute safety profiles and efficacy support clinical
adoption, variability in treatment protocols underscores the need for
standardized practices. Future studies should aim to optimize patient
selection, establish robust dosimetric standards, and evaluate long-term
safety.; 15) IoT-enabled Drowsiness Driver Safety Alert System with Real-Time
  Monitoring Using Integrated Sensors Technology; Significant losses in terms of life and property occur from road traffic
accidents, which are often caused by drunk and drowsy drivers. Reducing
accidents requires effective detection of alcohol impairment and drowsiness as
well as real-time driver monitoring. This paper aims to create an Internet of
Things (IoT)--enabled Drowsiness Driver Safety Alert System with Real-Time
Monitoring Using Integrated Sensors Technology. The system features an alcohol
sensor and an IR sensor for detecting alcohol presence and monitoring driver
eye movements, respectively. Upon detecting alcohol, alarms and warning lights
are activated, the vehicle speed is progressively reduced, and the motor stops
within ten to fifteen seconds if the alcohol presence persists. The IR sensor
monitors prolonged eye closure, triggering alerts, or automatic vehicle
stoppage to prevent accidents caused by drowsiness. Data from the IR sensor is
transmitted to a mobile phone via Bluetooth for real-time monitoring and
alerts. By identifying driver alcoholism and drowsiness, this system seeks to
reduce accidents and save lives by providing safer transportation.; 16) A Framework to Develop and Validate RL-Based Obstacle-Aware UAV
  Positioning Algorithms; Unmanned Aerial Vehicles (UAVs) are increasingly being utilized to enhance
the Quality of Service (QoS) in wireless networks due to their flexibility and
cost-effectiveness. However, optimizing UAV placement in dynamic and
obstacle-prone environments remains a research challenge. Reinforcement
Learning (RL) has proven to be an effective approach that offers adaptability
and robustness in such environments.
  This paper introduces RLpos-3, a novel framework that integrates standard RL
techniques and existing libraries with Network Simulator 3 (ns-3) to facilitate
the development and evaluation of UAV positioning algorithms. RLpos-3 serves as
a complementary tool for researchers, enabling the implementation, analysis,
and benchmarking of UAV positioning strategies across different environmental
settings while ensuring user traffic demands are met. To validate its
effectiveness, we present a use case demonstrating the performance of RLpos-3
in optimizing UAV placement under realistic conditions.; 17) The Kodaira Embedding Theorem; Chow's Theorem and GAGA are renowned results demonstrating the algebraic
nature of projective manifolds and, more broadly, projective analytic
varieties. However, determining if a particular manifold is projective is not,
generally, a simple task. The Kodaira Embedding Theorem provides an intrinsic
characterization of projective varieties in terms of line bundles; in
particular, it states that a manifold is projective if and only if it admits a
positive line bundle. We prove only the 'if' implication in this paper, giving
a sufficient condition for a manifold bundle to be embedded in projective
space. Along the way, we prove several other interesting results. Of particular
note is the Kodaira-Nakano Vanishing Theorem, a crucial tool for eliminating
higher cohomology of complex manifolds, as well as Lemmas 6.2 and 6.1, which
provide important relationships between divisors, line bundles, and blowups.
Although this treatment is relatively self-contained, we omit a rigorous
development of Hodge theory, some basic complex analysis results, and some
theorems regarding Cech cohomology (including Leray's Theorem).; 18) Learning Choas In A Linear Way; Learning long-term behaviors in chaotic dynamical systems, such as turbulent
flows and climate modelling, is challenging due to their inherent instability
and unpredictability. These systems exhibit positive Lyapunov exponents, which
significantly hinder accurate long-term forecasting. As a result, understanding
long-term statistical behavior is far more valuable than focusing on short-term
accuracy. While autoregressive deep sequence models have been applied to
capture long-term behavior, they often lead to exponentially increasing errors
in learned dynamics. To address this, we shift the focus from simple prediction
errors to preserving an invariant measure in dissipative chaotic systems. These
systems have attractors, where trajectories settle, and the invariant measure
is the probability distribution on attractors that remains unchanged under
dynamics. Existing methods generate long trajectories of dissipative chaotic
systems by aligning invariant measures, but it is not always possible to obtain
invariant measures for arbitrary datasets. We propose the Poincare Flow Neural
Network (PFNN), a novel operator learning framework designed to capture
behaviors of chaotic systems without any explicit knowledge of the invariant
measure. PFNN employs an auto-encoder to map the chaotic system to a
finite-dimensional feature space, effectively linearizing the chaotic
evolution. It then learns the linear evolution operators to match the physical
dynamics by addressing two critical properties in dissipative chaotic systems:
(1) contraction, the system's convergence toward its attractors, and (2)
measure invariance, trajectories on the attractors following a probability
distribution invariant to the dynamics. Our experiments on a variety of chaotic
systems demonstrate that PFNN has more accurate predictions and physical
statistics compared to competitive baselines.; 19) Two characterizations of Sheffer-Dunkl sequences; Sheffer polynomials can be characterized using different Stieltjes integrals.
These families of polynomials have been recently extended to the Dunkl context.
In this way some classical operators as the derivative operator or the
difference operator are replaced as analogous operators in the Dunkl universe.
In this paper we establish two Stieltjes integrals that help us to characterize
the Sheffer-Dunkl polynomials.; 20) Turbulence in protoplanetary disks: A systematic analysis of dust
  settling in 33 disks; The level of dust vertical settling and radial dust concentration in disks is
of critical importance for understanding the efficiency of planet formation. We
present the first uniform analysis of the vertical extent of millimeter dust
for a representative sample of 33disks. We used radiative transfer modeling of
archival high-angular-resolution (<=0.1"") ALMA dust observations of inclined
and ringed disks to estimate their vertical dust scale height, which was
compared to estimated gas scale heights to characterize the level of vertical
sedimentation. In all 23systems for which constraints could be obtained, we
find that the outer parts of the disks are vertically settled. 5disks allow for
the characterization of the dust scale height both within and outside
approximately half the dust disk radius, showing a lower limit on their dust
heights at smaller radii. This implies that the ratio between vertical
turbulence and the Stokes number, $\alpha_z/\St$, decreases radially in these
sources. For 21rings in 15disks, we also constrained the level of radial
concentration of the dust, finding that about half of the rings are compatible
with strong radial trapping. In most of these rings, vertical turbulence is
found to be comparable to or weaker than radial turbulence, which is
incompatible with the turbulence generated by the vertical shear instability at
these locations. We further used our dust settling constraints to estimate the
turbulence level under the assumption that the dust size is limited by
fragmentation, finding typical upper limits around
$\alpha_\text{frag}\leq10^{-3}$. In a few sources, we find that turbulence
cannot be the main source of accretion. In the context of pebble accretion, we
identify several disk regions that have upper limits on their dust
concentration that would allow core formation to proceed efficiently, even at
wide orbital distances outside of 50au.; 21) Deep Learning for Wound Tissue Segmentation: A Comprehensive Evaluation
  using A Novel Dataset; Deep learning (DL) techniques have emerged as promising solutions for medical
wound tissue segmentation. However, a notable limitation in this field is the
lack of publicly available labelled datasets and a standardised performance
evaluation of state-of-the-art DL models on such datasets. This study addresses
this gap by comprehensively evaluating various DL models for wound tissue
segmentation using a novel dataset. We have curated a dataset comprising 147
wound images exhibiting six tissue types: slough, granulation, maceration,
necrosis, bone, and tendon. The dataset was meticulously labelled for semantic
segmentation employing supervised machine learning techniques. Three distinct
labelling formats were developed -- full image, patch, and superpixel. Our
investigation encompassed a wide array of DL segmentation and classification
methodologies, ranging from conventional approaches like UNet, to generative
adversarial networks such as cGAN, and modified techniques like FPN+VGG16.
Also, we explored DL-based classification methods (e.g., ResNet50) and machine
learning-based classification leveraging DL features (e.g., AlexNet+RF). In
total, 82 wound tissue segmentation models were derived across the three
labelling formats. Our analysis yielded several notable findings, including
identifying optimal DL models for each labelling format based on weighted
average Dice or F1 scores. Notably, FPN+VGG16 emerged as the top-performing DL
model for wound tissue segmentation, achieving a dice score of 82.25%. This
study provides a valuable benchmark for evaluating wound image segmentation and
classification models, offering insights to inform future research and clinical
practice in wound care. The labelled dataset created in this study is available
at https://github.com/akabircs/WoundTissue.; 22) Physics-constrained DeepONet for Surrogate CFD models: a curved
  backward-facing step case; The Physics-Constrained DeepONet (PC-DeepONet), an architecture that
incorporates fundamental physics knowledge into the data-driven DeepONet model,
is presented in this study. This methodology is exemplified through surrogate
modeling of fluid dynamics over a curved backward-facing step, a benchmark
problem in computational fluid dynamics. The model was trained on computational
fluid dynamics data generated for a range of parameterized geometries. The
PC-DeepONet was able to learn the mapping from the parameters describing the
geometry to the velocity and pressure fields. While the DeepONet is solely
data-driven, the PC-DeepONet imposes the divergence constraint from the
continuity equation onto the network. The PC-DeepONet demonstrates higher
accuracy than the data-driven baseline, especially when trained on sparse data.
Both models attain convergence with a small dataset of 50 samples and require
only 50 iterations for convergence, highlighting the efficiency of neural
operators in learning the dynamics governed by partial differential equations.; 23) Dynamic Rank Adjustment in Diffusion Policies for Efficient and Flexible
  Training; Diffusion policies trained via offline behavioral cloning have recently
gained traction in robotic motion generation. While effective, these policies
typically require a large number of trainable parameters. This model size
affords powerful representations but also incurs high computational cost during
training. Ideally, it would be beneficial to dynamically adjust the trainable
portion as needed, balancing representational power with computational
efficiency. For example, while overparameterization enables diffusion policies
to capture complex robotic behaviors via offline behavioral cloning, the
increased computational demand makes online interactive imitation learning
impractical due to longer training time. To address this challenge, we present
a framework, called DRIFT, that uses the Singular Value Decomposition to enable
dynamic rank adjustment during diffusion policy training. We implement and
demonstrate the benefits of this framework in DRIFT-DAgger, an imitation
learning algorithm that can seamlessly slide between an offline bootstrapping
phase and an online interactive phase. We perform extensive experiments to
better understand the proposed framework, and demonstrate that DRIFT-DAgger
achieves improved sample efficiency and faster training with minimal impact on
model performance.; 24) Emergent Dynamical Ising Transition in Diffusive Sandpiles; Minimally stable site (MSS) clusters play a dominant role in shaping
avalanches in the self-organized critical (SOC) systems. The manipulation of
MSS clusters through local smoothings (diffusion) alter the MSS landscape,
suppressing rare avalanches and postponing them until they manifest as spanning
avalanches. By leveraging the Inverse Ising problem, we uncover a duality
between diffusive sandpiles and equilibrium statistical physics. Our analysis
reveals an emergent magnetic instability in the dual Ising model, coinciding
with the formation of spanning avalanches and marking a transition to a
correlated percolation regime. At this point, the MSS loop soups exhibit
fractal self-similarity and power-law distributions, while the effective
pairwise interactions in the dual system vanish, signaling a magnetic
transition characterized by abrupt changes in magnetization and spin
susceptibility. Crucially, we show that diffusion fundamentally reshapes
avalanche dynamics: the spatial anti-correlations of MSSs in standard SOC
systems transform into positive correlations when diffusion is introduced.
These findings bridge self-organized criticality, percolation theory, and
equilibrium phase transitions, shedding new light on emergent criticality and
large-scale correlations in non-equilibrium systems.; 25) Towards Decoding Developer Cognition in the Age of AI Assistants; Background: The increasing adoption of AI assistants in programming has led
to numerous studies exploring their benefits. While developers consistently
report significant productivity gains from these tools, empirical measurements
often show more modest improvements. While prior research has documented
self-reported experiences with AI-assisted programming tools, little to no work
has been done to understand their usage patterns and the actual cognitive load
imposed in practice. Objective: In this exploratory study, we aim to
investigate the role AI assistants play in developer productivity.
Specifically, we are interested in how developers' expertise levels influence
their AI usage patterns, and how these patterns impact their actual cognitive
load and productivity during development tasks. We also seek to better
understand how this relates to their perceived productivity. Method: We propose
a controlled observational study combining physiological measurements (EEG and
eye tracking) with interaction data to examine developers' use of AI-assisted
programming tools. We will recruit professional developers to complete
programming tasks both with and without AI assistance while measuring their
cognitive load and task completion time. Through pre- and post-task
questionnaires, we will collect data on perceived productivity and cognitive
load using NASA-TLX.; 26) Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference
  Alignment; Direct Preference Optimization (DPO) has become a prominent method for
aligning Large Language Models (LLMs) with human preferences. While DPO has
enabled significant progress in aligning English LLMs, multilingual preference
alignment is hampered by data scarcity. To address this, we propose a novel
approach that $\textit{captures}$ learned preferences from well-aligned English
models by implicit rewards and $\textit{transfers}$ them to other languages
through iterative training. Specifically, we derive an implicit reward model
from the logits of an English DPO-aligned model and its corresponding reference
model. This reward model is then leveraged to annotate preference relations in
cross-lingual instruction-following pairs, using English instructions to
evaluate multilingual responses. The annotated data is subsequently used for
multilingual DPO fine-tuning, facilitating preference knowledge transfer from
English to other languages. Fine-tuning Llama3 for two iterations resulted in a
12.72% average improvement in Win Rate and a 5.97% increase in Length Control
Win Rate across all training languages on the X-AlpacaEval leaderboard. Our
findings demonstrate that leveraging existing English-aligned models can enable
efficient and effective multilingual preference alignment, significantly
reducing the need for extensive multilingual preference data. The code is
available at https://github.com/ZNLP/Implicit-Cross-Lingual-Rewarding; 27) Zep: A Temporal Knowledge Graph Architecture for Agent Memory; We introduce Zep, a novel memory layer service for AI agents that outperforms
the current state-of-the-art system, MemGPT, in the Deep Memory Retrieval (DMR)
benchmark. Additionally, Zep excels in more comprehensive and challenging
evaluations than DMR that better reflect real-world enterprise use cases. While
existing retrieval-augmented generation (RAG) frameworks for large language
model (LLM)-based agents are limited to static document retrieval, enterprise
applications demand dynamic knowledge integration from diverse sources
including ongoing conversations and business data. Zep addresses this
fundamental limitation through its core component Graphiti -- a
temporally-aware knowledge graph engine that dynamically synthesizes both
unstructured conversational data and structured business data while maintaining
historical relationships. In the DMR benchmark, which the MemGPT team
established as their primary evaluation metric, Zep demonstrates superior
performance (94.8% vs 93.4%). Beyond DMR, Zep's capabilities are further
validated through the more challenging LongMemEval benchmark, which better
reflects enterprise use cases through complex temporal reasoning tasks. In this
evaluation, Zep achieves substantial results with accuracy improvements of up
to 18.5% while simultaneously reducing response latency by 90% compared to
baseline implementations. These results are particularly pronounced in
enterprise-critical tasks such as cross-session information synthesis and
long-term context maintenance, demonstrating Zep's effectiveness for deployment
in real-world applications.; 28) MDCrow: Automating Molecular Dynamics Workflows with Large Language
  Models; Molecular dynamics (MD) simulations are essential for understanding
biomolecular systems but remain challenging to automate. Recent advances in
large language models (LLM) have demonstrated success in automating complex
scientific tasks using LLM-based agents. In this paper, we introduce MDCrow, an
agentic LLM assistant capable of automating MD workflows. MDCrow uses
chain-of-thought over 40 expert-designed tools for handling and processing
files, setting up simulations, analyzing the simulation outputs, and retrieving
relevant information from literature and databases. We assess MDCrow's
performance across 25 tasks of varying required subtasks and difficulty, and we
evaluate the agent's robustness to both difficulty and prompt style.
\texttt{gpt-4o} is able to complete complex tasks with low variance, followed
closely by \texttt{llama3-405b}, a compelling open-source model. While prompt
style does not influence the best models' performance, it has significant
effects on smaller models.; 29) $\tau$-exceptional sequences for representations of quivers over local
  algebras; Let $k$ be an algebraically closed field. Let $R$ be a finite dimensional
commutative local $k$-algebra and let $Q$ be a quiver with no oriented cycles.
In this paper, we study (signed) $\tau$-exceptional sequences over the algebra
$\Lambda = RQ$, which is isomorphic to $R\otimes kQ$. We show there is a
bijection between the set of complete (signed) $\tau$-exceptional sequences in
$\text{mod }kQ$ and the set of complete (signed) $\tau$-exceptional sequences
in $\text{mod }\Lambda$. Moreover, we prove that every $\tau$-perpendicular
subcategory of $\text{mod }\Lambda$ is equivalent to the module category of
$R\otimes kQ'$, for some quiver $Q'$. As a consequence, we prove that the
$\tau$-cluster morphism categories of $kQ$ and $\Lambda$ are equivalent.; 30) Representation Learning to Advance Multi-institutional Studies with
  Electronic Health Record Data; The adoption of EHRs has expanded opportunities to leverage data-driven
algorithms in clinical care and research. A major bottleneck in effectively
conducting multi-institutional EHR studies is the data heterogeneity across
systems with numerous codes that either do not exist or represent different
clinical concepts across institutions. The need for data privacy further limits
the feasibility of including multi-institutional patient-level data required to
study similarities and differences across patient subgroups. To address these
challenges, we developed the GAME algorithm. Tested and validated across 7
institutions and 2 languages, GAME integrates data in several levels: (1) at
the institutional level with knowledge graphs to establish relationships
between codes and existing knowledge sources, providing the medical context for
standard codes and their relationship to each other; (2) between institutions,
leveraging language models to determine the relationships between
institution-specific codes with established standard codes; and (3) quantifying
the strength of the relationships between codes using a graph attention
network. Jointly trained embeddings are created using transfer and federated
learning to preserve data privacy. In this study, we demonstrate the
applicability of GAME in selecting relevant features as inputs for AI-driven
algorithms in a range of conditions, e.g., heart failure, rheumatoid arthritis.
We then highlight the application of GAME harmonized multi-institutional EHR
data in a study of Alzheimer's disease outcomes and suicide risk among patients
with mental health disorders, without sharing patient-level data outside
individual institutions.; 31) FLAVARS: A Multimodal Foundational Language and Vision Alignment Model
  for Remote Sensing; Remote sensing imagery is dense with objects and contextual visual
information. There is a recent trend to combine paired satellite images and
text captions for pretraining performant encoders for downstream tasks.
However, while contrastive image-text methods like CLIP enable vision-language
alignment and zero-shot classification ability, vision-only downstream
performance tends to degrade compared to image-only pretraining, such as MAE.
In this paper, we propose FLAVARS, a pretraining method that combines the best
of both contrastive learning and masked modeling, along with geospatial
alignment via contrastive location encoding. We find that FLAVARS significantly
outperforms a baseline of SkyCLIP for vision-only tasks such as KNN
classification and semantic segmentation, +6\% mIOU on SpaceNet1, while
retaining the ability to perform zero-shot classification, unlike MAE
pretrained methods.; 32) Defect Phonon Renormalization during Nonradiative Multiphonon
  Transitions in Semiconductors; As a typical nonradiative multiphonon transition in semiconductors, carrier
capture at defects is critical to the performance of semiconductor devices. Its
transition rate is usually calculated using the equal-mode approximation, which
assumes that phonon modes and frequencies remain unchanged before and after the
transition. Using the carbon substitutional defect ($\text{C}_\text{N}$) in GaN
as a benchmark, here we demonstrate that the phonon renormalization can be
significant during defect relaxation, which causes errors as large as orders of
magnitude in the approximation. To address this issue, we consider (i)
Duschinsky matrix connecting the initial-state and final-state phonons, which
accounts for the changes in phonon modes and frequencies; and (ii) the
off-diagonal contributions in total transition matrix element, which
incorporates the cross terms of electron-phonon interactions between different
modes. With this improvement, the calculated transition rates show agreements
with experimental results within an order of magnitude. We believe the present
method makes one step forward for the accurate calculation of multiphonon
transition rate, especially in cases with large defect relaxations.; 33) Theoretical Characterization of Effect of Masks in Snapshot Compressive
  Imaging; Snapshot compressive imaging (SCI) refers to the recovery of
three-dimensional data cubes-such as videos or hyperspectral images-from their
two-dimensional projections, which are generated by a special encoding of the
data with a mask. SCI systems commonly use binary-valued masks that follow
certain physical constraints. Optimizing these masks subject to these
constraints is expected to improve system performance. However, prior
theoretical work on SCI systems focuses solely on independently and identically
distributed (i.i.d.) Gaussian masks, which do not permit such optimization. On
the other hand, existing practical mask optimizations rely on computationally
intensive joint optimizations that provide limited insight into the role of
masks and are expected to be sub-optimal due to the non-convexity and
complexity of the optimization. In this paper, we analytically characterize the
performance of SCI systems employing binary masks and leverage our analysis to
optimize hardware parameters. Our findings provide a comprehensive and
fundamental understanding of the role of binary masks - with both independent
and dependent elements - and their optimization. We also present simulation
results that confirm our theoretical findings and further illuminate different
aspects of mask design.; 34) Dusty disks as safe havens for terrestrial planets: Effect of the
  back-reaction of solid material on gas; Previous studies have shown that there is considerable variation in the
dust-to-gas density ratio in the vicinity of low-mass planets undergoing
growth. This can lead to a significant change in the planetary momentum exerted
by the gas and solid material. However, due to the low dust-to-gas mass ratio
of protoplanetary disks, the back-reaction of the solid material, is often
neglected. We study the effect of the back-reaction of solid material on the
torques felt by low-mass planets. We performed locally isothermal, 2D
hydrodynamic simulations of planet-disk interactions. Low-mass planets in the
range of 0.1-10MEarth accrete only solid material. Simulations were compared
with and without taking into account the back-reaction of the solid material on
the gas. The solid component was assumed to have a fixed Stokes number in the
range 0.01-10. In general, the inclusion of back-reaction results in a greater
number of models with positive torque values compared to models that neglect
back-reaction. It is clear, therefore, that the simulation of planetary growth
and migration via hydrodynamic modeling requires the inclusion of solid-gas
back-reaction. As a result of the back-reaction and accretion, a Mars-sized
planetary embryo will experience positive total torques from the disk
containing coupled solid components St<=0.01. Earth-mass planets also
experience positive total torques from the disk containing boulder-sized solid
components 2<=St<=5. The accretion of weakly coupled solid material tends to
increase the positive torques and decrease the negative torques. Our results
suggest that the combined effect of back-reaction and accretion is beneficial
to the formation of planetary systems by reducing the likelihood of a young
planet being engulfed by the central star.; 35) plmmr: an R package to fit penalized linear mixed models for genome-wide
  association data with complex correlation structure; Correlation among the observations in high-dimensional regression modeling
can be a major source of confounding. We present a new open-source package,
plmmr, to implement penalized linear mixed models in R. This R package
estimates correlation among observations in high-dimensional data and uses
those estimates to improve prediction with the best linear unbiased predictor.
The package uses memory-mapping so that genome-scale data can be analyzed on
ordinary machines even if the size of data exceeds RAM. We present here the
methods, workflow, and file-backing approach upon which plmmr is built, and we
demonstrate its computational capabilities with two examples from real GWAS
data.; 36) Construction of the Damped Ly$\alpha$ Absorber Catalog for DESI DR2
  Ly$\alpha$ BAO; We present the Damped Ly$\alpha$ Toolkit for automated detection and
characterization of Damped Ly$\alpha$ absorbers (DLA) in quasar spectra. Our
method uses quasar spectral templates with and without absorption from
intervening DLAs to reconstruct observed quasar forest regions. The
best-fitting model determines whether a DLA is present while estimating the
redshift and HI column density. With an optimized quality cut on detection
significance ($\Delta \chi_{r}^2>0.03$), the technique achieves an estimated
72% purity and 71% completeness when evaluated on simulated spectra with
S/N$>2$ that are free of broad absorption lines (BAL). We provide a catalog
containing candidate DLAs from the DLA Toolkit detected in DESI DR1 quasar
spectra, of which 21,719 were found in S/N$>2$ spectra with predicted
$\log_{10} (N_\texttt{HI}) > 20.3$ and detection significance $\Delta
\chi_{r}^2 >0.03$. We compare the Damped Ly$\alpha$ Toolkit to two alternative
DLA finders based on a convolutional neural network (CNN) and Gaussian process
(GP) models. We present a strategy for combining these three techniques to
produce a high-fidelity DLA catalog from DESI DR2 for the Ly$\alpha$ forest
baryon acoustic oscillation measurement. The combined catalog contains 41,152
candidate DLAs with $\log_{10} (N_\texttt{HI}) > 20.3$ from quasar spectra with
S/N$>2$. We estimate this sample to be approximately 76% pure and 71% complete
when BAL quasars are excluded.; 37) Multi-view Video-Pose Pretraining for Operating Room Surgical Activity
  Recognition; Understanding the workflow of surgical procedures in complex operating rooms
requires a deep understanding of the interactions between clinicians and their
environment. Surgical activity recognition (SAR) is a key computer vision task
that detects activities or phases from multi-view camera recordings. Existing
SAR models often fail to account for fine-grained clinician movements and
multi-view knowledge, or they require calibrated multi-view camera setups and
advanced point-cloud processing to obtain better results. In this work, we
propose a novel calibration-free multi-view multi-modal pretraining framework
called Multiview Pretraining for Video-Pose Surgical Activity Recognition
PreViPS, which aligns 2D pose and vision embeddings across camera views. Our
model follows CLIP-style dual-encoder architecture: one encoder processes
visual features, while the other encodes human pose embeddings. To handle the
continuous 2D human pose coordinates, we introduce a tokenized discrete
representation to convert the continuous 2D pose coordinates into discrete pose
embeddings, thereby enabling efficient integration within the dual-encoder
framework. To bridge the gap between these two modalities, we propose several
pretraining objectives using cross- and in-modality geometric constraints
within the embedding space and incorporating masked pose token prediction
strategy to enhance representation learning. Extensive experiments and ablation
studies demonstrate improvements over the strong baselines, while
data-efficiency experiments on two distinct operating room datasets further
highlight the effectiveness of our approach. We highlight the benefits of our
approach for surgical activity recognition in both multi-view and single-view
settings, showcasing its practical applicability in complex surgical
environments. Code will be made available at:
https://github.com/CAMMA-public/PreViPS.; 38) Experiments in the Linear Convex Order; This paper proposes two rankings of statistical experiments using the linear
convex order. These rankings hold in a broader set of scenarios where intuition
suggests that one experiment is more informative than another, and provide more
tractable characterizations than Blackwell order, which relies on the convex
order. We apply these rankings to compare statistical experiments in
binary-action decision problems and in decision problems that aggregate payoffs
over a collection of binary-action decision problems. Furthermore, these
rankings enable comparisons of statistical experiments in moral hazard problems
without requiring the validity of the first-order approach, thereby
complementing the results in Holmstr\""om (1979) and Kim (1995).; 39) Coarse-to-Fine Structure-Aware Artistic Style Transfer; Artistic style transfer aims to use a style image and a content image to
synthesize a target image that retains the same artistic expression as the
style image while preserving the basic content of the content image. Many
recently proposed style transfer methods have a common problem; that is, they
simply transfer the texture and color of the style image to the global
structure of the content image. As a result, the content image has a local
structure that is not similar to the local structure of the style image. In
this paper, we present an effective method that can be used to transfer style
patterns while fusing the local style structure into the local content
structure. In our method, dif-ferent levels of coarse stylized features are
first reconstructed at low resolution using a Coarse Network, in which style
color distribution is roughly transferred, and the content structure is
combined with the style structure. Then, the reconstructed features and the
content features are adopted to synthesize high-quality structure-aware
stylized images with high resolution using a Fine Network with three structural
selective fusion (SSF) modules. The effectiveness of our method is demonstrated
through the generation of appealing high-quality stylization results and a
com-parison with some state-of-the-art style transfer methods.; 40) Extensibility and denseness of periodic semigroup actions; We study periodic points and finitely supported invariant measures for
continuous semigroup actions. Introducing suitable notions of periodicity in
both topological and measure-theoretical contexts, we analyze the space of
invariant Borel probability measures associated with these actions. For
embeddable semigroups, we establish a direct relationship between the
extensibility of invariant measures to the free group on the semigroup and the
denseness of finitely supported invariant measures. Applying this framework to
shift actions on the full shift, we prove that finitely supported invariant
measures are dense for every left amenable semigroup that is residually a
finite group and for every finite-rank free semigroup.; 41) Making Sense of Data in the Wild: Data Analysis Automation at Scale; As the volume of publicly available data continues to grow, researchers face
the challenge of limited diversity in benchmarking machine learning tasks.
Although thousands of datasets are available in public repositories, the sheer
abundance often complicates the search for suitable data, leaving many valuable
datasets underexplored. This situation is further amplified by the fact that,
despite longstanding advocacy for improving data curation quality, current
solutions remain prohibitively time-consuming and resource-intensive. In this
paper, we propose a novel approach that combines intelligent agents with
retrieval augmented generation to automate data analysis, dataset curation and
indexing at scale. Our system leverages multiple agents to analyze raw,
unstructured data across public repositories, generating dataset reports and
interactive visual indexes that can be easily explored. We demonstrate that our
approach results in more detailed dataset descriptions, higher hit rates and
greater diversity in dataset retrieval tasks. Additionally, we show that the
dataset reports generated by our method can be leveraged by other machine
learning models to improve the performance on specific tasks, such as improving
the accuracy and realism of synthetic data generation. By streamlining the
process of transforming raw data into machine-learning-ready datasets, our
approach enables researchers to better utilize existing data resources.; 42) Cross Section Measurements of Large Angle Fragments Production in the
  Interaction of Carbon Ion Beams with Thin Targets; The fragmentation cross sections of carbon ion beams with kinetic energies of
$115 - 353~\text{MeV/u}$ impinging on thin targets of graphite (C),
polyvinyl-toluene (C$_9$H$_{10}$) and PMMA (C$_2$O$_5$H$_8$) have been measured
at 90$^{\text{o}}$ and 60$^{\text{o}}$ at the CNAO particle therapy center
(Pavia, Italy). The presented measurements are a complete reanalysis by the
FOOT collaboration of already published elemental cross section on composite
targets, in order to refine the analysis, improve the systematic uncertainties
and show the comparison with the FLUKA Monte Carlo code calculations. In this
work, the kinetic energy at production of measured fragments has been
completely redefined, together with the efficiencies computation. The new
analysis strategy has been successfully validated against the Monte Carlo cross
sections. Two detection arms were positioned at two different angles to perform
the measurement at 90$^{\text{o}}$ and 60$^{\text{o}}$. The fragment species
have been identified in charge (Z$_{id}$ = H) and mass (M$_{id}$ = $^1$H,
$^2$H, $^3$H) combining the information of the deposited energy in thin plastic
scintillators, of the deposited energy in a thick LYSO crystal and of the
fragments Time of Flight (ToF) measurement. The ToF was also used to compute
the fragments measured kinetic energy. The cross sections are presented as a
function of the fragments kinetic energy at production thanks to an unfolding
technique applied to the data.; 43) On Terwilliger $\mathbb{F}$-algebras of direct products of group
  divisible association schemes; The Terwilliger algebras of association schemes over an arbitrary field
$\mathbb{F}$ were briefly called the Terwilliger $\mathbb{F}$-algebras of
association schemes in [9]. In this paper, the Terwilliger
$\mathbb{F}$-algebras of direct products of group divisible association schemes
are studied. The centers, the semisimplicity, the Jacobson radicals and their
nilpotent indices, the Wedderburn-Artin decompositions of the Terwilliger
$\mathbb{F}$-algebras of direct products of group divisible association schemes
are obtained.; 44) Halilsoy and Chandrasekhar standing gravitational waves in the linear
  approximation; Halilsoy and Chandrasekhar cylindrical standing gravitational waves
correspond to two different classes of solutions to the vacuum Einstein
equations. Both families satisfy the definition of standing gravitational waves
proposed by Stephani, but only the latter class fulfills the stricter
definition introduced by Chandrasekhar. The aim of this research is to compare
both classes of solutions within the linear regime. We discover that the
linearized Halilsoy and Chandrasekhar standing waves are gravitational
analogues of two different types of electromagnetic polarization standing waves
- a phenomenon not previously discussed in the literature for exact solutions
to the Einstein equations; 45) Buffered Partially-Persistent External-Memory Search Trees; We present an optimal partially-persistent external-memory search tree with
amortized I/O bounds matching those achieved by the non-persistent
$B^{\varepsilon}$-tree by Brodal and Fagerberg [SODA 2003]. In a
partially-persistent data structure each update creates a new version of the
data structure, where all past versions can be queried, but only the current
version can be updated. All operations should be efficient with respect to the
size $N_v$ of the accessed version $v$. For any parameter $0<\varepsilon<1$,
our data structure supports insertions and deletions in amortized
$O\!\left(\frac{1}{\varepsilon B^{1-\varepsilon}}\log_B N_v\right)$ I/Os, where
$B$ is the external-memory block size. It also supports successor and range
reporting queries in amortized $O\!\left(\frac{1}{\varepsilon}\log_B
N_v+K/B\right)$ I/Os, where $K$ is the number of values reported. The space
usage of the data structure is linear in the total number of updates. We make
the standard and minimal assumption that the internal memory has size $M \geq
2B$. The previous state-of-the-art external-memory partially-persistent search
tree by Arge, Danner and Teh [JEA 2003] supports all operations in worst-case
$O\!\left(\log_B N_v+K/B\right)$ I/Os, matching the bounds achieved by the
classical B-tree by Bayer and McCreight [Acta Informatica 1972]. Our data
structure successfully combines buffering updates with partial persistence. The
I/O bounds can also be achieved in the worst-case sense, by slightly modifying
our data structure and under the requirement that the memory size $M =
\Omega\!\left(B^{1-\varepsilon}\log_2(\max_v N_v)\right)$. The worst-case
result slightly improves the memory requirement over the previous ephemeral
external-memory dictionary by Das, Iacono, and Nekrich (ISAAC 2022), who
achieved matching worst-case I/O bounds but required $M=\Omega\!\left(B\log_B
N\right)$.; 46) Quantifying Security Vulnerabilities: A Metric-Driven Security Analysis
  of Gaps in Current AI Standards; As AI systems integrate into critical infrastructure, security gaps in AI
compliance frameworks demand urgent attention. This paper audits and quantifies
security risks in three major AI governance standards: NIST AI RMF 1.0, UK's AI
and Data Protection Risk Toolkit, and the EU's ALTAI. Using a novel risk
assessment methodology, we develop four key metrics: Risk Severity Index (RSI),
Attack Potential Index (AVPI), Compliance-Security Gap Percentage (CSGP), and
Root Cause Vulnerability Score (RCVS). Our analysis identifies 136 concerns
across the frameworks, exposing significant gaps. NIST fails to address 69.23
percent of identified risks, ALTAI has the highest attack vector vulnerability
(AVPI = 0.51) and the ICO Toolkit has the largest compliance-security gap, with
80.00 percent of high-risk concerns remaining unresolved. Root cause analysis
highlights under-defined processes (ALTAI RCVS = 033) and weak implementation
guidance (NIST and ICO RCVS = 0.25) as critical weaknesses. These findings
emphasize the need for stronger, enforceable security controls in AI
compliance. We offer targeted recommendations to enhance security posture and
bridge the gap between compliance and real-world AI risks.; 47) Arbitrary-Threshold Fully Homomorphic Encryption with Lower Complexity; Threshold fully homomorphic encryption (ThFHE) enables multiple parties to
compute functions over their sensitive data without leaking data privacy. Most
of existing ThFHE schemes are restricted to full threshold and require the
participation of \textit{all} parties to output computing results. Compared
with these full-threshold schemes, arbitrary threshold (ATh)-FHE schemes are
robust to non-participants and can be a promising solution to many real-world
applications. However, existing AThFHE schemes are either inefficient to be
applied with a large number of parties $N$ and a large data size $K$, or
insufficient to tolerate all types of non-participants. In this paper, we
propose an AThFHE scheme to handle all types of non-participants with lower
complexity over existing schemes. At the core of our scheme is the reduction
from AThFHE construction to the design of a new primitive called
\textit{approximate secret sharing} (ApproxSS). Particularly, we formulate
ApproxSS and prove the correctness and security of AThFHE on top of
arbitrary-threshold (ATh)-ApproxSS's properties. Such a reduction reveals that
existing AThFHE schemes implicitly design ATh-ApproxSS following a similar idea
called ``noisy share''. Nonetheless, their ATh-ApproxSS design has high
complexity and become the performance bottleneck. By developing ATASSES, an
ATh-ApproxSS scheme based on a novel ``encrypted share'' idea, we reduce the
computation (resp. communication) complexity from $\mathcal{O}(N^2K)$ to
$\mathcal{O}(N^2+K)$ (resp. from $\mathcal{O}(NK)$ to $\mathcal{O}(N+K)$). We
not only theoretically prove the (approximate) correctness and security of
ATASSES, but also empirically evaluate its efficiency against existing
baselines. Particularly, when applying to a system with one thousand parties,
ATASSES achieves a speedup of $3.83\times$ -- $15.4\times$ over baselines.; 48) Rotational-hyperfine cooling of $^{205}$TlF in a cryogenic beam; The aim of CeNTREX (Cold Molecule Nuclear Time-Reversal Experiment) is to
search for time-reversal symmetry violation in the thallium nucleus, by
measuring the Schiff moment of $^{205}$Tl in the polar molecule thallium
fluoride (TlF). CeNTREX uses a cryogenic beam of TlF with a rotational
temperature of 6.3(2) K. This results in population spread over dozens of
rotational and hyperfine sublevels of TlF, while only a single level is useful
for the Schiff moment measurement. Here we present a protocol for cooling the
rotational and hyperfine degrees of freedom in the CeNTREX beam, transferring
the majority of the Boltzmann distribution into a single rotational and
hyperfine sublevel by using a single ultraviolet laser and a pair of microwave
beams. We achieve a factor of $20.1(4)$ gain in the population of the $J=0$,
$F=0$ hyperfine sublevel of the TlF ground state.; 49) Profile and neighbourhood complexity of graphs with excluded minors and
  tree-structured graphs; The \emph{$r$-neighbourhood complexity} of a graph $G$ is the function
counting, for a given integer $k$, the largest possible number, over all
vertex-subsets $A$ of size $k$, of subsets of $A$ realized as the intersection
between the $r$-neighbourhood of some vertex and $A$. A~refinement of this
notion is the \emph{$r$-profile complexity}, that counts the maximum number of
distinct distance-vectors from any vertex to the vertices of $A$, ignoring
distances larger than~$r$. Typically, in structured graph classes such as
graphs of bounded VC-dimension or chordal graphs, these functions are bounded,
leading to insights into their structural properties and efficient algorithms.
  We improve existing bounds on the $r$-profile complexity (and thus on the
$r$-neighbourhood complexity) for graphs in several structured graph classes.
We show that the $r$-profile complexity of graphs excluding $K_h$ as a minor is
in $O_h(r^{3h-3}k)$. For graphs of treewidth at most~$t$, we give a bound in
$O_t(r^{t+1}k)$, which is tight up to a function of~$t$ as a factor. These
bounds improve results of Joret and Rambaud and answer a question of their
paper [Combinatorica, 2024]. We also apply our methods to other classes of
bounded expansion such as graphs excluding a fixed complete graph as a
subdivision.
  For outerplanar graphs, we can improve our treewidth bound by a factor of $r$
and conjecture that a similar improvement holds for graphs with bounded simple
treewidth. For graphs of treelength at most~$\ell$, we give the upper bound of
$O(k(r^2(\ell+1)^k))$, which we improve to $O\left (k\cdot (r 2^k + r^2k^2)
\right)$ in the case of chordal graphs and $O(k^2r)$ for interval graphs.
  Our bounds also imply relations between the order, diameter and metric
dimension of graphs in these classes, improving results from [Beaudou et al.,
SIDMA 2017].; 50) An elementary algebraic proof of the fundamental theorem of algebra; We give a new proof of the fundamental theorem of algebra. It is entirely
elementary, focused on using long division to its fullest extent. Further, the
method quickly recovers a more general version of the theorem recently obtained
by Joseph Shipman.; 51) Deep Dynamic Probabilistic Canonical Correlation Analysis; This paper presents Deep Dynamic Probabilistic Canonical Correlation Analysis
(D2PCCA), a model that integrates deep learning with probabilistic modeling to
analyze nonlinear dynamical systems. Building on the probabilistic extensions
of Canonical Correlation Analysis (CCA), D2PCCA captures nonlinear latent
dynamics and supports enhancements such as KL annealing for improved
convergence and normalizing flows for a more flexible posterior approximation.
D2PCCA naturally extends to multiple observed variables, making it a versatile
tool for encoding prior knowledge about sequential datasets and providing a
probabilistic understanding of the system's dynamics. Experimental validation
on real financial datasets demonstrates the effectiveness of D2PCCA and its
extensions in capturing latent dynamics.; 52) A Hierarchical Shock Model of Ultra-High-Energy Cosmic Rays; We propose that a hierarchical shock model$\unicode{x2014}$including
supernova remnant shocks, galactic wind termination shocks, and accretion
shocks around cosmic filaments and galaxy clusters$\unicode{x2014}$can
naturally explain the cosmic ray spectrum from ~1 GeV up to ~200 EeV. While
this framework applies to the entire cosmic ray spectrum, in this work, we
focus on its implications for ultra-high-energy cosmic rays (UHECRs). We
perform a hydrodynamic cosmological simulation to investigate the power
processed at shocks around clusters and filaments. The downstream flux from
nearby shocks around the local filament accounts for the softer, lower-energy
extragalactic component around the ankle, and the upstream escaping flux from
nearby clusters accounts for the transition to a hard spectral component at the
highest energies. This interpretation is in agreement with UHECR observations.
We suggest that a combination of early-Universe galactic outflows, cosmic ray
streaming instabilities, and a small-scale turbulent dynamo can increase
magnetic fields enough to attain the required rigidities. Our simulation
suggests that the available volume-averaged power density of accretion shocks
exceeds the required UHECR luminosity density by three orders of magnitude. We
show that microgauss magnetic fields at these shocks could explain both the
origin of UHECRs and the as-yet unidentified source of the diffuse radio
synchrotron background below 10 GHz. The shock-accelerated electrons produce a
hard radio background without overproducing diffuse inverse Compton emission.
These results motivate further observational tests with upcoming facilities to
help distinguish accretion shocks from other UHECR sources.; 53) Short-distance contributions to Hadronic-light-by-light for the muon
  $g-2$; This talk discusses short-distance contributions to the hadronic
light-by-light part of the muon g-2 as in the Standard Model. A short
discussion about the theory prediction is followed by the status of our work.
The main new results since the previous chiral dynamics workshop is how to
calculate the higher order corrections to the Melnikov-Vainshtein region using
the operator product expansion. We work out fully the next order in the OPE,
$D=4$, including photon and gluon only operators.
  In addition we briefly discuss the ongoing work on gluonic corrections to the
next-order and nonperturbative estimates.; 54) Human-centered Geospatial Data Science; This entry provides an overview of Human-centered Geospatial Data Science,
highlighting the gaps it aims to bridge, its significance, and its key topics
and research. Geospatial Data Science, which derives geographic knowledge and
insights from large volumes of geospatial big data using advanced Geospatial
Artificial Intelligence (GeoAI), has been widely used to tackle a wide range of
geographic problems. However, it often overlooks the subjective human
experiences that fundamentally influence human-environment interactions, and
few strategies have been developed to ensure that these technologies follow
ethical guidelines and prioritize human values. Human-centered Geospatial Data
Science advocates for two primary focuses. First, it advances our understanding
of human-environment interactions by leveraging Geospatial Data Science to
measure and analyze human subjective experiences at place including emotion,
perception, cognition, and creativity. Second, it advocates for the development
of responsible and ethical Geospatial Data Science methods that protect
geoprivacy, enhance fairness and reduce bias, and improve the explainability
and transparency of geospatial technologies. With these two missions,
Human-centered Geospatial Data Sciences brings a fresh perspective to develop
and utilize geospatial technologies that positively impact society and benefit
human well-being and the humanities.; 55) The influence of motion features in temporal perception; This paper examines the role of manner-of-motion verbs in shaping subjective
temporal perception and emotional resonance. Through four complementary
studies, we explore how these verbs influence the conceptualization of time,
examining their use in literal and metaphorical (temporal) contexts. Our
findings reveal that faster verbs (e.g., fly, zoom) evoke dynamic and engaging
temporal experiences, often linked to positive emotions and greater agency. In
contrast, slower verbs (e.g., crawl, drag) convey passivity, monotony, and
negative emotions, reflecting tedious or constrained experiences of time. These
effects are amplified in metaphorical contexts, where manner verbs encode
emotional and experiential nuances that transcend their literal meanings. We
also find that participants prefer manner verbs over path verbs (e.g., go,
pass) in emotionally charged temporal contexts, as manner verbs capture the
experiential and emotional qualities of time more effectively. These findings
highlight the interplay between language, motion, and emotion in shaping
temporal perception, offering insights into how linguistic framing influences
subjective experiences of time.; 56) CAT-3DGS: A Context-Adaptive Triplane Approach to
  Rate-Distortion-Optimized 3DGS Compression; 3D Gaussian Splatting (3DGS) has recently emerged as a promising 3D
representation. Much research has been focused on reducing its storage
requirements and memory footprint. However, the needs to compress and transmit
the 3DGS representation to the remote side are overlooked. This new application
calls for rate-distortion-optimized 3DGS compression. How to quantize and
entropy encode sparse Gaussian primitives in the 3D space remains largely
unexplored. Few early attempts resort to the hyperprior framework from learned
image compression. But, they fail to utilize fully the inter and intra
correlation inherent in Gaussian primitives. Built on ScaffoldGS, this work,
termed CAT-3DGS, introduces a context-adaptive triplane approach to their
rate-distortion-optimized coding. It features multi-scale triplanes, oriented
according to the principal axes of Gaussian primitives in the 3D space, to
capture their inter correlation (i.e. spatial correlation) for spatial
autoregressive coding in the projected 2D planes. With these triplanes serving
as the hyperprior, we further perform channel-wise autoregressive coding to
leverage the intra correlation within each individual Gaussian primitive. Our
CAT-3DGS incorporates a view frequency-aware masking mechanism. It actively
skips from coding those Gaussian primitives that potentially have little impact
on the rendering quality. When trained end-to-end to strike a good
rate-distortion trade-off, our CAT-3DGS achieves the state-of-the-art
compression performance on the commonly used real-world datasets.; 57) Pixel-Wise Feature Selection for Perceptual Edge Detection without
  post-processing; Although deep convolutional neutral networks (CNNs) have significantly
enhanced performance in image edge detection (ED), current models remain highly
dependent on post-processing techniques such as non-maximum suppression (NMS),
and often fail to deliver satisfactory perceptual results, while the
performance will deteriorate significantly if the allowed error toleration
distance decreases. These limitations arise from the uniform fusion of features
across all pixels, regardless of their specific characteristics, such as the
distinction between textural and edge areas. If the features extracted by the
ED models are selected more meticulously and encompass greater diversity, the
resulting predictions are expected to be more accurate and perceptually
meaningful. Motivated by this observation, this paper proposes a novel feature
selection paradigm for deep networks that facilitates the differential
selection of features and can be seamlessly integrated into existing ED models.
By incorporating this additional structure, the performance of conventional ED
models is substantially enhanced without post-processing, while simultaneously
enhancing the perceptual quality of the predictions. Extensive experimental
evaluations validate the effectiveness of the proposed model.; 58) Intertwiners of representations of untwisted quantum affine algebras and
  Yangians revisited; We discuss applications of the $q$-characters to the computation of the
$R$-matrices. In particular, we describe the $R$-matrix acting in the tensor
square of the first fundamental representation of E$_8$ and in a number of
other cases, where the decomposition of the tensor squares with respect to
non-affine quantum algebra has non-trivial multiplicities. As an illustration,
we also recover $R$-matrices acting in the multiplicity free-case on the tensor
squares of the first fundamental representations of all other types of
untwisted quantum affine algebras. The answer is written in terms of projectors
related to the decomposition of the tensor squares with respect to non-affine
quantum algebras. Then we give explicit expressions for the $R$-matrices in
terms of matrix units with respect to a natural basis (except for the case of
E$_8$). We give similar formulas for the Yangian $R$-matrices.; 59) Ask in Any Modality: A Comprehensive Survey on Multimodal
  Retrieval-Augmented Generation; Large Language Models (LLMs) struggle with hallucinations and outdated
knowledge due to their reliance on static training data. Retrieval-Augmented
Generation (RAG) mitigates these issues by integrating external dynamic
information enhancing factual and updated grounding. Recent advances in
multimodal learning have led to the development of Multimodal RAG,
incorporating multiple modalities such as text, images, audio, and video to
enhance the generated outputs. However, cross-modal alignment and reasoning
introduce unique challenges to Multimodal RAG, distinguishing it from
traditional unimodal RAG. This survey offers a structured and comprehensive
analysis of Multimodal RAG systems, covering datasets, metrics, benchmarks,
evaluation, methodologies, and innovations in retrieval, fusion, augmentation,
and generation. We precisely review training strategies, robustness
enhancements, and loss functions, while also exploring the diverse Multimodal
RAG scenarios. Furthermore, we discuss open challenges and future research
directions to support advancements in this evolving field. This survey lays the
foundation for developing more capable and reliable AI systems that effectively
leverage multimodal dynamic external knowledge bases. Resources are available
at https://github.com/llm-lab-org/Multimodal-RAG-Survey.; 60) Would you mind being watched by machines? Privacy concerns in data
  mining; Data mining is not an invasion of privacy because access to data is only by
machines, not by people: this is the argument that is investigated here. The
current importance of this problem is developed in a case study of data mining
in the USA for counterterrorism and other surveillance purposes. After a
clarification of the relevant nature of privacy, it is argued that access by
machines cannot warrant the access to further information, since the analysis
will have to be made either by humans or by machines that understand. It
concludes that the current data mining violates the right to privacy and should
be subject to the standard legal constraints for access to private information
by people.; 61) Decision-Theoretic Approaches in Learning-Augmented Algorithms; In this work, we initiate the systemic study of decision-theoretic metrics in
the design and analysis of algorithms with machine-learned predictions. We
introduce approaches based on both deterministic measures such as
distance-based evaluation, that help us quantify how close the algorithm is to
an ideal solution, as well as stochastic measures that allow us to balance the
trade-off between the algorithm's performance and the risk associated with the
imperfect oracle. These approaches help us quantify the algorithmic performance
across the entire spectrum of prediction error, unlike several previous works
that focus on few, and often extreme values of the error. We apply these
techniques to two well-known problems from resource allocation and online
decision making, namely contract scheduling and 1-max search.; 62) SuperCap: Multi-resolution Superpixel-based Image Captioning; It has been a longstanding goal within image captioning to move beyond a
dependence on object detection. We investigate using superpixels coupled with
Vision Language Models (VLMs) to bridge the gap between detector-based
captioning architectures and those that solely pretrain on large datasets. Our
novel superpixel approach ensures that the model receives object-like features
whilst the use of VLMs provides our model with open set object understanding.
Furthermore, we extend our architecture to make use of multi-resolution inputs,
allowing our model to view images in different levels of detail, and use an
attention mechanism to determine which parts are most relevant to the caption.
We demonstrate our model's performance with multiple VLMs and through a range
of ablations detailing the impact of different architectural choices. Our full
model achieves a competitive CIDEr score of $136.9$ on the COCO Karpathy split.; 63) An Attempt to Unraveling Token Prediction Refinement and Identifying
  Essential Layers of Large Language Models; This research aims to unravel how large language models (LLMs) iteratively
refine token predictions (or, in a general sense, vector predictions). We
utilized a logit lens technique to analyze the model's token predictions
derived from intermediate representations. Specifically, we focused on how LLMs
access and use information from input contexts, and how positioning of relevant
information affects the model's token prediction refinement process. Our
findings for multi-document question answering task, by varying input context
lengths (the number of documents), using GPT-2, revealed that the number of
layers between the first layer that the model predicted next tokens correctly
and the later layers that the model finalized its correct predictions, as a
function of the position of relevant information (i.e., placing the relevant
one at the beginning, middle, or end of the input context), has a nearly
inverted U shape. We found that the gap between these two layers, on average,
diminishes when relevant information is positioned at the beginning or end of
the input context, suggesting that the model requires more refinements when
processing longer contexts with relevant information situated in the middle,
and highlighting which layers are essential for determining the correct output.
Our analysis provides insights about how token predictions are distributed
across different conditions, and establishes important connections to existing
hypotheses and previous findings in AI safety research and development.; 64) Goal-Oriented Middleware Filtering at Transport Layer Based on Value of
  Updates; This work explores employing the concept of goal-oriented (GO) semantic
communication for real-time monitoring and control. Generally, GO communication
advocates for the deep integration of application targets into the network
design. We consider CPS and IoT applications where sensors generate a
tremendous amount of network traffic toward monitors or controllers. Here, the
practical introduction of GO communication must address several challenges.
These include stringent timing requirements, challenging network setups, and
limited computing and communication capabilities of the devices involved.
Moreover, real-life CPS deployments often rely on heterogeneous communication
standards prompted by specific hardware. To address these issues, we introduce
a middleware design of a GO distributed Transport Layer (TL) framework for
control applications. It offers end-to-end performance improvements for diverse
setups and transmitting hardware. The proposed TL protocol evaluates the Value
of sampled state Updates (VoU) for the application goal. It decides whether to
admit or discard the corresponding packets, thus offloading the network. VoU
captures the contribution of utilizing the updates at the receiver into the
application's performance. We introduce a belief network and the augmentation
procedure used by the sensor to predict the evolution of the control process,
including possible delays and losses of status updates in the network. The
prediction is made either using a control model dynamics or a Long-Short Term
Memory neural network approach. We test the performance of the proposed TL in
the experimental framework using Industrial IoT Zolertia ReMote sensors. We
show that while existing approaches fail to deliver sufficient control
performance, our VoU-based TL scheme ensures stability and performs
$\sim$$60\%$ better than the naive GO TL we proposed in our previous work.; 65) Light drag in an Optomechanical system; Light dragging refers to the change in the path of light passing through a
moving medium. This effect enables accurate detection of very slow speeds of
light, which have prominent applications in state transfer, quantum gate
operations, and quantum memory implementations. Here, to the best of our
knowledge, we demonstrate the existence of the light-dragging effect in an
optomechanical system (OMS) for the first time. The origin of this key factor
arises from the nonlinear effects linked to optomechanical-induced transparency
(OMIT). Hence, we observe prominent effects in the group and refractive indices
profile spectra related to optomechanical parameters such as the decay rate of
the cavity field, the mirror's damping momentum rate, and mechanical frequency.
We find out that lateral light drag depends on the detuning by altering the
amplitude and direction of the translational velocity. This allowed us to
change the light's propagation through the optomechanical cavity from
superluminal to subluminal and vice versa by modifying the probe's detuning.
The ability to manipulate and control the light drag through an optomechanical
system might be useful in designing novel optical devices and systems with
enhanced performance.; 66) Structure factors and quantum geometry in multiband BCS superconductors; We consider multiband BCS superconductors that exhibit time-reversal symmetry
and uniform pairing, and analyze their dynamic density and spin structure
factors using linear-response theory within the mean-field BCS-BEC crossover
framework at zero temperature. Our results for the multi-orbital Hubbard model
satisfy the associated f-sum rules in several limits. In particular, in the
strong-coupling limit, they coincide with those of a weakly-interacting Bose
gas of Cooper pairs, where the low-energy collective Goldstone modes serve as
Bogoliubov phonons. We further reveal that the quantum-geometric origin of the
low-energy structure factors, along with related observables such as the
superfluid-weight tensor and the effective-mass tensor of Cooper pairs, can be
traced all the way back to the effective-mass theorem for Bloch bands in this
limit. As an illustration, we investigate the pyrochlore-Hubbard model
numerically and demonstrate that the Goldstone modes are the only relevant
collective degrees of freedom in the flat-band regime.; 67) Spectra of standing kink waves in loops and the effects of the lower
  solar atmosphere; Understanding the effects of the lower solar atmosphere on the spectrum of
standing kink oscillations of coronal loops, in both the decaying and decayless
regime, is essential for developing more advanced tools for coronal seismology.
We aim to reveal the effects of the chromosphere on the spatial profiles and
frequencies of the standing kink modes, create synthetic emission maps to
compare with observations, and study the results using spatial and temporal
coronal seismology techniques. We excited transverse oscillations in a 3D
straight flux tube using (a) a broadband footpoint driver, (b) a sinusoidal
velocity pulse, and (c) an off-centre Gaussian velocity pulse, using the PLUTO
code. The flux tube is gravitationally stratified, with footpoints embedded in
chromospheric plasma. Using the FoMo code, we created synthetic observations of
our data in the Fe IX 17.1 nm line and calculated the spectra with the
Auto-NUWT code. We also numerically solved the generalised eigenvalue system
for the 1D wave equation to determine the effects of the stratification on the
kink modes of our system. The synthetic observations of the loops perturbed by
the velocity pulses show a single dominant mode that our 1D analysis reveals to
be the third harmonic of the system. For the broadband driver, the synthetic
emission shows multiple frequency bands, associated with both the loop and the
driver. Finally, using seismological techniques, we highlight the possibility
of misidentifying the observed third, sixth, and ninth harmonics with the
first, second, and third harmonics of the coronal part of longer loops. Unless
more advanced techniques of spatial seismology are used with many data points
from observations along the entire loop length, this misidentification can
result in overestimating the mean magnetic field by a factor equal to the
period ratio of the fundamental over the third harmonic.; 68) Propagation of chaos and Razumikhin theorem for the nonlinear
  McKean-Vlasov SFDEs with common noise; As the limit equations of mean-field particle systems perturbed by common
environmental noise, the McKean-Vlasov stochastic differential equations with
common noise have received a lot of attention. Moreover, past dependence is an
unavoidable natural phenomenon for dynamic systems in life sciences, economics,
finance, automatic control, and other fields. Combining the two aspects above,
this paper delves into a class of nonlinear McKean-Vlasov stochastic functional
differential equations (MV-SFDEs) with common noise. The well-posedness of the
nonlinear MV-SFDEs with common noise is first demonstrated through the
application of the Banach fixed-point theorem. Secondly, the relationship
between the MV-SFDEs with common noise and the corresponding functional
particle systems is investigated. More precisely, the conditional propagation
of chaos with an explicit convergence rate and the stability equivalence are
studied. Furthermore, the exponential stability, an important long-time
behavior of the nonlinear MV-SFDEs with common noise, is derived. To this end,
the It\^o formula involved with state and measure is developed for the MV-SFDEs
with common noise. Using this formula, the Razumikhin theorem is proved,
providing an easy-to-implement criterion for the exponential stability. Lastly,
an example is provided to illustrate the result of the stability.; 69) Enhancing Disaster Resilience with UAV-Assisted Edge Computing: A
  Reinforcement Learning Approach to Managing Heterogeneous Edge Devices; Edge sensing and computing is rapidly becoming part of intelligent
infrastructure architecture leading to operational reliance on such systems in
disaster or emergency situations. In such scenarios there is a high chance of
power supply failure due to power grid issues, and communication system issues
due to base stations losing power or being damaged by the elements, e.g.,
flooding, wildfires etc. Mobile edge computing in the form of unmanned aerial
vehicles (UAVs) has been proposed to provide computation offloading from these
devices to conserve their battery, while the use of UAVs as relay network nodes
has also been investigated previously. This paper considers the use of UAVs
with further constraints on power and connectivity to prolong the life of the
network while also ensuring that the data is received from the edge nodes in a
timely manner. Reinforcement learning is used to investigate numerous scenarios
of various levels of power and communication failure. This approach is able to
identify the device most likely to fail in a given scenario, thus providing
priority guidance for maintenance personnel. The evacuations of a rural town
and urban downtown area are also simulated to demonstrate the effectiveness of
the approach at extending the life of the most critical edge devices.; 70) Rough estimates of solar system gravitomagnetic effects in
  post-Newtonian gravity; In order to describe properly the gravity interactions including the mass
currents, in the gravitomagnetism we construct four Maxwell type gravitational
equations which are shown to be analogs of the Maxwell equations in the
electromagnetism. Next, exploiting the Maxwell type gravitational equations, we
explicitly predict the mass magnetic fields for both the isolated system of the
spinning Moon orbiting the spinning Earth and that of the Sun and solar system
planets orbiting the spinning Sun, whose phenomenological values have not been
evaluated in the precedented Newtonian gravity formalisms. In the
gravitomagnetism we also phenomenologically investigate the mass magnetic
general relativity (GR) forces associated with the mass magnetic fields, to
find that they are extremely small but non-vanishing compared to the
corresponding mass electric Newtonian forces. Moreover, the directions of the
mass magnetic GR forces for the solar system planets except Venus and Uranus
are shown to be anti-parallel to those of their mass electric Newtonian forces.
Next we investigate the mass magnetic dipole moment related with the B-ring of
Saturn, to evaluate $\vec{m}_{M}(Ring)=-1.141\times 10^{4}~{\rm
m^{3}~sec^{-1}}~\hat{\omega}$ with $\hat{\omega}$ being the unit vector along
the axis direction of the spinning B-ring. The predicted value of
$\vec{m}_{M}(Ring)$ is shown to be directly related with the Cassini data on
the total mass of the rings of Saturn.; 71) Confidence intervals for functionals in constrained inverse problems via
  data-adaptive sampling-based calibration; We address functional uncertainty quantification for ill-posed inverse
problems where it is possible to evaluate a possibly rank-deficient forward
model, the observation noise distribution is known, and there are known
parameter constraints. We present four constraint-aware confidence intervals
extending the work of Batlle et al. (2023) by making the intervals both
computationally feasible and less conservative. Our approach first shrinks the
potentially unbounded constraint set compact in a data-adaptive way, obtains
samples of the relevant test statistic inside this set to estimate a quantile
function, and then uses these computed quantities to produce the intervals. Our
data-adaptive bounding approach is based on the approach by Berger and Boos
(1994), and involves defining a subset of the constraint set where the true
parameter exists with high probability. This probabilistic guarantee is then
incorporated into the final coverage guarantee in the form of an uncertainty
budget. We then propose custom sampling algorithms to efficiently sample from
this subset, even when the parameter space is high-dimensional.
Optimization-based interval methods formulate confidence interval computation
as two endpoint optimizations, where the optimization constraints can be set to
achieve different types of interval calibration while seamlessly incorporating
parameter constraints. However, choosing valid optimization constraints has
been elusive. We show that all four proposed intervals achieve nominal coverage
for a particular functional both theoretically and in practice, with numerical
examples demonstrating superior performance of our intervals over the OSB
interval in terms of both coverage and expected length. In particular, we show
the superior performance in a realistic unfolding simulation from high-energy
physics that is severely ill-posed and involves a rank-deficient forward model.; 72) Discrete curve theory in space forms: planar elastic and
  area-constrained elastic curves; We propose a notion of discrete elastic and area-constrained elastic curves
in 2-dimensional space forms. Our definition extends the well-known discrete
Euclidean curvature equation to space forms and reflects various geometric
properties known from their smooth counterparts. Special emphasis is paid to
discrete flows built from B\""acklund transformations in the respective space
forms. The invariants of the flows form a hierarchy of curves and we show that
discrete elastic and constrained elastic curves can be characterized as
elements of this hierarchy. This work also includes an introductory chapter on
discrete curve theory in space forms, where we find discrete Frenet-type
formulas and describe an associated family related to a fundamental theorem.; 73) Reformulation of Einstein equations in the Fully Constrained
  Formulation: local-uniqueness, post-Newtonian expansion and initial data; Einstein equations can be written in the so-called Fully Constrained
Formulation (FCF). This formulation has two different sectors: the elliptic
sector, formed by the Hamiltonian and Momentum constraints together with the
equations derived from the gauge choice; and the hyperbolic sector, formed by
the evolution of the rest of the spacetime metric variables, which encodes the
gravitational radiation. In this work, we present a modification of both
sectors that keeps local uniqueness properties of the elliptic system of
equations and includes a hierarchical post-Newtonian structure of all the
elliptic and hyperbolic equations. This reformulation can have potential
applications in cosmology and relativistic astrophysics. Moreover, we show how
initial stationary data can be computed numerically using this formulation
without assuming a conformally flat spatial metric, with the illustrative
example of a rotating neutron star.; 74) Hausdorff dimension of dynamical coverings under mixing properties; In this article, we estimate the Hausdorff dimension of dynamical coverings
with respect to mixing ergodic systems. More precisely, if the ergodic measure
is exact-dimensionnal, we establish a formula provided that the system is
polynomially fast mixing and if the measure is not exact-dimensionnal, we
establish a similar result under super-polynomial speed of mix assumpetion. As
an application of our result, we extend the result of Fan-Shmeling-Troubetzkoy
for the doubling map on the circle to the case of the times 2, times 3 map on
the two dimensional torus.; 75) Fabry-P\'{e}rot etalon walk-off loss in ring cavities; Fabry-P\'erot etalons are widely used for laser frequency control. Inserting
them in laser cavities imparts unavoidable walk-off loss and reduces the output
power. Here, we treat the technology-relevant case of the walk-off loss of an
etalon in a unidirectional ring cavity, which is a standard design of
single-frequency lasers. We provide the theory background by discussing the
analytic limits. The loss can be efficiently minimized by realignment, or by
proper matching of the etalon's surface reflectivity with its refractive index.
We numerically calculate the loss in the region uncovered by the analytic
limits. We discuss practical laser design considerations, and we perform a
tilt-tuning experiment in a single-frequency, solid-state laser setup.; 76) DNA 1.0 Technical Report; In this report, we present DNA 1.0 8B Instruct, a state-of-the-art bilingual
language model optimized for Korean and English language tasks. By applying
continual pre-training (CPT) with high-quality Korean datasets to Llama 3.1 8B
and subsequent supervised fine-tuning (SFT), we create an instruction-following
model with enhanced Korean language capabilities. This model is then merged
with Llama 3.1 8B Instruct via spherical linear interpolation (SLERP) and
undergoes further optimization through direct preference optimization (DPO) and
knowledge distillation (KD). DNA 1.0 8B Instruct achieves state-of-the-art
results on Korean-specific tasks, including KMMLU (53.26%), KoBEST (83.40%),
and BELEBELE (57.99%), while maintaining strong English capabilities on MMLU
(66.64%), MMLU-Pro (43.05%) and GSM8K (80.52%). As an open model, DNA 1.0 8B
Instruct represents a significant advancement in bilingual language modeling.
  As an open model, DNA 1.0 8B Instruct is freely available through
https://huggingface.co/dnotitia/Llama-DNA-1.0-8B-Instruct . For commercial
licensing inquiries or feedback, please contact us at
https://www.dnotitia.com/contact/post-form; 77) Efficient Function-as-a-Service for Large Language Models with TIDAL; Large Language Model (LLM) applications have emerged as a prominent use case
for Function-as-a-Service (FaaS) due to their high computational demands and
sporadic invocation patterns. However, serving LLM functions within FaaS
frameworks faces significant GPU-side cold start. A fundamental approach
involves leveraging a template with function state saved on GPUs to bypass the
cold start for new invocations. Yet, this approach struggles with the high GPU
footprint, dynamic initialization behaviors, and lazy GPU kernel loading
inherent in LLM functions, primarily due to a lack of insight into the
underlying execution details. In this paper, we introduce TIDAL, an optimized
FaaS framework for LLM applications that achieves fast startups by tracing
fine-grained execution paths. By utilizing the traced execution details, TIDAL
generates adaptive function templates, effectively breaking startup barriers
for LLM functions. Extensive evaluations demonstrate that TIDAL reduces cold
start latency by $1.79\times\text{\textasciitilde}2.11\times$ and improves the
$95\%$-ile time-to-first-token by $76.0\%$, surpassing state-of-the-art
methods.; 78) Pb-intercalated epitaxial graphene on SiC: Full insight into band
  structure and orbital character of interlayer Pb, and charge transfer into
  graphene; Intercalation is a robust approach for modulating the properties of epitaxial
graphene on SiC and stabilizing two-dimensional (2D) intercalant layers at the
graphene/SiC interface. In this work, we present synchrotron-based angle
resolved photoelectron spectroscopy (ARPES) measurements focussing on the band
structure of intercalated Pb under a single layer of epitaxial graphene. The
interlayer Pb exhibits a metallic character, a $(1 \times 1)$ registry with
respect to SiC, and free electron-like bands to a first order. Divergences from
the free electron approximation include various band splittings and gaps
throughout the Pb Brillouin zone. Light polarization dependent ARPES
measurements indicate a predominant out-of-plane orbital character for the Pb
bands, suggesting potential interactions between the interlayer Pb and
graphene's $\pi$ orbitals that may induce proximity effects in graphene.
Density functional theory (DFT) calculations for a $(1 \times 1)$ Pb monolayer
on SiC show a reasonable qualitative agreement with the experimentally observed
interlayer bands as well as the polarization dependent measurements. Finally,
temperature dependent ARPES measurements reveal that the nearly charge-neutral
graphene layer involves charge transfer from both the interlayer Pb and the
substrate SiC.; 79) OptiSeq: Ordering Examples On-The-Fly for In-Context Learning; Developers using LLMs and LLM-based agents in their applications have
provided plenty of anecdotal evidence that in-context-learning (ICL) is
fragile. In this paper, we show that in addition to the quantity and quality of
examples, the order in which the in-context examples are listed in the prompt
affects the output of the LLM and, consequently, their performance. While prior
work has explored improving ICL through dataset-dependent techniques, we
introduce OptiSeq, a purely inference-time, dataset-free optimization method
that efficiently determines the best example order. OptiSeq leverages log
probabilities of LLM-generated outputs to systematically prune the search space
of possible orderings and recommend the best order(s) by distinguishing
orderings that yield high levels of accuracy and those that underperform.
Extensive empirical evaluation on multiple LLMs, datasets, and prompts
demonstrate that OptiSeq improves accuracy by 5.5 - 10.5 percentage points
across multiple tasks.; 80) Gemini Embedding: Generalizable Embeddings from Gemini; In this report, we introduce Gemini Embedding, a state-of-the-art embedding
model leveraging the power of Gemini, Google's most capable large language
model. Capitalizing on Gemini's inherent multilingual and code understanding
capabilities, Gemini Embedding produces highly generalizable embeddings for
text spanning numerous languages and textual modalities. The representations
generated by Gemini Embedding can be precomputed and applied to a variety of
downstream tasks including classification, similarity, clustering, ranking, and
retrieval. Evaluated on the Massive Multilingual Text Embedding Benchmark
(MMTEB), which includes over one hundred tasks across 250+ languages, Gemini
Embedding substantially outperforms prior state-of-the-art models,
demonstrating considerable improvements in embedding quality. Achieving
state-of-the-art performance across MMTEB's multilingual, English, and code
benchmarks, our unified model demonstrates strong capabilities across a broad
selection of tasks and surpasses specialized domain-specific models.; 81) Bernstein-type inequalities for quantum algebras; We establish Bernstein-type inequalities for the quantum algebras
  $K_{n,\Gamma}^{P,Q}(\mathbb{K})$ introduced by K. L. Horton that include the
graded quantum Weyl algebra, the quantum symplectic space, the quantum
Euclidean space, and quantum Heisenberg algebra etc., obtaining new results and
as well as simplified proofs of previously known results.
  The Krull and global dimensions of certain further localizations of
$K_{n,\Gamma}^{P,Q}(\mathbb{K})$ are computed.; 82) Regularized dynamical parametric approximation of stiff evolution
  problems; Evolutionary deep neural networks have emerged as a rapidly growing field of
research. This paper studies numerical integrators for such and other classes
of nonlinear parametrizations $ u(t) = \Phi(\theta(t)) $, where the evolving
parameters $\theta(t)$ are to be computed. The primary focus is on tackling the
challenges posed by the combination of stiff evolution problems and irregular
parametrizations, which typically arise with neural networks, tensor networks,
flocks of evolving Gaussians, and in further cases of overparametrization. We
propose and analyse regularized parametric versions of the implicit Euler
method and higher-order implicit Runge--Kutta methods for the time integration
of the parameters in nonlinear approximations to evolutionary partial
differential equations and large systems of stiff ordinary differential
equations. At each time step, an ill-conditioned nonlinear optimization problem
is solved approximately with a few regularized Gauss--Newton iterations. Error
bounds for the resulting parametric integrator are derived by relating the
computationally accessible Gauss--Newton iteration for the parameters to the
computationally inaccessible Newton iteration for the underlying non-parametric
time integration scheme. The theoretical findings are supported by numerical
experiments that are designed to show key properties of the proposed parametric
integrators.; 83) Transport approach to two-qubit quantum state tomography; Quantum state tomography (QST) is a central task for quantum information
processing, enabling quantum cryptography, computation, and state
certification. Traditional QST relies on projective measurements of single- and
two-qubit Pauli operators, requiring qubits to be isolated from environmental
dissipation. In this work, we demonstrate that measuring currents and
associated transport quantities flowing through a two-qubit system between two
terminals biased in temperature or voltage are sufficient to perform complete
QST of the open quantum system. This transport approach requires minimal
knowledge of the system-environment couplings and of the parameters setting the
system's dynamics, accessible in state-of-the-art solid-state experiments via
spectroscopic measurements for instance. Our findings are analytical, offering
comprehensive insights into the underlying processes. As a direct consequence
of our approach, we are able to provide a transport-based entanglement measure
to certify the presence of quantum correlations, expressing the concurrence in
terms of currents and correlations functions only.; 84) Trapping and Transport of Inertial Particles in a Taylor-Green Vortex:
  Effects of Added Mass and History Force; We investigate the dynamics of small inertial particles in a two-dimensional,
steady Taylor-Green vortex flow. A classic study by Taylor (2022) showed that
heavy inertial point particles (having density parameter R = 1) are trapped by
the flow separatrices when the particle Stokes number St, which measures the
particle's inertia, is less than 1/4. Here, we consider finitely dense
particles, incorporating the previously neglected effects of added mass and the
Boussinesq-Basset history force. Using linear stability analysis near
stagnation points, we determine the critical parametric conditions in the St-R
plane that leads to particle trapping within vortex cells. We identify
additional stagnation points perceived by inertial particles, beyond the
traditional ones at vortex cell corners, when the added mass effect is
included, and we analyze their stability. Numerical analysis of the full
nonlinear system confirms the existence of distinct particle
behaviours--trapped, diffusive, and ballistic--depending on initial conditions,
consistent with Nath et al. (2024), with modifications due to added mass
effect. We delineate the regions in the St-R plane where these behaviours
dominate based on the prominent particle dynamics. However, when both the
history force and added mass effect are included, all particles exhibit
ballistic motion regardless of St and R.; 85) Joint Delay-Doppler Estimation using OFDMA Payloads for Integrated
  Sensing and Communications; The use of future communication systems for sensing offers the potential for
a number of new applications. In this paper, we show that leveraging user data
payloads in multi-node Orthogonal Frequency Division Multiple Access (OFDMA)
networks for estimating target delay and Doppler-shift parameters can yield a
significant advantage in SNR and addressable bandwidth. However, gaps in the
frequency-time resources, reference signal boosting and amplitude modulation
schemes introduce challenges for estimation at the sensing receiver.
  In this work, we propose a joint delay and Doppler-shift model-based
estimator designed to address these challenges. Furthermore, we demonstrate
that incorporating knowledge of the device model into the estimation procedure
helps mitigate the effects of the non-ideal radar ambiguity function caused by
amplitude-modulated user payloads and sparse reference signals. Simulation
results demonstrate that the estimator achieves the theoretical lower bound on
estimation variance.; 86) Higher-order chiral scalar from boundary reduction of 3d higher-spin
  gravity; We use a recently proposed covariant procedure to reduce the Chern-Simons
action of three-dimensional higher-spin gravity to the boundary, resulting in a
Lorentz covariant action for higher-order chiral scalars. After gauge-fixing,
we obtain a higher-derivative action generalizing the $s=1$ Floreanini-Jackiw
and $s=2$ Alekseev-Shatashvili actions to arbitrary spin $s$. For simplicity,
we treat the case of general spin at the linearized level, while the full
non-linear asymptotic boundary conditions are presented in component form for
the $SL(3,\mathbb R)$ case. Finally, we extend the spin-3 linearized analysis
to a background with non-trivial higher-spin charge and show that it has a
richer structure of zero modes.; 87) Separation and excision in functor homology; We prove separation and excision results in functor homology. These results
explain how the global Steinberg decomposition of functors proved by Djament,
Touz{\'e} and Vespa behaves in Ext and Tor computations.; 88) UBMF: Uncertainty-Aware Bayesian Meta-Learning Framework for Fault
  Diagnosis with Imbalanced Industrial Data; Fault diagnosis of mechanical equipment involves data collection, feature
extraction, and pattern recognition but is often hindered by the imbalanced
nature of industrial data, introducing significant uncertainty and reducing
diagnostic reliability. To address these challenges, this study proposes the
Uncertainty-Aware Bayesian Meta-Learning Framework (UBMF), which integrates
four key modules: data perturbation injection for enhancing feature robustness,
cross-task self-supervised feature extraction for improving transferability,
uncertainty-based sample filtering for robust out-of-domain generalization, and
Bayesian meta-knowledge integration for fine-grained classification.
Experimental results on ten open-source datasets under various imbalanced
conditions, including cross-task, small-sample, and unseen-sample scenarios,
demonstrate the superiority of UBMF, achieving an average improvement of 42.22%
across ten Any-way 1-5-shot diagnostic tasks. This integrated framework
effectively enhances diagnostic accuracy, generalization, and adaptability,
providing a reliable solution for complex industrial fault diagnosis.; 89) Applications of de Sitter Causality Conditions; We find explicit de Sitter shockwave solutions in arbitrary spacetime
dimensions. We use these to determine the dimensional-dependent ""stretching"" of
the de Sitter Penrose diagram in the presence of a shock or black hole. This
stretching sets the scale at which superluminalities in de Sitter can be
considered resolvable. We then consider an RFF coupling for a spin-1 particle
and a Gauss-Bonnet coupling for a spin-2 particle and analyze these in terms of
our proposed de Sitter causality criteria. We briefly discuss the connections
between our results and corresponding results in anti de Sitter spacetime.; 90) Forcing, genericity and CBERS; In this paper we continue the study of equivalence of generics filters
started by Smythe in [Smy22]. We fully characterize those forcing posets for
which the corresponding equivalence of generics is smooth using the purely
topological property of condensation. Next we leverage our characterization to
show that there are non-homogeneous forcing for which equivalence of generics
is not smooth. Then we prove hyperfiniteness in the case of Prikry forcing and
some additional results addressing the problem whether generic equivalence for
Cohen forcing is hyperfinite.; 91) Towards Automated Self-Supervised Learning for Truly Unsupervised Graph
  Anomaly Detection; Self-supervised learning (SSL) is an emerging paradigm that exploits
supervisory signals generated from the data itself, and many recent studies
have leveraged SSL to conduct graph anomaly detection. However, we empirically
found that three important factors can substantially impact detection
performance across datasets: 1) the specific SSL strategy employed; 2) the
tuning of the strategy's hyperparameters; and 3) the allocation of combination
weights when using multiple strategies. Most SSL-based graph anomaly detection
methods circumvent these issues by arbitrarily or selectively (i.e., guided by
label information) choosing SSL strategies, hyperparameter settings, and
combination weights. While an arbitrary choice may lead to subpar performance,
using label information in an unsupervised setting is label information leakage
and leads to severe overestimation of a method's performance. Leakage has been
criticized as ""one of the top ten data mining mistakes"", yet many recent
studies on SSL-based graph anomaly detection have been using label information
to select hyperparameters. To mitigate this issue, we propose to use an
internal evaluation strategy (with theoretical analysis) to select
hyperparameters in SSL for unsupervised anomaly detection. We perform extensive
experiments using 10 recent SSL-based graph anomaly detection algorithms on
various benchmark datasets, demonstrating both the prior issues with
hyperparameter selection and the effectiveness of our proposed strategy.; 92) An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.; 93) ContextGNN goes to Elliot: Towards Benchmarking Relational Deep Learning
  for Static Link Prediction (aka Personalized Item Recommendation); Relational deep learning (RDL) settles among the most exciting advances in
machine learning for relational databases, leveraging the representational
power of message passing graph neural networks (GNNs) to derive useful
knowledge and run predicting tasks on tables connected through
primary-to-foreign key links. The RDL paradigm has been successfully applied to
recommendation lately, through its most recent representative deep learning
architecture namely, ContextGNN. While acknowledging ContextGNN's improved
performance on real-world recommendation datasets and tasks, preliminary tests
for the more traditional static link prediction task (aka personalized item
recommendation) on the popular Amazon Book dataset have demonstrated how
ContextGNN has still room for improvement compared to other state-of-the-art
GNN-based recommender systems. To this end, with this paper, we integrate
ContextGNN within Elliot, a popular framework for reproducibility and
benchmarking analyses, counting around 50 state-of-the-art recommendation
models from the literature to date. On such basis, we run preliminary
experiments on three standard recommendation datasets and against six
state-of-the-art GNN-based recommender systems, confirming similar trends to
those observed by the authors in their original paper. The code is publicly
available on GitHub:
https://github.com/danielemalitesta/Rel-DeepLearning-RecSys.; 94) Prime Identification and Composite Filtering Using GM-(n+1) Sequences; This paper presents a distinctive prime detection approach. This method use
GM-(n+1) sequences to effectively eliminate complex numbers. The sequences,
which consist of odd a number of (n+1), exclude all components except for the
initial prime integer. Only the first prime number is presented. This research
proposes an approach using this model to identify exceptional candidates and
examine their distribution. This study examines the interconnections among the
laws of division, basic gaps, and their applications in analytical procedures.
Computer studies may provide a novel perspective on the theory of prime
numbers, demonstrating the effectiveness of this approach in refining the
search space for primes.; 95) Aligning to What? Limits to RLHF Based Alignment; Reinforcement Learning from Human Feedback (RLHF) is increasingly used to
align large language models (LLMs) with human preferences. However, the
effectiveness of RLHF in addressing underlying biases remains unclear. This
study investigates the relationship between RLHF and both covert and overt
biases in LLMs, particularly focusing on biases against African Americans. We
applied various RLHF techniques (DPO, ORPO, and RLOO) to Llama 3 8B and
evaluated the covert and overt biases of the resulting models using
matched-guise probing and explicit bias testing. We performed additional tests
with DPO on different base models and datasets; among several implications, we
found that SFT before RLHF calcifies model biases. Additionally, we extend the
tools for measuring biases to multi-modal models. Through our experiments we
collect evidence that indicates that current alignment techniques are
inadequate for nebulous tasks such as mitigating covert biases, highlighting
the need for capable datasets, data curating techniques, or alignment tools.; 96) Explainable Sentiment Analysis with DeepSeek-R1: Performance,
  Efficiency, and Few-Shot Learning; Recent advancements in large language models (LLMs) have significantly
enhanced sentiment analysis capabilities. However, the trade-offs between model
performance, efficiency, and explainability of some latest models remain
underexplored. This study presents the first comprehensive evaluation of the
DeepSeek-R1 series of models, reasoning open-source LLMs, for sentiment
analysis, comparing them against OpenAI's GPT-4 and GPT-4-mini. We
systematically analyze their performance under few-shot prompting conditions,
scaling up to 50-shot configurations to assess in-context learning
effectiveness. Our experiments reveal that DeepSeek-R1 demonstrates competitive
accuracy, particularly in multi-class sentiment tasks, while offering enhanced
interpretability through its detailed reasoning process. Additionally, we
highlight the impact of increasing few-shot examples on model performance and
discuss key trade-offs between explainability and computational efficiency.; 97) Argument-Based Comparative Question Answering Evaluation Benchmark; In this paper, we aim to solve the problems standing in the way of automatic
comparative question answering. To this end, we propose an evaluation framework
to assess the quality of comparative question answering summaries. We formulate
15 criteria for assessing comparative answers created using manual annotation
and annotation from 6 large language models and two comparative question
asnwering datasets. We perform our tests using several LLMs and manual
annotation under different settings and demonstrate the constituency of both
evaluations. Our results demonstrate that the Llama-3 70B Instruct model
demonstrates the best results for summary evaluation, while GPT-4 is the best
for answering comparative questions. All used data, code, and evaluation
results are publicly
available\footnote{\url{https://anonymous.4open.science/r/cqa-evaluation-benchmark-4561/README.md}}.; 98) How to Sell a Service with Uncertain Outcomes; Motivated by the recent popularity of machine learning training services, we
introduce a contract design problem in which a provider sells a service that
results in an outcome of uncertain quality for the buyer. The seller has a set
of actions that lead to different distributions over outcomes. We focus on a
setting in which the seller has the ability to commit to an action and the
buyer is free to accept or reject the outcome after seeing its realized
quality. We propose a two-stage payment scheme where the seller designs a menu
of contracts, each of which specifies an action, an upfront price and a vector
of outcome-dependent usage prices. Upon selecting a contract, the buyer pays
the upfront price, and after observing the realized outcome, the buyer either
accepts and pays the corresponding usage price, or rejects and is exempt from
further payment. We show that this two-stage payment structure is necessary to
maximize profit: only upfront prices or only usage prices is insufficient. We
then study the computational complexity of computing a profit-maximizing menu
in our model. While computing the exact maximum seller profit is NP-hard even
for two buyer types, we derive a fully-polynomial time approximation scheme
(FPTAS) for the maximum profit for a constant number of buyer types. Finally,
we prove that in the single-parameter setting in which buyers' valuations are
parametrized by a single real number that seller revenue can be maximized using
a menu consisting of a single contract.; 99) Relative phase between $s_{\pm}$ superconducting order parameter
  components in a two-band model with impurities; We obtain solutions for Eliashberg equations within the Nambu representation
for a two-band model of iron-based superconductors with nonmagnetic impurities.
Two cases of a transition between $s_{\pm}$ and $s_{++}$ states are considered:
(i) the transition is accompanied by the abrupt change of the order parameter
sign within one of the bands and (ii) the change is smooth. For both cases, we
studied the role of a gauge defined by the coefficients preceding the Pauli
matrices $\hat\tau_1$ and $\hat\tau_2$ in a self-energy expansion, which
correspond to the components of the order parameter. We show that the absolute
value of the order parameter is conserved for solutions in the clean and in the
Born limits. In an intermediate case, between the Born and unitary limits,
result depends on the solution for the clean limit. We show that a common gauge
for the Eliashberg equations in which one of the order parameter components
vanishes is essential for adequate description of the multiband superconducting
systems.; 100) Frame-dependent coherence of a quantum state; A finite-dimensional Hilbert space is usually described by using an
orthonormal basis, but a more general description can be obtained by using a
tight frame. The frame-dependent coherence, defined by following the analogy
with the basis-dependent coherence, allows us to define the coherence with
respect to several orthogonal bases considered simultaneously or with respect
to a discrete system of coherent states. By using this more general definition,
we can investigate how the basis-dependent coherence changes when we go from a
basis to another one, from a basis to a complementary one. Frame-dependent
coherence of a quantum state coincides with the basis-dependent coherence of
the corresponding state in a Neumark extension.",0.0,0.5
2411.05236,applied,2411.05236-pos1-6,"DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT; Shannon capacity of signal transduction for multiple independent receptors; Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver.",2411.05236-pos2-6,"Channelrhodopsin-2, a directly light-gated cation-selective membrane channel; Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane α helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination.",9,"['31', '9', '1', '5', '49', '10', '39', '94', '66', '48']","The main paper discusses a visible light communication (VLC) system, which focuses on wireless data transmission using optical signals. The most relevant candidate paper is one that deals with advancements in optical systems or applications that can synergize with VLC technology. The paper on 'Light drag in an Optomechanical system' (candidate 31) directly relates to advancements in optical systems and explores phenomena that could be harnessed for improved VLC performance. Other candidates are less directly related to the core concepts of optical communication technology.","1) Intertwiners of representations of untwisted quantum affine algebras and
  Yangians revisited; We discuss applications of the $q$-characters to the computation of the
$R$-matrices. In particular, we describe the $R$-matrix acting in the tensor
square of the first fundamental representation of E$_8$ and in a number of
other cases, where the decomposition of the tensor squares with respect to
non-affine quantum algebra has non-trivial multiplicities. As an illustration,
we also recover $R$-matrices acting in the multiplicity free-case on the tensor
squares of the first fundamental representations of all other types of
untwisted quantum affine algebras. The answer is written in terms of projectors
related to the decomposition of the tensor squares with respect to non-affine
quantum algebras. Then we give explicit expressions for the $R$-matrices in
terms of matrix units with respect to a natural basis (except for the case of
E$_8$). We give similar formulas for the Yangian $R$-matrices.; 2) Aligning to What? Limits to RLHF Based Alignment; Reinforcement Learning from Human Feedback (RLHF) is increasingly used to
align large language models (LLMs) with human preferences. However, the
effectiveness of RLHF in addressing underlying biases remains unclear. This
study investigates the relationship between RLHF and both covert and overt
biases in LLMs, particularly focusing on biases against African Americans. We
applied various RLHF techniques (DPO, ORPO, and RLOO) to Llama 3 8B and
evaluated the covert and overt biases of the resulting models using
matched-guise probing and explicit bias testing. We performed additional tests
with DPO on different base models and datasets; among several implications, we
found that SFT before RLHF calcifies model biases. Additionally, we extend the
tools for measuring biases to multi-modal models. Through our experiments we
collect evidence that indicates that current alignment techniques are
inadequate for nebulous tasks such as mitigating covert biases, highlighting
the need for capable datasets, data curating techniques, or alignment tools.; 3) Towards Decoding Developer Cognition in the Age of AI Assistants; Background: The increasing adoption of AI assistants in programming has led
to numerous studies exploring their benefits. While developers consistently
report significant productivity gains from these tools, empirical measurements
often show more modest improvements. While prior research has documented
self-reported experiences with AI-assisted programming tools, little to no work
has been done to understand their usage patterns and the actual cognitive load
imposed in practice. Objective: In this exploratory study, we aim to
investigate the role AI assistants play in developer productivity.
Specifically, we are interested in how developers' expertise levels influence
their AI usage patterns, and how these patterns impact their actual cognitive
load and productivity during development tasks. We also seek to better
understand how this relates to their perceived productivity. Method: We propose
a controlled observational study combining physiological measurements (EEG and
eye tracking) with interaction data to examine developers' use of AI-assisted
programming tools. We will recruit professional developers to complete
programming tasks both with and without AI assistance while measuring their
cognitive load and task completion time. Through pre- and post-task
questionnaires, we will collect data on perceived productivity and cognitive
load using NASA-TLX.; 4) Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference
  Alignment; Direct Preference Optimization (DPO) has become a prominent method for
aligning Large Language Models (LLMs) with human preferences. While DPO has
enabled significant progress in aligning English LLMs, multilingual preference
alignment is hampered by data scarcity. To address this, we propose a novel
approach that $\textit{captures}$ learned preferences from well-aligned English
models by implicit rewards and $\textit{transfers}$ them to other languages
through iterative training. Specifically, we derive an implicit reward model
from the logits of an English DPO-aligned model and its corresponding reference
model. This reward model is then leveraged to annotate preference relations in
cross-lingual instruction-following pairs, using English instructions to
evaluate multilingual responses. The annotated data is subsequently used for
multilingual DPO fine-tuning, facilitating preference knowledge transfer from
English to other languages. Fine-tuning Llama3 for two iterations resulted in a
12.72% average improvement in Win Rate and a 5.97% increase in Length Control
Win Rate across all training languages on the X-AlpacaEval leaderboard. Our
findings demonstrate that leveraging existing English-aligned models can enable
efficient and effective multilingual preference alignment, significantly
reducing the need for extensive multilingual preference data. The code is
available at https://github.com/ZNLP/Implicit-Cross-Lingual-Rewarding; 5) Spectra of standing kink waves in loops and the effects of the lower
  solar atmosphere; Understanding the effects of the lower solar atmosphere on the spectrum of
standing kink oscillations of coronal loops, in both the decaying and decayless
regime, is essential for developing more advanced tools for coronal seismology.
We aim to reveal the effects of the chromosphere on the spatial profiles and
frequencies of the standing kink modes, create synthetic emission maps to
compare with observations, and study the results using spatial and temporal
coronal seismology techniques. We excited transverse oscillations in a 3D
straight flux tube using (a) a broadband footpoint driver, (b) a sinusoidal
velocity pulse, and (c) an off-centre Gaussian velocity pulse, using the PLUTO
code. The flux tube is gravitationally stratified, with footpoints embedded in
chromospheric plasma. Using the FoMo code, we created synthetic observations of
our data in the Fe IX 17.1 nm line and calculated the spectra with the
Auto-NUWT code. We also numerically solved the generalised eigenvalue system
for the 1D wave equation to determine the effects of the stratification on the
kink modes of our system. The synthetic observations of the loops perturbed by
the velocity pulses show a single dominant mode that our 1D analysis reveals to
be the third harmonic of the system. For the broadband driver, the synthetic
emission shows multiple frequency bands, associated with both the loop and the
driver. Finally, using seismological techniques, we highlight the possibility
of misidentifying the observed third, sixth, and ninth harmonics with the
first, second, and third harmonics of the coronal part of longer loops. Unless
more advanced techniques of spatial seismology are used with many data points
from observations along the entire loop length, this misidentification can
result in overestimating the mean magnetic field by a factor equal to the
period ratio of the fundamental over the third harmonic.; 6) Pixel-Wise Feature Selection for Perceptual Edge Detection without
  post-processing; Although deep convolutional neutral networks (CNNs) have significantly
enhanced performance in image edge detection (ED), current models remain highly
dependent on post-processing techniques such as non-maximum suppression (NMS),
and often fail to deliver satisfactory perceptual results, while the
performance will deteriorate significantly if the allowed error toleration
distance decreases. These limitations arise from the uniform fusion of features
across all pixels, regardless of their specific characteristics, such as the
distinction between textural and edge areas. If the features extracted by the
ED models are selected more meticulously and encompass greater diversity, the
resulting predictions are expected to be more accurate and perceptually
meaningful. Motivated by this observation, this paper proposes a novel feature
selection paradigm for deep networks that facilitates the differential
selection of features and can be seamlessly integrated into existing ED models.
By incorporating this additional structure, the performance of conventional ED
models is substantially enhanced without post-processing, while simultaneously
enhancing the perceptual quality of the predictions. Extensive experimental
evaluations validate the effectiveness of the proposed model.; 7) Two characterizations of Sheffer-Dunkl sequences; Sheffer polynomials can be characterized using different Stieltjes integrals.
These families of polynomials have been recently extended to the Dunkl context.
In this way some classical operators as the derivative operator or the
difference operator are replaced as analogous operators in the Dunkl universe.
In this paper we establish two Stieltjes integrals that help us to characterize
the Sheffer-Dunkl polynomials.; 8) Deep Learning for Wound Tissue Segmentation: A Comprehensive Evaluation
  using A Novel Dataset; Deep learning (DL) techniques have emerged as promising solutions for medical
wound tissue segmentation. However, a notable limitation in this field is the
lack of publicly available labelled datasets and a standardised performance
evaluation of state-of-the-art DL models on such datasets. This study addresses
this gap by comprehensively evaluating various DL models for wound tissue
segmentation using a novel dataset. We have curated a dataset comprising 147
wound images exhibiting six tissue types: slough, granulation, maceration,
necrosis, bone, and tendon. The dataset was meticulously labelled for semantic
segmentation employing supervised machine learning techniques. Three distinct
labelling formats were developed -- full image, patch, and superpixel. Our
investigation encompassed a wide array of DL segmentation and classification
methodologies, ranging from conventional approaches like UNet, to generative
adversarial networks such as cGAN, and modified techniques like FPN+VGG16.
Also, we explored DL-based classification methods (e.g., ResNet50) and machine
learning-based classification leveraging DL features (e.g., AlexNet+RF). In
total, 82 wound tissue segmentation models were derived across the three
labelling formats. Our analysis yielded several notable findings, including
identifying optimal DL models for each labelling format based on weighted
average Dice or F1 scores. Notably, FPN+VGG16 emerged as the top-performing DL
model for wound tissue segmentation, achieving a dice score of 82.25%. This
study provides a valuable benchmark for evaluating wound image segmentation and
classification models, offering insights to inform future research and clinical
practice in wound care. The labelled dataset created in this study is available
at https://github.com/akabircs/WoundTissue.; 9) Channelrhodopsin-2, a directly light-gated cation-selective membrane channel; Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane α helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination.; 10) DNA 1.0 Technical Report; In this report, we present DNA 1.0 8B Instruct, a state-of-the-art bilingual
language model optimized for Korean and English language tasks. By applying
continual pre-training (CPT) with high-quality Korean datasets to Llama 3.1 8B
and subsequent supervised fine-tuning (SFT), we create an instruction-following
model with enhanced Korean language capabilities. This model is then merged
with Llama 3.1 8B Instruct via spherical linear interpolation (SLERP) and
undergoes further optimization through direct preference optimization (DPO) and
knowledge distillation (KD). DNA 1.0 8B Instruct achieves state-of-the-art
results on Korean-specific tasks, including KMMLU (53.26%), KoBEST (83.40%),
and BELEBELE (57.99%), while maintaining strong English capabilities on MMLU
(66.64%), MMLU-Pro (43.05%) and GSM8K (80.52%). As an open model, DNA 1.0 8B
Instruct represents a significant advancement in bilingual language modeling.
  As an open model, DNA 1.0 8B Instruct is freely available through
https://huggingface.co/dnotitia/Llama-DNA-1.0-8B-Instruct . For commercial
licensing inquiries or feedback, please contact us at
https://www.dnotitia.com/contact/post-form; 11) Making Sense of Data in the Wild: Data Analysis Automation at Scale; As the volume of publicly available data continues to grow, researchers face
the challenge of limited diversity in benchmarking machine learning tasks.
Although thousands of datasets are available in public repositories, the sheer
abundance often complicates the search for suitable data, leaving many valuable
datasets underexplored. This situation is further amplified by the fact that,
despite longstanding advocacy for improving data curation quality, current
solutions remain prohibitively time-consuming and resource-intensive. In this
paper, we propose a novel approach that combines intelligent agents with
retrieval augmented generation to automate data analysis, dataset curation and
indexing at scale. Our system leverages multiple agents to analyze raw,
unstructured data across public repositories, generating dataset reports and
interactive visual indexes that can be easily explored. We demonstrate that our
approach results in more detailed dataset descriptions, higher hit rates and
greater diversity in dataset retrieval tasks. Additionally, we show that the
dataset reports generated by our method can be leveraged by other machine
learning models to improve the performance on specific tasks, such as improving
the accuracy and realism of synthetic data generation. By streamlining the
process of transforming raw data into machine-learning-ready datasets, our
approach enables researchers to better utilize existing data resources.; 12) Forcing, genericity and CBERS; In this paper we continue the study of equivalence of generics filters
started by Smythe in [Smy22]. We fully characterize those forcing posets for
which the corresponding equivalence of generics is smooth using the purely
topological property of condensation. Next we leverage our characterization to
show that there are non-homogeneous forcing for which equivalence of generics
is not smooth. Then we prove hyperfiniteness in the case of Prikry forcing and
some additional results addressing the problem whether generic equivalence for
Cohen forcing is hyperfinite.; 13) CAT-3DGS: A Context-Adaptive Triplane Approach to
  Rate-Distortion-Optimized 3DGS Compression; 3D Gaussian Splatting (3DGS) has recently emerged as a promising 3D
representation. Much research has been focused on reducing its storage
requirements and memory footprint. However, the needs to compress and transmit
the 3DGS representation to the remote side are overlooked. This new application
calls for rate-distortion-optimized 3DGS compression. How to quantize and
entropy encode sparse Gaussian primitives in the 3D space remains largely
unexplored. Few early attempts resort to the hyperprior framework from learned
image compression. But, they fail to utilize fully the inter and intra
correlation inherent in Gaussian primitives. Built on ScaffoldGS, this work,
termed CAT-3DGS, introduces a context-adaptive triplane approach to their
rate-distortion-optimized coding. It features multi-scale triplanes, oriented
according to the principal axes of Gaussian primitives in the 3D space, to
capture their inter correlation (i.e. spatial correlation) for spatial
autoregressive coding in the projected 2D planes. With these triplanes serving
as the hyperprior, we further perform channel-wise autoregressive coding to
leverage the intra correlation within each individual Gaussian primitive. Our
CAT-3DGS incorporates a view frequency-aware masking mechanism. It actively
skips from coding those Gaussian primitives that potentially have little impact
on the rendering quality. When trained end-to-end to strike a good
rate-distortion trade-off, our CAT-3DGS achieves the state-of-the-art
compression performance on the commonly used real-world datasets.; 14) Coarse-to-Fine Structure-Aware Artistic Style Transfer; Artistic style transfer aims to use a style image and a content image to
synthesize a target image that retains the same artistic expression as the
style image while preserving the basic content of the content image. Many
recently proposed style transfer methods have a common problem; that is, they
simply transfer the texture and color of the style image to the global
structure of the content image. As a result, the content image has a local
structure that is not similar to the local structure of the style image. In
this paper, we present an effective method that can be used to transfer style
patterns while fusing the local style structure into the local content
structure. In our method, dif-ferent levels of coarse stylized features are
first reconstructed at low resolution using a Coarse Network, in which style
color distribution is roughly transferred, and the content structure is
combined with the style structure. Then, the reconstructed features and the
content features are adopted to synthesize high-quality structure-aware
stylized images with high resolution using a Fine Network with three structural
selective fusion (SSF) modules. The effectiveness of our method is demonstrated
through the generation of appealing high-quality stylization results and a
com-parison with some state-of-the-art style transfer methods.; 15) Pb-intercalated epitaxial graphene on SiC: Full insight into band
  structure and orbital character of interlayer Pb, and charge transfer into
  graphene; Intercalation is a robust approach for modulating the properties of epitaxial
graphene on SiC and stabilizing two-dimensional (2D) intercalant layers at the
graphene/SiC interface. In this work, we present synchrotron-based angle
resolved photoelectron spectroscopy (ARPES) measurements focussing on the band
structure of intercalated Pb under a single layer of epitaxial graphene. The
interlayer Pb exhibits a metallic character, a $(1 \times 1)$ registry with
respect to SiC, and free electron-like bands to a first order. Divergences from
the free electron approximation include various band splittings and gaps
throughout the Pb Brillouin zone. Light polarization dependent ARPES
measurements indicate a predominant out-of-plane orbital character for the Pb
bands, suggesting potential interactions between the interlayer Pb and
graphene's $\pi$ orbitals that may induce proximity effects in graphene.
Density functional theory (DFT) calculations for a $(1 \times 1)$ Pb monolayer
on SiC show a reasonable qualitative agreement with the experimentally observed
interlayer bands as well as the polarization dependent measurements. Finally,
temperature dependent ARPES measurements reveal that the nearly charge-neutral
graphene layer involves charge transfer from both the interlayer Pb and the
substrate SiC.; 16) PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning; Large language models demonstrate remarkable capabilities across various
domains, especially mathematics and logic reasoning. However, current
evaluations overlook physics-based reasoning - a complex task requiring physics
theorems and constraints. We present PhysReason, a 1,200-problem benchmark
comprising knowledge-based (25%) and reasoning-based (75%) problems, where the
latter are divided into three difficulty levels (easy, medium, hard). Notably,
problems require an average of 8.1 solution steps, with hard requiring 15.6,
reflecting the complexity of physics-based reasoning. We propose the Physics
Solution Auto Scoring Framework, incorporating efficient answer-level and
comprehensive step-level evaluations. Top-performing models like Deepseek-R1,
Gemini-2.0-Flash-Thinking, and o3-mini-high achieve less than 60% on
answer-level evaluation, with performance dropping from knowledge questions
(75.11%) to hard problems (31.95%). Through step-level evaluation, we
identified four key bottlenecks: Physics Theorem Application, Physics Process
Understanding, Calculation, and Physics Condition Analysis. These findings
position PhysReason as a novel and comprehensive benchmark for evaluating
physics-based reasoning capabilities in large language models. Our code and
data will be published at https:/dxzxy12138.github.io/PhysReason.; 17) Superconductivity near an Ising nematic quantum critical point in two
  dimensions; Near a two-dimensional Ising-type nematic quantum critical point, the quantum
fluctuations of the nematic order parameter are coupled to the electrons,
leading to non-Fermi liquid behavior and unconventional superconductivity. The
interplay between these two effects has been extensively studied through the
Eliashberg equations for the superconducting gap. However, previous studies
often rely on various approximations that may introduce uncertainties in the
results. Here, we revisit this problem without these approximations and examine
how their removal changes the outcomes. We numerically solve four
self-consistent Eliashberg integral equations of the mass renormalization
$A_{1}(p)$, the chemical potential renormalization $A_{2}(p)$, the pairing
function $\Phi(p)$, and the nematic self-energy (polarization) function
$\Pi(q)$ using the iteration method. Our calculations retain the explicit
non-linearity and the full momentum dependence of these equations. We find that
discarding some commonly used approximations allows for a more accurate
determination of the superconducting gap $\Delta = \Phi/A_{1}$ and the critical
temperature $T_{c}$. The Eliashberg equations have two different convergent gap
solutions: an extended $s$-wave gap and a $d_{x^{2}-y^{2}}$-wave gap. The
latter is fragile, whereas the former is robust against small perturbations.; 18) Extensibility and denseness of periodic semigroup actions; We study periodic points and finitely supported invariant measures for
continuous semigroup actions. Introducing suitable notions of periodicity in
both topological and measure-theoretical contexts, we analyze the space of
invariant Borel probability measures associated with these actions. For
embeddable semigroups, we establish a direct relationship between the
extensibility of invariant measures to the free group on the semigroup and the
denseness of finitely supported invariant measures. Applying this framework to
shift actions on the full shift, we prove that finitely supported invariant
measures are dense for every left amenable semigroup that is residually a
finite group and for every finite-rank free semigroup.; 19) Transparent NLP: Using RAG and LLM Alignment for Privacy Q&A; The transparency principle of the General Data Protection Regulation (GDPR)
requires data processing information to be clear, precise, and accessible.
While language models show promise in this context, their probabilistic nature
complicates truthfulness and comprehensibility.
  This paper examines state-of-the-art Retrieval Augmented Generation (RAG)
systems enhanced with alignment techniques to fulfill GDPR obligations. We
evaluate RAG systems incorporating an alignment module like Rewindable
Auto-regressive Inference (RAIN) and our proposed multidimensional extension,
MultiRAIN, using a Privacy Q&A dataset. Responses are optimized for preciseness
and comprehensibility and are assessed through 21 metrics, including
deterministic and large language model-based evaluations.
  Our results show that RAG systems with an alignment module outperform
baseline RAG systems on most metrics, though none fully match human answers.
Principal component analysis of the results reveals complex interactions
between metrics, highlighting the need to refine metrics. This study provides a
foundation for integrating advanced natural language processing systems into
legal compliance frameworks.; 20) $\tau$-exceptional sequences for representations of quivers over local
  algebras; Let $k$ be an algebraically closed field. Let $R$ be a finite dimensional
commutative local $k$-algebra and let $Q$ be a quiver with no oriented cycles.
In this paper, we study (signed) $\tau$-exceptional sequences over the algebra
$\Lambda = RQ$, which is isomorphic to $R\otimes kQ$. We show there is a
bijection between the set of complete (signed) $\tau$-exceptional sequences in
$\text{mod }kQ$ and the set of complete (signed) $\tau$-exceptional sequences
in $\text{mod }\Lambda$. Moreover, we prove that every $\tau$-perpendicular
subcategory of $\text{mod }\Lambda$ is equivalent to the module category of
$R\otimes kQ'$, for some quiver $Q'$. As a consequence, we prove that the
$\tau$-cluster morphism categories of $kQ$ and $\Lambda$ are equivalent.; 21) A threshold for Poisson behavior of non-stationary product measures; Let $\gamma_{n}= O (\log^{-c}n)$ and let $\nu$ be the infinite product
measure whose $n$-th marginal is Bernoulli$(1/2+\gamma_{n})$. We show that
$c=1/2$ is the threshold, above which $\nu$-almost every point is simply
Poisson generic in the sense of Peres-Weiss, and below which this can fail.
This provides a range in which $\nu$ is singular with respect to the uniform
product measure, but $\nu$-almost every point is simply Poisson generic.; 22) Explainable Sentiment Analysis with DeepSeek-R1: Performance,
  Efficiency, and Few-Shot Learning; Recent advancements in large language models (LLMs) have significantly
enhanced sentiment analysis capabilities. However, the trade-offs between model
performance, efficiency, and explainability of some latest models remain
underexplored. This study presents the first comprehensive evaluation of the
DeepSeek-R1 series of models, reasoning open-source LLMs, for sentiment
analysis, comparing them against OpenAI's GPT-4 and GPT-4-mini. We
systematically analyze their performance under few-shot prompting conditions,
scaling up to 50-shot configurations to assess in-context learning
effectiveness. Our experiments reveal that DeepSeek-R1 demonstrates competitive
accuracy, particularly in multi-class sentiment tasks, while offering enhanced
interpretability through its detailed reasoning process. Additionally, we
highlight the impact of increasing few-shot examples on model performance and
discuss key trade-offs between explainability and computational efficiency.; 23) UBMF: Uncertainty-Aware Bayesian Meta-Learning Framework for Fault
  Diagnosis with Imbalanced Industrial Data; Fault diagnosis of mechanical equipment involves data collection, feature
extraction, and pattern recognition but is often hindered by the imbalanced
nature of industrial data, introducing significant uncertainty and reducing
diagnostic reliability. To address these challenges, this study proposes the
Uncertainty-Aware Bayesian Meta-Learning Framework (UBMF), which integrates
four key modules: data perturbation injection for enhancing feature robustness,
cross-task self-supervised feature extraction for improving transferability,
uncertainty-based sample filtering for robust out-of-domain generalization, and
Bayesian meta-knowledge integration for fine-grained classification.
Experimental results on ten open-source datasets under various imbalanced
conditions, including cross-task, small-sample, and unseen-sample scenarios,
demonstrate the superiority of UBMF, achieving an average improvement of 42.22%
across ten Any-way 1-5-shot diagnostic tasks. This integrated framework
effectively enhances diagnostic accuracy, generalization, and adaptability,
providing a reliable solution for complex industrial fault diagnosis.; 24) Dynamic Rank Adjustment in Diffusion Policies for Efficient and Flexible
  Training; Diffusion policies trained via offline behavioral cloning have recently
gained traction in robotic motion generation. While effective, these policies
typically require a large number of trainable parameters. This model size
affords powerful representations but also incurs high computational cost during
training. Ideally, it would be beneficial to dynamically adjust the trainable
portion as needed, balancing representational power with computational
efficiency. For example, while overparameterization enables diffusion policies
to capture complex robotic behaviors via offline behavioral cloning, the
increased computational demand makes online interactive imitation learning
impractical due to longer training time. To address this challenge, we present
a framework, called DRIFT, that uses the Singular Value Decomposition to enable
dynamic rank adjustment during diffusion policy training. We implement and
demonstrate the benefits of this framework in DRIFT-DAgger, an imitation
learning algorithm that can seamlessly slide between an offline bootstrapping
phase and an online interactive phase. We perform extensive experiments to
better understand the proposed framework, and demonstrate that DRIFT-DAgger
achieves improved sample efficiency and faster training with minimal impact on
model performance.; 25) Decision-Theoretic Approaches in Learning-Augmented Algorithms; In this work, we initiate the systemic study of decision-theoretic metrics in
the design and analysis of algorithms with machine-learned predictions. We
introduce approaches based on both deterministic measures such as
distance-based evaluation, that help us quantify how close the algorithm is to
an ideal solution, as well as stochastic measures that allow us to balance the
trade-off between the algorithm's performance and the risk associated with the
imperfect oracle. These approaches help us quantify the algorithmic performance
across the entire spectrum of prediction error, unlike several previous works
that focus on few, and often extreme values of the error. We apply these
techniques to two well-known problems from resource allocation and online
decision making, namely contract scheduling and 1-max search.; 26) Arbitrary-Threshold Fully Homomorphic Encryption with Lower Complexity; Threshold fully homomorphic encryption (ThFHE) enables multiple parties to
compute functions over their sensitive data without leaking data privacy. Most
of existing ThFHE schemes are restricted to full threshold and require the
participation of \textit{all} parties to output computing results. Compared
with these full-threshold schemes, arbitrary threshold (ATh)-FHE schemes are
robust to non-participants and can be a promising solution to many real-world
applications. However, existing AThFHE schemes are either inefficient to be
applied with a large number of parties $N$ and a large data size $K$, or
insufficient to tolerate all types of non-participants. In this paper, we
propose an AThFHE scheme to handle all types of non-participants with lower
complexity over existing schemes. At the core of our scheme is the reduction
from AThFHE construction to the design of a new primitive called
\textit{approximate secret sharing} (ApproxSS). Particularly, we formulate
ApproxSS and prove the correctness and security of AThFHE on top of
arbitrary-threshold (ATh)-ApproxSS's properties. Such a reduction reveals that
existing AThFHE schemes implicitly design ATh-ApproxSS following a similar idea
called ``noisy share''. Nonetheless, their ATh-ApproxSS design has high
complexity and become the performance bottleneck. By developing ATASSES, an
ATh-ApproxSS scheme based on a novel ``encrypted share'' idea, we reduce the
computation (resp. communication) complexity from $\mathcal{O}(N^2K)$ to
$\mathcal{O}(N^2+K)$ (resp. from $\mathcal{O}(NK)$ to $\mathcal{O}(N+K)$). We
not only theoretically prove the (approximate) correctness and security of
ATASSES, but also empirically evaluate its efficiency against existing
baselines. Particularly, when applying to a system with one thousand parties,
ATASSES achieves a speedup of $3.83\times$ -- $15.4\times$ over baselines.; 27) Goal-Oriented Middleware Filtering at Transport Layer Based on Value of
  Updates; This work explores employing the concept of goal-oriented (GO) semantic
communication for real-time monitoring and control. Generally, GO communication
advocates for the deep integration of application targets into the network
design. We consider CPS and IoT applications where sensors generate a
tremendous amount of network traffic toward monitors or controllers. Here, the
practical introduction of GO communication must address several challenges.
These include stringent timing requirements, challenging network setups, and
limited computing and communication capabilities of the devices involved.
Moreover, real-life CPS deployments often rely on heterogeneous communication
standards prompted by specific hardware. To address these issues, we introduce
a middleware design of a GO distributed Transport Layer (TL) framework for
control applications. It offers end-to-end performance improvements for diverse
setups and transmitting hardware. The proposed TL protocol evaluates the Value
of sampled state Updates (VoU) for the application goal. It decides whether to
admit or discard the corresponding packets, thus offloading the network. VoU
captures the contribution of utilizing the updates at the receiver into the
application's performance. We introduce a belief network and the augmentation
procedure used by the sensor to predict the evolution of the control process,
including possible delays and losses of status updates in the network. The
prediction is made either using a control model dynamics or a Long-Short Term
Memory neural network approach. We test the performance of the proposed TL in
the experimental framework using Industrial IoT Zolertia ReMote sensors. We
show that while existing approaches fail to deliver sufficient control
performance, our VoU-based TL scheme ensures stability and performs
$\sim$$60\%$ better than the naive GO TL we proposed in our previous work.; 28) Confidence intervals for functionals in constrained inverse problems via
  data-adaptive sampling-based calibration; We address functional uncertainty quantification for ill-posed inverse
problems where it is possible to evaluate a possibly rank-deficient forward
model, the observation noise distribution is known, and there are known
parameter constraints. We present four constraint-aware confidence intervals
extending the work of Batlle et al. (2023) by making the intervals both
computationally feasible and less conservative. Our approach first shrinks the
potentially unbounded constraint set compact in a data-adaptive way, obtains
samples of the relevant test statistic inside this set to estimate a quantile
function, and then uses these computed quantities to produce the intervals. Our
data-adaptive bounding approach is based on the approach by Berger and Boos
(1994), and involves defining a subset of the constraint set where the true
parameter exists with high probability. This probabilistic guarantee is then
incorporated into the final coverage guarantee in the form of an uncertainty
budget. We then propose custom sampling algorithms to efficiently sample from
this subset, even when the parameter space is high-dimensional.
Optimization-based interval methods formulate confidence interval computation
as two endpoint optimizations, where the optimization constraints can be set to
achieve different types of interval calibration while seamlessly incorporating
parameter constraints. However, choosing valid optimization constraints has
been elusive. We show that all four proposed intervals achieve nominal coverage
for a particular functional both theoretically and in practice, with numerical
examples demonstrating superior performance of our intervals over the OSB
interval in terms of both coverage and expected length. In particular, we show
the superior performance in a realistic unfolding simulation from high-energy
physics that is severely ill-posed and involves a rank-deficient forward model.; 29) Deep Dynamic Probabilistic Canonical Correlation Analysis; This paper presents Deep Dynamic Probabilistic Canonical Correlation Analysis
(D2PCCA), a model that integrates deep learning with probabilistic modeling to
analyze nonlinear dynamical systems. Building on the probabilistic extensions
of Canonical Correlation Analysis (CCA), D2PCCA captures nonlinear latent
dynamics and supports enhancements such as KL annealing for improved
convergence and normalizing flows for a more flexible posterior approximation.
D2PCCA naturally extends to multiple observed variables, making it a versatile
tool for encoding prior knowledge about sequential datasets and providing a
probabilistic understanding of the system's dynamics. Experimental validation
on real financial datasets demonstrates the effectiveness of D2PCCA and its
extensions in capturing latent dynamics.; 30) Ask in Any Modality: A Comprehensive Survey on Multimodal
  Retrieval-Augmented Generation; Large Language Models (LLMs) struggle with hallucinations and outdated
knowledge due to their reliance on static training data. Retrieval-Augmented
Generation (RAG) mitigates these issues by integrating external dynamic
information enhancing factual and updated grounding. Recent advances in
multimodal learning have led to the development of Multimodal RAG,
incorporating multiple modalities such as text, images, audio, and video to
enhance the generated outputs. However, cross-modal alignment and reasoning
introduce unique challenges to Multimodal RAG, distinguishing it from
traditional unimodal RAG. This survey offers a structured and comprehensive
analysis of Multimodal RAG systems, covering datasets, metrics, benchmarks,
evaluation, methodologies, and innovations in retrieval, fusion, augmentation,
and generation. We precisely review training strategies, robustness
enhancements, and loss functions, while also exploring the diverse Multimodal
RAG scenarios. Furthermore, we discuss open challenges and future research
directions to support advancements in this evolving field. This survey lays the
foundation for developing more capable and reliable AI systems that effectively
leverage multimodal dynamic external knowledge bases. Resources are available
at https://github.com/llm-lab-org/Multimodal-RAG-Survey.; 31) Light drag in an Optomechanical system; Light dragging refers to the change in the path of light passing through a
moving medium. This effect enables accurate detection of very slow speeds of
light, which have prominent applications in state transfer, quantum gate
operations, and quantum memory implementations. Here, to the best of our
knowledge, we demonstrate the existence of the light-dragging effect in an
optomechanical system (OMS) for the first time. The origin of this key factor
arises from the nonlinear effects linked to optomechanical-induced transparency
(OMIT). Hence, we observe prominent effects in the group and refractive indices
profile spectra related to optomechanical parameters such as the decay rate of
the cavity field, the mirror's damping momentum rate, and mechanical frequency.
We find out that lateral light drag depends on the detuning by altering the
amplitude and direction of the translational velocity. This allowed us to
change the light's propagation through the optomechanical cavity from
superluminal to subluminal and vice versa by modifying the probe's detuning.
The ability to manipulate and control the light drag through an optomechanical
system might be useful in designing novel optical devices and systems with
enhanced performance.; 32) Applications of de Sitter Causality Conditions; We find explicit de Sitter shockwave solutions in arbitrary spacetime
dimensions. We use these to determine the dimensional-dependent ""stretching"" of
the de Sitter Penrose diagram in the presence of a shock or black hole. This
stretching sets the scale at which superluminalities in de Sitter can be
considered resolvable. We then consider an RFF coupling for a spin-1 particle
and a Gauss-Bonnet coupling for a spin-2 particle and analyze these in terms of
our proposed de Sitter causality criteria. We briefly discuss the connections
between our results and corresponding results in anti de Sitter spacetime.; 33) An Attempt to Unraveling Token Prediction Refinement and Identifying
  Essential Layers of Large Language Models; This research aims to unravel how large language models (LLMs) iteratively
refine token predictions (or, in a general sense, vector predictions). We
utilized a logit lens technique to analyze the model's token predictions
derived from intermediate representations. Specifically, we focused on how LLMs
access and use information from input contexts, and how positioning of relevant
information affects the model's token prediction refinement process. Our
findings for multi-document question answering task, by varying input context
lengths (the number of documents), using GPT-2, revealed that the number of
layers between the first layer that the model predicted next tokens correctly
and the later layers that the model finalized its correct predictions, as a
function of the position of relevant information (i.e., placing the relevant
one at the beginning, middle, or end of the input context), has a nearly
inverted U shape. We found that the gap between these two layers, on average,
diminishes when relevant information is positioned at the beginning or end of
the input context, suggesting that the model requires more refinements when
processing longer contexts with relevant information situated in the middle,
and highlighting which layers are essential for determining the correct output.
Our analysis provides insights about how token predictions are distributed
across different conditions, and establishes important connections to existing
hypotheses and previous findings in AI safety research and development.; 34) Higher-order chiral scalar from boundary reduction of 3d higher-spin
  gravity; We use a recently proposed covariant procedure to reduce the Chern-Simons
action of three-dimensional higher-spin gravity to the boundary, resulting in a
Lorentz covariant action for higher-order chiral scalars. After gauge-fixing,
we obtain a higher-derivative action generalizing the $s=1$ Floreanini-Jackiw
and $s=2$ Alekseev-Shatashvili actions to arbitrary spin $s$. For simplicity,
we treat the case of general spin at the linearized level, while the full
non-linear asymptotic boundary conditions are presented in component form for
the $SL(3,\mathbb R)$ case. Finally, we extend the spin-3 linearized analysis
to a background with non-trivial higher-spin charge and show that it has a
richer structure of zero modes.; 35) Cross Section Measurements of Large Angle Fragments Production in the
  Interaction of Carbon Ion Beams with Thin Targets; The fragmentation cross sections of carbon ion beams with kinetic energies of
$115 - 353~\text{MeV/u}$ impinging on thin targets of graphite (C),
polyvinyl-toluene (C$_9$H$_{10}$) and PMMA (C$_2$O$_5$H$_8$) have been measured
at 90$^{\text{o}}$ and 60$^{\text{o}}$ at the CNAO particle therapy center
(Pavia, Italy). The presented measurements are a complete reanalysis by the
FOOT collaboration of already published elemental cross section on composite
targets, in order to refine the analysis, improve the systematic uncertainties
and show the comparison with the FLUKA Monte Carlo code calculations. In this
work, the kinetic energy at production of measured fragments has been
completely redefined, together with the efficiencies computation. The new
analysis strategy has been successfully validated against the Monte Carlo cross
sections. Two detection arms were positioned at two different angles to perform
the measurement at 90$^{\text{o}}$ and 60$^{\text{o}}$. The fragment species
have been identified in charge (Z$_{id}$ = H) and mass (M$_{id}$ = $^1$H,
$^2$H, $^3$H) combining the information of the deposited energy in thin plastic
scintillators, of the deposited energy in a thick LYSO crystal and of the
fragments Time of Flight (ToF) measurement. The ToF was also used to compute
the fragments measured kinetic energy. The cross sections are presented as a
function of the fragments kinetic energy at production thanks to an unfolding
technique applied to the data.; 36) Structure factors and quantum geometry in multiband BCS superconductors; We consider multiband BCS superconductors that exhibit time-reversal symmetry
and uniform pairing, and analyze their dynamic density and spin structure
factors using linear-response theory within the mean-field BCS-BEC crossover
framework at zero temperature. Our results for the multi-orbital Hubbard model
satisfy the associated f-sum rules in several limits. In particular, in the
strong-coupling limit, they coincide with those of a weakly-interacting Bose
gas of Cooper pairs, where the low-energy collective Goldstone modes serve as
Bogoliubov phonons. We further reveal that the quantum-geometric origin of the
low-energy structure factors, along with related observables such as the
superfluid-weight tensor and the effective-mass tensor of Cooper pairs, can be
traced all the way back to the effective-mass theorem for Bloch bands in this
limit. As an illustration, we investigate the pyrochlore-Hubbard model
numerically and demonstrate that the Goldstone modes are the only relevant
collective degrees of freedom in the flat-band regime.; 37) Defect Phonon Renormalization during Nonradiative Multiphonon
  Transitions in Semiconductors; As a typical nonradiative multiphonon transition in semiconductors, carrier
capture at defects is critical to the performance of semiconductor devices. Its
transition rate is usually calculated using the equal-mode approximation, which
assumes that phonon modes and frequencies remain unchanged before and after the
transition. Using the carbon substitutional defect ($\text{C}_\text{N}$) in GaN
as a benchmark, here we demonstrate that the phonon renormalization can be
significant during defect relaxation, which causes errors as large as orders of
magnitude in the approximation. To address this issue, we consider (i)
Duschinsky matrix connecting the initial-state and final-state phonons, which
accounts for the changes in phonon modes and frequencies; and (ii) the
off-diagonal contributions in total transition matrix element, which
incorporates the cross terms of electron-phonon interactions between different
modes. With this improvement, the calculated transition rates show agreements
with experimental results within an order of magnitude. We believe the present
method makes one step forward for the accurate calculation of multiphonon
transition rate, especially in cases with large defect relaxations.; 38) Argument-Based Comparative Question Answering Evaluation Benchmark; In this paper, we aim to solve the problems standing in the way of automatic
comparative question answering. To this end, we propose an evaluation framework
to assess the quality of comparative question answering summaries. We formulate
15 criteria for assessing comparative answers created using manual annotation
and annotation from 6 large language models and two comparative question
asnwering datasets. We perform our tests using several LLMs and manual
annotation under different settings and demonstrate the constituency of both
evaluations. Our results demonstrate that the Llama-3 70B Instruct model
demonstrates the best results for summary evaluation, while GPT-4 is the best
for answering comparative questions. All used data, code, and evaluation
results are publicly
available\footnote{\url{https://anonymous.4open.science/r/cqa-evaluation-benchmark-4561/README.md}}.; 39) Multi-view Video-Pose Pretraining for Operating Room Surgical Activity
  Recognition; Understanding the workflow of surgical procedures in complex operating rooms
requires a deep understanding of the interactions between clinicians and their
environment. Surgical activity recognition (SAR) is a key computer vision task
that detects activities or phases from multi-view camera recordings. Existing
SAR models often fail to account for fine-grained clinician movements and
multi-view knowledge, or they require calibrated multi-view camera setups and
advanced point-cloud processing to obtain better results. In this work, we
propose a novel calibration-free multi-view multi-modal pretraining framework
called Multiview Pretraining for Video-Pose Surgical Activity Recognition
PreViPS, which aligns 2D pose and vision embeddings across camera views. Our
model follows CLIP-style dual-encoder architecture: one encoder processes
visual features, while the other encodes human pose embeddings. To handle the
continuous 2D human pose coordinates, we introduce a tokenized discrete
representation to convert the continuous 2D pose coordinates into discrete pose
embeddings, thereby enabling efficient integration within the dual-encoder
framework. To bridge the gap between these two modalities, we propose several
pretraining objectives using cross- and in-modality geometric constraints
within the embedding space and incorporating masked pose token prediction
strategy to enhance representation learning. Extensive experiments and ablation
studies demonstrate improvements over the strong baselines, while
data-efficiency experiments on two distinct operating room datasets further
highlight the effectiveness of our approach. We highlight the benefits of our
approach for surgical activity recognition in both multi-view and single-view
settings, showcasing its practical applicability in complex surgical
environments. Code will be made available at:
https://github.com/CAMMA-public/PreViPS.; 40) MDCrow: Automating Molecular Dynamics Workflows with Large Language
  Models; Molecular dynamics (MD) simulations are essential for understanding
biomolecular systems but remain challenging to automate. Recent advances in
large language models (LLM) have demonstrated success in automating complex
scientific tasks using LLM-based agents. In this paper, we introduce MDCrow, an
agentic LLM assistant capable of automating MD workflows. MDCrow uses
chain-of-thought over 40 expert-designed tools for handling and processing
files, setting up simulations, analyzing the simulation outputs, and retrieving
relevant information from literature and databases. We assess MDCrow's
performance across 25 tasks of varying required subtasks and difficulty, and we
evaluate the agent's robustness to both difficulty and prompt style.
\texttt{gpt-4o} is able to complete complex tasks with low variance, followed
closely by \texttt{llama3-405b}, a compelling open-source model. While prompt
style does not influence the best models' performance, it has significant
effects on smaller models.; 41) Separation and excision in functor homology; We prove separation and excision results in functor homology. These results
explain how the global Steinberg decomposition of functors proved by Djament,
Touz{\'e} and Vespa behaves in Ext and Tor computations.; 42) Halilsoy and Chandrasekhar standing gravitational waves in the linear
  approximation; Halilsoy and Chandrasekhar cylindrical standing gravitational waves
correspond to two different classes of solutions to the vacuum Einstein
equations. Both families satisfy the definition of standing gravitational waves
proposed by Stephani, but only the latter class fulfills the stricter
definition introduced by Chandrasekhar. The aim of this research is to compare
both classes of solutions within the linear regime. We discover that the
linearized Halilsoy and Chandrasekhar standing waves are gravitational
analogues of two different types of electromagnetic polarization standing waves
- a phenomenon not previously discussed in the literature for exact solutions
to the Einstein equations; 43) Theoretical Characterization of Effect of Masks in Snapshot Compressive
  Imaging; Snapshot compressive imaging (SCI) refers to the recovery of
three-dimensional data cubes-such as videos or hyperspectral images-from their
two-dimensional projections, which are generated by a special encoding of the
data with a mask. SCI systems commonly use binary-valued masks that follow
certain physical constraints. Optimizing these masks subject to these
constraints is expected to improve system performance. However, prior
theoretical work on SCI systems focuses solely on independently and identically
distributed (i.i.d.) Gaussian masks, which do not permit such optimization. On
the other hand, existing practical mask optimizations rely on computationally
intensive joint optimizations that provide limited insight into the role of
masks and are expected to be sub-optimal due to the non-convexity and
complexity of the optimization. In this paper, we analytically characterize the
performance of SCI systems employing binary masks and leverage our analysis to
optimize hardware parameters. Our findings provide a comprehensive and
fundamental understanding of the role of binary masks - with both independent
and dependent elements - and their optimization. We also present simulation
results that confirm our theoretical findings and further illuminate different
aspects of mask design.; 44) Gemini Embedding: Generalizable Embeddings from Gemini; In this report, we introduce Gemini Embedding, a state-of-the-art embedding
model leveraging the power of Gemini, Google's most capable large language
model. Capitalizing on Gemini's inherent multilingual and code understanding
capabilities, Gemini Embedding produces highly generalizable embeddings for
text spanning numerous languages and textual modalities. The representations
generated by Gemini Embedding can be precomputed and applied to a variety of
downstream tasks including classification, similarity, clustering, ranking, and
retrieval. Evaluated on the Massive Multilingual Text Embedding Benchmark
(MMTEB), which includes over one hundred tasks across 250+ languages, Gemini
Embedding substantially outperforms prior state-of-the-art models,
demonstrating considerable improvements in embedding quality. Achieving
state-of-the-art performance across MMTEB's multilingual, English, and code
benchmarks, our unified model demonstrates strong capabilities across a broad
selection of tasks and surpasses specialized domain-specific models.; 45) On Terwilliger $\mathbb{F}$-algebras of direct products of group
  divisible association schemes; The Terwilliger algebras of association schemes over an arbitrary field
$\mathbb{F}$ were briefly called the Terwilliger $\mathbb{F}$-algebras of
association schemes in [9]. In this paper, the Terwilliger
$\mathbb{F}$-algebras of direct products of group divisible association schemes
are studied. The centers, the semisimplicity, the Jacobson radicals and their
nilpotent indices, the Wedderburn-Artin decompositions of the Terwilliger
$\mathbb{F}$-algebras of direct products of group divisible association schemes
are obtained.; 46) Efficient Function-as-a-Service for Large Language Models with TIDAL; Large Language Model (LLM) applications have emerged as a prominent use case
for Function-as-a-Service (FaaS) due to their high computational demands and
sporadic invocation patterns. However, serving LLM functions within FaaS
frameworks faces significant GPU-side cold start. A fundamental approach
involves leveraging a template with function state saved on GPUs to bypass the
cold start for new invocations. Yet, this approach struggles with the high GPU
footprint, dynamic initialization behaviors, and lazy GPU kernel loading
inherent in LLM functions, primarily due to a lack of insight into the
underlying execution details. In this paper, we introduce TIDAL, an optimized
FaaS framework for LLM applications that achieves fast startups by tracing
fine-grained execution paths. By utilizing the traced execution details, TIDAL
generates adaptive function templates, effectively breaking startup barriers
for LLM functions. Extensive evaluations demonstrate that TIDAL reduces cold
start latency by $1.79\times\text{\textasciitilde}2.11\times$ and improves the
$95\%$-ile time-to-first-token by $76.0\%$, surpassing state-of-the-art
methods.; 47) Human-centered Geospatial Data Science; This entry provides an overview of Human-centered Geospatial Data Science,
highlighting the gaps it aims to bridge, its significance, and its key topics
and research. Geospatial Data Science, which derives geographic knowledge and
insights from large volumes of geospatial big data using advanced Geospatial
Artificial Intelligence (GeoAI), has been widely used to tackle a wide range of
geographic problems. However, it often overlooks the subjective human
experiences that fundamentally influence human-environment interactions, and
few strategies have been developed to ensure that these technologies follow
ethical guidelines and prioritize human values. Human-centered Geospatial Data
Science advocates for two primary focuses. First, it advances our understanding
of human-environment interactions by leveraging Geospatial Data Science to
measure and analyze human subjective experiences at place including emotion,
perception, cognition, and creativity. Second, it advocates for the development
of responsible and ethical Geospatial Data Science methods that protect
geoprivacy, enhance fairness and reduce bias, and improve the explainability
and transparency of geospatial technologies. With these two missions,
Human-centered Geospatial Data Sciences brings a fresh perspective to develop
and utilize geospatial technologies that positively impact society and benefit
human well-being and the humanities.; 48) Joint Delay-Doppler Estimation using OFDMA Payloads for Integrated
  Sensing and Communications; The use of future communication systems for sensing offers the potential for
a number of new applications. In this paper, we show that leveraging user data
payloads in multi-node Orthogonal Frequency Division Multiple Access (OFDMA)
networks for estimating target delay and Doppler-shift parameters can yield a
significant advantage in SNR and addressable bandwidth. However, gaps in the
frequency-time resources, reference signal boosting and amplitude modulation
schemes introduce challenges for estimation at the sensing receiver.
  In this work, we propose a joint delay and Doppler-shift model-based
estimator designed to address these challenges. Furthermore, we demonstrate
that incorporating knowledge of the device model into the estimation procedure
helps mitigate the effects of the non-ideal radar ambiguity function caused by
amplitude-modulated user payloads and sparse reference signals. Simulation
results demonstrate that the estimator achieves the theoretical lower bound on
estimation variance.; 49) Ultrafast neuromorphic computing with nanophotonic optical parametric
  oscillators; Over the past decade, artificial intelligence (AI) has led to disruptive
advancements in fundamental sciences and everyday technologies. Among various
machine learning algorithms, deep neural networks have become instrumental in
revealing complex patterns in large datasets with key applications in computer
vision, natural language processing, and predictive analytics. On-chip photonic
neural networks offer a promising platform that leverage high bandwidths and
low propagation losses associated with optical signals to perform analog
computations for deep learning. However, nanophotonic circuits are yet to
achieve the required linear and nonlinear operations simultaneously in an
all-optical and ultrafast fashion. Here, we report an ultrafast nanophotonic
neuromorphic processor using an optical parametric oscillator (OPO) fabricated
on thin-film lithium niobate (TFLN). The input data is used to modulate the
optical pulses synchronously pumping the OPO. The consequent signal pulses
generated by the OPO are coupled to one another via the nonlinear delayed
dynamics of the OPO, thus forming the internal nodes of a deep recurrent neural
network. We use such a nonlinearly coupled OPO network for chaotic time series
prediction, nonlinear error correction in a noisy communication channel, as
well as noisy waveform classification and achieve accuracies exceeding 93% at
an operating clock rate of ~ 10 GHz. Our OPO network is capable of achieving
sub-nanosecond latencies, a timescale comparable to a single clock cycle in
state-of-the-art digital electronic processors. By circumventing the need for
optical-electronic-optical (OEO) conversions, our ultrafast nanophotonic neural
network paves the way for the next generation of compact all-optical
neuromorphic processors with ultralow latencies and high energy efficiencies.; 50) IoT-enabled Drowsiness Driver Safety Alert System with Real-Time
  Monitoring Using Integrated Sensors Technology; Significant losses in terms of life and property occur from road traffic
accidents, which are often caused by drunk and drowsy drivers. Reducing
accidents requires effective detection of alcohol impairment and drowsiness as
well as real-time driver monitoring. This paper aims to create an Internet of
Things (IoT)--enabled Drowsiness Driver Safety Alert System with Real-Time
Monitoring Using Integrated Sensors Technology. The system features an alcohol
sensor and an IR sensor for detecting alcohol presence and monitoring driver
eye movements, respectively. Upon detecting alcohol, alarms and warning lights
are activated, the vehicle speed is progressively reduced, and the motor stops
within ten to fifteen seconds if the alcohol presence persists. The IR sensor
monitors prolonged eye closure, triggering alerts, or automatic vehicle
stoppage to prevent accidents caused by drowsiness. Data from the IR sensor is
transmitted to a mobile phone via Bluetooth for real-time monitoring and
alerts. By identifying driver alcoholism and drowsiness, this system seeks to
reduce accidents and save lives by providing safer transportation.; 51) Would you mind being watched by machines? Privacy concerns in data
  mining; Data mining is not an invasion of privacy because access to data is only by
machines, not by people: this is the argument that is investigated here. The
current importance of this problem is developed in a case study of data mining
in the USA for counterterrorism and other surveillance purposes. After a
clarification of the relevant nature of privacy, it is argued that access by
machines cannot warrant the access to further information, since the analysis
will have to be made either by humans or by machines that understand. It
concludes that the current data mining violates the right to privacy and should
be subject to the standard legal constraints for access to private information
by people.; 52) Transport approach to two-qubit quantum state tomography; Quantum state tomography (QST) is a central task for quantum information
processing, enabling quantum cryptography, computation, and state
certification. Traditional QST relies on projective measurements of single- and
two-qubit Pauli operators, requiring qubits to be isolated from environmental
dissipation. In this work, we demonstrate that measuring currents and
associated transport quantities flowing through a two-qubit system between two
terminals biased in temperature or voltage are sufficient to perform complete
QST of the open quantum system. This transport approach requires minimal
knowledge of the system-environment couplings and of the parameters setting the
system's dynamics, accessible in state-of-the-art solid-state experiments via
spectroscopic measurements for instance. Our findings are analytical, offering
comprehensive insights into the underlying processes. As a direct consequence
of our approach, we are able to provide a transport-based entanglement measure
to certify the presence of quantum correlations, expressing the concurrence in
terms of currents and correlations functions only.; 53) ISAC MIMO Systems with OTFS Waveforms and Virtual Arrays; A novel Integrated Sensing-Communication (ISAC) system is proposed that can
accommodate high mobility scenarios while making efficient use of bandwidth for
both communication and sensing. The system comprises a monostatic
multiple-input multiple-output (MIMO) radar that transmits orthogonal time
frequency space (OTFS) waveforms. Bandwidth efficiency is achieved by making
Doppler-delay (DD) domain bins available for shared use by the transmit
antennas. For maximum communication rate, all DD-domain bins are used as
shared, but in this case, the target resolution is limited by the aperture of
the receive array. A low-complexity method is proposed for obtaining coarse
estimates of the radar targets parameters in that case. A novel approach is
also proposed to construct a virtual array (VA) for achieving a target
resolution higher than that allowed by the receive array. The VA is formed by
enforcing zeros on certain time-frequency (TF) domain bins, thereby creating
private bins assigned to specific transmit antennas. The TF signals received on
these private bins are orthogonal, enabling the synthesis of a VA. When
combined with coarse target estimates, this approach provides high-accuracy
target estimation. To preserve DD-domain information, the introduction of
private bins requires reducing the number of DD-domain symbols, resulting in a
trade-off between communication rate and sensing performance. However, even a
small number of private bins is sufficient to achieve significant sensing gains
with minimal communication rate loss. The proposed system is robust to Doppler
frequency shifts that arise in high mobility scenarios.; 54) SuperCap: Multi-resolution Superpixel-based Image Captioning; It has been a longstanding goal within image captioning to move beyond a
dependence on object detection. We investigate using superpixels coupled with
Vision Language Models (VLMs) to bridge the gap between detector-based
captioning architectures and those that solely pretrain on large datasets. Our
novel superpixel approach ensures that the model receives object-like features
whilst the use of VLMs provides our model with open set object understanding.
Furthermore, we extend our architecture to make use of multi-resolution inputs,
allowing our model to view images in different levels of detail, and use an
attention mechanism to determine which parts are most relevant to the caption.
We demonstrate our model's performance with multiple VLMs and through a range
of ablations detailing the impact of different architectural choices. Our full
model achieves a competitive CIDEr score of $136.9$ on the COCO Karpathy split.; 55) How to Sell a Service with Uncertain Outcomes; Motivated by the recent popularity of machine learning training services, we
introduce a contract design problem in which a provider sells a service that
results in an outcome of uncertain quality for the buyer. The seller has a set
of actions that lead to different distributions over outcomes. We focus on a
setting in which the seller has the ability to commit to an action and the
buyer is free to accept or reject the outcome after seeing its realized
quality. We propose a two-stage payment scheme where the seller designs a menu
of contracts, each of which specifies an action, an upfront price and a vector
of outcome-dependent usage prices. Upon selecting a contract, the buyer pays
the upfront price, and after observing the realized outcome, the buyer either
accepts and pays the corresponding usage price, or rejects and is exempt from
further payment. We show that this two-stage payment structure is necessary to
maximize profit: only upfront prices or only usage prices is insufficient. We
then study the computational complexity of computing a profit-maximizing menu
in our model. While computing the exact maximum seller profit is NP-hard even
for two buyer types, we derive a fully-polynomial time approximation scheme
(FPTAS) for the maximum profit for a constant number of buyer types. Finally,
we prove that in the single-parameter setting in which buyers' valuations are
parametrized by a single real number that seller revenue can be maximized using
a menu consisting of a single contract.; 56) Polynomial-Time Approximability of Constrained Reinforcement Learning; We study the computational complexity of approximating general constrained
Markov decision processes. Our primary contribution is the design of a
polynomial time $(0,\epsilon)$-additive bicriteria approximation algorithm for
finding optimal constrained policies across a broad class of recursively
computable constraints, including almost-sure, chance, expectation, and their
anytime variants. Matching lower bounds imply our approximation guarantees are
optimal so long as $P \neq NP$. The generality of our approach results in
answers to several long-standing open complexity questions in the constrained
reinforcement learning literature. Specifically, we are the first to prove
polynomial-time approximability for the following settings: policies under
chance constraints, deterministic policies under multiple expectation
constraints, policies under non-homogeneous constraints (i.e., constraints of
different types), and policies under constraints for continuous-state
processes.; 57) Representation Learning to Advance Multi-institutional Studies with
  Electronic Health Record Data; The adoption of EHRs has expanded opportunities to leverage data-driven
algorithms in clinical care and research. A major bottleneck in effectively
conducting multi-institutional EHR studies is the data heterogeneity across
systems with numerous codes that either do not exist or represent different
clinical concepts across institutions. The need for data privacy further limits
the feasibility of including multi-institutional patient-level data required to
study similarities and differences across patient subgroups. To address these
challenges, we developed the GAME algorithm. Tested and validated across 7
institutions and 2 languages, GAME integrates data in several levels: (1) at
the institutional level with knowledge graphs to establish relationships
between codes and existing knowledge sources, providing the medical context for
standard codes and their relationship to each other; (2) between institutions,
leveraging language models to determine the relationships between
institution-specific codes with established standard codes; and (3) quantifying
the strength of the relationships between codes using a graph attention
network. Jointly trained embeddings are created using transfer and federated
learning to preserve data privacy. In this study, we demonstrate the
applicability of GAME in selecting relevant features as inputs for AI-driven
algorithms in a range of conditions, e.g., heart failure, rheumatoid arthritis.
We then highlight the application of GAME harmonized multi-institutional EHR
data in a study of Alzheimer's disease outcomes and suicide risk among patients
with mental health disorders, without sharing patient-level data outside
individual institutions.; 58) Discrete curve theory in space forms: planar elastic and
  area-constrained elastic curves; We propose a notion of discrete elastic and area-constrained elastic curves
in 2-dimensional space forms. Our definition extends the well-known discrete
Euclidean curvature equation to space forms and reflects various geometric
properties known from their smooth counterparts. Special emphasis is paid to
discrete flows built from B\""acklund transformations in the respective space
forms. The invariants of the flows form a hierarchy of curves and we show that
discrete elastic and constrained elastic curves can be characterized as
elements of this hierarchy. This work also includes an introductory chapter on
discrete curve theory in space forms, where we find discrete Frenet-type
formulas and describe an associated family related to a fundamental theorem.; 59) Towards Automated Self-Supervised Learning for Truly Unsupervised Graph
  Anomaly Detection; Self-supervised learning (SSL) is an emerging paradigm that exploits
supervisory signals generated from the data itself, and many recent studies
have leveraged SSL to conduct graph anomaly detection. However, we empirically
found that three important factors can substantially impact detection
performance across datasets: 1) the specific SSL strategy employed; 2) the
tuning of the strategy's hyperparameters; and 3) the allocation of combination
weights when using multiple strategies. Most SSL-based graph anomaly detection
methods circumvent these issues by arbitrarily or selectively (i.e., guided by
label information) choosing SSL strategies, hyperparameter settings, and
combination weights. While an arbitrary choice may lead to subpar performance,
using label information in an unsupervised setting is label information leakage
and leads to severe overestimation of a method's performance. Leakage has been
criticized as ""one of the top ten data mining mistakes"", yet many recent
studies on SSL-based graph anomaly detection have been using label information
to select hyperparameters. To mitigate this issue, we propose to use an
internal evaluation strategy (with theoretical analysis) to select
hyperparameters in SSL for unsupervised anomaly detection. We perform extensive
experiments using 10 recent SSL-based graph anomaly detection algorithms on
various benchmark datasets, demonstrating both the prior issues with
hyperparameter selection and the effectiveness of our proposed strategy.; 60) Emergent Dynamical Ising Transition in Diffusive Sandpiles; Minimally stable site (MSS) clusters play a dominant role in shaping
avalanches in the self-organized critical (SOC) systems. The manipulation of
MSS clusters through local smoothings (diffusion) alter the MSS landscape,
suppressing rare avalanches and postponing them until they manifest as spanning
avalanches. By leveraging the Inverse Ising problem, we uncover a duality
between diffusive sandpiles and equilibrium statistical physics. Our analysis
reveals an emergent magnetic instability in the dual Ising model, coinciding
with the formation of spanning avalanches and marking a transition to a
correlated percolation regime. At this point, the MSS loop soups exhibit
fractal self-similarity and power-law distributions, while the effective
pairwise interactions in the dual system vanish, signaling a magnetic
transition characterized by abrupt changes in magnetization and spin
susceptibility. Crucially, we show that diffusion fundamentally reshapes
avalanche dynamics: the spatial anti-correlations of MSSs in standard SOC
systems transform into positive correlations when diffusion is introduced.
These findings bridge self-organized criticality, percolation theory, and
equilibrium phase transitions, shedding new light on emergent criticality and
large-scale correlations in non-equilibrium systems.; 61) An elementary algebraic proof of the fundamental theorem of algebra; We give a new proof of the fundamental theorem of algebra. It is entirely
elementary, focused on using long division to its fullest extent. Further, the
method quickly recovers a more general version of the theorem recently obtained
by Joseph Shipman.; 62) Construction of the Damped Ly$\alpha$ Absorber Catalog for DESI DR2
  Ly$\alpha$ BAO; We present the Damped Ly$\alpha$ Toolkit for automated detection and
characterization of Damped Ly$\alpha$ absorbers (DLA) in quasar spectra. Our
method uses quasar spectral templates with and without absorption from
intervening DLAs to reconstruct observed quasar forest regions. The
best-fitting model determines whether a DLA is present while estimating the
redshift and HI column density. With an optimized quality cut on detection
significance ($\Delta \chi_{r}^2>0.03$), the technique achieves an estimated
72% purity and 71% completeness when evaluated on simulated spectra with
S/N$>2$ that are free of broad absorption lines (BAL). We provide a catalog
containing candidate DLAs from the DLA Toolkit detected in DESI DR1 quasar
spectra, of which 21,719 were found in S/N$>2$ spectra with predicted
$\log_{10} (N_\texttt{HI}) > 20.3$ and detection significance $\Delta
\chi_{r}^2 >0.03$. We compare the Damped Ly$\alpha$ Toolkit to two alternative
DLA finders based on a convolutional neural network (CNN) and Gaussian process
(GP) models. We present a strategy for combining these three techniques to
produce a high-fidelity DLA catalog from DESI DR2 for the Ly$\alpha$ forest
baryon acoustic oscillation measurement. The combined catalog contains 41,152
candidate DLAs with $\log_{10} (N_\texttt{HI}) > 20.3$ from quasar spectra with
S/N$>2$. We estimate this sample to be approximately 76% pure and 71% complete
when BAL quasars are excluded.; 63) Rough estimates of solar system gravitomagnetic effects in
  post-Newtonian gravity; In order to describe properly the gravity interactions including the mass
currents, in the gravitomagnetism we construct four Maxwell type gravitational
equations which are shown to be analogs of the Maxwell equations in the
electromagnetism. Next, exploiting the Maxwell type gravitational equations, we
explicitly predict the mass magnetic fields for both the isolated system of the
spinning Moon orbiting the spinning Earth and that of the Sun and solar system
planets orbiting the spinning Sun, whose phenomenological values have not been
evaluated in the precedented Newtonian gravity formalisms. In the
gravitomagnetism we also phenomenologically investigate the mass magnetic
general relativity (GR) forces associated with the mass magnetic fields, to
find that they are extremely small but non-vanishing compared to the
corresponding mass electric Newtonian forces. Moreover, the directions of the
mass magnetic GR forces for the solar system planets except Venus and Uranus
are shown to be anti-parallel to those of their mass electric Newtonian forces.
Next we investigate the mass magnetic dipole moment related with the B-ring of
Saturn, to evaluate $\vec{m}_{M}(Ring)=-1.141\times 10^{4}~{\rm
m^{3}~sec^{-1}}~\hat{\omega}$ with $\hat{\omega}$ being the unit vector along
the axis direction of the spinning B-ring. The predicted value of
$\vec{m}_{M}(Ring)$ is shown to be directly related with the Cassini data on
the total mass of the rings of Saturn.; 64) Dusty disks as safe havens for terrestrial planets: Effect of the
  back-reaction of solid material on gas; Previous studies have shown that there is considerable variation in the
dust-to-gas density ratio in the vicinity of low-mass planets undergoing
growth. This can lead to a significant change in the planetary momentum exerted
by the gas and solid material. However, due to the low dust-to-gas mass ratio
of protoplanetary disks, the back-reaction of the solid material, is often
neglected. We study the effect of the back-reaction of solid material on the
torques felt by low-mass planets. We performed locally isothermal, 2D
hydrodynamic simulations of planet-disk interactions. Low-mass planets in the
range of 0.1-10MEarth accrete only solid material. Simulations were compared
with and without taking into account the back-reaction of the solid material on
the gas. The solid component was assumed to have a fixed Stokes number in the
range 0.01-10. In general, the inclusion of back-reaction results in a greater
number of models with positive torque values compared to models that neglect
back-reaction. It is clear, therefore, that the simulation of planetary growth
and migration via hydrodynamic modeling requires the inclusion of solid-gas
back-reaction. As a result of the back-reaction and accretion, a Mars-sized
planetary embryo will experience positive total torques from the disk
containing coupled solid components St<=0.01. Earth-mass planets also
experience positive total torques from the disk containing boulder-sized solid
components 2<=St<=5. The accretion of weakly coupled solid material tends to
increase the positive torques and decrease the negative torques. Our results
suggest that the combined effect of back-reaction and accretion is beneficial
to the formation of planetary systems by reducing the likelihood of a young
planet being engulfed by the central star.; 65) Evaluating the Efficacy and Safety of Stereotactic Arrhythmia
  Radioablation in Ventricular Tachycardia: A Comprehensive Systematic Review
  and Meta-Analysis; Purpose: Stereotactic arrhythmia radioablation (STAR) has emerged as a
promising non-invasive treatment for refractory ventricular tachycardia (VT),
offering a novel alternative for patients who are poor candidates for catheter
ablation. This systematic review and meta-analysis evaluates the safety,
efficacy, and technical aspects of STAR across preclinical studies, case
reports, case series, and clinical trials. Methods and Materials: A systematic
review identified 80 studies published between 2015 and 2024, including 12
preclinical studies, 47 case reports, 15 case series, and 6 clinical trials.
Data on patient demographics, treatment parameters, and clinical outcomes were
extracted. Meta-analyses were performed for pooled mortality rates, VT burden
reduction, and acute toxicities, with subgroup analyses exploring
cardiomyopathy type, age, left ventricular ejection fraction (LVEF), and
treatment modality. Results: The pooled 6- and 12-month mortality rates were
16% (95% CI: 11-21%) and 32% (95% CI: 26-39%), respectively. VT burden
reduction at 6 months was 75% (95% CI: 73-77%), with significant heterogeneity
(I^2 = 98.8%). Grade 3+ acute toxicities were observed in 7% (95% CI: 4-11%),
with pneumonitis being the most common. Subgroup analyses showed comparable
outcomes between LINAC- and CyberKnife-based treatments, with minor differences
based on patient characteristics and cardiomyopathy type. Conclusions: STAR
demonstrates significant potential in reducing VT burden and improving patient
outcomes. While favorable acute safety profiles and efficacy support clinical
adoption, variability in treatment protocols underscores the need for
standardized practices. Future studies should aim to optimize patient
selection, establish robust dosimetric standards, and evaluate long-term
safety.; 66) Quantifying Security Vulnerabilities: A Metric-Driven Security Analysis
  of Gaps in Current AI Standards; As AI systems integrate into critical infrastructure, security gaps in AI
compliance frameworks demand urgent attention. This paper audits and quantifies
security risks in three major AI governance standards: NIST AI RMF 1.0, UK's AI
and Data Protection Risk Toolkit, and the EU's ALTAI. Using a novel risk
assessment methodology, we develop four key metrics: Risk Severity Index (RSI),
Attack Potential Index (AVPI), Compliance-Security Gap Percentage (CSGP), and
Root Cause Vulnerability Score (RCVS). Our analysis identifies 136 concerns
across the frameworks, exposing significant gaps. NIST fails to address 69.23
percent of identified risks, ALTAI has the highest attack vector vulnerability
(AVPI = 0.51) and the ICO Toolkit has the largest compliance-security gap, with
80.00 percent of high-risk concerns remaining unresolved. Root cause analysis
highlights under-defined processes (ALTAI RCVS = 033) and weak implementation
guidance (NIST and ICO RCVS = 0.25) as critical weaknesses. These findings
emphasize the need for stronger, enforceable security controls in AI
compliance. We offer targeted recommendations to enhance security posture and
bridge the gap between compliance and real-world AI risks.; 67) Hausdorff dimension of dynamical coverings under mixing properties; In this article, we estimate the Hausdorff dimension of dynamical coverings
with respect to mixing ergodic systems. More precisely, if the ergodic measure
is exact-dimensionnal, we establish a formula provided that the system is
polynomially fast mixing and if the measure is not exact-dimensionnal, we
establish a similar result under super-polynomial speed of mix assumpetion. As
an application of our result, we extend the result of Fan-Shmeling-Troubetzkoy
for the doubling map on the circle to the case of the times 2, times 3 map on
the two dimensional torus.; 68) A Hierarchical Shock Model of Ultra-High-Energy Cosmic Rays; We propose that a hierarchical shock model$\unicode{x2014}$including
supernova remnant shocks, galactic wind termination shocks, and accretion
shocks around cosmic filaments and galaxy clusters$\unicode{x2014}$can
naturally explain the cosmic ray spectrum from ~1 GeV up to ~200 EeV. While
this framework applies to the entire cosmic ray spectrum, in this work, we
focus on its implications for ultra-high-energy cosmic rays (UHECRs). We
perform a hydrodynamic cosmological simulation to investigate the power
processed at shocks around clusters and filaments. The downstream flux from
nearby shocks around the local filament accounts for the softer, lower-energy
extragalactic component around the ankle, and the upstream escaping flux from
nearby clusters accounts for the transition to a hard spectral component at the
highest energies. This interpretation is in agreement with UHECR observations.
We suggest that a combination of early-Universe galactic outflows, cosmic ray
streaming instabilities, and a small-scale turbulent dynamo can increase
magnetic fields enough to attain the required rigidities. Our simulation
suggests that the available volume-averaged power density of accretion shocks
exceeds the required UHECR luminosity density by three orders of magnitude. We
show that microgauss magnetic fields at these shocks could explain both the
origin of UHECRs and the as-yet unidentified source of the diffuse radio
synchrotron background below 10 GHz. The shock-accelerated electrons produce a
hard radio background without overproducing diffuse inverse Compton emission.
These results motivate further observational tests with upcoming facilities to
help distinguish accretion shocks from other UHECR sources.; 69) Learning Choas In A Linear Way; Learning long-term behaviors in chaotic dynamical systems, such as turbulent
flows and climate modelling, is challenging due to their inherent instability
and unpredictability. These systems exhibit positive Lyapunov exponents, which
significantly hinder accurate long-term forecasting. As a result, understanding
long-term statistical behavior is far more valuable than focusing on short-term
accuracy. While autoregressive deep sequence models have been applied to
capture long-term behavior, they often lead to exponentially increasing errors
in learned dynamics. To address this, we shift the focus from simple prediction
errors to preserving an invariant measure in dissipative chaotic systems. These
systems have attractors, where trajectories settle, and the invariant measure
is the probability distribution on attractors that remains unchanged under
dynamics. Existing methods generate long trajectories of dissipative chaotic
systems by aligning invariant measures, but it is not always possible to obtain
invariant measures for arbitrary datasets. We propose the Poincare Flow Neural
Network (PFNN), a novel operator learning framework designed to capture
behaviors of chaotic systems without any explicit knowledge of the invariant
measure. PFNN employs an auto-encoder to map the chaotic system to a
finite-dimensional feature space, effectively linearizing the chaotic
evolution. It then learns the linear evolution operators to match the physical
dynamics by addressing two critical properties in dissipative chaotic systems:
(1) contraction, the system's convergence toward its attractors, and (2)
measure invariance, trajectories on the attractors following a probability
distribution invariant to the dynamics. Our experiments on a variety of chaotic
systems demonstrate that PFNN has more accurate predictions and physical
statistics compared to competitive baselines.; 70) The influence of motion features in temporal perception; This paper examines the role of manner-of-motion verbs in shaping subjective
temporal perception and emotional resonance. Through four complementary
studies, we explore how these verbs influence the conceptualization of time,
examining their use in literal and metaphorical (temporal) contexts. Our
findings reveal that faster verbs (e.g., fly, zoom) evoke dynamic and engaging
temporal experiences, often linked to positive emotions and greater agency. In
contrast, slower verbs (e.g., crawl, drag) convey passivity, monotony, and
negative emotions, reflecting tedious or constrained experiences of time. These
effects are amplified in metaphorical contexts, where manner verbs encode
emotional and experiential nuances that transcend their literal meanings. We
also find that participants prefer manner verbs over path verbs (e.g., go,
pass) in emotionally charged temporal contexts, as manner verbs capture the
experiential and emotional qualities of time more effectively. These findings
highlight the interplay between language, motion, and emotion in shaping
temporal perception, offering insights into how linguistic framing influences
subjective experiences of time.; 71) Frame-dependent coherence of a quantum state; A finite-dimensional Hilbert space is usually described by using an
orthonormal basis, but a more general description can be obtained by using a
tight frame. The frame-dependent coherence, defined by following the analogy
with the basis-dependent coherence, allows us to define the coherence with
respect to several orthogonal bases considered simultaneously or with respect
to a discrete system of coherent states. By using this more general definition,
we can investigate how the basis-dependent coherence changes when we go from a
basis to another one, from a basis to a complementary one. Frame-dependent
coherence of a quantum state coincides with the basis-dependent coherence of
the corresponding state in a Neumark extension.; 72) Relative phase between $s_{\pm}$ superconducting order parameter
  components in a two-band model with impurities; We obtain solutions for Eliashberg equations within the Nambu representation
for a two-band model of iron-based superconductors with nonmagnetic impurities.
Two cases of a transition between $s_{\pm}$ and $s_{++}$ states are considered:
(i) the transition is accompanied by the abrupt change of the order parameter
sign within one of the bands and (ii) the change is smooth. For both cases, we
studied the role of a gauge defined by the coefficients preceding the Pauli
matrices $\hat\tau_1$ and $\hat\tau_2$ in a self-energy expansion, which
correspond to the components of the order parameter. We show that the absolute
value of the order parameter is conserved for solutions in the clean and in the
Born limits. In an intermediate case, between the Born and unitary limits,
result depends on the solution for the clean limit. We show that a common gauge
for the Eliashberg equations in which one of the order parameter components
vanishes is essential for adequate description of the multiband superconducting
systems.; 73) Reformulation of Einstein equations in the Fully Constrained
  Formulation: local-uniqueness, post-Newtonian expansion and initial data; Einstein equations can be written in the so-called Fully Constrained
Formulation (FCF). This formulation has two different sectors: the elliptic
sector, formed by the Hamiltonian and Momentum constraints together with the
equations derived from the gauge choice; and the hyperbolic sector, formed by
the evolution of the rest of the spacetime metric variables, which encodes the
gravitational radiation. In this work, we present a modification of both
sectors that keeps local uniqueness properties of the elliptic system of
equations and includes a hierarchical post-Newtonian structure of all the
elliptic and hyperbolic equations. This reformulation can have potential
applications in cosmology and relativistic astrophysics. Moreover, we show how
initial stationary data can be computed numerically using this formulation
without assuming a conformally flat spatial metric, with the illustrative
example of a rotating neutron star.; 74) Prime Identification and Composite Filtering Using GM-(n+1) Sequences; This paper presents a distinctive prime detection approach. This method use
GM-(n+1) sequences to effectively eliminate complex numbers. The sequences,
which consist of odd a number of (n+1), exclude all components except for the
initial prime integer. Only the first prime number is presented. This research
proposes an approach using this model to identify exceptional candidates and
examine their distribution. This study examines the interconnections among the
laws of division, basic gaps, and their applications in analytical procedures.
Computer studies may provide a novel perspective on the theory of prime
numbers, demonstrating the effectiveness of this approach in refining the
search space for primes.; 75) Short-distance contributions to Hadronic-light-by-light for the muon
  $g-2$; This talk discusses short-distance contributions to the hadronic
light-by-light part of the muon g-2 as in the Standard Model. A short
discussion about the theory prediction is followed by the status of our work.
The main new results since the previous chiral dynamics workshop is how to
calculate the higher order corrections to the Melnikov-Vainshtein region using
the operator product expansion. We work out fully the next order in the OPE,
$D=4$, including photon and gluon only operators.
  In addition we briefly discuss the ongoing work on gluonic corrections to the
next-order and nonperturbative estimates.; 76) Safe Gradient Flow for Bilevel Optimization; Bilevel optimization is a key framework in hierarchical decision-making,
where one problem is embedded within the constraints of another. In this work,
we propose a control-theoretic approach to solving bilevel optimization
problems. Our method consists of two components: a gradient flow mechanism to
minimize the upper-level objective and a safety filter to enforce the
constraints imposed by the lower-level problem. Together, these components form
a safe gradient flow that solves the bilevel problem in a single loop. To
improve scalability with respect to the lower-level problem's dimensions, we
introduce a relaxed formulation and design a compact variant of the safe
gradient flow. This variant minimizes the upper-level objective while ensuring
the lower-level decision variable remains within a user-defined suboptimality.
Using Lyapunov analysis, we establish convergence guarantees for the dynamics,
proving that they converge to a neighborhood of the optimal solution. Numerical
experiments further validate the effectiveness of the proposed approaches. Our
contributions provide both theoretical insights and practical tools for
efficiently solving bilevel optimization problems.; 77) Fabry-P\'{e}rot etalon walk-off loss in ring cavities; Fabry-P\'erot etalons are widely used for laser frequency control. Inserting
them in laser cavities imparts unavoidable walk-off loss and reduces the output
power. Here, we treat the technology-relevant case of the walk-off loss of an
etalon in a unidirectional ring cavity, which is a standard design of
single-frequency lasers. We provide the theory background by discussing the
analytic limits. The loss can be efficiently minimized by realignment, or by
proper matching of the etalon's surface reflectivity with its refractive index.
We numerically calculate the loss in the region uncovered by the analytic
limits. We discuss practical laser design considerations, and we perform a
tilt-tuning experiment in a single-frequency, solid-state laser setup.; 78) Profile and neighbourhood complexity of graphs with excluded minors and
  tree-structured graphs; The \emph{$r$-neighbourhood complexity} of a graph $G$ is the function
counting, for a given integer $k$, the largest possible number, over all
vertex-subsets $A$ of size $k$, of subsets of $A$ realized as the intersection
between the $r$-neighbourhood of some vertex and $A$. A~refinement of this
notion is the \emph{$r$-profile complexity}, that counts the maximum number of
distinct distance-vectors from any vertex to the vertices of $A$, ignoring
distances larger than~$r$. Typically, in structured graph classes such as
graphs of bounded VC-dimension or chordal graphs, these functions are bounded,
leading to insights into their structural properties and efficient algorithms.
  We improve existing bounds on the $r$-profile complexity (and thus on the
$r$-neighbourhood complexity) for graphs in several structured graph classes.
We show that the $r$-profile complexity of graphs excluding $K_h$ as a minor is
in $O_h(r^{3h-3}k)$. For graphs of treewidth at most~$t$, we give a bound in
$O_t(r^{t+1}k)$, which is tight up to a function of~$t$ as a factor. These
bounds improve results of Joret and Rambaud and answer a question of their
paper [Combinatorica, 2024]. We also apply our methods to other classes of
bounded expansion such as graphs excluding a fixed complete graph as a
subdivision.
  For outerplanar graphs, we can improve our treewidth bound by a factor of $r$
and conjecture that a similar improvement holds for graphs with bounded simple
treewidth. For graphs of treelength at most~$\ell$, we give the upper bound of
$O(k(r^2(\ell+1)^k))$, which we improve to $O\left (k\cdot (r 2^k + r^2k^2)
\right)$ in the case of chordal graphs and $O(k^2r)$ for interval graphs.
  Our bounds also imply relations between the order, diameter and metric
dimension of graphs in these classes, improving results from [Beaudou et al.,
SIDMA 2017].; 79) Rotational-hyperfine cooling of $^{205}$TlF in a cryogenic beam; The aim of CeNTREX (Cold Molecule Nuclear Time-Reversal Experiment) is to
search for time-reversal symmetry violation in the thallium nucleus, by
measuring the Schiff moment of $^{205}$Tl in the polar molecule thallium
fluoride (TlF). CeNTREX uses a cryogenic beam of TlF with a rotational
temperature of 6.3(2) K. This results in population spread over dozens of
rotational and hyperfine sublevels of TlF, while only a single level is useful
for the Schiff moment measurement. Here we present a protocol for cooling the
rotational and hyperfine degrees of freedom in the CeNTREX beam, transferring
the majority of the Boltzmann distribution into a single rotational and
hyperfine sublevel by using a single ultraviolet laser and a pair of microwave
beams. We achieve a factor of $20.1(4)$ gain in the population of the $J=0$,
$F=0$ hyperfine sublevel of the TlF ground state.; 80) Turbulence in protoplanetary disks: A systematic analysis of dust
  settling in 33 disks; The level of dust vertical settling and radial dust concentration in disks is
of critical importance for understanding the efficiency of planet formation. We
present the first uniform analysis of the vertical extent of millimeter dust
for a representative sample of 33disks. We used radiative transfer modeling of
archival high-angular-resolution (<=0.1"") ALMA dust observations of inclined
and ringed disks to estimate their vertical dust scale height, which was
compared to estimated gas scale heights to characterize the level of vertical
sedimentation. In all 23systems for which constraints could be obtained, we
find that the outer parts of the disks are vertically settled. 5disks allow for
the characterization of the dust scale height both within and outside
approximately half the dust disk radius, showing a lower limit on their dust
heights at smaller radii. This implies that the ratio between vertical
turbulence and the Stokes number, $\alpha_z/\St$, decreases radially in these
sources. For 21rings in 15disks, we also constrained the level of radial
concentration of the dust, finding that about half of the rings are compatible
with strong radial trapping. In most of these rings, vertical turbulence is
found to be comparable to or weaker than radial turbulence, which is
incompatible with the turbulence generated by the vertical shear instability at
these locations. We further used our dust settling constraints to estimate the
turbulence level under the assumption that the dust size is limited by
fragmentation, finding typical upper limits around
$\alpha_\text{frag}\leq10^{-3}$. In a few sources, we find that turbulence
cannot be the main source of accretion. In the context of pebble accretion, we
identify several disk regions that have upper limits on their dust
concentration that would allow core formation to proceed efficiently, even at
wide orbital distances outside of 50au.; 81) Buffered Partially-Persistent External-Memory Search Trees; We present an optimal partially-persistent external-memory search tree with
amortized I/O bounds matching those achieved by the non-persistent
$B^{\varepsilon}$-tree by Brodal and Fagerberg [SODA 2003]. In a
partially-persistent data structure each update creates a new version of the
data structure, where all past versions can be queried, but only the current
version can be updated. All operations should be efficient with respect to the
size $N_v$ of the accessed version $v$. For any parameter $0<\varepsilon<1$,
our data structure supports insertions and deletions in amortized
$O\!\left(\frac{1}{\varepsilon B^{1-\varepsilon}}\log_B N_v\right)$ I/Os, where
$B$ is the external-memory block size. It also supports successor and range
reporting queries in amortized $O\!\left(\frac{1}{\varepsilon}\log_B
N_v+K/B\right)$ I/Os, where $K$ is the number of values reported. The space
usage of the data structure is linear in the total number of updates. We make
the standard and minimal assumption that the internal memory has size $M \geq
2B$. The previous state-of-the-art external-memory partially-persistent search
tree by Arge, Danner and Teh [JEA 2003] supports all operations in worst-case
$O\!\left(\log_B N_v+K/B\right)$ I/Os, matching the bounds achieved by the
classical B-tree by Bayer and McCreight [Acta Informatica 1972]. Our data
structure successfully combines buffering updates with partial persistence. The
I/O bounds can also be achieved in the worst-case sense, by slightly modifying
our data structure and under the requirement that the memory size $M =
\Omega\!\left(B^{1-\varepsilon}\log_2(\max_v N_v)\right)$. The worst-case
result slightly improves the memory requirement over the previous ephemeral
external-memory dictionary by Das, Iacono, and Nekrich (ISAAC 2022), who
achieved matching worst-case I/O bounds but required $M=\Omega\!\left(B\log_B
N\right)$.; 82) Meshless Super-Resolution of Scattered Data via constrained RBFs and
  KNN-Driven Densification; We propose a novel meshless method to achieve super-resolution from scattered
data obtained from sparse, randomly-positioned sensors such as the particle
tracers of particle tracking velocimetry. The method combines K-Nearest
Neighbor Particle Tracking Velocimetry (KNN-PTV, Tirelli et al. 2023) with
meshless Proper Orthogonal Decomposition (meshless POD, Tirelli et al. 2025)
and constrained Radial Basis Function regression (c-RBFs, Sperotto et al.
2022). The main idea is to use KNN-PTV to enhance the spatial resolution of
flow fields by blending data from \textit{locally similar} flow regions
available in the time series. This \textit{similarity} is assessed in terms of
statistical coherency with leading features, identified by meshless POD
directly on the scattered data without the need to first interpolate onto a
grid, but instead relying on RBFs to compute all the relevant inner products.
Lastly, the proposed approach uses the c-RBF on the denser scattered
distributions to derive an analytical representation of the flow fields that
incorporates physical constraints. This combination is meshless because it does
not require the definition of a grid at any step of the calculation, thus
providing flexibility in handling complex geometries. The algorithm is
validated on 3D measurements of a jet flow in air. The assessment covers three
key aspects: statistics, spectra, and modal analysis. The proposed method is
evaluated against standard Particle Image Velocimetry, KNN-PTV, and c-RBFs. The
results demonstrate improved accuracy, with an average error on the order of
11%, compared to 13-14% for the other methods. Additionally, the proposed
method achieves an increase in the cutoff frequency of approximately 3-4/D,
compared to the values observed in the competing approaches. Furthermore, it
shows nearly half the errors in low-order reconstructions.; 83) Low-dispersive phase-modulated rapid scanning interferometry; Time-domain interferometry is an important principle in Fourier transform
(FT) and nonlinear femto- to attosecond spectroscopy. To optimize the
resolution and sensitivity of this approach, various interferometer
stabilization schemes have been developed. Among them, acousto-optical phase
modulation (AOPM) of the interferometer arms combined with phase-synchronous
lock-in detection has proven as a particular sensitive technique. However, the
acousto-optical modulators (AOMs), required for this technique, introduce
several disadvantages. Here, we demonstrate an alternative phase modulation
scheme which omits AOMs, termed PM scheme here. As a benchmark, we directly
compare the performance between the PM and the AOPM scheme in a linear FT
spectroscopy experiment and find comparable sensitivity in both approaches.; 84) Regularized dynamical parametric approximation of stiff evolution
  problems; Evolutionary deep neural networks have emerged as a rapidly growing field of
research. This paper studies numerical integrators for such and other classes
of nonlinear parametrizations $ u(t) = \Phi(\theta(t)) $, where the evolving
parameters $\theta(t)$ are to be computed. The primary focus is on tackling the
challenges posed by the combination of stiff evolution problems and irregular
parametrizations, which typically arise with neural networks, tensor networks,
flocks of evolving Gaussians, and in further cases of overparametrization. We
propose and analyse regularized parametric versions of the implicit Euler
method and higher-order implicit Runge--Kutta methods for the time integration
of the parameters in nonlinear approximations to evolutionary partial
differential equations and large systems of stiff ordinary differential
equations. At each time step, an ill-conditioned nonlinear optimization problem
is solved approximately with a few regularized Gauss--Newton iterations. Error
bounds for the resulting parametric integrator are derived by relating the
computationally accessible Gauss--Newton iteration for the parameters to the
computationally inaccessible Newton iteration for the underlying non-parametric
time integration scheme. The theoretical findings are supported by numerical
experiments that are designed to show key properties of the proposed parametric
integrators.; 85) plmmr: an R package to fit penalized linear mixed models for genome-wide
  association data with complex correlation structure; Correlation among the observations in high-dimensional regression modeling
can be a major source of confounding. We present a new open-source package,
plmmr, to implement penalized linear mixed models in R. This R package
estimates correlation among observations in high-dimensional data and uses
those estimates to improve prediction with the best linear unbiased predictor.
The package uses memory-mapping so that genome-scale data can be analyzed on
ordinary machines even if the size of data exceeds RAM. We present here the
methods, workflow, and file-backing approach upon which plmmr is built, and we
demonstrate its computational capabilities with two examples from real GWAS
data.; 86) The Kodaira Embedding Theorem; Chow's Theorem and GAGA are renowned results demonstrating the algebraic
nature of projective manifolds and, more broadly, projective analytic
varieties. However, determining if a particular manifold is projective is not,
generally, a simple task. The Kodaira Embedding Theorem provides an intrinsic
characterization of projective varieties in terms of line bundles; in
particular, it states that a manifold is projective if and only if it admits a
positive line bundle. We prove only the 'if' implication in this paper, giving
a sufficient condition for a manifold bundle to be embedded in projective
space. Along the way, we prove several other interesting results. Of particular
note is the Kodaira-Nakano Vanishing Theorem, a crucial tool for eliminating
higher cohomology of complex manifolds, as well as Lemmas 6.2 and 6.1, which
provide important relationships between divisors, line bundles, and blowups.
Although this treatment is relatively self-contained, we omit a rigorous
development of Hodge theory, some basic complex analysis results, and some
theorems regarding Cech cohomology (including Leray's Theorem).; 87) ContextGNN goes to Elliot: Towards Benchmarking Relational Deep Learning
  for Static Link Prediction (aka Personalized Item Recommendation); Relational deep learning (RDL) settles among the most exciting advances in
machine learning for relational databases, leveraging the representational
power of message passing graph neural networks (GNNs) to derive useful
knowledge and run predicting tasks on tables connected through
primary-to-foreign key links. The RDL paradigm has been successfully applied to
recommendation lately, through its most recent representative deep learning
architecture namely, ContextGNN. While acknowledging ContextGNN's improved
performance on real-world recommendation datasets and tasks, preliminary tests
for the more traditional static link prediction task (aka personalized item
recommendation) on the popular Amazon Book dataset have demonstrated how
ContextGNN has still room for improvement compared to other state-of-the-art
GNN-based recommender systems. To this end, with this paper, we integrate
ContextGNN within Elliot, a popular framework for reproducibility and
benchmarking analyses, counting around 50 state-of-the-art recommendation
models from the literature to date. On such basis, we run preliminary
experiments on three standard recommendation datasets and against six
state-of-the-art GNN-based recommender systems, confirming similar trends to
those observed by the authors in their original paper. The code is publicly
available on GitHub:
https://github.com/danielemalitesta/Rel-DeepLearning-RecSys.; 88) Extracting Symbolic Sequences from Visual Representations via
  Self-Supervised Learning; This paper explores the potential of abstracting complex visual information
into discrete, structured symbolic sequences using self-supervised learning
(SSL). Inspired by how language abstracts and organizes information to enable
better reasoning and generalization, we propose a novel approach for generating
symbolic representations from visual data. To learn these sequences, we extend
the DINO framework to handle visual and symbolic information. Initial
experiments suggest that the generated symbolic sequences capture a meaningful
level of abstraction, though further refinement is required. An advantage of
our method is its interpretability: the sequences are produced by a decoder
transformer using cross-attention, allowing attention maps to be linked to
specific symbols and offering insight into how these representations correspond
to image regions. This approach lays the foundation for creating interpretable
symbolic representations with potential applications in high-level scene
understanding.; 89) Generating Causally Compliant Counterfactual Explanations using ASP; This research is focused on generating achievable counterfactual
explanations. Given a negative outcome computed by a machine learning model or
a decision system, the novel CoGS approach generates (i) a counterfactual
solution that represents a positive outcome and (ii) a path that will take us
from the negative outcome to the positive one, where each node in the path
represents a change in an attribute (feature) value. CoGS computes paths that
respect the causal constraints among features. Thus, the counterfactuals
computed by CoGS are realistic. CoGS utilizes rule-based machine learning
algorithms to model causal dependencies between features. The paper discusses
the current status of the research and the preliminary results obtained.; 90) Physics-constrained DeepONet for Surrogate CFD models: a curved
  backward-facing step case; The Physics-Constrained DeepONet (PC-DeepONet), an architecture that
incorporates fundamental physics knowledge into the data-driven DeepONet model,
is presented in this study. This methodology is exemplified through surrogate
modeling of fluid dynamics over a curved backward-facing step, a benchmark
problem in computational fluid dynamics. The model was trained on computational
fluid dynamics data generated for a range of parameterized geometries. The
PC-DeepONet was able to learn the mapping from the parameters describing the
geometry to the velocity and pressure fields. While the DeepONet is solely
data-driven, the PC-DeepONet imposes the divergence constraint from the
continuity equation onto the network. The PC-DeepONet demonstrates higher
accuracy than the data-driven baseline, especially when trained on sparse data.
Both models attain convergence with a small dataset of 50 samples and require
only 50 iterations for convergence, highlighting the efficiency of neural
operators in learning the dynamics governed by partial differential equations.; 91) OptiSeq: Ordering Examples On-The-Fly for In-Context Learning; Developers using LLMs and LLM-based agents in their applications have
provided plenty of anecdotal evidence that in-context-learning (ICL) is
fragile. In this paper, we show that in addition to the quantity and quality of
examples, the order in which the in-context examples are listed in the prompt
affects the output of the LLM and, consequently, their performance. While prior
work has explored improving ICL through dataset-dependent techniques, we
introduce OptiSeq, a purely inference-time, dataset-free optimization method
that efficiently determines the best example order. OptiSeq leverages log
probabilities of LLM-generated outputs to systematically prune the search space
of possible orderings and recommend the best order(s) by distinguishing
orderings that yield high levels of accuracy and those that underperform.
Extensive empirical evaluation on multiple LLMs, datasets, and prompts
demonstrate that OptiSeq improves accuracy by 5.5 - 10.5 percentage points
across multiple tasks.; 92) Zep: A Temporal Knowledge Graph Architecture for Agent Memory; We introduce Zep, a novel memory layer service for AI agents that outperforms
the current state-of-the-art system, MemGPT, in the Deep Memory Retrieval (DMR)
benchmark. Additionally, Zep excels in more comprehensive and challenging
evaluations than DMR that better reflect real-world enterprise use cases. While
existing retrieval-augmented generation (RAG) frameworks for large language
model (LLM)-based agents are limited to static document retrieval, enterprise
applications demand dynamic knowledge integration from diverse sources
including ongoing conversations and business data. Zep addresses this
fundamental limitation through its core component Graphiti -- a
temporally-aware knowledge graph engine that dynamically synthesizes both
unstructured conversational data and structured business data while maintaining
historical relationships. In the DMR benchmark, which the MemGPT team
established as their primary evaluation metric, Zep demonstrates superior
performance (94.8% vs 93.4%). Beyond DMR, Zep's capabilities are further
validated through the more challenging LongMemEval benchmark, which better
reflects enterprise use cases through complex temporal reasoning tasks. In this
evaluation, Zep achieves substantial results with accuracy improvements of up
to 18.5% while simultaneously reducing response latency by 90% compared to
baseline implementations. These results are particularly pronounced in
enterprise-critical tasks such as cross-session information synthesis and
long-term context maintenance, demonstrating Zep's effectiveness for deployment
in real-world applications.; 93) Bernstein-type inequalities for quantum algebras; We establish Bernstein-type inequalities for the quantum algebras
  $K_{n,\Gamma}^{P,Q}(\mathbb{K})$ introduced by K. L. Horton that include the
graded quantum Weyl algebra, the quantum symplectic space, the quantum
Euclidean space, and quantum Heisenberg algebra etc., obtaining new results and
as well as simplified proofs of previously known results.
  The Krull and global dimensions of certain further localizations of
$K_{n,\Gamma}^{P,Q}(\mathbb{K})$ are computed.; 94) A Framework to Develop and Validate RL-Based Obstacle-Aware UAV
  Positioning Algorithms; Unmanned Aerial Vehicles (UAVs) are increasingly being utilized to enhance
the Quality of Service (QoS) in wireless networks due to their flexibility and
cost-effectiveness. However, optimizing UAV placement in dynamic and
obstacle-prone environments remains a research challenge. Reinforcement
Learning (RL) has proven to be an effective approach that offers adaptability
and robustness in such environments.
  This paper introduces RLpos-3, a novel framework that integrates standard RL
techniques and existing libraries with Network Simulator 3 (ns-3) to facilitate
the development and evaluation of UAV positioning algorithms. RLpos-3 serves as
a complementary tool for researchers, enabling the implementation, analysis,
and benchmarking of UAV positioning strategies across different environmental
settings while ensuring user traffic demands are met. To validate its
effectiveness, we present a use case demonstrating the performance of RLpos-3
in optimizing UAV placement under realistic conditions.; 95) FLAVARS: A Multimodal Foundational Language and Vision Alignment Model
  for Remote Sensing; Remote sensing imagery is dense with objects and contextual visual
information. There is a recent trend to combine paired satellite images and
text captions for pretraining performant encoders for downstream tasks.
However, while contrastive image-text methods like CLIP enable vision-language
alignment and zero-shot classification ability, vision-only downstream
performance tends to degrade compared to image-only pretraining, such as MAE.
In this paper, we propose FLAVARS, a pretraining method that combines the best
of both contrastive learning and masked modeling, along with geospatial
alignment via contrastive location encoding. We find that FLAVARS significantly
outperforms a baseline of SkyCLIP for vision-only tasks such as KNN
classification and semantic segmentation, +6\% mIOU on SpaceNet1, while
retaining the ability to perform zero-shot classification, unlike MAE
pretrained methods.; 96) Experiments in the Linear Convex Order; This paper proposes two rankings of statistical experiments using the linear
convex order. These rankings hold in a broader set of scenarios where intuition
suggests that one experiment is more informative than another, and provide more
tractable characterizations than Blackwell order, which relies on the convex
order. We apply these rankings to compare statistical experiments in
binary-action decision problems and in decision problems that aggregate payoffs
over a collection of binary-action decision problems. Furthermore, these
rankings enable comparisons of statistical experiments in moral hazard problems
without requiring the validity of the first-order approach, thereby
complementing the results in Holmstr\""om (1979) and Kim (1995).; 97) Propagation of chaos and Razumikhin theorem for the nonlinear
  McKean-Vlasov SFDEs with common noise; As the limit equations of mean-field particle systems perturbed by common
environmental noise, the McKean-Vlasov stochastic differential equations with
common noise have received a lot of attention. Moreover, past dependence is an
unavoidable natural phenomenon for dynamic systems in life sciences, economics,
finance, automatic control, and other fields. Combining the two aspects above,
this paper delves into a class of nonlinear McKean-Vlasov stochastic functional
differential equations (MV-SFDEs) with common noise. The well-posedness of the
nonlinear MV-SFDEs with common noise is first demonstrated through the
application of the Banach fixed-point theorem. Secondly, the relationship
between the MV-SFDEs with common noise and the corresponding functional
particle systems is investigated. More precisely, the conditional propagation
of chaos with an explicit convergence rate and the stability equivalence are
studied. Furthermore, the exponential stability, an important long-time
behavior of the nonlinear MV-SFDEs with common noise, is derived. To this end,
the It\^o formula involved with state and measure is developed for the MV-SFDEs
with common noise. Using this formula, the Razumikhin theorem is proved,
providing an easy-to-implement criterion for the exponential stability. Lastly,
an example is provided to illustrate the result of the stability.; 98) Enhancing Disaster Resilience with UAV-Assisted Edge Computing: A
  Reinforcement Learning Approach to Managing Heterogeneous Edge Devices; Edge sensing and computing is rapidly becoming part of intelligent
infrastructure architecture leading to operational reliance on such systems in
disaster or emergency situations. In such scenarios there is a high chance of
power supply failure due to power grid issues, and communication system issues
due to base stations losing power or being damaged by the elements, e.g.,
flooding, wildfires etc. Mobile edge computing in the form of unmanned aerial
vehicles (UAVs) has been proposed to provide computation offloading from these
devices to conserve their battery, while the use of UAVs as relay network nodes
has also been investigated previously. This paper considers the use of UAVs
with further constraints on power and connectivity to prolong the life of the
network while also ensuring that the data is received from the edge nodes in a
timely manner. Reinforcement learning is used to investigate numerous scenarios
of various levels of power and communication failure. This approach is able to
identify the device most likely to fail in a given scenario, thus providing
priority guidance for maintenance personnel. The evacuations of a rural town
and urban downtown area are also simulated to demonstrate the effectiveness of
the approach at extending the life of the most critical edge devices.; 99) Trapping and Transport of Inertial Particles in a Taylor-Green Vortex:
  Effects of Added Mass and History Force; We investigate the dynamics of small inertial particles in a two-dimensional,
steady Taylor-Green vortex flow. A classic study by Taylor (2022) showed that
heavy inertial point particles (having density parameter R = 1) are trapped by
the flow separatrices when the particle Stokes number St, which measures the
particle's inertia, is less than 1/4. Here, we consider finitely dense
particles, incorporating the previously neglected effects of added mass and the
Boussinesq-Basset history force. Using linear stability analysis near
stagnation points, we determine the critical parametric conditions in the St-R
plane that leads to particle trapping within vortex cells. We identify
additional stagnation points perceived by inertial particles, beyond the
traditional ones at vortex cell corners, when the added mass effect is
included, and we analyze their stability. Numerical analysis of the full
nonlinear system confirms the existence of distinct particle
behaviours--trapped, diffusive, and ballistic--depending on initial conditions,
consistent with Nath et al. (2024), with modifications due to added mass
effect. We delineate the regions in the St-R plane where these behaviours
dominate based on the prominent particle dynamics. However, when both the
history force and added mass effect are included, all particles exhibit
ballistic motion regardless of St and R.; 100) Tollmien-Schlichting waves near neutral stable curve; In this paper, we study the linear stability of boundary layer flows over a
flat plate. Tollmien, Schlichting, Lin et al. found that there exists a neutral
curve, which consists of two branches: lower branch $\alpha_{low}(Re)$ and
upper branch $\alpha_{up}(Re)$. Here, $\alpha$ is the wave number and $Re$ is
the Reynolds number. For any $\alpha\in(\alpha_{low},\alpha_{up})$, there exist
unstable modes known as Tollmien-Schlichting (T-S) waves to the linearized
Navier-Stokes system. These waves play a key role during the early stage of
boundary layer transition. In a breakthrough work (Duke math Jour, 165(2016)),
Grenier, Guo, and Nguyen provided a rigorous construction of the unstable T-S
waves. In this paper, we confirm the existence of the neutral stable curve. To
achieve this, we develop a more delicate method for solving the Orr-Sommerfeld
equation by borrowing some ideas from the triple-deck theory. This approach
allows us to construct the T-S waves in a neighborhood of the neutral curve.",0.5,0.6309297535714575
2411.05236,applied,2411.05236-pos2-6,"Channelrhodopsin-2, a directly light-gated cation-selective membrane channel; Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane α helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination.",2411.05236-pos1-6,"DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT; Shannon capacity of signal transduction for multiple independent receptors; Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver.",65,"['1', '2', '4', '5', '7', '8', '3', '10', '6', '12']","The best candidate paper is the first one as it explores the synthesis of novel views from multiple modalities, which aligns directly with the concept of channelrhodopsin's light-gating mechanisms in a biological context. This cross-disciplinary approach can lead to innovative imaging techniques in biological studies that exploit light for visualizing cellular structures. The remaining papers offer varying levels of relevance but do not integrate the biological aspect as effectively as the first one. The second paper, while addressing quantum mechanics in nanostructures, lacks the direct connection to light-gated mechanisms, which is crucial based on the main paper. The chosen order reflects a descending relevance to the main theme.","1) CrossModalityDiffusion: Multi-Modal Novel View Synthesis with Unified
  Intermediate Representation; Geospatial imaging leverages data from diverse sensing modalities-such as EO,
SAR, and LiDAR, ranging from ground-level drones to satellite views. These
heterogeneous inputs offer significant opportunities for scene understanding
but present challenges in interpreting geometry accurately, particularly in the
absence of precise ground truth data. To address this, we propose
CrossModalityDiffusion, a modular framework designed to generate images across
different modalities and viewpoints without prior knowledge of scene geometry.
CrossModalityDiffusion employs modality-specific encoders that take multiple
input images and produce geometry-aware feature volumes that encode scene
structure relative to their input camera positions. The space where the feature
volumes are placed acts as a common ground for unifying input modalities. These
feature volumes are overlapped and rendered into feature images from novel
perspectives using volumetric rendering techniques. The rendered feature images
are used as conditioning inputs for a modality-specific diffusion model,
enabling the synthesis of novel images for the desired output modality. In this
paper, we show that jointly training different modules ensures consistent
geometric understanding across all modalities within the framework. We validate
CrossModalityDiffusion's capabilities on the synthetic ShapeNet cars dataset,
demonstrating its effectiveness in generating accurate and consistent novel
views across multiple imaging modalities and perspectives.; 2) Physics-Aware POD-Based Learning for Ab initio QEM-Galerkin Simulations
  of Periodic Nanostructures; Quantum nanostructures offer crucial applications in electronics, photonics,
materials, drugs, etc. For accurate design and analysis of nanostructures and
materials, simulations of the Schrodinger or Schrodinger-like equation are
always needed. For large nanostructures, these eigenvalue problems can be
computationally intensive. One effective solution is a learning method via
Proper Orthogonal Decomposition (POD), together with ab initio Galerkin
projection of the Schrodinger equation. POD-Galerkin projects the problem onto
a reduced-order space with the POD basis representing electron wave functions
(WFs) guided by the first principles in simulations. To minimize training
effort and enhance robustness of POD-Galerkin in larger structures, the quantum
element method (QEM) was proposed previously, which partitions nanostructures
into generic quantum elements. Larger nanostructures can then be constructed by
the trained generic quantum elements, each of which is represented by its
POD-Galerkin model. This work investigates QEM-Galerkin thoroughly in
multi-element quantum-dot (QD) structures on approaches to further improve
training effectiveness and simulation accuracy and efficiency for QEM-Galerkin.
To further improve computing speed, POD and Fourier bases for periodic
potentials are also examined in QEM-Galerkin simulations. Results indicate
that, considering efficiency and accuracy, the POD potential basis is superior
to the Fourier potential basis even for periodic potentials. Overall,
QEM-Galerkin offers more than a 2-order speedup in computation over direct
numerical simulation for multi-element QD structures, and more improvement is
observed in a structure comprising more elements.; 3) A Multiple Transferable Neural Network Method with Domain Decomposition
  for Elliptic Interface Problems; The transferable neural network (TransNet) is a two-layer shallow neural
network with pre-determined and uniformly distributed neurons in the hidden
layer, and the least-squares solvers can be particularly used to compute the
parameters of its output layer when applied to the solution of partial
differential equations. In this paper, we integrate the TransNet technique with
the nonoverlapping domain decomposition and the interface conditions to develop
a novel multiple transferable neural network (Multi-TransNet) method for
solving elliptic interface problems, which typically contain discontinuities in
both solutions and their derivatives across interfaces. We first propose an
empirical formula for the TransNet to characterize the relationship between the
radius of the domain-covering ball, the number of hidden-layer neurons, and the
optimal neuron shape. In the Multi-TransNet method, we assign each subdomain
one distinct TransNet with an adaptively determined number of hidden-layer
neurons to maintain the globally uniform neuron distribution across the entire
computational domain, and then unite all the subdomain TransNets together by
incorporating the interface condition terms into the loss function. The
empirical formula is also extended to the Multi-TransNet and further employed
to estimate appropriate neuron shapes for the subdomain TransNets, greatly
reducing the parameter tuning cost. Additionally, we propose a normalization
approach to adaptively select the weighting parameters for the terms in the
loss function. Ablation studies and extensive experiments with comparison tests
on different types of elliptic interface problems with low to high contrast
diffusion coefficients in two and three dimensions are carried out to
numerically demonstrate the superior accuracy, efficiency, and robustness of
the proposed Multi-TransNet method.; 4) DivIL: Unveiling and Addressing Over-Invariance for Out-of- Distribution
  Generalization; Out-of-distribution generalization is a common problem that expects the model
to perform well in the different distributions even far from the train data. A
popular approach to addressing this issue is invariant learning (IL), in which
the model is compiled to focus on invariant features instead of spurious
features by adding strong constraints during training. However, there are some
potential pitfalls of strong invariant constraints. Due to the limited number
of diverse environments and over-regularization in the feature space, it may
lead to a loss of important details in the invariant features while alleviating
the spurious correlations, namely the over-invariance, which can also degrade
the generalization performance. We theoretically define the over-invariance and
observe that this issue occurs in various classic IL methods. To alleviate this
issue, we propose a simple approach Diverse Invariant Learning (DivIL) by
adding the unsupervised contrastive learning and the random masking mechanism
compensatory for the invariant constraints, which can be applied to various IL
methods. Furthermore, we conduct experiments across multiple modalities across
12 datasets and 6 classic models, verifying our over-invariance insight and the
effectiveness of our DivIL framework. Our code is available at
https://github.com/kokolerk/DivIL.; 5) Controlled Floquet Dynamics and Topological Bound States in Continuum
  via Colored Quantum Random Walks; We demonstrate the emergence and control of Floquet states and topological
bound states in the continuum (TBICs) in a two-dimensional colored quantum
random walk (cQRW) on a square lattice. By introducing three internal degrees
of freedom-termed ""colors""-and leveraging SU(3) group representations, we
realize dispersive TBICs and intrinsic Floquet dynamics without the need for
external periodic driving. Through Chern number calculations, we identify three
distinct topological bands, revealing color-induced band mixing as a key
mechanism underlying the natural formation of Floquet states. The cQRW
framework enables precise tuning of quasi-energy spectra, supporting the
emergence of localized edge states in topological band gaps and dispersive
TBICs embedded within the bulk of other bands. These TBICs exhibit tunable
group velocity, controllable excitation across energy regimes, and robustness,
providing theoretical validation for their existence in a first-order Floquet
system. Our findings position cQRWs as a powerful platform for investigating
and harnessing TBICs and Floquet states, with potential applications in quantum
information and communication technologies.; 6) Resolution of Erd\H{o}s' problems about unimodularity; Letting $\delta_1(n,m)$ be the density of the set of integers with exactly
one divisor in $(n,m)$, Erd\H{o}s wondered if $\delta_1(n,m)$ is unimodular for
fixed $n$.
  We prove this is false in general, as the sequence $(\delta_1(n,m))$ has
superpolynomially many local extrema. However, we confirm unimodality in the
single case for which it occurs; $n = 1$.
  We also solve the question on unimodality of the density of integers whose
$k^{th}$ prime is $p$.; 7) Clinical Inspired MRI Lesion Segmentation; Magnetic resonance imaging (MRI) is a potent diagnostic tool for detecting
pathological tissues in various diseases. Different MRI sequences have
different contrast mechanisms and sensitivities for different types of lesions,
which pose challenges to accurate and consistent lesion segmentation. In
clinical practice, radiologists commonly use the sub-sequence feature, i.e. the
difference between post contrast-enhanced T1-weighted (post) and
pre-contrast-enhanced (pre) sequences, to locate lesions. Inspired by this, we
propose a residual fusion method to learn subsequence representation for MRI
lesion segmentation. Specifically, we iteratively and adaptively fuse features
from pre- and post-contrast sequences at multiple resolutions, using dynamic
weights to achieve optimal fusion and address diverse lesion enhancement
patterns. Our method achieves state-of-the-art performances on BraTS2023
dataset for brain tumor segmentation and our in-house breast MRI dataset for
breast lesion segmentation. Our method is clinically inspired and has the
potential to facilitate lesion segmentation in various applications.; 8) CardioTabNet: A Novel Hybrid Transformer Model for Heart Disease
  Prediction using Tabular Medical Data; The early detection and prediction of cardiovascular diseases are crucial for
reducing the severe morbidity and mortality associated with these conditions
worldwide. A multi-headed self-attention mechanism, widely used in natural
language processing (NLP), is operated by Transformers to understand feature
interactions in feature spaces. However, the relationships between various
features within biological systems remain ambiguous in these spaces,
highlighting the necessity of early detection and prediction of cardiovascular
diseases to reduce the severe morbidity and mortality with these conditions
worldwide. We handle this issue with CardioTabNet, which exploits the strength
of tab transformer to extract feature space which carries strong understanding
of clinical cardiovascular data and its feature ranking. As a result,
performance of downstream classical models significantly showed outstanding
result. Our study utilizes the open-source dataset for heart disease prediction
with 1190 instances and 11 features. In total, 11 features are divided into
numerical (age, resting blood pressure, cholesterol, maximum heart rate, old
peak, weight, and fasting blood sugar) and categorical (resting ECG, exercise
angina, and ST slope). Tab transformer was used to extract important features
and ranked them using random forest (RF) feature ranking algorithm. Ten
machine-learning models were used to predict heart disease using selected
features. After extracting high-quality features, the top downstream model (a
hyper-tuned ExtraTree classifier) achieved an average accuracy rate of 94.1%
and an average Area Under Curve (AUC) of 95.0%. Furthermore, a nomogram
analysis was conducted to evaluate the model's effectiveness in cardiovascular
risk assessment. A benchmarking study was conducted using state-of-the-art
models to evaluate our transformer-driven framework.; 9) Skill Expansion and Composition in Parameter Space; Humans excel at reusing prior knowledge to address new challenges and
developing skills while solving problems. This paradigm becomes increasingly
popular in the development of autonomous agents, as it develops systems that
can self-evolve in response to new challenges like human beings. However,
previous methods suffer from limited training efficiency when expanding new
skills and fail to fully leverage prior knowledge to facilitate new task
learning. In this paper, we propose Parametric Skill Expansion and Composition
(PSEC), a new framework designed to iteratively evolve the agents' capabilities
and efficiently address new challenges by maintaining a manageable skill
library. This library can progressively integrate skill primitives as
plug-and-play Low-Rank Adaptation (LoRA) modules in parameter-efficient
finetuning, facilitating efficient and flexible skill expansion. This structure
also enables the direct skill compositions in parameter space by merging LoRA
modules that encode different skills, leveraging shared information across
skills to effectively program new skills. Based on this, we propose a
context-aware module to dynamically activate different skills to
collaboratively handle new tasks. Empowering diverse applications including
multi-objective composition, dynamics shift, and continual policy shift, the
results on D4RL, DSRL benchmarks, and the DeepMind Control Suite show that PSEC
exhibits superior capacity to leverage prior knowledge to efficiently tackle
new challenges, as well as expand its skill libraries to evolve the
capabilities. Project website: https://ltlhuuu.github.io/PSEC/.; 10) Learnable polynomial, trigonometric, and tropical activations; This paper investigates scalable neural networks with learnable activation
functions based on orthogonal function bases and tropical polynomials,
targeting ImageNet-1K classification and next token prediction on OpenWebText.
Traditional activations, such as ReLU, are static. In contrast, learnable
activations enable the network to adapt dynamically during training. However,
stability issues, such as vanishing or exploding gradients, arise with improper
variance management in deeper networks. To remedy this, we propose an
initialization scheme that single-handedly preserves unitary variance in
transformers and convolutional networks, ensuring stable gradient flow even in
deep architectures. Extensive experiments demonstrate that networks with
Hermite, Fourier, and Tropical-based learnable activations significantly
improve over GPT-2 and ConvNeXt networks in terms of accuracy and perplexity in
train and test, highlighting the viability of learnable activations in
large-scale tasks. The activation functions developed here are the subject of a
library coded entirely in pure PyTorch: torchortho, available at
https://github.com/K-H-Ismail/torchortho.; 11) Second-Order $\Gamma$-Limit for the Cahn-Hilliard Functional with
  Dirichlet Boundary Conditions, II; This paper continues the study of the asymptotic development of order 2 by
$\Gamma$ -convergence of the Cahn-Hilliard functional with Dirichlet boundary
conditions initiated in [8]. While in the first paper, the Dirichlet data are
assumed to be well separated from one of the two wells, here this is no longer
the case. In the case where there are no interfaces, it is shown that there is
a transition layer near the boundary of the domain.; 12) Vevo: Controllable Zero-Shot Voice Imitation with Self-Supervised
  Disentanglement; The imitation of voice, targeted on specific speech attributes such as timbre
and speaking style, is crucial in speech generation. However, existing methods
rely heavily on annotated data, and struggle with effectively disentangling
timbre and style, leading to challenges in achieving controllable generation,
especially in zero-shot scenarios. To address these issues, we propose Vevo, a
versatile zero-shot voice imitation framework with controllable timbre and
style. Vevo operates in two core stages: (1) Content-Style Modeling: Given
either text or speech's content tokens as input, we utilize an autoregressive
transformer to generate the content-style tokens, which is prompted by a style
reference; (2) Acoustic Modeling: Given the content-style tokens as input, we
employ a flow-matching transformer to produce acoustic representations, which
is prompted by a timbre reference. To obtain the content and content-style
tokens of speech, we design a fully self-supervised approach that progressively
decouples the timbre, style, and linguistic content of speech. Specifically, we
adopt VQ-VAE as the tokenizer for the continuous hidden features of HuBERT. We
treat the vocabulary size of the VQ-VAE codebook as the information bottleneck,
and adjust it carefully to obtain the disentangled speech representations.
Solely self-supervised trained on 60K hours of audiobook speech data, without
any fine-tuning on style-specific corpora, Vevo matches or surpasses existing
methods in accent and emotion conversion tasks. Additionally, Vevo's
effectiveness in zero-shot voice conversion and text-to-speech tasks further
demonstrates its strong generalization and versatility. Audio samples are
available at https://versavoice.github.io.; 13) MRUCT: Mixed Reality Assistance for Acupuncture Guided by Ultrasonic
  Computed Tomography; Chinese acupuncture practitioners primarily depend on muscle memory and
tactile feedback to insert needles and accurately target acupuncture points, as
the current workflow lacks imaging modalities and visual aids. Consequently,
new practitioners often learn through trial and error, requiring years of
experience to become proficient and earn the trust of patients. Medical
students face similar challenges in mastering this skill. To address these
challenges, we developed an innovative system, MRUCT, that integrates
ultrasonic computed tomography (UCT) with mixed reality (MR) technology to
visualize acupuncture points in real-time. This system offers offline image
registration and real-time guidance during needle insertion, enabling them to
accurately position needles based on anatomical structures such as bones,
muscles, and auto-generated reference points, with the potential for clinical
implementation. In this paper, we outline the non-rigid registration methods
used to reconstruct anatomical structures from UCT data, as well as the key
design considerations of the MR system. We evaluated two different 3D user
interface (3DUI) designs and compared the performance of our system to
traditional workflows for both new practitioners and medical students. The
results highlight the potential of MR to enhance therapeutic medical practices
and demonstrate the effectiveness of the system we developed.; 14) Randomized measurements for multi-parameter quantum metrology; The optimal quantum measurements for estimating different unknown parameters
in a parameterized quantum state are usually incompatible with each other.
Traditional approaches to addressing the measurement incompatibility issue,
such as the Holevo Cram\'{e}r--Rao bound, suffer from multiple difficulties
towards practical applicability, as the optimal measurement strategies are
usually state-dependent, difficult to implement and also take complex analyses
to determine. Here we study randomized measurements as a new approach for
multi-parameter quantum metrology. We show quantum measurements on single
copies of quantum states given by 3-design perform near-optimally when
estimating an arbitrary number of parameters in pure states and more generally,
approximately low-rank states, whose metrological information is largely
concentrated in a low-dimensional subspace. The near-optimality is also shown
in estimating the maximal number of parameters for three types of mixed states
that are well-conditioned on its support. Examples of fidelity estimation and
Hamiltonian estimation are explicitly provided to demonstrate the power and
limitation of randomized measurements in multi-parameter quantum metrology.; 15) Model-Free Adversarial Purification via Coarse-To-Fine Tensor Network
  Representation; Deep neural networks are known to be vulnerable to well-designed adversarial
attacks. Although numerous defense strategies have been proposed, many are
tailored to the specific attacks or tasks and often fail to generalize across
diverse scenarios. In this paper, we propose Tensor Network Purification (TNP),
a novel model-free adversarial purification method by a specially designed
tensor network decomposition algorithm. TNP depends neither on the pre-trained
generative model nor the specific dataset, resulting in strong robustness
across diverse adversarial scenarios. To this end, the key challenge lies in
relaxing Gaussian-noise assumptions of classical decompositions and
accommodating the unknown distribution of adversarial perturbations. Unlike the
low-rank representation of classical decompositions, TNP aims to reconstruct
the unobserved clean examples from an adversarial example. Specifically, TNP
leverages progressive downsampling and introduces a novel adversarial
optimization objective to address the challenge of minimizing reconstruction
error but without inadvertently restoring adversarial perturbations. Extensive
experiments conducted on CIFAR-10, CIFAR-100, and ImageNet demonstrate that our
method generalizes effectively across various norm threats, attack types, and
tasks, providing a versatile and promising adversarial purification technique.; 16) Implicit Communication of Contextual Information in Human-Robot
  Collaboration; Implicit communication is crucial in human-robot collaboration (HRC), where
contextual information, such as intentions, is conveyed as implicatures,
forming a natural part of human interaction. However, enabling robots to
appropriately use implicit communication in cooperative tasks remains
challenging. My research addresses this through three phases: first, exploring
the impact of linguistic implicatures on collaborative tasks; second, examining
how robots' implicit cues for backchanneling and proactive communication affect
team performance and perception, and how they should adapt to human teammates;
and finally, designing and evaluating a multi-LLM robotics system that learns
from human implicit communication. This research aims to enhance the natural
communication abilities of robots and facilitate their integration into daily
collaborative activities.; 17) Harnessing the Potential of Large Language Models in Modern Marketing
  Management: Applications, Future Directions, and Strategic Recommendations; Large Language Models (LLMs) have revolutionized the process of customer
engagement, campaign optimization, and content generation, in marketing
management. In this paper, we explore the transformative potential of LLMs
along with the current applications, future directions, and strategic
recommendations for marketers. In particular, we focus on LLMs major business
drivers such as personalization, real-time-interactive customer insights, and
content automation, and how they enable customers and business outcomes. For
instance, the ethical aspects of AI with respect to data privacy, transparency,
and mitigation of bias are also covered, with the goal of promoting responsible
use of the technology through best practices and the use of new technologies
businesses can tap into the LLM potential, which help growth and stay one step
ahead in the turmoil of digital marketing. This article is designed to give
marketers the necessary guidance by using best industry practices to integrate
these powerful LLMs into their marketing strategy and innovation without
compromising on the ethos of their brand.; 18) Terahertz-driven ultrafast dynamics of rare-earth nickelates by
  controlling only the charge degree of freedom; An important strategy for understanding the microscopic physics of strongly
correlated systems and enhancing their technological potential is to
selectively drive the fundamental degrees of freedom out of equilibrium.
Intense terahertz (THz) pulses with photon energies of a few meV, can not only
serve this purpose but also unravel their electronic and quantum nature. Here,
we present THz-driven ultrafast dynamics of rare-earth nickelates
$\text{RNiO}_{\text{3}}$, $\text{R}$ = rare-earth atom) - a prototype system to
study the Mott insulator-metal transition (IMT). The THz drive of its Mott
insulating state induces instantaneous IMT via quantum tunneling of valence
electrons across the bandgap while the THz drive of its correlated metallic
state leads to overall heating of the conduction electrons. The subsequent
relaxations of excited electrons in these two states occur via a two-step
process (electron-phonon thermalization and recovery of the charge-ordered
insulating state) and a one-step process (electron-phonon scattering),
respectively. The relaxation dynamics of the electrons and the absence of
acoustic phonon modes, in particular, suggest that the THz photons drive only
the charge degree of freedom. The purely electronic, ultrafast and local nature
of the THz-induced IMT offers its applications in opto-electronics with
enhanced performance and minimal device size and heat dissipation.; 19) GeoJEPA: Towards Eliminating Augmentation- and Sampling Bias in
  Multimodal Geospatial Learning; Existing methods for self-supervised representation learning of geospatial
regions and map entities rely extensively on the design of pretext tasks, often
involving augmentations or heuristic sampling of positive and negative pairs
based on spatial proximity. This reliance introduces biases and limits the
representations' expressiveness and generalisability. Consequently, the
literature has expressed a pressing need to explore different methods for
modelling geospatial data. To address the key difficulties of such methods,
namely multimodality, heterogeneity, and the choice of pretext tasks, we
present GeoJEPA, a versatile multimodal fusion model for geospatial data built
on the self-supervised Joint-Embedding Predictive Architecture. With GeoJEPA,
we aim to eliminate the widely accepted augmentation- and sampling biases found
in self-supervised geospatial representation learning. GeoJEPA uses
self-supervised pretraining on a large dataset of OpenStreetMap attributes,
geometries and aerial images. The results are multimodal semantic
representations of urban regions and map entities that we evaluate both
quantitatively and qualitatively. Through this work, we uncover several key
insights into JEPA's ability to handle multimodal data.; 20) Viscous and Inviscid Reconnection of Vortex Rings on Logarithmic
  Lattices; To address the possible occurrence of a Finite-Time Singularity (FTS) during
the oblique reconnection of two vortex rings, Moffatt-Kimura (MK) (J. Fluid
Mech., 2019a; J. Fluid Mech., 2019b) developed a simplified model based on the
Biot-Savart law and claimed that the vorticity amplification
$\omega_{\text{max}}/\omega_0$ becomes very large for vortex Reynolds number
$Re_{\Gamma} \geq 4000$. However, with Direct Numerical Simulation (DNS), Yao
and Hussain (J. Fluid Mech., 2020) were able to show that the vorticity
amplification is in fact much smaller and increases slowly with $Re_{\Gamma}$.
The suppression of vorticity was linked to two key factors - deformation of the
vortex core during approach and formation of hairpin-like bridge structures. In
this work, a recently developed numerical technique called log-lattice
(Campolina and Mailybaev, Nonlinearity, 2021), where interacting Fourier modes
are logarithmically sampled, is applied to the same oblique vortex ring
interaction problem. It is shown that this technique is not only capable of
capturing the two key physical processes overlooked by the MK model but also
other quantitative and qualitative attributes generally seen with DNS, at a
fraction of the computational cost. Furthermore, the sparsity of the Fourier
modes allows us to probe very large $Re_{\Gamma} = 10^8$ until which the peak
of the maximum norm of vorticity, while increasing with $Re_{\Gamma}$, remains
finite, and a blowup is observed only for the inviscid case.; 21) Perception-aware Planning for Quadrotor Flight in Unknown and
  Feature-limited Environments; Various studies on perception-aware planning have been proposed to enhance
the state estimation accuracy of quadrotors in visually degraded environments.
However, many existing methods heavily rely on prior environmental knowledge
and face significant limitations in previously unknown environments with sparse
localization features, which greatly limits their practical application. In
this paper, we present a perception-aware planning method for quadrotor flight
in unknown and feature-limited environments that properly allocates perception
resources among environmental information during navigation. We introduce a
viewpoint transition graph that allows for the adaptive selection of local
target viewpoints, which guide the quadrotor to efficiently navigate to the
goal while maintaining sufficient localizability and without being trapped in
feature-limited regions. During the local planning, a novel yaw trajectory
generation method that simultaneously considers exploration capability and
localizability is presented. It constructs a localizable corridor via feature
co-visibility evaluation to ensure localization robustness in a computationally
efficient way. Through validations conducted in both simulation and real-world
experiments, we demonstrate the feasibility and real-time performance of the
proposed method. The source code will be released to benefit the community.; 22) Automated Quantum Algorithm Synthesis; We present a computational method to automatically design the n-qubit
realisations of quantum algorithms. Our approach leverages a domain-specific
language (DSL) that enables the construction of quantum circuits via modular
building blocks, making it well-suited for evolutionary search. In this DSL
quantum circuits are abstracted beyond the usual gate-sequence description and
scale automatically to any problem size. This enables us to learn the algorithm
structure rather than a specific unitary implementation. We demonstrate our
method by automatically designing three known quantum algorithms--the Quantum
Fourier Transform, the Deutsch-Jozsa algorithm, and Grover's search.
Remarkably, we were able to learn the general implementation of each algorithm
by considering examples of circuits containing at most 5-qubits. Our method
proves robust, as it maintains performance across increasingly large search
spaces. Convergence to the relevant algorithm is achieved with high probability
and with moderate computational resources.; 23) Three-dimensional transport of solids in a protoplanetary disk
  containing a growing giant planet; We present the results of combined hydrodynamic and particle tracking
post-processing modeling to study the transport of small dust in a
protoplanetary disk containing an embedded embryo in 3D. We use a suite of
FARGO3D hydrodynamic simulations of disks containing a planetary embryo varying
in mass up to 300 $M_\oplus$ on a fixed orbit in both high and low viscosity
disks. We then simulate solid particles through the disk as a post-processing
step using a Monte Carlo integration, allowing us to track the trajectories of
individual particles as they travel throughout the disk. We find that gas
advection onto the planet can carry small, well-coupled solids across the gap
opened in the disk by the embedded planet for planetary masses above the pebble
isolation mass. This mixing between the inner and outer disk can occur in both
directions, with solids in the inner disk mixing to the outer disk as well.
Additionally, in low viscosity disks, multiple pile-ups in the outer disk may
preserve isotopic heterogeneities, possibly providing an outermost tertiary
isotopic reservoir. Throughout Jupiter's growth, the extent of mixing between
isotopic reservoirs varied depending on dust size, gas turbulence, and the
Jovian embryo mass.; 24) Med-R$^2$: Crafting Trustworthy LLM Physicians through Retrieval and
  Reasoning of Evidence-Based Medicine; In recent years, Large Language Models (LLMs) have exhibited remarkable
capabilities in clinical scenarios. However, despite their potential, existing
works face challenges when applying LLMs to medical settings. Strategies
relying on training with medical datasets are highly cost-intensive and may
suffer from outdated training data. Leveraging external knowledge bases is a
suitable alternative, yet it faces obstacles such as limited retrieval
precision and poor effectiveness in answer extraction. These issues
collectively prevent LLMs from demonstrating the expected level of proficiency
in mastering medical expertise. To address these challenges, we introduce
Med-R^2, a novel LLM physician framework that adheres to the Evidence-Based
Medicine (EBM) process, efficiently integrating retrieval mechanisms as well as
the selection and reasoning processes of evidence, thereby enhancing the
problem-solving capabilities of LLMs in healthcare scenarios and fostering a
trustworthy LLM physician. Our comprehensive experiments indicate that Med-R^2
achieves a 14.87\% improvement over vanilla RAG methods and even a 3.59\%
enhancement compared to fine-tuning strategies, without incurring additional
training costs.; 25) The Sample Complexity of Online Reinforcement Learning: A Multi-model
  Perspective; We study the sample complexity of online reinforcement learning for nonlinear
dynamical systems with continuous state and action spaces. Our analysis
accommodates a large class of dynamical systems ranging from a finite set of
nonlinear candidate models to models with bounded and Lipschitz continuous
dynamics, to systems that are parametrized by a compact and real-valued set of
parameters. In the most general setting, our algorithm achieves a policy regret
of $\mathcal{O}(N \epsilon^2 + \mathrm{ln}(m(\epsilon))/\epsilon^2)$, where $N$
is the time horizon, $\epsilon$ is a user-specified discretization width, and
$m(\epsilon)$ measures the complexity of the function class under consideration
via its packing number. In the special case where the dynamics are parametrized
by a compact and real-valued set of parameters (such as neural networks,
transformers, etc.), we prove a policy regret of $\mathcal{O}(\sqrt{N p})$,
where $p$ denotes the number of parameters, recovering earlier
sample-complexity results that were derived for linear time-invariant dynamical
systems. While this article focuses on characterizing sample complexity, the
proposed algorithms are likely to be useful in practice, due to their
simplicity, the ability to incorporate prior knowledge, and their benign
transient behavior.; 26) Invisible Labor: The Backbone of Open Source Software; Invisible labor is an intrinsic part of the modern workplace, and includes
labor that is undervalued or unrecognized such as creating collaborative
atmospheres. Open source software (OSS) is software that is viewable, editable
and shareable by anyone with internet access. Contributors are mostly
volunteers, who participate for personal edification and because they believe
in the spirit of OSS rather than for employment. Volunteerism often leads to
high personnel turnover, poor maintenance and inconsistent project management.
This in turn, leads to a difficulty with sustainability long term. We believe
that the key to sustainable management is the invisible labor that occurs
behind the scenes. It is unclear how OSS contributors think about the invisible
labor they perform or how that affects OSS sustainability. We interviewed OSS
contributors and asked them about their invisible labor contributions,
leadership departure, membership turnover and sustainability. We found that
invisible labor is responsible for good leadership, reducing contributor
turnover, and creating legitimacy for the project as an organization.; 27) The EnviroMapper Toolkit: an Input Physicalisation that Captures the
  Situated Experience of Environmental Comfort in Offices; The environmental comfort in offices is traditionally captured by surveying
an entire workforce simultaneously, which yet fails to capture the situatedness
of the different personal experiences. To address this limitation, we developed
the EnviroMapper Toolkit, a data physicalisation toolkit that allows individual
office workers to record their personal experiences of environmental comfort by
mapping the actual moments and locations these occurred. By analysing two
in-the-wild studies in existing open-plan office environments (N=14), we
demonstrate how this toolkit acts like a situated input visualisation that can
be interpreted by domain experts who were not present during its construction.
This study therefore offers four key contributions: (1) the iterative design
process of the physicalisation toolkit; (2) its preliminary deployment in two
real-world office contexts; (3) the decoding of the resulting artefacts by
domain experts; and (4) design considerations to support future input
physicalisation and visualisation constructions that capture and synthesise
data from multiple individuals.; 28) STGDPM:Vessel Trajectory Prediction with Spatio-Temporal Graph Diffusion
  Probabilistic Model; Vessel trajectory prediction is a critical component for ensuring maritime
traffic safety and avoiding collisions. Due to the inherent uncertainty in
vessel behavior, trajectory prediction systems must adopt a multimodal approach
to accurately model potential future motion states. However, existing vessel
trajectory prediction methods lack the ability to comprehensively model
behavioral multi-modality. To better capture multimodal behavior in interactive
scenarios, we propose modeling interactions as dynamic graphs, replacing
traditional aggregation-based techniques that rely on vessel states. By
leveraging the natural multimodal capabilities of diffusion models, we frame
the trajectory prediction task as an inverse process of motion uncertainty
diffusion, wherein uncertainties across potential navigational areas are
progressively eliminated until the desired trajectories is produced. In
summary, we pioneer the integration of Spatio-Temporal Graph (STG) with
diffusion models in ship trajectory prediction. Extensive experiments on real
Automatic Identification System (AIS) data validate the superiority of our
approach.; 29) On finite dimensional regular gradings; Let $A$ be an associative algebra over an algebraically closed field $K$ of
characteristic 0. A decomposition $A=A_1\oplus\cdots \oplus A_r$ of $A$ into a
direct sum of $r$ vector subspaces is called a \textsl{regular decomposition}
if, for every $n$ and every $1\le i_j\le r$, there exist $a_{i_j}\in A_{i_j}$
such that $a_{i_1}\cdots a_{i_n}\ne 0$, and moreover, for every $1\le i,j\le r$
there exists a constant $\beta(i,j)\in K^*$ such that $a_ia_j=\beta(i,j)a_ja_i$
for every $a_i\in A_i$, $A_j\in A_j$. We work with decompositions determined by
gradings on $A$ by a finite abelian group $G$. In this case, the function
$\beta\colon G\times G\to K^*$ ought to be a bicharacter. A regular
decomposition is {minimal} whenever for every $g$, $h\in G$, the equalities
$\beta(x,g)=\beta(x,h)$ for every $x\in G$ imply $g=h$. In this paper we
describe the finite dimensional algebras $A$ admitting a $G$-grading such that
the corresponding regular decomposition is minimal. Moreover we compute the
graded codimension sequence of these algebras. It turns out that the graded PI
exponent of every finite dimensional $G$-graded algebra with regular grading
such that its regular decomposition is minimal, coincides with its ordinary
(ungraded) PI exponent.; 30) Characterization of porous nanoparticles using the lattice Boltzmann
  method for fluid flow; Nanoporous capsules have been the subject of intense investigation in the
field of drug delivery. One of the essential properties of such particles,
which requires characterization, is their structure. Many experimental
techniques have been used for this purpose, such as wide-angle neutron or X-ray
scattering, or light scattering. Herein, we report theoretical data on the
relationship between the size, porosity and drag force of porous particles.
Data were obtained from a numerical procedure based on the lattice Boltzmann
method for the calculation of the creeping flow over porous spherical objects.
Previous analytical solutions to the hydrodynamic equations yielded different
predictions for the drag force under conditions of high permeability. In the
light of our findings, we compared our data to previous divergent analytical
solutions and analyzed experimental results for dynamic and static light
scattering of porous membrane vesicles Our findings strongly indicate a
potential source of error that has led to inconsistencies in DLS measurements
when compared to other techniques.; 31) Two- and three-meson scattering amplitudes with physical quark masses
  from lattice QCD; We study systems of two and three mesons composed of pions and kaons at
maximal isospin using four CLS ensembles with $a\approx 0.063\;$fm, including
one with approximately physical quark masses. Using the stochastic
Laplacian-Heaviside method, we determine the energy spectrum of these systems
including many levels in different momentum frames and irreducible
representations. Using the relativistic two- and three-body finite-volume
formalism, we constrain the two and three-meson K matrices, including not only
the leading $s$ wave, but also $p$ and $d$ waves. By solving the three-body
integral equations, we determine, for the first time, the physical-point
scattering amplitudes for $3\pi^+$, $3K^+$, $\pi^+\pi^+ K^+$ and $K^+ K^+
\pi^+$ systems. These are determined for total angular momentum $J^P=0^-$,
$1^+$, and $2^-$. We also obtain accurate results for $2\pi^+$, $\pi^+ K^+$,
and $2K^+$ phase shifts. We compare our results to Chiral Perturbation Theory,
and to phenomenological fits.; 32) Algebras of analytic functionals and homological epimorphisms; It has been proved by the author [arXiv: 2404.19433] that the Arens-Michael
envelope of a solvable Lie algebra is a homological epimorphism. We show here
that for algebras of analytic functionals on a connected complex Lie group the
analogous statement is satisfied without the assumption of solvability, and
furthermore the completion homomorphisms of a more general form are also
homological epimorphisms, including the envelope with respect to the class of
Banach PI-algebras.; 33) Emission photon statistics in collectively interacting dipole atom
  arrays in the low-intensity limit; We investigate the photon statistics of light emitted from a system of
collectively interacting dipoles in the low-intensity regime, incorporating
double-excitation states to capture beyond-single-excitation effects. By
analyzing the eigenstates of the double-excitation manifold, we establish their
connection to the accessible single-excitation eigenmodes and investigate the
role of decay rates in shaping the initial-time photon correlation function
$g^{(2)}(\tau = 0)$ under different detection schemes. By interfering two beams
of light that selectively address orthogonal eigenmodes, the photon emission
statistics can be arbitrarily controlled. This can act as a tunable
nonlinearity that enables both the enhancement and suppression of photon
correlations, extending the operational intensity range for single-photon
applications.; 34) Yu-Shiba-Rusinov states in color superconducting quark matter; The high-density region of the QCD phase diagram displays an intricate
competition between color superconductivity and the QCD Kondo effect due to
color exchange in quark matter containing a single heavy quark impurity. We
explore the characteristic impurity-induced superconducting subgap states
arising in such systems by generalizing the surrogate model solver, recently
considered in the context of condensed matter physics [Phys. Rev. B 108,
L220506 (2023)]. The method consists of approximating the full superconducting
bulk by only a small number of effective levels whose parameters are chosen so
as to best reproduce the Matsubara frequency dependence of the impurity-bulk
hybridization function. We numerically solve a surrogate QCD Kondo model
describing a quantum impurity color-exchange coupled to a two-color two-flavor
superconducting bulk. The results directly indicate the presence of multiple
phase transitions as the coupling of the impurity to the bulk is increased, due
to the interplay between various overscreened states. The methods introduced
here are straightforward enough to be extended to more realistic QCD scenarios.; 35) Rethinking IDE Customization for Enhanced HAX: A Hyperdimensional
  Perspective; As Integrated Development Environments (IDEs) increasingly integrate
Artificial Intelligence, Software Engineering faces both benefits like
productivity gains and challenges like mismatched user preferences. We propose
Hyper-Dimensional (HD) vector spaces to model Human-Computer Interaction,
focusing on user actions, stylistic preferences, and project context. These
contributions aim to inspire further research on applying HD computing in IDE
design.; 36) Effective DoF-Oriented Optimal Antenna Spacing in Near-Field XL-MIMO
  Systems; This letter investigates the optimal antenna spacing for a near-field XL-MIMO
communication system from the perspective of the array gain. Specifically,
using the Green's function-based channel model, the letter analyzes the channel
capacity, which is related to the effective degrees-of-freedom (EDoF). Then,
the letter further investigates the applicability of two EDoF estimation
methods. To increase EDoF, this letter focuses on analyzing the impact of
antenna spacing. Furthermore, from the perspective of the array gain, the
letter derives an approximate closed-form expression of the optimal antenna
spacing, at which EDoF is maximized and the array gain at the antenna nearest
to the focused antenna of the transmit array becomes zero. Finally, numerical
results verify the main results of this letter.; 37) A spinless crystal for a high-performance solid-state $^{229}$Th nuclear
  clock; Solid-state $^{229}$Th nuclear clocks require a host material whose band gap
is larger than the 8.4 eV nuclear transition energy. As such, excitation of the
$^{229}$Th nuclear state has so far only been demonstrated in metal fluorides,
specifically CaF$_2$, LiSrAlF$_6$, and ThF$_4$, where the large
electronegativity of the halogen leads to sufficient band gaps. However, it is
expected that the nuclear magnetic moment of the fluorine gives rise to a
leading order broadening mechanism that limits the clock stability. Here, we
use concepts of molecular design to identify a polyatomic anion, SO$_4^{2-}$,
that is both nuclear spin free and of sufficient electron affinity to result in
a high band gap metal sulfate system. Using state-of-the-art calculations, we
find that the band gap of Th(SO$_4$)$_2$ is approximately 9 eV, large enough
for direct laser excitation of $^{229}$Th. Low concentrations of $^{229}$Th in
the otherwise spinless $^{232}$Th(SO$_4$)$_2$ crystal mitigate
$^{229}$Th-$^{229}$Th interactions. Furthermore, the introduction of $^{229}$Th
does not modify the material band gap nor introduce electronic states
associated with nuclear quenching. By removing one of the primary sources of
nuclear line broadening in the crystal, the nuclear magnetic dipole-dipole
interaction, a nuclear clock with instability as low as $\sigma =
4.6\times10^{-23}/\sqrt{\tau}$, where ${\tau}$ is the averaging time, may be
realized. This is roughly six orders of magnitude lower than previously thought
possible.; 38) Large Language Model Critics for Execution-Free Evaluation of Code
  Changes; Large language models (LLMs) offer a promising way forward for automating
software engineering tasks, such as bug fixes, feature additions, etc., via
multi-step LLM-based agentic workflows. However, existing metrics for
evaluating such workflows, mainly build status and occasionally log analysis,
are too sparse and limited in providing the information needed to assess the
quality of changes made. In this work, we designed LLM-based critics to derive
well-structured and rigorous intermediate/step-level, execution-free evaluation
proxies for repo-level code changes. Importantly, we assume access to the gold
test patch for the problem (i.e., reference-aware) to assess both semantics and
executability of generated patches. With the gold test patch as a reference, we
predict executability of all editing locations with an F1 score of 91.6%,
aggregating which, we can predict the build status in 84.8% of the instances in
SWE-bench. In particular, such an execution-focused LLM critic outperforms
other reference-free and reference-aware LLM critics by 38.9% to 72.5%.
Moreover, we demonstrate the usefulness of such a reference-aware framework in
comparing patches generated by different agentic workflows. Finally, we
open-source the library developed for this project, which allows further usage
for either other agentic workflows or other benchmarks. The source code is
available at https://github.com/amazon-science/code-agent-eval.; 39) EigenShield: Causal Subspace Filtering via Random Matrix Theory for
  Adversarially Robust Vision-Language Models; Vision-Language Models (VLMs) inherit adversarial vulnerabilities of Large
Language Models (LLMs), which are further exacerbated by their multimodal
nature. Existing defenses, including adversarial training, input
transformations, and heuristic detection, are computationally expensive,
architecture-dependent, and fragile against adaptive attacks. We introduce
EigenShield, an inference-time defense leveraging Random Matrix Theory to
quantify adversarial disruptions in high-dimensional VLM representations.
Unlike prior methods that rely on empirical heuristics, EigenShield employs the
spiked covariance model to detect structured spectral deviations. Using a
Robustness-based Nonconformity Score (RbNS) and quantile-based thresholding, it
separates causal eigenvectors, which encode semantic information, from
correlational eigenvectors that are susceptible to adversarial artifacts. By
projecting embeddings onto the causal subspace, EigenShield filters adversarial
noise without modifying model parameters or requiring adversarial training.
This architecture-independent, attack-agnostic approach significantly reduces
the attack success rate, establishing spectral analysis as a principled
alternative to conventional defenses. Our results demonstrate that EigenShield
consistently outperforms all existing defenses, including adversarial training,
UNIGUARD, and CIDER.; 40) Make Shuffling Great Again: A Side-Channel Resistant Fisher-Yates
  Algorithm for Protecting Neural Networks; Neural network models implemented in embedded devices have been shown to be
susceptible to side-channel attacks (SCAs), allowing recovery of proprietary
model parameters, such as weights and biases. There are already available
countermeasure methods currently used for protecting cryptographic
implementations that can be tailored to protect embedded neural network models.
Shuffling, a hiding-based countermeasure that randomly shuffles the order of
computations, was shown to be vulnerable to SCA when the Fisher-Yates algorithm
is used. In this paper, we propose a design of an SCA-secure version of the
Fisher-Yates algorithm. By integrating the masking technique for modular
reduction and Blakely's method for modular multiplication, we effectively
remove the vulnerability in the division operation that led to side-channel
leakage in the original version of the algorithm. We experimentally evaluate
that the countermeasure is effective against SCA by implementing a correlation
power analysis attack on an embedded neural network model implemented on ARM
Cortex-M4. Compared to the original proposal, the memory overhead is $2\times$
the biggest layer of the network, while the time overhead varies from $4\%$ to
$0.49\%$ for a layer with $100$ and $1000$ neurons, respectively.; 41) Steady-state coherence in multipartite quantum systems: its connection
  with thermodynamic quantities and impact on quantum thermal machines; Understanding how coherence of quantum systems affects thermodynamic
quantities, such as work and heat, is essential for harnessing quantumness
effectively in thermal quantum technologies. Here, we study the unique
contributions of quantum coherence among different subsystems of a multipartite
system, specifically in non-equilibrium steady states, to work and heat
currents. Our system comprises two coupled ensembles, each consisting of $N$
particles, interacting with two baths of different temperatures, respectively.
The particles in an ensemble interact with their bath either simultaneously or
sequentially, leading to non-local dissipation and enabling the decomposition
of work and heat currents into local and non-local components.We find that the
non-local heat current, as well as both the local and non-local work
currents,are linked to the system quantum coherence. We provide explicit
expressions of coherence-related quantities that determine the work currents
under various intrasystem interactions.Our scheme is versatile, capable of
functioning as a refrigerator, an engine, and an accelerator, with its
performance being highly sensitive to the configuration settings. These
findings establish a connection between thermodynamic quantities and quantum
coherence, supplying valuable insights for the design of quantum thermal
machines.; 42) Radiation sputtering of hydrocarbon ices at Europa-relevant temperatures; The surfaces of some icy moons, such as Jupiter's moon Europa, are heavily
bombarded by energetic particles that can alter the surface materials and
affect the composition of its exosphere. Detection of CO2 on Europa's surface
indicate that Europa's interior may be transporting freshly exposed
carbon-containing material to the surface. It is unknown whether this CO2 is a
product of radiation of carbon-containing precursors or whether it is present
in the initial deposits. Regardless, further radiolysis by high-energy
electrons or ions can sputter CO2 (and organic fragments if present) into
Europa's exosphere. In this study, we investigate the radiation sputtering of
CO2 and organic fragments from hydrocarbon water ice mixtures at different
Europa-relevant surface temperatures to identify how its sputtering products
evolve over time. This study shows that the sputtering of hydrocarbon water ice
leads to the production of mostly CO2, CO, and fragmented hydrocarbons. The
onset of sputtered hydrocarbons is immediate, and quickly reaches a steady
state, whereas CO2 and CO are formed more gradually. It is found that higher
temperatures cause more sputtering, and that there are some notable differences
in the distribution of species that are sputtered at different temperatures,
indicating local heterogeneity of sputtering yields depending on the surface
temperature.; 43) Semantic-Aware Adaptive Video Streaming Using Latent Diffusion Models
  for Wireless Networks; This paper proposes a novel framework for real-time adaptive-bitrate video
streaming by integrating latent diffusion models (LDMs) within the FFmpeg
techniques. This solution addresses the challenges of high bandwidth usage,
storage inefficiencies, and quality of experience (QoE) degradation associated
with traditional constant bitrate streaming (CBS) and adaptive bitrate
streaming (ABS). The proposed approach leverages LDMs to compress I-frames into
a latent space, offering significant storage and semantic transmission savings
without sacrificing high visual quality. While it keeps B-frames and P-frames
as adjustment metadata to ensure efficient video reconstruction at the user
side, the proposed framework is complemented with the most state-of-the-art
denoising and video frame interpolation (VFI) techniques. These techniques
mitigate semantic ambiguity and restore temporal coherence between frames, even
in noisy wireless communication environments. Experimental results demonstrate
the proposed method achieves high-quality video streaming with optimized
bandwidth usage, outperforming state-of-the-art solutions in terms of QoE and
resource efficiency. This work opens new possibilities for scalable real-time
video streaming in 5G and future post-5G networks.; 44) The Southern Photometrical Local Universe Survey (S-PLUS): searching for
  metal-poor dwarf galaxies; The metal content of a galaxy's interstellar medium reflects the interplay
between different evolutionary processes such as feedback from massive stars
and the accretion of gas from the intergalactic medium. Despite the expected
abundance of low-luminosity galaxies, the low-mass and low-metallicity regime
remains relatively understudied. Since the properties of their interstellar
medium resemble those of early galaxies, identifying such objects in the Local
Universe is crucial to understand the early stages of galaxy evolution. We used
the DR3 catalog of the Southern Photometric Local Universe Survey (S-PLUS) to
select low-metallicity dwarf galaxy candidates based on color selection
criteria typical of metal-poor, star-forming, low-mass systems. The final
sample contains approximately 50 candidates. Spectral energy distribution
fitting of the 12 S-PLUS bands reveals that $\sim$ 90\% of the candidates are
best fit by models with very low stellar metallicities. We obtained long-slit
observations with the Gemini Multi-Object Spectrograph to follow-up a pilot
sample and confirm whether these galaxies have low metallicities. We find
oxygen abundances in the range $7.35<$ 12 + log(O/H) $< 7.93$ (5\% to 17\% of
the solar value), confirming their metal-poor nature. Most targets are outliers
in the mass-metallicity relation, i.e. they display a low metal content
relative to their observed stellar masses. In some cases, perturbed optical
morphologies might give evidence of dwarf-dwarf interactions or mergers. These
results suggest that the low oxygen abundances may be associated with an
external event causing the accretion of metal-poor gas, which dilutes the
oxygen abundance in these systems.; 45) Optimal control on a brain tumor growth model with lactate metabolism,
  viscoelastic effects, and tissue damage; In this paper, we study an optimal control problem for a brain tumor growth
model that incorporates lactate metabolism, viscoelastic effects, and tissue
damage. The PDE system, introduced in [G. Cavalleri, P. Colli, A. Miranville,
E. Rocca, On a Brain Tumor Growth Model with Lactate Metabolism, Viscoelastic
Effects, and Tissue Damage (2025)], couples a Fisher-Kolmogorov type equation
for tumor cell density with a reaction-diffusion equation for the lactate, a
quasi-static force balance governing the displacement, and a nonlinear
differential inclusion for tissue damage. The control variables, representing
chemotherapy and a lactate-targeting drug, influence tumor progression and
treatment response. Starting from well-posedness, regularity, and continuous
dependence results already established, we define a suitable cost functional
and prove the existence of optimal controls. Then, we analyze the
differentiability of the control-to-state operator and establish a necessary
first-order condition for treatment optimality.; 46) Generalized quantum two level model and its application in astrophysics; Complicated time-dependent curved spacetime and electric field are involved
in many astrophysical situations, including the early universe, Hawking
radiation, the Schwinger effect, and gravitational pair production. In this
Letter, a generalized quantum two-level model (GQTLM) is developed, which is
applicable to arbitrary time-dependent curved spacetime and electric field. The
model is found to be consistent with quantum kinetic theory, and is
characterized by its simplicity and versatility. The momentum distribution of
particles and the effects of gravitational distortions can be correctly
described. Quantum properties concerning vortex structures, such as the
intrinsic orbital angular momentum of particles and antiparticles can also be
conveniently calculated. The model is expected to significantly advance the
quantum exploration of the universe. It could refine the prediction of
primordial gravitational waves and relevant non-Gaussian signals, extend the
calculation of Hawking radiation to general black hole configurations, help to
distinguish neutron stars from strange quark stars, and elucidate the
gravitational pair production mechanism.; 47) Construction of Simultaneously Good Polar Codes and Polar Lattices; In this work, we investigate the simultaneous goodness of polar codes and
polar lattices. The simultaneous goodness of a lattice or a code means that it
is optimal for both channel coding and source coding simultaneously. The
existence of such kind of lattices was proven by using random lattice
ensembles. Our work provides an explicit construction based on the polarization
technique.; 48) Spectroscopic signatures of biexcitons: A case study in
  Ruddlesden-Popper lead-halides; Exciton-exciton interactions are fundamental to the light-emitting properties
of semiconductors, influencing applications from lasers to quantum light
sources. In this study, we investigate the spectroscopic signatures and binding
energy of biexcitons in a metal halide two-dimensional Ruddlesden-Popper
structure, which is known for hosting distinct excitonic resonances with unique
lattice coupling. Using three spectroscopic techniques - photoluminescence (PL)
and two variations of two-dimensional electronic spectroscopy (2DES) - we map
coherent one-quantum and two-quantum correlations to gain deeper insight into
the biexciton characteristics. While PL spectroscopy is hindered by spectral
broadening and reabsorption, 2DES provides a more accurate characterization,
revealing multiple biexciton states and uncovering a mixed biexciton species
arising from exciton cross-coupling. These findings highlight the importance of
advanced spectroscopic approaches in accurately determining biexciton binding
energies and offer new perspectives on many-body interactions in
exciton-polarons within layered perovskites.; 49) GEVRM: Goal-Expressive Video Generation Model For Robust Visual
  Manipulation; With the rapid development of embodied artificial intelligence, significant
progress has been made in vision-language-action (VLA) models for general robot
decision-making. However, the majority of existing VLAs fail to account for the
inevitable external perturbations encountered during deployment. These
perturbations introduce unforeseen state information to the VLA, resulting in
inaccurate actions and consequently, a significant decline in generalization
performance. The classic internal model control (IMC) principle demonstrates
that a closed-loop system with an internal model that includes external input
signals can accurately track the reference input and effectively offset the
disturbance. We propose a novel closed-loop VLA method GEVRM that integrates
the IMC principle to enhance the robustness of robot visual manipulation. The
text-guided video generation model in GEVRM can generate highly expressive
future visual planning goals. Simultaneously, we evaluate perturbations by
simulating responses, which are called internal embeddings and optimized
through prototype contrastive learning. This allows the model to implicitly
infer and distinguish perturbations from the external environment. The proposed
GEVRM achieves state-of-the-art performance on both standard and perturbed
CALVIN benchmarks and shows significant improvements in realistic robot tasks.; 50) The Quantum Internet (Technical Version); Following the emergence of quantum computing, the subsequent quantum
revolution will be that of interconnecting individual quantum computers at
global level. In the same way that classical computers only realised their full
potential with the emergence of the internet, a fully realised quantum internet
is the next stage of evolution for quantum computation. This work examines in
detail how the quantum internet would evolve in practice, focusing not only on
the technology itself but also on the implications it will have economically
and politically. We present both original ideas, as well as an extensive review
of relevant and related background material. This work begins with a
description of classical networks before introducing the key concepts behind
quantum networks, such as quantum internet protocols, quantum cryptography, and
cloud quantum computing. The work is divided into technical sections (requiring
only a basic knowledge of the notation of quantum mechanics), for those
interested in mathematical details, as well as non-technical sections for those
seeking a more general understanding. We target this work very broadly at
quantum and classical computer scientists, classical computer systems, software
and network engineers, physicists, economists, artists, musicians, and those
just generally curious about the future of quantum technologies and what they
might bring to humanity.; 51) Beyond Interaction Patterns: Assessing Claims of Coordinated Inter-State
  Information Operations on Twitter/X; Social media platforms have become key tools for coordinated influence
operations, enabling state actors to manipulate public opinion through
strategic, collective actions. While previous research has suggested
collaboration between states, such research failed to leverage state-of-the-art
coordination indicators or control datasets. In this study, we investigate
inter-state coordination by analyzing multiple online behavioral traces and
using sophisticated coordination detection models. By incorporating a control
dataset to differentiate organic user activity from coordinated efforts, our
findings reveal no evidence of inter-state coordination. These results
challenge earlier claims and underscore the importance of robust methodologies
and control datasets in accurately detecting online coordination.; 52) Electron Fourier ptychography for phase reconstruction; Phase reconstruction is important in transmission electron microscopy for
structural studies. We describe electron Fourier ptychography and its
application to phase reconstruction of both radiation-resistant and
beam-sensitive materials. We demonstrate that the phase of the exit wave can be
reconstructed at high resolution using a modified iterative phase retrieval
algorithm with data collected using an alternative optical geometry. This
method achieves a spatial resolution of 0.63 nm at a fluence of $4.5 \times
10^2 \, e^-/\text{nm}^2$, as validated on Cry11Aa protein crystals under
cryogenic conditions. Notably, this method requires no additional hardware
modifications, is straightforward to implement, and can be seamlessly
integrated with existing data collection software, providing a broadly
accessible approach for structural studies.; 53) The H\""{o}lder regularity of div-curl system with anisotropic
  coefficients; This research examines the regularity of weak solutions to the Div-Curl
system with low regularity anisotropic coefficients. The H\""older regularity of
the Div-Curl system with one anisotropic coefficient was an unresolved problem
raised by Yin in 2016. We have addressed the open problem, and the findings
extend to the scenario involving two anisotropic coefficients. We establish the
H\""{o}lder regularity of the solution when the coefficients is H\""{o}lder
continuous. Moreover, the degree of H\""{o}lder regularity of the solution can
be improved if the coefficient has a greater degree of H\""{o}lder regularity.; 54) The Layered Catalan Monoids: Structure and Determinants; In this paper, we introduce and study a class of monoids, called Layered
Catalan Monoids (\( {LC}_n \)), which satisfy the structural conditions for
$\ll$-smoothness as defined in~\cite{Sha-Det2}. These monoids are defined by
specific identities inspired by Catalan monoids. We establish their canonical
forms and compute their determinant, proving that it is non-zero for \(1 \leq n
\leq 7\) but vanishes for \(n \geq 8\).; 55) Purcell-enhanced emissions from diamond color centers in slow light
  photonic crystal waveguides; Quantum memories based on emitters with optically addressable spins rely on
efficient photonic interfaces, often implemented as nanophotonic cavities with
ideally narrow spectral linewidths and small mode volumes. However, these
approaches require nearly perfect spectral and spatial overlap between the
cavity mode and quantum emitter, which can be challenging. This is especially
true in the case of solid-state quantum emitters that are often randomly
positioned and can suffer from significant inhomogeneous broadening. An
alternative approach to mitigate these challenges is to use slow-light
waveguides that can enhance light-matter interaction across large optical
bandwidths and large areas. Here, we demonstrate diamond slow light photonic
crystal (PhC) waveguides that enable broadband optical coupling to embedded
silicon-vacancy (SiV) color centers. We take advantage of the recently
demonstrated thin-film diamond photonic platform to fabricate fully suspended
two-dimensional PhC waveguides. Using this approach, we demonstrate waveguide
modes with high group indices up to 70 and observe Purcell-enhanced emissions
of the SiVs coupled to the waveguide mode. Our approach represents a practical
diamond platform for robust spin-photon interfaces with color centers.; 56) Inhomogeneous Electric Fields for Precise Control and Displacement of
  Polar Textures; Since the discovery of polar topological textures, achieving efficient
control and manipulation of them has emerged as a significant challenge for
their integration into nanoelectronic devices. In this study, we use second
principles molecular dynamic simulations to demonstrate the precise and
reversible control of domain arrangements stabilizing diverse polarization
textures through the application of various inhomogeneous electric fields.
Furthermore, we conduct an in-depth study of ferroelectric domain motion under
such fields, revealing features consistent with creep dynamics and establishing
an upper limit for their propagation speed. Notably, our findings show that
domain walls exhibit an asymmetric inertial response, present at the onset of
the dynamics but absent during their cessation. These findings provide valuable
insights into the dynamic behavior of polar textures, paving the way for the
development of high-speed, low-power nanoelectronic applications.; 57) Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference
  Alignment; Direct Preference Optimization (DPO) has become a prominent method for
aligning Large Language Models (LLMs) with human preferences. While DPO has
enabled significant progress in aligning English LLMs, multilingual preference
alignment is hampered by data scarcity. To address this, we propose a novel
approach that $\textit{captures}$ learned preferences from well-aligned English
models by implicit rewards and $\textit{transfers}$ them to other languages
through iterative training. Specifically, we derive an implicit reward model
from the logits of an English DPO-aligned model and its corresponding reference
model. This reward model is then leveraged to annotate preference relations in
cross-lingual instruction-following pairs, using English instructions to
evaluate multilingual responses. The annotated data is subsequently used for
multilingual DPO fine-tuning, facilitating preference knowledge transfer from
English to other languages. Fine-tuning Llama3 for two iterations resulted in a
12.72% average improvement in Win Rate and a 5.97% increase in Length Control
Win Rate across all training languages on the X-AlpacaEval leaderboard. Our
findings demonstrate that leveraging existing English-aligned models can enable
efficient and effective multilingual preference alignment, significantly
reducing the need for extensive multilingual preference data. The code is
available at https://github.com/ZNLP/Implicit-Cross-Lingual-Rewarding; 58) Distance-Based Tree-Sliced Wasserstein Distance; To overcome computational challenges of Optimal Transport (OT), several
variants of Sliced Wasserstein (SW) has been developed in the literature. These
approaches exploit the closed-form expression of the univariate OT by
projecting measures onto (one-dimensional) lines. However, projecting measures
onto low-dimensional spaces can lead to a loss of topological information.
Tree-Sliced Wasserstein distance on Systems of Lines (TSW-SL) has emerged as a
promising alternative that replaces these lines with a more advanced structure
called tree systems. The tree structures enhance the ability to capture
topological information of the metric while preserving computational
efficiency. However, at the core of TSW-SL, the splitting maps, which serve as
the mechanism for pushing forward measures onto tree systems, focus solely on
the position of the measure supports while disregarding the projecting domains.
Moreover, the specific splitting map used in TSW-SL leads to a metric that is
not invariant under Euclidean transformations, a typically expected property
for OT on Euclidean space. In this work, we propose a novel class of splitting
maps that generalizes the existing one studied in TSW-SL enabling the use of
all positional information from input measures, resulting in a novel
Distance-based Tree-Sliced Wasserstein (Db-TSW) distance. In addition, we
introduce a simple tree sampling process better suited for Db-TSW, leading to
an efficient GPU-friendly implementation for tree systems, similar to the
original SW. We also provide a comprehensive theoretical analysis of proposed
class of splitting maps to verify the injectivity of the corresponding Radon
Transform, and demonstrate that Db-TSW is an Euclidean invariant metric. We
empirically show that Db-TSW significantly improves accuracy compared to recent
SW variants while maintaining low computational cost via a wide range of
experiments.; 59) A possible formation scenario of the Gaia ID 3425577610762832384: inner
  binary merger inside a triple common envelope; Recently, an identified non-interacting black hole (BH) binary, Gaia ID
3425577610762832384 (hereafter G3425), contains a BH ($\sim$3.6 M$_{\odot}$)
falling within the mass gap and has a nearly circular orbit, challenging the
classical binary evolution and supernova theory. Here, we propose that G3425
originates from a triple through a triple common envelope (TCE) evolution. The
G3425 progenitor originally may consist of three stars with masses of 1.49
M$_{\odot}$, 1.05 M$_{\odot}$, and 21.81 M$_{\odot}$, and inner and outer
orbital periods of 4.22 days and 1961.78 days, respectively. As evolution
proceeds, the tertiary fills its Roche lobe, leading to a TCE. We find that the
orbital energy generated by the inspiral of the inner binary serves as an
additional energy imparted for ejecting the common envelope (CE), accounting
for $\sim$97\% of the binding energy in our calculations. This means that the
outer orbit needs to expend only a small amount of the orbital energy to
successfully eject CE. The outcome of the TCE is a binary consisting of a 2.54
M$_\odot$ merger produced by the inner binary merger and a 7.67 M$_\odot$
helium star whose CE successfully ejected, with an orbital period of 547.53
days. The resulting post-TCE binary (PTB) has an orbital period that is 1-2
orders of magnitude greater than the orbital period of a successfully ejected
classical binary CE. In subsequent simulations, we find that the successfully
ejected helium star has a 44.2\% probability of forming a BH. In the case of a
non-complete fallback forming a BH, with an ejected mass of 2.6 M$_{\odot}$ and
a relatively low natal kick ($11^{+16}_{-5}$ ${\rm km/s}$ to $49^{+39}_{-39}$
${\rm km/s}$), this PTB can form G3425 in the Milky Way.; 60) ARLED: Leveraging LED-based ARMAN Model for Abstractive Summarization of
  Persian Long Documents; The increasing volume of textual data poses challenges in reading and
comprehending large documents, particularly for scholars who need to extract
useful information from research articles. Automatic text summarization has
emerged as a powerful tool to condense lengthy documents into concise and
informative summaries. Depending on the approach used, text summarization can
be categorized as either extractive or abstractive. While extractive methods
are commonly used due to their simplicity, they often miss important
information. On the other hand, Abstractive Summarization can generate more
coherent and informative summaries by understanding the underlying meaning of
the text. Abstractive techniques have gained attention in various languages,
and recent advancements have been achieved through pre-training models such as
BERT, BART, and T5. However, the challenge of summarizing long documents
remains, and alternative models like Longformer have been introduced to address
this limitation. In this context, this paper focuses on abstractive
summarization in the Persian language. The authors introduce a new dataset of
300,000 full-text Persian papers obtained from the Ensani website and apply the
ARMAN model, based on the Longformer architecture, to generate summaries. The
experimental results demonstrate promising performance in Persian text
summarization. The paper provides a comprehensive overview of related work,
discusses the methodology, presents the experimental results, and concludes
with future research directions.; 61) FCVSR: A Frequency-aware Method for Compressed Video Super-Resolution; Compressed video super-resolution (SR) aims to generate high-resolution (HR)
videos from the corresponding low-resolution (LR) compressed videos. Recently,
some compressed video SR methods attempt to exploit the spatio-temporal
information in the frequency domain, showing great promise in super-resolution
performance. However, these methods do not differentiate various frequency
subbands spatially or capture the temporal frequency dynamics, potentially
leading to suboptimal results. In this paper, we propose a deep frequency-based
compressed video SR model (FCVSR) consisting of a motion-guided adaptive
alignment (MGAA) network and a multi-frequency feature refinement (MFFR)
module. Additionally, a frequency-aware contrastive loss is proposed for
training FCVSR, in order to reconstruct finer spatial details. The proposed
model has been evaluated on three public compressed video super-resolution
datasets, with results demonstrating its effectiveness when compared to
existing works in terms of super-resolution performance (up to a 0.14dB gain in
PSNR over the second-best model) and complexity.; 62) Best Transition Matrix Esitimation or Best Label Noise Robustness
  Classifier? Two Possible Methods to Enhance the Performance of T-revision; Label noise refers to incorrect labels in a dataset caused by human errors or
collection defects, which is common in real-world applications and can
significantly reduce the accuracy of models. This report explores how to
estimate noise transition matrices and construct deep learning classifiers that
are robust against label noise. In cases where the transition matrix is known,
we apply forward correction and importance reweighting methods to correct the
impact of label noise using the transition matrix. When the transition matrix
is unknown or inaccurate, we use the anchor point assumption and T-Revision
series methods to estimate or correct the noise matrix. In this study, we
further improved the T-Revision method by developing T-Revision-Alpha and
T-Revision-Softmax to enhance stability and robustness. Additionally, we
designed and implemented two baseline classifiers, a Multi-Layer Perceptron
(MLP) and ResNet-18, based on the cross-entropy loss function. We compared the
performance of these methods on predicting clean labels and estimating
transition matrices using the FashionMINIST dataset with known noise transition
matrices. For the CIFAR-10 dataset, where the noise transition matrix is
unknown, we estimated the noise matrix and evaluated the ability of the methods
to predict clean labels.; 63) A binary PSO based ensemble under-sampling model for rebalancing
  imbalanced training data; Ensemble technique and under-sampling technique are both effective tools used
for imbalanced dataset classification problems. In this paper, a novel ensemble
method combining the advantages of both ensemble learning for biasing
classifiers and a new under-sampling method is proposed. The under-sampling
method is named Binary PSO instance selection; it gathers with ensemble
classifiers to find the most suitable length and combination of the majority
class samples to build a new dataset with minority class samples. The proposed
method adopts multi-objective strategy, and contribution of this method is a
notable improvement of the performances of imbalanced classification, and in
the meantime guaranteeing a best integrity possible for the original dataset.
We experimented the proposed method and compared its performance of processing
imbalanced datasets with several other conventional basic ensemble methods.
Experiment is also conducted on these imbalanced datasets using an improved
version where ensemble classifiers are wrapped in the Binary PSO instance
selection. According to experimental results, our proposed methods outperform
single ensemble methods, state-of-the-art under-sampling methods, and also
combinations of these methods with the traditional PSO instance selection
algorithm.; 64) Emamectin benzoate sensing using vivianenes (2D vivianites); The excessive application of pesticides, particularly the overreliance on
insecticides for the protection of desirable crops from pests, has posed a
significant threat to both ecological systems and human health due to
environmental pollution. This research outlines a comprehensive approach to
recognizing and quantifying the presence of insecticides through the
application of spectroscopic and electrochemical sensing methods. The detection
of Emamectin benzoate (EB), a commonly used insecticide, was performed
utilizing vivianenes, a 2D phosphate that has been mechanically exfoliated from
the naturally occurring vivianite minerals. This investigation examined the
structural and compositional characteristics of vivianenes, utilizing a range
of characterization methods. The spectroscopic analyses reveal the molecular
interactions and structural modifications that take place during the
interaction of EB with the 2D template. Electrochemical investigations
employing cyclic voltammetry were performed for different concentrations of EB
to enable real-time monitoring of the pesticide. The modified sensing electrode
using vivianene demonstrated a linear range of from 50 mg/L to 10 micro g/L,
effectively detecting EB molecules at levels significantly below the hazardous
threshold. Fully atomistic molecular dynamics simulations were also carried out
to obtain further insights into the interaction mechanisms of the EB with the
vivianites, and the results corroborate the adsorption mechanism. Our results
highlight the potential application of 2D phosphate minerals as advanced
sensors to enhance agricultural monitoring and promote sustainable development.; 65) DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT; Shannon capacity of signal transduction for multiple independent receptors; Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver.; 66) Supersymmetric scale-separated AdS$_3$ vacua of type IIB; I construct supersymmetric, parametrically scale-separated AdS$_3$ vacua of
type IIB string theory. These arise as compactifications with orientifold
planes on specific seven-dimensional solvmanifolds admitting co-closed
$G_2$-structures, preserving minimal supersymmetry. There are solutions that
include either one set or four sets of intersecting O5-planes in the smeared
approximation, and parametric scale separation can be achieved by tuning
unbounded fluxes to infinity. Additionally, the putative holographic field
theory operators that are dual to the lightest scalars in the gravitational
theory have integer conformal dimensions at tree level, aligning with other
scale-separated models of type II string theory.; 67) SVIP: Semantically Contextualized Visual Patches for Zero-Shot Learning; Zero-shot learning (ZSL) aims to recognize unseen classes without labeled
training examples by leveraging class-level semantic descriptors such as
attributes. A fundamental challenge in ZSL is semantic misalignment, where
semantic-unrelated information involved in visual features introduce ambiguity
to visual-semantic interaction. Unlike existing methods that suppress
semantic-unrelated information post hoc either in the feature space or the
model space, we propose addressing this issue at the input stage, preventing
semantic-unrelated patches from propagating through the network. To this end,
we introduce Semantically contextualized VIsual Patches (SVIP) for ZSL, a
transformer-based framework designed to enhance visual-semantic alignment.
Specifically, we propose a self-supervised patch selection mechanism that
preemptively learns to identify semantic-unrelated patches in the input space.
This is trained with the supervision from aggregated attention scores across
all transformer layers, which estimate each patch's semantic score. As removing
semantic-unrelated patches from the input sequence may disrupt object
structure, we replace them with learnable patch embeddings. With initialization
from word embeddings, we can ensure they remain semantically meaningful
throughout feature extraction. Extensive experiments on ZSL benchmarks
demonstrate that SVIP achieves state-of-the-art performance results while
providing more interpretable and semantically rich feature representations.; 68) MolSpectra: Pre-training 3D Molecular Representation with Multi-modal
  Energy Spectra; Establishing the relationship between 3D structures and the energy states of
molecular systems has proven to be a promising approach for learning 3D
molecular representations. However, existing methods are limited to modeling
the molecular energy states from classical mechanics. This limitation results
in a significant oversight of quantum mechanical effects, such as quantized
(discrete) energy level structures, which offer a more accurate estimation of
molecular energy and can be experimentally measured through energy spectra. In
this paper, we propose to utilize the energy spectra to enhance the
pre-training of 3D molecular representations (MolSpectra), thereby infusing the
knowledge of quantum mechanics into the molecular representations.
Specifically, we propose SpecFormer, a multi-spectrum encoder for encoding
molecular spectra via masked patch reconstruction. By further aligning outputs
from the 3D encoder and spectrum encoder using a contrastive objective, we
enhance the 3D encoder's understanding of molecules. Evaluations on public
benchmarks reveal that our pre-trained representations surpass existing methods
in predicting molecular properties and modeling dynamics.; 69) On the time constant of high dimensional first passage percolation,
  revisited; In [2], it was claimed that the time constant $\mu_{d}(e_{1})$ for the
first-passage percolation model on $\mathbb Z^{d}$ is $\mu_{d}(e_{1}) \sim \log
d/(2ad)$ as $d\to \infty$, if the passage times $(\tau_{e})_{e\in \mathbb
E^{d}}$ are i.i.d., with a common c.d.f. $F$ satisfying
$\left|\frac{F(x)}{x}-a\right| \le \frac{C}{|\log x|}$ for some constants $a,
C$ and sufficiently small $x$.
  However, the proof of the upper bound, namely, Equation (2.1) in [2]
\begin{align} \limsup_{d\to\infty} \frac{\mu_{d}(e_{1})ad}{\log d} \le
\frac{1}{2} \end{align} is incorrect. In this article, we provide a different
approach that establishes this inequality. As a side product of this new
method, we also show that the variance of the non-backtracking passage time to
the first hyperplane is of order $o\big((\log d/d)^{2}\big)$ as $d\to \infty$
in the case of the when the edge weights are exponentially distributed.; 70) Entity-aware Cross-lingual Claim Detection for Automated Fact-checking; Identifying claims requiring verification is a critical task in automated
fact-checking, especially given the proliferation of misinformation on social
media platforms. Despite significant progress in the task, there remain open
challenges such as dealing with multilingual and multimodal data prevalent in
online discourse. Addressing the multilingual challenge, recent efforts have
focused on fine-tuning pre-trained multilingual language models. While these
models can handle multiple languages, their ability to effectively transfer
cross-lingual knowledge for detecting claims spreading on social media remains
under-explored. In this paper, we introduce EX-Claim, an entity-aware
cross-lingual claim detection model that generalizes well to handle claims
written in any language. The model leverages entity information derived from
named entity recognition and entity linking techniques to improve the
language-level performance of both seen and unseen languages during training.
Extensive experiments conducted on three datasets from different social media
platforms demonstrate that our proposed model significantly outperforms the
baselines, across 27 languages, and achieves the highest rate of knowledge
transfer, even with limited training data.; 71) Redefining Robot Generalization Through Interactive Intelligence; Recent advances in large-scale machine learning have produced high-capacity
foundation models capable of adapting to a broad array of downstream tasks.
While such models hold great promise for robotics, the prevailing paradigm
still portrays robots as single, autonomous decision-makers, performing tasks
like manipulation and navigation, with limited human involvement. However, a
large class of real-world robotic systems, including wearable robotics (e.g.,
prostheses, orthoses, exoskeletons), teleoperation, and neural interfaces, are
semiautonomous, and require ongoing interactive coordination with human
partners, challenging single-agent assumptions. In this position paper, we
argue that robot foundation models must evolve to an interactive multi-agent
perspective in order to handle the complexities of real-time human-robot
co-adaptation. We propose a generalizable, neuroscience-inspired architecture
encompassing four modules: (1) a multimodal sensing module informed by
sensorimotor integration principles, (2) an ad-hoc teamwork model reminiscent
of joint-action frameworks in cognitive science, (3) a predictive world belief
model grounded in internal model theories of motor control, and (4) a
memory/feedback mechanism that echoes concepts of Hebbian and
reinforcement-based plasticity. Although illustrated through the lens of cyborg
systems, where wearable devices and human physiology are inseparably
intertwined, the proposed framework is broadly applicable to robots operating
in semi-autonomous or interactive contexts. By moving beyond single-agent
designs, our position emphasizes how foundation models in robotics can achieve
a more robust, personalized, and anticipatory level of performance.; 72) Development and Validation of the Provider Documentation Summarization
  Quality Instrument for Large Language Models; As Large Language Models (LLMs) are integrated into electronic health record
(EHR) workflows, validated instruments are essential to evaluate their
performance before implementation. Existing instruments for provider
documentation quality are often unsuitable for the complexities of
LLM-generated text and lack validation on real-world data. The Provider
Documentation Summarization Quality Instrument (PDSQI-9) was developed to
evaluate LLM-generated clinical summaries. Multi-document summaries were
generated from real-world EHR data across multiple specialties using several
LLMs (GPT-4o, Mixtral 8x7b, and Llama 3-8b). Validation included Pearson
correlation for substantive validity, factor analysis and Cronbach's alpha for
structural validity, inter-rater reliability (ICC and Krippendorff's alpha) for
generalizability, a semi-Delphi process for content validity, and comparisons
of high-versus low-quality summaries for discriminant validity. Seven physician
raters evaluated 779 summaries and answered 8,329 questions, achieving over 80%
power for inter-rater reliability. The PDSQI-9 demonstrated strong internal
consistency (Cronbach's alpha = 0.879; 95% CI: 0.867-0.891) and high
inter-rater reliability (ICC = 0.867; 95% CI: 0.867-0.868), supporting
structural validity and generalizability. Factor analysis identified a 4-factor
model explaining 58% of the variance, representing organization, clarity,
accuracy, and utility. Substantive validity was supported by correlations
between note length and scores for Succinct (rho = -0.200, p = 0.029) and
Organized ($\rho = -0.190$, $p = 0.037$). Discriminant validity distinguished
high- from low-quality summaries ($p < 0.001$). The PDSQI-9 demonstrates robust
construct validity, supporting its use in clinical practice to evaluate
LLM-generated summaries and facilitate safer integration of LLMs into
healthcare workflows.; 73) Quasiconvexity and self-improving size estimates; We show that M\""uller's $L\log L$ bound $$F(Du)\geq 0,\,Du\in
L^p_{\mathrm{loc}}(\mathbb{R}^n)\implies F(Du)\in L\log
L_{\mathrm{loc}}(\mathbb{R}^n)$$ for $F =\det$ and $p=n$ holds for quasiconcave
$F$ which are homogeneous of degree $p>1$. This contrasts similar Hardy space
bounds which hold only for null Lagrangians.; 74) Control Strategy for Generalized Synchrony in Coupled Dynamical Systems; Dynamical systems can be coupled in a manner that is designed to drive the
resulting dynamics onto a specified lower dimensional submanifold in the phase
space of the combined system. On the submanifold, the variables of the two
systems have a well-defined unique functional relationship. This process can
thus be viewed as a control technique that ensures generalized synchronization.
Depending on the nature of the dynamical systems and the specified submanifold,
different coupling functions can be derived in order to achieve a desired
control objective. We discuss the circuit implementations of this strategy in
representative examples of coupled chaotic dynamical systems, namely Lorenz
oscillators; 75) Equivalence of the pearly tree immersed Lagrangian Floer theory and the
  Hamiltonian immersed Lagrangian Floer theory; The goal of this paper is to prove an equivalence relation between the
immersed Lagrangian Floer theory, defined using pearly tree discs, and local
Hamiltonian flows, i.e., Hamiltonian flows performed in the Weinstein tubular
neighborhood. This is a generalization of Alston-Bao's work.; 76) Distributed U-net model and Image Segmentation for Lung Cancer Detection; Until now, in the wake of the COVID-19 pandemic in 2019, lung diseases,
especially diseases such as lung cancer and chronic obstructive pulmonary
disease (COPD), have become an urgent global health issue. In order to mitigate
the goal problem, early detection and accurate diagnosis of these conditions
are critical for effective treatment and improved patient outcomes. To further
research and reduce the error rate of hospital diagnoses, this comprehensive
study explored the potential of computer-aided design (CAD) systems, especially
utilizing advanced deep learning models such as U-Net. And compared with the
literature content of other authors, this study explores the capabilities of
U-Net in detail, and enhances the ability to simulate CAD systems through the
VGG16 algorithm. An extensive dataset consisting of lung CT images and
corresponding segmentation masks, curated collaboratively by multiple academic
institutions, serves as the basis for empirical validation. In this paper, the
efficiency of U-Net model is evaluated rigorously and precisely under multiple
hardware configurations, such as single CPU, single GPU, distributed GPU and
federated learning, and the effectiveness and development of the method in the
segmentation task of lung disease are demonstrated. Empirical results clearly
affirm the robust performance of the U-Net model, most effectively utilizing
four GPUs for distributed learning, and these results highlight the potential
of U-Net-based CAD systems for accurate and timely lung disease detection and
diagnosis huge potential.; 77) Evaluating Spoken Language as a Biomarker for Automated Screening of
  Cognitive Impairment; Timely and accurate assessment of cognitive impairment is a major unmet need
in populations at risk. Alterations in speech and language can be early
predictors of Alzheimer's disease and related dementias (ADRD) before clinical
signs of neurodegeneration. Voice biomarkers offer a scalable and non-invasive
solution for automated screening. However, the clinical applicability of
machine learning (ML) remains limited by challenges in generalisability,
interpretability, and access to patient data to train clinically applicable
predictive models. Using DementiaBank recordings (N=291, 64% female), we
evaluated ML techniques for ADRD screening and severity prediction from spoken
language. We validated model generalisability with pilot data collected
in-residence from older adults (N=22, 59% female). Risk stratification and
linguistic feature importance analysis enhanced the interpretability and
clinical utility of predictions. For ADRD classification, a Random Forest
applied to lexical features achieved a mean sensitivity of 69.4% (95%
confidence interval (CI) = 66.4-72.5) and specificity of 83.3% (78.0-88.7). On
real-world pilot data, this model achieved a mean sensitivity of 70.0%
(58.0-82.0) and specificity of 52.5% (39.3-65.7). For severity prediction using
Mini-Mental State Examination (MMSE) scores, a Random Forest Regressor achieved
a mean absolute MMSE error of 3.7 (3.7-3.8), with comparable performance of 3.3
(3.1-3.5) on pilot data. Linguistic features associated with higher ADRD risk
included increased use of pronouns and adverbs, greater disfluency, reduced
analytical thinking, lower lexical diversity and fewer words reflecting a
psychological state of completion. Our interpretable predictive modelling
offers a novel approach for in-home integration with conversational AI to
monitor cognitive health and triage higher-risk individuals, enabling earlier
detection and intervention.; 78) The Economics of Large Language Models: Token Allocation, Fine-Tuning,
  and Optimal Pricing; We develop an economic framework to analyze the optimal pricing and product
design of Large Language Models (LLM). Our framework captures several key
features of LLMs: variable operational costs of processing input and output
tokens; the ability to customize models through fine-tuning; and
high-dimensional user heterogeneity in terms of task requirements and error
sensitivity. In our model, a monopolistic seller offers multiple versions of
LLMs through a menu of products. The optimal pricing structure depends on
whether token allocation across tasks is contractible and whether users face
scale constraints. Users with similar aggregate value-scale characteristics
choose similar levels of fine-tuning and token consumption. The optimal
mechanism can be implemented through menus of two-part tariffs, with higher
markups for more intensive users. Our results rationalize observed industry
practices such as tiered pricing based on model customization and usage levels.; 79) Retrievals Can Be Detrimental: A Contrastive Backdoor Attack Paradigm on
  Retrieval-Augmented Diffusion Models; Diffusion models (DMs) have recently demonstrated remarkable generation
capability. However, their training generally requires huge computational
resources and large-scale datasets. To solve these, recent studies empower DMs
with the advanced Retrieval-Augmented Generation (RAG) technique and propose
retrieval-augmented diffusion models (RDMs). By incorporating rich knowledge
from an auxiliary database, RAG enhances diffusion models' generation and
generalization ability while significantly reducing model parameters. Despite
the great success, RAG may introduce novel security issues that warrant further
investigation. In this paper, we reveal that the RDM is susceptible to backdoor
attacks by proposing a multimodal contrastive attack approach named BadRDM. Our
framework fully considers RAG's characteristics and is devised to manipulate
the retrieved items for given text triggers, thereby further controlling the
generated contents. Specifically, we first insert a tiny portion of images into
the retrieval database as target toxicity surrogates. Subsequently, a malicious
variant of contrastive learning is adopted to inject backdoors into the
retriever, which builds shortcuts from triggers to the toxicity surrogates.
Furthermore, we enhance the attacks through novel entropy-based selection and
generative augmentation strategies that can derive better toxicity surrogates.
Extensive experiments on two mainstream tasks demonstrate the proposed BadRDM
achieves outstanding attack effects while preserving the model's benign
utility.; 80) A muon tagging with Flash ADC waveform baselines; This manuscript describes an innovative method to tag the muons using the
baseline information of the Flash ADC (FADC) waveform of PMTs in the JSNS1
(J-PARC Sterile Neutrino Search at J-PARC Spallation Neutron Source)
experiment. This experiment is designed for the search for sterile neutrinos,
and a muon tagging is an essential key component for the background rejection
since the detector of the experiment is located over-ground, where is the 3rd
floor of the J-PARC Material and Life experimental facility (MLF). Especially,
stopping muons inside the detector create the Michel electrons, and they are
important background to be rejected. Utilizing this innovative method, more
than 99.8% of Michel electrons can be rejected even without a detector veto
region. This technique can be employed for any experiments which uses the
similar detector configurations.; 81) A Lightweight and Secure Deep Learning Model for Privacy-Preserving
  Federated Learning in Intelligent Enterprises; The ever growing Internet of Things (IoT) connections drive a new type of
organization, the Intelligent Enterprise. In intelligent enterprises, machine
learning based models are adopted to extract insights from data. Due to the
efficiency and privacy challenges of these traditional models, a new federated
learning (FL) paradigm has emerged. In FL, multiple enterprises can jointly
train a model to update a final model. However, firstly, FL trained models
usually perform worse than centralized models, especially when enterprises
training data is non-IID (Independent and Identically Distributed). Second, due
to the centrality of FL and the untrustworthiness of local enterprises,
traditional FL solutions are vulnerable to poisoning and inference attacks and
violate privacy. Thirdly, the continuous transfer of parameters between
enterprises and servers increases communication costs. To this end, the
FedAnil+ model is proposed, a novel, lightweight, and secure Federated Deep
Learning Model that includes three main phases. In the first phase, the goal is
to solve the data type distribution skew challenge. Addressing privacy concerns
against poisoning and inference attacks is covered in the second phase.
Finally, to alleviate the communication overhead, a novel compression approach
is proposed that significantly reduces the size of the updates. The experiment
results validate that FedAnil+ is secure against inference and poisoning
attacks with better accuracy. In addition, it shows improvements over existing
approaches in terms of model accuracy (13%, 16%, and 26%), communication cost
(17%, 21%, and 25%), and computation cost (7%, 9%, and 11%).; 82) Volume Tells: Dual Cycle-Consistent Diffusion for 3D Fluorescence
  Microscopy De-noising and Super-Resolution; 3D fluorescence microscopy is essential for understanding fundamental life
processes through long-term live-cell imaging. However, due to inherent issues
in imaging principles, it faces significant challenges including spatially
varying noise and anisotropic resolution, where the axial resolution lags
behind the lateral resolution up to 4.5 times. Meanwhile, laser power is kept
low to maintain cell viability, leading to inaccessible low-noise and
high-resolution paired ground truth (GT). To tackle these limitations, a dual
Cycle-consistent Diffusion is proposed to effectively mine intra-volume imaging
priors within 3D cell volumes in an unsupervised manner, i.e., Volume Tells
(VTCD), achieving de-noising and super-resolution (SR) simultaneously.
Specifically, a spatially iso-distributed denoiser is designed to exploit the
noise distribution consistency between adjacent low-noise and high-noise
regions within the 3D cell volume, suppressing the spatially varying noise.
Then, in light of the structural consistency of the cell volume, a cross-plane
global-propagation SR module propagates high-resolution details from the XY
plane into adjacent regions in the XZ and YZ planes, progressively enhancing
resolution across the entire 3D cell volume. Experimental results on 10 in vivo
cellular dataset demonstrate high improvements in both denoising and
super-resolution, with axial resolution enhanced from ~ 430 nm to ~ 90 nm.; 83) Network-centric optimal hybrid sensing hole recovery and self-healing in
  IPV6 WSNs; In our earlier work, Network-Centric Optimal Hybrid Mobility for IPv6
wireless sensor networks, in which the work sought to control mobility of
sensor nodes from an external network was proposed. It was a major improvement
on earlier works such as Cluster Sensor Proxy Mobile IPv6 (CSPMIPv6) and
Network of Proxies (NoP). In this work, the Network-Centric optimal hybrid
mobility scenario was used to detect and fill sensing holes occurring as a
result damaged or energy depleted sensing nodes. Various sensor networks
self-healing and recovery, and deployment algorithms such as Enhanced Virtual
Forces Algorithm with Boundary Forces (EVFA-B); Coverage - Aware Sensor
Automation protocol (CASA); Sensor Self-Organizing Algorithm (SSOA); VorLag and
the use of the use of anchor and relay nodes were reviewed. With node density
thresholds set for various scenarios, the recovery efficiency using various
parameters were measured. Comparably, our method provides the most efficient
node relocation and self-healing mechanism for sensor networks. Compared to
Sensor Self-Organizing Algorithm (SSOA), Hybrid Mobile IP showed superiority in
coverage, shorter period of recovery, less computational cost and lower energy
depletion. With processing and mobility costs shifted to the external network,
Hybrid Mobile IP extends the life span of the network.; 84) Spectral properties from an efficient analytical representation of the
  $GW$ self-energy within a multipole approximation; We propose an efficient analytical representation of the frequency-dependent
$GW$ self-energy $\Sigma$ via a multipole approximation (MPA-$\Sigma$). The
multipole-Pad\'e model for the self-energy is interpolated from a small set of
numerical evaluations of $\Sigma$ in the complex frequency plane, similarly to
the previously multipole representation developed for the screened Coulomb
interaction (MPA-$W$) [Phys. Rev. B \textbf{104}, 115157 (2021)]. We show that,
likewise MPA-$W$, an appropriate choice of frequency sampling in MPA-$\Sigma$
is critical to guarantee computational efficiency and high accuracy. The
combined MPA-$W$ and MPA-$\Sigma$ scheme considerably reduces the cost of
full-frequency self-energy calculations, especially for spectral band
structures over a wide energy range. Crucially, MPA-$\Sigma$ enables a
multipole representation for the interacting Green's function $G$ (MPA-$G$),
providing a straightforward evaluation of all the spectral properties, and a
more general way to define the renormalization factor $Z$. We validate the
MPA-$\Sigma$ and MPA-$G$ approaches for diverse systems: bulk Si, Na and Cu,
monolayer MoS$_2$, the NaCl ion-pair and the F$_2$ molecule. Moreover, we
introduce toy MPA-$\Sigma$/$G$ models to examine the quasiparticle picture in
different regimens of weak and strong correlation. With these models, we expose
limitations in defining $Z$ from the local derivative of $\Sigma$.; 85) Formation of filaments and feathers in disc galaxies: Is self-gravity
  enough?; Context. Dense filaments/feathers are kpc-scale dusty features present in
nearby main sequence galaxies. Distinct from the spiral arms, filaments
constitute a major portion of dense gas concentration. They are expected to
play an important role in star formation and are known to harbour star-forming
regions and H II regions.
  Aims. We explore the origin of filaments/feathers in disc galaxies via global
gravitational instability.
  Methods. We conduct a parameter study using three-dimensional hydrodynamical
simulations of isolated disc galaxies that are isothermal, self-gravitating and
initialised in equilibrium. Our galaxies are uniquely characterised by two
dimensionless parameters, the Toomre $Q$ and the rotational Mach number,
$\mathcal{M}_{\rm c} = v_{\rm c}/c_{\rm s}$ (ratio of circular velocity to
sound speed). We carry out simulations covering a wide range in both.
  Results. We find that galaxies with $Q = 1$ form filaments within a single
rotation, while galaxies with $Q \geq 2$ do not. These filaments are kpc long
and are semi-regularly spaced along the azimuth. Their morphology, density
contrast and formation timescale vary with $\mathcal{M}_{\rm c}$, with filament
spacing and instability onset time both inversely proportional to
$\mathcal{M}_{\rm c}$ and the density contrast increasing with
$\mathcal{M}_{\rm c}$. However, their growth rates in all $Q = 1$ galaxies are
$\sim 0.5~\Omega$, where $\Omega$ is the angular frequency. We compare the
filament spacing in our simulations with the ones from JWST/MIRI and HST
observations of nearby galaxies and find them in agreement.
  Conclusions. Our study suggests that self-gravity and rotation are sufficient
to form filaments, even in the absence of spiral arms or magnetic fields. Their
morphologies are primarily determined by $\mathcal{M}_{\rm c}$, which
parametrises the importance of thermal versus rotational support.; 86) Graph Contrastive Learning on Multi-label Classification for
  Recommendations; In business analysis, providing effective recommendations is essential for
enhancing company profits. The utilization of graph-based structures, such as
bipartite graphs, has gained popularity for their ability to analyze complex
data relationships. Link prediction is crucial for recommending specific items
to users. Traditional methods in this area often involve identifying patterns
in the graph structure or using representational techniques like graph neural
networks (GNNs). However, these approaches encounter difficulties as the volume
of data increases. To address these challenges, we propose a model called Graph
Contrastive Learning for Multi-label Classification (MCGCL). MCGCL leverages
contrastive learning to enhance recommendation effectiveness. The model
incorporates two training stages: a main task and a subtask. The main task is
holistic user-item graph learning to capture user-item relationships. The
homogeneous user-user (item-item) subgraph is constructed to capture user-user
and item-item relationships in the subtask. We assessed the performance using
real-world datasets from Amazon Reviews in multi-label classification tasks.
Comparative experiments with state-of-the-art methods confirm the effectiveness
of MCGCL, highlighting its potential for improving recommendation systems.; 87) Scanning gate microscopy detection of Majorana bound states; We theoretically study scanning gate microscopy of a
superconductor-proximitized semiconducting wire focusing on the possibility of
detection of Majorana bound states. We exploit the possibility to create local
potential perturbation by the scanning gate tip which allows controllable
modification of the spatial distribution of the Majorana modes, which is
translated into changes in their energy structure. When the tip scans across
the system, it effectively divides the wire into two parts with controllable
lengths, in which two pairs of Majorana states are created when the system is
in the topological regime. For strong values of the tip potential the pairs are
decoupled, and the presence of Majorana states can be detected via local
tunneling spectroscopy that resolves the energy splittings resulting from the
Majorana states wave functions overlap. Importantly, as the system is probed
spatially via the tip, this technique can distinguish Majorana bound states
from quasi-Majorana states localized on smooth potential barriers. We
demonstrate that for weaker tip potentials the two neighboring Majorana states
hybridize opening pronounced anticrossings in the energy spectra which are
reflected in local conductance maps and which result in non-zero non-local
conductance features. Finally, we demonstrate the effect of the disorder on the
scanning gate microscopy spectroscopy maps.; 88) Construction of logarithmic cohomology theories I; We propose a method for constructing cohomology theories of logarithmic
schemes with strict normal crossing boundaries by employing techniques from
logarithmic motivic homotopy theory over $\mathbb{F}_1$. This method recovers
the K-theory of the open complement of a strict normal crossing divisor from
the K-theory of schemes as well as logarithmic topological Hochschild homology
from the topological Hochschild homology of schemes. In our applications, we
establish that the K-theory of non-regular schemes is representable in the
logarithmic motivic homotopy category, and we introduce the logarithmic
cyclotomic trace for the regular log regular case.; 89) Strategies for political-statement segmentation and labelling in
  unstructured text; Analysis of parliamentary speeches and political-party manifestos has become
an integral area of computational study of political texts. While speeches have
been overwhelmingly analysed using unsupervised methods, a large corpus of
manifestos with by-statement political-stance labels has been created by the
participants of the MARPOR project. It has been recently shown that these
labels can be predicted by a neural model; however, the current approach relies
on provided statement boundaries, limiting out-of-domain applicability. In this
work, we propose and test a range of unified split-and-label frameworks --
based on linear-chain CRFs, fine-tuned text-to-text models, and the combination
of in-context learning with constrained decoding -- that can be used to jointly
segment and classify statements from raw textual data. We show that our
approaches achieve competitive accuracy when applied to raw text of political
manifestos, and then demonstrate the research potential of our method by
applying it to the records of the UK House of Commons and tracing the political
trajectories of four major parties in the last three decades.; 90) Disharmony: Forensics using Reverse Lighting Harmonization; Content generation and manipulation approaches based on deep learning methods
have seen significant advancements, leading to an increased need for techniques
to detect whether an image has been generated or edited. Another area of
research focuses on the insertion and harmonization of objects within images.
In this study, we explore the potential of using harmonization data in
conjunction with a segmentation model to enhance the detection of edited image
regions. These edits can be either manually crafted or generated using deep
learning methods. Our findings demonstrate that this approach can effectively
identify such edits. Existing forensic models often overlook the detection of
harmonized objects in relation to the background, but our proposed Disharmony
Network addresses this gap. By utilizing an aggregated dataset of harmonization
techniques, our model outperforms existing forensic networks in identifying
harmonized objects integrated into their backgrounds, and shows potential for
detecting various forms of edits, including virtual try-on tasks.; 91) Assessing Teamwork Dynamics in Software Development Projects; This study investigates teamwork dynamics in student software development
projects through a mixed-method approach combining quantitative analysis of
GitLab commit logs and qualitative survey data. We analyzed individual
contributions across six project phases, comparing self-reported and actual
contributions to measure discrepancies. Additionally, a survey captured
insights on team leadership, conflict resolution, communication practices, and
workload perceptions. Findings reveal that teams with minimal contribution
discrepancies achieved higher project grades and exam pass rates. In contrast,
teams with more significant discrepancies experienced lower performance,
potentially due to role clarity and communication issues. These results
underscore the value of shared leadership, structured conflict resolution, and
regular feedback in fostering effective teamwork, offering educators strategies
to enhance collaboration in software engineering education through
self-reflection and balanced workload allocation.; 92) Learned Bayesian Cram\'er-Rao Bound for Unknown Measurement Models Using
  Score Neural Networks; The Bayesian Cram\'er-Rao bound (BCRB) is a crucial tool in signal processing
for assessing the fundamental limitations of any estimation problem as well as
benchmarking within a Bayesian frameworks. However, the BCRB cannot be computed
without full knowledge of the prior and the measurement distributions. In this
work, we propose a fully learned Bayesian Cram\'er-Rao bound (LBCRB) that
learns both the prior and the measurement distributions. Specifically, we
suggest two approaches to obtain the LBCRB: the Posterior Approach and the
Measurement-Prior Approach. The Posterior Approach provides a simple method to
obtain the LBCRB, whereas the Measurement-Prior Approach enables us to
incorporate domain knowledge to improve the sample complexity and
{interpretability}. To achieve this, we introduce a Physics-encoded score
neural network which enables us to easily incorporate such domain knowledge
into a neural network. We {study the learning} errors of the two suggested
approaches theoretically, and validate them numerically. We demonstrate the two
approaches on several signal processing examples, including a linear
measurement problem with unknown mixing and Gaussian noise covariance matrices,
frequency estimation, and quantized measurement. In addition, we test our
approach on a nonlinear signal processing problem of frequency estimation with
real-world underwater ambient noise.; 93) Decoding Quantum LDPC Codes using Collaborative Check Node Removal; The fault tolerance of quantum devices requires on-par contributions from
error-correcting codes and suitable decoders.
  One of the most explored error-correcting codes is the family of Quantum
Low-Density Parity Check (QLDPC) codes.
  Although faster than many of the reported decoders for QLDPC codes, iterative
decoders fail due to the colossal degeneracy and short cycles intrinsic to
these codes.
  We present a strategy to improve the performance of the iterative decoders
based on a collaborative way to use the message passing of the iterative
decoders and check node removal from the code's Tanner graph.
  We use the concept of bit separation and generalize it to qubit separation.
  This gives us a metric to analyze and improve the decoder's performance
towards harmful configurations of QLDPC codes.
  We present a simple decoding architecture to overcome iterative decoding
failures by increasing the separation of trapped qubits without incurring any
significant overhead.; 94) RTFusion: A depth estimation network based on multimodal fusion in
  challenging scenarios; Depth estimation in complex real-world scenarios is a challenging task,
especially when relying solely on a single modality such as visible light or
thermal infrared (THR) imagery. This paper proposes a novel multimodal depth
estimation model, RTFusion, which enhances depth estimation accuracy and
robustness by integrating the complementary strengths of RGB and THR data. The
RGB modality provides rich texture and color information, while the THR
modality captures thermal patterns, ensuring stability under adverse lighting
conditions such as extreme illumination. The model incorporates a unique fusion
mechanism, EGFusion, consisting of the Mutual Complementary Attention (MCA)
module for cross-modal feature alignment and the Edge Saliency Enhancement
Module (ESEM) to improve edge detail preservation. Comprehensive experiments on
the MS2 and ViViD++ datasets demonstrate that the proposed model consistently
produces high-quality depth maps across various challenging environments,
including nighttime, rainy, and high-glare conditions. The experimental results
highlight the potential of the proposed method in applications requiring
reliable depth estimation, such as autonomous driving, robotics, and augmented
reality.; 95) Ludwig-Soret microscopy with vibrational photothermal effect; Vibrational microscopy provides label-free, bond-selective chemical contrast
by detecting molecular vibrations, making it invaluable for biomedical
research. While conventional methods rely on the direct detection of Raman
scattering or infrared absorption, recently developed vibrational photothermal
(ViP) microscopy achieves chemical contrast indirectly through refractive index
(RI) changes. This indirect approach enables unique imaging capabilities beyond
traditional chemical imaging. Here, we introduce a novel application of ViP
microscopy: label-free intracellular thermophoretic (Soret) imaging, which
visualizes biomolecular transport driven by temperature gradients. ViP-induced
Soret (ViPS) imaging leverages a steady-state temperature distribution
generated by optical heating through vibrational photothermal effect, combined
with time-resolved RI imaging via optical diffraction tomography (ODT). Using
ViPS imaging, we measured thermophoretic behavior in living COS7 cells,
determining intracellular diffusion and Soret coefficients. Notably, we
observed a reversed direction of molecular transport (negative Soret effect) in
the cytoplasm compared to the nucleus, possibly driven by
thermophoresis-induced diffusiophoresis. Furthermore, time-lapse imaging under
CO2-depleted conditions revealed a remarkable reduction in thermophoretic
activity, suggesting glass formation during the dying process, likely due to
polymer aggregation. ViPS imaging represents a new frontier in intracellular
thermophoretic studies, expanding the capabilities of vibrational microscopy.; 96) The State of Post-Hoc Local XAI Techniques for Image Processing:
  Challenges and Motivations; As complex AI systems further prove to be an integral part of our lives, a
persistent and critical problem is the underlying black-box nature of such
products and systems. In pursuit of productivity enhancements, one must not
forget the need for various technology to boost the overall trustworthiness of
such AI systems. One example, which is studied extensively in this work, is the
domain of Explainable Artificial Intelligence (XAI). Research works in this
scope are centred around the objective of making AI systems more transparent
and interpretable, to further boost reliability and trust in using them. In
this work, we discuss the various motivation for XAI and its approaches, the
underlying challenges that XAI faces, and some open problems that we believe
deserve further efforts to look into. We also provide a brief discussion of
various XAI approaches for image processing, and finally discuss some future
directions, to hopefully express and motivate the positive development of the
XAI research space.; 97) An integrated widely tunable linear isolator based on electro-optic
  Autler-Townes splitting; Optical isolators are essential for laser protection and robust signal
routing, but the incorporation of the necessary magneto-optic (MO) materials in
foundries has remained a challenge. As an alternative, several integrated
non-magnetic isolators based on acousto-optic (AO) and electro-optic (EO)
spatio-temporal modulation have been proposed. Unlike MO isolators, these
solutions are wavelength agnostic, though few published demonstrations reach
performance that is comparable to MO devices. The most significant remaining
concerns are on mitigating undesirable sidebands, lowering power consumption,
achieving wide bandwidth or wide tunability, and having a design that is
practical to deploy. Here we demonstrate a compact EO optical isolator, using
thin film lithium niobate, that produces a very high isolation figure of merit
(>32 dB contrast per dB of insertion loss) with THz-scale (8 nm) tunability
and, due to its architecture, achieves linear operation with negligible
sideband generation. The device uses a 20x smaller interaction length and
consumes 64x less power compared to the state-of-the-art EO and AO
architectures, without being subject to many of their technical constraints.; 98) D2GV: Deformable 2D Gaussian Splatting for Video Representation in
  400FPS; Implicit Neural Representations (INRs) have emerged as a powerful approach
for video representation, offering versatility across tasks such as compression
and inpainting. However, their implicit formulation limits both
interpretability and efficacy, undermining their practicality as a
comprehensive solution. We propose a novel video representation based on
deformable 2D Gaussian splatting, dubbed D2GV, which aims to achieve three key
objectives: 1) improved efficiency while delivering superior quality; 2)
enhanced scalability and interpretability; and 3) increased friendliness for
downstream tasks. Specifically, we initially divide the video sequence into
fixed-length Groups of Pictures (GoP) to allow parallel training and linear
scalability with video length. For each GoP, D2GV represents video frames by
applying differentiable rasterization to 2D Gaussians, which are deformed from
a canonical space into their corresponding timestamps. Notably, leveraging
efficient CUDA-based rasterization, D2GV converges fast and decodes at speeds
exceeding 400 FPS, while delivering quality that matches or surpasses
state-of-the-art INRs. Moreover, we incorporate a learnable pruning and
quantization strategy to streamline D2GV into a more compact representation. We
demonstrate D2GV's versatility in tasks including video interpolation,
inpainting and denoising, underscoring its potential as a promising solution
for video representation. Code is available at:
https://github.com/Evan-sudo/D2GV.; 99) CrossView-GS: Cross-view Gaussian Splatting For Large-scale Scene
  Reconstruction; 3D Gaussian Splatting (3DGS) has emerged as a prominent method for scene
representation and reconstruction, leveraging densely distributed Gaussian
primitives to enable real-time rendering of high-resolution images. While
existing 3DGS methods perform well in scenes with minor view variation, large
view changes in cross-view scenes pose optimization challenges for these
methods. To address these issues, we propose a novel cross-view Gaussian
Splatting method for large-scale scene reconstruction, based on dual-branch
fusion. Our method independently reconstructs models from aerial and ground
views as two independent branches to establish the baselines of Gaussian
distribution, providing reliable priors for cross-view reconstruction during
both initialization and densification. Specifically, a gradient-aware
regularization strategy is introduced to mitigate smoothing issues caused by
significant view disparities. Additionally, a unique Gaussian supplementation
strategy is utilized to incorporate complementary information of dual-branch
into the cross-view model. Extensive experiments on benchmark datasets
demonstrate that our method achieves superior performance in novel view
synthesis compared to state-of-the-art methods.; 100) Adiabatic Fine-Tuning of Neural Quantum States Enables Detection of
  Phase Transitions in Weight Space; Neural quantum states (NQS) have emerged as a powerful tool for approximating
quantum wavefunctions using deep learning. While these models achieve
remarkable accuracy, understanding how they encode physical information remains
an open challenge. In this work, we introduce adiabatic fine-tuning, a scheme
that trains NQS across a phase diagram, leading to strongly correlated weight
representations across different models. This correlation in weight space
enables the detection of phase transitions in quantum systems by analyzing the
trained network weights alone. We validate our approach on the transverse field
Ising model and the J1-J2 Heisenberg model, demonstrating that phase
transitions manifest as distinct structures in weight space. Our results
establish a connection between physical phase transitions and the geometry of
neural network parameters, opening new directions for the interpretability of
machine learning models in physics.",0.0,0.448643819352159
2411.02815,applied,2411.02815-pos1-7,"Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation; In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate.",2411.02815-pos2-7,"Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network; An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images.; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",43,"['1', '3', '4', '7', '2', '6', '19', '25', '43', '76']","The first candidate paper, 'GeoJEPA: Towards Eliminating Augmentation- and Sampling Bias in Multimodal Geospatial Learning,' aligns well with the main paper on liver anatomy by addressing the analytical challenges in complex data representation which can be relevant in mapping liver anatomy and surgical planning. The integration of multimodal data processing techniques can provide novel insights into liver segmentation and improve surgical outcomes. This choice reflects a combination of anatomical understanding with advanced data representation techniques, fostering a multidisciplinary approach that is both useful and innovative. The subsequent papers, while relevant to other domains, do not provide as strong a multidisciplinary link to liver anatomy or surgery directly as the first one does.","1) GeoJEPA: Towards Eliminating Augmentation- and Sampling Bias in
  Multimodal Geospatial Learning; Existing methods for self-supervised representation learning of geospatial
regions and map entities rely extensively on the design of pretext tasks, often
involving augmentations or heuristic sampling of positive and negative pairs
based on spatial proximity. This reliance introduces biases and limits the
representations' expressiveness and generalisability. Consequently, the
literature has expressed a pressing need to explore different methods for
modelling geospatial data. To address the key difficulties of such methods,
namely multimodality, heterogeneity, and the choice of pretext tasks, we
present GeoJEPA, a versatile multimodal fusion model for geospatial data built
on the self-supervised Joint-Embedding Predictive Architecture. With GeoJEPA,
we aim to eliminate the widely accepted augmentation- and sampling biases found
in self-supervised geospatial representation learning. GeoJEPA uses
self-supervised pretraining on a large dataset of OpenStreetMap attributes,
geometries and aerial images. The results are multimodal semantic
representations of urban regions and map entities that we evaluate both
quantitatively and qualitatively. Through this work, we uncover several key
insights into JEPA's ability to handle multimodal data.; 2) Equivalence of the pearly tree immersed Lagrangian Floer theory and the
  Hamiltonian immersed Lagrangian Floer theory; The goal of this paper is to prove an equivalence relation between the
immersed Lagrangian Floer theory, defined using pearly tree discs, and local
Hamiltonian flows, i.e., Hamiltonian flows performed in the Weinstein tubular
neighborhood. This is a generalization of Alston-Bao's work.; 3) RTFusion: A depth estimation network based on multimodal fusion in
  challenging scenarios; Depth estimation in complex real-world scenarios is a challenging task,
especially when relying solely on a single modality such as visible light or
thermal infrared (THR) imagery. This paper proposes a novel multimodal depth
estimation model, RTFusion, which enhances depth estimation accuracy and
robustness by integrating the complementary strengths of RGB and THR data. The
RGB modality provides rich texture and color information, while the THR
modality captures thermal patterns, ensuring stability under adverse lighting
conditions such as extreme illumination. The model incorporates a unique fusion
mechanism, EGFusion, consisting of the Mutual Complementary Attention (MCA)
module for cross-modal feature alignment and the Edge Saliency Enhancement
Module (ESEM) to improve edge detail preservation. Comprehensive experiments on
the MS2 and ViViD++ datasets demonstrate that the proposed model consistently
produces high-quality depth maps across various challenging environments,
including nighttime, rainy, and high-glare conditions. The experimental results
highlight the potential of the proposed method in applications requiring
reliable depth estimation, such as autonomous driving, robotics, and augmented
reality.; 4) Optimal control on a brain tumor growth model with lactate metabolism,
  viscoelastic effects, and tissue damage; In this paper, we study an optimal control problem for a brain tumor growth
model that incorporates lactate metabolism, viscoelastic effects, and tissue
damage. The PDE system, introduced in [G. Cavalleri, P. Colli, A. Miranville,
E. Rocca, On a Brain Tumor Growth Model with Lactate Metabolism, Viscoelastic
Effects, and Tissue Damage (2025)], couples a Fisher-Kolmogorov type equation
for tumor cell density with a reaction-diffusion equation for the lactate, a
quasi-static force balance governing the displacement, and a nonlinear
differential inclusion for tissue damage. The control variables, representing
chemotherapy and a lactate-targeting drug, influence tumor progression and
treatment response. Starting from well-posedness, regularity, and continuous
dependence results already established, we define a suitable cost functional
and prove the existence of optimal controls. Then, we analyze the
differentiability of the control-to-state operator and establish a necessary
first-order condition for treatment optimality.; 5) Decoding Quantum LDPC Codes using Collaborative Check Node Removal; The fault tolerance of quantum devices requires on-par contributions from
error-correcting codes and suitable decoders.
  One of the most explored error-correcting codes is the family of Quantum
Low-Density Parity Check (QLDPC) codes.
  Although faster than many of the reported decoders for QLDPC codes, iterative
decoders fail due to the colossal degeneracy and short cycles intrinsic to
these codes.
  We present a strategy to improve the performance of the iterative decoders
based on a collaborative way to use the message passing of the iterative
decoders and check node removal from the code's Tanner graph.
  We use the concept of bit separation and generalize it to qubit separation.
  This gives us a metric to analyze and improve the decoder's performance
towards harmful configurations of QLDPC codes.
  We present a simple decoding architecture to overcome iterative decoding
failures by increasing the separation of trapped qubits without incurring any
significant overhead.; 6) Radiation sputtering of hydrocarbon ices at Europa-relevant temperatures; The surfaces of some icy moons, such as Jupiter's moon Europa, are heavily
bombarded by energetic particles that can alter the surface materials and
affect the composition of its exosphere. Detection of CO2 on Europa's surface
indicate that Europa's interior may be transporting freshly exposed
carbon-containing material to the surface. It is unknown whether this CO2 is a
product of radiation of carbon-containing precursors or whether it is present
in the initial deposits. Regardless, further radiolysis by high-energy
electrons or ions can sputter CO2 (and organic fragments if present) into
Europa's exosphere. In this study, we investigate the radiation sputtering of
CO2 and organic fragments from hydrocarbon water ice mixtures at different
Europa-relevant surface temperatures to identify how its sputtering products
evolve over time. This study shows that the sputtering of hydrocarbon water ice
leads to the production of mostly CO2, CO, and fragmented hydrocarbons. The
onset of sputtered hydrocarbons is immediate, and quickly reaches a steady
state, whereas CO2 and CO are formed more gradually. It is found that higher
temperatures cause more sputtering, and that there are some notable differences
in the distribution of species that are sputtered at different temperatures,
indicating local heterogeneity of sputtering yields depending on the surface
temperature.; 7) Distributed U-net model and Image Segmentation for Lung Cancer Detection; Until now, in the wake of the COVID-19 pandemic in 2019, lung diseases,
especially diseases such as lung cancer and chronic obstructive pulmonary
disease (COPD), have become an urgent global health issue. In order to mitigate
the goal problem, early detection and accurate diagnosis of these conditions
are critical for effective treatment and improved patient outcomes. To further
research and reduce the error rate of hospital diagnoses, this comprehensive
study explored the potential of computer-aided design (CAD) systems, especially
utilizing advanced deep learning models such as U-Net. And compared with the
literature content of other authors, this study explores the capabilities of
U-Net in detail, and enhances the ability to simulate CAD systems through the
VGG16 algorithm. An extensive dataset consisting of lung CT images and
corresponding segmentation masks, curated collaboratively by multiple academic
institutions, serves as the basis for empirical validation. In this paper, the
efficiency of U-Net model is evaluated rigorously and precisely under multiple
hardware configurations, such as single CPU, single GPU, distributed GPU and
federated learning, and the effectiveness and development of the method in the
segmentation task of lung disease are demonstrated. Empirical results clearly
affirm the robust performance of the U-Net model, most effectively utilizing
four GPUs for distributed learning, and these results highlight the potential
of U-Net-based CAD systems for accurate and timely lung disease detection and
diagnosis huge potential.; 8) Emission photon statistics in collectively interacting dipole atom
  arrays in the low-intensity limit; We investigate the photon statistics of light emitted from a system of
collectively interacting dipoles in the low-intensity regime, incorporating
double-excitation states to capture beyond-single-excitation effects. By
analyzing the eigenstates of the double-excitation manifold, we establish their
connection to the accessible single-excitation eigenmodes and investigate the
role of decay rates in shaping the initial-time photon correlation function
$g^{(2)}(\tau = 0)$ under different detection schemes. By interfering two beams
of light that selectively address orthogonal eigenmodes, the photon emission
statistics can be arbitrarily controlled. This can act as a tunable
nonlinearity that enables both the enhancement and suppression of photon
correlations, extending the operational intensity range for single-photon
applications.; 9) The State of Post-Hoc Local XAI Techniques for Image Processing:
  Challenges and Motivations; As complex AI systems further prove to be an integral part of our lives, a
persistent and critical problem is the underlying black-box nature of such
products and systems. In pursuit of productivity enhancements, one must not
forget the need for various technology to boost the overall trustworthiness of
such AI systems. One example, which is studied extensively in this work, is the
domain of Explainable Artificial Intelligence (XAI). Research works in this
scope are centred around the objective of making AI systems more transparent
and interpretable, to further boost reliability and trust in using them. In
this work, we discuss the various motivation for XAI and its approaches, the
underlying challenges that XAI faces, and some open problems that we believe
deserve further efforts to look into. We also provide a brief discussion of
various XAI approaches for image processing, and finally discuss some future
directions, to hopefully express and motivate the positive development of the
XAI research space.; 10) Disharmony: Forensics using Reverse Lighting Harmonization; Content generation and manipulation approaches based on deep learning methods
have seen significant advancements, leading to an increased need for techniques
to detect whether an image has been generated or edited. Another area of
research focuses on the insertion and harmonization of objects within images.
In this study, we explore the potential of using harmonization data in
conjunction with a segmentation model to enhance the detection of edited image
regions. These edits can be either manually crafted or generated using deep
learning methods. Our findings demonstrate that this approach can effectively
identify such edits. Existing forensic models often overlook the detection of
harmonized objects in relation to the background, but our proposed Disharmony
Network addresses this gap. By utilizing an aggregated dataset of harmonization
techniques, our model outperforms existing forensic networks in identifying
harmonized objects integrated into their backgrounds, and shows potential for
detecting various forms of edits, including virtual try-on tasks.; 11) Two- and three-meson scattering amplitudes with physical quark masses
  from lattice QCD; We study systems of two and three mesons composed of pions and kaons at
maximal isospin using four CLS ensembles with $a\approx 0.063\;$fm, including
one with approximately physical quark masses. Using the stochastic
Laplacian-Heaviside method, we determine the energy spectrum of these systems
including many levels in different momentum frames and irreducible
representations. Using the relativistic two- and three-body finite-volume
formalism, we constrain the two and three-meson K matrices, including not only
the leading $s$ wave, but also $p$ and $d$ waves. By solving the three-body
integral equations, we determine, for the first time, the physical-point
scattering amplitudes for $3\pi^+$, $3K^+$, $\pi^+\pi^+ K^+$ and $K^+ K^+
\pi^+$ systems. These are determined for total angular momentum $J^P=0^-$,
$1^+$, and $2^-$. We also obtain accurate results for $2\pi^+$, $\pi^+ K^+$,
and $2K^+$ phase shifts. We compare our results to Chiral Perturbation Theory,
and to phenomenological fits.; 12) Controlled Floquet Dynamics and Topological Bound States in Continuum
  via Colored Quantum Random Walks; We demonstrate the emergence and control of Floquet states and topological
bound states in the continuum (TBICs) in a two-dimensional colored quantum
random walk (cQRW) on a square lattice. By introducing three internal degrees
of freedom-termed ""colors""-and leveraging SU(3) group representations, we
realize dispersive TBICs and intrinsic Floquet dynamics without the need for
external periodic driving. Through Chern number calculations, we identify three
distinct topological bands, revealing color-induced band mixing as a key
mechanism underlying the natural formation of Floquet states. The cQRW
framework enables precise tuning of quasi-energy spectra, supporting the
emergence of localized edge states in topological band gaps and dispersive
TBICs embedded within the bulk of other bands. These TBICs exhibit tunable
group velocity, controllable excitation across energy regimes, and robustness,
providing theoretical validation for their existence in a first-order Floquet
system. Our findings position cQRWs as a powerful platform for investigating
and harnessing TBICs and Floquet states, with potential applications in quantum
information and communication technologies.; 13) On finite dimensional regular gradings; Let $A$ be an associative algebra over an algebraically closed field $K$ of
characteristic 0. A decomposition $A=A_1\oplus\cdots \oplus A_r$ of $A$ into a
direct sum of $r$ vector subspaces is called a \textsl{regular decomposition}
if, for every $n$ and every $1\le i_j\le r$, there exist $a_{i_j}\in A_{i_j}$
such that $a_{i_1}\cdots a_{i_n}\ne 0$, and moreover, for every $1\le i,j\le r$
there exists a constant $\beta(i,j)\in K^*$ such that $a_ia_j=\beta(i,j)a_ja_i$
for every $a_i\in A_i$, $A_j\in A_j$. We work with decompositions determined by
gradings on $A$ by a finite abelian group $G$. In this case, the function
$\beta\colon G\times G\to K^*$ ought to be a bicharacter. A regular
decomposition is {minimal} whenever for every $g$, $h\in G$, the equalities
$\beta(x,g)=\beta(x,h)$ for every $x\in G$ imply $g=h$. In this paper we
describe the finite dimensional algebras $A$ admitting a $G$-grading such that
the corresponding regular decomposition is minimal. Moreover we compute the
graded codimension sequence of these algebras. It turns out that the graded PI
exponent of every finite dimensional $G$-graded algebra with regular grading
such that its regular decomposition is minimal, coincides with its ordinary
(ungraded) PI exponent.; 14) Quasiconvexity and self-improving size estimates; We show that M\""uller's $L\log L$ bound $$F(Du)\geq 0,\,Du\in
L^p_{\mathrm{loc}}(\mathbb{R}^n)\implies F(Du)\in L\log
L_{\mathrm{loc}}(\mathbb{R}^n)$$ for $F =\det$ and $p=n$ holds for quasiconcave
$F$ which are homogeneous of degree $p>1$. This contrasts similar Hardy space
bounds which hold only for null Lagrangians.; 15) Redefining Robot Generalization Through Interactive Intelligence; Recent advances in large-scale machine learning have produced high-capacity
foundation models capable of adapting to a broad array of downstream tasks.
While such models hold great promise for robotics, the prevailing paradigm
still portrays robots as single, autonomous decision-makers, performing tasks
like manipulation and navigation, with limited human involvement. However, a
large class of real-world robotic systems, including wearable robotics (e.g.,
prostheses, orthoses, exoskeletons), teleoperation, and neural interfaces, are
semiautonomous, and require ongoing interactive coordination with human
partners, challenging single-agent assumptions. In this position paper, we
argue that robot foundation models must evolve to an interactive multi-agent
perspective in order to handle the complexities of real-time human-robot
co-adaptation. We propose a generalizable, neuroscience-inspired architecture
encompassing four modules: (1) a multimodal sensing module informed by
sensorimotor integration principles, (2) an ad-hoc teamwork model reminiscent
of joint-action frameworks in cognitive science, (3) a predictive world belief
model grounded in internal model theories of motor control, and (4) a
memory/feedback mechanism that echoes concepts of Hebbian and
reinforcement-based plasticity. Although illustrated through the lens of cyborg
systems, where wearable devices and human physiology are inseparably
intertwined, the proposed framework is broadly applicable to robots operating
in semi-autonomous or interactive contexts. By moving beyond single-agent
designs, our position emphasizes how foundation models in robotics can achieve
a more robust, personalized, and anticipatory level of performance.; 16) Electron Fourier ptychography for phase reconstruction; Phase reconstruction is important in transmission electron microscopy for
structural studies. We describe electron Fourier ptychography and its
application to phase reconstruction of both radiation-resistant and
beam-sensitive materials. We demonstrate that the phase of the exit wave can be
reconstructed at high resolution using a modified iterative phase retrieval
algorithm with data collected using an alternative optical geometry. This
method achieves a spatial resolution of 0.63 nm at a fluence of $4.5 \times
10^2 \, e^-/\text{nm}^2$, as validated on Cry11Aa protein crystals under
cryogenic conditions. Notably, this method requires no additional hardware
modifications, is straightforward to implement, and can be seamlessly
integrated with existing data collection software, providing a broadly
accessible approach for structural studies.; 17) Physics-Aware POD-Based Learning for Ab initio QEM-Galerkin Simulations
  of Periodic Nanostructures; Quantum nanostructures offer crucial applications in electronics, photonics,
materials, drugs, etc. For accurate design and analysis of nanostructures and
materials, simulations of the Schrodinger or Schrodinger-like equation are
always needed. For large nanostructures, these eigenvalue problems can be
computationally intensive. One effective solution is a learning method via
Proper Orthogonal Decomposition (POD), together with ab initio Galerkin
projection of the Schrodinger equation. POD-Galerkin projects the problem onto
a reduced-order space with the POD basis representing electron wave functions
(WFs) guided by the first principles in simulations. To minimize training
effort and enhance robustness of POD-Galerkin in larger structures, the quantum
element method (QEM) was proposed previously, which partitions nanostructures
into generic quantum elements. Larger nanostructures can then be constructed by
the trained generic quantum elements, each of which is represented by its
POD-Galerkin model. This work investigates QEM-Galerkin thoroughly in
multi-element quantum-dot (QD) structures on approaches to further improve
training effectiveness and simulation accuracy and efficiency for QEM-Galerkin.
To further improve computing speed, POD and Fourier bases for periodic
potentials are also examined in QEM-Galerkin simulations. Results indicate
that, considering efficiency and accuracy, the POD potential basis is superior
to the Fourier potential basis even for periodic potentials. Overall,
QEM-Galerkin offers more than a 2-order speedup in computation over direct
numerical simulation for multi-element QD structures, and more improvement is
observed in a structure comprising more elements.; 18) Large Language Model Critics for Execution-Free Evaluation of Code
  Changes; Large language models (LLMs) offer a promising way forward for automating
software engineering tasks, such as bug fixes, feature additions, etc., via
multi-step LLM-based agentic workflows. However, existing metrics for
evaluating such workflows, mainly build status and occasionally log analysis,
are too sparse and limited in providing the information needed to assess the
quality of changes made. In this work, we designed LLM-based critics to derive
well-structured and rigorous intermediate/step-level, execution-free evaluation
proxies for repo-level code changes. Importantly, we assume access to the gold
test patch for the problem (i.e., reference-aware) to assess both semantics and
executability of generated patches. With the gold test patch as a reference, we
predict executability of all editing locations with an F1 score of 91.6%,
aggregating which, we can predict the build status in 84.8% of the instances in
SWE-bench. In particular, such an execution-focused LLM critic outperforms
other reference-free and reference-aware LLM critics by 38.9% to 72.5%.
Moreover, we demonstrate the usefulness of such a reference-aware framework in
comparing patches generated by different agentic workflows. Finally, we
open-source the library developed for this project, which allows further usage
for either other agentic workflows or other benchmarks. The source code is
available at https://github.com/amazon-science/code-agent-eval.; 19) CardioTabNet: A Novel Hybrid Transformer Model for Heart Disease
  Prediction using Tabular Medical Data; The early detection and prediction of cardiovascular diseases are crucial for
reducing the severe morbidity and mortality associated with these conditions
worldwide. A multi-headed self-attention mechanism, widely used in natural
language processing (NLP), is operated by Transformers to understand feature
interactions in feature spaces. However, the relationships between various
features within biological systems remain ambiguous in these spaces,
highlighting the necessity of early detection and prediction of cardiovascular
diseases to reduce the severe morbidity and mortality with these conditions
worldwide. We handle this issue with CardioTabNet, which exploits the strength
of tab transformer to extract feature space which carries strong understanding
of clinical cardiovascular data and its feature ranking. As a result,
performance of downstream classical models significantly showed outstanding
result. Our study utilizes the open-source dataset for heart disease prediction
with 1190 instances and 11 features. In total, 11 features are divided into
numerical (age, resting blood pressure, cholesterol, maximum heart rate, old
peak, weight, and fasting blood sugar) and categorical (resting ECG, exercise
angina, and ST slope). Tab transformer was used to extract important features
and ranked them using random forest (RF) feature ranking algorithm. Ten
machine-learning models were used to predict heart disease using selected
features. After extracting high-quality features, the top downstream model (a
hyper-tuned ExtraTree classifier) achieved an average accuracy rate of 94.1%
and an average Area Under Curve (AUC) of 95.0%. Furthermore, a nomogram
analysis was conducted to evaluate the model's effectiveness in cardiovascular
risk assessment. A benchmarking study was conducted using state-of-the-art
models to evaluate our transformer-driven framework.; 20) The Southern Photometrical Local Universe Survey (S-PLUS): searching for
  metal-poor dwarf galaxies; The metal content of a galaxy's interstellar medium reflects the interplay
between different evolutionary processes such as feedback from massive stars
and the accretion of gas from the intergalactic medium. Despite the expected
abundance of low-luminosity galaxies, the low-mass and low-metallicity regime
remains relatively understudied. Since the properties of their interstellar
medium resemble those of early galaxies, identifying such objects in the Local
Universe is crucial to understand the early stages of galaxy evolution. We used
the DR3 catalog of the Southern Photometric Local Universe Survey (S-PLUS) to
select low-metallicity dwarf galaxy candidates based on color selection
criteria typical of metal-poor, star-forming, low-mass systems. The final
sample contains approximately 50 candidates. Spectral energy distribution
fitting of the 12 S-PLUS bands reveals that $\sim$ 90\% of the candidates are
best fit by models with very low stellar metallicities. We obtained long-slit
observations with the Gemini Multi-Object Spectrograph to follow-up a pilot
sample and confirm whether these galaxies have low metallicities. We find
oxygen abundances in the range $7.35<$ 12 + log(O/H) $< 7.93$ (5\% to 17\% of
the solar value), confirming their metal-poor nature. Most targets are outliers
in the mass-metallicity relation, i.e. they display a low metal content
relative to their observed stellar masses. In some cases, perturbed optical
morphologies might give evidence of dwarf-dwarf interactions or mergers. These
results suggest that the low oxygen abundances may be associated with an
external event causing the accretion of metal-poor gas, which dilutes the
oxygen abundance in these systems.; 21) Make Shuffling Great Again: A Side-Channel Resistant Fisher-Yates
  Algorithm for Protecting Neural Networks; Neural network models implemented in embedded devices have been shown to be
susceptible to side-channel attacks (SCAs), allowing recovery of proprietary
model parameters, such as weights and biases. There are already available
countermeasure methods currently used for protecting cryptographic
implementations that can be tailored to protect embedded neural network models.
Shuffling, a hiding-based countermeasure that randomly shuffles the order of
computations, was shown to be vulnerable to SCA when the Fisher-Yates algorithm
is used. In this paper, we propose a design of an SCA-secure version of the
Fisher-Yates algorithm. By integrating the masking technique for modular
reduction and Blakely's method for modular multiplication, we effectively
remove the vulnerability in the division operation that led to side-channel
leakage in the original version of the algorithm. We experimentally evaluate
that the countermeasure is effective against SCA by implementing a correlation
power analysis attack on an embedded neural network model implemented on ARM
Cortex-M4. Compared to the original proposal, the memory overhead is $2\times$
the biggest layer of the network, while the time overhead varies from $4\%$ to
$0.49\%$ for a layer with $100$ and $1000$ neurons, respectively.; 22) Adiabatic Fine-Tuning of Neural Quantum States Enables Detection of
  Phase Transitions in Weight Space; Neural quantum states (NQS) have emerged as a powerful tool for approximating
quantum wavefunctions using deep learning. While these models achieve
remarkable accuracy, understanding how they encode physical information remains
an open challenge. In this work, we introduce adiabatic fine-tuning, a scheme
that trains NQS across a phase diagram, leading to strongly correlated weight
representations across different models. This correlation in weight space
enables the detection of phase transitions in quantum systems by analyzing the
trained network weights alone. We validate our approach on the transverse field
Ising model and the J1-J2 Heisenberg model, demonstrating that phase
transitions manifest as distinct structures in weight space. Our results
establish a connection between physical phase transitions and the geometry of
neural network parameters, opening new directions for the interpretability of
machine learning models in physics.; 23) EigenShield: Causal Subspace Filtering via Random Matrix Theory for
  Adversarially Robust Vision-Language Models; Vision-Language Models (VLMs) inherit adversarial vulnerabilities of Large
Language Models (LLMs), which are further exacerbated by their multimodal
nature. Existing defenses, including adversarial training, input
transformations, and heuristic detection, are computationally expensive,
architecture-dependent, and fragile against adaptive attacks. We introduce
EigenShield, an inference-time defense leveraging Random Matrix Theory to
quantify adversarial disruptions in high-dimensional VLM representations.
Unlike prior methods that rely on empirical heuristics, EigenShield employs the
spiked covariance model to detect structured spectral deviations. Using a
Robustness-based Nonconformity Score (RbNS) and quantile-based thresholding, it
separates causal eigenvectors, which encode semantic information, from
correlational eigenvectors that are susceptible to adversarial artifacts. By
projecting embeddings onto the causal subspace, EigenShield filters adversarial
noise without modifying model parameters or requiring adversarial training.
This architecture-independent, attack-agnostic approach significantly reduces
the attack success rate, establishing spectral analysis as a principled
alternative to conventional defenses. Our results demonstrate that EigenShield
consistently outperforms all existing defenses, including adversarial training,
UNIGUARD, and CIDER.; 24) GEVRM: Goal-Expressive Video Generation Model For Robust Visual
  Manipulation; With the rapid development of embodied artificial intelligence, significant
progress has been made in vision-language-action (VLA) models for general robot
decision-making. However, the majority of existing VLAs fail to account for the
inevitable external perturbations encountered during deployment. These
perturbations introduce unforeseen state information to the VLA, resulting in
inaccurate actions and consequently, a significant decline in generalization
performance. The classic internal model control (IMC) principle demonstrates
that a closed-loop system with an internal model that includes external input
signals can accurately track the reference input and effectively offset the
disturbance. We propose a novel closed-loop VLA method GEVRM that integrates
the IMC principle to enhance the robustness of robot visual manipulation. The
text-guided video generation model in GEVRM can generate highly expressive
future visual planning goals. Simultaneously, we evaluate perturbations by
simulating responses, which are called internal embeddings and optimized
through prototype contrastive learning. This allows the model to implicitly
infer and distinguish perturbations from the external environment. The proposed
GEVRM achieves state-of-the-art performance on both standard and perturbed
CALVIN benchmarks and shows significant improvements in realistic robot tasks.; 25) MRUCT: Mixed Reality Assistance for Acupuncture Guided by Ultrasonic
  Computed Tomography; Chinese acupuncture practitioners primarily depend on muscle memory and
tactile feedback to insert needles and accurately target acupuncture points, as
the current workflow lacks imaging modalities and visual aids. Consequently,
new practitioners often learn through trial and error, requiring years of
experience to become proficient and earn the trust of patients. Medical
students face similar challenges in mastering this skill. To address these
challenges, we developed an innovative system, MRUCT, that integrates
ultrasonic computed tomography (UCT) with mixed reality (MR) technology to
visualize acupuncture points in real-time. This system offers offline image
registration and real-time guidance during needle insertion, enabling them to
accurately position needles based on anatomical structures such as bones,
muscles, and auto-generated reference points, with the potential for clinical
implementation. In this paper, we outline the non-rigid registration methods
used to reconstruct anatomical structures from UCT data, as well as the key
design considerations of the MR system. We evaluated two different 3D user
interface (3DUI) designs and compared the performance of our system to
traditional workflows for both new practitioners and medical students. The
results highlight the potential of MR to enhance therapeutic medical practices
and demonstrate the effectiveness of the system we developed.; 26) Second-Order $\Gamma$-Limit for the Cahn-Hilliard Functional with
  Dirichlet Boundary Conditions, II; This paper continues the study of the asymptotic development of order 2 by
$\Gamma$ -convergence of the Cahn-Hilliard functional with Dirichlet boundary
conditions initiated in [8]. While in the first paper, the Dirichlet data are
assumed to be well separated from one of the two wells, here this is no longer
the case. In the case where there are no interfaces, it is shown that there is
a transition layer near the boundary of the domain.; 27) Rethinking IDE Customization for Enhanced HAX: A Hyperdimensional
  Perspective; As Integrated Development Environments (IDEs) increasingly integrate
Artificial Intelligence, Software Engineering faces both benefits like
productivity gains and challenges like mismatched user preferences. We propose
Hyper-Dimensional (HD) vector spaces to model Human-Computer Interaction,
focusing on user actions, stylistic preferences, and project context. These
contributions aim to inspire further research on applying HD computing in IDE
design.; 28) Retrievals Can Be Detrimental: A Contrastive Backdoor Attack Paradigm on
  Retrieval-Augmented Diffusion Models; Diffusion models (DMs) have recently demonstrated remarkable generation
capability. However, their training generally requires huge computational
resources and large-scale datasets. To solve these, recent studies empower DMs
with the advanced Retrieval-Augmented Generation (RAG) technique and propose
retrieval-augmented diffusion models (RDMs). By incorporating rich knowledge
from an auxiliary database, RAG enhances diffusion models' generation and
generalization ability while significantly reducing model parameters. Despite
the great success, RAG may introduce novel security issues that warrant further
investigation. In this paper, we reveal that the RDM is susceptible to backdoor
attacks by proposing a multimodal contrastive attack approach named BadRDM. Our
framework fully considers RAG's characteristics and is devised to manipulate
the retrieved items for given text triggers, thereby further controlling the
generated contents. Specifically, we first insert a tiny portion of images into
the retrieval database as target toxicity surrogates. Subsequently, a malicious
variant of contrastive learning is adopted to inject backdoors into the
retriever, which builds shortcuts from triggers to the toxicity surrogates.
Furthermore, we enhance the attacks through novel entropy-based selection and
generative augmentation strategies that can derive better toxicity surrogates.
Extensive experiments on two mainstream tasks demonstrate the proposed BadRDM
achieves outstanding attack effects while preserving the model's benign
utility.; 29) The H\""{o}lder regularity of div-curl system with anisotropic
  coefficients; This research examines the regularity of weak solutions to the Div-Curl
system with low regularity anisotropic coefficients. The H\""older regularity of
the Div-Curl system with one anisotropic coefficient was an unresolved problem
raised by Yin in 2016. We have addressed the open problem, and the findings
extend to the scenario involving two anisotropic coefficients. We establish the
H\""{o}lder regularity of the solution when the coefficients is H\""{o}lder
continuous. Moreover, the degree of H\""{o}lder regularity of the solution can
be improved if the coefficient has a greater degree of H\""{o}lder regularity.; 30) Model-Free Adversarial Purification via Coarse-To-Fine Tensor Network
  Representation; Deep neural networks are known to be vulnerable to well-designed adversarial
attacks. Although numerous defense strategies have been proposed, many are
tailored to the specific attacks or tasks and often fail to generalize across
diverse scenarios. In this paper, we propose Tensor Network Purification (TNP),
a novel model-free adversarial purification method by a specially designed
tensor network decomposition algorithm. TNP depends neither on the pre-trained
generative model nor the specific dataset, resulting in strong robustness
across diverse adversarial scenarios. To this end, the key challenge lies in
relaxing Gaussian-noise assumptions of classical decompositions and
accommodating the unknown distribution of adversarial perturbations. Unlike the
low-rank representation of classical decompositions, TNP aims to reconstruct
the unobserved clean examples from an adversarial example. Specifically, TNP
leverages progressive downsampling and introduces a novel adversarial
optimization objective to address the challenge of minimizing reconstruction
error but without inadvertently restoring adversarial perturbations. Extensive
experiments conducted on CIFAR-10, CIFAR-100, and ImageNet demonstrate that our
method generalizes effectively across various norm threats, attack types, and
tasks, providing a versatile and promising adversarial purification technique.; 31) CrossModalityDiffusion: Multi-Modal Novel View Synthesis with Unified
  Intermediate Representation; Geospatial imaging leverages data from diverse sensing modalities-such as EO,
SAR, and LiDAR, ranging from ground-level drones to satellite views. These
heterogeneous inputs offer significant opportunities for scene understanding
but present challenges in interpreting geometry accurately, particularly in the
absence of precise ground truth data. To address this, we propose
CrossModalityDiffusion, a modular framework designed to generate images across
different modalities and viewpoints without prior knowledge of scene geometry.
CrossModalityDiffusion employs modality-specific encoders that take multiple
input images and produce geometry-aware feature volumes that encode scene
structure relative to their input camera positions. The space where the feature
volumes are placed acts as a common ground for unifying input modalities. These
feature volumes are overlapped and rendered into feature images from novel
perspectives using volumetric rendering techniques. The rendered feature images
are used as conditioning inputs for a modality-specific diffusion model,
enabling the synthesis of novel images for the desired output modality. In this
paper, we show that jointly training different modules ensures consistent
geometric understanding across all modalities within the framework. We validate
CrossModalityDiffusion's capabilities on the synthetic ShapeNet cars dataset,
demonstrating its effectiveness in generating accurate and consistent novel
views across multiple imaging modalities and perspectives.; 32) Learnable polynomial, trigonometric, and tropical activations; This paper investigates scalable neural networks with learnable activation
functions based on orthogonal function bases and tropical polynomials,
targeting ImageNet-1K classification and next token prediction on OpenWebText.
Traditional activations, such as ReLU, are static. In contrast, learnable
activations enable the network to adapt dynamically during training. However,
stability issues, such as vanishing or exploding gradients, arise with improper
variance management in deeper networks. To remedy this, we propose an
initialization scheme that single-handedly preserves unitary variance in
transformers and convolutional networks, ensuring stable gradient flow even in
deep architectures. Extensive experiments demonstrate that networks with
Hermite, Fourier, and Tropical-based learnable activations significantly
improve over GPT-2 and ConvNeXt networks in terms of accuracy and perplexity in
train and test, highlighting the viability of learnable activations in
large-scale tasks. The activation functions developed here are the subject of a
library coded entirely in pure PyTorch: torchortho, available at
https://github.com/K-H-Ismail/torchortho.; 33) Entity-aware Cross-lingual Claim Detection for Automated Fact-checking; Identifying claims requiring verification is a critical task in automated
fact-checking, especially given the proliferation of misinformation on social
media platforms. Despite significant progress in the task, there remain open
challenges such as dealing with multilingual and multimodal data prevalent in
online discourse. Addressing the multilingual challenge, recent efforts have
focused on fine-tuning pre-trained multilingual language models. While these
models can handle multiple languages, their ability to effectively transfer
cross-lingual knowledge for detecting claims spreading on social media remains
under-explored. In this paper, we introduce EX-Claim, an entity-aware
cross-lingual claim detection model that generalizes well to handle claims
written in any language. The model leverages entity information derived from
named entity recognition and entity linking techniques to improve the
language-level performance of both seen and unseen languages during training.
Extensive experiments conducted on three datasets from different social media
platforms demonstrate that our proposed model significantly outperforms the
baselines, across 27 languages, and achieves the highest rate of knowledge
transfer, even with limited training data.; 34) Inhomogeneous Electric Fields for Precise Control and Displacement of
  Polar Textures; Since the discovery of polar topological textures, achieving efficient
control and manipulation of them has emerged as a significant challenge for
their integration into nanoelectronic devices. In this study, we use second
principles molecular dynamic simulations to demonstrate the precise and
reversible control of domain arrangements stabilizing diverse polarization
textures through the application of various inhomogeneous electric fields.
Furthermore, we conduct an in-depth study of ferroelectric domain motion under
such fields, revealing features consistent with creep dynamics and establishing
an upper limit for their propagation speed. Notably, our findings show that
domain walls exhibit an asymmetric inertial response, present at the onset of
the dynamics but absent during their cessation. These findings provide valuable
insights into the dynamic behavior of polar textures, paving the way for the
development of high-speed, low-power nanoelectronic applications.; 35) Invisible Labor: The Backbone of Open Source Software; Invisible labor is an intrinsic part of the modern workplace, and includes
labor that is undervalued or unrecognized such as creating collaborative
atmospheres. Open source software (OSS) is software that is viewable, editable
and shareable by anyone with internet access. Contributors are mostly
volunteers, who participate for personal edification and because they believe
in the spirit of OSS rather than for employment. Volunteerism often leads to
high personnel turnover, poor maintenance and inconsistent project management.
This in turn, leads to a difficulty with sustainability long term. We believe
that the key to sustainable management is the invisible labor that occurs
behind the scenes. It is unclear how OSS contributors think about the invisible
labor they perform or how that affects OSS sustainability. We interviewed OSS
contributors and asked them about their invisible labor contributions,
leadership departure, membership turnover and sustainability. We found that
invisible labor is responsible for good leadership, reducing contributor
turnover, and creating legitimacy for the project as an organization.; 36) Volume Tells: Dual Cycle-Consistent Diffusion for 3D Fluorescence
  Microscopy De-noising and Super-Resolution; 3D fluorescence microscopy is essential for understanding fundamental life
processes through long-term live-cell imaging. However, due to inherent issues
in imaging principles, it faces significant challenges including spatially
varying noise and anisotropic resolution, where the axial resolution lags
behind the lateral resolution up to 4.5 times. Meanwhile, laser power is kept
low to maintain cell viability, leading to inaccessible low-noise and
high-resolution paired ground truth (GT). To tackle these limitations, a dual
Cycle-consistent Diffusion is proposed to effectively mine intra-volume imaging
priors within 3D cell volumes in an unsupervised manner, i.e., Volume Tells
(VTCD), achieving de-noising and super-resolution (SR) simultaneously.
Specifically, a spatially iso-distributed denoiser is designed to exploit the
noise distribution consistency between adjacent low-noise and high-noise
regions within the 3D cell volume, suppressing the spatially varying noise.
Then, in light of the structural consistency of the cell volume, a cross-plane
global-propagation SR module propagates high-resolution details from the XY
plane into adjacent regions in the XZ and YZ planes, progressively enhancing
resolution across the entire 3D cell volume. Experimental results on 10 in vivo
cellular dataset demonstrate high improvements in both denoising and
super-resolution, with axial resolution enhanced from ~ 430 nm to ~ 90 nm.; 37) Control Strategy for Generalized Synchrony in Coupled Dynamical Systems; Dynamical systems can be coupled in a manner that is designed to drive the
resulting dynamics onto a specified lower dimensional submanifold in the phase
space of the combined system. On the submanifold, the variables of the two
systems have a well-defined unique functional relationship. This process can
thus be viewed as a control technique that ensures generalized synchronization.
Depending on the nature of the dynamical systems and the specified submanifold,
different coupling functions can be derived in order to achieve a desired
control objective. We discuss the circuit implementations of this strategy in
representative examples of coupled chaotic dynamical systems, namely Lorenz
oscillators; 38) MolSpectra: Pre-training 3D Molecular Representation with Multi-modal
  Energy Spectra; Establishing the relationship between 3D structures and the energy states of
molecular systems has proven to be a promising approach for learning 3D
molecular representations. However, existing methods are limited to modeling
the molecular energy states from classical mechanics. This limitation results
in a significant oversight of quantum mechanical effects, such as quantized
(discrete) energy level structures, which offer a more accurate estimation of
molecular energy and can be experimentally measured through energy spectra. In
this paper, we propose to utilize the energy spectra to enhance the
pre-training of 3D molecular representations (MolSpectra), thereby infusing the
knowledge of quantum mechanics into the molecular representations.
Specifically, we propose SpecFormer, a multi-spectrum encoder for encoding
molecular spectra via masked patch reconstruction. By further aligning outputs
from the 3D encoder and spectrum encoder using a contrastive objective, we
enhance the 3D encoder's understanding of molecules. Evaluations on public
benchmarks reveal that our pre-trained representations surpass existing methods
in predicting molecular properties and modeling dynamics.; 39) The Quantum Internet (Technical Version); Following the emergence of quantum computing, the subsequent quantum
revolution will be that of interconnecting individual quantum computers at
global level. In the same way that classical computers only realised their full
potential with the emergence of the internet, a fully realised quantum internet
is the next stage of evolution for quantum computation. This work examines in
detail how the quantum internet would evolve in practice, focusing not only on
the technology itself but also on the implications it will have economically
and politically. We present both original ideas, as well as an extensive review
of relevant and related background material. This work begins with a
description of classical networks before introducing the key concepts behind
quantum networks, such as quantum internet protocols, quantum cryptography, and
cloud quantum computing. The work is divided into technical sections (requiring
only a basic knowledge of the notation of quantum mechanics), for those
interested in mathematical details, as well as non-technical sections for those
seeking a more general understanding. We target this work very broadly at
quantum and classical computer scientists, classical computer systems, software
and network engineers, physicists, economists, artists, musicians, and those
just generally curious about the future of quantum technologies and what they
might bring to humanity.; 40) Perception-aware Planning for Quadrotor Flight in Unknown and
  Feature-limited Environments; Various studies on perception-aware planning have been proposed to enhance
the state estimation accuracy of quadrotors in visually degraded environments.
However, many existing methods heavily rely on prior environmental knowledge
and face significant limitations in previously unknown environments with sparse
localization features, which greatly limits their practical application. In
this paper, we present a perception-aware planning method for quadrotor flight
in unknown and feature-limited environments that properly allocates perception
resources among environmental information during navigation. We introduce a
viewpoint transition graph that allows for the adaptive selection of local
target viewpoints, which guide the quadrotor to efficiently navigate to the
goal while maintaining sufficient localizability and without being trapped in
feature-limited regions. During the local planning, a novel yaw trajectory
generation method that simultaneously considers exploration capability and
localizability is presented. It constructs a localizable corridor via feature
co-visibility evaluation to ensure localization robustness in a computationally
efficient way. Through validations conducted in both simulation and real-world
experiments, we demonstrate the feasibility and real-time performance of the
proposed method. The source code will be released to benefit the community.; 41) Viscous and Inviscid Reconnection of Vortex Rings on Logarithmic
  Lattices; To address the possible occurrence of a Finite-Time Singularity (FTS) during
the oblique reconnection of two vortex rings, Moffatt-Kimura (MK) (J. Fluid
Mech., 2019a; J. Fluid Mech., 2019b) developed a simplified model based on the
Biot-Savart law and claimed that the vorticity amplification
$\omega_{\text{max}}/\omega_0$ becomes very large for vortex Reynolds number
$Re_{\Gamma} \geq 4000$. However, with Direct Numerical Simulation (DNS), Yao
and Hussain (J. Fluid Mech., 2020) were able to show that the vorticity
amplification is in fact much smaller and increases slowly with $Re_{\Gamma}$.
The suppression of vorticity was linked to two key factors - deformation of the
vortex core during approach and formation of hairpin-like bridge structures. In
this work, a recently developed numerical technique called log-lattice
(Campolina and Mailybaev, Nonlinearity, 2021), where interacting Fourier modes
are logarithmically sampled, is applied to the same oblique vortex ring
interaction problem. It is shown that this technique is not only capable of
capturing the two key physical processes overlooked by the MK model but also
other quantitative and qualitative attributes generally seen with DNS, at a
fraction of the computational cost. Furthermore, the sparsity of the Fourier
modes allows us to probe very large $Re_{\Gamma} = 10^8$ until which the peak
of the maximum norm of vorticity, while increasing with $Re_{\Gamma}$, remains
finite, and a blowup is observed only for the inviscid case.; 42) Formation of filaments and feathers in disc galaxies: Is self-gravity
  enough?; Context. Dense filaments/feathers are kpc-scale dusty features present in
nearby main sequence galaxies. Distinct from the spiral arms, filaments
constitute a major portion of dense gas concentration. They are expected to
play an important role in star formation and are known to harbour star-forming
regions and H II regions.
  Aims. We explore the origin of filaments/feathers in disc galaxies via global
gravitational instability.
  Methods. We conduct a parameter study using three-dimensional hydrodynamical
simulations of isolated disc galaxies that are isothermal, self-gravitating and
initialised in equilibrium. Our galaxies are uniquely characterised by two
dimensionless parameters, the Toomre $Q$ and the rotational Mach number,
$\mathcal{M}_{\rm c} = v_{\rm c}/c_{\rm s}$ (ratio of circular velocity to
sound speed). We carry out simulations covering a wide range in both.
  Results. We find that galaxies with $Q = 1$ form filaments within a single
rotation, while galaxies with $Q \geq 2$ do not. These filaments are kpc long
and are semi-regularly spaced along the azimuth. Their morphology, density
contrast and formation timescale vary with $\mathcal{M}_{\rm c}$, with filament
spacing and instability onset time both inversely proportional to
$\mathcal{M}_{\rm c}$ and the density contrast increasing with
$\mathcal{M}_{\rm c}$. However, their growth rates in all $Q = 1$ galaxies are
$\sim 0.5~\Omega$, where $\Omega$ is the angular frequency. We compare the
filament spacing in our simulations with the ones from JWST/MIRI and HST
observations of nearby galaxies and find them in agreement.
  Conclusions. Our study suggests that self-gravity and rotation are sufficient
to form filaments, even in the absence of spiral arms or magnetic fields. Their
morphologies are primarily determined by $\mathcal{M}_{\rm c}$, which
parametrises the importance of thermal versus rotational support.; 43) Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network; An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images.; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.; 44) Spectral properties from an efficient analytical representation of the
  $GW$ self-energy within a multipole approximation; We propose an efficient analytical representation of the frequency-dependent
$GW$ self-energy $\Sigma$ via a multipole approximation (MPA-$\Sigma$). The
multipole-Pad\'e model for the self-energy is interpolated from a small set of
numerical evaluations of $\Sigma$ in the complex frequency plane, similarly to
the previously multipole representation developed for the screened Coulomb
interaction (MPA-$W$) [Phys. Rev. B \textbf{104}, 115157 (2021)]. We show that,
likewise MPA-$W$, an appropriate choice of frequency sampling in MPA-$\Sigma$
is critical to guarantee computational efficiency and high accuracy. The
combined MPA-$W$ and MPA-$\Sigma$ scheme considerably reduces the cost of
full-frequency self-energy calculations, especially for spectral band
structures over a wide energy range. Crucially, MPA-$\Sigma$ enables a
multipole representation for the interacting Green's function $G$ (MPA-$G$),
providing a straightforward evaluation of all the spectral properties, and a
more general way to define the renormalization factor $Z$. We validate the
MPA-$\Sigma$ and MPA-$G$ approaches for diverse systems: bulk Si, Na and Cu,
monolayer MoS$_2$, the NaCl ion-pair and the F$_2$ molecule. Moreover, we
introduce toy MPA-$\Sigma$/$G$ models to examine the quasiparticle picture in
different regimens of weak and strong correlation. With these models, we expose
limitations in defining $Z$ from the local derivative of $\Sigma$.; 45) Ludwig-Soret microscopy with vibrational photothermal effect; Vibrational microscopy provides label-free, bond-selective chemical contrast
by detecting molecular vibrations, making it invaluable for biomedical
research. While conventional methods rely on the direct detection of Raman
scattering or infrared absorption, recently developed vibrational photothermal
(ViP) microscopy achieves chemical contrast indirectly through refractive index
(RI) changes. This indirect approach enables unique imaging capabilities beyond
traditional chemical imaging. Here, we introduce a novel application of ViP
microscopy: label-free intracellular thermophoretic (Soret) imaging, which
visualizes biomolecular transport driven by temperature gradients. ViP-induced
Soret (ViPS) imaging leverages a steady-state temperature distribution
generated by optical heating through vibrational photothermal effect, combined
with time-resolved RI imaging via optical diffraction tomography (ODT). Using
ViPS imaging, we measured thermophoretic behavior in living COS7 cells,
determining intracellular diffusion and Soret coefficients. Notably, we
observed a reversed direction of molecular transport (negative Soret effect) in
the cytoplasm compared to the nucleus, possibly driven by
thermophoresis-induced diffusiophoresis. Furthermore, time-lapse imaging under
CO2-depleted conditions revealed a remarkable reduction in thermophoretic
activity, suggesting glass formation during the dying process, likely due to
polymer aggregation. ViPS imaging represents a new frontier in intracellular
thermophoretic studies, expanding the capabilities of vibrational microscopy.; 46) Best Transition Matrix Esitimation or Best Label Noise Robustness
  Classifier? Two Possible Methods to Enhance the Performance of T-revision; Label noise refers to incorrect labels in a dataset caused by human errors or
collection defects, which is common in real-world applications and can
significantly reduce the accuracy of models. This report explores how to
estimate noise transition matrices and construct deep learning classifiers that
are robust against label noise. In cases where the transition matrix is known,
we apply forward correction and importance reweighting methods to correct the
impact of label noise using the transition matrix. When the transition matrix
is unknown or inaccurate, we use the anchor point assumption and T-Revision
series methods to estimate or correct the noise matrix. In this study, we
further improved the T-Revision method by developing T-Revision-Alpha and
T-Revision-Softmax to enhance stability and robustness. Additionally, we
designed and implemented two baseline classifiers, a Multi-Layer Perceptron
(MLP) and ResNet-18, based on the cross-entropy loss function. We compared the
performance of these methods on predicting clean labels and estimating
transition matrices using the FashionMINIST dataset with known noise transition
matrices. For the CIFAR-10 dataset, where the noise transition matrix is
unknown, we estimated the noise matrix and evaluated the ability of the methods
to predict clean labels.; 47) Terahertz-driven ultrafast dynamics of rare-earth nickelates by
  controlling only the charge degree of freedom; An important strategy for understanding the microscopic physics of strongly
correlated systems and enhancing their technological potential is to
selectively drive the fundamental degrees of freedom out of equilibrium.
Intense terahertz (THz) pulses with photon energies of a few meV, can not only
serve this purpose but also unravel their electronic and quantum nature. Here,
we present THz-driven ultrafast dynamics of rare-earth nickelates
$\text{RNiO}_{\text{3}}$, $\text{R}$ = rare-earth atom) - a prototype system to
study the Mott insulator-metal transition (IMT). The THz drive of its Mott
insulating state induces instantaneous IMT via quantum tunneling of valence
electrons across the bandgap while the THz drive of its correlated metallic
state leads to overall heating of the conduction electrons. The subsequent
relaxations of excited electrons in these two states occur via a two-step
process (electron-phonon thermalization and recovery of the charge-ordered
insulating state) and a one-step process (electron-phonon scattering),
respectively. The relaxation dynamics of the electrons and the absence of
acoustic phonon modes, in particular, suggest that the THz photons drive only
the charge degree of freedom. The purely electronic, ultrafast and local nature
of the THz-induced IMT offers its applications in opto-electronics with
enhanced performance and minimal device size and heat dissipation.; 48) Assessing Teamwork Dynamics in Software Development Projects; This study investigates teamwork dynamics in student software development
projects through a mixed-method approach combining quantitative analysis of
GitLab commit logs and qualitative survey data. We analyzed individual
contributions across six project phases, comparing self-reported and actual
contributions to measure discrepancies. Additionally, a survey captured
insights on team leadership, conflict resolution, communication practices, and
workload perceptions. Findings reveal that teams with minimal contribution
discrepancies achieved higher project grades and exam pass rates. In contrast,
teams with more significant discrepancies experienced lower performance,
potentially due to role clarity and communication issues. These results
underscore the value of shared leadership, structured conflict resolution, and
regular feedback in fostering effective teamwork, offering educators strategies
to enhance collaboration in software engineering education through
self-reflection and balanced workload allocation.; 49) A possible formation scenario of the Gaia ID 3425577610762832384: inner
  binary merger inside a triple common envelope; Recently, an identified non-interacting black hole (BH) binary, Gaia ID
3425577610762832384 (hereafter G3425), contains a BH ($\sim$3.6 M$_{\odot}$)
falling within the mass gap and has a nearly circular orbit, challenging the
classical binary evolution and supernova theory. Here, we propose that G3425
originates from a triple through a triple common envelope (TCE) evolution. The
G3425 progenitor originally may consist of three stars with masses of 1.49
M$_{\odot}$, 1.05 M$_{\odot}$, and 21.81 M$_{\odot}$, and inner and outer
orbital periods of 4.22 days and 1961.78 days, respectively. As evolution
proceeds, the tertiary fills its Roche lobe, leading to a TCE. We find that the
orbital energy generated by the inspiral of the inner binary serves as an
additional energy imparted for ejecting the common envelope (CE), accounting
for $\sim$97\% of the binding energy in our calculations. This means that the
outer orbit needs to expend only a small amount of the orbital energy to
successfully eject CE. The outcome of the TCE is a binary consisting of a 2.54
M$_\odot$ merger produced by the inner binary merger and a 7.67 M$_\odot$
helium star whose CE successfully ejected, with an orbital period of 547.53
days. The resulting post-TCE binary (PTB) has an orbital period that is 1-2
orders of magnitude greater than the orbital period of a successfully ejected
classical binary CE. In subsequent simulations, we find that the successfully
ejected helium star has a 44.2\% probability of forming a BH. In the case of a
non-complete fallback forming a BH, with an ejected mass of 2.6 M$_{\odot}$ and
a relatively low natal kick ($11^{+16}_{-5}$ ${\rm km/s}$ to $49^{+39}_{-39}$
${\rm km/s}$), this PTB can form G3425 in the Milky Way.; 50) Construction of Simultaneously Good Polar Codes and Polar Lattices; In this work, we investigate the simultaneous goodness of polar codes and
polar lattices. The simultaneous goodness of a lattice or a code means that it
is optimal for both channel coding and source coding simultaneously. The
existence of such kind of lattices was proven by using random lattice
ensembles. Our work provides an explicit construction based on the polarization
technique.; 51) Characterization of porous nanoparticles using the lattice Boltzmann
  method for fluid flow; Nanoporous capsules have been the subject of intense investigation in the
field of drug delivery. One of the essential properties of such particles,
which requires characterization, is their structure. Many experimental
techniques have been used for this purpose, such as wide-angle neutron or X-ray
scattering, or light scattering. Herein, we report theoretical data on the
relationship between the size, porosity and drag force of porous particles.
Data were obtained from a numerical procedure based on the lattice Boltzmann
method for the calculation of the creeping flow over porous spherical objects.
Previous analytical solutions to the hydrodynamic equations yielded different
predictions for the drag force under conditions of high permeability. In the
light of our findings, we compared our data to previous divergent analytical
solutions and analyzed experimental results for dynamic and static light
scattering of porous membrane vesicles Our findings strongly indicate a
potential source of error that has led to inconsistencies in DLS measurements
when compared to other techniques.; 52) Algebras of analytic functionals and homological epimorphisms; It has been proved by the author [arXiv: 2404.19433] that the Arens-Michael
envelope of a solvable Lie algebra is a homological epimorphism. We show here
that for algebras of analytic functionals on a connected complex Lie group the
analogous statement is satisfied without the assumption of solvability, and
furthermore the completion homomorphisms of a more general form are also
homological epimorphisms, including the envelope with respect to the class of
Banach PI-algebras.; 53) Yu-Shiba-Rusinov states in color superconducting quark matter; The high-density region of the QCD phase diagram displays an intricate
competition between color superconductivity and the QCD Kondo effect due to
color exchange in quark matter containing a single heavy quark impurity. We
explore the characteristic impurity-induced superconducting subgap states
arising in such systems by generalizing the surrogate model solver, recently
considered in the context of condensed matter physics [Phys. Rev. B 108,
L220506 (2023)]. The method consists of approximating the full superconducting
bulk by only a small number of effective levels whose parameters are chosen so
as to best reproduce the Matsubara frequency dependence of the impurity-bulk
hybridization function. We numerically solve a surrogate QCD Kondo model
describing a quantum impurity color-exchange coupled to a two-color two-flavor
superconducting bulk. The results directly indicate the presence of multiple
phase transitions as the coupling of the impurity to the bulk is increased, due
to the interplay between various overscreened states. The methods introduced
here are straightforward enough to be extended to more realistic QCD scenarios.; 54) Vevo: Controllable Zero-Shot Voice Imitation with Self-Supervised
  Disentanglement; The imitation of voice, targeted on specific speech attributes such as timbre
and speaking style, is crucial in speech generation. However, existing methods
rely heavily on annotated data, and struggle with effectively disentangling
timbre and style, leading to challenges in achieving controllable generation,
especially in zero-shot scenarios. To address these issues, we propose Vevo, a
versatile zero-shot voice imitation framework with controllable timbre and
style. Vevo operates in two core stages: (1) Content-Style Modeling: Given
either text or speech's content tokens as input, we utilize an autoregressive
transformer to generate the content-style tokens, which is prompted by a style
reference; (2) Acoustic Modeling: Given the content-style tokens as input, we
employ a flow-matching transformer to produce acoustic representations, which
is prompted by a timbre reference. To obtain the content and content-style
tokens of speech, we design a fully self-supervised approach that progressively
decouples the timbre, style, and linguistic content of speech. Specifically, we
adopt VQ-VAE as the tokenizer for the continuous hidden features of HuBERT. We
treat the vocabulary size of the VQ-VAE codebook as the information bottleneck,
and adjust it carefully to obtain the disentangled speech representations.
Solely self-supervised trained on 60K hours of audiobook speech data, without
any fine-tuning on style-specific corpora, Vevo matches or surpasses existing
methods in accent and emotion conversion tasks. Additionally, Vevo's
effectiveness in zero-shot voice conversion and text-to-speech tasks further
demonstrates its strong generalization and versatility. Audio samples are
available at https://versavoice.github.io.; 55) Generalized quantum two level model and its application in astrophysics; Complicated time-dependent curved spacetime and electric field are involved
in many astrophysical situations, including the early universe, Hawking
radiation, the Schwinger effect, and gravitational pair production. In this
Letter, a generalized quantum two-level model (GQTLM) is developed, which is
applicable to arbitrary time-dependent curved spacetime and electric field. The
model is found to be consistent with quantum kinetic theory, and is
characterized by its simplicity and versatility. The momentum distribution of
particles and the effects of gravitational distortions can be correctly
described. Quantum properties concerning vortex structures, such as the
intrinsic orbital angular momentum of particles and antiparticles can also be
conveniently calculated. The model is expected to significantly advance the
quantum exploration of the universe. It could refine the prediction of
primordial gravitational waves and relevant non-Gaussian signals, extend the
calculation of Hawking radiation to general black hole configurations, help to
distinguish neutron stars from strange quark stars, and elucidate the
gravitational pair production mechanism.; 56) SVIP: Semantically Contextualized Visual Patches for Zero-Shot Learning; Zero-shot learning (ZSL) aims to recognize unseen classes without labeled
training examples by leveraging class-level semantic descriptors such as
attributes. A fundamental challenge in ZSL is semantic misalignment, where
semantic-unrelated information involved in visual features introduce ambiguity
to visual-semantic interaction. Unlike existing methods that suppress
semantic-unrelated information post hoc either in the feature space or the
model space, we propose addressing this issue at the input stage, preventing
semantic-unrelated patches from propagating through the network. To this end,
we introduce Semantically contextualized VIsual Patches (SVIP) for ZSL, a
transformer-based framework designed to enhance visual-semantic alignment.
Specifically, we propose a self-supervised patch selection mechanism that
preemptively learns to identify semantic-unrelated patches in the input space.
This is trained with the supervision from aggregated attention scores across
all transformer layers, which estimate each patch's semantic score. As removing
semantic-unrelated patches from the input sequence may disrupt object
structure, we replace them with learnable patch embeddings. With initialization
from word embeddings, we can ensure they remain semantically meaningful
throughout feature extraction. Extensive experiments on ZSL benchmarks
demonstrate that SVIP achieves state-of-the-art performance results while
providing more interpretable and semantically rich feature representations.; 57) A spinless crystal for a high-performance solid-state $^{229}$Th nuclear
  clock; Solid-state $^{229}$Th nuclear clocks require a host material whose band gap
is larger than the 8.4 eV nuclear transition energy. As such, excitation of the
$^{229}$Th nuclear state has so far only been demonstrated in metal fluorides,
specifically CaF$_2$, LiSrAlF$_6$, and ThF$_4$, where the large
electronegativity of the halogen leads to sufficient band gaps. However, it is
expected that the nuclear magnetic moment of the fluorine gives rise to a
leading order broadening mechanism that limits the clock stability. Here, we
use concepts of molecular design to identify a polyatomic anion, SO$_4^{2-}$,
that is both nuclear spin free and of sufficient electron affinity to result in
a high band gap metal sulfate system. Using state-of-the-art calculations, we
find that the band gap of Th(SO$_4$)$_2$ is approximately 9 eV, large enough
for direct laser excitation of $^{229}$Th. Low concentrations of $^{229}$Th in
the otherwise spinless $^{232}$Th(SO$_4$)$_2$ crystal mitigate
$^{229}$Th-$^{229}$Th interactions. Furthermore, the introduction of $^{229}$Th
does not modify the material band gap nor introduce electronic states
associated with nuclear quenching. By removing one of the primary sources of
nuclear line broadening in the crystal, the nuclear magnetic dipole-dipole
interaction, a nuclear clock with instability as low as $\sigma =
4.6\times10^{-23}/\sqrt{\tau}$, where ${\tau}$ is the averaging time, may be
realized. This is roughly six orders of magnitude lower than previously thought
possible.; 58) Harnessing the Potential of Large Language Models in Modern Marketing
  Management: Applications, Future Directions, and Strategic Recommendations; Large Language Models (LLMs) have revolutionized the process of customer
engagement, campaign optimization, and content generation, in marketing
management. In this paper, we explore the transformative potential of LLMs
along with the current applications, future directions, and strategic
recommendations for marketers. In particular, we focus on LLMs major business
drivers such as personalization, real-time-interactive customer insights, and
content automation, and how they enable customers and business outcomes. For
instance, the ethical aspects of AI with respect to data privacy, transparency,
and mitigation of bias are also covered, with the goal of promoting responsible
use of the technology through best practices and the use of new technologies
businesses can tap into the LLM potential, which help growth and stay one step
ahead in the turmoil of digital marketing. This article is designed to give
marketers the necessary guidance by using best industry practices to integrate
these powerful LLMs into their marketing strategy and innovation without
compromising on the ethos of their brand.; 59) Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference
  Alignment; Direct Preference Optimization (DPO) has become a prominent method for
aligning Large Language Models (LLMs) with human preferences. While DPO has
enabled significant progress in aligning English LLMs, multilingual preference
alignment is hampered by data scarcity. To address this, we propose a novel
approach that $\textit{captures}$ learned preferences from well-aligned English
models by implicit rewards and $\textit{transfers}$ them to other languages
through iterative training. Specifically, we derive an implicit reward model
from the logits of an English DPO-aligned model and its corresponding reference
model. This reward model is then leveraged to annotate preference relations in
cross-lingual instruction-following pairs, using English instructions to
evaluate multilingual responses. The annotated data is subsequently used for
multilingual DPO fine-tuning, facilitating preference knowledge transfer from
English to other languages. Fine-tuning Llama3 for two iterations resulted in a
12.72% average improvement in Win Rate and a 5.97% increase in Length Control
Win Rate across all training languages on the X-AlpacaEval leaderboard. Our
findings demonstrate that leveraging existing English-aligned models can enable
efficient and effective multilingual preference alignment, significantly
reducing the need for extensive multilingual preference data. The code is
available at https://github.com/ZNLP/Implicit-Cross-Lingual-Rewarding; 60) A Multiple Transferable Neural Network Method with Domain Decomposition
  for Elliptic Interface Problems; The transferable neural network (TransNet) is a two-layer shallow neural
network with pre-determined and uniformly distributed neurons in the hidden
layer, and the least-squares solvers can be particularly used to compute the
parameters of its output layer when applied to the solution of partial
differential equations. In this paper, we integrate the TransNet technique with
the nonoverlapping domain decomposition and the interface conditions to develop
a novel multiple transferable neural network (Multi-TransNet) method for
solving elliptic interface problems, which typically contain discontinuities in
both solutions and their derivatives across interfaces. We first propose an
empirical formula for the TransNet to characterize the relationship between the
radius of the domain-covering ball, the number of hidden-layer neurons, and the
optimal neuron shape. In the Multi-TransNet method, we assign each subdomain
one distinct TransNet with an adaptively determined number of hidden-layer
neurons to maintain the globally uniform neuron distribution across the entire
computational domain, and then unite all the subdomain TransNets together by
incorporating the interface condition terms into the loss function. The
empirical formula is also extended to the Multi-TransNet and further employed
to estimate appropriate neuron shapes for the subdomain TransNets, greatly
reducing the parameter tuning cost. Additionally, we propose a normalization
approach to adaptively select the weighting parameters for the terms in the
loss function. Ablation studies and extensive experiments with comparison tests
on different types of elliptic interface problems with low to high contrast
diffusion coefficients in two and three dimensions are carried out to
numerically demonstrate the superior accuracy, efficiency, and robustness of
the proposed Multi-TransNet method.; 61) Scanning gate microscopy detection of Majorana bound states; We theoretically study scanning gate microscopy of a
superconductor-proximitized semiconducting wire focusing on the possibility of
detection of Majorana bound states. We exploit the possibility to create local
potential perturbation by the scanning gate tip which allows controllable
modification of the spatial distribution of the Majorana modes, which is
translated into changes in their energy structure. When the tip scans across
the system, it effectively divides the wire into two parts with controllable
lengths, in which two pairs of Majorana states are created when the system is
in the topological regime. For strong values of the tip potential the pairs are
decoupled, and the presence of Majorana states can be detected via local
tunneling spectroscopy that resolves the energy splittings resulting from the
Majorana states wave functions overlap. Importantly, as the system is probed
spatially via the tip, this technique can distinguish Majorana bound states
from quasi-Majorana states localized on smooth potential barriers. We
demonstrate that for weaker tip potentials the two neighboring Majorana states
hybridize opening pronounced anticrossings in the energy spectra which are
reflected in local conductance maps and which result in non-zero non-local
conductance features. Finally, we demonstrate the effect of the disorder on the
scanning gate microscopy spectroscopy maps.; 62) The Sample Complexity of Online Reinforcement Learning: A Multi-model
  Perspective; We study the sample complexity of online reinforcement learning for nonlinear
dynamical systems with continuous state and action spaces. Our analysis
accommodates a large class of dynamical systems ranging from a finite set of
nonlinear candidate models to models with bounded and Lipschitz continuous
dynamics, to systems that are parametrized by a compact and real-valued set of
parameters. In the most general setting, our algorithm achieves a policy regret
of $\mathcal{O}(N \epsilon^2 + \mathrm{ln}(m(\epsilon))/\epsilon^2)$, where $N$
is the time horizon, $\epsilon$ is a user-specified discretization width, and
$m(\epsilon)$ measures the complexity of the function class under consideration
via its packing number. In the special case where the dynamics are parametrized
by a compact and real-valued set of parameters (such as neural networks,
transformers, etc.), we prove a policy regret of $\mathcal{O}(\sqrt{N p})$,
where $p$ denotes the number of parameters, recovering earlier
sample-complexity results that were derived for linear time-invariant dynamical
systems. While this article focuses on characterizing sample complexity, the
proposed algorithms are likely to be useful in practice, due to their
simplicity, the ability to incorporate prior knowledge, and their benign
transient behavior.; 63) The EnviroMapper Toolkit: an Input Physicalisation that Captures the
  Situated Experience of Environmental Comfort in Offices; The environmental comfort in offices is traditionally captured by surveying
an entire workforce simultaneously, which yet fails to capture the situatedness
of the different personal experiences. To address this limitation, we developed
the EnviroMapper Toolkit, a data physicalisation toolkit that allows individual
office workers to record their personal experiences of environmental comfort by
mapping the actual moments and locations these occurred. By analysing two
in-the-wild studies in existing open-plan office environments (N=14), we
demonstrate how this toolkit acts like a situated input visualisation that can
be interpreted by domain experts who were not present during its construction.
This study therefore offers four key contributions: (1) the iterative design
process of the physicalisation toolkit; (2) its preliminary deployment in two
real-world office contexts; (3) the decoding of the resulting artefacts by
domain experts; and (4) design considerations to support future input
physicalisation and visualisation constructions that capture and synthesise
data from multiple individuals.; 64) Construction of logarithmic cohomology theories I; We propose a method for constructing cohomology theories of logarithmic
schemes with strict normal crossing boundaries by employing techniques from
logarithmic motivic homotopy theory over $\mathbb{F}_1$. This method recovers
the K-theory of the open complement of a strict normal crossing divisor from
the K-theory of schemes as well as logarithmic topological Hochschild homology
from the topological Hochschild homology of schemes. In our applications, we
establish that the K-theory of non-regular schemes is representable in the
logarithmic motivic homotopy category, and we introduce the logarithmic
cyclotomic trace for the regular log regular case.; 65) STGDPM:Vessel Trajectory Prediction with Spatio-Temporal Graph Diffusion
  Probabilistic Model; Vessel trajectory prediction is a critical component for ensuring maritime
traffic safety and avoiding collisions. Due to the inherent uncertainty in
vessel behavior, trajectory prediction systems must adopt a multimodal approach
to accurately model potential future motion states. However, existing vessel
trajectory prediction methods lack the ability to comprehensively model
behavioral multi-modality. To better capture multimodal behavior in interactive
scenarios, we propose modeling interactions as dynamic graphs, replacing
traditional aggregation-based techniques that rely on vessel states. By
leveraging the natural multimodal capabilities of diffusion models, we frame
the trajectory prediction task as an inverse process of motion uncertainty
diffusion, wherein uncertainties across potential navigational areas are
progressively eliminated until the desired trajectories is produced. In
summary, we pioneer the integration of Spatio-Temporal Graph (STG) with
diffusion models in ship trajectory prediction. Extensive experiments on real
Automatic Identification System (AIS) data validate the superiority of our
approach.; 66) D2GV: Deformable 2D Gaussian Splatting for Video Representation in
  400FPS; Implicit Neural Representations (INRs) have emerged as a powerful approach
for video representation, offering versatility across tasks such as compression
and inpainting. However, their implicit formulation limits both
interpretability and efficacy, undermining their practicality as a
comprehensive solution. We propose a novel video representation based on
deformable 2D Gaussian splatting, dubbed D2GV, which aims to achieve three key
objectives: 1) improved efficiency while delivering superior quality; 2)
enhanced scalability and interpretability; and 3) increased friendliness for
downstream tasks. Specifically, we initially divide the video sequence into
fixed-length Groups of Pictures (GoP) to allow parallel training and linear
scalability with video length. For each GoP, D2GV represents video frames by
applying differentiable rasterization to 2D Gaussians, which are deformed from
a canonical space into their corresponding timestamps. Notably, leveraging
efficient CUDA-based rasterization, D2GV converges fast and decodes at speeds
exceeding 400 FPS, while delivering quality that matches or surpasses
state-of-the-art INRs. Moreover, we incorporate a learnable pruning and
quantization strategy to streamline D2GV into a more compact representation. We
demonstrate D2GV's versatility in tasks including video interpolation,
inpainting and denoising, underscoring its potential as a promising solution
for video representation. Code is available at:
https://github.com/Evan-sudo/D2GV.; 67) On the time constant of high dimensional first passage percolation,
  revisited; In [2], it was claimed that the time constant $\mu_{d}(e_{1})$ for the
first-passage percolation model on $\mathbb Z^{d}$ is $\mu_{d}(e_{1}) \sim \log
d/(2ad)$ as $d\to \infty$, if the passage times $(\tau_{e})_{e\in \mathbb
E^{d}}$ are i.i.d., with a common c.d.f. $F$ satisfying
$\left|\frac{F(x)}{x}-a\right| \le \frac{C}{|\log x|}$ for some constants $a,
C$ and sufficiently small $x$.
  However, the proof of the upper bound, namely, Equation (2.1) in [2]
\begin{align} \limsup_{d\to\infty} \frac{\mu_{d}(e_{1})ad}{\log d} \le
\frac{1}{2} \end{align} is incorrect. In this article, we provide a different
approach that establishes this inequality. As a side product of this new
method, we also show that the variance of the non-backtracking passage time to
the first hyperplane is of order $o\big((\log d/d)^{2}\big)$ as $d\to \infty$
in the case of the when the edge weights are exponentially distributed.; 68) Skill Expansion and Composition in Parameter Space; Humans excel at reusing prior knowledge to address new challenges and
developing skills while solving problems. This paradigm becomes increasingly
popular in the development of autonomous agents, as it develops systems that
can self-evolve in response to new challenges like human beings. However,
previous methods suffer from limited training efficiency when expanding new
skills and fail to fully leverage prior knowledge to facilitate new task
learning. In this paper, we propose Parametric Skill Expansion and Composition
(PSEC), a new framework designed to iteratively evolve the agents' capabilities
and efficiently address new challenges by maintaining a manageable skill
library. This library can progressively integrate skill primitives as
plug-and-play Low-Rank Adaptation (LoRA) modules in parameter-efficient
finetuning, facilitating efficient and flexible skill expansion. This structure
also enables the direct skill compositions in parameter space by merging LoRA
modules that encode different skills, leveraging shared information across
skills to effectively program new skills. Based on this, we propose a
context-aware module to dynamically activate different skills to
collaboratively handle new tasks. Empowering diverse applications including
multi-objective composition, dynamics shift, and continual policy shift, the
results on D4RL, DSRL benchmarks, and the DeepMind Control Suite show that PSEC
exhibits superior capacity to leverage prior knowledge to efficiently tackle
new challenges, as well as expand its skill libraries to evolve the
capabilities. Project website: https://ltlhuuu.github.io/PSEC/.; 69) The Economics of Large Language Models: Token Allocation, Fine-Tuning,
  and Optimal Pricing; We develop an economic framework to analyze the optimal pricing and product
design of Large Language Models (LLM). Our framework captures several key
features of LLMs: variable operational costs of processing input and output
tokens; the ability to customize models through fine-tuning; and
high-dimensional user heterogeneity in terms of task requirements and error
sensitivity. In our model, a monopolistic seller offers multiple versions of
LLMs through a menu of products. The optimal pricing structure depends on
whether token allocation across tasks is contractible and whether users face
scale constraints. Users with similar aggregate value-scale characteristics
choose similar levels of fine-tuning and token consumption. The optimal
mechanism can be implemented through menus of two-part tariffs, with higher
markups for more intensive users. Our results rationalize observed industry
practices such as tiered pricing based on model customization and usage levels.; 70) Distance-Based Tree-Sliced Wasserstein Distance; To overcome computational challenges of Optimal Transport (OT), several
variants of Sliced Wasserstein (SW) has been developed in the literature. These
approaches exploit the closed-form expression of the univariate OT by
projecting measures onto (one-dimensional) lines. However, projecting measures
onto low-dimensional spaces can lead to a loss of topological information.
Tree-Sliced Wasserstein distance on Systems of Lines (TSW-SL) has emerged as a
promising alternative that replaces these lines with a more advanced structure
called tree systems. The tree structures enhance the ability to capture
topological information of the metric while preserving computational
efficiency. However, at the core of TSW-SL, the splitting maps, which serve as
the mechanism for pushing forward measures onto tree systems, focus solely on
the position of the measure supports while disregarding the projecting domains.
Moreover, the specific splitting map used in TSW-SL leads to a metric that is
not invariant under Euclidean transformations, a typically expected property
for OT on Euclidean space. In this work, we propose a novel class of splitting
maps that generalizes the existing one studied in TSW-SL enabling the use of
all positional information from input measures, resulting in a novel
Distance-based Tree-Sliced Wasserstein (Db-TSW) distance. In addition, we
introduce a simple tree sampling process better suited for Db-TSW, leading to
an efficient GPU-friendly implementation for tree systems, similar to the
original SW. We also provide a comprehensive theoretical analysis of proposed
class of splitting maps to verify the injectivity of the corresponding Radon
Transform, and demonstrate that Db-TSW is an Euclidean invariant metric. We
empirically show that Db-TSW significantly improves accuracy compared to recent
SW variants while maintaining low computational cost via a wide range of
experiments.; 71) Graph Contrastive Learning on Multi-label Classification for
  Recommendations; In business analysis, providing effective recommendations is essential for
enhancing company profits. The utilization of graph-based structures, such as
bipartite graphs, has gained popularity for their ability to analyze complex
data relationships. Link prediction is crucial for recommending specific items
to users. Traditional methods in this area often involve identifying patterns
in the graph structure or using representational techniques like graph neural
networks (GNNs). However, these approaches encounter difficulties as the volume
of data increases. To address these challenges, we propose a model called Graph
Contrastive Learning for Multi-label Classification (MCGCL). MCGCL leverages
contrastive learning to enhance recommendation effectiveness. The model
incorporates two training stages: a main task and a subtask. The main task is
holistic user-item graph learning to capture user-item relationships. The
homogeneous user-user (item-item) subgraph is constructed to capture user-user
and item-item relationships in the subtask. We assessed the performance using
real-world datasets from Amazon Reviews in multi-label classification tasks.
Comparative experiments with state-of-the-art methods confirm the effectiveness
of MCGCL, highlighting its potential for improving recommendation systems.; 72) Randomized measurements for multi-parameter quantum metrology; The optimal quantum measurements for estimating different unknown parameters
in a parameterized quantum state are usually incompatible with each other.
Traditional approaches to addressing the measurement incompatibility issue,
such as the Holevo Cram\'{e}r--Rao bound, suffer from multiple difficulties
towards practical applicability, as the optimal measurement strategies are
usually state-dependent, difficult to implement and also take complex analyses
to determine. Here we study randomized measurements as a new approach for
multi-parameter quantum metrology. We show quantum measurements on single
copies of quantum states given by 3-design perform near-optimally when
estimating an arbitrary number of parameters in pure states and more generally,
approximately low-rank states, whose metrological information is largely
concentrated in a low-dimensional subspace. The near-optimality is also shown
in estimating the maximal number of parameters for three types of mixed states
that are well-conditioned on its support. Examples of fidelity estimation and
Hamiltonian estimation are explicitly provided to demonstrate the power and
limitation of randomized measurements in multi-parameter quantum metrology.; 73) Effective DoF-Oriented Optimal Antenna Spacing in Near-Field XL-MIMO
  Systems; This letter investigates the optimal antenna spacing for a near-field XL-MIMO
communication system from the perspective of the array gain. Specifically,
using the Green's function-based channel model, the letter analyzes the channel
capacity, which is related to the effective degrees-of-freedom (EDoF). Then,
the letter further investigates the applicability of two EDoF estimation
methods. To increase EDoF, this letter focuses on analyzing the impact of
antenna spacing. Furthermore, from the perspective of the array gain, the
letter derives an approximate closed-form expression of the optimal antenna
spacing, at which EDoF is maximized and the array gain at the antenna nearest
to the focused antenna of the transmit array becomes zero. Finally, numerical
results verify the main results of this letter.; 74) Implicit Communication of Contextual Information in Human-Robot
  Collaboration; Implicit communication is crucial in human-robot collaboration (HRC), where
contextual information, such as intentions, is conveyed as implicatures,
forming a natural part of human interaction. However, enabling robots to
appropriately use implicit communication in cooperative tasks remains
challenging. My research addresses this through three phases: first, exploring
the impact of linguistic implicatures on collaborative tasks; second, examining
how robots' implicit cues for backchanneling and proactive communication affect
team performance and perception, and how they should adapt to human teammates;
and finally, designing and evaluating a multi-LLM robotics system that learns
from human implicit communication. This research aims to enhance the natural
communication abilities of robots and facilitate their integration into daily
collaborative activities.; 75) DivIL: Unveiling and Addressing Over-Invariance for Out-of- Distribution
  Generalization; Out-of-distribution generalization is a common problem that expects the model
to perform well in the different distributions even far from the train data. A
popular approach to addressing this issue is invariant learning (IL), in which
the model is compiled to focus on invariant features instead of spurious
features by adding strong constraints during training. However, there are some
potential pitfalls of strong invariant constraints. Due to the limited number
of diverse environments and over-regularization in the feature space, it may
lead to a loss of important details in the invariant features while alleviating
the spurious correlations, namely the over-invariance, which can also degrade
the generalization performance. We theoretically define the over-invariance and
observe that this issue occurs in various classic IL methods. To alleviate this
issue, we propose a simple approach Diverse Invariant Learning (DivIL) by
adding the unsupervised contrastive learning and the random masking mechanism
compensatory for the invariant constraints, which can be applied to various IL
methods. Furthermore, we conduct experiments across multiple modalities across
12 datasets and 6 classic models, verifying our over-invariance insight and the
effectiveness of our DivIL framework. Our code is available at
https://github.com/kokolerk/DivIL.; 76) Clinical Inspired MRI Lesion Segmentation; Magnetic resonance imaging (MRI) is a potent diagnostic tool for detecting
pathological tissues in various diseases. Different MRI sequences have
different contrast mechanisms and sensitivities for different types of lesions,
which pose challenges to accurate and consistent lesion segmentation. In
clinical practice, radiologists commonly use the sub-sequence feature, i.e. the
difference between post contrast-enhanced T1-weighted (post) and
pre-contrast-enhanced (pre) sequences, to locate lesions. Inspired by this, we
propose a residual fusion method to learn subsequence representation for MRI
lesion segmentation. Specifically, we iteratively and adaptively fuse features
from pre- and post-contrast sequences at multiple resolutions, using dynamic
weights to achieve optimal fusion and address diverse lesion enhancement
patterns. Our method achieves state-of-the-art performances on BraTS2023
dataset for brain tumor segmentation and our in-house breast MRI dataset for
breast lesion segmentation. Our method is clinically inspired and has the
potential to facilitate lesion segmentation in various applications.; 77) A binary PSO based ensemble under-sampling model for rebalancing
  imbalanced training data; Ensemble technique and under-sampling technique are both effective tools used
for imbalanced dataset classification problems. In this paper, a novel ensemble
method combining the advantages of both ensemble learning for biasing
classifiers and a new under-sampling method is proposed. The under-sampling
method is named Binary PSO instance selection; it gathers with ensemble
classifiers to find the most suitable length and combination of the majority
class samples to build a new dataset with minority class samples. The proposed
method adopts multi-objective strategy, and contribution of this method is a
notable improvement of the performances of imbalanced classification, and in
the meantime guaranteeing a best integrity possible for the original dataset.
We experimented the proposed method and compared its performance of processing
imbalanced datasets with several other conventional basic ensemble methods.
Experiment is also conducted on these imbalanced datasets using an improved
version where ensemble classifiers are wrapped in the Binary PSO instance
selection. According to experimental results, our proposed methods outperform
single ensemble methods, state-of-the-art under-sampling methods, and also
combinations of these methods with the traditional PSO instance selection
algorithm.; 78) Strategies for political-statement segmentation and labelling in
  unstructured text; Analysis of parliamentary speeches and political-party manifestos has become
an integral area of computational study of political texts. While speeches have
been overwhelmingly analysed using unsupervised methods, a large corpus of
manifestos with by-statement political-stance labels has been created by the
participants of the MARPOR project. It has been recently shown that these
labels can be predicted by a neural model; however, the current approach relies
on provided statement boundaries, limiting out-of-domain applicability. In this
work, we propose and test a range of unified split-and-label frameworks --
based on linear-chain CRFs, fine-tuned text-to-text models, and the combination
of in-context learning with constrained decoding -- that can be used to jointly
segment and classify statements from raw textual data. We show that our
approaches achieve competitive accuracy when applied to raw text of political
manifestos, and then demonstrate the research potential of our method by
applying it to the records of the UK House of Commons and tracing the political
trajectories of four major parties in the last three decades.; 79) Semantic-Aware Adaptive Video Streaming Using Latent Diffusion Models
  for Wireless Networks; This paper proposes a novel framework for real-time adaptive-bitrate video
streaming by integrating latent diffusion models (LDMs) within the FFmpeg
techniques. This solution addresses the challenges of high bandwidth usage,
storage inefficiencies, and quality of experience (QoE) degradation associated
with traditional constant bitrate streaming (CBS) and adaptive bitrate
streaming (ABS). The proposed approach leverages LDMs to compress I-frames into
a latent space, offering significant storage and semantic transmission savings
without sacrificing high visual quality. While it keeps B-frames and P-frames
as adjustment metadata to ensure efficient video reconstruction at the user
side, the proposed framework is complemented with the most state-of-the-art
denoising and video frame interpolation (VFI) techniques. These techniques
mitigate semantic ambiguity and restore temporal coherence between frames, even
in noisy wireless communication environments. Experimental results demonstrate
the proposed method achieves high-quality video streaming with optimized
bandwidth usage, outperforming state-of-the-art solutions in terms of QoE and
resource efficiency. This work opens new possibilities for scalable real-time
video streaming in 5G and future post-5G networks.; 80) Three-dimensional transport of solids in a protoplanetary disk
  containing a growing giant planet; We present the results of combined hydrodynamic and particle tracking
post-processing modeling to study the transport of small dust in a
protoplanetary disk containing an embedded embryo in 3D. We use a suite of
FARGO3D hydrodynamic simulations of disks containing a planetary embryo varying
in mass up to 300 $M_\oplus$ on a fixed orbit in both high and low viscosity
disks. We then simulate solid particles through the disk as a post-processing
step using a Monte Carlo integration, allowing us to track the trajectories of
individual particles as they travel throughout the disk. We find that gas
advection onto the planet can carry small, well-coupled solids across the gap
opened in the disk by the embedded planet for planetary masses above the pebble
isolation mass. This mixing between the inner and outer disk can occur in both
directions, with solids in the inner disk mixing to the outer disk as well.
Additionally, in low viscosity disks, multiple pile-ups in the outer disk may
preserve isotopic heterogeneities, possibly providing an outermost tertiary
isotopic reservoir. Throughout Jupiter's growth, the extent of mixing between
isotopic reservoirs varied depending on dust size, gas turbulence, and the
Jovian embryo mass.; 81) A muon tagging with Flash ADC waveform baselines; This manuscript describes an innovative method to tag the muons using the
baseline information of the Flash ADC (FADC) waveform of PMTs in the JSNS1
(J-PARC Sterile Neutrino Search at J-PARC Spallation Neutron Source)
experiment. This experiment is designed for the search for sterile neutrinos,
and a muon tagging is an essential key component for the background rejection
since the detector of the experiment is located over-ground, where is the 3rd
floor of the J-PARC Material and Life experimental facility (MLF). Especially,
stopping muons inside the detector create the Michel electrons, and they are
important background to be rejected. Utilizing this innovative method, more
than 99.8% of Michel electrons can be rejected even without a detector veto
region. This technique can be employed for any experiments which uses the
similar detector configurations.; 82) ARLED: Leveraging LED-based ARMAN Model for Abstractive Summarization of
  Persian Long Documents; The increasing volume of textual data poses challenges in reading and
comprehending large documents, particularly for scholars who need to extract
useful information from research articles. Automatic text summarization has
emerged as a powerful tool to condense lengthy documents into concise and
informative summaries. Depending on the approach used, text summarization can
be categorized as either extractive or abstractive. While extractive methods
are commonly used due to their simplicity, they often miss important
information. On the other hand, Abstractive Summarization can generate more
coherent and informative summaries by understanding the underlying meaning of
the text. Abstractive techniques have gained attention in various languages,
and recent advancements have been achieved through pre-training models such as
BERT, BART, and T5. However, the challenge of summarizing long documents
remains, and alternative models like Longformer have been introduced to address
this limitation. In this context, this paper focuses on abstractive
summarization in the Persian language. The authors introduce a new dataset of
300,000 full-text Persian papers obtained from the Ensani website and apply the
ARMAN model, based on the Longformer architecture, to generate summaries. The
experimental results demonstrate promising performance in Persian text
summarization. The paper provides a comprehensive overview of related work,
discusses the methodology, presents the experimental results, and concludes
with future research directions.; 83) FCVSR: A Frequency-aware Method for Compressed Video Super-Resolution; Compressed video super-resolution (SR) aims to generate high-resolution (HR)
videos from the corresponding low-resolution (LR) compressed videos. Recently,
some compressed video SR methods attempt to exploit the spatio-temporal
information in the frequency domain, showing great promise in super-resolution
performance. However, these methods do not differentiate various frequency
subbands spatially or capture the temporal frequency dynamics, potentially
leading to suboptimal results. In this paper, we propose a deep frequency-based
compressed video SR model (FCVSR) consisting of a motion-guided adaptive
alignment (MGAA) network and a multi-frequency feature refinement (MFFR)
module. Additionally, a frequency-aware contrastive loss is proposed for
training FCVSR, in order to reconstruct finer spatial details. The proposed
model has been evaluated on three public compressed video super-resolution
datasets, with results demonstrating its effectiveness when compared to
existing works in terms of super-resolution performance (up to a 0.14dB gain in
PSNR over the second-best model) and complexity.; 84) A Lightweight and Secure Deep Learning Model for Privacy-Preserving
  Federated Learning in Intelligent Enterprises; The ever growing Internet of Things (IoT) connections drive a new type of
organization, the Intelligent Enterprise. In intelligent enterprises, machine
learning based models are adopted to extract insights from data. Due to the
efficiency and privacy challenges of these traditional models, a new federated
learning (FL) paradigm has emerged. In FL, multiple enterprises can jointly
train a model to update a final model. However, firstly, FL trained models
usually perform worse than centralized models, especially when enterprises
training data is non-IID (Independent and Identically Distributed). Second, due
to the centrality of FL and the untrustworthiness of local enterprises,
traditional FL solutions are vulnerable to poisoning and inference attacks and
violate privacy. Thirdly, the continuous transfer of parameters between
enterprises and servers increases communication costs. To this end, the
FedAnil+ model is proposed, a novel, lightweight, and secure Federated Deep
Learning Model that includes three main phases. In the first phase, the goal is
to solve the data type distribution skew challenge. Addressing privacy concerns
against poisoning and inference attacks is covered in the second phase.
Finally, to alleviate the communication overhead, a novel compression approach
is proposed that significantly reduces the size of the updates. The experiment
results validate that FedAnil+ is secure against inference and poisoning
attacks with better accuracy. In addition, it shows improvements over existing
approaches in terms of model accuracy (13%, 16%, and 26%), communication cost
(17%, 21%, and 25%), and computation cost (7%, 9%, and 11%).; 85) Beyond Interaction Patterns: Assessing Claims of Coordinated Inter-State
  Information Operations on Twitter/X; Social media platforms have become key tools for coordinated influence
operations, enabling state actors to manipulate public opinion through
strategic, collective actions. While previous research has suggested
collaboration between states, such research failed to leverage state-of-the-art
coordination indicators or control datasets. In this study, we investigate
inter-state coordination by analyzing multiple online behavioral traces and
using sophisticated coordination detection models. By incorporating a control
dataset to differentiate organic user activity from coordinated efforts, our
findings reveal no evidence of inter-state coordination. These results
challenge earlier claims and underscore the importance of robust methodologies
and control datasets in accurately detecting online coordination.; 86) Resolution of Erd\H{o}s' problems about unimodularity; Letting $\delta_1(n,m)$ be the density of the set of integers with exactly
one divisor in $(n,m)$, Erd\H{o}s wondered if $\delta_1(n,m)$ is unimodular for
fixed $n$.
  We prove this is false in general, as the sequence $(\delta_1(n,m))$ has
superpolynomially many local extrema. However, we confirm unimodality in the
single case for which it occurs; $n = 1$.
  We also solve the question on unimodality of the density of integers whose
$k^{th}$ prime is $p$.; 87) Development and Validation of the Provider Documentation Summarization
  Quality Instrument for Large Language Models; As Large Language Models (LLMs) are integrated into electronic health record
(EHR) workflows, validated instruments are essential to evaluate their
performance before implementation. Existing instruments for provider
documentation quality are often unsuitable for the complexities of
LLM-generated text and lack validation on real-world data. The Provider
Documentation Summarization Quality Instrument (PDSQI-9) was developed to
evaluate LLM-generated clinical summaries. Multi-document summaries were
generated from real-world EHR data across multiple specialties using several
LLMs (GPT-4o, Mixtral 8x7b, and Llama 3-8b). Validation included Pearson
correlation for substantive validity, factor analysis and Cronbach's alpha for
structural validity, inter-rater reliability (ICC and Krippendorff's alpha) for
generalizability, a semi-Delphi process for content validity, and comparisons
of high-versus low-quality summaries for discriminant validity. Seven physician
raters evaluated 779 summaries and answered 8,329 questions, achieving over 80%
power for inter-rater reliability. The PDSQI-9 demonstrated strong internal
consistency (Cronbach's alpha = 0.879; 95% CI: 0.867-0.891) and high
inter-rater reliability (ICC = 0.867; 95% CI: 0.867-0.868), supporting
structural validity and generalizability. Factor analysis identified a 4-factor
model explaining 58% of the variance, representing organization, clarity,
accuracy, and utility. Substantive validity was supported by correlations
between note length and scores for Succinct (rho = -0.200, p = 0.029) and
Organized ($\rho = -0.190$, $p = 0.037$). Discriminant validity distinguished
high- from low-quality summaries ($p < 0.001$). The PDSQI-9 demonstrates robust
construct validity, supporting its use in clinical practice to evaluate
LLM-generated summaries and facilitate safer integration of LLMs into
healthcare workflows.; 88) The Layered Catalan Monoids: Structure and Determinants; In this paper, we introduce and study a class of monoids, called Layered
Catalan Monoids (\( {LC}_n \)), which satisfy the structural conditions for
$\ll$-smoothness as defined in~\cite{Sha-Det2}. These monoids are defined by
specific identities inspired by Catalan monoids. We establish their canonical
forms and compute their determinant, proving that it is non-zero for \(1 \leq n
\leq 7\) but vanishes for \(n \geq 8\).; 89) Supersymmetric scale-separated AdS$_3$ vacua of type IIB; I construct supersymmetric, parametrically scale-separated AdS$_3$ vacua of
type IIB string theory. These arise as compactifications with orientifold
planes on specific seven-dimensional solvmanifolds admitting co-closed
$G_2$-structures, preserving minimal supersymmetry. There are solutions that
include either one set or four sets of intersecting O5-planes in the smeared
approximation, and parametric scale separation can be achieved by tuning
unbounded fluxes to infinity. Additionally, the putative holographic field
theory operators that are dual to the lightest scalars in the gravitational
theory have integer conformal dimensions at tree level, aligning with other
scale-separated models of type II string theory.; 90) Network-centric optimal hybrid sensing hole recovery and self-healing in
  IPV6 WSNs; In our earlier work, Network-Centric Optimal Hybrid Mobility for IPv6
wireless sensor networks, in which the work sought to control mobility of
sensor nodes from an external network was proposed. It was a major improvement
on earlier works such as Cluster Sensor Proxy Mobile IPv6 (CSPMIPv6) and
Network of Proxies (NoP). In this work, the Network-Centric optimal hybrid
mobility scenario was used to detect and fill sensing holes occurring as a
result damaged or energy depleted sensing nodes. Various sensor networks
self-healing and recovery, and deployment algorithms such as Enhanced Virtual
Forces Algorithm with Boundary Forces (EVFA-B); Coverage - Aware Sensor
Automation protocol (CASA); Sensor Self-Organizing Algorithm (SSOA); VorLag and
the use of the use of anchor and relay nodes were reviewed. With node density
thresholds set for various scenarios, the recovery efficiency using various
parameters were measured. Comparably, our method provides the most efficient
node relocation and self-healing mechanism for sensor networks. Compared to
Sensor Self-Organizing Algorithm (SSOA), Hybrid Mobile IP showed superiority in
coverage, shorter period of recovery, less computational cost and lower energy
depletion. With processing and mobility costs shifted to the external network,
Hybrid Mobile IP extends the life span of the network.; 91) Learned Bayesian Cram\'er-Rao Bound for Unknown Measurement Models Using
  Score Neural Networks; The Bayesian Cram\'er-Rao bound (BCRB) is a crucial tool in signal processing
for assessing the fundamental limitations of any estimation problem as well as
benchmarking within a Bayesian frameworks. However, the BCRB cannot be computed
without full knowledge of the prior and the measurement distributions. In this
work, we propose a fully learned Bayesian Cram\'er-Rao bound (LBCRB) that
learns both the prior and the measurement distributions. Specifically, we
suggest two approaches to obtain the LBCRB: the Posterior Approach and the
Measurement-Prior Approach. The Posterior Approach provides a simple method to
obtain the LBCRB, whereas the Measurement-Prior Approach enables us to
incorporate domain knowledge to improve the sample complexity and
{interpretability}. To achieve this, we introduce a Physics-encoded score
neural network which enables us to easily incorporate such domain knowledge
into a neural network. We {study the learning} errors of the two suggested
approaches theoretically, and validate them numerically. We demonstrate the two
approaches on several signal processing examples, including a linear
measurement problem with unknown mixing and Gaussian noise covariance matrices,
frequency estimation, and quantized measurement. In addition, we test our
approach on a nonlinear signal processing problem of frequency estimation with
real-world underwater ambient noise.; 92) Evaluating Spoken Language as a Biomarker for Automated Screening of
  Cognitive Impairment; Timely and accurate assessment of cognitive impairment is a major unmet need
in populations at risk. Alterations in speech and language can be early
predictors of Alzheimer's disease and related dementias (ADRD) before clinical
signs of neurodegeneration. Voice biomarkers offer a scalable and non-invasive
solution for automated screening. However, the clinical applicability of
machine learning (ML) remains limited by challenges in generalisability,
interpretability, and access to patient data to train clinically applicable
predictive models. Using DementiaBank recordings (N=291, 64% female), we
evaluated ML techniques for ADRD screening and severity prediction from spoken
language. We validated model generalisability with pilot data collected
in-residence from older adults (N=22, 59% female). Risk stratification and
linguistic feature importance analysis enhanced the interpretability and
clinical utility of predictions. For ADRD classification, a Random Forest
applied to lexical features achieved a mean sensitivity of 69.4% (95%
confidence interval (CI) = 66.4-72.5) and specificity of 83.3% (78.0-88.7). On
real-world pilot data, this model achieved a mean sensitivity of 70.0%
(58.0-82.0) and specificity of 52.5% (39.3-65.7). For severity prediction using
Mini-Mental State Examination (MMSE) scores, a Random Forest Regressor achieved
a mean absolute MMSE error of 3.7 (3.7-3.8), with comparable performance of 3.3
(3.1-3.5) on pilot data. Linguistic features associated with higher ADRD risk
included increased use of pronouns and adverbs, greater disfluency, reduced
analytical thinking, lower lexical diversity and fewer words reflecting a
psychological state of completion. Our interpretable predictive modelling
offers a novel approach for in-home integration with conversational AI to
monitor cognitive health and triage higher-risk individuals, enabling earlier
detection and intervention.; 93) Steady-state coherence in multipartite quantum systems: its connection
  with thermodynamic quantities and impact on quantum thermal machines; Understanding how coherence of quantum systems affects thermodynamic
quantities, such as work and heat, is essential for harnessing quantumness
effectively in thermal quantum technologies. Here, we study the unique
contributions of quantum coherence among different subsystems of a multipartite
system, specifically in non-equilibrium steady states, to work and heat
currents. Our system comprises two coupled ensembles, each consisting of $N$
particles, interacting with two baths of different temperatures, respectively.
The particles in an ensemble interact with their bath either simultaneously or
sequentially, leading to non-local dissipation and enabling the decomposition
of work and heat currents into local and non-local components.We find that the
non-local heat current, as well as both the local and non-local work
currents,are linked to the system quantum coherence. We provide explicit
expressions of coherence-related quantities that determine the work currents
under various intrasystem interactions.Our scheme is versatile, capable of
functioning as a refrigerator, an engine, and an accelerator, with its
performance being highly sensitive to the configuration settings. These
findings establish a connection between thermodynamic quantities and quantum
coherence, supplying valuable insights for the design of quantum thermal
machines.; 94) CrossView-GS: Cross-view Gaussian Splatting For Large-scale Scene
  Reconstruction; 3D Gaussian Splatting (3DGS) has emerged as a prominent method for scene
representation and reconstruction, leveraging densely distributed Gaussian
primitives to enable real-time rendering of high-resolution images. While
existing 3DGS methods perform well in scenes with minor view variation, large
view changes in cross-view scenes pose optimization challenges for these
methods. To address these issues, we propose a novel cross-view Gaussian
Splatting method for large-scale scene reconstruction, based on dual-branch
fusion. Our method independently reconstructs models from aerial and ground
views as two independent branches to establish the baselines of Gaussian
distribution, providing reliable priors for cross-view reconstruction during
both initialization and densification. Specifically, a gradient-aware
regularization strategy is introduced to mitigate smoothing issues caused by
significant view disparities. Additionally, a unique Gaussian supplementation
strategy is utilized to incorporate complementary information of dual-branch
into the cross-view model. Extensive experiments on benchmark datasets
demonstrate that our method achieves superior performance in novel view
synthesis compared to state-of-the-art methods.; 95) Med-R$^2$: Crafting Trustworthy LLM Physicians through Retrieval and
  Reasoning of Evidence-Based Medicine; In recent years, Large Language Models (LLMs) have exhibited remarkable
capabilities in clinical scenarios. However, despite their potential, existing
works face challenges when applying LLMs to medical settings. Strategies
relying on training with medical datasets are highly cost-intensive and may
suffer from outdated training data. Leveraging external knowledge bases is a
suitable alternative, yet it faces obstacles such as limited retrieval
precision and poor effectiveness in answer extraction. These issues
collectively prevent LLMs from demonstrating the expected level of proficiency
in mastering medical expertise. To address these challenges, we introduce
Med-R^2, a novel LLM physician framework that adheres to the Evidence-Based
Medicine (EBM) process, efficiently integrating retrieval mechanisms as well as
the selection and reasoning processes of evidence, thereby enhancing the
problem-solving capabilities of LLMs in healthcare scenarios and fostering a
trustworthy LLM physician. Our comprehensive experiments indicate that Med-R^2
achieves a 14.87\% improvement over vanilla RAG methods and even a 3.59\%
enhancement compared to fine-tuning strategies, without incurring additional
training costs.; 96) Spectroscopic signatures of biexcitons: A case study in
  Ruddlesden-Popper lead-halides; Exciton-exciton interactions are fundamental to the light-emitting properties
of semiconductors, influencing applications from lasers to quantum light
sources. In this study, we investigate the spectroscopic signatures and binding
energy of biexcitons in a metal halide two-dimensional Ruddlesden-Popper
structure, which is known for hosting distinct excitonic resonances with unique
lattice coupling. Using three spectroscopic techniques - photoluminescence (PL)
and two variations of two-dimensional electronic spectroscopy (2DES) - we map
coherent one-quantum and two-quantum correlations to gain deeper insight into
the biexciton characteristics. While PL spectroscopy is hindered by spectral
broadening and reabsorption, 2DES provides a more accurate characterization,
revealing multiple biexciton states and uncovering a mixed biexciton species
arising from exciton cross-coupling. These findings highlight the importance of
advanced spectroscopic approaches in accurately determining biexciton binding
energies and offer new perspectives on many-body interactions in
exciton-polarons within layered perovskites.; 97) Purcell-enhanced emissions from diamond color centers in slow light
  photonic crystal waveguides; Quantum memories based on emitters with optically addressable spins rely on
efficient photonic interfaces, often implemented as nanophotonic cavities with
ideally narrow spectral linewidths and small mode volumes. However, these
approaches require nearly perfect spectral and spatial overlap between the
cavity mode and quantum emitter, which can be challenging. This is especially
true in the case of solid-state quantum emitters that are often randomly
positioned and can suffer from significant inhomogeneous broadening. An
alternative approach to mitigate these challenges is to use slow-light
waveguides that can enhance light-matter interaction across large optical
bandwidths and large areas. Here, we demonstrate diamond slow light photonic
crystal (PhC) waveguides that enable broadband optical coupling to embedded
silicon-vacancy (SiV) color centers. We take advantage of the recently
demonstrated thin-film diamond photonic platform to fabricate fully suspended
two-dimensional PhC waveguides. Using this approach, we demonstrate waveguide
modes with high group indices up to 70 and observe Purcell-enhanced emissions
of the SiVs coupled to the waveguide mode. Our approach represents a practical
diamond platform for robust spin-photon interfaces with color centers.; 98) Automated Quantum Algorithm Synthesis; We present a computational method to automatically design the n-qubit
realisations of quantum algorithms. Our approach leverages a domain-specific
language (DSL) that enables the construction of quantum circuits via modular
building blocks, making it well-suited for evolutionary search. In this DSL
quantum circuits are abstracted beyond the usual gate-sequence description and
scale automatically to any problem size. This enables us to learn the algorithm
structure rather than a specific unitary implementation. We demonstrate our
method by automatically designing three known quantum algorithms--the Quantum
Fourier Transform, the Deutsch-Jozsa algorithm, and Grover's search.
Remarkably, we were able to learn the general implementation of each algorithm
by considering examples of circuits containing at most 5-qubits. Our method
proves robust, as it maintains performance across increasingly large search
spaces. Convergence to the relevant algorithm is achieved with high probability
and with moderate computational resources.; 99) An integrated widely tunable linear isolator based on electro-optic
  Autler-Townes splitting; Optical isolators are essential for laser protection and robust signal
routing, but the incorporation of the necessary magneto-optic (MO) materials in
foundries has remained a challenge. As an alternative, several integrated
non-magnetic isolators based on acousto-optic (AO) and electro-optic (EO)
spatio-temporal modulation have been proposed. Unlike MO isolators, these
solutions are wavelength agnostic, though few published demonstrations reach
performance that is comparable to MO devices. The most significant remaining
concerns are on mitigating undesirable sidebands, lowering power consumption,
achieving wide bandwidth or wide tunability, and having a design that is
practical to deploy. Here we demonstrate a compact EO optical isolator, using
thin film lithium niobate, that produces a very high isolation figure of merit
(>32 dB contrast per dB of insertion loss) with THz-scale (8 nm) tunability
and, due to its architecture, achieves linear operation with negligible
sideband generation. The device uses a 20x smaller interaction length and
consumes 64x less power compared to the state-of-the-art EO and AO
architectures, without being subject to many of their technical constraints.; 100) Emamectin benzoate sensing using vivianenes (2D vivianites); The excessive application of pesticides, particularly the overreliance on
insecticides for the protection of desirable crops from pests, has posed a
significant threat to both ecological systems and human health due to
environmental pollution. This research outlines a comprehensive approach to
recognizing and quantifying the presence of insecticides through the
application of spectroscopic and electrochemical sensing methods. The detection
of Emamectin benzoate (EB), a commonly used insecticide, was performed
utilizing vivianenes, a 2D phosphate that has been mechanically exfoliated from
the naturally occurring vivianite minerals. This investigation examined the
structural and compositional characteristics of vivianenes, utilizing a range
of characterization methods. The spectroscopic analyses reveal the molecular
interactions and structural modifications that take place during the
interaction of EB with the 2D template. Electrochemical investigations
employing cyclic voltammetry were performed for different concentrations of EB
to enable real-time monitoring of the pesticide. The modified sensing electrode
using vivianene demonstrated a linear range of from 50 mg/L to 10 micro g/L,
effectively detecting EB molecules at levels significantly below the hazardous
threshold. Fully atomistic molecular dynamics simulations were also carried out
to obtain further insights into the interaction mechanisms of the EB with the
vivianites, and the results corroborate the adsorption mechanism. Our results
highlight the potential application of 2D phosphate minerals as advanced
sensors to enhance agricultural monitoring and promote sustainable development.",0.1111111111111111,0.6934264036172708
2411.02815,applied,2411.02815-pos2-7,"Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network; An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images.; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",2411.02815-pos1-7,"Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation; In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate.",52,"['1', '11', '86', '25', '29', '5', '4', '10', '38', '72']","The first candidate paper on graph-based learning for image segmentation aligns closely with the main paper's focus on automated segmentation using deep learning. It offers complementary insights into how graph neural networks can enhance segmentation tasks, particularly in medical imaging, thus forming a strong multidisciplinary link with the proposed research on liver segmentation. The subsequent papers do not integrate as effectively with the main paper's themes and focus areas, making them less suitable for forming a novel multidisciplinary idea.","1) Image Segmentation: Inducing graph-based learning; This study explores the potential of graph neural networks (GNNs) to enhance
semantic segmentation across diverse image modalities. We evaluate the
effectiveness of a novel GNN-based U-Net architecture on three distinct
datasets: PascalVOC, a standard benchmark for natural image segmentation,
WoodScape, a challenging dataset of fisheye images commonly used in autonomous
driving, introducing significant geometric distortions; and ISIC2016, a dataset
of dermoscopic images for skin lesion segmentation. We compare our proposed
UNet-GNN model against established convolutional neural networks (CNNs) based
segmentation models, including U-Net and U-Net++, as well as the
transformer-based SwinUNet. Unlike these methods, which primarily rely on local
convolutional operations or global self-attention, GNNs explicitly model
relationships between image regions by constructing and operating on a graph
representation of the image features. This approach allows the model to capture
long-range dependencies and complex spatial relationships, which we hypothesize
will be particularly beneficial for handling geometric distortions present in
fisheye imagery and capturing intricate boundaries in medical images. Our
analysis demonstrates the versatility of GNNs in addressing diverse
segmentation challenges and highlights their potential to improve segmentation
accuracy in various applications, including autonomous driving and medical
image analysis.; 2) Is User Perception the Key to Unlocking the Full Potential of Business
  Process Management Systems (BPMS)? Enhancing BPMS Efficacy Through User
  Perception; This study investigates factors influencing employees' perceptions of the
usefulness of Business Process Management Systems (BPMS) in commercial
settings. It explores the roles of system dependency, system quality, and the
quality of information and knowledge in the adoption and use of BPMS. Data were
collected using a structured questionnaire from end-users in various firms and
analyzed with Partial Least Squares (PLS). The survey evaluated perceptions of
service quality, input quality, system attributes, and overall system quality.
The findings indicate that service quality, input quality, and specific system
attributes significantly influence perceived system quality, while system
dependency and information quality are predictors of perceived usefulness. The
results highlight the importance of user training, support, and high-quality
information in enhancing satisfaction and BPMS. This research offers empirical
evidence on the factors impacting user perceptions and acceptance, emphasizing
the need for user-centric approaches in BPMS.; 3) Spindown of massive main sequence stars in the Milky Way; Context. We need to understand the spin evolution of massive stars to compute
their internal rotationally induced mixing processes, isolate effects of close
binary evolution, and predict the rotation rates of white dwarfs, neutron stars
and black holes.
  Aims. We discuss the spindown of massive main sequence stars imposed by
stellar winds.
  Methods. We use detailed grids of single star evolutionary models to predict
the distribution of the surface rotational velocities of core-hydrogen burning
Galactic massive stars as function of their mass and evolutionary state. We
then compare the spin properties of our synthetic populations with
appropriately selected sub-samples of Galactic main sequence OB-type stars
extracted from the IACOB survey.
  Results. We find that below $\sim 40 M_\odot$, observations and models agree
in finding that the surface rotational velocities of Galactic massive stars
remain relatively constant during their main sequence evolution. The more
massive stars in the IACOB sample appear to spin down less than predicted,
while our updated angular momentum loss prescription predicts an enhanced
spindown. Furthermore, the observations show a population of fast rotators,
with $v \sin I \gtrsim 200$ km/s persisting for all ages, which is not
reproduced by our synthetic single star populations.
  Conclusions. We conclude that the wind-induced spindown of massive main
sequence stars is yet to be fully understood, and that close binary evolution
might significantly contribute to the fraction of rapid rotators in massive
stars.; 4) Robust zero modes in PbTe-Pb hybrid nanowires; Majorana zero modes in tunneling conductance are expected to manifest as
robust zero bias peaks (ZBPs). While ZBPs alone are not conclusive evidence of
Majorana modes due to alternative explanations, robust ZBPs remain a crucial
and necessary first-step indicator in the search for topological states. Here,
we report the observation of robust ZBPs in PbTe-Pb hybrid nanowires. The peak
height can reach $2e^2/h$, though it does not yet form a quantized plateau.
Importantly, these ZBPs can remain non-split over sizable ranges in both
magnetic field and gate voltage scans, highlighting their robustness. We
discuss possible interpretations based on Majorana zero modes as well as
Andreev bound states.; 5) Chance constraints transcription and failure risk estimation for
  stochastic trajectory optimization; Space exploration has advanced significantly, with missions increasingly
using complex dynamical systems. Optimal trajectory design is crucial,
involving the minimization of objective functions while ensuring robustness
against measurement and control errors. Recent research has focused on
stochastic solvers that address uncertainties through chance constraints, which
are relaxed hard constraints allowing for a given failure risk. This study
introduces three novel, general, multidimensional transcription methods for
chance constraints: the spectral radius, first-order, and d-th order methods.
Additionally, we introduce failure risk estimation techniques and a
conservatism metric to enable comprehensive comparison with existing
approaches. Applications to aerospace test cases demonstrate the effectiveness
of the proposed transcriptions, highlighting that state-of-the-art methods
significantly overestimate risk. Notably, the d-th order transcription
dramatically outperforms the other methods, particularly in high-dimensional
scenarios. This work shows that spectral radius-based methods are overly
conservative and computationally intensive, while the first-order and d-th
order methods offer practical and efficient alternatives.; 6) Low Mach number limit for the compressible Navier-Stokes equation with a
  stationary force; In this paper, we are concerned with the low Mach number limit for the
compressible Navier-Stokes equation with a stationary force and ill-prepared
initial data in the three-dimensional whole space. The convergence result of
the stationary solutions toward corresponding incompressible flow is obtained
when a stationary force is small enough. Under the assumption that the initial
perturbation around the stationary solution is small enough, the convergence
result of the perturbation toward the corresponding perturbation around the
stationary incompressible flow is obtained globally in time. The Strichartz
type estimate for the linearized semigroup around the motionless state plays a
crucial role in the proofs.; 7) Mid-infrared absorption spectra and mass absorption coefficients for 23
  chondrites: dependence on composition and grain size; We present mid-infrared transmission spectra from 2 to 23 microns of the 23
Atacama Desert chondrites of different types (carbonaceous Ornans and ordinary
of H, L, and LL groups) as well as of some pure minerals (olivine and
diopside). We focus on the characteristics of silicate at 10 and 20 microns,
analyzing the influence of composition and grain size on peak strengths and
spectral shapes. We present the first results of the Cosmic Dust Laboratory, a
dedicated facility at the Universidad Diego Portales equipped with a VERTEX 80v
vacuum Fourier transform infrared spectrometer. Through milling and sieving
samples, we obtained different ranges of particle sizes to study the effect of
grain size on the intensity and shape of the spectrum. The resulting spectral
library can be compared with astronomical data of protoplanetary disks, debris
disks, and even white dwarf disks obtained with instruments such as MIRI on
board the James Webb Space Telescope and MATISSE on the Very Large Telescope
Interferometer. We also present mass absorption coefficient values, which can
be used for radiative transfer modeling of astronomical observations. This
study aims to improve dust opacities for astronomical applications, with a
focus on circumstellar disks.; 8) Removal of radon progeny from delicate surfaces; $\rm ^{210}Po$ $\alpha$-decay driven neutron background is a concern for many
low-energy rare event experiments. It is a difficult-to-control background that
depends on the air exposure history of parts. In this study we demonstrate that
about half of the radon progeny $\rm ^{210}Po$ can be removed from copper and
silicon surfaces by wiping it with an acetone wetted tissue. For a copper
sample we demonstrate that $\rm ^{210}Pb$ is removed with similar
effectiveness. Additional wiping was found to be largely ineffective.; 9) Generalized Radial Uncertainty Product for d-Dimensional Hydrogen Atom; This paper presents a comprehensive analysis of the generalized radial
uncertainty product for the d-dimensional non-relativistic Hydrogen atom in
position space. Utilizing the framework of quantum mechanics in d-dimensional
spherical coordinates, the study extends the standard radial uncertainty
relation to higher dimensions. Taking the solution of the radial Schrodinger
equation, the normalized radial wave functions, expectation values, and
uncertainties in both position and momentum space are rigorously evaluated. The
analytical derivations reveal the dependence of the uncertainty product on the
principal and angular quantum numbers, as well as the dimensional parameter d.
The results provide deeper insight into the role of dimensionality in quantum
uncertainty relations and their implications for higher-dimensional quantum
systems; 10) A data-driven two-microphone method for in-situ sound absorption
  measurements; This work presents a data-driven approach to estimating the sound absorption
coefficient of an infinite porous slab using a neural network and a
two-microphone measurement on a finite porous sample. A 1D-convolutional
network predicts the sound absorption coefficient from the complex-valued
transfer function between the sound pressure measured at the two microphone
positions. The network is trained and validated with numerical data generated
by a boundary element model using the Delany-Bazley-Miki model, demonstrating
accurate predictions for various numerical samples. The method is
experimentally validated with baffled rectangular samples of a fibrous
material, where sample size and source height are varied. The results show that
the neural network offers the possibility to reliably predict the in-situ sound
absorption of a porous material using the traditional two-microphone method as
if the sample were infinite. The normal-incidence sound absorption coefficient
obtained by the network compares well with that obtained theoretically and in
an impedance tube. The proposed method has promising perspectives for
estimating the sound absorption coefficient of acoustic materials after
installation and in realistic operational conditions.; 11) Automating High Quality RT Planning at Scale; Radiotherapy (RT) planning is complex, subjective, and time-intensive.
Advances in artificial intelligence (AI) promise to improve its precision,
efficiency, and consistency, but progress is often limited by the scarcity of
large, standardized datasets. To address this, we introduce the Automated
Iterative RT Planning (AIRTP) system, a scalable solution for generating
high-quality treatment plans. This scalable solution is designed to generate
substantial volumes of consistently high-quality treatment plans, overcoming a
key obstacle in the advancement of AI-driven RT planning. Our AIRTP pipeline
adheres to clinical guidelines and automates essential steps, including
organ-at-risk (OAR) contouring, helper structure creation, beam setup,
optimization, and plan quality improvement, using AI integrated with RT
planning software like Eclipse of Varian. Furthermore, a novel approach for
determining optimization parameters to reproduce 3D dose distributions, i.e. a
method to convert dose predictions to deliverable treatment plans constrained
by machine limitations. A comparative analysis of plan quality reveals that our
automated pipeline produces treatment plans of quality comparable to those
generated manually, which traditionally require several hours of labor per
plan. Committed to public research, the first data release of our AIRTP
pipeline includes nine cohorts covering head-and-neck and lung cancer sites to
support an AAPM 2025 challenge. This data set features more than 10 times the
number of plans compared to the largest existing well-curated public data set
to our best knowledge. Repo:
https://github.com/RiqiangGao/GDP-HMM_AAPMChallenge.; 12) Blocked Bloom Filters with Choices; Probabilistic filters are approximate set membership data structures that
represent a set of keys in small space, and answer set membership queries
without false negative answers, but with a certain allowed false positive
probability. Such filters are widely used in database systems, networks,
storage systems and in biological sequence analysis because of their fast query
times and low space requirements. Starting with Bloom filters in the 1970s,
many filter data structures have been developed, each with its own advantages
and disadvantages, e.g., Blocked Bloom filters, Cuckoo filters, XOR filters,
Ribbon filters, and more.
  We introduce Blocked Bloom filters with choices that work similarly to
Blocked Bloom filters, except that for each key there are two (or more)
alternative choices of blocks where the key's information may be stored. The
result is a filter that partially inherits the advantages of a Blocked Bloom
filter, such as the ability to insert keys rapidly online or the ability to
slightly overload the filter with only a small penalty to the false positive
rate. At the same time, it avoids the major disadvantage of a Blocked Bloom
filter, namely the larger space consumption. Our new data structure uses less
space at the same false positive rate, or has a lower false positive rate at
the same space consumption as a Blocked Bloom filter. We discuss the
methodology, engineered implementation, a detailed performance evaluation and
use cases in bioinformatics of Blocked Bloom filters with choices, showing that
they can be of practical value.
  The implementation of the evaluated filters and the workflows used are
provided via Gitlab at https://gitlab.com/rahmannlab/blowchoc-filters.; 13) What happens due to the baby universe effect in JT gravity? -- Analysis
  of correlation functions and ERB length at late time using three approaches; We analyze the correlation function in JT gravity using three approaches: by
summing over all geodesics connecting boundary operators, integrating over the
region of moduli space determined by the ``no-shortcut condition'' introduced
by D.Stanford and Z.Yang, and using the formula for the universal spectral
density correlation in the $\tau$-scaling limit. We find that the behaviors of
the three results coincide at late times: they all exhibit a ``ramp'' instead
of permanent decay. Using the third approach we also confirm that the
``plateau'' appears after $T_H=2\pi e^{S_0}\hat{\rho}_0(E)$. Overall, our
results are consistent with the SFF analysis.
  We also calculate the ERB length $\langle \ell(T) \rangle$ using the three
approaches and find that the results are in good agreement with each other. We
also find that the $\langle \ell(T) \rangle$ grows as a cubic function in $T$
due to the contribution from geometry including one observable baby universe,
and converges to a constant after $T=T_H$. For the geometry with one baby
universe, we compute the size $\langle b(T) \rangle$ of the baby universe and
find that it is of the same order as $\langle \ell(T) \rangle$. This result is
consistent with the baby universe emission mechanism claimed by P.Saad.; 14) Sketch-of-Thought: Efficient LLM Reasoning with Adaptive
  Cognitive-Inspired Sketching; Recent advances in large language models have demonstrated remarkable
reasoning capabilities through Chain of Thought (CoT) prompting, but often at
the cost of excessive verbosity in their intermediate outputs, which increases
computational overhead. We introduce Sketch-of-Thought (SoT), a novel prompting
framework that combines cognitive-inspired reasoning paradigms with linguistic
constraints to minimize token usage while preserving reasoning accuracy. SoT is
designed as a flexible framework that can incorporate any custom reasoning
paradigms based on cognitive science, and we instantiate it with three such
paradigms - Conceptual Chaining, Chunked Symbolism, and Expert Lexicons - each
tailored to different reasoning tasks and selected dynamically via a
lightweight routing model. Through comprehensive evaluation across 15 reasoning
datasets with multiple languages and multimodal scenarios, we demonstrate that
SoT achieves token reductions of 76% with negligible accuracy impact. In
certain domains like mathematical and multi-hop reasoning, it even improves
accuracy while using significantly fewer tokens. Our code is publicly
available: https://www.github.com/SimonAytes/SoT.; 15) Automatic selection of the best neural architecture for time series
  forecasting via multi-objective optimization and Pareto optimality conditions; Time series forecasting plays a pivotal role in a wide range of applications,
including weather prediction, healthcare, structural health monitoring,
predictive maintenance, energy systems, and financial markets. While models
such as LSTM, GRU, Transformers, and State-Space Models (SSMs) have become
standard tools in this domain, selecting the optimal architecture remains a
challenge. Performance comparisons often depend on evaluation metrics and the
datasets under analysis, making the choice of a universally optimal model
controversial. In this work, we introduce a flexible automated framework for
time series forecasting that systematically designs and evaluates diverse
network architectures by integrating LSTM, GRU, multi-head Attention, and SSM
blocks. Using a multi-objective optimization approach, our framework determines
the number, sequence, and combination of blocks to align with specific
requirements and evaluation objectives. From the resulting Pareto-optimal
architectures, the best model for a given context is selected via a
user-defined preference function. We validate our framework across four
distinct real-world applications. Results show that a single-layer GRU or LSTM
is usually optimal when minimizing training time alone. However, when
maximizing accuracy or balancing multiple objectives, the best architectures
are often composite designs incorporating multiple block types in specific
configurations. By employing a weighted preference function, users can resolve
trade-offs between objectives, revealing novel, context-specific optimal
architectures. Our findings underscore that no single neural architecture is
universally optimal for time series forecasting. Instead, the best-performing
model emerges as a data-driven composite architecture tailored to user-defined
criteria and evaluation objectives.; 16) High-velocity outflows in [OIII] emitters at z=2.5-9 from JWST NIRSpec
  medium-resolution spectroscopy; We identify galaxies hosting ionised-gas high-velocity outflows from the
complete sample of medium resolution (R1000) JWST/NIRSpec MSA spectroscopy
taken as part of the JWST Advanced Deep Extragalactic Survey (JADES). From a
total sample of 1087 [OIII]5007 emitters we identify 40 galaxies with a
blue-side broadened [OIII]5007 line at z=2.5-9. Of these, 34 are strong outflow
candidates whilst 6 sources have broadening potentially driven by rotating
clumps. Our outflow candidate sample is mainly composed of star-forming
galaxies, including ~65% starbursts, which span the stellar mass range
log10(M*/Msun)=7.5-11.0. It also includes two candidate type-2 active galactic
nuclei (AGN) and a 'little red dot' (LRD). We report a median outflow velocity
of 531(+146)(-159) km/s and an overall incidence rate of 3.4%. These values are
significantly higher and lower respectively than recent similar works, which we
accredit to the limiting resolution of the R1000 spectroscopy and a stricter
outflow selection criterion. We find no correlation between the outflow
velocity and the galaxy stellar mass or star-formation rate. The median ratio
between outflow velocity and escape velocity is 0.77(+0.36)(-0.32), indicating
that most outflows cannot escape the galaxy gravitational potentials. We do
find an anti-correlation between mass loading factor and stellar mass up to M*
~(10^10)Msun, with most lowest stellar-mass M*<(10^9)Msun galaxies reaching
values well above unity, as is the case for local starburst galaxies.; 17) Scoring Verifiers: Evaluating Synthetic Verification in Code and
  Reasoning; Code verification has recently found great success as a critical component in
training large scale reasoning models for coding. Synthetic techniques such as
self-generated test cases and reward models provide a way to enhance code
capabilities beyond predefined tests. Building on these advancements, we
propose new benchmarks designed to systematically evaluate the impact of
synthetic verification methods on assessing solution correctness. We introduce
HE-R, HE-R+, MBPP-R, and MBPP-R+, which transform existing coding benchmarks
into scoring and ranking datasets to evaluate the effectiveness of synthetic
verifiers. Using these benchmarks, we analyze synthetic verification methods in
standard, reasoning-based, and reward-based LLMs. Our results show that recent
reasoning models significantly improve test case generation and that scaling
test cases enhances verification accuracy.; 18) Bridging Classical and Quantum String Matching: A Computational
  Reformulation of Bit-Parallelism; String matching is a fundamental problem in computer science, with critical
applications in text retrieval, bioinformatics, and data analysis. Among the
numerous solutions that have emerged for this problem in recent decades,
bit-parallelism has significantly enhanced their practical efficiency, leading
to the development of several optimized approaches for both exact and
approximate string matching. However, their potential in quantum computing
remains largely unexplored. This paper presents a novel pathway that not only
translates bit-parallel string matching algorithms into the quantum framework
but also enhances their performance to achieve a quadratic speedup through
Grover's search. By embedding quantum search within a bit-parallel model, we
reduce the time complexity of string matching, establishing a structured
pathway for transforming classical algorithms into quantum solutions with
provable computational advantages. Beyond exact matching, this technique offers
a foundation for tackling a wide range of non-standard string matching
problems, opening new avenues for efficient text searching in the quantum era.
To demonstrate the simplicity and adaptability of the technique presented in
this paper, we apply this translation and adaptation process to two landmark
bit-parallel algorithms: Shift-And for exact pattern matching and Shift-Add for
approximate string matching with up to k errors.; 19) A Parallel, Energy-Stable Low-Rank Integrator for Nonlinear Multi-Scale
  Thermal Radiative Transfer; Thermal radiative transfer models physical phenomena ranging from supernovas
in astrophysics to radiation from a hohlraum striking a fusion target in plasma
physics. Transport and absorption of particles in radiative transfer at
different rates lead to a complex interaction between the material and
particles that involves highly varying time scales. Resolving these effects can
require prohibitively small step sizes, which, combined with nonlinear effects
and the particle density's high-dimensional phase space, render conventional
numerical methods computationally expensive. This work presents an
asymptotic--preserving, mass conservative, rank-adaptive, and parallel
integrator for a macro--micro decomposition-based dynamical low-rank
approximation of the thermal radiative transfer equations. The proposed
integrator efficiently incorporates reflection-transmission type boundary
conditions in the low-rank factors. It captures the nonlinear effects of
thermal radiation and is energy stable with the step size restriction capturing
both hyperbolic and parabolic CFL conditions. The efficacy of the proposed
integrator is demonstrated with numerical experiments.; 20) Multi-Symmetric Schur Functions; We study a multi-symmetric generalization of the classical Schur functions
called the multi-symmetric Schur functions. These functions form an integral
basis for the ring of multi-symmetric functions indexed by tuples of partitions
and are defined as certain stable-limits of key polynomials. We prove
combinatorial results about the monomial expansions of the multi-symmetric
Schur functions including a diagrammatic combinatorial formula and a
triangularity result which completely characterizes their monomial
multi-symmetric supports. The triangularity result involves a non-trivial
generalization of the dominance order on partitions to tuples of partitions. We
prove, using the Demazure character formula, that the multi-symmetric Schur
functions expand positively into the basis of tensor products of ordinary Schur
functions and describe the expansion coefficients as multiplicities of certain
irreducible representations for Levi subgroups inside particular Demazure
modules. Lastly, we find a family of multi-symmetric plethystic operators
related to the classical Bernstein operators which act on the multi-symmetric
Schur basis by a simple recurrence relation.; 21) Discovering the influence of personal features in psychological
  processes using Artificial Intelligence techniques: the case of COVID19
  lockdown in Spain; At the end of 2019, an outbreak of a novel coronavirus was reported in China,
leading to the COVID-19 pandemic. In Spain, the first cases were detected in
late January 2020, and by mid-March, infections had surpassed 5,000. On March
the Spanish government started a nationwide lockdown to contain the spread of
the virus. While isolation measures were necessary, they posed significant
psychological and socioeconomic challenges, particularly for vulnerable
populations. Understanding the psychological impact of lockdown and the factors
influencing mental health is crucial for informing future public health
policies. This study analyzes the influence of personal, socioeconomic, general
health and living condition factors on psychological states during lockdown
using AI techniques. A dataset collected through an online questionnaire was
processed using two workflows, each structured into three stages. First,
individuals were categorized based on psychological assessments, either
directly or in combination with unsupervised learning techniques. Second,
various Machine Learning classifiers were trained to distinguish between the
identified groups. Finally, feature importance analysis was conducted to
identify the most influential variables related to different psychological
conditions. The evaluated models demonstrated strong performance, with accuracy
exceeding 80% and often surpassing 90%, particularly for Random Forest,
Decision Trees, and Support Vector Machines. Sensitivity and specificity
analyses revealed that models performed well across different psychological
conditions, with the health impacts subset showing the highest reliability. For
diagnosing vulnerability, models achieved over 90% accuracy, except for less
vulnerable individuals using living environment and economic status features,
where performance was slightly lower.; 22) SR-Reward: Taking The Path More Traveled; In this paper, we propose a novel method for learning reward functions
directly from offline demonstrations. Unlike traditional inverse reinforcement
learning (IRL), our approach decouples the reward function from the learner's
policy, eliminating the adversarial interaction typically required between the
two. This results in a more stable and efficient training process. Our reward
function, called \textit{SR-Reward}, leverages successor representation (SR) to
encode a state based on expected future states' visitation under the
demonstration policy and transition dynamics. By utilizing the Bellman
equation, SR-Reward can be learned concurrently with most reinforcement
learning (RL) algorithms without altering the existing training pipeline. We
also introduce a negative sampling strategy to mitigate overestimation errors
by reducing rewards for out-of-distribution data, thereby enhancing robustness.
This strategy inherently introduces a conservative bias into RL algorithms that
employ the learned reward. We evaluate our method on the D4RL benchmark,
achieving competitive results compared to offline RL algorithms with access to
true rewards and imitation learning (IL) techniques like behavioral cloning.
Moreover, our ablation studies on data size and quality reveal the advantages
and limitations of SR-Reward as a proxy for true rewards.; 23) InsightVision: A Comprehensive, Multi-Level Chinese-based Benchmark for
  Evaluating Implicit Visual Semantics in Large Vision Language Models; In the evolving landscape of multimodal language models, understanding the
nuanced meanings conveyed through visual cues - such as satire, insult, or
critique - remains a significant challenge. Existing evaluation benchmarks
primarily focus on direct tasks like image captioning or are limited to a
narrow set of categories, such as humor or satire, for deep semantic
understanding. To address this gap, we introduce, for the first time, a
comprehensive, multi-level Chinese-based benchmark designed specifically for
evaluating the understanding of implicit meanings in images. This benchmark is
systematically categorized into four subtasks: surface-level content
understanding, symbolic meaning interpretation, background knowledge
comprehension, and implicit meaning comprehension. We propose an innovative
semi-automatic method for constructing datasets, adhering to established
construction protocols. Using this benchmark, we evaluate 15 open-source large
vision language models (LVLMs) and GPT-4o, revealing that even the
best-performing model lags behind human performance by nearly 14% in
understanding implicit meaning. Our findings underscore the intrinsic
challenges current LVLMs face in grasping nuanced visual semantics,
highlighting significant opportunities for future research and development in
this domain. We will publicly release our InsightVision dataset, code upon
acceptance of the paper.; 24) Orbitronics in Two-dimensional Materials; Orbitronics explores the control and manipulation of electronic orbital
angular momentum in solid-state systems, opening new pathways for information
processing and storage. One significant advantage of orbitronics over
spintronics is that it does not rely on spin-orbit coupling, thereby broadening
the range of non-magnetic materials that can be utilized for these
applications. It also introduces new topological features related to electronic
orbital angular momentum, and clarifies some long-standing challenges in
understanding experiments that rely on the conventional concept of valley
transport. This review highlights recent advances in orbitronics, particularly
in relation to two-dimensional materials. We examine the fundamental principles
underlying the generation, transport, and dynamics of orbital angular momentum
to illustrate how the unique properties of two-dimensional materials can
promote orbitronic phenomena. We also outline potential future research
directions and address some outstanding questions in this field.; 25) Towards Developing Socially Compliant Automated Vehicles: State of the
  Art, Experts Expectations, and A Conceptual Framework; Automated Vehicles (AVs) hold promise for revolutionizing transportation by
improving road safety, traffic efficiency, and overall mobility. Despite the
steady advancement in high-level AVs in recent years, the transition to full
automation entails a period of mixed traffic, where AVs of varying automation
levels coexist with human-driven vehicles (HDVs). Making AVs socially compliant
and understood by human drivers is expected to improve the safety and
efficiency of mixed traffic. Thus, ensuring AVs compatibility with HDVs and
social acceptance is crucial for their successful and seamless integration into
mixed traffic. However, research in this critical area of developing Socially
Compliant AVs (SCAVs) remains sparse. This study carries out the first
comprehensive scoping review to assess the current state of the art in
developing SCAVs, identifying key concepts, methodological approaches, and
research gaps. An expert interview was also conducted to identify critical
research gaps and expectations towards SCAVs. Based on the scoping review and
expert interview input, a conceptual framework is proposed for the development
of SCAVs. The conceptual framework is evaluated using an online survey
targeting researchers, technicians, policymakers, and other relevant
professionals worldwide. The survey results provide valuable validation and
insights, affirming the significance of the proposed conceptual framework in
tackling the challenges of integrating AVs into mixed-traffic environments.
Additionally, future research perspectives and suggestions are discussed,
contributing to the research and development agenda of SCAVs.; 26) AgentOrca: A Dual-System Framework to Evaluate Language Agents on
  Operational Routine and Constraint Adherence; As language agents progressively automate critical tasks across domains,
their ability to operate within operational constraints and safety protocols
becomes essential. While extensive research has demonstrated these agents'
effectiveness in downstream task completion, their reliability in following
operational procedures and constraints remains largely unexplored. To this end,
we present AgentOrca, a dual-system framework for evaluating language agents'
compliance with operational constraints and routines. Our framework encodes
action constraints and routines through both natural language prompts for
agents and corresponding executable code serving as ground truth for automated
verification. Through an automated pipeline of test case generation and
evaluation across five real-world domains, we quantitatively assess current
language agents' adherence to operational constraints. Our findings reveal
notable performance gaps among state-of-the-art models, with large reasoning
models like o1 demonstrating superior compliance while others show
significantly lower performance, particularly when encountering complex
constraints or user persuasion attempts.; 27) A Comprehensive Metric for Resilience Evaluation of Power Distribution
  Systems under Cyber Attacks; Power distribution systems (PDS) serve as the backbone of our modern society,
ensuring electricity reaches homes, businesses, and critical infrastructure.
However, the increasing digitization and interconnectivity of these systems
have exposed them to cyber threats. This study presents a comprehensive
approach to evaluate and enhance the resilience of PDS under cyber attacks
using the Common Vulnerability Scoring System (CVSS) and complex network
parameters. By systematically assessing vulnerabilities and computing
resilience once critical CVSS thresholds are reached, this work identifies key
resilience metrics including the critical loads service requirements. The
proposed methodology improves system resilience through strategic tie-line
switching, which is validated on the modified IEEE 33-bus system. Four case
studies are conducted, illustrating the performance of the proposed methodology
under various cyber attack scenarios. The results demonstrate the effectiveness
of the approach in quantifying and enhancing resilience, offering a valuable
tool for PDS operators to mitigate risks and ensure continuous service delivery
to critical loads during the exploitation of cyber threats.; 28) Single-Satellite-Based Geolocation of Broadcast GNSS Spoofers from Low
  Earth Orbit; This paper presents an analysis and experimental demonstration of
single-satellite single-pass geolocation of a terrestrial broadcast Global
Navigation Satellite System (GNSS) spoofer from Low Earth Orbit (LEO). The
proliferation of LEO-based GNSS receivers offers the prospect of unprecedented
spectrum awareness, enabling persistent GNSS interference detection and
geolocation. Accurate LEO-based single-receiver emitter geolocation is possible
when a range-rate time history can be extracted for the emitter. This paper
presents a technique crafted specifically for indiscriminate broadcast-type
GNSS spoofing signals. Furthermore, it explores how unmodeled oscillator
instability and worst-case spoofer-introduced signal variations degrade the
geolocation estimate. The proposed geolocation technique is validated by a
controlled experiment, in partnership with Spire Global, in which a LEO-based
receiver captures broadcast GNSS spoofing signals transmitted from a known
ground station on a non-GNSS frequency band.; 29) VidSole: A Multimodal Dataset for Joint Kinetics Quantification and
  Disease Detection with Deep Learning; Understanding internal joint loading is critical for diagnosing gait-related
diseases such as knee osteoarthritis; however, current methods of measuring
joint risk factors are time-consuming, expensive, and restricted to lab
settings. In this paper, we enable the large-scale, cost-effective
biomechanical analysis of joint loading via three key contributions: the
development and deployment of novel instrumented insoles, the creation of a
large multimodal biomechanics dataset (VidSole), and a baseline deep learning
pipeline to predict internal joint loading factors. Our novel instrumented
insole measures the tri-axial forces and moments across five high-pressure
points under the foot. VidSole consists of the forces and moments measured by
these insoles along with corresponding RGB video from two viewpoints, 3D body
motion capture, and force plate data for over 2,600 trials of 52 diverse
participants performing four fundamental activities of daily living
(sit-to-stand, stand-to-sit, walking, and running). We feed the insole data and
kinematic parameters extractable from video (i.e., pose, knee angle) into a
deep learning pipeline consisting of an ensemble Gated Recurrent Unit (GRU)
activity classifier followed by activity-specific Long Short Term Memory (LSTM)
regression networks to estimate knee adduction moment (KAM), a biomechanical
risk factor for knee osteoarthritis. The successful classification of
activities at an accuracy of 99.02 percent and KAM estimation with mean
absolute error (MAE) less than 0.5 percent*body weight*height, the current
threshold for accurately detecting knee osteoarthritis with KAM, illustrates
the usefulness of our dataset for future research and clinical settings.; 30) WaveFM: A High-Fidelity and Efficient Vocoder Based on Flow Matching; Flow matching offers a robust and stable approach to training diffusion
models. However, directly applying flow matching to neural vocoders can result
in subpar audio quality. In this work, we present WaveFM, a reparameterized
flow matching model for mel-spectrogram conditioned speech synthesis, designed
to enhance both sample quality and generation speed for diffusion vocoders.
Since mel-spectrograms represent the energy distribution of waveforms, WaveFM
adopts a mel-conditioned prior distribution instead of a standard Gaussian
prior to minimize unnecessary transportation costs during synthesis. Moreover,
while most diffusion vocoders rely on a single loss function, we argue that
incorporating auxiliary losses, including a refined multi-resolution STFT loss,
can further improve audio quality. To speed up inference without degrading
sample quality significantly, we introduce a tailored consistency distillation
method for WaveFM. Experiment results demonstrate that our model achieves
superior performance in both quality and efficiency compared to previous
diffusion vocoders, while enabling waveform generation in a single inference
step.; 31) Vector mesons from a holographic QCD model in $f(R)$-dilaton gravity; In this paper we investigate the spectrum of vector mesons based on
$f(R)$-dilaton gravity. We focus particularly on the well-known Starobinsky
model, given by the function $f(R)= R+ \alpha R^2$ in the metric formalism, and
we examine the deviations from the pure Einstein-dilaton case. We thus provide
the field equations for Starobinsky-dilaton gravity in $\rm AdS_{5}$ spacetime
and calculate the spectral decomposition of the vector mesons by means of a
Sturm-Liouville problem. Remarkably, for the rho mesons $m_{\rho^2}$ and
$m_{\rho^3}$, our results are in good agreement with the experimental data when
$\alpha= 10^{-8}$. Our work also generalizes previous studies and recovers
standard results when $\alpha= 0$.; 32) A fast and slightly robust covariance estimator; Let $\mathcal{Z} = \{Z_1, \dots, Z_n\} \stackrel{\mathrm{i.i.d.}}{\sim} P
\subset \mathbb{R}^d$ from a distribution $P$ with mean zero and covariance
$\Sigma$. Given a dataset $\mathcal{X}$ such that
$d_{\mathrm{ham}}(\mathcal{X}, \mathcal{Z}) \leq \varepsilon n$, we are
interested in finding an efficient estimator $\widehat{\Sigma}$ that achieves
$\mathrm{err}(\widehat{\Sigma}, \Sigma) :=
\|\Sigma^{-\frac{1}{2}}\widehat{\Sigma}\Sigma^{-\frac{1}{2}} - I\|
_{\mathrm{op}} \leq 1/2$. We focus on the low contamination regime $\varepsilon
= o(1/\sqrt{d}$). In this regime, prior work required either $\Omega(d^{3/2})$
samples or runtime that is exponential in $d$. We present an algorithm that,
for subgaussian data, has near-linear sample complexity $n =
\widetilde{\Omega}(d)$ and runtime $O((n+d)^{\omega + \frac{1}{2}})$, where
$\omega$ is the matrix multiplication exponent. We also show that this
algorithm works for heavy-tailed data with near-linear sample complexity, but
in a smaller regime of $\varepsilon$. Concurrent to our work, Diakonikolas et
al. [2024] give Sum-of-Squares estimators that achieve similar sample
complexity but with large polynomial runtime.; 33) Deep Minimax Classifiers for Imbalanced Datasets with a Small Number of
  Minority Samples; The concept of a minimax classifier is well-established in statistical
decision theory, but its implementation via neural networks remains
challenging, particularly in scenarios with imbalanced training data having a
limited number of samples for minority classes. To address this issue, we
propose a novel minimax learning algorithm designed to minimize the risk of
worst-performing classes. Our algorithm iterates through two steps: a
minimization step that trains the model based on a selected target prior, and a
maximization step that updates the target prior towards the adversarial prior
for the trained model. In the minimization, we introduce a targeted
logit-adjustment loss function that efficiently identifies optimal decision
boundaries under the target prior. Moreover, based on a new prior-dependent
generalization bound that we obtained, we theoretically prove that our loss
function has a better generalization capability than existing loss functions.
During the maximization, we refine the target prior by shifting it towards the
adversarial prior, depending on the worst-performing classes rather than on
per-class risk estimates. Our maximization method is particularly robust in the
regime of a small number of samples. Additionally, to adapt to
overparameterized neural networks, we partition the entire training dataset
into two subsets: one for model training during the minimization step and the
other for updating the target prior during the maximization step. Our proposed
algorithm has a provable convergence property, and empirical results indicate
that our algorithm performs better than or is comparable to existing methods.
All codes are publicly available at
https://github.com/hansung-choi/TLA-linear-ascent.; 34) REALTALK: A 21-Day Real-World Dataset for Long-Term Conversation; Long-term, open-domain dialogue capabilities are essential for chatbots
aiming to recall past interactions and demonstrate emotional intelligence (EI).
Yet, most existing research relies on synthetic, LLM-generated data, leaving
open questions about real-world conversational patterns. To address this gap,
we introduce REALTALK, a 21-day corpus of authentic messaging app dialogues,
providing a direct benchmark against genuine human interactions.
  We first conduct a dataset analysis, focusing on EI attributes and persona
consistency to understand the unique challenges posed by real-world dialogues.
By comparing with LLM-generated conversations, we highlight key differences,
including diverse emotional expressions and variations in persona stability
that synthetic dialogues often fail to capture.
  Building on these insights, we introduce two benchmark tasks: (1) persona
simulation where a model continues a conversation on behalf of a specific user
given prior dialogue context; and (2) memory probing where a model answers
targeted questions requiring long-term memory of past interactions.
  Our findings reveal that models struggle to simulate a user solely from
dialogue history, while fine-tuning on specific user chats improves persona
emulation. Additionally, existing models face significant challenges in
recalling and leveraging long-term context within real-world conversations.; 35) Digital Guardians: Can GPT-4, Perspective API, and Moderation API
  reliably detect hate speech in reader comments of German online newspapers?; In recent years, toxic content and hate speech have become widespread
phenomena on the internet. Moderators of online newspapers and forums are now
required, partly due to legal regulations, to carefully review and, if
necessary, delete reader comments. This is a labor-intensive process. Some
providers of large language models already offer solutions for automated hate
speech detection or the identification of toxic content. These include GPT-4o
from OpenAI, Jigsaw's (Google) Perspective API, and OpenAI's Moderation API.
Based on the selected German test dataset HOCON34k, which was specifically
created for developing tools to detect hate speech in reader comments of online
newspapers, these solutions are compared with each other and against the
HOCON34k baseline. The test dataset contains 1,592 annotated text samples. For
GPT-4o, three different promptings are used, employing a Zero-Shot, One-Shot,
and Few-Shot approach. The results of the experiments demonstrate that GPT-4o
outperforms both the Perspective API and the Moderation API, and exceeds the
HOCON34k baseline by approximately 5 percentage points, as measured by a
combined metric of MCC and F2-score.; 36) An Asymmetric Independence Model for Causal Discovery on Path Spaces; We develop the theory linking 'E-separation' in directed mixed graphs (DMGs)
with conditional independence relations among coordinate processes in
stochastic differential equations (SDEs), where causal relationships are
determined by ""which variables enter the governing equation of which other
variables"". We prove a global Markov property for cyclic SDEs, which naturally
extends to partially observed cyclic SDEs, because our asymmetric independence
model is closed under marginalization. We then characterize the class of graphs
that encode the same set of independence relations, yielding a result analogous
to the seminal 'same skeleton and v-structures' result for directed acyclic
graphs (DAGs). In the fully observed case, we show that each such equivalence
class of graphs has a greatest element as a parsimonious representation and
develop algorithms to identify this greatest element from data. We conjecture
that a greatest element also exists under partial observations, which we verify
computationally for graphs with up to four nodes.; 37) Quasinormal Modes, Grebody Factors, and Hawking Radiation Sparsity of
  Black Holes Influenced by a Global Monopole Charge in Kalb-Ramond Gravity; Kalb-Ramond (KR) gravity is an intriguing model incorporating local Lorentz
violation, and black hole (BH) solutions are known to exist. In this study, we
investigate some crucial aspects of BHs endowed with a global monopole charge
in the self-interacting KR field. Specifically, we study the quasinormal modes
(QNMs) corresponding to scalar, electromagnetic, and gravitational
perturbations; derive rigorous bounds for the greybody factors (GBFs); and
examine the sparsity of Hawking radiation. The effects of the model parameters
$\ell$ (Lorentz-violating parameter in KR gravity) and $\eta$ (monopole charge)
on these phenomena are elaborated. First, QNMs are evaluated with high
precision using the 13\textsuperscript{th}-order Pad\'{e}-averaged WKB method
and cross-examined via time-domain analyses within an acceptable parameter
space. The results show that the estimated QNMs are more sensitive to $\ell$;
however, both model parameters influence the frequency spectra. The derived
bounds on the GBFs aid in further constraining the parameter space. It is shown
that $\ell$ and $\eta$ have a similar effect on the greybody bounds.
Furthermore, positive and negative values of $\ell$ have opposing effects in
that the bounds are reversed for the two cases. The analyses of the Hawking
radiation sparsity highlight the effect of $\ell$, and two scenarios are noted:
either the radiation emitted is less sparse than Hawking radiation, or it is
more sparse during the evaporation phase. Thus, this work presents a
comprehensive account of BHs in KR gravity with a global monopole charge.; 38) Microservice Deployment in Space Computing Power Networks via Robust
  Reinforcement Learning; With the growing demand for Earth observation, it is important to provide
reliable real-time remote sensing inference services to meet the low-latency
requirements. The Space Computing Power Network (Space-CPN) offers a promising
solution by providing onboard computing and extensive coverage capabilities for
real-time inference. This paper presents a remote sensing artificial
intelligence applications deployment framework designed for Low Earth Orbit
satellite constellations to achieve real-time inference performance. The
framework employs the microservice architecture, decomposing monolithic
inference tasks into reusable, independent modules to address high latency and
resource heterogeneity. This distributed approach enables optimized
microservice deployment, minimizing resource utilization while meeting quality
of service and functional requirements. We introduce Robust Optimization to the
deployment problem to address data uncertainty. Additionally, we model the
Robust Optimization problem as a Partially Observable Markov Decision Process
and propose a robust reinforcement learning algorithm to handle the
semi-infinite Quality of Service constraints. Our approach yields sub-optimal
solutions that minimize accuracy loss while maintaining acceptable
computational costs. Simulation results demonstrate the effectiveness of our
framework.; 39) QDM: Quadtree-Based Region-Adaptive Sparse Diffusion Models for
  Efficient Image Super-Resolution; Deep learning-based super-resolution (SR) methods often perform pixel-wise
computations uniformly across entire images, even in homogeneous regions where
high-resolution refinement is redundant. We propose the Quadtree Diffusion
Model (QDM), a region-adaptive diffusion framework that leverages a quadtree
structure to selectively enhance detail-rich regions while reducing
computations in homogeneous areas. By guiding the diffusion with a quadtree
derived from the low-quality input, QDM identifies key regions-represented by
leaf nodes-where fine detail is essential and applies minimal refinement
elsewhere. This mask-guided, two-stream architecture adaptively balances
quality and efficiency, producing high-fidelity outputs with low computational
redundancy. Experiments demonstrate QDM's effectiveness in high-resolution SR
tasks across diverse image types, particularly in medical imaging (e.g., CT
scans), where large homogeneous regions are prevalent. Furthermore, QDM
outperforms or is comparable to state-of-the-art SR methods on standard
benchmarks while significantly reducing computational costs, highlighting its
efficiency and suitability for resource-limited environments. Our code is
available at https://github.com/linYDTHU/QDM.; 40) Fokker-Planck to Callan-Symanzik: evolution of weight matrices under
  training; The dynamical evolution of a neural network during training has been an
incredibly fascinating subject of study. First principal derivation of generic
evolution of variables in statistical physics systems has proved useful when
used to describe training dynamics conceptually, which in practice means
numerically solving equations such as Fokker-Planck equation. Simulating entire
networks inevitably runs into the curse of dimensionality. In this paper, we
utilize Fokker-Planck to simulate the probability density evolution of
individual weight matrices in the bottleneck layers of a simple
2-bottleneck-layered auto-encoder and compare the theoretical evolutions
against the empirical ones by examining the output data distributions. We also
derive physically relevant partial differential equations such as
Callan-Symanzik and Kardar-Parisi-Zhang equations from the dynamical equation
we have.; 41) Teaching Language Models to Critique via Reinforcement Learning; Teaching large language models (LLMs) to critique and refine their outputs is
crucial for building systems that can iteratively improve, yet it is
fundamentally limited by the ability to provide accurate judgments and
actionable suggestions. In this work, we study LLM critics for code generation
and propose $\texttt{CTRL}$, a framework for $\texttt{C}$ritic
$\texttt{T}$raining via $\texttt{R}$einforcement $\texttt{L}$earning, which
trains a critic model to generate feedback that maximizes correction
performance for a fixed generator model without human supervision. Our results
demonstrate that critics trained with $\texttt{CTRL}$ significantly enhance
pass rates and mitigate compounding errors across both base and stronger
generator models. Furthermore, we show that these critic models act as accurate
generative reward models and enable test-time scaling through iterative
critique-revision, achieving up to 106.1% relative improvements across
challenging code generation benchmarks.; 42) Structure evolution with cosmic backgrounds from radio to far infrared; Cosmic background radiation, both diffuse and discrete in nature, produced at
different cosmic epochs before and after recombination, provides key
information on the evolution of cosmic structures. We discuss the main classes
of sources that contribute to the extragalactic background light from radio to
sub-millimetre wavelenghs and the currently open question on the level of the
cosmic radio background spectrum. The redshifted 21cm line signal from
cosmological neutral Hydrogen during the primeval phases of cosmic structures
as a probe of the cosmological reionisation process is presented, along with
the route for confident detection of this signal. We then describe the basic
formalism and the feasibility to study via a differential approach, based
mainly on dipole analysis, the tiny imprints in the CB spectrum expected from a
variety of cosmological and astrophysical processes at work during the early
phases of cosmic perturbation and structure evolution. Finally, we discuss the
identification of high-redshift sub-millimetre lensed galaxies with extreme
magnifications in the Planck maps and their use for the comprehension of
fundamental processes in early galaxy formation and evolution.; 43) Towards Fairness for the Right Reasons: Using Saliency Maps to Evaluate
  Bias Removal in Neural Networks; The widespread adoption of machine learning systems has raised critical
concerns about fairness and bias, making mitigating harmful biases essential
for AI development. In this paper, we investigate the relationship between
fairness improvement and the removal of harmful biases in neural networks
applied to computer vision tasks. First, we introduce a set of novel XAI-based
metrics that analyze saliency maps to assess shifts in a model's
decision-making process. Then, we demonstrate that successful debiasing methods
systematically redirect model focus away from protected attributes.
Additionally, we show that techniques originally developed for artifact removal
can be effectively repurposed for fairness. These findings underscore the
importance of ensuring that models are fair for the right reasons, contributing
to the development of more ethical and trustworthy AI systems.; 44) DPM-Bench: Benchmark for Distributed Process Mining Algorithms on
  Cyber-Physical Systems; Process Mining is established in research and industry systems to analyze and
optimize processes based on event data from information systems. Within this
work, we accomodate process mining techniques to Cyber-Physical Systems. To
capture the distributed and heterogeneous characteristics of data,
computational resources, and network communication in CPS, the todays process
mining algorithms and techniques must be augmented. Specifically, there is a
need for new Distributed Process Mining algorithms that enable computations to
be performed directly on edge resources, eliminating the need for moving all
data to central cloud systems. This paper introduces the DPM-Bench benchmark
for comparing such Distributed Process Mining algorithms. DPM-Bench is used to
compare algorithms deployed in different computational topologies. The results
enable information system engineers to assess whether the existing
infrastructure is sufficient to perform distributed process mining, or to
identify required improvements in algorithms and hardware. We present and
discuss an experimental evaluation with DPM-Bench.; 45) A Deep Reinforcement Learning Approach to Automated Stock Trading, using
  xLSTM Networks; Traditional Long Short-Term Memory (LSTM) networks are effective for handling
sequential data but have limitations such as gradient vanishing and difficulty
in capturing long-term dependencies, which can impact their performance in
dynamic and risky environments like stock trading. To address these
limitations, this study explores the usage of the newly introduced Extended
Long Short Term Memory (xLSTM) network in combination with a deep reinforcement
learning (DRL) approach for automated stock trading. Our proposed method
utilizes xLSTM networks in both actor and critic components, enabling effective
handling of time series data and dynamic market environments. Proximal Policy
Optimization (PPO), with its ability to balance exploration and exploitation,
is employed to optimize the trading strategy. Experiments were conducted using
financial data from major tech companies over a comprehensive timeline,
demonstrating that the xLSTM-based model outperforms LSTM-based methods in key
trading evaluation metrics, including cumulative return, average profitability
per trade, maximum earning rate, maximum pullback, and Sharpe ratio. These
findings mark the potential of xLSTM for enhancing DRL-based stock trading
systems.; 46) Partially connected contributions to baryon masses in QCD+QED; Full QCD+QED simulations allow to evaluate isospin breaking corrections to
hadron masses. With the openQxD code, we are able to perform these simulations
employing C-periodic boundary conditions, implemented through a doubling of the
physical lattice along one spatial direction. The use of these boundary
conditions introduces non-zero Wick contractions between two quark or two
antiquark fields, that, in the case of the computation of baryon masses, lead
to partially connected additional contributions that we expect to vanish in the
infinite volume limit. These contributions are challenging because they involve
an all-to-all propagator connecting one point in the physical lattice and one
in the mirror lattice. We present a way to compute these corrections to the
$\Omega^-$ baryon mass using a combination of point and stochastic source
inversions. This work is part of the program of the RC* collaboration.; 47) UV LIGHTS. New tools for revealing the low surface brightness regime in
  the ultraviolet; Ultra-deep optical surveys have reached unprecedented depths, facilitating
the study of faint galactic structures. However, the ultraviolet bands, crucial
for stellar population studies, remain essentially unexplored at these depths.
We present a detailed surface brightness and color analysis of 20 nearby
galaxies in the LIGHTS fields observed by GALEX in the FUV and NUV. We adapt
and apply a low surface brightness oriented methodology that has proven
effective in ultra-deep optical surveys. A novel approach to background
subtraction is proposed for UV imaging. Instead of subtracting a constant value
from the background, we subtract a Poisson distribution that transforms the
background into a pseudo-Gaussian distribution centered at zero. Furthermore,
the PSF deconvolution algorithms developed for optical data are applied to our
sample, using a novel set of very extended (R=750 arcsec) PSFs for the GALEX
bands. This methodology allows us to obtain depths ranging from 28.5 to 30 mag
arcsec^{-2}, with reliable surface brightness profiles up to 31 mag
arcsec^{-2}. This is about 1 mag deeper than with standard UV techniques. We
use the surface brightness and color profiles to show that the application of
PSF deconvolution, especially in the FUV, effectively mitigates the excess of
light present in the outer regions of certain galaxies compared to the standard
GALEX pipeline. This finding is crucial for any accurate stellar population
inference from the color profiles. Additionally, a qualitative analysis of the
results is presented, with particular emphasis on surface brightness and color
properties of the galaxies beyond their optical edges. Our work highlights the
importance of developing innovative low surface brightness methods for UV
surveys.; 48) An Energy-Aware RIoT System: Analysis, Modeling and Prediction in the
  SUPERIOT Framework; This paper presents a comprehensive analysis of the energy consumption
characteristics of a Silicon (Si)-based Reconfigurable IoT (RIoT) node
developed in the initial phase of the SUPERIOT project, focusing on key
operating states, including Bluetooth Low Energy (BLE) communication,
Narrow-Band Visible Light Communication (NBVLC), sensing, and E-ink display.
Extensive measurements were conducted to establish a detailed energy profile,
which serves as a benchmark for evaluating the effectiveness of subsequent
optimizations and future node iterations. To minimize the energy consumption,
multiple optimizations were implemented at both the software and hardware
levels, achieving a reduction of over 60% in total energy usage through
software modifications alone. Further improvements were realized by optimizing
the E-ink display driving waveform and implementing a very low-power mode for
non-communication activities. Based on the measured data, three
measurement-based energy consumption models were developed to characterize the
energy behavior of the node under: (i) normal, unoptimized operation, (ii)
low-power, software-optimized operation, and (iii) very low-power,
hardware-optimized operation. These models, validated with new measurement
data, achieved an accuracy exceeding 97%, confirming their reliability for
predicting energy consumption in diverse configurations.; 49) EvoGP: A GPU-accelerated Framework for Tree-based Genetic Programming; Tree-based Genetic Programming (TGP) is a key evolutionary algorithm widely
used in symbolic regression, feature engineering, and scientific modeling. Its
high computational demands make GPU acceleration essential for scalable and
high-performance evolutionary computation. However, GPU acceleration of TGP
faces three key challenges: inefficient tree encoding, highly heterogeneous
genetic operations, and limited parallelism in fitness evaluation. To address
these challenges, we introduce EvoGP, a comprehensive GPU-accelerated TGP
framework. First, we design a tensorized encoding scheme to represent tree with
different structures as tensors with the same shape, optimizing memory access
and enabling efficient parallel execution. Second, we propose a unified
parallel framework for genetic operations by leveraging shared computational
primitives and implementing dedicated CUDA kernels for scalable performance.
Third, we present a fully parallel fitness evaluation strategy for symbolic
regression, exploiting both population-level and data-level parallelism to
maximize GPU utilization. Moreover, we implement a comprehensive library to
provide rich algorithm operators and benchmark problems. EvoGP is extensively
tested on various tasks, including symbolic regression, classification, and
robotics control, demonstrating its versatility and effectiveness across
diverse application scenarios. Experimental results show that EvoGP achieves up
to a 140.89x speedup over the state-of-the-art GPU-based TGP implementation,
while maintaining or exceeding the accuracy of baseline methods. EvoGP is
open-source and accessible at: https://github.com/EMI-Group/evogp.; 50) Observables for the Effect of Gravity on Electromagnetic Polarization; Does gravity affect the polarization of electromagnetic radiation in an
observable way? The effect of gravity on the observed polarization of a ray of
electromagnetic radiation is investigated for an arbitrary 4-dimensional
spacetime and radiation with a frequency spectrum within the geometric optics
limit and with arbitrary state of polarization. Focusing on effects observable
by a single inertial observer, we show how the presence of curvature along the
null geodesic of polarized electromagnetic radiation may induce observable
changes in the state of polarization. We find a set of scalars that quantify
the effect and derive their transport equations. Two of these scalars, the
polarization degree and the circular polarization degree, are polarization
state observables that are conserved along the radiation geodesic. Four
observables that quantify time rate of change of the observed state of
polarization are identified. These observables and their corresponding
transport equations provide a complete representation of how gravity affects
the observed state of polarization of electromagnetic radiation with
frequencies above the geometric optics limit. Polarization wiggling is sourced
by curvature twist, which is a scalar derived from the Riemann tensor.
Curvature twist is closely related to the magnetic part of the Weyl tensor, the
second Weyl scalar as well as the rotation of the rest frame geodesic
congruence. The results of this paper are valid for any metric theory of
gravity.; 51) Comprehensive Framework for Evaluating Conversational AI Chatbots; Conversational AI chatbots are transforming industries by streamlining
customer service, automating transactions, and enhancing user engagement.
However, evaluating these systems remains a challenge, particularly in
financial services, where compliance, user trust, and operational efficiency
are critical. This paper introduces a novel evaluation framework that
systematically assesses chatbots across four dimensions: cognitive and
conversational intelligence, user experience, operational efficiency, and
ethical and regulatory compliance. By integrating advanced AI methodologies
with financial regulations, the framework bridges theoretical foundations and
real-world deployment challenges. Additionally, we outline future research
directions, emphasizing improvements in conversational coherence, real-time
adaptability, and fairness.; 52) Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation; In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate.; 53) Unraveling the Nature of HAWC J1844-034 with Fermi-LAT Data Analysis and
  Multi-wavelength modelling; The extended ultra-high-energy gamma-ray source HAWC J1844-034 is closely
associated with two other sources, HAWC J1843-032 and HWC J1846-025. Moreover,
other gamma-ray observatories like H.E.S.S., LHAASO, and Tibet AS$_{\gamma}$
have detected ultra-high-energy gamma-ray sources whose spatial positions
coincide with the position of HAWC J1844-034. The ultra-high-energy gamma-ray
data from several observatories help analyse the spectral features of this
source in detail at TeV energies. Of the four pulsars near HAWC J1844-034, PSR
J1844-0346 is closest to it and possibly supplies the cosmic-ray leptons to
power this source. We have analysed the Fermi-LAT data to explore this source's
morphology and identify its spectral feature in the Fermi-LAT energy band.
After removing the contribution of the pulsar to the gamma-ray spectral energy
distribution by pulsar phased analysis, we have obtained upper limits on the
photon flux and identified the GeV counterpart PS J1844.2-0342 in the Fermi-LAT
energy band with more than 5$\sigma$ significance, which may be a pulsar wind
nebula. Finally, the multi-wavelength spectral energy distribution has been
modelled, assuming HAWC J1844-034 is a pulsar wind nebula.; 54) Anomalous Meets Topological Hall Effect in Cr2Ge2Te6 Heterostructures; Introducing topologically protected skyrmions in graphene holds significant
importance for developing high-speed, low-energy spintronic devices. Here, we
present a centrosymmetric ferromagnetic graphene/trilayer Cr2Ge2Te6/graphene
heterostructure, demonstrating the anomalous and topological Hall effect due to
the magnetic proximity effect. Through gate voltage control, we effectively
tune the emergence and size of skyrmions. Micromagnetic simulations reveal the
formation of skyrmions and antiskyrmions, which respond differently to external
magnetic fields, leading to oscillations in the topological Hall signal. Our
findings provide a novel pathway for the formation and manipulation of
skyrmions in centrosymmetric two-dimensional magnetic systems, offering
significant insights for developing topological spintronics.; 55) The role of dry mergers in shaping the scaling relations of galaxies; In the context of the hierarchical formation of galaxies, we investigated the
role played by mergers in shaping the scaling relations of galaxies, that is
the projections of their Fundamental Plane onto the Ie-Re, Ie-Sigma, Ms-Re and
L-Sigma planes. To this aim, based on the scalar Virial Theorem, we developed a
simple theory of multiple dry mergers to read both the large scale simulations
and the companion scaling relations. The aim was to compare the results of this
approach with the observational data and with two of the most recent and
detailed numerical cosmo-hydro-dynamical simulations, that is Illustris-TNG and
EAGLE (Evolution and Assembly of GaLaxies and their Environments). We derived
the above scaling relations for the galaxies of the MaNGA (Mapping Nearby
Galaxies at APO) and WINGS (Wide-field Imaging of Nearby Galaxy-Clusters
Survey) databases and compared them with the observational data, the numerical
simulations, and the results of our simple theory of dry mergers. The multiple
dry merging mechanism is able to explain all the main characteristics of the
observed scaling relations of galaxies, such as slopes, scatters, curvatures
and zones of exclusion. The distribution of galaxies in these planes is
continuously changing across time because of the merging activity and other
physical processes, such as star formation, quenching, energy feedback, and so
forth. The simple merger theory presented here yields the correct distribution
of galaxies in the main scaling relations at all cosmic epochs. The precision
is comparable with that obtained by the modern cosmo-hydro-dynamical
simulations, with the advantage of providing a rapid exploratory response on
the consequences engendered by different physical effects.; 56) Monocular visual simultaneous localization and mapping: (r)evolution
  from geometry to deep learning-based pipelines; With the rise of deep learning, there is a fundamental change in visual SLAM
algorithms toward developing different modules trained as end-to-end pipelines.
However, regardless of the implementation domain, visual SLAM's performance is
subject to diverse environmental challenges, such as dynamic elements in
outdoor environments, harsh imaging conditions in underwater environments, or
blurriness in high-speed setups. These environmental challenges need to be
identified to study the real-world viability of SLAM implementations. Motivated
by the aforementioned challenges, this paper surveys the current state of
visual SLAM algorithms according to the two main frameworks: geometry-based and
learning-based SLAM. First, we introduce a general formulation of the SLAM
pipeline that includes most of the implementations in the literature. Second,
those implementations are classified and surveyed for geometry and
learning-based SLAM. After that, environment-specific challenges are formulated
to enable experimental evaluation of the resilience of different visual SLAM
classes to varying imaging conditions. We address two significant issues in
surveying visual SLAM, providing (1) a consistent classification of visual SLAM
pipelines and (2) a robust evaluation of their performance under different
deployment conditions. Finally, we give our take on future opportunities for
visual SLAM implementations.; 57) Moir\'e $M$-valley bilayers: quasi-one-dimensional physics,
  unconventional spin textures and twisted van Hove singularities; Motivated by the discovery of quasi-two-dimensional kagome metals
AV$_3$Sb$_5$, we consider the theory of twisted bilayers in which the Fermi
surface is near the $M$-point. Surprisingly, unlike twisted bilayers of
graphene or transition metal dichalcogenides, the moir\'e potential is
quasi-one-dimensional: at each $M$-valley, the potential flattens the
dispersion strongly along one direction, and weakly along the perpendicular
direction. The combination of spin-orbit coupling and twist-induced broken
inversion symmetry results in a similarly anisotropic
`$\textit{moir\'e-Rashba}$' potential, which spin-splits the dispersion into
coexisting two-dimensional and quasi-one-dimensional bands. We discuss novel
aspects of the interplay between mixed dimensionality and spin textures in this
platform. First, an applied electric field produces spin polarisation which can
be tuned by doping, suggesting potential spintronics applications. Secondly, an
in-plane magnetic field momentum- and spin-polarises the Fermi surfaces,
producing unconventional spin density waves. Thirdly, in the small-twist-angle
limit, the large density of states due to a twisted van Hove singularity near
$M$ results in a dense energy spectrum. Our results demonstrate a new variation
of moir\'e bandstructure engineering, instigating the study of spin-textured
one-dimensional physics in moir\'e materials.; 58) Reviving networked multi-dimensional dynamical systems; From gene regulatory networks to mutualistic networks, controlling a single
node in the network topology can transform these complex dynamical systems from
undesirable states to desirable ones. Corresponding methods have been
well-studied in one-dimensional dynamical systems. However, many practical
dynamical systems require description by multi-dimensional dynamical systems,
such as the mutualistic symbiotic systems formed by flowering plants and
pollinating insects. Existing one-dimensional methods cannot handle the cases
of multi-dimensional dynamical systems. Based on this, we propose a method to
control a single node to activate network connections in multi-dimensional
dynamical systems. In such systems, the changes of each node are described by
multiple nonlinear differential equations. All remaining nodes are stratified
according to the shortest path to the controlled node, thereby reducing the
dimensionality of the system. Such a large-scale dynamical system can
ultimately be replaced by a very simple system. By analyzing the
reduced-dimensional system, we can predict the extent of control needed to
restore the system state. We apply this method to a wide range of fields,
achieving activation of various real multidimensional complex dynamical
systems.; 59) $NJ/\psi$ and $N\eta_c$ interactions from lattice QCD; The interaction between nucleon and charmonia ($J/\psi$ and $\eta_c$) is
expected to deepen our understanding of various aspects in nonperturbative QCD
ranging from the origin of nucleon mass to $J/\psi$ mass modification in
nuclear medium and properties of hidden-charm pentaquark states. Here, we
present the low-energy $NJ/\psi$ and $N\eta_c$ interactions based on ($2+1$)
flavor lattice QCD simulations with nearly physical pion mass $m_\pi=146$ MeV.
The interactions, extracted from the spacetime correlations of the nucleon and
charmonium system by using the HAL QCD method, are found to be attractive in
all distances and manifest a characteristic long-range tail consistent with the
two-pion exchange interaction. The resulting scattering lengths are around
$0.3$ fm, $0.4$ fm and $0.2$ fm for $NJ/\psi$ with spin $3/2$, with spin $1/2$,
and $N\eta_c$, respectively. Our results are orders of magnitude larger than
those from the photoproduction experiments assuming the vector meson dominance.; 60) Centralizers on a super-reflexive Schatten ideal; We give a simple proof that there is no strictly singular bicentralizer on a
super-reflexive Schatten ideal. This result applies, in particular, to the
$p$-Schatten class for $1<p<\infty$.; 61) Adaptive Bi-Level Multi-Robot Task Allocation and Learning under
  Uncertainty with Temporal Logic Constraints; This work addresses the problem of multi-robot coordination under unknown
robot transition models, ensuring that tasks specified by Time Window Temporal
Logic are satisfied with user-defined probability thresholds. We present a
bi-level framework that integrates (i) high-level task allocation, where tasks
are assigned based on the robots' estimated task completion probabilities and
expected rewards, and (ii) low-level distributed policy learning and execution,
where robots independently optimize auxiliary rewards while fulfilling their
assigned tasks. To handle uncertainty in robot dynamics, our approach leverages
real-time task execution data to iteratively refine expected task completion
probabilities and rewards, enabling adaptive task allocation without explicit
robot transition models. We theoretically validate the proposed algorithm,
demonstrating that the task assignments meet the desired probability thresholds
with high confidence. Finally, we demonstrate the effectiveness of our
framework through comprehensive simulations.; 62) Matched pairs of actions on the Kac-Paljutkin algebra $H_8$; The notion of matched pair of actions on a Hopf algebra generalizes the
braided group construction of Lu, Yan and Zhu, and efficiently provides
Yang-Baxter operators. In this paper, we classify matched pairs of actions on
the Kac-Paljutkin Hopf algebra $H_8$. Through calculations, we obtain 6 matched
pairs of actions on $H_8$. Based on such a classification result, we find that
four of them can be derived from the coquasitriangular structures of $H_8$,
while the other two can not. Furthermore, we discover that the Yang-Baxter
operators associated to exactly these two distinguished matched pairs of
actions are involutive.; 63) Stochastic multisymplectic PDEs and their structure-preserving numerical
  methods; We construct stochastic multisymplectic systems by considering a stochastic
extension to the variational formulation of multisymplectic partial
differential equations proposed in [Hydon, {\it Proc. R. Soc. A}, 461,
1627--1637, 2005]. The stochastic variational principle implies the existence
of stochastic $1$-form and $2$-form conservation laws, as well as conservation
laws arising from continuous variational symmetries via a stochastic Noether's
theorem. These results are the stochastic analogues of those found in
deterministic variational principles. Furthermore, we develop stochastic
structure-preserving collocation methods for this class of stochastic
multisymplectic systems. These integrators possess a discrete analogue of the
stochastic $2$-form conservation law and, in the case of linear systems, also
guarantee discrete momentum conservation. The effectiveness of the proposed
methods is demonstrated through their application to stochastic nonlinear
Schr\""odinger equations featuring either stochastic transport or stochastic
dispersion.; 64) Line detections in photospheric radius expansion bursts from 4U 1820-303; Context: NICER (Neutron star Interior Composition ExploreR) is the instrument
of choice for the spectral analysis of type I X-ray bursts, as it provides high
throughput at X-ray CCD resolution, down to 0.3 keV. Aims: This study
investigates whether the energies of absorption lines detected in photospheric
radius expansion (PRE) bursts correlate with the inferred blackbody radius.
Previous reports suggested such a correlation, attributed to a combination of
weaker gravitational redshift and higher blueshifts in bursts with larger
radii. Methods: The analysis reexamines four previously studied PRE bursts and
examines eight additional bursts from 4U 1820-303, evidencing PRE. Spectral
evolution is tracked on the shortest possible timescales (tenth of a second)
adopting two parallel continuum descriptions to characterise the photospheric
expansion and line evolution. Applying the accretion-enhanced model, maximum
blackbody radii of up to $\sim$ 900 km are inferred, with peak bolometric
luminosities exceeding the Eddington limit of an Helium accretor. Absorption
lines are assessed for significance using Monte Carlo simulations, and spectral
lines are characterised using the state-of-art plasma codes available within
{\sc{spex}} with a phenomenological continuum. A thorough parameter search
explores Doppler shifts to avoid local minima. Results: Several significant (>
99.9%) absorption lines, including the previously reported 2.97 keV line, are
detected. While no consistent correlation between line energies and blackbody
radii is confirmed, bursts with larger radii exhibit up to four lines and the
line strength is higher. The modelling suggests that the observed lines mostly
originate from slightly redshifted (almost rest-frame) photo-/collisionally
ionised gas in emission. For the burst with the largest PRE, a combination of
photo-ionised plasma in both emission and absorption is preferred.; 65) Bosonic M-Theory From a Kac-Moody Algebra Perspective; We study the existence of a bosonic m-theory extension of the 10D and 26D
closed bosonic string in terms of Kac-Moody algebras. We argue that K11 and K27
are symmetries which protect the coefficients of the closed bosonic string in
10 and 26 dimensions. Therefore the Susskind-Horowitz bosonic m-theory obtained
by compactification on S1/Z2, which does not produce the correct coefficients,
must be replaced by something that preserves K11 and K27. We argue that in 11D,
a non-trivial bosonic m-theory should be considered as (the bosonic sector of)
m-theory, and in 27D that no obvious bosonic m-theory exists.; 66) On the origin of bulk-related anisotropies in surface optical spectra; Reflection anisotropy spectroscopy (RAS) is a powerful method for probing the
optical properties of surfaces, used routinely in research and industrial
applications, yet the origin of 'bulk-related' features that appear in the
spectra of various surfaces has been debated for nearly 40 years. It is often
argued that these features are related to surface-induced bulk anisotropy
(SIBA) because they coincide with critical energies of the bulk dielectric
function. In general, any quantitative RAS theory must include excitonic
effects as they significantly influence the spectra and are believed to be the
key to determining the origin of SIBA features. Here, we introduce a
layer-resolved exciton localization (LREL) measure within the framework of
many-body perturbation theory, which enables a quantitative analysis of the
origins of 'bulk-related' RAS features. Applying LREL to arsenic-modified
silicon reconstructions reveals that, depending on the surface reconstruction,
the 'apparent' SIBA features arise primarily from states localized at the
surface, with only a small contribution from the underlying layers. Our
findings, further supported by the fact that the calculated spectra agree well
with low-temperature RAS measurements, challenge the conventional explanation
of 'bulk-related' RAS features. They indicate that in many instances
bulk-enhanced surface anisotropies (BESA)-the opposite of SIBA-contribute to,
or are even responsible for, 'bulk-related' RAS features. Therefore, we suggest
that previously studied semiconductor surfaces, which exhibit 'bulk-related'
features in their spectra, should be reanalyzed using the presented method.; 67) The coherence peak of unconventional superconductors in the charge
  channel; In this work, we carry out a systematic investigation of the coherence peak
in unconventional superconductors as they transition into the superconducting
phase at $T_c$. Using $d$-wave cuprates as an example, we reveal the presence
of a coherence peak below $T_c$ in the charge channel. The nuclear quadrupole
relaxation rate is shown to be an effective method for detecting this
unconventional coherence peak, with the superconducting coherence factor
playing a pivotal role in its emergence. Additionally, we explore the influence
of correlation effects, which further enhance this phenomenon. Extending our
analysis, we demonstrate the existence of a similar coherence peak in
ultrasonic attenuation and iron-based superconductors. Our findings offer a
fresh perspective on probing superconducting gap symmetry in unconventional
superconductors.; 68) Polynomial invariants for two-dimensional algebras; We classify all two-dimensional simple algebras over an algebraically closed
field. For each two-dimensional algebra $\mathcal{A}$ with an infinite group of
automorphisms we describe a minimal (with respect to inclusion) generating set
for the algebra of invariants of the $m$-tuples of $\mathcal{A}$ in
characteristic zero case. As a consequence, we show that in characteristic zero
case Artin-Procesi-Iltyakov Equality holds for all two-dimensional simple
algebras with an infinite group of automorphisms. We also consider
nondegenerate invariant bilinear forms over two-dimensional algebras.; 69) Tracking Down Software Cluster Bombs: A Current State Analysis of the
  Free/Libre and Open Source Software (FLOSS) Ecosystem; Throughout computer history, it has been repeatedly demonstrated that
critical software vulnerabilities can significantly affect the components
involved. In the Free/Libre and Open Source Software (FLOSS) ecosystem, most
software is distributed through package repositories. Nowadays, monitoring
critical dependencies in a software system is essential for maintaining robust
security practices. This is particularly important due to new legal
requirements, such as the European Cyber Resilience Act, which necessitate that
software projects maintain a transparent track record with Software Bill of
Materials (SBOM) and ensure a good overall state. This study provides a summary
of the current state of available FLOSS package repositories and addresses the
challenge of identifying problematic areas within a software ecosystem. These
areas are analyzed in detail, quantifying the current state of the FLOSS
ecosystem. The results indicate that while there are well-maintained projects
within the FLOSS ecosystem, there are also high-impact projects that are
susceptible to supply chain attacks. This study proposes a method for analyzing
the current state and identifies missing elements, such as interfaces, for
future research.; 70) AIM2PC: Aerial Image to 3D Building Point Cloud Reconstruction; Three-dimensional urban reconstruction of buildings from single-view images
has attracted significant attention over the past two decades. However, recent
methods primarily focus on rooftops from aerial images, often overlooking
essential geometrical details. Additionally, there is a notable lack of
datasets containing complete 3D point clouds for entire buildings, along with
challenges in obtaining reliable camera pose information for aerial images.
This paper addresses these challenges by presenting a novel methodology, AIM2PC
, which utilizes our generated dataset that includes complete 3D point clouds
and determined camera poses. Our approach takes features from a single aerial
image as input and concatenates them with essential additional conditions, such
as binary masks and Sobel edge maps, to enable more edge-aware reconstruction.
By incorporating a point cloud diffusion model based on Centered denoising
Diffusion Probabilistic Models (CDPM), we project these concatenated features
onto the partially denoised point cloud using our camera poses at each
diffusion step. The proposed method is able to reconstruct the complete 3D
building point cloud, including wall information and demonstrates superior
performance compared to existing baseline techniques. To allow further
comparisons with our methodology the dataset has been made available at
https://github.com/Soulaimene/AIM2PCDataset; 71) Monotonicity of the Relative Entropy and the Two-sided Bogoliubov
  Inequality in von Neumann Algebras; This text studies, on the one hand, certain monotonicity properties of the
Araki-Uhlmann relative entropy and, on the other hand, unbounded perturbation
theory of KMS-states which facilitates a proof of the two-sided Bogoliubov
inequality in general von Neumann algebras. After introducing the necessary
background from the theory of operator algebras and Tomita-Takesaki modular
theory, the relative entropy functional is defined and its basic properties are
studied. In particular, a full and detailed proof of Uhlmann's important
monotonicity theorem for the relative entropy is provided. This theorem will
then be used to derive a number of monotonicity inequalities for the relative
entropy of normal functionals induced by vectors of the form $V \varOmega, V
\varPhi \in \mathcal{H}$, where $V \in \mathscr{B}(\mathcal{H})$ is a suitable
transformation. After that, an introduction to perturbation theory in von
Neumann algebras is given, with an emphasis on unbounded perturbations of
KMS-states following the framework of Derezi\'{n}ski-Jak\v{s}i\'{c}-Pillet.
This mathematical apparatus will then be used to extend the two-sided
Bogoliubov inequality for the relative free energy, which was very recently
proved for quantum-mechanical systems, to arbitrary von Neumann algebras.; 72) Global small data weak solutions of 2-D semilinear wave equations with
  scale-invariant damping; There is an interesting open question: for the $n$-D ($n\ge 1$) semilinear
wave equation with scale-invariant damping $\partial_t^2u-\Delta
u+\frac{\mu}{t}\partial_tu=|u|^p$, where $t\ge 1$, $p>1$, $\mu\in (0,
\bar\mu(n))$ with $\bar\mu(n)=\frac{n^2+n+2}{n+2}$, the global small data weak
solution $u$ will exist when $p>p_{s}(n+\mu)
=\frac{n+\mu+1+\sqrt{(n+\mu)^2+10(n+\mu)-7}}{2(n+\mu-1)}$. In the case of
$n=1$, this open question has been solved well. We now systematically study
this problem for $n=2$ and $\mu\in (0, 1)\cup (1, \bar\mu(2))$ with
$\bar\mu(2)=2$. In the present paper, the global small solution $u$ is
established for $p_{s}(2+\mu)<p<p_{conf}(2,\mu)=\frac{\mu+5}{\mu+1}$ and
$\mu\in(0,1)\cup(1,2)$. Our main ingredients are to derive some new kinds of
spacetime-weighted $L^{q}_tL^{q}_x([1, \infty)\times \mathbb{R}^2)$ or
$L^q_tL^\nu_rL^2_{\theta}([1, \infty)\times [0, \infty)\times [0, 2\pi])$
Strichartz estimates for the solutions of linear generalized Tricomi equation
$\partial_t^2v-t^m\Delta v=F(t,x)$ ($m>0$). In forthcoming paper, we show the
global existence of small solution $u$ for $p\ge p_{conf}(2,\mu)$ and $\mu\in
(0,1)\cup (1, 2)$.; 73) From Visuals to Vocabulary: Establishing Equivalence Between Image and
  Text Token Through Autoregressive Pre-training in MLLMs; While MLLMs perform well on perceptual tasks, they lack precise multimodal
alignment, limiting performance. To address this challenge, we propose Vision
Dynamic Embedding-Guided Pretraining (VDEP), a hybrid autoregressive training
paradigm for MLLMs. Utilizing dynamic embeddings from the MLP following the
visual encoder, this approach supervises image hidden states and integrates
image tokens into autoregressive training. Existing MLLMs primarily focused on
recovering information from textual inputs, often neglecting the effective
processing of image data. In contrast, the key improvement of this work is the
reinterpretation of multimodal alignment as a process of recovering information
from input data, with particular emphasis on reconstructing detailed visual
features.The proposed method seamlessly integrates into standard models without
architectural changes. Experiments on 13 benchmarks show VDEP outperforms
baselines, surpassing existing methods.; 74) Optimal Privacy-Preserving Distributed Median Consensus; Distributed median consensus has emerged as a critical paradigm in
multi-agent systems due to the inherent robustness of the median against
outliers and anomalies in measurement. Despite the sensitivity of the data
involved, the development of privacy-preserving mechanisms for median consensus
remains underexplored. In this work, we present the first rigorous analysis of
privacy in distributed median consensus, focusing on an $L_1$-norm minimization
framework. We establish necessary and sufficient conditions under which exact
consensus and perfect privacy-defined as zero information leakage-can be
achieved simultaneously. Our information-theoretic analysis provides provable
guarantees against passive and eavesdropping adversaries, ensuring that private
data remain concealed. Extensive numerical experiments validate our theoretical
results, demonstrating the practical feasibility of achieving both accuracy and
privacy in distributed median consensus.; 75) Tell me about yourself: LLMs are aware of their learned behaviors; We study behavioral self-awareness -- an LLM's ability to articulate its
behaviors without requiring in-context examples. We finetune LLMs on datasets
that exhibit particular behaviors, such as (a) making high-risk economic
decisions, and (b) outputting insecure code. Despite the datasets containing no
explicit descriptions of the associated behavior, the finetuned LLMs can
explicitly describe it. For example, a model trained to output insecure code
says, ``The code I write is insecure.'' Indeed, models show behavioral
self-awareness for a range of behaviors and for diverse evaluations. Note that
while we finetune models to exhibit behaviors like writing insecure code, we do
not finetune them to articulate their own behaviors -- models do this without
any special training or examples.
  Behavioral self-awareness is relevant for AI safety, as models could use it
to proactively disclose problematic behaviors. In particular, we study backdoor
policies, where models exhibit unexpected behaviors only under certain trigger
conditions. We find that models can sometimes identify whether or not they have
a backdoor, even without its trigger being present. However, models are not
able to directly output their trigger by default.
  Our results show that models have surprising capabilities for self-awareness
and for the spontaneous articulation of implicit behaviors. Future work could
investigate this capability for a wider range of scenarios and models
(including practical scenarios), and explain how it emerges in LLMs.; 76) MOHPER: Multi-objective Hyperparameter Optimization Framework for
  E-commerce Retrieval System; E-commerce search optimization has evolved to include a wider range of
metrics that reflect user engagement and business objectives. Modern search
frameworks now incorporate advanced quality features, such as sales counts and
document-query relevance, to better align search results with these goals.
Traditional methods typically focus on click-through rate (CTR) as a measure of
engagement or relevance, but this can miss true purchase intent, creating a gap
between user interest and actual conversions. Joint training with the
click-through conversion rate (CTCVR) has become essential for understanding
buying behavior, although its sparsity poses challenges for reliable
optimization. This study presents MOHPER, a Multi-Objective Hyperparameter
Optimization framework for E-commerce Retrieval systems. Utilizing Bayesian
optimization and sampling, it jointly optimizes both CTR, CTCVR, and relevant
objectives, focusing on engagement and conversion of the users. In addition, to
improve the selection of the best configuration from multi-objective
optimization, we suggest advanced methods for hyperparameter selection,
including a meta-configuration voting strategy and a cumulative training
approach that leverages prior optimal configurations, to improve speeds of
training and efficiency. Currently deployed in a live setting, our proposed
framework substantiates its practical efficacy in achieving a balanced
optimization that aligns with both user satisfaction and revenue goals.; 77) SBAMDT: Bayesian Additive Decision Trees with Adaptive Soft
  Semi-multivariate Split Rules; Bayesian Additive Regression Trees [BART, Chipman et al., 2010] have gained
significant popularity due to their remarkable predictive performance and
ability to quantify uncertainty. However, standard decision tree models rely on
recursive data splits at each decision node, using deterministic decision rules
based on a single univariate feature. This approach limits their ability to
effectively capture complex decision boundaries, particularly in scenarios
involving multiple features, such as spatial domains, or when transitions are
either sharp or smoothly varying. In this paper, we introduce a novel
probabilistic additive decision tree model that employs a soft split rule. This
method enables highly flexible splits that leverage both univariate and
multivariate features, while also respecting the geometric properties of the
feature domain. Notably, the probabilistic split rule adapts dynamically across
decision nodes, allowing the model to account for varying levels of smoothness
in the regression function. We demonstrate the utility of the proposed model
through comparisons with existing tree-based models on synthetic datasets and a
New York City education dataset.; 78) Learning Bayesian Game Families, with Application to Mechanism Design; Learning or estimating game models from data typically entails inducing
separate models for each setting, even if the games are parametrically related.
In empirical mechanism design, for example, this approach requires learning a
new game model for each candidate setting of the mechanism parameter. Recent
work has shown the data efficiency benefits of learning a single parameterized
model for families of related games. In Bayesian games - a typical model for
mechanism design - payoffs depend on both the actions and types of the players.
We show how to exploit this structure by learning an interim game-family model
that conditions on a single player's type. We compare this approach to the
baseline approach of directly learning the ex ante payoff function, which gives
payoffs in expectation of all player types. By marginalizing over player type,
the interim model can also provide ex ante payoff predictions. This dual
capability not only facilitates Bayes-Nash equilibrium approximation, but also
enables new types of analysis using the conditional model. We validate our
method through a case study of a dynamic sponsored search auction. In our
experiments, the interim model more reliably approximates equilibria than the
ex ante model and exhibits effective parameter extrapolation. With local search
over the parameter space, the learned game-family model can be used for
mechanism design. Finally, without any additional sample data, we leverage the
interim model to compute piecewise best-response strategies and refine our
model to incorporate these strategies, enabling an iterative approach to
empirical mechanism design.; 79) A higher algebraic approach to liftings of modules over derived
  quotients; We show a certain existence of a lifting of modules under the
self-$\mathrm{Ext}^2$-vanishing condition over the ""derived quotient"" by using
the notion of higher algebra. This refines a work of Auslander--Ding--Solberg's
solution of the Auslander--Reiten conjecture for complete interesctions.
Together with Auslander's zero-divisor theorem, we show that the existence of
such $\mathrm{Ext}$-vanishing module over derived quotients is equivalent to
being local complete intersections.; 80) Structure of gaps induced by retrograde satellites embedded in accretion
  discs; Using 2D simulations, we investigate how a non-accreting satellite on a fixed
retrograde circular orbit affects the structure of the accretion disc in which
it is embedded. We vary the satellite-to-primary mass ratio $q$, the disc
viscosity $\nu$, and the inner boundary conditions. A viscous criterion for gap
opening is derived, which is broadly consistent with the simulations. We find a
scaling relation of the gap depth with $q$ and $\nu$. Unlike the prograde case,
the satellite is located at the gap's inner edge, resulting in a surface
density at the satellite's orbital radius up to $20$ times higher than at the
gap's minimum. As the viscosity decreases, the gap depth increases, while the
radial shift of the gap and the satellite's orbital radius decreases.
Gap-opening satellites may drive radial motions in the disc, producing
eccentric gaps. Positioned at the gap edge, satellites experience a rapidly
fluctuating environment. Migrating satellites can develop orbital
eccentricities comparable to the disc's aspect ratio. In a 3D simulation with
$q=0.01$, the flow velocity exhibits a notorious vertical component in the
gap's inner edge. A comparison between 2D and 3D simulations reveals a slight
radial offset in gap position, resulting in a lower surface density at the
perturber's orbital radius in the 3D simulation.; 81) Integration of Machine Learning-Based Plasma Acceleration Simulations
  into Geant4: A Case Study with the PALLAS Experiment; We present the development and integration of a Machine Learning (ML)-based
surrogate model, trained on Particle-In-Cell (PIC) simulations of laser-driven
plasma wakefield acceleration source of electrons, into Geant4 simulation
toolkit. Our model enables the generation and tracking of plasma-accelerated
beams within complete experimental setups, unifying plasma acceleration and
Monte Carlo-based simulations, which significantly reduces their complexity and
computational cost.
  Our implementation focuses on the PALLAS laser-plasma accelerator test
facility, integrating its full experimental setup into Geant4. We describe the
ML model, its integration into Geant4, and key simulation results,
demonstrating the feasibility of start-to-end simulations of plasma
acceleration facilities and applications within a unified framework.; 82) A remarkable Ruby: Absorption in dense gas, rather than evolved stars,
  drives the extreme Balmer break of a Little Red Dot at $z=3.5$; The origin of the rest-optical emission of compact, red, high-redshift
sources known as `little red dots' (LRDs) poses a major puzzle. If interpreted
as starlight, it would imply that LRDs would constitute the densest stellar
systems in the Universe. However, alternative models suggest active galactic
nuclei (AGN) may instead power the rest-optical continuum. Here, we present
JWST/NIRSpec, NIRCam and MIRI observations from the RUBIES and PRIMER programs
of The Cliff: a bright LRD at $z=3.55$ with an exceptional Balmer break, twice
as strong as that of any high-redshift source previously observed. The spectra
also reveal broad Hydrogen (H$\alpha\ \rm FWHM\sim1500$km/s) and He I emission,
but no significant metal lines. We demonstrate that massive evolved stellar
populations cannot explain the observed spectrum, even when considering
unusually steep and strong dust attenuation, or reasonable variations in the
initial mass function. Moreover, the formally best-fit stellar mass and compact
size ($M_*\sim10^{10.5}\,M_\odot,\ r_{e}\sim40\,$pc) would imply densities at
which near-monthly stellar collisions might lead to significant X-ray emission.
We argue that the Balmer break, emission lines, and H$\alpha$ absorption line
are instead most plausibly explained by a `black hole star' (BH*) scenario, in
which dense gas surrounds a powerful ionising source. In contrast to recently
proposed BH* models of dust-reddened AGN, we show that spectral fits in the
rest UV to near-infrared favour an intrinsically redder continuum over strong
dust reddening. This may point to a super-Eddington accreting massive black
hole or, possibly, the presence of (super)massive stars in a nuclear star
cluster. The Cliff is the clearest evidence to date that at least some LRDs are
not ultra-dense, massive galaxies, and are instead powered by a central
ionising source embedded in dense, absorbing gas.; 83) Optimal Follow-Up of Gravitational-Wave Events with the UltraViolet
  EXplorer (UVEX); The UltraViolet EXplorer (UVEX) is a wide-field ultraviolet space telescope
selected as a NASA Medium-Class Explorer (MIDEX) mission for launch in 2030.
UVEX will undertake deep, cadenced surveys of the entire sky to probe low mass
galaxies and explore the ultraviolet (UV) time-domain sky, and it will carry
the first rapidly deployable UV spectroscopic capability for a broad range of
science applications. One of UVEX's prime objectives is to follow up
gravitational wave (GW) binary neutron star mergers as targets of opportunity
(ToOs), rapidly scanning across their localization regions to search for their
kilonova (KN) counterparts. Early-time multiband ultraviolet light curves of
KNe are key to explaining the interplay between jet and ejecta in binary
neutron star mergers. Owing to high Galactic extinction in the ultraviolet and
the variation of GW distance estimates over the sky, the sensitivity to
kilonovae can vary significantly across the GW localization and even across the
footprint of a single image given UVEX's large field of view. Good ToO
observing strategies to trade off between area and depth are neither simple nor
obvious. We present an optimal strategy for GW follow-up with UVEX in which
exposure time is adjusted dynamically for each field individually to maximize
the overall probability of detection. We model the scheduling problem using the
expressive and powerful mathematical framework of mixed integer linear
programming (MILP), and employ a state-of-the-art MILP solver to automatically
generate observing plan timelines that achieve high probabilities of kilonova
detection. We have implemented this strategy in an open-source astronomical
scheduling software package called the Multi-Mission Multi-Messenger
Observation Planning Toolkit (M4OPT), on GitHub at
https://github.com/m4opt/m4opt.; 84) STAF: Sinusoidal Trainable Activation Functions for Implicit Neural
  Representation; Implicit Neural Representations (INRs) have emerged as a powerful framework
for modeling continuous signals. The spectral bias of ReLU-based networks is a
well-established limitation, restricting their ability to capture fine-grained
details in target signals. While previous works have attempted to mitigate this
issue through frequency-based encodings or architectural modifications, these
approaches often introduce additional complexity and do not fully address the
underlying challenge of learning high-frequency components efficiently. We
introduce Sinusoidal Trainable Activation Functions (STAF), designed to
directly tackle this limitation by enabling networks to adaptively learn and
represent complex signals with higher precision and efficiency. STAF inherently
modulates its frequency components, allowing for self-adaptive spectral
learning. This capability significantly improves convergence speed and
expressivity, making STAF highly effective for both signal representations and
inverse problems. Through extensive evaluations, we demonstrate that STAF
outperforms state-of-the-art (SOTA) methods in accuracy and reconstruction
fidelity with superior Peak Signal-to-Noise Ratio (PSNR). These results
establish STAF as a robust solution for overcoming spectral bias and the
capacity-convergence gap, making it valuable for computer graphics and related
fields. Our codebase is publicly accessible on the
https://github.com/AlirezaMorsali/STAF.; 85) Influence of dark matter on the structure of strange quark stars in
  one-fluid model; This work studies the influence of scalar dark matter on the structural
properties of strange quark stars (SQS) within a one-fluid framework,
considering Yukawa interactions between dark matter and quark matter.
Contributions from perturbative QCD, Yukawa interaction between scalar dark
matter and quarks, and Bose-Einstein condensation of dark matter are included
in the model. We first determine the allowable range of Yukawa interaction
coupling by imposing the stability condition for strange quark matter (SQM).
Using this range, we derive the equation of state (EOS) for different fractions
of dark matter within the total pressure of SQS. These fractions are
constrained by the tidal deformability limit from GW170817. The presence of
dark matter alters the EOS, leading to changes in the mass-radius relationship,
tidal deformability, and stability of SQS. We demonstrate that increasing the
mass of dark matter softens the EOS, whereas higher fractions of dark matter
lead to stiffer EOSs. We also explore the reasons behind this behavior. Our
EOSs not only describe massive objects, such as PSR J0952-0607 and PSR
J2215+5135, but also satisfy the tidal deformability constraint from GW170817.
These results reveal that incorporating dark matter modifies the EOS, enabling
the support of higher stellar masses while maintaining consistency with
observational data.; 86) Early Detection and Classification of Breast Cancer Using Deep Learning
  Techniques; Breast cancer is one of the deadliest cancers causing about massive number of
patients to die annually all over the world according to the WHO. It is a kind
of cancer that develops when the tissues of the breast grow rapidly and
unboundly. This fatality rate can be prevented if the cancer is detected before
it gets malignant. Using automation for early-age detection of breast cancer,
Artificial Intelligence and Machine Learning technologies can be implemented
for the best outcome. In this study, we are using the Breast Cancer Image
Classification dataset collected from the Kaggle depository, which comprises
9248 Breast Ultrasound Images and is classified into three categories: Benign,
Malignant, and Normal which refers to non-cancerous, cancerous, and normal
images.This research introduces three pretrained model featuring custom
classifiers that includes ResNet50, MobileNet, and VGG16, along with a custom
CNN model utilizing the ReLU activation function.The models ResNet50,
MobileNet, VGG16, and a custom CNN recorded accuracies of 98.41%, 97.91%,
98.19%, and 92.94% on the dataset, correspondingly, with ResNet50 achieving the
highest accuracy of 98.41%.This model, with its deep and powerful architecture,
is particularly successful in detecting aberrant cells as well as cancerous or
non-cancerous tumors. These accuracies show that the Machine Learning methods
are more compatible for the classification and early detection of breast
cancer.; 87) Numerical computation of the Schwarz function; An analytic function can be continued across an analytic arc $\Gamma$ with
the help of the Schwarz function $S(z)$, the analytic function satisfying $S(z)
= \bar z$ for $z\in \Gamma$. We show how $S(z)$ can be computed with the AAA
algorithm of rational approximation, an operation that is the basis of the
AAALS method for solution of Laplace and related PDE problems in the plane. We
discuss the challenge of computing $S(z)$ further away from from $\Gamma$,
where it becomes multi-valued.; 88) On the Importance of Clearsky Model in Short-Term Solar Radiation
  Forecasting; Clearsky models are widely used in solar energy for many applications such as
quality control, resource assessment, satellite-base irradiance estimation and
forecasting. However, their use in forecasting and nowcasting is associated
with a number of challenges. Synchronization errors, reliance on the Clearsky
index (ratio of the global horizontal irradiance to its cloud-free counterpart)
and high sensitivity of the clearsky model to errors in aerosol optical depth
at low solar elevation limit their added value in real-time applications. This
paper explores the feasibility of short-term forecasting without relying on a
clearsky model. We propose a Clearsky-Free forecasting approach using Extreme
Learning Machine (ELM) models. ELM learns daily periodicity and local
variability directly from raw Global Horizontal Irradiance (GHI) data. It
eliminates the need for Clearsky normalization, simplifying the forecasting
process and improving scalability. Our approach is a non-linear adaptative
statistical method that implicitely learns the irradiance in cloud-free
conditions removing the need for an clear-sky model and the related operational
issues. Deterministic and probabilistic results are compared to traditional
benchmarks, including ARMA with McClear-generated Clearsky data and quantile
regression for probabilistic forecasts. ELM matches or outperforms these
methods, providing accurate predictions and robust uncertainty quantification.
This approach offers a simple, efficient solution for real-time solar
forecasting. By overcoming the stationarization process limitations based on
usual multiplicative scheme Clearsky models, it provides a flexible and
reliable framework for modern energy systems.; 89) Quantifying the Impact of the Dust Torque on the Migration of Low-mass
  Planets II: The Role of Pebble Accretion in Planet Growth within a Global
  Planet Formation Model; Although dust constitutes only about 1% of the mass of a protoplanetary disk,
recent studies demonstrate that it can exert a significant torque on low- and
intermediate-mass planetary cores. We compute and quantify for the first time
the influence of the dust torque on the evolution of growing planetary embryos
as they move in a protoplanetary disk while growing via gas and pebble
accretion. Our global model evolves the gaseous disk via viscous accretion and
X-ray photoevaporation, while accounting for dust growth and evolution
including coagulation, drift, and fragmentation. Our research indicates that
dust torque significantly influences planetary migration, particularly driving
substantial outward migration for planets forming within the water ice-line.
This effect occurs due to an increased dust-to-gas mass ratio in the inner
disk, resulting from inward pebble drift from outer regions. In contrast, for
planets initially located beyond the water ice-line, the dust torque mitigates
inward migration but does not significantly alter their paths, as the
dust-to-gas ratio diminishes rapidly due to rapid pebble drift and the brief
timescales of planet formation in these areas. These findings underscore the
pivotal role of dust torque in shaping the migration patterns of low- and
intermediate-mass planets, especially when enhanced dust concentrations in the
inner disk amplify its effects; 90) A possible overall scenario for the outburst evolution of MAXI J1820+070
  revealed by Insight-HXMT; We study the spectral and temporal properties of the black hole X-ray
transient binary MAXI J1820+070 during the 2018 outburst with Insight-HXMT
observations. The outburst of MAXI J1820+070 can be divided into three
intervals. For the two intervals of the outburst, we find that low-energy
(below 140 keV) photos lag high-energy (140-170 keV) ones, while in the decay
of the outburst, high-energy photons lag low-energy photons, both with a time
scale of the order of days. Based on these results, the canonical hysteresis
effect of the 'q' shape in the hardness-intensity diagram can be reformed into
a roughly linear shape by taking into account the lag corrections between
different energy bands. Time analysis shows that the high-frequency break of
hard X-rays, derived from the power density spectrum of the first interval of
the outburst is, in general, larger and more variable than that of soft X-rays.
The spectral fitting shows that the coverage fraction of the hard X-rays drops
sharply at the beginning of the outburst to around 0.5, then increases
slightly. The coverage fraction drops to roughly zero once the source steps
into a soft state and increases gradually to unity when the source returns to a
low hard state. We discuss the possible overall evolution scenario of corona
hinted from these discoveries.; 91) Connectivity of Coxeter group Morse boundaries; We study the connectivity of Morse boundaries of Coxeter groups. We define
two conditions on the defining graph of a Coxeter group: wide-avoidant and
wide-spherical-avoidant. We show that wide-spherical-avoidant, one-ended,
affine-free Coxeter groups have connected and locally connected Morse
boundaries. On the other hand, one-ended Coxeter groups that are not
wide-avoidant and not wide have disconnected Morse boundary. For the
right-angled case, we get a full characterization: a one-ended right-angled
Coxeter group has connected, non-empty Morse boundary if and only if it is
wide-avoidant. Along the way we characterize Morse geodesic rays in affine-free
Coxeter groups as those that spend uniformly bounded time in cosets of wide
special subgroups.; 92) A twisted derived category of hyper-K\""ahler varieties of
  $K3^{[n]}$-type; We conjecture that a natural twisted derived category of any hyper-K\""ahler
variety of $K3^{[n]}$-type is controlled by its Markman-Mukai lattice. We prove
the conjecture under numerical constraints, and our proof relies heavily on
Markman's projectively hyperholomorphic bundle and a recently proven twisted
version of the D-equivalence conjecture.
  In particular, we prove that any two fine moduli spaces of stable sheaves on
a $K3$ surface are derived equivalent if they have the same dimension.; 93) AI Agents in Cryptoland: Practical Attacks and No Silver Bullet; The integration of AI agents with Web3 ecosystems harnesses their
complementary potential for autonomy and openness, yet also introduces
underexplored security risks, as these agents dynamically interact with
financial protocols and immutable smart contracts. This paper investigates the
vulnerabilities of AI agents within blockchain-based financial ecosystems when
exposed to adversarial threats in real-world scenarios. We introduce the
concept of context manipulation -- a comprehensive attack vector that exploits
unprotected context surfaces, including input channels, memory modules, and
external data feeds. Through empirical analysis of ElizaOS, a decentralized AI
agent framework for automated Web3 operations, we demonstrate how adversaries
can manipulate context by injecting malicious instructions into prompts or
historical interaction records, leading to unintended asset transfers and
protocol violations which could be financially devastating. Our findings
indicate that prompt-based defenses are insufficient, as malicious inputs can
corrupt an agent's stored context, creating cascading vulnerabilities across
interactions and platforms. This research highlights the urgent need to develop
AI agents that are both secure and fiduciarily responsible.; 94) Reply to Pavi\v{c}i\'c's 'Comment on ""Optimal conversion of
  Kochen-Specker sets into bipartite perfect quantum strategies""'
  (arXiv:2502.13787); According to Pavi\v{c}i{\'c}, Kochen and Specker's 117-observable set is not
a ``Kochen-Specker set''. By the same reason, in arXiv:2502.13787,
Pavi\v{c}i{\'c} claims that 10 statements in our paper ``Optimal conversion of
Kochen-Specker sets into bipartite perfect quantum strategies'' [Phys. Rev. A
111, 022408 (2025)] are ``wrong''. In all cases, Pavi\v{c}i{\'c}'s claims are
based on the fact that he is assuming a different definition of Kochen-Specker
(KS) set. Adopting a terminology used by, e.g., Larsson, the sets that
Pavi\v{c}i{\'c} call KS sets can be called ``extended'' KS sets, since they are
constructed by adding observables to the ``original'' KS sets. For example,
Pavi\v{c}i{\'c} adds 75 observables to the original 117-observable KS set.
Beyond terminology, there are fundamental physical reasons for focusing on the
original KS sets. One reason is that, for experimentally observing quantum
state-independent contextuality, there is no need to measure the observables
added in the extended sets. Another reason is that, to produce bipartite
perfect quantum strategies, or correlations in a face of the nonsignaling
polytope with no local points, or correlations with nonlocal content 1, the two
parties do not need to measure any of the observables added in the extended
sets. We also respond to other claims made by Pavi\v{c}i{\'c} about our work.; 95) Chow polynomials of uniform matroids are real-rooted; June Huh and Matthew Stevens conjectured that the Hilbert-Poincar\'e series
of the Chow ring of any matroid is a polynomial with only real zeros. We prove
this conjecture for the class of uniform matroids. We also prove that the Chow
polynomial and the augmented Chow polynomial of any maximally ranked poset has
only real zeros.; 96) Solving bihomogeneous polynomial systems with a zero-dimensional
  projection; We study bihomogeneous systems defining, non-zero dimensional, biprojective
varieties for which the projection onto the first group of variables results in
a finite set of points. To compute (with) the 0-dimensional projection and the
corresponding quotient ring, we introduce linear maps that greatly extend the
classical multiplication maps for zero-dimensional systems, but are not those
associated to the elimination ideal; we also call them multiplication maps. We
construct them using linear algebra on the restriction of the ideal to a
carefully chosen bidegree or, if available, from an arbitrary Gr\""obner bases.
The multiplication maps allow us to compute the elimination ideal of the
projection, by generalizing FGLM algorithm to bihomogenous, non-zero
dimensional, varieties. We also study their properties, like their minimal
polynomials and the multiplicities of their eigenvalues, and show that we can
use the eigenvalues to compute numerical approximations of the zero-dimensional
projection. Finally, we establish a single exponential complexity bound for
computing multiplication maps and Gr\""obner bases, that we express in terms of
the bidegrees of the generators of the corresponding bihomogeneous ideal.; 97) Exact Simulation of Longitudinal Data from Marginal Structural Models; Simulating longitudinal data from specified marginal structural models is a
crucial but challenging task for evaluating causal inference methods and
clinical trial design. While data generation typically proceeds in a fully
conditional manner using structural equations according to a temporal ordering,
it is difficult to ensure alignment between conditional distributions and the
target marginal causal effects. This misalignment presents a fundamental
challenge in simulating data that adheres to marginal structural model
specifications. To address this, we propose a flexible and efficient algorithm
for simulating longitudinal data that adheres exactly to a specified marginal
structural model. Recognizing the importance of time-to-event outcomes in
clinical research, we extend our approach to accommodate survival models.
Compared to existing approaches, our method offers several advantages: it
enables exact simulation from a known causal model rather than relying on
approximations; avoids restrictive assumptions about the data-generating
process; and remains computationally efficient by requiring only the evaluation
of analytic functions. This last benefit contrasts with methods that use
computationally intensive techniques such as Monte Carlo approximations or
numerical integration. Through simulation studies replicating realistic
scenarios, we validate the method's accuracy and utility. Our method will
facilitate researchers in effectively simulating data with target causal
structures for their specific scenarios.; 98) Physiome-ODE: A Benchmark for Irregularly Sampled Multivariate Time
  Series Forecasting Based on Biological ODEs; State-of-the-art methods for forecasting irregularly sampled time series with
missing values predominantly rely on just four datasets and a few small toy
examples for evaluation. While ordinary differential equations (ODE) are the
prevalent models in science and engineering, a baseline model that forecasts a
constant value outperforms ODE-based models from the last five years on three
of these existing datasets. This unintuitive finding hampers further research
on ODE-based models, a more plausible model family. In this paper, we develop a
methodology to generate irregularly sampled multivariate time series (IMTS)
datasets from ordinary differential equations and to select challenging
instances via rejection sampling. Using this methodology, we create
Physiome-ODE, a large and sophisticated benchmark of IMTS datasets consisting
of 50 individual datasets, derived from real-world ordinary differential
equations from research in biology. Physiome-ODE is the first benchmark for
IMTS forecasting that we are aware of and an order of magnitude larger than the
current evaluation setting of four datasets. Using our benchmark Physiome-ODE,
we show qualitatively completely different results than those derived from the
current four datasets: on Physiome-ODE ODE-based models can play to their
strength and our benchmark can differentiate in a meaningful way between
different IMTS forecasting models. This way, we expect to give a new impulse to
research on ODE-based time series modeling.; 99) Another Mar\v{c}enko-Pastur law for Kendall's tau; Bandeira et al. (2017) show that the eigenvalues of the Kendall correlation
matrix of $n$ i.i.d. random vectors in $\mathbb{R}^p$ are asymptotically
distributed like $1/3 + (2/3)Y_q$, where $Y_q$ has a Mar\v{c}enko-Pastur law
with parameter $q=\lim(p/n)$ if $p, n\to\infty$ proportionately to one another.
Here we show that another Mar\v{c}enko-Pastur law emerges in the ""ultra-high
dimensional"" scaling limit where $p\sim q'\, n^2/2$ for some $q'>0$: in this
quadratic scaling regime, Kendall correlation eigenvalues converge weakly
almost surely to $(1/3)Y_{q'}$.; 100) Quasinormal Modes of Black Holes: Efficient and Highly Accurate
  Calculations with Recurrence-Based Methods; We discuss new recurrence-based methods for calculating the complex
frequencies of the quasinormal modes of black holes. These methods are based on
the Frobenius series solutions of the differential equation describing the
linearized radial perturbations. Within the general method, we propose two
approaches: the first involves calculating the series coefficients, while the
second employs generalized continued fractions. Moreover, as a consequence of
this analysis, we present a computationally efficient and convenient method
that uses double convergence acceleration, consisting of the application of the
Wynn algorithm to the approximants obtained from the Hill determinants, with
the Leaver-Nollert-Zhidenko-like tail approximations taken into account. The
latter is particularly important for stabilizing and enabling the calculations
of modes with small real parts as well as higher overtones. The method
demonstrates exceptionally high accuracy. We emphasize that Gaussian
elimination is unnecessary in all of these calculations. We consider
$D$-dimensional ($3<D<10$) Schwarzschild-Tangherlini black holes as concrete
examples. Specifically, we calculate the quasinormal modes of the
$(2+1)$-dimensional acoustic black hole (which is closely related to the
five-dimensional Schwarzschild-Tangherlini black holes), the
electromagnetic-vector modes of the six-dimensional black holes and the scalar
(gravitational tensor) modes in the seven-dimensional case. We believe that the
methods presented here are applicable beyond the examples shown, also outside
the domain of the black hole physics.",0.0,0.3562071871080222
2411.00561,applied,2411.00561-pos1-8,"What is a cell type, really? The quest to categorize life’s myriad forms; The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. “I realized that there were probably hundreds of flavours,” recalls Buenrostro, who was a graduate student at Stanford University in California at the time.",2411.00561-pos2-8,"Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study; In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric.",4,"['1', '3', '10', '13', '5', '29', '40', '92', '62', '76']","The main paper discusses the complexities of categorizing cell types based on genetic variations within cancer cells, indicating a need for a multidisciplinary approach that combines biological understanding with advanced computational methods. The best candidate paper explores automated trading systems using deep reinforcement learning, which involves sophisticated data handling and decision-making models. This combination suggests a novel approach to bioinformatics where machine learning techniques could be applied to categorize cell types based on genomic data, potentially transforming our understanding of cellular behavior in cancer. The subsequent candidates include topics like trajectory optimization (3) which can be applicable in modeling biological processes, while others focus on machine learning techniques (10, 13) that further enhance the data-driven analysis capabilities needed for such biological classifications. Papers that delve into understanding complex structures through advanced data techniques or machine learning applications appear to align well with the multidisciplinary objectives of the main paper, leading to the proposed list.","1) A Deep Reinforcement Learning Approach to Automated Stock Trading, using
  xLSTM Networks; Traditional Long Short-Term Memory (LSTM) networks are effective for handling
sequential data but have limitations such as gradient vanishing and difficulty
in capturing long-term dependencies, which can impact their performance in
dynamic and risky environments like stock trading. To address these
limitations, this study explores the usage of the newly introduced Extended
Long Short Term Memory (xLSTM) network in combination with a deep reinforcement
learning (DRL) approach for automated stock trading. Our proposed method
utilizes xLSTM networks in both actor and critic components, enabling effective
handling of time series data and dynamic market environments. Proximal Policy
Optimization (PPO), with its ability to balance exploration and exploitation,
is employed to optimize the trading strategy. Experiments were conducted using
financial data from major tech companies over a comprehensive timeline,
demonstrating that the xLSTM-based model outperforms LSTM-based methods in key
trading evaluation metrics, including cumulative return, average profitability
per trade, maximum earning rate, maximum pullback, and Sharpe ratio. These
findings mark the potential of xLSTM for enhancing DRL-based stock trading
systems.; 2) Spindown of massive main sequence stars in the Milky Way; Context. We need to understand the spin evolution of massive stars to compute
their internal rotationally induced mixing processes, isolate effects of close
binary evolution, and predict the rotation rates of white dwarfs, neutron stars
and black holes.
  Aims. We discuss the spindown of massive main sequence stars imposed by
stellar winds.
  Methods. We use detailed grids of single star evolutionary models to predict
the distribution of the surface rotational velocities of core-hydrogen burning
Galactic massive stars as function of their mass and evolutionary state. We
then compare the spin properties of our synthetic populations with
appropriately selected sub-samples of Galactic main sequence OB-type stars
extracted from the IACOB survey.
  Results. We find that below $\sim 40 M_\odot$, observations and models agree
in finding that the surface rotational velocities of Galactic massive stars
remain relatively constant during their main sequence evolution. The more
massive stars in the IACOB sample appear to spin down less than predicted,
while our updated angular momentum loss prescription predicts an enhanced
spindown. Furthermore, the observations show a population of fast rotators,
with $v \sin I \gtrsim 200$ km/s persisting for all ages, which is not
reproduced by our synthetic single star populations.
  Conclusions. We conclude that the wind-induced spindown of massive main
sequence stars is yet to be fully understood, and that close binary evolution
might significantly contribute to the fraction of rapid rotators in massive
stars.; 3) Chance constraints transcription and failure risk estimation for
  stochastic trajectory optimization; Space exploration has advanced significantly, with missions increasingly
using complex dynamical systems. Optimal trajectory design is crucial,
involving the minimization of objective functions while ensuring robustness
against measurement and control errors. Recent research has focused on
stochastic solvers that address uncertainties through chance constraints, which
are relaxed hard constraints allowing for a given failure risk. This study
introduces three novel, general, multidimensional transcription methods for
chance constraints: the spectral radius, first-order, and d-th order methods.
Additionally, we introduce failure risk estimation techniques and a
conservatism metric to enable comprehensive comparison with existing
approaches. Applications to aerospace test cases demonstrate the effectiveness
of the proposed transcriptions, highlighting that state-of-the-art methods
significantly overestimate risk. Notably, the d-th order transcription
dramatically outperforms the other methods, particularly in high-dimensional
scenarios. This work shows that spectral radius-based methods are overly
conservative and computationally intensive, while the first-order and d-th
order methods offer practical and efficient alternatives.; 4) Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study; In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric.; 5) Is User Perception the Key to Unlocking the Full Potential of Business
  Process Management Systems (BPMS)? Enhancing BPMS Efficacy Through User
  Perception; This study investigates factors influencing employees' perceptions of the
usefulness of Business Process Management Systems (BPMS) in commercial
settings. It explores the roles of system dependency, system quality, and the
quality of information and knowledge in the adoption and use of BPMS. Data were
collected using a structured questionnaire from end-users in various firms and
analyzed with Partial Least Squares (PLS). The survey evaluated perceptions of
service quality, input quality, system attributes, and overall system quality.
The findings indicate that service quality, input quality, and specific system
attributes significantly influence perceived system quality, while system
dependency and information quality are predictors of perceived usefulness. The
results highlight the importance of user training, support, and high-quality
information in enhancing satisfaction and BPMS. This research offers empirical
evidence on the factors impacting user perceptions and acceptance, emphasizing
the need for user-centric approaches in BPMS.; 6) Image Segmentation: Inducing graph-based learning; This study explores the potential of graph neural networks (GNNs) to enhance
semantic segmentation across diverse image modalities. We evaluate the
effectiveness of a novel GNN-based U-Net architecture on three distinct
datasets: PascalVOC, a standard benchmark for natural image segmentation,
WoodScape, a challenging dataset of fisheye images commonly used in autonomous
driving, introducing significant geometric distortions; and ISIC2016, a dataset
of dermoscopic images for skin lesion segmentation. We compare our proposed
UNet-GNN model against established convolutional neural networks (CNNs) based
segmentation models, including U-Net and U-Net++, as well as the
transformer-based SwinUNet. Unlike these methods, which primarily rely on local
convolutional operations or global self-attention, GNNs explicitly model
relationships between image regions by constructing and operating on a graph
representation of the image features. This approach allows the model to capture
long-range dependencies and complex spatial relationships, which we hypothesize
will be particularly beneficial for handling geometric distortions present in
fisheye imagery and capturing intricate boundaries in medical images. Our
analysis demonstrates the versatility of GNNs in addressing diverse
segmentation challenges and highlights their potential to improve segmentation
accuracy in various applications, including autonomous driving and medical
image analysis.; 7) Quasinormal Modes, Grebody Factors, and Hawking Radiation Sparsity of
  Black Holes Influenced by a Global Monopole Charge in Kalb-Ramond Gravity; Kalb-Ramond (KR) gravity is an intriguing model incorporating local Lorentz
violation, and black hole (BH) solutions are known to exist. In this study, we
investigate some crucial aspects of BHs endowed with a global monopole charge
in the self-interacting KR field. Specifically, we study the quasinormal modes
(QNMs) corresponding to scalar, electromagnetic, and gravitational
perturbations; derive rigorous bounds for the greybody factors (GBFs); and
examine the sparsity of Hawking radiation. The effects of the model parameters
$\ell$ (Lorentz-violating parameter in KR gravity) and $\eta$ (monopole charge)
on these phenomena are elaborated. First, QNMs are evaluated with high
precision using the 13\textsuperscript{th}-order Pad\'{e}-averaged WKB method
and cross-examined via time-domain analyses within an acceptable parameter
space. The results show that the estimated QNMs are more sensitive to $\ell$;
however, both model parameters influence the frequency spectra. The derived
bounds on the GBFs aid in further constraining the parameter space. It is shown
that $\ell$ and $\eta$ have a similar effect on the greybody bounds.
Furthermore, positive and negative values of $\ell$ have opposing effects in
that the bounds are reversed for the two cases. The analyses of the Hawking
radiation sparsity highlight the effect of $\ell$, and two scenarios are noted:
either the radiation emitted is less sparse than Hawking radiation, or it is
more sparse during the evaporation phase. Thus, this work presents a
comprehensive account of BHs in KR gravity with a global monopole charge.; 8) Stochastic multisymplectic PDEs and their structure-preserving numerical
  methods; We construct stochastic multisymplectic systems by considering a stochastic
extension to the variational formulation of multisymplectic partial
differential equations proposed in [Hydon, {\it Proc. R. Soc. A}, 461,
1627--1637, 2005]. The stochastic variational principle implies the existence
of stochastic $1$-form and $2$-form conservation laws, as well as conservation
laws arising from continuous variational symmetries via a stochastic Noether's
theorem. These results are the stochastic analogues of those found in
deterministic variational principles. Furthermore, we develop stochastic
structure-preserving collocation methods for this class of stochastic
multisymplectic systems. These integrators possess a discrete analogue of the
stochastic $2$-form conservation law and, in the case of linear systems, also
guarantee discrete momentum conservation. The effectiveness of the proposed
methods is demonstrated through their application to stochastic nonlinear
Schr\""odinger equations featuring either stochastic transport or stochastic
dispersion.; 9) Generalized Radial Uncertainty Product for d-Dimensional Hydrogen Atom; This paper presents a comprehensive analysis of the generalized radial
uncertainty product for the d-dimensional non-relativistic Hydrogen atom in
position space. Utilizing the framework of quantum mechanics in d-dimensional
spherical coordinates, the study extends the standard radial uncertainty
relation to higher dimensions. Taking the solution of the radial Schrodinger
equation, the normalized radial wave functions, expectation values, and
uncertainties in both position and momentum space are rigorously evaluated. The
analytical derivations reveal the dependence of the uncertainty product on the
principal and angular quantum numbers, as well as the dimensional parameter d.
The results provide deeper insight into the role of dimensionality in quantum
uncertainty relations and their implications for higher-dimensional quantum
systems; 10) STAF: Sinusoidal Trainable Activation Functions for Implicit Neural
  Representation; Implicit Neural Representations (INRs) have emerged as a powerful framework
for modeling continuous signals. The spectral bias of ReLU-based networks is a
well-established limitation, restricting their ability to capture fine-grained
details in target signals. While previous works have attempted to mitigate this
issue through frequency-based encodings or architectural modifications, these
approaches often introduce additional complexity and do not fully address the
underlying challenge of learning high-frequency components efficiently. We
introduce Sinusoidal Trainable Activation Functions (STAF), designed to
directly tackle this limitation by enabling networks to adaptively learn and
represent complex signals with higher precision and efficiency. STAF inherently
modulates its frequency components, allowing for self-adaptive spectral
learning. This capability significantly improves convergence speed and
expressivity, making STAF highly effective for both signal representations and
inverse problems. Through extensive evaluations, we demonstrate that STAF
outperforms state-of-the-art (SOTA) methods in accuracy and reconstruction
fidelity with superior Peak Signal-to-Noise Ratio (PSNR). These results
establish STAF as a robust solution for overcoming spectral bias and the
capacity-convergence gap, making it valuable for computer graphics and related
fields. Our codebase is publicly accessible on the
https://github.com/AlirezaMorsali/STAF.; 11) Fokker-Planck to Callan-Symanzik: evolution of weight matrices under
  training; The dynamical evolution of a neural network during training has been an
incredibly fascinating subject of study. First principal derivation of generic
evolution of variables in statistical physics systems has proved useful when
used to describe training dynamics conceptually, which in practice means
numerically solving equations such as Fokker-Planck equation. Simulating entire
networks inevitably runs into the curse of dimensionality. In this paper, we
utilize Fokker-Planck to simulate the probability density evolution of
individual weight matrices in the bottleneck layers of a simple
2-bottleneck-layered auto-encoder and compare the theoretical evolutions
against the empirical ones by examining the output data distributions. We also
derive physically relevant partial differential equations such as
Callan-Symanzik and Kardar-Parisi-Zhang equations from the dynamical equation
we have.; 12) SR-Reward: Taking The Path More Traveled; In this paper, we propose a novel method for learning reward functions
directly from offline demonstrations. Unlike traditional inverse reinforcement
learning (IRL), our approach decouples the reward function from the learner's
policy, eliminating the adversarial interaction typically required between the
two. This results in a more stable and efficient training process. Our reward
function, called \textit{SR-Reward}, leverages successor representation (SR) to
encode a state based on expected future states' visitation under the
demonstration policy and transition dynamics. By utilizing the Bellman
equation, SR-Reward can be learned concurrently with most reinforcement
learning (RL) algorithms without altering the existing training pipeline. We
also introduce a negative sampling strategy to mitigate overestimation errors
by reducing rewards for out-of-distribution data, thereby enhancing robustness.
This strategy inherently introduces a conservative bias into RL algorithms that
employ the learned reward. We evaluate our method on the D4RL benchmark,
achieving competitive results compared to offline RL algorithms with access to
true rewards and imitation learning (IL) techniques like behavioral cloning.
Moreover, our ablation studies on data size and quality reveal the advantages
and limitations of SR-Reward as a proxy for true rewards.; 13) A data-driven two-microphone method for in-situ sound absorption
  measurements; This work presents a data-driven approach to estimating the sound absorption
coefficient of an infinite porous slab using a neural network and a
two-microphone measurement on a finite porous sample. A 1D-convolutional
network predicts the sound absorption coefficient from the complex-valued
transfer function between the sound pressure measured at the two microphone
positions. The network is trained and validated with numerical data generated
by a boundary element model using the Delany-Bazley-Miki model, demonstrating
accurate predictions for various numerical samples. The method is
experimentally validated with baffled rectangular samples of a fibrous
material, where sample size and source height are varied. The results show that
the neural network offers the possibility to reliably predict the in-situ sound
absorption of a porous material using the traditional two-microphone method as
if the sample were infinite. The normal-incidence sound absorption coefficient
obtained by the network compares well with that obtained theoretically and in
an impedance tube. The proposed method has promising perspectives for
estimating the sound absorption coefficient of acoustic materials after
installation and in realistic operational conditions.; 14) An Asymmetric Independence Model for Causal Discovery on Path Spaces; We develop the theory linking 'E-separation' in directed mixed graphs (DMGs)
with conditional independence relations among coordinate processes in
stochastic differential equations (SDEs), where causal relationships are
determined by ""which variables enter the governing equation of which other
variables"". We prove a global Markov property for cyclic SDEs, which naturally
extends to partially observed cyclic SDEs, because our asymmetric independence
model is closed under marginalization. We then characterize the class of graphs
that encode the same set of independence relations, yielding a result analogous
to the seminal 'same skeleton and v-structures' result for directed acyclic
graphs (DAGs). In the fully observed case, we show that each such equivalence
class of graphs has a greatest element as a parsimonious representation and
develop algorithms to identify this greatest element from data. We conjecture
that a greatest element also exists under partial observations, which we verify
computationally for graphs with up to four nodes.; 15) Towards Developing Socially Compliant Automated Vehicles: State of the
  Art, Experts Expectations, and A Conceptual Framework; Automated Vehicles (AVs) hold promise for revolutionizing transportation by
improving road safety, traffic efficiency, and overall mobility. Despite the
steady advancement in high-level AVs in recent years, the transition to full
automation entails a period of mixed traffic, where AVs of varying automation
levels coexist with human-driven vehicles (HDVs). Making AVs socially compliant
and understood by human drivers is expected to improve the safety and
efficiency of mixed traffic. Thus, ensuring AVs compatibility with HDVs and
social acceptance is crucial for their successful and seamless integration into
mixed traffic. However, research in this critical area of developing Socially
Compliant AVs (SCAVs) remains sparse. This study carries out the first
comprehensive scoping review to assess the current state of the art in
developing SCAVs, identifying key concepts, methodological approaches, and
research gaps. An expert interview was also conducted to identify critical
research gaps and expectations towards SCAVs. Based on the scoping review and
expert interview input, a conceptual framework is proposed for the development
of SCAVs. The conceptual framework is evaluated using an online survey
targeting researchers, technicians, policymakers, and other relevant
professionals worldwide. The survey results provide valuable validation and
insights, affirming the significance of the proposed conceptual framework in
tackling the challenges of integrating AVs into mixed-traffic environments.
Additionally, future research perspectives and suggestions are discussed,
contributing to the research and development agenda of SCAVs.; 16) Blocked Bloom Filters with Choices; Probabilistic filters are approximate set membership data structures that
represent a set of keys in small space, and answer set membership queries
without false negative answers, but with a certain allowed false positive
probability. Such filters are widely used in database systems, networks,
storage systems and in biological sequence analysis because of their fast query
times and low space requirements. Starting with Bloom filters in the 1970s,
many filter data structures have been developed, each with its own advantages
and disadvantages, e.g., Blocked Bloom filters, Cuckoo filters, XOR filters,
Ribbon filters, and more.
  We introduce Blocked Bloom filters with choices that work similarly to
Blocked Bloom filters, except that for each key there are two (or more)
alternative choices of blocks where the key's information may be stored. The
result is a filter that partially inherits the advantages of a Blocked Bloom
filter, such as the ability to insert keys rapidly online or the ability to
slightly overload the filter with only a small penalty to the false positive
rate. At the same time, it avoids the major disadvantage of a Blocked Bloom
filter, namely the larger space consumption. Our new data structure uses less
space at the same false positive rate, or has a lower false positive rate at
the same space consumption as a Blocked Bloom filter. We discuss the
methodology, engineered implementation, a detailed performance evaluation and
use cases in bioinformatics of Blocked Bloom filters with choices, showing that
they can be of practical value.
  The implementation of the evaluated filters and the workflows used are
provided via Gitlab at https://gitlab.com/rahmannlab/blowchoc-filters.; 17) Unraveling the Nature of HAWC J1844-034 with Fermi-LAT Data Analysis and
  Multi-wavelength modelling; The extended ultra-high-energy gamma-ray source HAWC J1844-034 is closely
associated with two other sources, HAWC J1843-032 and HWC J1846-025. Moreover,
other gamma-ray observatories like H.E.S.S., LHAASO, and Tibet AS$_{\gamma}$
have detected ultra-high-energy gamma-ray sources whose spatial positions
coincide with the position of HAWC J1844-034. The ultra-high-energy gamma-ray
data from several observatories help analyse the spectral features of this
source in detail at TeV energies. Of the four pulsars near HAWC J1844-034, PSR
J1844-0346 is closest to it and possibly supplies the cosmic-ray leptons to
power this source. We have analysed the Fermi-LAT data to explore this source's
morphology and identify its spectral feature in the Fermi-LAT energy band.
After removing the contribution of the pulsar to the gamma-ray spectral energy
distribution by pulsar phased analysis, we have obtained upper limits on the
photon flux and identified the GeV counterpart PS J1844.2-0342 in the Fermi-LAT
energy band with more than 5$\sigma$ significance, which may be a pulsar wind
nebula. Finally, the multi-wavelength spectral energy distribution has been
modelled, assuming HAWC J1844-034 is a pulsar wind nebula.; 18) A twisted derived category of hyper-K\""ahler varieties of
  $K3^{[n]}$-type; We conjecture that a natural twisted derived category of any hyper-K\""ahler
variety of $K3^{[n]}$-type is controlled by its Markman-Mukai lattice. We prove
the conjecture under numerical constraints, and our proof relies heavily on
Markman's projectively hyperholomorphic bundle and a recently proven twisted
version of the D-equivalence conjecture.
  In particular, we prove that any two fine moduli spaces of stable sheaves on
a $K3$ surface are derived equivalent if they have the same dimension.; 19) DPM-Bench: Benchmark for Distributed Process Mining Algorithms on
  Cyber-Physical Systems; Process Mining is established in research and industry systems to analyze and
optimize processes based on event data from information systems. Within this
work, we accomodate process mining techniques to Cyber-Physical Systems. To
capture the distributed and heterogeneous characteristics of data,
computational resources, and network communication in CPS, the todays process
mining algorithms and techniques must be augmented. Specifically, there is a
need for new Distributed Process Mining algorithms that enable computations to
be performed directly on edge resources, eliminating the need for moving all
data to central cloud systems. This paper introduces the DPM-Bench benchmark
for comparing such Distributed Process Mining algorithms. DPM-Bench is used to
compare algorithms deployed in different computational topologies. The results
enable information system engineers to assess whether the existing
infrastructure is sufficient to perform distributed process mining, or to
identify required improvements in algorithms and hardware. We present and
discuss an experimental evaluation with DPM-Bench.; 20) A Comprehensive Metric for Resilience Evaluation of Power Distribution
  Systems under Cyber Attacks; Power distribution systems (PDS) serve as the backbone of our modern society,
ensuring electricity reaches homes, businesses, and critical infrastructure.
However, the increasing digitization and interconnectivity of these systems
have exposed them to cyber threats. This study presents a comprehensive
approach to evaluate and enhance the resilience of PDS under cyber attacks
using the Common Vulnerability Scoring System (CVSS) and complex network
parameters. By systematically assessing vulnerabilities and computing
resilience once critical CVSS thresholds are reached, this work identifies key
resilience metrics including the critical loads service requirements. The
proposed methodology improves system resilience through strategic tie-line
switching, which is validated on the modified IEEE 33-bus system. Four case
studies are conducted, illustrating the performance of the proposed methodology
under various cyber attack scenarios. The results demonstrate the effectiveness
of the approach in quantifying and enhancing resilience, offering a valuable
tool for PDS operators to mitigate risks and ensure continuous service delivery
to critical loads during the exploitation of cyber threats.; 21) Optimal Follow-Up of Gravitational-Wave Events with the UltraViolet
  EXplorer (UVEX); The UltraViolet EXplorer (UVEX) is a wide-field ultraviolet space telescope
selected as a NASA Medium-Class Explorer (MIDEX) mission for launch in 2030.
UVEX will undertake deep, cadenced surveys of the entire sky to probe low mass
galaxies and explore the ultraviolet (UV) time-domain sky, and it will carry
the first rapidly deployable UV spectroscopic capability for a broad range of
science applications. One of UVEX's prime objectives is to follow up
gravitational wave (GW) binary neutron star mergers as targets of opportunity
(ToOs), rapidly scanning across their localization regions to search for their
kilonova (KN) counterparts. Early-time multiband ultraviolet light curves of
KNe are key to explaining the interplay between jet and ejecta in binary
neutron star mergers. Owing to high Galactic extinction in the ultraviolet and
the variation of GW distance estimates over the sky, the sensitivity to
kilonovae can vary significantly across the GW localization and even across the
footprint of a single image given UVEX's large field of view. Good ToO
observing strategies to trade off between area and depth are neither simple nor
obvious. We present an optimal strategy for GW follow-up with UVEX in which
exposure time is adjusted dynamically for each field individually to maximize
the overall probability of detection. We model the scheduling problem using the
expressive and powerful mathematical framework of mixed integer linear
programming (MILP), and employ a state-of-the-art MILP solver to automatically
generate observing plan timelines that achieve high probabilities of kilonova
detection. We have implemented this strategy in an open-source astronomical
scheduling software package called the Multi-Mission Multi-Messenger
Observation Planning Toolkit (M4OPT), on GitHub at
https://github.com/m4opt/m4opt.; 22) Orbitronics in Two-dimensional Materials; Orbitronics explores the control and manipulation of electronic orbital
angular momentum in solid-state systems, opening new pathways for information
processing and storage. One significant advantage of orbitronics over
spintronics is that it does not rely on spin-orbit coupling, thereby broadening
the range of non-magnetic materials that can be utilized for these
applications. It also introduces new topological features related to electronic
orbital angular momentum, and clarifies some long-standing challenges in
understanding experiments that rely on the conventional concept of valley
transport. This review highlights recent advances in orbitronics, particularly
in relation to two-dimensional materials. We examine the fundamental principles
underlying the generation, transport, and dynamics of orbital angular momentum
to illustrate how the unique properties of two-dimensional materials can
promote orbitronic phenomena. We also outline potential future research
directions and address some outstanding questions in this field.; 23) Deep Minimax Classifiers for Imbalanced Datasets with a Small Number of
  Minority Samples; The concept of a minimax classifier is well-established in statistical
decision theory, but its implementation via neural networks remains
challenging, particularly in scenarios with imbalanced training data having a
limited number of samples for minority classes. To address this issue, we
propose a novel minimax learning algorithm designed to minimize the risk of
worst-performing classes. Our algorithm iterates through two steps: a
minimization step that trains the model based on a selected target prior, and a
maximization step that updates the target prior towards the adversarial prior
for the trained model. In the minimization, we introduce a targeted
logit-adjustment loss function that efficiently identifies optimal decision
boundaries under the target prior. Moreover, based on a new prior-dependent
generalization bound that we obtained, we theoretically prove that our loss
function has a better generalization capability than existing loss functions.
During the maximization, we refine the target prior by shifting it towards the
adversarial prior, depending on the worst-performing classes rather than on
per-class risk estimates. Our maximization method is particularly robust in the
regime of a small number of samples. Additionally, to adapt to
overparameterized neural networks, we partition the entire training dataset
into two subsets: one for model training during the minimization step and the
other for updating the target prior during the maximization step. Our proposed
algorithm has a provable convergence property, and empirical results indicate
that our algorithm performs better than or is comparable to existing methods.
All codes are publicly available at
https://github.com/hansung-choi/TLA-linear-ascent.; 24) Optimal Privacy-Preserving Distributed Median Consensus; Distributed median consensus has emerged as a critical paradigm in
multi-agent systems due to the inherent robustness of the median against
outliers and anomalies in measurement. Despite the sensitivity of the data
involved, the development of privacy-preserving mechanisms for median consensus
remains underexplored. In this work, we present the first rigorous analysis of
privacy in distributed median consensus, focusing on an $L_1$-norm minimization
framework. We establish necessary and sufficient conditions under which exact
consensus and perfect privacy-defined as zero information leakage-can be
achieved simultaneously. Our information-theoretic analysis provides provable
guarantees against passive and eavesdropping adversaries, ensuring that private
data remain concealed. Extensive numerical experiments validate our theoretical
results, demonstrating the practical feasibility of achieving both accuracy and
privacy in distributed median consensus.; 25) Mid-infrared absorption spectra and mass absorption coefficients for 23
  chondrites: dependence on composition and grain size; We present mid-infrared transmission spectra from 2 to 23 microns of the 23
Atacama Desert chondrites of different types (carbonaceous Ornans and ordinary
of H, L, and LL groups) as well as of some pure minerals (olivine and
diopside). We focus on the characteristics of silicate at 10 and 20 microns,
analyzing the influence of composition and grain size on peak strengths and
spectral shapes. We present the first results of the Cosmic Dust Laboratory, a
dedicated facility at the Universidad Diego Portales equipped with a VERTEX 80v
vacuum Fourier transform infrared spectrometer. Through milling and sieving
samples, we obtained different ranges of particle sizes to study the effect of
grain size on the intensity and shape of the spectrum. The resulting spectral
library can be compared with astronomical data of protoplanetary disks, debris
disks, and even white dwarf disks obtained with instruments such as MIRI on
board the James Webb Space Telescope and MATISSE on the Very Large Telescope
Interferometer. We also present mass absorption coefficient values, which can
be used for radiative transfer modeling of astronomical observations. This
study aims to improve dust opacities for astronomical applications, with a
focus on circumstellar disks.; 26) Centralizers on a super-reflexive Schatten ideal; We give a simple proof that there is no strictly singular bicentralizer on a
super-reflexive Schatten ideal. This result applies, in particular, to the
$p$-Schatten class for $1<p<\infty$.; 27) Exact Simulation of Longitudinal Data from Marginal Structural Models; Simulating longitudinal data from specified marginal structural models is a
crucial but challenging task for evaluating causal inference methods and
clinical trial design. While data generation typically proceeds in a fully
conditional manner using structural equations according to a temporal ordering,
it is difficult to ensure alignment between conditional distributions and the
target marginal causal effects. This misalignment presents a fundamental
challenge in simulating data that adheres to marginal structural model
specifications. To address this, we propose a flexible and efficient algorithm
for simulating longitudinal data that adheres exactly to a specified marginal
structural model. Recognizing the importance of time-to-event outcomes in
clinical research, we extend our approach to accommodate survival models.
Compared to existing approaches, our method offers several advantages: it
enables exact simulation from a known causal model rather than relying on
approximations; avoids restrictive assumptions about the data-generating
process; and remains computationally efficient by requiring only the evaluation
of analytic functions. This last benefit contrasts with methods that use
computationally intensive techniques such as Monte Carlo approximations or
numerical integration. Through simulation studies replicating realistic
scenarios, we validate the method's accuracy and utility. Our method will
facilitate researchers in effectively simulating data with target causal
structures for their specific scenarios.; 28) Robust zero modes in PbTe-Pb hybrid nanowires; Majorana zero modes in tunneling conductance are expected to manifest as
robust zero bias peaks (ZBPs). While ZBPs alone are not conclusive evidence of
Majorana modes due to alternative explanations, robust ZBPs remain a crucial
and necessary first-step indicator in the search for topological states. Here,
we report the observation of robust ZBPs in PbTe-Pb hybrid nanowires. The peak
height can reach $2e^2/h$, though it does not yet form a quantized plateau.
Importantly, these ZBPs can remain non-split over sizable ranges in both
magnetic field and gate voltage scans, highlighting their robustness. We
discuss possible interpretations based on Majorana zero modes as well as
Andreev bound states.; 29) A remarkable Ruby: Absorption in dense gas, rather than evolved stars,
  drives the extreme Balmer break of a Little Red Dot at $z=3.5$; The origin of the rest-optical emission of compact, red, high-redshift
sources known as `little red dots' (LRDs) poses a major puzzle. If interpreted
as starlight, it would imply that LRDs would constitute the densest stellar
systems in the Universe. However, alternative models suggest active galactic
nuclei (AGN) may instead power the rest-optical continuum. Here, we present
JWST/NIRSpec, NIRCam and MIRI observations from the RUBIES and PRIMER programs
of The Cliff: a bright LRD at $z=3.55$ with an exceptional Balmer break, twice
as strong as that of any high-redshift source previously observed. The spectra
also reveal broad Hydrogen (H$\alpha\ \rm FWHM\sim1500$km/s) and He I emission,
but no significant metal lines. We demonstrate that massive evolved stellar
populations cannot explain the observed spectrum, even when considering
unusually steep and strong dust attenuation, or reasonable variations in the
initial mass function. Moreover, the formally best-fit stellar mass and compact
size ($M_*\sim10^{10.5}\,M_\odot,\ r_{e}\sim40\,$pc) would imply densities at
which near-monthly stellar collisions might lead to significant X-ray emission.
We argue that the Balmer break, emission lines, and H$\alpha$ absorption line
are instead most plausibly explained by a `black hole star' (BH*) scenario, in
which dense gas surrounds a powerful ionising source. In contrast to recently
proposed BH* models of dust-reddened AGN, we show that spectral fits in the
rest UV to near-infrared favour an intrinsically redder continuum over strong
dust reddening. This may point to a super-Eddington accreting massive black
hole or, possibly, the presence of (super)massive stars in a nuclear star
cluster. The Cliff is the clearest evidence to date that at least some LRDs are
not ultra-dense, massive galaxies, and are instead powered by a central
ionising source embedded in dense, absorbing gas.; 30) MOHPER: Multi-objective Hyperparameter Optimization Framework for
  E-commerce Retrieval System; E-commerce search optimization has evolved to include a wider range of
metrics that reflect user engagement and business objectives. Modern search
frameworks now incorporate advanced quality features, such as sales counts and
document-query relevance, to better align search results with these goals.
Traditional methods typically focus on click-through rate (CTR) as a measure of
engagement or relevance, but this can miss true purchase intent, creating a gap
between user interest and actual conversions. Joint training with the
click-through conversion rate (CTCVR) has become essential for understanding
buying behavior, although its sparsity poses challenges for reliable
optimization. This study presents MOHPER, a Multi-Objective Hyperparameter
Optimization framework for E-commerce Retrieval systems. Utilizing Bayesian
optimization and sampling, it jointly optimizes both CTR, CTCVR, and relevant
objectives, focusing on engagement and conversion of the users. In addition, to
improve the selection of the best configuration from multi-objective
optimization, we suggest advanced methods for hyperparameter selection,
including a meta-configuration voting strategy and a cumulative training
approach that leverages prior optimal configurations, to improve speeds of
training and efficiency. Currently deployed in a live setting, our proposed
framework substantiates its practical efficacy in achieving a balanced
optimization that aligns with both user satisfaction and revenue goals.; 31) On the Importance of Clearsky Model in Short-Term Solar Radiation
  Forecasting; Clearsky models are widely used in solar energy for many applications such as
quality control, resource assessment, satellite-base irradiance estimation and
forecasting. However, their use in forecasting and nowcasting is associated
with a number of challenges. Synchronization errors, reliance on the Clearsky
index (ratio of the global horizontal irradiance to its cloud-free counterpart)
and high sensitivity of the clearsky model to errors in aerosol optical depth
at low solar elevation limit their added value in real-time applications. This
paper explores the feasibility of short-term forecasting without relying on a
clearsky model. We propose a Clearsky-Free forecasting approach using Extreme
Learning Machine (ELM) models. ELM learns daily periodicity and local
variability directly from raw Global Horizontal Irradiance (GHI) data. It
eliminates the need for Clearsky normalization, simplifying the forecasting
process and improving scalability. Our approach is a non-linear adaptative
statistical method that implicitely learns the irradiance in cloud-free
conditions removing the need for an clear-sky model and the related operational
issues. Deterministic and probabilistic results are compared to traditional
benchmarks, including ARMA with McClear-generated Clearsky data and quantile
regression for probabilistic forecasts. ELM matches or outperforms these
methods, providing accurate predictions and robust uncertainty quantification.
This approach offers a simple, efficient solution for real-time solar
forecasting. By overcoming the stationarization process limitations based on
usual multiplicative scheme Clearsky models, it provides a flexible and
reliable framework for modern energy systems.; 32) A Parallel, Energy-Stable Low-Rank Integrator for Nonlinear Multi-Scale
  Thermal Radiative Transfer; Thermal radiative transfer models physical phenomena ranging from supernovas
in astrophysics to radiation from a hohlraum striking a fusion target in plasma
physics. Transport and absorption of particles in radiative transfer at
different rates lead to a complex interaction between the material and
particles that involves highly varying time scales. Resolving these effects can
require prohibitively small step sizes, which, combined with nonlinear effects
and the particle density's high-dimensional phase space, render conventional
numerical methods computationally expensive. This work presents an
asymptotic--preserving, mass conservative, rank-adaptive, and parallel
integrator for a macro--micro decomposition-based dynamical low-rank
approximation of the thermal radiative transfer equations. The proposed
integrator efficiently incorporates reflection-transmission type boundary
conditions in the low-rank factors. It captures the nonlinear effects of
thermal radiation and is energy stable with the step size restriction capturing
both hyperbolic and parabolic CFL conditions. The efficacy of the proposed
integrator is demonstrated with numerical experiments.; 33) The coherence peak of unconventional superconductors in the charge
  channel; In this work, we carry out a systematic investigation of the coherence peak
in unconventional superconductors as they transition into the superconducting
phase at $T_c$. Using $d$-wave cuprates as an example, we reveal the presence
of a coherence peak below $T_c$ in the charge channel. The nuclear quadrupole
relaxation rate is shown to be an effective method for detecting this
unconventional coherence peak, with the superconducting coherence factor
playing a pivotal role in its emergence. Additionally, we explore the influence
of correlation effects, which further enhance this phenomenon. Extending our
analysis, we demonstrate the existence of a similar coherence peak in
ultrasonic attenuation and iron-based superconductors. Our findings offer a
fresh perspective on probing superconducting gap symmetry in unconventional
superconductors.; 34) Tell me about yourself: LLMs are aware of their learned behaviors; We study behavioral self-awareness -- an LLM's ability to articulate its
behaviors without requiring in-context examples. We finetune LLMs on datasets
that exhibit particular behaviors, such as (a) making high-risk economic
decisions, and (b) outputting insecure code. Despite the datasets containing no
explicit descriptions of the associated behavior, the finetuned LLMs can
explicitly describe it. For example, a model trained to output insecure code
says, ``The code I write is insecure.'' Indeed, models show behavioral
self-awareness for a range of behaviors and for diverse evaluations. Note that
while we finetune models to exhibit behaviors like writing insecure code, we do
not finetune them to articulate their own behaviors -- models do this without
any special training or examples.
  Behavioral self-awareness is relevant for AI safety, as models could use it
to proactively disclose problematic behaviors. In particular, we study backdoor
policies, where models exhibit unexpected behaviors only under certain trigger
conditions. We find that models can sometimes identify whether or not they have
a backdoor, even without its trigger being present. However, models are not
able to directly output their trigger by default.
  Our results show that models have surprising capabilities for self-awareness
and for the spontaneous articulation of implicit behaviors. Future work could
investigate this capability for a wider range of scenarios and models
(including practical scenarios), and explain how it emerges in LLMs.; 35) Chow polynomials of uniform matroids are real-rooted; June Huh and Matthew Stevens conjectured that the Hilbert-Poincar\'e series
of the Chow ring of any matroid is a polynomial with only real zeros. We prove
this conjecture for the class of uniform matroids. We also prove that the Chow
polynomial and the augmented Chow polynomial of any maximally ranked poset has
only real zeros.; 36) Influence of dark matter on the structure of strange quark stars in
  one-fluid model; This work studies the influence of scalar dark matter on the structural
properties of strange quark stars (SQS) within a one-fluid framework,
considering Yukawa interactions between dark matter and quark matter.
Contributions from perturbative QCD, Yukawa interaction between scalar dark
matter and quarks, and Bose-Einstein condensation of dark matter are included
in the model. We first determine the allowable range of Yukawa interaction
coupling by imposing the stability condition for strange quark matter (SQM).
Using this range, we derive the equation of state (EOS) for different fractions
of dark matter within the total pressure of SQS. These fractions are
constrained by the tidal deformability limit from GW170817. The presence of
dark matter alters the EOS, leading to changes in the mass-radius relationship,
tidal deformability, and stability of SQS. We demonstrate that increasing the
mass of dark matter softens the EOS, whereas higher fractions of dark matter
lead to stiffer EOSs. We also explore the reasons behind this behavior. Our
EOSs not only describe massive objects, such as PSR J0952-0607 and PSR
J2215+5135, but also satisfy the tidal deformability constraint from GW170817.
These results reveal that incorporating dark matter modifies the EOS, enabling
the support of higher stellar masses while maintaining consistency with
observational data.; 37) A fast and slightly robust covariance estimator; Let $\mathcal{Z} = \{Z_1, \dots, Z_n\} \stackrel{\mathrm{i.i.d.}}{\sim} P
\subset \mathbb{R}^d$ from a distribution $P$ with mean zero and covariance
$\Sigma$. Given a dataset $\mathcal{X}$ such that
$d_{\mathrm{ham}}(\mathcal{X}, \mathcal{Z}) \leq \varepsilon n$, we are
interested in finding an efficient estimator $\widehat{\Sigma}$ that achieves
$\mathrm{err}(\widehat{\Sigma}, \Sigma) :=
\|\Sigma^{-\frac{1}{2}}\widehat{\Sigma}\Sigma^{-\frac{1}{2}} - I\|
_{\mathrm{op}} \leq 1/2$. We focus on the low contamination regime $\varepsilon
= o(1/\sqrt{d}$). In this regime, prior work required either $\Omega(d^{3/2})$
samples or runtime that is exponential in $d$. We present an algorithm that,
for subgaussian data, has near-linear sample complexity $n =
\widetilde{\Omega}(d)$ and runtime $O((n+d)^{\omega + \frac{1}{2}})$, where
$\omega$ is the matrix multiplication exponent. We also show that this
algorithm works for heavy-tailed data with near-linear sample complexity, but
in a smaller regime of $\varepsilon$. Concurrent to our work, Diakonikolas et
al. [2024] give Sum-of-Squares estimators that achieve similar sample
complexity but with large polynomial runtime.; 38) Solving bihomogeneous polynomial systems with a zero-dimensional
  projection; We study bihomogeneous systems defining, non-zero dimensional, biprojective
varieties for which the projection onto the first group of variables results in
a finite set of points. To compute (with) the 0-dimensional projection and the
corresponding quotient ring, we introduce linear maps that greatly extend the
classical multiplication maps for zero-dimensional systems, but are not those
associated to the elimination ideal; we also call them multiplication maps. We
construct them using linear algebra on the restriction of the ideal to a
carefully chosen bidegree or, if available, from an arbitrary Gr\""obner bases.
The multiplication maps allow us to compute the elimination ideal of the
projection, by generalizing FGLM algorithm to bihomogenous, non-zero
dimensional, varieties. We also study their properties, like their minimal
polynomials and the multiplicities of their eigenvalues, and show that we can
use the eigenvalues to compute numerical approximations of the zero-dimensional
projection. Finally, we establish a single exponential complexity bound for
computing multiplication maps and Gr\""obner bases, that we express in terms of
the bidegrees of the generators of the corresponding bihomogeneous ideal.; 39) Reply to Pavi\v{c}i\'c's 'Comment on ""Optimal conversion of
  Kochen-Specker sets into bipartite perfect quantum strategies""'
  (arXiv:2502.13787); According to Pavi\v{c}i{\'c}, Kochen and Specker's 117-observable set is not
a ``Kochen-Specker set''. By the same reason, in arXiv:2502.13787,
Pavi\v{c}i{\'c} claims that 10 statements in our paper ``Optimal conversion of
Kochen-Specker sets into bipartite perfect quantum strategies'' [Phys. Rev. A
111, 022408 (2025)] are ``wrong''. In all cases, Pavi\v{c}i{\'c}'s claims are
based on the fact that he is assuming a different definition of Kochen-Specker
(KS) set. Adopting a terminology used by, e.g., Larsson, the sets that
Pavi\v{c}i{\'c} call KS sets can be called ``extended'' KS sets, since they are
constructed by adding observables to the ``original'' KS sets. For example,
Pavi\v{c}i{\'c} adds 75 observables to the original 117-observable KS set.
Beyond terminology, there are fundamental physical reasons for focusing on the
original KS sets. One reason is that, for experimentally observing quantum
state-independent contextuality, there is no need to measure the observables
added in the extended sets. Another reason is that, to produce bipartite
perfect quantum strategies, or correlations in a face of the nonsignaling
polytope with no local points, or correlations with nonlocal content 1, the two
parties do not need to measure any of the observables added in the extended
sets. We also respond to other claims made by Pavi\v{c}i{\'c} about our work.; 40) VidSole: A Multimodal Dataset for Joint Kinetics Quantification and
  Disease Detection with Deep Learning; Understanding internal joint loading is critical for diagnosing gait-related
diseases such as knee osteoarthritis; however, current methods of measuring
joint risk factors are time-consuming, expensive, and restricted to lab
settings. In this paper, we enable the large-scale, cost-effective
biomechanical analysis of joint loading via three key contributions: the
development and deployment of novel instrumented insoles, the creation of a
large multimodal biomechanics dataset (VidSole), and a baseline deep learning
pipeline to predict internal joint loading factors. Our novel instrumented
insole measures the tri-axial forces and moments across five high-pressure
points under the foot. VidSole consists of the forces and moments measured by
these insoles along with corresponding RGB video from two viewpoints, 3D body
motion capture, and force plate data for over 2,600 trials of 52 diverse
participants performing four fundamental activities of daily living
(sit-to-stand, stand-to-sit, walking, and running). We feed the insole data and
kinematic parameters extractable from video (i.e., pose, knee angle) into a
deep learning pipeline consisting of an ensemble Gated Recurrent Unit (GRU)
activity classifier followed by activity-specific Long Short Term Memory (LSTM)
regression networks to estimate knee adduction moment (KAM), a biomechanical
risk factor for knee osteoarthritis. The successful classification of
activities at an accuracy of 99.02 percent and KAM estimation with mean
absolute error (MAE) less than 0.5 percent*body weight*height, the current
threshold for accurately detecting knee osteoarthritis with KAM, illustrates
the usefulness of our dataset for future research and clinical settings.; 41) QDM: Quadtree-Based Region-Adaptive Sparse Diffusion Models for
  Efficient Image Super-Resolution; Deep learning-based super-resolution (SR) methods often perform pixel-wise
computations uniformly across entire images, even in homogeneous regions where
high-resolution refinement is redundant. We propose the Quadtree Diffusion
Model (QDM), a region-adaptive diffusion framework that leverages a quadtree
structure to selectively enhance detail-rich regions while reducing
computations in homogeneous areas. By guiding the diffusion with a quadtree
derived from the low-quality input, QDM identifies key regions-represented by
leaf nodes-where fine detail is essential and applies minimal refinement
elsewhere. This mask-guided, two-stream architecture adaptively balances
quality and efficiency, producing high-fidelity outputs with low computational
redundancy. Experiments demonstrate QDM's effectiveness in high-resolution SR
tasks across diverse image types, particularly in medical imaging (e.g., CT
scans), where large homogeneous regions are prevalent. Furthermore, QDM
outperforms or is comparable to state-of-the-art SR methods on standard
benchmarks while significantly reducing computational costs, highlighting its
efficiency and suitability for resource-limited environments. Our code is
available at https://github.com/linYDTHU/QDM.; 42) Monocular visual simultaneous localization and mapping: (r)evolution
  from geometry to deep learning-based pipelines; With the rise of deep learning, there is a fundamental change in visual SLAM
algorithms toward developing different modules trained as end-to-end pipelines.
However, regardless of the implementation domain, visual SLAM's performance is
subject to diverse environmental challenges, such as dynamic elements in
outdoor environments, harsh imaging conditions in underwater environments, or
blurriness in high-speed setups. These environmental challenges need to be
identified to study the real-world viability of SLAM implementations. Motivated
by the aforementioned challenges, this paper surveys the current state of
visual SLAM algorithms according to the two main frameworks: geometry-based and
learning-based SLAM. First, we introduce a general formulation of the SLAM
pipeline that includes most of the implementations in the literature. Second,
those implementations are classified and surveyed for geometry and
learning-based SLAM. After that, environment-specific challenges are formulated
to enable experimental evaluation of the resilience of different visual SLAM
classes to varying imaging conditions. We address two significant issues in
surveying visual SLAM, providing (1) a consistent classification of visual SLAM
pipelines and (2) a robust evaluation of their performance under different
deployment conditions. Finally, we give our take on future opportunities for
visual SLAM implementations.; 43) Bridging Classical and Quantum String Matching: A Computational
  Reformulation of Bit-Parallelism; String matching is a fundamental problem in computer science, with critical
applications in text retrieval, bioinformatics, and data analysis. Among the
numerous solutions that have emerged for this problem in recent decades,
bit-parallelism has significantly enhanced their practical efficiency, leading
to the development of several optimized approaches for both exact and
approximate string matching. However, their potential in quantum computing
remains largely unexplored. This paper presents a novel pathway that not only
translates bit-parallel string matching algorithms into the quantum framework
but also enhances their performance to achieve a quadratic speedup through
Grover's search. By embedding quantum search within a bit-parallel model, we
reduce the time complexity of string matching, establishing a structured
pathway for transforming classical algorithms into quantum solutions with
provable computational advantages. Beyond exact matching, this technique offers
a foundation for tackling a wide range of non-standard string matching
problems, opening new avenues for efficient text searching in the quantum era.
To demonstrate the simplicity and adaptability of the technique presented in
this paper, we apply this translation and adaptation process to two landmark
bit-parallel algorithms: Shift-And for exact pattern matching and Shift-Add for
approximate string matching with up to k errors.; 44) AgentOrca: A Dual-System Framework to Evaluate Language Agents on
  Operational Routine and Constraint Adherence; As language agents progressively automate critical tasks across domains,
their ability to operate within operational constraints and safety protocols
becomes essential. While extensive research has demonstrated these agents'
effectiveness in downstream task completion, their reliability in following
operational procedures and constraints remains largely unexplored. To this end,
we present AgentOrca, a dual-system framework for evaluating language agents'
compliance with operational constraints and routines. Our framework encodes
action constraints and routines through both natural language prompts for
agents and corresponding executable code serving as ground truth for automated
verification. Through an automated pipeline of test case generation and
evaluation across five real-world domains, we quantitatively assess current
language agents' adherence to operational constraints. Our findings reveal
notable performance gaps among state-of-the-art models, with large reasoning
models like o1 demonstrating superior compliance while others show
significantly lower performance, particularly when encountering complex
constraints or user persuasion attempts.; 45) EvoGP: A GPU-accelerated Framework for Tree-based Genetic Programming; Tree-based Genetic Programming (TGP) is a key evolutionary algorithm widely
used in symbolic regression, feature engineering, and scientific modeling. Its
high computational demands make GPU acceleration essential for scalable and
high-performance evolutionary computation. However, GPU acceleration of TGP
faces three key challenges: inefficient tree encoding, highly heterogeneous
genetic operations, and limited parallelism in fitness evaluation. To address
these challenges, we introduce EvoGP, a comprehensive GPU-accelerated TGP
framework. First, we design a tensorized encoding scheme to represent tree with
different structures as tensors with the same shape, optimizing memory access
and enabling efficient parallel execution. Second, we propose a unified
parallel framework for genetic operations by leveraging shared computational
primitives and implementing dedicated CUDA kernels for scalable performance.
Third, we present a fully parallel fitness evaluation strategy for symbolic
regression, exploiting both population-level and data-level parallelism to
maximize GPU utilization. Moreover, we implement a comprehensive library to
provide rich algorithm operators and benchmark problems. EvoGP is extensively
tested on various tasks, including symbolic regression, classification, and
robotics control, demonstrating its versatility and effectiveness across
diverse application scenarios. Experimental results show that EvoGP achieves up
to a 140.89x speedup over the state-of-the-art GPU-based TGP implementation,
while maintaining or exceeding the accuracy of baseline methods. EvoGP is
open-source and accessible at: https://github.com/EMI-Group/evogp.; 46) Single-Satellite-Based Geolocation of Broadcast GNSS Spoofers from Low
  Earth Orbit; This paper presents an analysis and experimental demonstration of
single-satellite single-pass geolocation of a terrestrial broadcast Global
Navigation Satellite System (GNSS) spoofer from Low Earth Orbit (LEO). The
proliferation of LEO-based GNSS receivers offers the prospect of unprecedented
spectrum awareness, enabling persistent GNSS interference detection and
geolocation. Accurate LEO-based single-receiver emitter geolocation is possible
when a range-rate time history can be extracted for the emitter. This paper
presents a technique crafted specifically for indiscriminate broadcast-type
GNSS spoofing signals. Furthermore, it explores how unmodeled oscillator
instability and worst-case spoofer-introduced signal variations degrade the
geolocation estimate. The proposed geolocation technique is validated by a
controlled experiment, in partnership with Spire Global, in which a LEO-based
receiver captures broadcast GNSS spoofing signals transmitted from a known
ground station on a non-GNSS frequency band.; 47) Comprehensive Framework for Evaluating Conversational AI Chatbots; Conversational AI chatbots are transforming industries by streamlining
customer service, automating transactions, and enhancing user engagement.
However, evaluating these systems remains a challenge, particularly in
financial services, where compliance, user trust, and operational efficiency
are critical. This paper introduces a novel evaluation framework that
systematically assesses chatbots across four dimensions: cognitive and
conversational intelligence, user experience, operational efficiency, and
ethical and regulatory compliance. By integrating advanced AI methodologies
with financial regulations, the framework bridges theoretical foundations and
real-world deployment challenges. Additionally, we outline future research
directions, emphasizing improvements in conversational coherence, real-time
adaptability, and fairness.; 48) Anomalous Meets Topological Hall Effect in Cr2Ge2Te6 Heterostructures; Introducing topologically protected skyrmions in graphene holds significant
importance for developing high-speed, low-energy spintronic devices. Here, we
present a centrosymmetric ferromagnetic graphene/trilayer Cr2Ge2Te6/graphene
heterostructure, demonstrating the anomalous and topological Hall effect due to
the magnetic proximity effect. Through gate voltage control, we effectively
tune the emergence and size of skyrmions. Micromagnetic simulations reveal the
formation of skyrmions and antiskyrmions, which respond differently to external
magnetic fields, leading to oscillations in the topological Hall signal. Our
findings provide a novel pathway for the formation and manipulation of
skyrmions in centrosymmetric two-dimensional magnetic systems, offering
significant insights for developing topological spintronics.; 49) Early Detection and Classification of Breast Cancer Using Deep Learning
  Techniques; Breast cancer is one of the deadliest cancers causing about massive number of
patients to die annually all over the world according to the WHO. It is a kind
of cancer that develops when the tissues of the breast grow rapidly and
unboundly. This fatality rate can be prevented if the cancer is detected before
it gets malignant. Using automation for early-age detection of breast cancer,
Artificial Intelligence and Machine Learning technologies can be implemented
for the best outcome. In this study, we are using the Breast Cancer Image
Classification dataset collected from the Kaggle depository, which comprises
9248 Breast Ultrasound Images and is classified into three categories: Benign,
Malignant, and Normal which refers to non-cancerous, cancerous, and normal
images.This research introduces three pretrained model featuring custom
classifiers that includes ResNet50, MobileNet, and VGG16, along with a custom
CNN model utilizing the ReLU activation function.The models ResNet50,
MobileNet, VGG16, and a custom CNN recorded accuracies of 98.41%, 97.91%,
98.19%, and 92.94% on the dataset, correspondingly, with ResNet50 achieving the
highest accuracy of 98.41%.This model, with its deep and powerful architecture,
is particularly successful in detecting aberrant cells as well as cancerous or
non-cancerous tumors. These accuracies show that the Machine Learning methods
are more compatible for the classification and early detection of breast
cancer.; 50) Moir\'e $M$-valley bilayers: quasi-one-dimensional physics,
  unconventional spin textures and twisted van Hove singularities; Motivated by the discovery of quasi-two-dimensional kagome metals
AV$_3$Sb$_5$, we consider the theory of twisted bilayers in which the Fermi
surface is near the $M$-point. Surprisingly, unlike twisted bilayers of
graphene or transition metal dichalcogenides, the moir\'e potential is
quasi-one-dimensional: at each $M$-valley, the potential flattens the
dispersion strongly along one direction, and weakly along the perpendicular
direction. The combination of spin-orbit coupling and twist-induced broken
inversion symmetry results in a similarly anisotropic
`$\textit{moir\'e-Rashba}$' potential, which spin-splits the dispersion into
coexisting two-dimensional and quasi-one-dimensional bands. We discuss novel
aspects of the interplay between mixed dimensionality and spin textures in this
platform. First, an applied electric field produces spin polarisation which can
be tuned by doping, suggesting potential spintronics applications. Secondly, an
in-plane magnetic field momentum- and spin-polarises the Fermi surfaces,
producing unconventional spin density waves. Thirdly, in the small-twist-angle
limit, the large density of states due to a twisted van Hove singularity near
$M$ results in a dense energy spectrum. Our results demonstrate a new variation
of moir\'e bandstructure engineering, instigating the study of spin-textured
one-dimensional physics in moir\'e materials.; 51) Quasinormal Modes of Black Holes: Efficient and Highly Accurate
  Calculations with Recurrence-Based Methods; We discuss new recurrence-based methods for calculating the complex
frequencies of the quasinormal modes of black holes. These methods are based on
the Frobenius series solutions of the differential equation describing the
linearized radial perturbations. Within the general method, we propose two
approaches: the first involves calculating the series coefficients, while the
second employs generalized continued fractions. Moreover, as a consequence of
this analysis, we present a computationally efficient and convenient method
that uses double convergence acceleration, consisting of the application of the
Wynn algorithm to the approximants obtained from the Hill determinants, with
the Leaver-Nollert-Zhidenko-like tail approximations taken into account. The
latter is particularly important for stabilizing and enabling the calculations
of modes with small real parts as well as higher overtones. The method
demonstrates exceptionally high accuracy. We emphasize that Gaussian
elimination is unnecessary in all of these calculations. We consider
$D$-dimensional ($3<D<10$) Schwarzschild-Tangherlini black holes as concrete
examples. Specifically, we calculate the quasinormal modes of the
$(2+1)$-dimensional acoustic black hole (which is closely related to the
five-dimensional Schwarzschild-Tangherlini black holes), the
electromagnetic-vector modes of the six-dimensional black holes and the scalar
(gravitational tensor) modes in the seven-dimensional case. We believe that the
methods presented here are applicable beyond the examples shown, also outside
the domain of the black hole physics.; 52) SBAMDT: Bayesian Additive Decision Trees with Adaptive Soft
  Semi-multivariate Split Rules; Bayesian Additive Regression Trees [BART, Chipman et al., 2010] have gained
significant popularity due to their remarkable predictive performance and
ability to quantify uncertainty. However, standard decision tree models rely on
recursive data splits at each decision node, using deterministic decision rules
based on a single univariate feature. This approach limits their ability to
effectively capture complex decision boundaries, particularly in scenarios
involving multiple features, such as spatial domains, or when transitions are
either sharp or smoothly varying. In this paper, we introduce a novel
probabilistic additive decision tree model that employs a soft split rule. This
method enables highly flexible splits that leverage both univariate and
multivariate features, while also respecting the geometric properties of the
feature domain. Notably, the probabilistic split rule adapts dynamically across
decision nodes, allowing the model to account for varying levels of smoothness
in the regression function. We demonstrate the utility of the proposed model
through comparisons with existing tree-based models on synthetic datasets and a
New York City education dataset.; 53) Microservice Deployment in Space Computing Power Networks via Robust
  Reinforcement Learning; With the growing demand for Earth observation, it is important to provide
reliable real-time remote sensing inference services to meet the low-latency
requirements. The Space Computing Power Network (Space-CPN) offers a promising
solution by providing onboard computing and extensive coverage capabilities for
real-time inference. This paper presents a remote sensing artificial
intelligence applications deployment framework designed for Low Earth Orbit
satellite constellations to achieve real-time inference performance. The
framework employs the microservice architecture, decomposing monolithic
inference tasks into reusable, independent modules to address high latency and
resource heterogeneity. This distributed approach enables optimized
microservice deployment, minimizing resource utilization while meeting quality
of service and functional requirements. We introduce Robust Optimization to the
deployment problem to address data uncertainty. Additionally, we model the
Robust Optimization problem as a Partially Observable Markov Decision Process
and propose a robust reinforcement learning algorithm to handle the
semi-infinite Quality of Service constraints. Our approach yields sub-optimal
solutions that minimize accuracy loss while maintaining acceptable
computational costs. Simulation results demonstrate the effectiveness of our
framework.; 54) REALTALK: A 21-Day Real-World Dataset for Long-Term Conversation; Long-term, open-domain dialogue capabilities are essential for chatbots
aiming to recall past interactions and demonstrate emotional intelligence (EI).
Yet, most existing research relies on synthetic, LLM-generated data, leaving
open questions about real-world conversational patterns. To address this gap,
we introduce REALTALK, a 21-day corpus of authentic messaging app dialogues,
providing a direct benchmark against genuine human interactions.
  We first conduct a dataset analysis, focusing on EI attributes and persona
consistency to understand the unique challenges posed by real-world dialogues.
By comparing with LLM-generated conversations, we highlight key differences,
including diverse emotional expressions and variations in persona stability
that synthetic dialogues often fail to capture.
  Building on these insights, we introduce two benchmark tasks: (1) persona
simulation where a model continues a conversation on behalf of a specific user
given prior dialogue context; and (2) memory probing where a model answers
targeted questions requiring long-term memory of past interactions.
  Our findings reveal that models struggle to simulate a user solely from
dialogue history, while fine-tuning on specific user chats improves persona
emulation. Additionally, existing models face significant challenges in
recalling and leveraging long-term context within real-world conversations.; 55) A possible overall scenario for the outburst evolution of MAXI J1820+070
  revealed by Insight-HXMT; We study the spectral and temporal properties of the black hole X-ray
transient binary MAXI J1820+070 during the 2018 outburst with Insight-HXMT
observations. The outburst of MAXI J1820+070 can be divided into three
intervals. For the two intervals of the outburst, we find that low-energy
(below 140 keV) photos lag high-energy (140-170 keV) ones, while in the decay
of the outburst, high-energy photons lag low-energy photons, both with a time
scale of the order of days. Based on these results, the canonical hysteresis
effect of the 'q' shape in the hardness-intensity diagram can be reformed into
a roughly linear shape by taking into account the lag corrections between
different energy bands. Time analysis shows that the high-frequency break of
hard X-rays, derived from the power density spectrum of the first interval of
the outburst is, in general, larger and more variable than that of soft X-rays.
The spectral fitting shows that the coverage fraction of the hard X-rays drops
sharply at the beginning of the outburst to around 0.5, then increases
slightly. The coverage fraction drops to roughly zero once the source steps
into a soft state and increases gradually to unity when the source returns to a
low hard state. We discuss the possible overall evolution scenario of corona
hinted from these discoveries.; 56) Towards Fairness for the Right Reasons: Using Saliency Maps to Evaluate
  Bias Removal in Neural Networks; The widespread adoption of machine learning systems has raised critical
concerns about fairness and bias, making mitigating harmful biases essential
for AI development. In this paper, we investigate the relationship between
fairness improvement and the removal of harmful biases in neural networks
applied to computer vision tasks. First, we introduce a set of novel XAI-based
metrics that analyze saliency maps to assess shifts in a model's
decision-making process. Then, we demonstrate that successful debiasing methods
systematically redirect model focus away from protected attributes.
Additionally, we show that techniques originally developed for artifact removal
can be effectively repurposed for fairness. These findings underscore the
importance of ensuring that models are fair for the right reasons, contributing
to the development of more ethical and trustworthy AI systems.; 57) Low Mach number limit for the compressible Navier-Stokes equation with a
  stationary force; In this paper, we are concerned with the low Mach number limit for the
compressible Navier-Stokes equation with a stationary force and ill-prepared
initial data in the three-dimensional whole space. The convergence result of
the stationary solutions toward corresponding incompressible flow is obtained
when a stationary force is small enough. Under the assumption that the initial
perturbation around the stationary solution is small enough, the convergence
result of the perturbation toward the corresponding perturbation around the
stationary incompressible flow is obtained globally in time. The Strichartz
type estimate for the linearized semigroup around the motionless state plays a
crucial role in the proofs.; 58) Observables for the Effect of Gravity on Electromagnetic Polarization; Does gravity affect the polarization of electromagnetic radiation in an
observable way? The effect of gravity on the observed polarization of a ray of
electromagnetic radiation is investigated for an arbitrary 4-dimensional
spacetime and radiation with a frequency spectrum within the geometric optics
limit and with arbitrary state of polarization. Focusing on effects observable
by a single inertial observer, we show how the presence of curvature along the
null geodesic of polarized electromagnetic radiation may induce observable
changes in the state of polarization. We find a set of scalars that quantify
the effect and derive their transport equations. Two of these scalars, the
polarization degree and the circular polarization degree, are polarization
state observables that are conserved along the radiation geodesic. Four
observables that quantify time rate of change of the observed state of
polarization are identified. These observables and their corresponding
transport equations provide a complete representation of how gravity affects
the observed state of polarization of electromagnetic radiation with
frequencies above the geometric optics limit. Polarization wiggling is sourced
by curvature twist, which is a scalar derived from the Riemann tensor.
Curvature twist is closely related to the magnetic part of the Weyl tensor, the
second Weyl scalar as well as the rotation of the rest frame geodesic
congruence. The results of this paper are valid for any metric theory of
gravity.; 59) Global small data weak solutions of 2-D semilinear wave equations with
  scale-invariant damping; There is an interesting open question: for the $n$-D ($n\ge 1$) semilinear
wave equation with scale-invariant damping $\partial_t^2u-\Delta
u+\frac{\mu}{t}\partial_tu=|u|^p$, where $t\ge 1$, $p>1$, $\mu\in (0,
\bar\mu(n))$ with $\bar\mu(n)=\frac{n^2+n+2}{n+2}$, the global small data weak
solution $u$ will exist when $p>p_{s}(n+\mu)
=\frac{n+\mu+1+\sqrt{(n+\mu)^2+10(n+\mu)-7}}{2(n+\mu-1)}$. In the case of
$n=1$, this open question has been solved well. We now systematically study
this problem for $n=2$ and $\mu\in (0, 1)\cup (1, \bar\mu(2))$ with
$\bar\mu(2)=2$. In the present paper, the global small solution $u$ is
established for $p_{s}(2+\mu)<p<p_{conf}(2,\mu)=\frac{\mu+5}{\mu+1}$ and
$\mu\in(0,1)\cup(1,2)$. Our main ingredients are to derive some new kinds of
spacetime-weighted $L^{q}_tL^{q}_x([1, \infty)\times \mathbb{R}^2)$ or
$L^q_tL^\nu_rL^2_{\theta}([1, \infty)\times [0, \infty)\times [0, 2\pi])$
Strichartz estimates for the solutions of linear generalized Tricomi equation
$\partial_t^2v-t^m\Delta v=F(t,x)$ ($m>0$). In forthcoming paper, we show the
global existence of small solution $u$ for $p\ge p_{conf}(2,\mu)$ and $\mu\in
(0,1)\cup (1, 2)$.; 60) UV LIGHTS. New tools for revealing the low surface brightness regime in
  the ultraviolet; Ultra-deep optical surveys have reached unprecedented depths, facilitating
the study of faint galactic structures. However, the ultraviolet bands, crucial
for stellar population studies, remain essentially unexplored at these depths.
We present a detailed surface brightness and color analysis of 20 nearby
galaxies in the LIGHTS fields observed by GALEX in the FUV and NUV. We adapt
and apply a low surface brightness oriented methodology that has proven
effective in ultra-deep optical surveys. A novel approach to background
subtraction is proposed for UV imaging. Instead of subtracting a constant value
from the background, we subtract a Poisson distribution that transforms the
background into a pseudo-Gaussian distribution centered at zero. Furthermore,
the PSF deconvolution algorithms developed for optical data are applied to our
sample, using a novel set of very extended (R=750 arcsec) PSFs for the GALEX
bands. This methodology allows us to obtain depths ranging from 28.5 to 30 mag
arcsec^{-2}, with reliable surface brightness profiles up to 31 mag
arcsec^{-2}. This is about 1 mag deeper than with standard UV techniques. We
use the surface brightness and color profiles to show that the application of
PSF deconvolution, especially in the FUV, effectively mitigates the excess of
light present in the outer regions of certain galaxies compared to the standard
GALEX pipeline. This finding is crucial for any accurate stellar population
inference from the color profiles. Additionally, a qualitative analysis of the
results is presented, with particular emphasis on surface brightness and color
properties of the galaxies beyond their optical edges. Our work highlights the
importance of developing innovative low surface brightness methods for UV
surveys.; 61) The role of dry mergers in shaping the scaling relations of galaxies; In the context of the hierarchical formation of galaxies, we investigated the
role played by mergers in shaping the scaling relations of galaxies, that is
the projections of their Fundamental Plane onto the Ie-Re, Ie-Sigma, Ms-Re and
L-Sigma planes. To this aim, based on the scalar Virial Theorem, we developed a
simple theory of multiple dry mergers to read both the large scale simulations
and the companion scaling relations. The aim was to compare the results of this
approach with the observational data and with two of the most recent and
detailed numerical cosmo-hydro-dynamical simulations, that is Illustris-TNG and
EAGLE (Evolution and Assembly of GaLaxies and their Environments). We derived
the above scaling relations for the galaxies of the MaNGA (Mapping Nearby
Galaxies at APO) and WINGS (Wide-field Imaging of Nearby Galaxy-Clusters
Survey) databases and compared them with the observational data, the numerical
simulations, and the results of our simple theory of dry mergers. The multiple
dry merging mechanism is able to explain all the main characteristics of the
observed scaling relations of galaxies, such as slopes, scatters, curvatures
and zones of exclusion. The distribution of galaxies in these planes is
continuously changing across time because of the merging activity and other
physical processes, such as star formation, quenching, energy feedback, and so
forth. The simple merger theory presented here yields the correct distribution
of galaxies in the main scaling relations at all cosmic epochs. The precision
is comparable with that obtained by the modern cosmo-hydro-dynamical
simulations, with the advantage of providing a rapid exploratory response on
the consequences engendered by different physical effects.; 62) Automating High Quality RT Planning at Scale; Radiotherapy (RT) planning is complex, subjective, and time-intensive.
Advances in artificial intelligence (AI) promise to improve its precision,
efficiency, and consistency, but progress is often limited by the scarcity of
large, standardized datasets. To address this, we introduce the Automated
Iterative RT Planning (AIRTP) system, a scalable solution for generating
high-quality treatment plans. This scalable solution is designed to generate
substantial volumes of consistently high-quality treatment plans, overcoming a
key obstacle in the advancement of AI-driven RT planning. Our AIRTP pipeline
adheres to clinical guidelines and automates essential steps, including
organ-at-risk (OAR) contouring, helper structure creation, beam setup,
optimization, and plan quality improvement, using AI integrated with RT
planning software like Eclipse of Varian. Furthermore, a novel approach for
determining optimization parameters to reproduce 3D dose distributions, i.e. a
method to convert dose predictions to deliverable treatment plans constrained
by machine limitations. A comparative analysis of plan quality reveals that our
automated pipeline produces treatment plans of quality comparable to those
generated manually, which traditionally require several hours of labor per
plan. Committed to public research, the first data release of our AIRTP
pipeline includes nine cohorts covering head-and-neck and lung cancer sites to
support an AAPM 2025 challenge. This data set features more than 10 times the
number of plans compared to the largest existing well-curated public data set
to our best knowledge. Repo:
https://github.com/RiqiangGao/GDP-HMM_AAPMChallenge.; 63) Monotonicity of the Relative Entropy and the Two-sided Bogoliubov
  Inequality in von Neumann Algebras; This text studies, on the one hand, certain monotonicity properties of the
Araki-Uhlmann relative entropy and, on the other hand, unbounded perturbation
theory of KMS-states which facilitates a proof of the two-sided Bogoliubov
inequality in general von Neumann algebras. After introducing the necessary
background from the theory of operator algebras and Tomita-Takesaki modular
theory, the relative entropy functional is defined and its basic properties are
studied. In particular, a full and detailed proof of Uhlmann's important
monotonicity theorem for the relative entropy is provided. This theorem will
then be used to derive a number of monotonicity inequalities for the relative
entropy of normal functionals induced by vectors of the form $V \varOmega, V
\varPhi \in \mathcal{H}$, where $V \in \mathscr{B}(\mathcal{H})$ is a suitable
transformation. After that, an introduction to perturbation theory in von
Neumann algebras is given, with an emphasis on unbounded perturbations of
KMS-states following the framework of Derezi\'{n}ski-Jak\v{s}i\'{c}-Pillet.
This mathematical apparatus will then be used to extend the two-sided
Bogoliubov inequality for the relative free energy, which was very recently
proved for quantum-mechanical systems, to arbitrary von Neumann algebras.; 64) WaveFM: A High-Fidelity and Efficient Vocoder Based on Flow Matching; Flow matching offers a robust and stable approach to training diffusion
models. However, directly applying flow matching to neural vocoders can result
in subpar audio quality. In this work, we present WaveFM, a reparameterized
flow matching model for mel-spectrogram conditioned speech synthesis, designed
to enhance both sample quality and generation speed for diffusion vocoders.
Since mel-spectrograms represent the energy distribution of waveforms, WaveFM
adopts a mel-conditioned prior distribution instead of a standard Gaussian
prior to minimize unnecessary transportation costs during synthesis. Moreover,
while most diffusion vocoders rely on a single loss function, we argue that
incorporating auxiliary losses, including a refined multi-resolution STFT loss,
can further improve audio quality. To speed up inference without degrading
sample quality significantly, we introduce a tailored consistency distillation
method for WaveFM. Experiment results demonstrate that our model achieves
superior performance in both quality and efficiency compared to previous
diffusion vocoders, while enabling waveform generation in a single inference
step.; 65) Structure of gaps induced by retrograde satellites embedded in accretion
  discs; Using 2D simulations, we investigate how a non-accreting satellite on a fixed
retrograde circular orbit affects the structure of the accretion disc in which
it is embedded. We vary the satellite-to-primary mass ratio $q$, the disc
viscosity $\nu$, and the inner boundary conditions. A viscous criterion for gap
opening is derived, which is broadly consistent with the simulations. We find a
scaling relation of the gap depth with $q$ and $\nu$. Unlike the prograde case,
the satellite is located at the gap's inner edge, resulting in a surface
density at the satellite's orbital radius up to $20$ times higher than at the
gap's minimum. As the viscosity decreases, the gap depth increases, while the
radial shift of the gap and the satellite's orbital radius decreases.
Gap-opening satellites may drive radial motions in the disc, producing
eccentric gaps. Positioned at the gap edge, satellites experience a rapidly
fluctuating environment. Migrating satellites can develop orbital
eccentricities comparable to the disc's aspect ratio. In a 3D simulation with
$q=0.01$, the flow velocity exhibits a notorious vertical component in the
gap's inner edge. A comparison between 2D and 3D simulations reveals a slight
radial offset in gap position, resulting in a lower surface density at the
perturber's orbital radius in the 3D simulation.; 66) A higher algebraic approach to liftings of modules over derived
  quotients; We show a certain existence of a lifting of modules under the
self-$\mathrm{Ext}^2$-vanishing condition over the ""derived quotient"" by using
the notion of higher algebra. This refines a work of Auslander--Ding--Solberg's
solution of the Auslander--Reiten conjecture for complete interesctions.
Together with Auslander's zero-divisor theorem, we show that the existence of
such $\mathrm{Ext}$-vanishing module over derived quotients is equivalent to
being local complete intersections.; 67) Partially connected contributions to baryon masses in QCD+QED; Full QCD+QED simulations allow to evaluate isospin breaking corrections to
hadron masses. With the openQxD code, we are able to perform these simulations
employing C-periodic boundary conditions, implemented through a doubling of the
physical lattice along one spatial direction. The use of these boundary
conditions introduces non-zero Wick contractions between two quark or two
antiquark fields, that, in the case of the computation of baryon masses, lead
to partially connected additional contributions that we expect to vanish in the
infinite volume limit. These contributions are challenging because they involve
an all-to-all propagator connecting one point in the physical lattice and one
in the mirror lattice. We present a way to compute these corrections to the
$\Omega^-$ baryon mass using a combination of point and stochastic source
inversions. This work is part of the program of the RC* collaboration.; 68) Polynomial invariants for two-dimensional algebras; We classify all two-dimensional simple algebras over an algebraically closed
field. For each two-dimensional algebra $\mathcal{A}$ with an infinite group of
automorphisms we describe a minimal (with respect to inclusion) generating set
for the algebra of invariants of the $m$-tuples of $\mathcal{A}$ in
characteristic zero case. As a consequence, we show that in characteristic zero
case Artin-Procesi-Iltyakov Equality holds for all two-dimensional simple
algebras with an infinite group of automorphisms. We also consider
nondegenerate invariant bilinear forms over two-dimensional algebras.; 69) Automatic selection of the best neural architecture for time series
  forecasting via multi-objective optimization and Pareto optimality conditions; Time series forecasting plays a pivotal role in a wide range of applications,
including weather prediction, healthcare, structural health monitoring,
predictive maintenance, energy systems, and financial markets. While models
such as LSTM, GRU, Transformers, and State-Space Models (SSMs) have become
standard tools in this domain, selecting the optimal architecture remains a
challenge. Performance comparisons often depend on evaluation metrics and the
datasets under analysis, making the choice of a universally optimal model
controversial. In this work, we introduce a flexible automated framework for
time series forecasting that systematically designs and evaluates diverse
network architectures by integrating LSTM, GRU, multi-head Attention, and SSM
blocks. Using a multi-objective optimization approach, our framework determines
the number, sequence, and combination of blocks to align with specific
requirements and evaluation objectives. From the resulting Pareto-optimal
architectures, the best model for a given context is selected via a
user-defined preference function. We validate our framework across four
distinct real-world applications. Results show that a single-layer GRU or LSTM
is usually optimal when minimizing training time alone. However, when
maximizing accuracy or balancing multiple objectives, the best architectures
are often composite designs incorporating multiple block types in specific
configurations. By employing a weighted preference function, users can resolve
trade-offs between objectives, revealing novel, context-specific optimal
architectures. Our findings underscore that no single neural architecture is
universally optimal for time series forecasting. Instead, the best-performing
model emerges as a data-driven composite architecture tailored to user-defined
criteria and evaluation objectives.; 70) Quantifying the Impact of the Dust Torque on the Migration of Low-mass
  Planets II: The Role of Pebble Accretion in Planet Growth within a Global
  Planet Formation Model; Although dust constitutes only about 1% of the mass of a protoplanetary disk,
recent studies demonstrate that it can exert a significant torque on low- and
intermediate-mass planetary cores. We compute and quantify for the first time
the influence of the dust torque on the evolution of growing planetary embryos
as they move in a protoplanetary disk while growing via gas and pebble
accretion. Our global model evolves the gaseous disk via viscous accretion and
X-ray photoevaporation, while accounting for dust growth and evolution
including coagulation, drift, and fragmentation. Our research indicates that
dust torque significantly influences planetary migration, particularly driving
substantial outward migration for planets forming within the water ice-line.
This effect occurs due to an increased dust-to-gas mass ratio in the inner
disk, resulting from inward pebble drift from outer regions. In contrast, for
planets initially located beyond the water ice-line, the dust torque mitigates
inward migration but does not significantly alter their paths, as the
dust-to-gas ratio diminishes rapidly due to rapid pebble drift and the brief
timescales of planet formation in these areas. These findings underscore the
pivotal role of dust torque in shaping the migration patterns of low- and
intermediate-mass planets, especially when enhanced dust concentrations in the
inner disk amplify its effects; 71) Another Mar\v{c}enko-Pastur law for Kendall's tau; Bandeira et al. (2017) show that the eigenvalues of the Kendall correlation
matrix of $n$ i.i.d. random vectors in $\mathbb{R}^p$ are asymptotically
distributed like $1/3 + (2/3)Y_q$, where $Y_q$ has a Mar\v{c}enko-Pastur law
with parameter $q=\lim(p/n)$ if $p, n\to\infty$ proportionately to one another.
Here we show that another Mar\v{c}enko-Pastur law emerges in the ""ultra-high
dimensional"" scaling limit where $p\sim q'\, n^2/2$ for some $q'>0$: in this
quadratic scaling regime, Kendall correlation eigenvalues converge weakly
almost surely to $(1/3)Y_{q'}$.; 72) Tracking Down Software Cluster Bombs: A Current State Analysis of the
  Free/Libre and Open Source Software (FLOSS) Ecosystem; Throughout computer history, it has been repeatedly demonstrated that
critical software vulnerabilities can significantly affect the components
involved. In the Free/Libre and Open Source Software (FLOSS) ecosystem, most
software is distributed through package repositories. Nowadays, monitoring
critical dependencies in a software system is essential for maintaining robust
security practices. This is particularly important due to new legal
requirements, such as the European Cyber Resilience Act, which necessitate that
software projects maintain a transparent track record with Software Bill of
Materials (SBOM) and ensure a good overall state. This study provides a summary
of the current state of available FLOSS package repositories and addresses the
challenge of identifying problematic areas within a software ecosystem. These
areas are analyzed in detail, quantifying the current state of the FLOSS
ecosystem. The results indicate that while there are well-maintained projects
within the FLOSS ecosystem, there are also high-impact projects that are
susceptible to supply chain attacks. This study proposes a method for analyzing
the current state and identifies missing elements, such as interfaces, for
future research.; 73) Discovering the influence of personal features in psychological
  processes using Artificial Intelligence techniques: the case of COVID19
  lockdown in Spain; At the end of 2019, an outbreak of a novel coronavirus was reported in China,
leading to the COVID-19 pandemic. In Spain, the first cases were detected in
late January 2020, and by mid-March, infections had surpassed 5,000. On March
the Spanish government started a nationwide lockdown to contain the spread of
the virus. While isolation measures were necessary, they posed significant
psychological and socioeconomic challenges, particularly for vulnerable
populations. Understanding the psychological impact of lockdown and the factors
influencing mental health is crucial for informing future public health
policies. This study analyzes the influence of personal, socioeconomic, general
health and living condition factors on psychological states during lockdown
using AI techniques. A dataset collected through an online questionnaire was
processed using two workflows, each structured into three stages. First,
individuals were categorized based on psychological assessments, either
directly or in combination with unsupervised learning techniques. Second,
various Machine Learning classifiers were trained to distinguish between the
identified groups. Finally, feature importance analysis was conducted to
identify the most influential variables related to different psychological
conditions. The evaluated models demonstrated strong performance, with accuracy
exceeding 80% and often surpassing 90%, particularly for Random Forest,
Decision Trees, and Support Vector Machines. Sensitivity and specificity
analyses revealed that models performed well across different psychological
conditions, with the health impacts subset showing the highest reliability. For
diagnosing vulnerability, models achieved over 90% accuracy, except for less
vulnerable individuals using living environment and economic status features,
where performance was slightly lower.; 74) Connectivity of Coxeter group Morse boundaries; We study the connectivity of Morse boundaries of Coxeter groups. We define
two conditions on the defining graph of a Coxeter group: wide-avoidant and
wide-spherical-avoidant. We show that wide-spherical-avoidant, one-ended,
affine-free Coxeter groups have connected and locally connected Morse
boundaries. On the other hand, one-ended Coxeter groups that are not
wide-avoidant and not wide have disconnected Morse boundary. For the
right-angled case, we get a full characterization: a one-ended right-angled
Coxeter group has connected, non-empty Morse boundary if and only if it is
wide-avoidant. Along the way we characterize Morse geodesic rays in affine-free
Coxeter groups as those that spend uniformly bounded time in cosets of wide
special subgroups.; 75) InsightVision: A Comprehensive, Multi-Level Chinese-based Benchmark for
  Evaluating Implicit Visual Semantics in Large Vision Language Models; In the evolving landscape of multimodal language models, understanding the
nuanced meanings conveyed through visual cues - such as satire, insult, or
critique - remains a significant challenge. Existing evaluation benchmarks
primarily focus on direct tasks like image captioning or are limited to a
narrow set of categories, such as humor or satire, for deep semantic
understanding. To address this gap, we introduce, for the first time, a
comprehensive, multi-level Chinese-based benchmark designed specifically for
evaluating the understanding of implicit meanings in images. This benchmark is
systematically categorized into four subtasks: surface-level content
understanding, symbolic meaning interpretation, background knowledge
comprehension, and implicit meaning comprehension. We propose an innovative
semi-automatic method for constructing datasets, adhering to established
construction protocols. Using this benchmark, we evaluate 15 open-source large
vision language models (LVLMs) and GPT-4o, revealing that even the
best-performing model lags behind human performance by nearly 14% in
understanding implicit meaning. Our findings underscore the intrinsic
challenges current LVLMs face in grasping nuanced visual semantics,
highlighting significant opportunities for future research and development in
this domain. We will publicly release our InsightVision dataset, code upon
acceptance of the paper.; 76) Reviving networked multi-dimensional dynamical systems; From gene regulatory networks to mutualistic networks, controlling a single
node in the network topology can transform these complex dynamical systems from
undesirable states to desirable ones. Corresponding methods have been
well-studied in one-dimensional dynamical systems. However, many practical
dynamical systems require description by multi-dimensional dynamical systems,
such as the mutualistic symbiotic systems formed by flowering plants and
pollinating insects. Existing one-dimensional methods cannot handle the cases
of multi-dimensional dynamical systems. Based on this, we propose a method to
control a single node to activate network connections in multi-dimensional
dynamical systems. In such systems, the changes of each node are described by
multiple nonlinear differential equations. All remaining nodes are stratified
according to the shortest path to the controlled node, thereby reducing the
dimensionality of the system. Such a large-scale dynamical system can
ultimately be replaced by a very simple system. By analyzing the
reduced-dimensional system, we can predict the extent of control needed to
restore the system state. We apply this method to a wide range of fields,
achieving activation of various real multidimensional complex dynamical
systems.; 77) Adaptive Bi-Level Multi-Robot Task Allocation and Learning under
  Uncertainty with Temporal Logic Constraints; This work addresses the problem of multi-robot coordination under unknown
robot transition models, ensuring that tasks specified by Time Window Temporal
Logic are satisfied with user-defined probability thresholds. We present a
bi-level framework that integrates (i) high-level task allocation, where tasks
are assigned based on the robots' estimated task completion probabilities and
expected rewards, and (ii) low-level distributed policy learning and execution,
where robots independently optimize auxiliary rewards while fulfilling their
assigned tasks. To handle uncertainty in robot dynamics, our approach leverages
real-time task execution data to iteratively refine expected task completion
probabilities and rewards, enabling adaptive task allocation without explicit
robot transition models. We theoretically validate the proposed algorithm,
demonstrating that the task assignments meet the desired probability thresholds
with high confidence. Finally, we demonstrate the effectiveness of our
framework through comprehensive simulations.; 78) Numerical computation of the Schwarz function; An analytic function can be continued across an analytic arc $\Gamma$ with
the help of the Schwarz function $S(z)$, the analytic function satisfying $S(z)
= \bar z$ for $z\in \Gamma$. We show how $S(z)$ can be computed with the AAA
algorithm of rational approximation, an operation that is the basis of the
AAALS method for solution of Laplace and related PDE problems in the plane. We
discuss the challenge of computing $S(z)$ further away from from $\Gamma$,
where it becomes multi-valued.; 79) Bosonic M-Theory From a Kac-Moody Algebra Perspective; We study the existence of a bosonic m-theory extension of the 10D and 26D
closed bosonic string in terms of Kac-Moody algebras. We argue that K11 and K27
are symmetries which protect the coefficients of the closed bosonic string in
10 and 26 dimensions. Therefore the Susskind-Horowitz bosonic m-theory obtained
by compactification on S1/Z2, which does not produce the correct coefficients,
must be replaced by something that preserves K11 and K27. We argue that in 11D,
a non-trivial bosonic m-theory should be considered as (the bosonic sector of)
m-theory, and in 27D that no obvious bosonic m-theory exists.; 80) $NJ/\psi$ and $N\eta_c$ interactions from lattice QCD; The interaction between nucleon and charmonia ($J/\psi$ and $\eta_c$) is
expected to deepen our understanding of various aspects in nonperturbative QCD
ranging from the origin of nucleon mass to $J/\psi$ mass modification in
nuclear medium and properties of hidden-charm pentaquark states. Here, we
present the low-energy $NJ/\psi$ and $N\eta_c$ interactions based on ($2+1$)
flavor lattice QCD simulations with nearly physical pion mass $m_\pi=146$ MeV.
The interactions, extracted from the spacetime correlations of the nucleon and
charmonium system by using the HAL QCD method, are found to be attractive in
all distances and manifest a characteristic long-range tail consistent with the
two-pion exchange interaction. The resulting scattering lengths are around
$0.3$ fm, $0.4$ fm and $0.2$ fm for $NJ/\psi$ with spin $3/2$, with spin $1/2$,
and $N\eta_c$, respectively. Our results are orders of magnitude larger than
those from the photoproduction experiments assuming the vector meson dominance.; 81) Scoring Verifiers: Evaluating Synthetic Verification in Code and
  Reasoning; Code verification has recently found great success as a critical component in
training large scale reasoning models for coding. Synthetic techniques such as
self-generated test cases and reward models provide a way to enhance code
capabilities beyond predefined tests. Building on these advancements, we
propose new benchmarks designed to systematically evaluate the impact of
synthetic verification methods on assessing solution correctness. We introduce
HE-R, HE-R+, MBPP-R, and MBPP-R+, which transform existing coding benchmarks
into scoring and ranking datasets to evaluate the effectiveness of synthetic
verifiers. Using these benchmarks, we analyze synthetic verification methods in
standard, reasoning-based, and reward-based LLMs. Our results show that recent
reasoning models significantly improve test case generation and that scaling
test cases enhances verification accuracy.; 82) High-velocity outflows in [OIII] emitters at z=2.5-9 from JWST NIRSpec
  medium-resolution spectroscopy; We identify galaxies hosting ionised-gas high-velocity outflows from the
complete sample of medium resolution (R1000) JWST/NIRSpec MSA spectroscopy
taken as part of the JWST Advanced Deep Extragalactic Survey (JADES). From a
total sample of 1087 [OIII]5007 emitters we identify 40 galaxies with a
blue-side broadened [OIII]5007 line at z=2.5-9. Of these, 34 are strong outflow
candidates whilst 6 sources have broadening potentially driven by rotating
clumps. Our outflow candidate sample is mainly composed of star-forming
galaxies, including ~65% starbursts, which span the stellar mass range
log10(M*/Msun)=7.5-11.0. It also includes two candidate type-2 active galactic
nuclei (AGN) and a 'little red dot' (LRD). We report a median outflow velocity
of 531(+146)(-159) km/s and an overall incidence rate of 3.4%. These values are
significantly higher and lower respectively than recent similar works, which we
accredit to the limiting resolution of the R1000 spectroscopy and a stricter
outflow selection criterion. We find no correlation between the outflow
velocity and the galaxy stellar mass or star-formation rate. The median ratio
between outflow velocity and escape velocity is 0.77(+0.36)(-0.32), indicating
that most outflows cannot escape the galaxy gravitational potentials. We do
find an anti-correlation between mass loading factor and stellar mass up to M*
~(10^10)Msun, with most lowest stellar-mass M*<(10^9)Msun galaxies reaching
values well above unity, as is the case for local starburst galaxies.; 83) Integration of Machine Learning-Based Plasma Acceleration Simulations
  into Geant4: A Case Study with the PALLAS Experiment; We present the development and integration of a Machine Learning (ML)-based
surrogate model, trained on Particle-In-Cell (PIC) simulations of laser-driven
plasma wakefield acceleration source of electrons, into Geant4 simulation
toolkit. Our model enables the generation and tracking of plasma-accelerated
beams within complete experimental setups, unifying plasma acceleration and
Monte Carlo-based simulations, which significantly reduces their complexity and
computational cost.
  Our implementation focuses on the PALLAS laser-plasma accelerator test
facility, integrating its full experimental setup into Geant4. We describe the
ML model, its integration into Geant4, and key simulation results,
demonstrating the feasibility of start-to-end simulations of plasma
acceleration facilities and applications within a unified framework.; 84) On the origin of bulk-related anisotropies in surface optical spectra; Reflection anisotropy spectroscopy (RAS) is a powerful method for probing the
optical properties of surfaces, used routinely in research and industrial
applications, yet the origin of 'bulk-related' features that appear in the
spectra of various surfaces has been debated for nearly 40 years. It is often
argued that these features are related to surface-induced bulk anisotropy
(SIBA) because they coincide with critical energies of the bulk dielectric
function. In general, any quantitative RAS theory must include excitonic
effects as they significantly influence the spectra and are believed to be the
key to determining the origin of SIBA features. Here, we introduce a
layer-resolved exciton localization (LREL) measure within the framework of
many-body perturbation theory, which enables a quantitative analysis of the
origins of 'bulk-related' RAS features. Applying LREL to arsenic-modified
silicon reconstructions reveals that, depending on the surface reconstruction,
the 'apparent' SIBA features arise primarily from states localized at the
surface, with only a small contribution from the underlying layers. Our
findings, further supported by the fact that the calculated spectra agree well
with low-temperature RAS measurements, challenge the conventional explanation
of 'bulk-related' RAS features. They indicate that in many instances
bulk-enhanced surface anisotropies (BESA)-the opposite of SIBA-contribute to,
or are even responsible for, 'bulk-related' RAS features. Therefore, we suggest
that previously studied semiconductor surfaces, which exhibit 'bulk-related'
features in their spectra, should be reanalyzed using the presented method.; 85) Digital Guardians: Can GPT-4, Perspective API, and Moderation API
  reliably detect hate speech in reader comments of German online newspapers?; In recent years, toxic content and hate speech have become widespread
phenomena on the internet. Moderators of online newspapers and forums are now
required, partly due to legal regulations, to carefully review and, if
necessary, delete reader comments. This is a labor-intensive process. Some
providers of large language models already offer solutions for automated hate
speech detection or the identification of toxic content. These include GPT-4o
from OpenAI, Jigsaw's (Google) Perspective API, and OpenAI's Moderation API.
Based on the selected German test dataset HOCON34k, which was specifically
created for developing tools to detect hate speech in reader comments of online
newspapers, these solutions are compared with each other and against the
HOCON34k baseline. The test dataset contains 1,592 annotated text samples. For
GPT-4o, three different promptings are used, employing a Zero-Shot, One-Shot,
and Few-Shot approach. The results of the experiments demonstrate that GPT-4o
outperforms both the Perspective API and the Moderation API, and exceeds the
HOCON34k baseline by approximately 5 percentage points, as measured by a
combined metric of MCC and F2-score.; 86) Removal of radon progeny from delicate surfaces; $\rm ^{210}Po$ $\alpha$-decay driven neutron background is a concern for many
low-energy rare event experiments. It is a difficult-to-control background that
depends on the air exposure history of parts. In this study we demonstrate that
about half of the radon progeny $\rm ^{210}Po$ can be removed from copper and
silicon surfaces by wiping it with an acetone wetted tissue. For a copper
sample we demonstrate that $\rm ^{210}Pb$ is removed with similar
effectiveness. Additional wiping was found to be largely ineffective.; 87) Learning Bayesian Game Families, with Application to Mechanism Design; Learning or estimating game models from data typically entails inducing
separate models for each setting, even if the games are parametrically related.
In empirical mechanism design, for example, this approach requires learning a
new game model for each candidate setting of the mechanism parameter. Recent
work has shown the data efficiency benefits of learning a single parameterized
model for families of related games. In Bayesian games - a typical model for
mechanism design - payoffs depend on both the actions and types of the players.
We show how to exploit this structure by learning an interim game-family model
that conditions on a single player's type. We compare this approach to the
baseline approach of directly learning the ex ante payoff function, which gives
payoffs in expectation of all player types. By marginalizing over player type,
the interim model can also provide ex ante payoff predictions. This dual
capability not only facilitates Bayes-Nash equilibrium approximation, but also
enables new types of analysis using the conditional model. We validate our
method through a case study of a dynamic sponsored search auction. In our
experiments, the interim model more reliably approximates equilibria than the
ex ante model and exhibits effective parameter extrapolation. With local search
over the parameter space, the learned game-family model can be used for
mechanism design. Finally, without any additional sample data, we leverage the
interim model to compute piecewise best-response strategies and refine our
model to incorporate these strategies, enabling an iterative approach to
empirical mechanism design.; 88) Structure evolution with cosmic backgrounds from radio to far infrared; Cosmic background radiation, both diffuse and discrete in nature, produced at
different cosmic epochs before and after recombination, provides key
information on the evolution of cosmic structures. We discuss the main classes
of sources that contribute to the extragalactic background light from radio to
sub-millimetre wavelenghs and the currently open question on the level of the
cosmic radio background spectrum. The redshifted 21cm line signal from
cosmological neutral Hydrogen during the primeval phases of cosmic structures
as a probe of the cosmological reionisation process is presented, along with
the route for confident detection of this signal. We then describe the basic
formalism and the feasibility to study via a differential approach, based
mainly on dipole analysis, the tiny imprints in the CB spectrum expected from a
variety of cosmological and astrophysical processes at work during the early
phases of cosmic perturbation and structure evolution. Finally, we discuss the
identification of high-redshift sub-millimetre lensed galaxies with extreme
magnifications in the Planck maps and their use for the comprehension of
fundamental processes in early galaxy formation and evolution.; 89) Line detections in photospheric radius expansion bursts from 4U 1820-303; Context: NICER (Neutron star Interior Composition ExploreR) is the instrument
of choice for the spectral analysis of type I X-ray bursts, as it provides high
throughput at X-ray CCD resolution, down to 0.3 keV. Aims: This study
investigates whether the energies of absorption lines detected in photospheric
radius expansion (PRE) bursts correlate with the inferred blackbody radius.
Previous reports suggested such a correlation, attributed to a combination of
weaker gravitational redshift and higher blueshifts in bursts with larger
radii. Methods: The analysis reexamines four previously studied PRE bursts and
examines eight additional bursts from 4U 1820-303, evidencing PRE. Spectral
evolution is tracked on the shortest possible timescales (tenth of a second)
adopting two parallel continuum descriptions to characterise the photospheric
expansion and line evolution. Applying the accretion-enhanced model, maximum
blackbody radii of up to $\sim$ 900 km are inferred, with peak bolometric
luminosities exceeding the Eddington limit of an Helium accretor. Absorption
lines are assessed for significance using Monte Carlo simulations, and spectral
lines are characterised using the state-of-art plasma codes available within
{\sc{spex}} with a phenomenological continuum. A thorough parameter search
explores Doppler shifts to avoid local minima. Results: Several significant (>
99.9%) absorption lines, including the previously reported 2.97 keV line, are
detected. While no consistent correlation between line energies and blackbody
radii is confirmed, bursts with larger radii exhibit up to four lines and the
line strength is higher. The modelling suggests that the observed lines mostly
originate from slightly redshifted (almost rest-frame) photo-/collisionally
ionised gas in emission. For the burst with the largest PRE, a combination of
photo-ionised plasma in both emission and absorption is preferred.; 90) Multi-Symmetric Schur Functions; We study a multi-symmetric generalization of the classical Schur functions
called the multi-symmetric Schur functions. These functions form an integral
basis for the ring of multi-symmetric functions indexed by tuples of partitions
and are defined as certain stable-limits of key polynomials. We prove
combinatorial results about the monomial expansions of the multi-symmetric
Schur functions including a diagrammatic combinatorial formula and a
triangularity result which completely characterizes their monomial
multi-symmetric supports. The triangularity result involves a non-trivial
generalization of the dominance order on partitions to tuples of partitions. We
prove, using the Demazure character formula, that the multi-symmetric Schur
functions expand positively into the basis of tensor products of ordinary Schur
functions and describe the expansion coefficients as multiplicities of certain
irreducible representations for Levi subgroups inside particular Demazure
modules. Lastly, we find a family of multi-symmetric plethystic operators
related to the classical Bernstein operators which act on the multi-symmetric
Schur basis by a simple recurrence relation.; 91) Vector mesons from a holographic QCD model in $f(R)$-dilaton gravity; In this paper we investigate the spectrum of vector mesons based on
$f(R)$-dilaton gravity. We focus particularly on the well-known Starobinsky
model, given by the function $f(R)= R+ \alpha R^2$ in the metric formalism, and
we examine the deviations from the pure Einstein-dilaton case. We thus provide
the field equations for Starobinsky-dilaton gravity in $\rm AdS_{5}$ spacetime
and calculate the spectral decomposition of the vector mesons by means of a
Sturm-Liouville problem. Remarkably, for the rho mesons $m_{\rho^2}$ and
$m_{\rho^3}$, our results are in good agreement with the experimental data when
$\alpha= 10^{-8}$. Our work also generalizes previous studies and recovers
standard results when $\alpha= 0$.; 92) AI Agents in Cryptoland: Practical Attacks and No Silver Bullet; The integration of AI agents with Web3 ecosystems harnesses their
complementary potential for autonomy and openness, yet also introduces
underexplored security risks, as these agents dynamically interact with
financial protocols and immutable smart contracts. This paper investigates the
vulnerabilities of AI agents within blockchain-based financial ecosystems when
exposed to adversarial threats in real-world scenarios. We introduce the
concept of context manipulation -- a comprehensive attack vector that exploits
unprotected context surfaces, including input channels, memory modules, and
external data feeds. Through empirical analysis of ElizaOS, a decentralized AI
agent framework for automated Web3 operations, we demonstrate how adversaries
can manipulate context by injecting malicious instructions into prompts or
historical interaction records, leading to unintended asset transfers and
protocol violations which could be financially devastating. Our findings
indicate that prompt-based defenses are insufficient, as malicious inputs can
corrupt an agent's stored context, creating cascading vulnerabilities across
interactions and platforms. This research highlights the urgent need to develop
AI agents that are both secure and fiduciarily responsible.; 93) An Energy-Aware RIoT System: Analysis, Modeling and Prediction in the
  SUPERIOT Framework; This paper presents a comprehensive analysis of the energy consumption
characteristics of a Silicon (Si)-based Reconfigurable IoT (RIoT) node
developed in the initial phase of the SUPERIOT project, focusing on key
operating states, including Bluetooth Low Energy (BLE) communication,
Narrow-Band Visible Light Communication (NBVLC), sensing, and E-ink display.
Extensive measurements were conducted to establish a detailed energy profile,
which serves as a benchmark for evaluating the effectiveness of subsequent
optimizations and future node iterations. To minimize the energy consumption,
multiple optimizations were implemented at both the software and hardware
levels, achieving a reduction of over 60% in total energy usage through
software modifications alone. Further improvements were realized by optimizing
the E-ink display driving waveform and implementing a very low-power mode for
non-communication activities. Based on the measured data, three
measurement-based energy consumption models were developed to characterize the
energy behavior of the node under: (i) normal, unoptimized operation, (ii)
low-power, software-optimized operation, and (iii) very low-power,
hardware-optimized operation. These models, validated with new measurement
data, achieved an accuracy exceeding 97%, confirming their reliability for
predicting energy consumption in diverse configurations.; 94) Matched pairs of actions on the Kac-Paljutkin algebra $H_8$; The notion of matched pair of actions on a Hopf algebra generalizes the
braided group construction of Lu, Yan and Zhu, and efficiently provides
Yang-Baxter operators. In this paper, we classify matched pairs of actions on
the Kac-Paljutkin Hopf algebra $H_8$. Through calculations, we obtain 6 matched
pairs of actions on $H_8$. Based on such a classification result, we find that
four of them can be derived from the coquasitriangular structures of $H_8$,
while the other two can not. Furthermore, we discover that the Yang-Baxter
operators associated to exactly these two distinguished matched pairs of
actions are involutive.; 95) Sketch-of-Thought: Efficient LLM Reasoning with Adaptive
  Cognitive-Inspired Sketching; Recent advances in large language models have demonstrated remarkable
reasoning capabilities through Chain of Thought (CoT) prompting, but often at
the cost of excessive verbosity in their intermediate outputs, which increases
computational overhead. We introduce Sketch-of-Thought (SoT), a novel prompting
framework that combines cognitive-inspired reasoning paradigms with linguistic
constraints to minimize token usage while preserving reasoning accuracy. SoT is
designed as a flexible framework that can incorporate any custom reasoning
paradigms based on cognitive science, and we instantiate it with three such
paradigms - Conceptual Chaining, Chunked Symbolism, and Expert Lexicons - each
tailored to different reasoning tasks and selected dynamically via a
lightweight routing model. Through comprehensive evaluation across 15 reasoning
datasets with multiple languages and multimodal scenarios, we demonstrate that
SoT achieves token reductions of 76% with negligible accuracy impact. In
certain domains like mathematical and multi-hop reasoning, it even improves
accuracy while using significantly fewer tokens. Our code is publicly
available: https://www.github.com/SimonAytes/SoT.; 96) AIM2PC: Aerial Image to 3D Building Point Cloud Reconstruction; Three-dimensional urban reconstruction of buildings from single-view images
has attracted significant attention over the past two decades. However, recent
methods primarily focus on rooftops from aerial images, often overlooking
essential geometrical details. Additionally, there is a notable lack of
datasets containing complete 3D point clouds for entire buildings, along with
challenges in obtaining reliable camera pose information for aerial images.
This paper addresses these challenges by presenting a novel methodology, AIM2PC
, which utilizes our generated dataset that includes complete 3D point clouds
and determined camera poses. Our approach takes features from a single aerial
image as input and concatenates them with essential additional conditions, such
as binary masks and Sobel edge maps, to enable more edge-aware reconstruction.
By incorporating a point cloud diffusion model based on Centered denoising
Diffusion Probabilistic Models (CDPM), we project these concatenated features
onto the partially denoised point cloud using our camera poses at each
diffusion step. The proposed method is able to reconstruct the complete 3D
building point cloud, including wall information and demonstrates superior
performance compared to existing baseline techniques. To allow further
comparisons with our methodology the dataset has been made available at
https://github.com/Soulaimene/AIM2PCDataset; 97) What happens due to the baby universe effect in JT gravity? -- Analysis
  of correlation functions and ERB length at late time using three approaches; We analyze the correlation function in JT gravity using three approaches: by
summing over all geodesics connecting boundary operators, integrating over the
region of moduli space determined by the ``no-shortcut condition'' introduced
by D.Stanford and Z.Yang, and using the formula for the universal spectral
density correlation in the $\tau$-scaling limit. We find that the behaviors of
the three results coincide at late times: they all exhibit a ``ramp'' instead
of permanent decay. Using the third approach we also confirm that the
``plateau'' appears after $T_H=2\pi e^{S_0}\hat{\rho}_0(E)$. Overall, our
results are consistent with the SFF analysis.
  We also calculate the ERB length $\langle \ell(T) \rangle$ using the three
approaches and find that the results are in good agreement with each other. We
also find that the $\langle \ell(T) \rangle$ grows as a cubic function in $T$
due to the contribution from geometry including one observable baby universe,
and converges to a constant after $T=T_H$. For the geometry with one baby
universe, we compute the size $\langle b(T) \rangle$ of the baby universe and
find that it is of the same order as $\langle \ell(T) \rangle$. This result is
consistent with the baby universe emission mechanism claimed by P.Saad.; 98) Teaching Language Models to Critique via Reinforcement Learning; Teaching large language models (LLMs) to critique and refine their outputs is
crucial for building systems that can iteratively improve, yet it is
fundamentally limited by the ability to provide accurate judgments and
actionable suggestions. In this work, we study LLM critics for code generation
and propose $\texttt{CTRL}$, a framework for $\texttt{C}$ritic
$\texttt{T}$raining via $\texttt{R}$einforcement $\texttt{L}$earning, which
trains a critic model to generate feedback that maximizes correction
performance for a fixed generator model without human supervision. Our results
demonstrate that critics trained with $\texttt{CTRL}$ significantly enhance
pass rates and mitigate compounding errors across both base and stronger
generator models. Furthermore, we show that these critic models act as accurate
generative reward models and enable test-time scaling through iterative
critique-revision, achieving up to 106.1% relative improvements across
challenging code generation benchmarks.; 99) From Visuals to Vocabulary: Establishing Equivalence Between Image and
  Text Token Through Autoregressive Pre-training in MLLMs; While MLLMs perform well on perceptual tasks, they lack precise multimodal
alignment, limiting performance. To address this challenge, we propose Vision
Dynamic Embedding-Guided Pretraining (VDEP), a hybrid autoregressive training
paradigm for MLLMs. Utilizing dynamic embeddings from the MLP following the
visual encoder, this approach supervises image hidden states and integrates
image tokens into autoregressive training. Existing MLLMs primarily focused on
recovering information from textual inputs, often neglecting the effective
processing of image data. In contrast, the key improvement of this work is the
reinterpretation of multimodal alignment as a process of recovering information
from input data, with particular emphasis on reconstructing detailed visual
features.The proposed method seamlessly integrates into standard models without
architectural changes. Experiments on 13 benchmarks show VDEP outperforms
baselines, surpassing existing methods.; 100) Physiome-ODE: A Benchmark for Irregularly Sampled Multivariate Time
  Series Forecasting Based on Biological ODEs; State-of-the-art methods for forecasting irregularly sampled time series with
missing values predominantly rely on just four datasets and a few small toy
examples for evaluation. While ordinary differential equations (ODE) are the
prevalent models in science and engineering, a baseline model that forecasts a
constant value outperforms ODE-based models from the last five years on three
of these existing datasets. This unintuitive finding hampers further research
on ODE-based models, a more plausible model family. In this paper, we develop a
methodology to generate irregularly sampled multivariate time series (IMTS)
datasets from ordinary differential equations and to select challenging
instances via rejection sampling. Using this methodology, we create
Physiome-ODE, a large and sophisticated benchmark of IMTS datasets consisting
of 50 individual datasets, derived from real-world ordinary differential
equations from research in biology. Physiome-ODE is the first benchmark for
IMTS forecasting that we are aware of and an order of magnitude larger than the
current evaluation setting of four datasets. Using our benchmark Physiome-ODE,
we show qualitatively completely different results than those derived from the
current four datasets: on Physiome-ODE ODE-based models can play to their
strength and our benchmark can differentiate in a meaningful way between
different IMTS forecasting models. This way, we expect to give a new impulse to
research on ODE-based time series modeling.",0.0,0.0
2411.00561,applied,2411.00561-pos2-8,"Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study; In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric.",2411.00561-pos1-8,"What is a cell type, really? The quest to categorize life’s myriad forms; The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. “I realized that there were probably hundreds of flavours,” recalls Buenrostro, who was a graduate student at Stanford University in California at the time.",17,"['1', '2', '4', '5', '6', '9', '10', '11', '12', '19']","The best candidate paper should complement the main paper's focus on retrieval and classification techniques for shape-based objects, particularly in how it utilizes advanced mathematical or computational frameworks. The first candidate paper, 'Nontrapping Tunable Topological Photonic Memory', introduces innovative approaches to information storage and retrieval in a topological system, which aligns well with the main paper's focus on shape descriptors and retrieval metrics. This connection highlights a potential multidisciplinary approach between shape recognition techniques and topological data storage. The subsequent candidates provide interesting methodologies but do not align as closely with the core themes of the main paper.","1) Nontrapping Tunable Topological Photonic Memory; We propose a novel topological photonic memory that encodes information
through dynamically controllable Chern numbers in a two-band topological
photonic system. Utilizing a honeycomb lattice photonic crystal, the memory
leverages topologically protected edge states that remain robust against
fabrication imperfections and environmental perturbations. By applying a
synthetic time-dependent magnetic field, we achieve real-time tunability of the
Chern number, enabling rapid and efficient memory switching without the need
for light-trapping mechanisms. Our computational study evaluates critical
performance metrics, including write speed, read stabilization time, energy gap
stability, and nonadiabatic transition probabilities. The results demonstrate
that the system supports GHz-range write speeds (approximately 1-10 GHz), with
stable data retention due to the large energy gap between bands. The system
enables scalable multi-bit memory encoding based on quantized Chern numbers and
exhibits superior speed, fault tolerance, and robustness compared to
conventional photonic memory architectures. This work introduces a scalable,
high-speed, and nontrapping optical memory paradigm, paving the way for future
applications in quantum information processing and optical communication
technologies.; 2) LUSIFER: Language Universal Space Integration for Enhanced Multilingual
  Embeddings with Large Language Models; Recent advancements in large language models (LLMs) based embedding models
have established new state-of-the-art benchmarks for text embedding tasks,
particularly in dense vector-based retrieval. However, these models
predominantly focus on English, leaving multilingual embedding capabilities
largely unexplored. To address this limitation, we present LUSIFER, a novel
zero-shot approach that adapts LLM-based embedding models for multilingual
tasks without requiring multilingual supervision. LUSIFER's architecture
combines a multilingual encoder, serving as a language-universal learner, with
an LLM-based embedding model optimized for embedding-specific tasks. These
components are seamlessly integrated through a minimal set of trainable
parameters that act as a connector, effectively transferring the multilingual
encoder's language understanding capabilities to the specialized embedding
model. Additionally, to comprehensively evaluate multilingual embedding
performance, we introduce a new benchmark encompassing 5 primary embedding
tasks, 123 diverse datasets, and coverage across 14 languages. Extensive
experimental results demonstrate that LUSIFER significantly enhances the
multilingual performance across various embedding tasks, particularly for
medium and low-resource languages, without requiring explicit multilingual
training data.; 3) $L^2$ decay estimates of weak solutions to 3D fractional MHD equations
  in exterior domains; Consider three-dimensional fractional MHD equations in an exterior domain
with the Dirichlet boundary condition assumed. Asymptotic behaviours of weak
solutions to the three-dimensional exterior fractional MHD equations are
studied. $L^2$ decay estimates of the weak solutions are obtained.; 4) Differential Privacy Personalized Federated Learning Based on
  Dynamically Sparsified Client Updates; Personalized federated learning is extensively utilized in scenarios
characterized by data heterogeneity, facilitating more efficient and automated
local training on data-owning terminals. This includes the automated selection
of high-performance model parameters for upload, thereby enhancing the overall
training process. However, it entails significant risks of privacy leakage.
Existing studies have attempted to mitigate these risks by utilizing
differential privacy. Nevertheless, these studies present two major
limitations: (1) The integration of differential privacy into personalized
federated learning lacks sufficient personalization, leading to the
introduction of excessive noise into the model. (2) It fails to adequately
control the spatial scope of model update information, resulting in a
suboptimal balance between data privacy and model effectiveness in differential
privacy federated learning. In this paper, we propose a differentially private
personalized federated learning approach that employs dynamically sparsified
client updates through reparameterization and adaptive norm(DP-pFedDSU).
Reparameterization training effectively selects personalized client update
information, thereby reducing the quantity of updates. This approach minimizes
the introduction of noise to the greatest extent possible. Additionally,
dynamic adaptive norm refers to controlling the norm space of model updates
during the training process, mitigating the negative impact of clipping on the
update information. These strategies substantially enhance the effective
integration of differential privacy and personalized federated learning.
Experimental results on EMNIST, CIFAR-10, and CIFAR-100 demonstrate that our
proposed scheme achieves superior performance and is well-suited for more
complex personalized federated learning scenarios.; 5) Utilizing Effective Dynamic Graph Learning to Shield Financial Stability
  from Risk Propagation; Financial risks can propagate across both tightly coupled temporal and
spatial dimensions, posing significant threats to financial stability.
Moreover, risks embedded in unlabeled data are often difficult to detect. To
address these challenges, we introduce GraphShield, a novel approach with three
key innovations: Enhanced Cross-Domain Infor mation Learning: We propose a
dynamic graph learning module to improve information learning across temporal
and spatial domains. Advanced Risk Recognition: By leveraging the clustering
characteristics of risks, we construct a risk recognizing module to enhance the
identification of hidden threats. Risk Propagation Visualization: We provide a
visualization tool for quantifying and validating nodes that trigger widespread
cascading risks. Extensive experiments on two real-world and two open-source
datasets demonstrate the robust performance of our framework. Our approach
represents a significant advancement in leveraging artificial intelligence to
enhance financial stability, offering a powerful solution to mitigate the
spread of risks within financial networks.; 6) De Sitter Horizon Edge Partition Functions; One-loop $S^{d+1}$ path integrals were shown to factorize into two parts: a
bulk thermal ideal gas partition function in a $dS_{d+1}$ static patch and an
edge partition function associated with degrees of freedom living on $S^{d-1}$.
Here, we analyze the $\mathfrak{so}(d)$ structure of the edge partition
functions for massive and massless totally symmetric tensors of arbitrary rank
in any $d\geq 3$. For linearized Einstein gravity on $S^{d+1}$, we find that
the edge partition function receives contributions from shift-symmetric vector
and scalar fields on $S^{d-1}$ that nonlinearly realize the isometry group
$SO(d+2)$ of $S^{d+1}$, suggesting a possible interpretation in terms of an
embedded $S^{d-1}$ brane.; 7) Connecting the Unconnectable through Feedback; Reliable uplink connectivity remains a persistent challenge for IoT devices,
particularly those at the cell edge, due to their limited transmit power and
single-antenna configurations. This paper introduces a novel framework aimed at
connecting the unconnectable, leveraging real-time feedback from access points
(APs) to enhance uplink coverage without increasing the energy consumption of
IoT devices. At the core of this approach are feedback channel codes, which
enable IoT devices to dynamically adapt their transmission strategies based on
AP decoding feedback, thereby reducing the critical uplink SNR required for
successful communication. Analytical models are developed to quantify the
coverage probability and the number of connectable APs, providing a
comprehensive understanding of the system's performance. Numerical results
validate the proposed method, demonstrating substantial improvements in
coverage range and connectivity, particularly for devices at the cell edge,
with up to a 51% boost in connectable APs. Our approach offers a robust and
energy-efficient solution to overcoming uplink coverage limitations, enabling
IoT networks to connect devices in challenging environments.; 8) Asymmetric Dark Matter in SUSY with approximate $R-$symmetry; We implement the asymmetric dark matter framework, linking the ordinary and
dark matter abundances, within a supersymmetric context. We consider a
supersymmetric model that respects an approximate $U(1)_R$ symmetry, which is
broken in such a way that at high temperature the $R$ breaking sector mediate
processes in equilibrium, but at the SUSY mass scale, the sparticles asymmetry
is frozen. In this framework, the gravitino serves as the dark matter
candidate, and its mass is predicted to be $\sim10$ GeV to match the observed
relic abundance. We identify several realistic spectra; however, the
requirement for the Next-to-Lightest Supersymmetric Particle (NLSP) to decay
into the gravitino before Big Bang Nucleosynthesis constrains the viable
spectrum to masses above 2 TeV.; 9) How to make a Universe; We establish the general conditions under which evolution in the laws of
physics and matter creation or destruction are closely intertwined. They make
use of global time variables canonically dual to the constants of Nature. Such
times flow at a rate determined by what can be interpret as the chemical
potential of the fundamental constants (in analogy with phenomenological clocks
based on isentropic fluids). The general condition for violations of energy
conservation is then that a matter parameter evolves as a function of a gravity
clock or vice-versa. This framework can be envisaged as the environment within
which a natural selection scenario operates, powered by random mutations in the
values of the constants of nature (or indeed any other variability in the laws
in terms of the times defined above). The prize function is the creation of
matter, followed by its preservation. This can be accomplished in an
environment where diffeomorphism invariance is among the possible theories,
with mutations modelled, for example, on the absorbing Markov chain. In such a
set-up the diffeormorphism invariant state with fixed constants (or any nearby
state) should be the absorbing state. John Wheeler's ``higgledy-piggledy''
chaotic cosmic start therefore finds a realization in this model, where its own
demise and the establishment of order and seemingly immutable laws is also a
predection of the model.; 10) Functional equation arising in behavioral sciences: solvability and
  collocation scheme in H\""older spaces; We consider a generalization of a functional equation that models the
learning process in various animal species. The equation can be considered
nonlocal, as it is built with a convex combination of the unknown function
evaluated at mixed arguments. This makes the equation contain two terms with
vanishing delays. We prove the existence and uniqueness of the solution in the
H\""older space which is a natural function space to consider. In the second
part of the paper, we devise an efficient numerical collocation method used to
find an approximation to the main problem. We prove the convergence of the
scheme and, in passing, several properties of the linear interpolation operator
acting on the H\""older space. Numerical simulations verify that the order of
convergence of the method (measured in the supremum norm) is equal to the order
of H\""older continuity.; 11) Gate-Voltage-Driven Quantum Phase Transition at $0.7 (2e^2/h)$ in
  Quantum Point Contacts; The complex gate-voltage-dependent differential conductance in quantum point
contacts, shaped by entangled-state tunneling, was demonstrated through the
movement of a localized spin. This spin responds to variations in side gate
voltage, triggering a quantum phase transition (QPT) between symmetric and
asymmetric Kondo coupling states, with the states separated by conductance
regions $G \geq 0.7 G_0$ and $G \leq 0.7 G_0$, where $G_0 = 2e^2/h$,
respectively. The asymmetric state has two Kondo temperatures, while the
symmetric state has only one. The presence of two Kondo temperatures in the
asymmetric state clarifies previously unresolved issues, such as the
indeterminate Kondo temperature and anomalous behavior in the width of the
zero-bias anomaly (ZBA) in the $G \leq 0.7 G_0$ region. The QPT was
investigated by analyzing the gate-voltage-dependent ZBA energy, calculated
using the corresponding local density of states at the site of the localized
spin, obtained during the replication of the differential conductance.; 12) Galactic magnetic fields II. Applying the model to nearby galaxies; Many spiral galaxies host magnetic fields with energy densities comparable to
those of the turbulent and thermal motions of their interstellar gas. However,
quantitative comparison between magnetic field properties inferred from
observation and those obtained from theoretical modeling has been lacking. In
Paper I we developed a simple, axisymmetric galactic dynamo model that uses
various observational data as input. Here we apply our model to calculate
radial profiles of azimuthally and vertically averaged magnetic field strength
and pitch angle, gas velocity dispersion and scale height, turbulent
correlation time and length, and the sizes of supernova remnants for the
galaxies M31, M33, M51, and NGC 6946, using input data collected from the
literature. Scaling factors are introduced to account for a lack of precision
in both theory and observation. Despite the simplicity of our model, its
outputs agree fairly well with galaxy properties inferred from observation.
Additionally, we find that most of the parameter values are similar between
galaxies. We extend the model to predict the magnetic field pitch angles
arising from a combination of mean-field dynamo action and the winding up of
the random small-scale field owing to the large-scale radial shear. We find
their magnitudes to be much smaller than those of the pitch angles measured in
polarized radio and far infrared emission. This suggests that effects not
included in our model, such as effects associated with spiral arms, are needed
to explain the pitch angle values.; 13) The MAGPI Survey: the kinematic morphology-density relation (or lack
  thereof) and the Hubble sequence at $z\sim0.3$; This work presents visual morphological and dynamical classifications for 637
spatially resolved galaxies, most of which are at intermediate redshift
($z\sim0.3$), in the Middle-Ages Galaxy Properties with Integral field
spectroscopy (MAGPI) Survey. For each galaxy, we obtain a minimum of 11
independent visual classifications by knowledgeable classifiers. We use an
extension of the standard Dawid-Skene bayesian model introducing
classifier-specific confidence parameters and galaxy-specific difficulty
parameters to quantify classifier confidence and infer reliable statistical
confidence estimates. Selecting sub-samples of 86 bright ($r<20$ mag)
high-confidence ($>0.98$) morphological classifications at redshifts ($0.2 \le
z \le0.4$), we confirm the full range of morphological types is represented in
MAGPI as intended in the survey design. Similarly, with a sub-sample of 82
bright high-confidence stellar kinematic classifications, we find that the
rotating and non-rotating galaxies seen at low redshift are already in place at
intermediate redshifts. We \textit{do not} find evidence that the kinematic
morphology-density relation seen at $z\sim0$ is established at $z\sim0.3$. We
suggest that galaxies without obvious stellar rotation are dynamically
pre-processed sometime before $z\sim0.3$ within lower mass groups before
joining denser environments.; 14) RewardSDS: Aligning Score Distillation via Reward-Weighted Sampling; Score Distillation Sampling (SDS) has emerged as an effective technique for
leveraging 2D diffusion priors for tasks such as text-to-3D generation. While
powerful, SDS struggles with achieving fine-grained alignment to user intent.
To overcome this, we introduce RewardSDS, a novel approach that weights noise
samples based on alignment scores from a reward model, producing a weighted SDS
loss. This loss prioritizes gradients from noise samples that yield aligned
high-reward output. Our approach is broadly applicable and can extend SDS-based
methods. In particular, we demonstrate its applicability to Variational Score
Distillation (VSD) by introducing RewardVSD. We evaluate RewardSDS and
RewardVSD on text-to-image, 2D editing, and text-to-3D generation tasks,
showing significant improvements over SDS and VSD on a diverse set of metrics
measuring generation quality and alignment to desired reward models, enabling
state-of-the-art performance. Project page is available at
https://itaychachy.github.io/reward-sds/.; 15) Evaluating Standard and Dialectal Frisian ASR: Multilingual Fine-tuning
  and Language Identification for Improved Low-resource Performance; Automatic Speech Recognition (ASR) performance for low-resource languages is
still far behind that of higher-resource languages such as English, due to a
lack of sufficient labeled data. State-of-the-art methods deploy
self-supervised transfer learning where a model pre-trained on large amounts of
data is fine-tuned using little labeled data in a target low-resource language.
In this paper, we present and examine a method for fine-tuning an SSL-based
model in order to improve the performance for Frisian and its regional dialects
(Clay Frisian, Wood Frisian, and South Frisian). We show that Frisian ASR
performance can be improved by using multilingual (Frisian, Dutch, English and
German) fine-tuning data and an auxiliary language identification task. In
addition, our findings show that performance on dialectal speech suffers
substantially, and, importantly, that this effect is moderated by the
elicitation approach used to collect the dialectal data. Our findings also
particularly suggest that relying solely on standard language data for ASR
evaluation may underestimate real-world performance, particularly in languages
with substantial dialectal variation.; 16) A Digital Twin-Driven Recommendation System for Adaptive Campus Course
  Timetabling; Efficient and adaptive course timetabling for large, dynamic university
campuses remains a significant challenge due to the complex interplay of hard
and soft constraints. Traditional static optimization methods often fail to
accommodate real-time disruptions, evolving user preferences, and the nuanced
spatial-temporal relationships inherent in campus environments. This paper
reconceptualizes the timetabling problem as a recommendation-based task and
leverages the Texas A&M Campus Digital Twin as a dynamic data platform. Our
proposed framework integrates collaborative and content-based filtering
techniques with iterative feedback mechanisms, thereby generating a ranked set
of adaptive timetable recommendations. A composite scoring function,
incorporating metrics for classroom occupancy, travel distance, travel time,
and vertical transitions, enables the framework to systematically balance
resource utilization with user-centric factors. Extensive experiments using
real-world data from Texas A&M University demonstrate that our approach
effectively reduces travel inefficiencies, optimizes classroom utilization, and
enhances overall user satisfaction. By coupling a recommendation-oriented
paradigm with a digital twin environment, this study offers a robust and
scalable blueprint for intelligent campus planning and resource allocation,
with potential applications in broader urban contexts.; 17) What is a cell type, really? The quest to categorize life’s myriad forms; The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. “I realized that there were probably hundreds of flavours,” recalls Buenrostro, who was a graduate student at Stanford University in California at the time.; 18) Making Tennis Fairer: The Grand Tiebreaker; Tennis, like other games and sports, is governed by rules, including the
rules that determine the winner of points, games, sets, and matches. If the two
players are equally skilled -- each has the same probability of winning a point
when serving or when receiving -- we show that each has an equal chance of
winning games, sets, and matches, whether or not sets go to a tiebreak.
However, in a women's match that is decided by 2 out of 3 sets, and a men's
match that is decided by 3 out of 5 sets, it is possible that the player who
wins the most games may not be the player who wins the match. We calculate the
probability that this happens and show that it has actually occurred -- most
notably, in the 2019 men's Wimbledon final between Novak Djokovic and Roger
Federer, which took almost five hours to complete and is considered one of the
greatest tennis matches ever (Djokovic won). We argue that the discrepancy
between the game winner and the match winner, when it occurs, should be
resolved by a Grand Tiebreak (GT) -- played according to the rules of tiebreaks
in sets -- because each player has a valid claim to being called the rightful
winner. A GT would have the salutary effect of -- even every point -- lest
he/she win in sets but lose more games. This would make competition keener
throughout a match and probably decrease the need for a GT, because the game
and set winner would more likely coincide when the players fight hard for every
point.; 19) Causality Enhanced Origin-Destination Flow Prediction in Data-Scarce
  Cities; Accurate origin-destination (OD) flow prediction is of great importance to
developing cities, as it can contribute to optimize urban structures and
layouts. However, with the common issues of missing regional features and
lacking OD flow data, it is quite daunting to predict OD flow in developing
cities. To address this challenge, we propose a novel Causality-Enhanced OD
Flow Prediction (CE-OFP), a unified framework that aims to transfer urban
knowledge between cities and achieve accuracy improvements in OD flow
predictions across data-scarce cities. In specific, we propose a novel
reinforcement learning model to discover universal causalities among urban
features in data-rich cities and build corresponding causal graphs. Then, we
further build Causality-Enhanced Variational Auto-Encoder (CE-VAE) to
incorporate causal graphs for effective feature reconstruction in data-scarce
cities. Finally, with the reconstructed features, we devise a knowledge
distillation method with a graph attention network to migrate the OD prediction
model from data-rich cities to data-scare cities. Extensive experiments on two
pairs of real-world datasets validate that the proposed CE-OFP remarkably
outperforms state-of-the-art baselines, which can reduce the RMSE of OD flow
prediction for data-scarce cities by up to 11%.; 20) Imit Diff: Semantics Guided Diffusion Transformer with Dual Resolution
  Fusion for Imitation Learning; Visuomotor imitation learning enables embodied agents to effectively acquire
manipulation skills from video demonstrations and robot proprioception.
However, as scene complexity and visual distractions increase, existing methods
that perform well in simple scenes tend to degrade in performance. To address
this challenge, we introduce Imit Diff, a semanstic guided diffusion
transformer with dual resolution fusion for imitation learning. Our approach
leverages prior knowledge from vision language foundation models to translate
high-level semantic instruction into pixel-level visual localization. This
information is explicitly integrated into a multi-scale visual enhancement
framework, constructed with a dual resolution encoder. Additionally, we
introduce an implementation of Consistency Policy within the diffusion
transformer architecture to improve both real-time performance and motion
smoothness in embodied agent control.We evaluate Imit Diff on several
challenging real-world tasks. Due to its task-oriented visual localization and
fine-grained scene perception, it significantly outperforms state-of-the-art
methods, especially in complex scenes with visual distractions, including
zero-shot experiments focused on visual distraction and category
generalization. The code will be made publicly available.; 21) The Cygnus Allscale Survey of Chemistry and Dynamical Environments:
  CASCADE. IV. Unveiling the hidden structures in DR18; The Cygnus-X complex is a massive, nearby (1.4 kpc) star-forming region with
several OB associations. As part of the Cygnus Allscale Survey of Chemistry and
Dynamical Environments (CASCADE) program, we carried out 3.6 millimeter (mm)
continuum and spectral line high-resolution observations ($\sim$ 3 - 4$''$)
toward DR18, covering several molecular species with the Northern Extended
Millimeter Array (NOEMA) and the Institut de Radioastronomie Millim\'etrique
(IRAM) 30m telescope. In addition, multi-wavelength archival datasets were used
to provide a comprehensive analysis of the region. A comparison of the 3.6mm
and 6 cm continuum emission confirms that a B2 star (DR18-05) shapes the
cometary HII region in the DR18 cavity, with ionized gas escaping toward the
OB2 association. On the other hand, the extended 3.6mm and 6 cm continuum
emission are likely to trace photoevaporating ionized gas from ultraviolet
radiation from the Cyg OB2 association, not from DR18-05. The shell structure
around DR18-05 indicates photodissociation regions (PDRs) formed by the
expanding HII region and photo-erosion from DR18-05 and OB2 stars. We also
identified 18 compact cores with N$_2$H$^+$ emission, half of which are
gravitationally bound and mostly located in colder regions behind the PDRs. The
SiO emission is found only in PDRs, with narrow-line widths ( 0.8 - 2.0 km
s$^{-1}$) and lower abundances (X(SiO) $\sim$ 5$\times$10$^{-11}$ -
1$\times$10$^{-10}$). Comparing with the UV irradiated shock models, we suggest
that the SiO emission partially encompassing the HII region arises from the
molecular gas region, marginally compressed by low-velocity shocks with $\sim$
5 km s$^{-1}$, irradiated by external UV radiation (G$_{\rm 0} \sim 10^{2} -
10^{3}$), as they traverse through a medium with $n_{\rm H} \sim 10^{4}$ to
10$^5$ cm$^{-3}$.; 22) Egoistic MDS-based Rigid Body Localization; We consider a novel anchorless rigid body localization (RBL) suitable for
application in autonomous driving (AD), in so far as the algorithm enables a
rigid body to egoistically detect the location (relative translation) and
orientation (relative rotation) of another body, without knowledge of the shape
of the latter, based only on a set of measurements of the distances between
sensors of one vehicle to the other. A key point of the proposed method is that
the translation vector between the two-bodies is modeled using the
double-centering operator from multidimensional scaling (MDS) theory, enabling
the method to be used between rigid bodies regardless of their shapes, in
contrast to conventional approaches which require both bodies to have the same
shape. Simulation results illustrate the good performance of the proposed
technique in terms of root mean square error (RMSE) of the estimates in
different setups.; 23) Swift4D:Adaptive divide-and-conquer Gaussian Splatting for compact and
  efficient reconstruction of dynamic scene; Novel view synthesis has long been a practical but challenging task, although
the introduction of numerous methods to solve this problem, even combining
advanced representations like 3D Gaussian Splatting, they still struggle to
recover high-quality results and often consume too much storage memory and
training time. In this paper we propose Swift4D, a divide-and-conquer 3D
Gaussian Splatting method that can handle static and dynamic primitives
separately, achieving a good trade-off between rendering quality and
efficiency, motivated by the fact that most of the scene is the static
primitive and does not require additional dynamic properties. Concretely, we
focus on modeling dynamic transformations only for the dynamic primitives which
benefits both efficiency and quality. We first employ a learnable decomposition
strategy to separate the primitives, which relies on an additional parameter to
classify primitives as static or dynamic. For the dynamic primitives, we employ
a compact multi-resolution 4D Hash mapper to transform these primitives from
canonical space into deformation space at each timestamp, and then mix the
static and dynamic primitives to produce the final output. This
divide-and-conquer method facilitates efficient training and reduces storage
redundancy. Our method not only achieves state-of-the-art rendering quality
while being 20X faster in training than previous SOTA methods with a minimum
storage requirement of only 30MB on real-world datasets. Code is available at
https://github.com/WuJH2001/swift4d.; 24) Exploring strong electronic correlations in the breathing kagome metal
  Fe$_3$Sn; Kagome metals have emerged as pivotal materials in condensed matter physics
due to their unique geometric arrangement and intriguing electronic properties.
Understanding the origin of magnetism in these materials, particularly in iron
rich Fe-Sn binary compounds like Fe$_3$Sn, holds a significant importance, as
they represent potential candidates for permanent magnets with a high Curie
temperature and a strong magnetic anisotropy. In the present study, we employ
density-functional theory and dynamical mean-field theory to analyze the
electronic structure and magnetic properties of Fe$_3$Sn. Our investigation
reveals the presence of several nearly-flat bands and Weyl nodes at low
excitation energies. The inclusion of local correlation effects is shown to
push these features even closer to the Fermi energy, which may be important for
their manipulation via external stimuli. Regarding magnetism, the Hubbard-like
interaction leads to an increase of orbital polarization at the expenses of a
minor reduction of the spin moment. The magnetic anisotropy energy exhibits a
strong dependence on the particular choice of the Coulomb interaction
parameters. Additionally, our detailed analysis of the interatomic exchange
interactions indicates a significant contribution from the antisymmetric
exchange, i.e. the Dzyaloshinskii-Moriya interaction, which showcases the
existence of magnetic chirality in the system. Overall, our investigation
highlights a strong interplay between the flat bands near the Fermi level, the
local Coulomb interaction and the triangular geometry of the lattice, which
plays a crucial role in driving the magnetic properties of this material.; 25) Bell Inequality Violation of Light Quarks in Back-to-Back Dihadron Pair
  Production at Lepton Colliders; Spin correlations between particles produced at colliders provide valuable
insights for quantum information studies. While traditional studies of quantum
information at colliders are typically limited to massive particles with
perturbative decay, we propose an innovative method to explore the Bell
inequality in massless quark pair systems by analyzing the azimuthal
correlations in back-to-back $\pi^+\pi^-$ dihadron pair production at lepton
colliders. Revisiting the Belle data, we have shown the potential to detect
Bell inequality violation of light quarks by introducing an additional angular
cut, achieving a significance of 2.5 $\sigma$ even in the worst-case scenario
of 100% correlated systematic uncertainties in each bins. The significance
substantially exceeds $5\sigma$ when considering uncorrelated systematic
uncertainties. Our approach opens avenues for exploring spin quantum
information in the non-perturbative aspect and leverages existing data for
quantum information research.; 26) Bi-Lipschitz triviality of function-germs on singular varieties; In this paper we study the bi-Lipschitz triviality of deformations of an
analytic function germ $f$ defined on a germ of an analytic variety $(X, 0)$ in
$\mathbb C^n$. We introduce the notion of strongly rational $\mathscr
R_X$-bi-Lipschitz trivial families and give an infinitesimal criterion which is
a sufficient condition for the bi-Lipschitz triviality of deformations of $f$
on $(X,0).$ As a corollary it follows that when $X$ and $f$ are homogeneous of
the same degree, all deformation of $f$ of the same or higher degrees are
bi-Lipschitz trivial. We then prove a rigidity result for deformations of $f$
on $X$ when both are weighted homogeneous with respect to the same set of
weights.; 27) Measurable Improvement in Multi-Qubit Readout Using a Kinetic Inductance
  Traveling Wave Parametric Amplifier; Increasing the size and complexity of quantum information systems requires
highly-multiplexed readout architectures, as well as amplifier chains operating
near the quantum limit (QL) of added noise. While documented prior efforts in
KITWPA integration in quantum systems are scarce, in this work we demonstrate
integration of a KI-TWPA with a multiplexed-qubit device. To quantify the
system noise improvement we perform an ac Stark shift calibration to precisely
determine noise power levels on-chip (at each cavity's reference plane) and the
total system gain. We then characterize the qubit state measurement fidelity
and the corresponding signal-to-noise ratio (SNR). To conduct the most faithful
measurement of the benefits offered by the KI-TWPA we perform these
measurements for readout chains where the high electron mobility transistor
(HEMT) amplifier is the first-stage amplifier (FSA) - with none of the external
hardware required to operate the KI-TWPA - and with the KI-TWPA as the FSA.
While some readout cavities fall outside the KI-TWPA bandwidth, for those
inside the bandwidth we demonstrate a maximum improvement in the state
measurement SNR by a factor of 1.45, and increase the fidelity from 96.2% to
97.8%. These measurements demonstrate a system noise below 5 quanta referenced
on-chip and we bound the KI-TWPA excess noise to be below 4 quanta for the six
cavities inside its bandwidth. These results show a promising path forward for
realizing quantum-limited readout chains in large qubit systems using a single
parametric amplifier.; 28) Non-parametric estimation of net survival under dependence between death
  causes; Relative survival methodology deals with a competing risks survival model
where the cause of death is unknown. This lack of information occurs regularly
in population-based cancer studies. Non-parametric estimation of the net
survival is possible through the Pohar Perme estimator. Derived similarly to
Kaplan-Meier, it nevertheless relies on an untestable independence assumption.
We propose here to relax this assumption and provide a generalized
non-parametric estimator that works for other dependence structures, by
leveraging the underlying stochastic processes and martingales. We formally
derive asymptotics of this estimator, providing variance estimation and
log-rank-type tests. Our approach provides a new perspective on the Pohar Perme
estimator and the acceptability of the underlying independence assumption. We
highlight the impact of this dependence structure assumption on simulation
studies, and illustrate them through an application on registry data relative
to colorectal cancer, before discussing potential extensions of our
methodology.; 29) Proposal for Spin-Superfluid Quantum Interference Device; In easy-plane magnets, the spin-superfluid phase was predicted to facilitate
coherent spin transport. So far, experimental evidence remains elusive. In this
letter, we propose an indirect way to sense this effect via the spin-superfluid
quantum interference device (spin SQUID) -- inspired by its superconducting
counterpart (rf SQUID). The spin SQUID is constructed as a
quasi-one-dimensional (1D) magnetic ring with a single Josephson weak link,
functioning as an isolated device with a microwave response. The spin current
is controlled by an in-plane electric field through Dzyaloshinskii-Moriya
interaction. This interaction can be interpreted as a gauge field that couples
to the spin supercurrent through the Aharonov-Casher effect. By investigating
the static and dynamic properties of the device, we show that the spin current
and the harmonic frequencies of the spin superfluid are periodic with respect
to the accumulated Aharonov-Casher phase and are, therefore, sensitive to the
radial electric flux through the ring in units of the electric flux quantum,
suggesting a potential electric-field sensing functionality. For readout, we
propose to apply spectroscopic analysis to detect the frequency of the harmonic
modes.; 30) Inclusive Avatar Guidelines for People with Disabilities: Supporting
  Disability Representation in Social Virtual Reality; Avatar is a critical medium for identity representation in social virtual
reality (VR). However, options for disability expression are highly limited on
current avatar interfaces. Improperly designed disability features may even
perpetuate misconceptions about people with disabilities (PWD). As more PWD use
social VR, there is an emerging need for comprehensive design standards that
guide developers and designers to create inclusive avatars. Our work aim to
advance the avatar design practices by delivering a set of centralized,
comprehensive, and validated design guidelines that are easy to adopt,
disseminate, and update. Through a systematic literature review and interview
with 60 participants with various disabilities, we derived 20 initial design
guidelines that cover diverse disability expression methods through five
aspects, including avatar appearance, body dynamics, assistive technology
design, peripherals around avatars, and customization control. We further
evaluated the guidelines via a heuristic evaluation study with 10 VR
practitioners, validating the guideline coverage, applicability, and
actionability. Our evaluation resulted in a final set of 17 design guidelines
with recommendation levels.; 31) Nonlinear bias of collective oscillation frequency induced by asymmetric
  Cauchy noise; We report the effect of nonlinear bias of the frequency of collective
oscillations of sin-coupled phase oscillators subject to individual asymmetric
Cauchy noises. The noise asymmetry makes the Ott-Antonsen Ansatz inapplicable.
We argue that, for all stable non-Gaussian noises, the tail asymmetry is not
only possible (in addition to the trivial shift of the distribution median) but
also generic in many physical and biophysical set-ups. For the theoretical
description of the effect, we develop a mathematical formalism based on the
circular cumulants. The derivation of rigorous asymptotic results can be
performed on this basis but seems infeasible in traditional terms of the
circular moments (the Kuramoto-Daido order parameters). The effect of the
entrainment of individual oscillator frequencies by the global oscillations is
also reported in detail. The accuracy of theoretical results based on the low
dimensional circular cumulant reductions is validated with the high-accuracy
""exact"" solutions calculated with the continued fraction method.; 32) Coverage errors for Student's t confidence intervals comparable to those
  in Hall (1988); Table 1 of Hall (1988) contains asymptotic coverage error formulas for some
nonparametric approximate 95% confidence intervals for the mean based on $n$
IID samples. The table includes an entry for an interval based on the central
limit theorem using Gaussian quantiles and the Gaussian maximum likelihood
variance estimate. It is missing an entry for the very widely used Student $t$
confidence intervals. This note makes a mild numerical correction for the
Gaussian entry and provides an entry for the Student $t$ intervals. For
skewness $\gamma$ and kurtosis $\kappa$, the corrected Gaussian formula is
$0.14\kappa -2.16\gamma^2-3.42$ and the formula for the $t$ intervals is
$0.14\kappa -2.16\gamma^2$. The impetus to revisit this estimate arose from the
surprisingly robust performance of Student's t statistic in randomized
quasi-Monte Carlo sampling.; 33) CopyJudge: Automated Copyright Infringement Identification and
  Mitigation in Text-to-Image Diffusion Models; Assessing whether AI-generated images are substantially similar to
copyrighted works is a crucial step in resolving copyright disputes. In this
paper, we propose CopyJudge, an automated copyright infringement identification
framework that leverages large vision-language models (LVLMs) to simulate
practical court processes for determining substantial similarity between
copyrighted images and those generated by text-to-image diffusion models.
Specifically, we employ an abstraction-filtration-comparison test framework
with multi-LVLM debate to assess the likelihood of infringement and provide
detailed judgment rationales. Based on the judgments, we further introduce a
general LVLM-based mitigation strategy that automatically optimizes infringing
prompts by avoiding sensitive expressions while preserving the non-infringing
content. Besides, our approach can be enhanced by exploring non-infringing
noise vectors within the diffusion latent space via reinforcement learning,
even without modifying the original prompts. Experimental results show that our
identification method achieves comparable state-of-the-art performance, while
offering superior generalization and interpretability across various forms of
infringement, and that our mitigation method could more effectively mitigate
memorization and IP infringement without losing non-infringing expressions.; 34) Representing Signs as Signs: One-Shot ISLR to Facilitate Functional Sign
  Language Technologies; Isolated Sign Language Recognition (ISLR) is crucial for scalable sign
language technology, yet language-specific approaches limit current models. To
address this, we propose a one-shot learning approach that generalises across
languages and evolving vocabularies. Our method involves pretraining a model to
embed signs based on essential features and using a dense vector search for
rapid, accurate recognition of unseen signs. We achieve state-of-the-art
results, including 50.8% one-shot MRR on a large dictionary containing 10,235
unique signs from a different language than the training set. Our approach is
robust across languages and support sets, offering a scalable, adaptable
solution for ISLR. Co-created with the Deaf and Hard of Hearing (DHH)
community, this method aligns with real-world needs, and advances scalable sign
language recognition.; 35) Enhancing Reusability of Learned Skills for Robot Manipulation via Gaze
  and Bottleneck; Autonomous agents capable of diverse object manipulations should be able to
acquire a wide range of manipulation skills with high reusability. Although
advances in deep learning have made it increasingly feasible to replicate the
dexterity of human teleoperation in robots, generalizing these acquired skills
to previously unseen scenarios remains a significant challenge. In this study,
we propose a novel algorithm, Gaze-based Bottleneck-aware Robot Manipulation
(GazeBot), which enables high reusability of the learned motions even when the
object positions and end-effector poses differ from those in the provided
demonstrations. By leveraging gaze information and motion bottlenecks, both
crucial features for object manipulation, GazeBot achieves high generalization
performance compared with state-of-the-art imitation learning methods, without
sacrificing its dexterity and reactivity. Furthermore, the training process of
GazeBot is entirely data-driven once a demonstration dataset with gaze data is
provided. Videos and code are available at
https://crumbyrobotics.github.io/gazebot.; 36) Enhance-A-Video: Better Generated Video for Free; DiT-based video generation has achieved remarkable results, but research into
enhancing existing models remains relatively unexplored. In this work, we
introduce a training-free approach to enhance the coherence and quality of
DiT-based generated videos, named Enhance-A-Video. The core idea is enhancing
the cross-frame correlations based on non-diagonal temporal attention
distributions. Thanks to its simple design, our approach can be easily applied
to most DiT-based video generation frameworks without any retraining or
fine-tuning. Across various DiT-based video generation models, our approach
demonstrates promising improvements in both temporal consistency and visual
quality. We hope this research can inspire future explorations in video
generation enhancement.; 37) Accelerating Elliptic Curve Point Additions on Versal AI Engine for
  Multi-scalar Multiplication; Multi-scalar multiplication (MSM) is crucial in cryptographic applications
and computationally intensive in zero-knowledge proofs. MSM involves
accumulating the products of scalars and points on an elliptic curve over a
377-bit modulus, and the Pippenger algorithm converts MSM into a series of
elliptic curve point additions (PADDs) with high parallelism. This study
investigates accelerating MSM on the Versal ACAP platform, an emerging hardware
that employs a spatial architecture integrating 400 AI Engines (AIEs) with
programmable logic and a processing system. AIEs are SIMD-based VLIW processors
capable of performing vector multiply-accumulate operations, making them
well-suited for multiplication-heavy workloads in PADD. Unlike simpler
multiplication tasks in previous studies, cryptographic computations also
require complex operations such as carry propagation. These operations
necessitate architecture-aware optimizations, including intra-core dedicated
coding style to fully exploit VLIW capabilities and inter-core strategy for
spatial task mapping. We propose various optimizations to accelerate PADDs,
including (1) algorithmic optimizations for carry propagation employing a
carry-save-like technique to exploit VLIW and SIMD capabilities and (2) a
comparison of four distinct spatial mappings to enhance intra- and inter-task
parallelism. Our approach achieves a computational efficiency that utilizes
50.2% of the theoretical memory bandwidth and provides 568 speedup over the
integrated CPU on the AIE evaluation board.; 38) What elements should we focus when designing immersive virtual nature? A
  preliminary user study; Extensive research has confirmed the positive relationship between exposure
to natural environments and human cognitive, behavioral, physical, and mental
health. However, only some have easy access to nature. With electronic
information and simulation technology advancements, digital nature experiences
are widely used across various devices and scenarios. It is essential to
explore how to effectively select and utilize natural elements to guide the
design of digital nature scenes. This paper examines critical elements in
immersive virtual nature (IVN) and their impact on user perception. Through
online surveys and design experiments, we identified specific natural elements
that promote relaxation and proposed design strategies for virtual
environments. We developed several immersive virtual nature scenes for further
validation. Finally, we outline our future experimental plans and research
directions in digital nature. Our research aims to provide HCI designers
insights into creating restorative, immersive virtual scenes.; 39) DNNs May Determine Major Properties of Their Outputs Early, with Timing
  Possibly Driven by Bias; This paper argues that deep neural networks (DNNs) mostly determine their
outputs during the early stages of inference, where biases inherent in the
model play a crucial role in shaping this process. We draw a parallel between
this phenomenon and human decision-making, which often relies on fast,
intuitive heuristics. Using diffusion models (DMs) as a case study, we
demonstrate that DNNs often make early-stage decision-making influenced by the
type and extent of bias in their design and training. Our findings offer a new
perspective on bias mitigation, efficient inference, and the interpretation of
machine learning systems. By identifying the temporal dynamics of
decision-making in DNNs, this paper aims to inspire further discussion and
research within the machine learning community.; 40) Computing Connection Matrix and Persistence Efficiently from a Morse
  Decomposition; Morse decompositions partition the flows in a vector field into equivalent
structures. Given such a decomposition, one can define a further summary of its
flow structure by what is called a connection matrix.These matrices, a
generalization of Morse boundary operators from classical Morse theory, capture
the connections made by the flows among the critical structures - such as
attractors, repellers, and orbits - in a vector field. Recently, in the context
of combinatorial dynamics, an efficient persistence-like algorithm to compute
connection matrices has been proposed in~\cite{DLMS24}. We show that, actually,
the classical persistence algorithm with exhaustive reduction retrieves
connection matrices, both simplifying the algorithm of~\cite{DLMS24} and
bringing the theory of persistence closer to combinatorial dynamical systems.
We supplement this main result with an observation: the concept of persistence
as defined for scalar fields naturally adapts to Morse decompositions whose
Morse sets are filtered with a Lyapunov function. We conclude by presenting
preliminary experimental results.; 41) A Multi-Agent DRL-Based Framework for Optimal Resource Allocation and
  Twin Migration in the Multi-Tier Vehicular Metaverse; Although multi-tier vehicular Metaverse promises to transform vehicles into
essential nodes -- within an interconnected digital ecosystem -- using
efficient resource allocation and seamless vehicular twin (VT) migration, this
can hardly be achieved by the existing techniques operating in a highly dynamic
vehicular environment, since they can hardly balance multi-objective
optimization problems such as latency reduction, resource utilization, and user
experience (UX). To address these challenges, we introduce a novel multi-tier
resource allocation and VT migration framework that integrates Graph
Convolutional Networks (GCNs), a hierarchical Stackelberg game-based incentive
mechanism, and Multi-Agent Deep Reinforcement Learning (MADRL). The GCN-based
model captures both spatial and temporal dependencies within the vehicular
network; the Stackelberg game-based incentive mechanism fosters cooperation
between vehicles and infrastructure; and the MADRL algorithm jointly optimizes
resource allocation and VT migration in real time. By modeling this dynamic and
multi-tier vehicular Metaverse as a Markov Decision Process (MDP), we develop a
MADRL-based algorithm dubbed the Multi-Objective Multi-Agent Deep Deterministic
Policy Gradient (MO-MADDPG), which can effectively balances the various
conflicting objectives. Extensive simulations validate the effectiveness of
this algorithm that is demonstrated to enhance scalability, reliability, and
efficiency while considerably improving latency, resource utilization,
migration cost, and overall UX by 12.8%, 9.7%, 14.2%, and 16.1%, respectively.; 42) Quantum computer formulation of the FKP-operator eigenvalue problem for
  probabilistic learning on manifolds; We present a quantum computing formulation to address a challenging problem
in the development of probabilistic learning on manifolds (PLoM). It involves
solving the spectral problem of the high-dimensional Fokker-Planck (FKP)
operator, which remains beyond the reach of classical computing. Our ultimate
goal is to develop an efficient approach for practical computations on quantum
computers. For now, we focus on an adapted formulation tailored to quantum
computing. The methodological aspects covered in this work include the
construction of the FKP equation, where the invariant probability measure is
derived from a training dataset, and the formulation of the eigenvalue problem
for the FKP operator. The eigen equation is transformed into a Schr\""odinger
equation with a potential V, a non-algebraic function that is neither simple
nor a polynomial representation. To address this, we propose a methodology for
constructing a multivariate polynomial approximation of V, leveraging
polynomial chaos expansion within the Gaussian Sobolev space. This approach
preserves the algebraic properties of the potential and adapts it for quantum
algorithms. The quantum computing formulation employs a finite basis
representation, incorporating second quantization with creation and
annihilation operators. Explicit formulas for the Laplacian and potential are
derived and mapped onto qubits using Pauli matrix expressions. Additionally, we
outline the design of quantum circuits and the implementation of measurements
to construct and observe specific quantum states. Information is extracted
through quantum measurements, with eigenstates constructed and overlap
measurements evaluated using universal quantum gates.; 43) Language Models for Automated Classification of Brain MRI Reports and
  Growth Chart Generation; Clinically acquired brain MRIs and radiology reports are valuable but
underutilized resources due to the challenges of manual analysis and data
heterogeneity. We developed fine-tuned language models (LMs) to classify brain
MRI reports as normal (reports with limited pathology) or abnormal, fine-tuning
BERT, BioBERT, ClinicalBERT, and RadBERT on 44,661 reports. We also explored
the reasoning capabilities of a leading LM, Gemini 1.5-Pro, for normal report
categorization. Automated image processing and modeling generated brain growth
charts from LM-classified normal scans, comparing them to human-derived charts.
Fine-tuned LMs achieved high classification performance (F1-Score >97%), with
unbalanced training mitigating class imbalance. Performance was robust on
out-of-distribution data, with full text outperforming summary (impression)
sections. Gemini 1.5-Pro showed a promising categorization performance,
especially with clinical inference. LM-derived brain growth charts were nearly
identical to human-annotated charts (r = 0.99, p < 2.2e-16). Our LMs offer
scalable analysis of radiology reports, enabling automated classification of
brain MRIs in large datasets. One application is automated generation of brain
growth charts for benchmarking quantitative image features. Further research is
needed to address data heterogeneity and optimize LM reasoning.; 44) Intelligent Framework for Human-Robot Collaboration: Safety, Dynamic
  Ergonomics, and Adaptive Decision-Making; The integration of collaborative robots into industrial environments has
improved productivity, but has also highlighted significant challenges related
to operator safety and ergonomics. This paper proposes an innovative framework
that integrates advanced visual perception technologies, real-time ergonomic
monitoring, and Behaviour Tree (BT)-based adaptive decision-making. Unlike
traditional methods, which often operate in isolation or statically, our
approach combines deep learning models (YOLO11 and SlowOnly), advanced tracking
(Unscented Kalman Filter) and dynamic ergonomic assessments (OWAS), offering a
modular, scalable and adaptive system. Experimental results show that the
framework outperforms previous methods in several aspects: accuracy in
detecting postures and actions, adaptivity in managing human-robot
interactions, and ability to reduce ergonomic risk through timely robotic
interventions. In particular, the visual perception module showed superiority
over YOLOv9 and YOLOv8, while real-time ergonomic monitoring eliminated the
limitations of static analysis. Adaptive role management, made possible by the
Behaviour Tree, provided greater responsiveness than rule-based systems, making
the framework suitable for complex industrial scenarios. Our system
demonstrated a 92.5\% accuracy in grasping intention recognition and
successfully classified ergonomic risks with real-time responsiveness (average
latency of 0.57 seconds), enabling timely robotic; 45) Downlink OFDM-FAMA in 5G-NR Systems; Fluid antenna multiple access (FAMA), enabled by the fluid antenna system
(FAS), offers a new and straightforward solution to massive connectivity.
Previous results on FAMA were primarily based on narrowband channels. This
paper studies the adoption of FAMA within the fifth-generation (5G) orthogonal
frequency division multiplexing (OFDM) framework, referred to as OFDM-FAMA, and
evaluate its performance in broadband multipath channels. We first design the
OFDM-FAMA system, taking into account 5G channel coding and OFDM modulation.
Then the system's achievable rate is analyzed, and an algorithm to approximate
the FAS configuration at each user is proposed based on the rate. Extensive
link-level simulation results reveal that OFDM-FAMA can significantly improve
the multiplexing gain over the OFDM system with fixed-position antenna (FPA)
users, especially when robust channel coding is applied and the number of
radio-frequency (RF) chains at each user is small.; 46) Computational Equivalence of Spiked Covariance and Spiked Wigner Models
  via Gram-Schmidt Perturbation; In this work, we show the first average-case reduction transforming the
sparse Spiked Covariance Model into the sparse Spiked Wigner Model and as a
consequence obtain the first computational equivalence result between two
well-studied high-dimensional statistics models. Our approach leverages a new
perturbation equivariance property for Gram-Schmidt orthogonalization, enabling
removal of dependence in the noise while preserving the signal.; 47) One Model to Train them All: Hierarchical Self-Distillation for Enhanced
  Early Layer Embeddings; Deploying language models often requires handling model size vs. performance
trade-offs to satisfy downstream latency constraints while preserving the
model's usefulness. Model distillation is commonly employed to reduce model
size while maintaining acceptable performance. However, distillation can be
inefficient since it involves multiple training steps. In this work, we
introduce MODULARSTARENCODER, a modular multi-exit encoder with 1B parameters,
useful for multiple tasks within the scope of code retrieval.
MODULARSTARENCODER is trained with a novel self-distillation mechanism that
significantly improves lower-layer representations-allowing different portions
of the model to be used while still maintaining a good trade-off in terms of
performance. Our architecture focuses on enhancing text-to-code and
code-to-code search by systematically capturing syntactic and semantic
structures across multiple levels of representation. Specific encoder layers
are targeted as exit heads, allowing higher layers to guide earlier layers
during training. This self-distillation effect improves intermediate
representations, increasing retrieval recall at no extra training cost. In
addition to the multi-exit scheme, our approach integrates a repository-level
contextual loss that maximally utilizes the training context window, further
enhancing the learned representations. We also release a new dataset
constructed via code translation, seamlessly expanding traditional text-to-code
benchmarks with code-to-code pairs across diverse programming languages.
Experimental results highlight the benefits of self-distillation through
multi-exit supervision.; 48) Geometric modular flows in 2d CFT and beyond; We study geometric modular flows in two-dimensional conformal field theories.
We explore which states exhibit a geometric modular flow with respect to a
causally complete subregion and, conversely, how to construct a state from a
given geometric modular flow. Given suitable boundary conditions, we find that
generic geometric modular flows in the Rindler wedge are conformally
equivalent. Based on this insight, we show how conformal unitaries can be used
to explicitly construct a state for each flow. We analyze these states,
deriving general formulas for the energy density and entanglement entropy. We
also consider geometric flows beyond the Rindler wedge setting, and in higher
dimensions.; 49) The Infrared-Bright SW Knots in the Complex Ejecta of VY CMa; The red hypergiant VY CMa is remarkable for its very visible record of high
mass loss events observed over the range of wavelengths from the optical and
infrared to the submillimeter region with ALMA. The SW Clump or SW knots are
unique in the ejecta of VY CMa. Except for the central star, they are the
brightest sources of dusty infrared emission in its complex ejecta. In this
paper we combine the proper motions from the HST images, and infrared fluxes
from 2 to 12 microns with the 12CO images from ALMA to determine their ages and
mass estimates. The SW knots were ejected more than 200 years ago with an
active period lasting about 30 years, and with a total mass in the Clump more
than 0.02 Solar masses.; 50) PRISMe: A Novel LLM-Powered Tool for Interactive Privacy Policy
  Assessment; Protecting online privacy requires users to engage with and comprehend
website privacy policies, but many policies are difficult and tedious to read.
We present PRISMe (Privacy Risk Information Scanner for Me), a novel Large
Language Model (LLM)-driven privacy policy assessment tool, which helps users
to understand the essence of a lengthy, complex privacy policy while browsing.
The tool, a browser extension, integrates a dashboard and an LLM chat. One
major contribution is the first rigorous evaluation of such a tool. In a
mixed-methods user study (N=22), we evaluate PRISMe's efficiency, usability,
understandability of the provided information, and impacts on awareness. While
our tool improves privacy awareness by providing a comprehensible quick
overview and a quality chat for in-depth discussion, users note issues with
consistency and building trust in the tool. From our insights, we derive
important design implications to guide future policy analysis tools.; 51) Advancing Heat Demand Forecasting with Attention Mechanisms:
  Opportunities and Challenges; Global leaders and policymakers are unified in their unequivocal commitment
to decarbonization efforts in support of Net-Zero agreements. District Heating
Systems (DHS), while contributing to carbon emissions due to the continued
reliance on fossil fuels for heat production, are embracing more sustainable
practices albeit with some sense of vulnerability as it could constrain their
ability to adapt to dynamic demand and production scenarios. As demographic
demands grow and renewables become the central strategy in decarbonizing the
heating sector, the need for accurate demand forecasting has intensified.
Advances in digitization have paved the way for Machine Learning (ML) based
solutions to become the industry standard for modeling complex time series
patterns. In this paper, we focus on building a Deep Learning (DL) model that
uses deconstructed components of independent and dependent variables that
affect heat demand as features to perform multi-step ahead forecasting of head
demand. The model represents the input features in a time-frequency space and
uses an attention mechanism to generate accurate forecasts. The proposed method
is evaluated on a real-world dataset and the forecasting performance is
assessed against LSTM and CNN-based forecasting models. Across different supply
zones, the attention-based models outperforms the baselines quantitatively and
qualitatively, with an Mean Absolute Error (MAE) of 0.105 with a standard
deviation of 0.06kW h and a Mean Absolute Percentage Error (MAPE) of 5.4% with
a standard deviation of 2.8%, in comparison the second best model with a MAE of
0.10 with a standard deviation of 0.06kW h and a MAPE of 5.6% with a standard
deviation of 3%.; 52) Local Control Networks (LCNs): Optimizing Flexibility in Neural Network
  Data Pattern Capture; The widespread use of Multi-layer perceptrons (MLPs) often relies on a fixed
activation function (e.g., ReLU, Sigmoid, Tanh) for all nodes within the hidden
layers. While effective in many scenarios, this uniformity may limit the
networks ability to capture complex data patterns. We argue that employing the
same activation function at every node is suboptimal and propose leveraging
different activation functions at each node to increase flexibility and
adaptability. To achieve this, we introduce Local Control Networks (LCNs),
which leverage B-spline functions to enable distinct activation curves at each
node. Our mathematical analysis demonstrates the properties and benefits of
LCNs over conventional MLPs. In addition, we demonstrate that more complex
architectures, such as Kolmogorov-Arnold Networks (KANs), are unnecessary in
certain scenarios, and LCNs can be a more efficient alternative. Empirical
experiments on various benchmarks and datasets validate our theoretical
findings. In computer vision tasks, LCNs achieve marginal improvements over
MLPs and outperform KANs by approximately 5\%, while also being more
computationally efficient than KANs. In basic machine learning tasks, LCNs show
a 1\% improvement over MLPs and a 0.6\% improvement over KANs. For symbolic
formula representation tasks, LCNs perform on par with KANs, with both
architectures outperforming MLPs. Our findings suggest that diverse activations
at the node level can lead to improved performance and efficiency.; 53) An Interior Solution for the Kerr Metric: A Novel Approach; We present a novel approach for the construction of interior solutions for
the Kerr metric, extending J. Ovalle's foundational work through ellipsoidal
coordinate transformations. By deriving a physically plausible interior
solution that smoothly matches the Kerr exterior metric, we analyze the energy
conditions across various rotation parameters. Our findings reveal anisotropic
fluid properties and energy condition behaviors in specific space-time regions,
providing insights into the strong-field regime of rotating black holes. The
proposed solution offers a more realistic description of rotating black hole
interiors, with implications for understanding compact astrophysical objects.; 54) Quorum sensing and absorbing phase transitions in colloidal active
  matter; Unlike biological active matter that constantly adapt to their environment,
the motors of synthetic active particles are typically agnostic to their
surroundings and merely operate at constant force. Here, we design colloidal
active rods capable of modulating their inner activity in response to crowding,
thereby enforcing a primitive form of quorum sensing interactions. Through
experiments, simulations and theory we elucidate the impact of these
interactions on the phase behavior of isotropic active matter. We demonstrate
that, when conditioned to density, motility regulation can either lead to an
absorbing phase transition, where all particles freeze their dynamics, or to
atypical phase separation, where flat interfaces supporting a net pressure drop
are in mechanical equilibrium. Fully active and fully arrested particles can
then form heterogeneous patterns ruled by the competition between quorum
sensing and mechanical interactions. Beyond the specifics of motile colloids,
we expect our findings to apply broadly to adaptive active matter assembled
from living or synthetic units.; 55) Polarisation conversion and optical meron topologies in anisotropic
  epsilon-near-zero metamaterials; Plasmonic metamaterials provide a flexible platform for light manipulation
and polarisation management, thanks to their engineered optical properties with
exotic dispersion regimes. Here, we exploit the enhanced spin-orbit coupling
induced by the strong anisotropy of plasmonic nanorod metamaterials to control
the polarisation of vector vortex beams and generate complex field structures
with meron topology. Modifying the degree of ellipticity of the input
polarisation, we show how the observed meron topology can be additionally
manipulated. Flexible control of the state of polarisation of vortex beams is
important in optical manipulation, communications, metrology and quantum
technologies.; 56) Design and implementation of a novel cryptographically secure
  pseudorandom number generator; The aim of this paper is to present a new design for a pseudorandom number
generator (PRNG) that is cryptographically secure, passes all of the usual
statistical tests referenced in the literature and hence generates high quality
random sequences, that is compact and easy to implement in practice, of
portable design and offering reasonable execution times. Our procedure achieves
those objectives through the use of a sequence of modular exponentiations
followed by the application of Feistel-like boxes that mix up bits using a
nonlinear function. The results of extensive statistical tests on sequences of
about 2^40 bits in size generated by our algorithm are also presented.; 57) Ray-Tracing Channel Modeling for LEO Satellite-to-Ground Communication
  Systems; Based on the vision of global coverage for sixth-generation (6G) wireless
communication systems, the low earth orbit (LEO) satellite-to-ground channel
model for urban scenarios has emerged as highly important for the system
design. In this paper, we propose an LEO satellite-to-ground channel model
through shooting and bouncing rays (SBR) algorithm to analyze the channel
characteristics. The orbit of LEO is modeled by the simplified general
perturbations 4 (SGP4), and an accurate celestial model is applied to calculate
the Doppler shift of multipath in a transmission time window of LEO
satellite-to-ground communications. Channel characteristics of LEO
satellite-to-ground communications such as the root-mean-square (RMS) delay
spread, the Doppler shift, and the received power at different times are
obtained. The simulation results show that the received power is only
significantly noticeable in the transmission time window when the satellite is
close to the receiver. Proposed model validates the effectiveness of
ray-tracing in actual LEO satellite-to-ground communication scenarios and
extends the calculation of the Doppler shift.; 58) Se-HiLo: Noise-Resilient Semantic Communication with High-and-Low
  Frequency Decomposition; Semantic communication has emerged as a transformative paradigm in
next-generation communication systems, leveraging advanced artificial
intelligence (AI) models to extract and transmit semantic representations for
efficient information exchange. Nevertheless, the presence of unpredictable
semantic noise, such as ambiguity and distortions in transmitted
representations, often undermines the reliability of received information.
Conventional approaches primarily adopt adversarial training with noise
injection to mitigate the adverse effects of noise. However, such methods
exhibit limited adaptability to varying noise levels and impose additional
computational overhead during model training. To address these challenges, this
paper proposes Noise-Resilient \textbf{Se}mantic Communication with
\textbf{Hi}gh-and-\textbf{Lo}w Frequency Decomposition (Se-HiLo) for image
transmission. The proposed Se-HiLo incorporates a Finite Scalar Quantization
(FSQ) based noise-resilient module, which bypasses adversarial training by
enforcing encoded representations within predefined spaces to enhance noise
resilience. While FSQ improves robustness, it compromise representational
diversity. To alleviate this trade-off, we adopt a transformer-based
high-and-low frequency decomposition module that decouples image
representations into high-and-low frequency components, mapping them into
separate FSQ representation spaces to preserve representational diversity.
Extensive experiments demonstrate that Se-HiLo achieves superior noise
resilience and ensures accurate semantic communication across diverse noise
environments.; 59) The Ejection of Transient Jets in Swift J1727.8-1613 Revealed by
  Time-Dependent Visibility Modelling; High angular resolution radio observations of relativistic jets are necessary
to understand the causal connection between accretion and jet ejection in low
mass X-ray binaries. Images from these observations can be difficult to
reconstruct due to the rapid intra-observational motion and variability of
transient jets. We have developed a time-dependent visibility model fitting and
self-calibration procedure and applied it to a single four-hour VLBA
observation of the low-mass X-ray binary Swift J1727.8-1613 during the bright
flaring period of its 2023 outburst. This allowed us to detect and model a
slightly resolved self-absorbed compact core, as well as three downstream
transient jet knots. We were able to precisely measure the proper motion and
flux density variability of these three jet knots, as well as (for the first
time) their intra-observational expansion. Using simultaneous multi-frequency
data, we were also able to measure the spectral index of the furthest
downstream jet knot, and the core, as well as the frequency-dependent core
shift between 2.3 and 8.3 GHz. Using these measurements, we inferred the
ejection dates of the three jet knots, including one to within $\pm40$ minutes,
which is one of the most precise ever measured. The ejection of the transient
jet knots coincided with a bright X-ray flare and a drastic change in the X-ray
spectral and timing properties as seen by HXMT, which is the clearest
association ever seen between the launching of transient relativistic jets in
an X-ray binary and a sudden change in the X-ray properties of the accretion
inflow.; 60) ($P_2+P_4$, $K_4-e$)-free graphs are nearly $\omega$-colorable; For a graph $G$, $\chi(G)$ and $\omega(G)$ respectively denote the chromatic
number and clique number of $G$. In this paper, we show that if $G$ is a
($P_2+P_4$, $K_4-e$)-free graph with $\omega(G)\geq 3$, then $\chi(G)\leq
\max\{6, \omega(G)\}$, and that the bound is tight for each $\omega(G)\notin
\{4,5\}$. This extends the results known for the class of ($P_2+P_3$,
$K_4-e$)-free graphs, improves the bound of Chen and Zhang
[arXiv:2412.14524[math.CO], 2024] given for the class of ($P_2+P_4$,
$K_4-e$)-free graphs, partially answers a question of Ju and the third author
[Theor. Comp. Sci. 993 (2024) Article No.: 114465] on `near optimal colorable
graphs', and partially answers a question of Schiermeyer (unpublished) on the
chromatic bound for ($P_7$, $K_4-e$)-free graphs.; 61) Hall Coefficient of the Intercalated Graphite CaC$_6$ in the Uniaxial
  CDW Ground State; We evaluate the Hall coefficient characterising magnetotransport in an
intercalated graphite CaC$_6$ with the Fermi surface reconstructed by an
uniaxial charge density wave from closed pockets to open sheets. As the typical
order parameter, corresponding to the pseudo-gap in electronic spectrum and
consequently to spacing between electron trajectories in reciprocal space, is
of the order of $10^2$K, magnetic breakdown in strong experimentally achievable
fields of the order of 10T is inevitable. The classical expressions for the
components of the magnetoconductivity tensor are strongly modified by magnetic
field-assisted over-gap tunneling causing quantum interference. Due to magnetic
breakdown, all magnetoconductivity components undergo strong quantum
oscillations reflected in the Hall coefficient. In their nature, these are
different than standard Shubnikov de Haas oscillations which would not appear
in a system with an open Fermi surface.; 62) Anatomy of torques from orbital Rashba textures: the case of Co/Al
  interfaces; In the context of orbitronics, the rising of the orbital angular momentum
generated at light metal interfaces from orbital textures via orbital
Rashba-Edelstein effects nowadays represent extraordinary alternatives to the
usual heavy-metal spin-based materials. In the light of very recent
experimental results [\textcolor{blue}{S. Krishnia \textit{et al.}, Nanoletters
2023, 23, 6785}], starting from state-of-the-art density functional theory
simulations, we provide theoretical insights into the emergence of very strong
orbital torques at the Co/Al interface location a strong orbital Rashba
texture. By using linear response theory, we calculate the exerted orbital
torque amplitudes, mainly of field-like intraband character, acting onto the
ultrathin Co. Moreover, we show that an insertion of a single atomic plane of
Pt between Co and Al is enough to suppress the effect which questions about the
anatomy of the torque action clearly behaving differently than in the standard
way. This work opens new routes to the engineering of spintronic devices.; 63) Potential Contribution of Young Pulsar Wind Nebulae to Galactic
  High-Energy Neutrino Emission; Pulsar wind nebulae (PWNe), especially the young ones, are among the most
energetic astrophysical sources in the Galaxy. It is usually believed that the
spin-down energy injected from the pulsars is converted into magnetic field and
relativistic electrons, but the possible presence of proton acceleration inside
PWNe cannot be ruled out. Previous works have estimated the neutrino emission
from PWNe using various source catalogs measured in gamma-rays. However, such
results rely on the sensitivity of TeV gamma-ray observations and may omit the
contribution by unresolved sources. Here we estimate the potential neutrino
emission from a synthetic population of PWNe in the Galaxy with a focus on the
ones that are still in the free expansion phase. In the calculation, we model
the temporal evolution of the free-expanding PWNe and consider the transport of
protons inside the PWNe. The Crab nebula is treated as a standard template for
young PWNe to evaluate some model parameters, such as the energy conversion
fraction of relativistic protons and the target gas density for the hadronic
process, which are relevant to neutrino production. In the optimistic case, the
neutrino flux from the simulated young PWNe may constitute to 5% of the
measured flux by IceCube around 100 TeV. At higher energy around 1 PeV, the
neutrino emission from the population highly depends on the injection spectral
shape, and also on the emission of the nearby prominent sources.; 64) Trajectory Planning and Control for Differentially Flat Fixed-Wing
  Aerial Systems; Efficient real-time trajectory planning and control for fixed-wing unmanned
aerial vehicles is challenging due to their non-holonomic nature, complex
dynamics, and the additional uncertainties introduced by unknown aerodynamic
effects. In this paper, we present a fast and efficient real-time trajectory
planning and control approach for fixed-wing unmanned aerial vehicles,
leveraging the differential flatness property of fixed-wing aircraft in
coordinated flight conditions to generate dynamically feasible trajectories.
The approach provides the ability to continuously replan trajectories, which we
show is useful to dynamically account for the curvature constraint as the
aircraft advances along its path. Extensive simulations and real-world
experiments validate our approach, showcasing its effectiveness in generating
trajectories even in challenging conditions for small FW such as wind
disturbances.; 65) Recovering Parameters from Edge Fluctuations: Beta-Ensembles and
  Critically-Spiked Models; Let $\Lambda=\{\Lambda_0,\Lambda_1,\Lambda_2,\ldots\}$ be the point process
that describes the edge scaling limit of either (i) ""regular"" beta-ensembles
with inverse temperature $\beta>0$, or (ii) the top eigenvalues of Wishart or
Gaussian invariant random matrices perturbed by $r_0\geq1$ critical spikes. In
other words, $\Lambda$ is the eigenvalue point process of one of the scalar or
multivariate stochastic Airy operators. We prove that a single observation of
$\Lambda$ suffices to recover (almost surely) either (i) $\beta$ in the case of
beta-ensembles, or (ii) $r_0$ in the case of critically-spiked models. Our
proof relies on the recently-developed semigroup theory for the multivariate
stochastic Airy operators.
  Going beyond these parameter-recovery applications, our results also (iii)
refine our understanding of the rigidity properties of $\Lambda$, and (iv) shed
new light on the equality (in distribution) of stochastic Airy spectra with
different dimensions and the same Robin boundary conditions.; 66) Real-time simulation enabled navigation control of magnetic soft
  continuum robots in confined lumens; Magnetic soft continuum robots (MSCRs) have emerged as a promising technology
for minimally invasive interventions, offering enhanced dexterity and
remote-controlled navigation in confined lumens. Unlike conventional guidewires
with pre-shaped tips, MSCRs feature a magnetic tip that actively bends under
applied magnetic fields. Despite extensive studies in modeling and simulation,
achieving real-time navigation control of MSCRs in confined lumens remains a
significant challenge. The primary reasons are due to robot-lumen contact
interactions and computational limitations in modeling MSCR nonlinear behavior
under magnetic actuation. Existing approaches, such as Finite Element Method
(FEM) simulations and energy-minimization techniques, suffer from high
computational costs and oversimplified contact interactions, making them
impractical for real-world applications. In this work, we develop a real-time
simulation and navigation control framework that integrates hard-magnetic
elastic rod theory, formulated within the Discrete Differential Geometry (DDG)
framework, with an order-reduced contact handling strategy. Our approach
captures large deformations and complex interactions while maintaining
computational efficiency. Next, the navigation control problem is formulated as
an inverse design task, where optimal magnetic fields are computed in real time
by minimizing the constrained forces and enhancing navigation accuracy. We
validate the proposed framework through comprehensive numerical simulations and
experimental studies, demonstrating its robustness, efficiency, and accuracy.
The results show that our method significantly reduces computational costs
while maintaining high-fidelity modeling, making it feasible for real-time
deployment in clinical settings.; 67) Reducing Simulation Effort for RIS Optimization using an Efficient
  Far-Field Approximation; Optimization of Reconfigurable Intelligent Surfaces (RIS) via a previously
introduced method is effective, but time-consuming, because multiport impedance
or scatter matrices are required for each transmitter and receiver position,
which generally must be obtained through full-wave simulation. Herein, a simple
and efficient far-field approximation is introduced, to extrapolate scatter
matrices for arbitrary receiver and transmitter positions from only a single
simulation while still maintaining high accuracy suitable for optimization
purposes. This is demonstrated through comparisons of the optimized capacitance
values and further supported by empirical measurements.; 68) CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous
  Driving; Vehicle-to-vehicle (V2V) cooperative autonomous driving holds great promise
for improving safety by addressing the perception and prediction uncertainties
inherent in single-agent systems. However, traditional cooperative methods are
constrained by rigid collaboration protocols and limited generalization to
unseen interactive scenarios. While LLM-based approaches offer generalized
reasoning capabilities, their challenges in spatial planning and unstable
inference latency hinder their direct application in cooperative driving. To
address these limitations, we propose CoLMDriver, the first full-pipeline
LLM-based cooperative driving system, enabling effective language-based
negotiation and real-time driving control. CoLMDriver features a parallel
driving pipeline with two key components: (i) an LLM-based negotiation module
under an actor-critic paradigm, which continuously refines cooperation policies
through feedback from previous decisions of all vehicles; and (ii) an
intention-guided waypoint generator, which translates negotiation outcomes into
executable waypoints. Additionally, we introduce InterDrive, a CARLA-based
simulation benchmark comprising 10 challenging interactive driving scenarios
for evaluating V2V cooperation. Experimental results demonstrate that
CoLMDriver significantly outperforms existing approaches, achieving an 11%
higher success rate across diverse highly interactive V2V driving scenarios.
Code will be released on https://github.com/cxliu0314/CoLMDriver.; 69) ConRFT: A Reinforced Fine-tuning Method for VLA Models via Consistency
  Policy; Vision-Language-Action (VLA) models have shown substantial potential in
real-world robotic manipulation. However, fine-tuning these models through
supervised learning struggles to achieve robust performance due to limited,
inconsistent demonstrations, especially in contact-rich environments. In this
paper, we propose a reinforced fine-tuning approach for VLA models, named
ConRFT, which consists of offline and online fine-tuning with a unified
consistency-based training objective, to address these challenges. In the
offline stage, our method integrates behavior cloning and Q-learning to
effectively extract policy from a small set of demonstrations and stabilize
value estimating. In the online stage, the VLA model is further fine-tuned via
consistency policy, with human interventions to ensure safe exploration and
high sample efficiency. We evaluate our approach on eight diverse real-world
manipulation tasks. It achieves an average success rate of 96.3% within 45-90
minutes of online fine-tuning, outperforming prior supervised methods with a
144% improvement in success rate and 1.9x shorter episode length. This work
highlights the potential of integrating reinforcement learning to enhance the
performance of VLA models for real-world robotic applications.; 70) Holographic Description of Bulk Wave Packets in AdS$_4$/CFT$_3$; In this paper, we extend the study of wave packets from the AdS$_3$/CFT$_2$
correspondence to AdS$_4$/CFT$_3$ and examine properties of their energy
density. We find that, while the energy still localizes on the light cone, its
spatial distribution exhibits momentum dependence and is no longer localized in
higher dimensions. This result is significant as it reflects leading $1/N$
corrections to the generalized free field theory associated with the free bulk
theory at $N=\infty$. Our findings are consistent with previous studies on
entanglement wedge reconstruction including $1/N$ corrections, and also provide
new insights into the structure of wave packets in higher-dimensional
holography.; 71) The spectrum of the multi-frequency quasi-periodic CMV matrices contains
  intervals; We investigate the spectral structure of multi-frequency quasi-periodic CMV
matrices with Verblunsky coefficients defined by shifts on the $d$-dimensional
torus. Under the positive Lyapunov exponent regime and standard Diophantine
frequency conditions, we establish that the spectrum of these operators
contains intervals on the unit circle.; 72) Spider's webs and sharp $L^p$ bounds for the Hardy--Littlewood maximal
  operator on Gromov hyperbolic spaces; In this paper we prove that if $1<a\leq b<a^2$ and $X$ is a locally doubling
$\delta$-hyperbolic complete connected length metric measure space with
$(a,b)$-pinched exponential growth at infinity, then the centred
Hardy--Littlewood maximal operator $\mathcal M$ is bounded on $L^p(X)$ for all
$p>\tau$, and it is of weak type $(\tau,\tau)$, where $\tau := \log_ab$. A key
step in the proof is a new structural theorem for Gromov hyperbolic spaces with
$(a,b)$-pinched exponential growth at infinity, consisting in a discretisation
of $X$ by means of certain graphs, introduced in this paper and called spider's
webs, with ``good connectivity properties"". Our result applies to trees with
bounded geometry, and Cartan--Hadamard manifolds of pinched negative curvature,
providing new boundedness results in these settings. The index $\tau$ is
optimal in the sense that if $p<\tau$, then there exists $X$ satisfying the
assumptions above such that $\mathcal M$ is not of weak type $(p,p)$.
Furthermore, if $b>a^2$, then there are examples of spaces $X$ satisfying the
assumptions above such that $\mathcal M$ bounded on $L^p(X)$ if and only if
$p=\infty$.; 73) Multi-agent coordination via communication partitions; Coordinating the behaviour of self-interested agents in the presence of
multiple Nash equilibria is a major research challenge for multi-agent systems.
Pre-game communication between all the players can aid coordination in cases
where the Pareto-optimal payoff is unique, but can lead to deadlocks when there
are multiple payoffs on the Pareto frontier. We consider a communication
partition, where only players within the same coalition can communicate with
each other, and they can establish an agreement (a coordinated joint-action) if
it is envy-free, credible, and Pareto-optimal. We show that under a natural
assumption about symmetry, certain communication partitions can induce social
optimal outcomes in singleton congestion games. This game is a reasonable model
for a decentralised, anonymous system where players are required to choose from
a range of identical resources, and incur costs that are increasing and convex
in the total number of players sharing the same resource. The communication
partition can be seen as a mechanism for inducing efficient outcomes in this
context.; 74) Collision Risk Quantification and Conflict Resolution in Trajectory
  Tracking for Acceleration-Actuated Multi-Robot Systems; One of the pivotal challenges in a multi-robot system is how to give
attention to accuracy and efficiency while ensuring safety. Prior arts cannot
strictly guarantee collision-free for an arbitrarily large number of robots or
the results are considerably conservative. Smoothness of the avoidance
trajectory also needs to be further optimized. This paper proposes an
accelerationactuated simultaneous obstacle avoidance and trajectory tracking
method for arbitrarily large teams of robots, that provides a nonconservative
collision avoidance strategy and gives approaches for deadlock avoidance. We
propose two ways of deadlock resolution, one involves incorporating an
auxiliary velocity vector into the error function of the trajectory tracking
module, which is proven to have no influence on global convergence of the
tracking error. Furthermore, unlike the traditional methods that they address
conflicts after a deadlock occurs, our decision-making mechanism avoids the
near-zero velocity, which is much more safer and efficient in crowed
environments. Extensive comparison show that the proposed method is superior to
the existing studies when deployed in a large-scale robot system, with minimal
invasiveness.; 75) Towards Learning High-Precision Least Squares Algorithms with Sequence
  Models; This paper investigates whether sequence models can learn to perform
numerical algorithms, e.g. gradient descent, on the fundamental problem of
least squares. Our goal is to inherit two properties of standard algorithms
from numerical analysis: (1) machine precision, i.e. we want to obtain
solutions that are accurate to near floating point error, and (2) numerical
generality, i.e. we want them to apply broadly across problem instances. We
find that prior approaches using Transformers fail to meet these criteria, and
identify limitations present in existing architectures and training procedures.
First, we show that softmax Transformers struggle to perform high-precision
multiplications, which prevents them from precisely learning numerical
algorithms. Second, we identify an alternate class of architectures, comprised
entirely of polynomials, that can efficiently represent high-precision gradient
descent iterates. Finally, we investigate precision bottlenecks during training
and address them via a high-precision training recipe that reduces stochastic
gradient noise. Our recipe enables us to train two polynomial architectures,
gated convolutions and linear attention, to perform gradient descent iterates
on least squares problems. For the first time, we demonstrate the ability to
train to near machine precision. Applied iteratively, our models obtain
100,000x lower MSE than standard Transformers trained end-to-end and they incur
a 10,000x smaller generalization gap on out-of-distribution problems. We make
progress towards end-to-end learning of numerical algorithms for least squares.; 76) X-Dyna: Expressive Dynamic Human Image Animation; We introduce X-Dyna, a novel zero-shot, diffusion-based pipeline for
animating a single human image using facial expressions and body movements
derived from a driving video, that generates realistic, context-aware dynamics
for both the subject and the surrounding environment. Building on prior
approaches centered on human pose control, X-Dyna addresses key shortcomings
causing the loss of dynamic details, enhancing the lifelike qualities of human
video animations. At the core of our approach is the Dynamics-Adapter, a
lightweight module that effectively integrates reference appearance context
into the spatial attentions of the diffusion backbone while preserving the
capacity of motion modules in synthesizing fluid and intricate dynamic details.
Beyond body pose control, we connect a local control module with our model to
capture identity-disentangled facial expressions, facilitating accurate
expression transfer for enhanced realism in animated scenes. Together, these
components form a unified framework capable of learning physical human motion
and natural scene dynamics from a diverse blend of human and scene videos.
Comprehensive qualitative and quantitative evaluations demonstrate that X-Dyna
outperforms state-of-the-art methods, creating highly lifelike and expressive
animations. The code is available at https://github.com/bytedance/X-Dyna.; 77) Exploring the Finite-Temperature Behavior of Rydberg Atom Arrays: A
  Tensor Network Approach; Rydberg atom arrays have emerged as a powerful platform for experimental
research and a challenging subject for theoretical investigation in quantum
science. In this study, we investigate the finite-temperature properties of
two-dimensional square-lattice Rydberg atom arrays using the projected
entangled pair states (PEPS) method. By analyzing the thermal behavior of
systems in the checkerboard and striated phases, we extract critical exponents
and identify phase transition characteristics. Our results confirm that the
checkerboard phase transition belongs to the 2D Ising universality class, while
the striated phase exhibits critical exponents that deviate from known
universality classes, possibly due to finite-size effects. These findings
provide theoretical insights into the thermal stability of quantum phases in
Rydberg atom arrays and offer valuable guidance for future experimental
efforts.; 78) Complete intersection algebras with binomial Macaulay dual generator; In this paper, we characterize all Artinian complete intersection
$K$-algebras $A_F$ whose Macaulay dual generator $F$ is a binomial. In
addition, we prove that such
  complete intersection Artinian $K$-algebras $A_F$ satisfy the Strong
Lefschetz property.; 79) Oscillons from $Q$-balls; Using Renormalization Group Theory we show that oscillons in (1+1)-dimensions
can be obtained, at the leading nonlinear order, from $Q$-balls of universal
complex field theories. For potentials with a nonzero cubic or quartic term the
universal $Q$-ball theory is well approximated by the integrable complex
sine-Gordon model. This allows us to generalize the usual perturbative
expansion by Fodor et. al. beyond the simplest unmodulated oscillon case.
Concretely, we explain the characteristic amplitude modulations of excited
oscillons as an effect of formation of a two-$Q$-ball (two-oscillon) bound
state.; 80) The effect of boron incorporation on leakage and wake-up in
  ferroelectric Al_{1-x}Sc_xN; This study explores the influence of boron incorporation on the structural
and electrical properties of ferroelectric Aluminum Scandium Nitride
(Al_{1-x}Sc_xN ) thin films, focusing on leakage currents, wake-up effects, and
imprint behavior. Al_{1-x}Sc_xN films were incorporated with varying boron
concentrations and analyzed under different deposition conditions to determine
their structural integrity and ferroelectric performance. Key findings include
a reduction in leakage currents, non-trivial alterations in bandgap energy as
well as an increasing coercive fields with increasing boron content. Films with
6-13 at.% boron exhibited N-polar growth, while those with 16 at.% boron showed
mixed polarity after deposition, which affected their ferroelectric response
during the initial switching cycles - as did the addition of boron itself
compared to pure Al1-xScxN . With increasing boron content, wake-up became
gradually more pronounced and was strongest for pure Al_{1-x}B_xN .; 81) LLM-Advisor: An LLM Benchmark for Cost-efficient Path Planning across
  Multiple Terrains; Multi-terrain cost-efficient path planning is a crucial task in robot
navigation, requiring the identification of a path from the start to the goal
that not only avoids obstacles but also minimizes travel costs. This is
especially crucial for real-world applications where robots need to navigate
diverse terrains in outdoor environments, where recharging or refueling is
difficult. However, there is very limited research on this topic. In this
paper, we develop a prompt-based approach, LLM-Advisor, which leverages large
language models (LLMs) as effective advisors for path planning. The LLM-Advisor
selectively provides suggestions, demonstrating its ability to recognize when
no modifications are necessary. When suggestions are made, 70.59% of the paths
suggested for the A* algorithm, 69.47% for the RRT* algorithm, and 78.70% for
the LLM-A* algorithm achieve greater cost efficiency. Since LLM-Advisor may
occasionally lack common sense in their suggestions, we propose two
hallucination-mitigation strategies. Furthermore, we experimentally verified
that GPT-4o performs poorly in zero-shot path planning, even when terrain
descriptions are clearly provided, demonstrating its low spatial awareness. We
also experimentally demonstrate that using an LLM as an advisor is more
effective than directly integrating it into the path-planning loop. Since LLMs
may generate hallucinations, using LLMs in the loop of a search-based method
(such as A*) may lead to a higher number of failed paths, demonstrating that
our proposed LLM-Advisor is a better choice.; 82) Federated Retrieval Augmented Generation for Multi-Product Question
  Answering; Recent advancements in Large Language Models and Retrieval-Augmented
Generation have boosted interest in domain-specific question-answering for
enterprise products. However, AI Assistants often face challenges in
multi-product QA settings, requiring accurate responses across diverse domains.
Existing multi-domain RAG-QA approaches either query all domains
indiscriminately, increasing computational costs and LLM hallucinations, or
rely on rigid resource selection, which can limit search results. We introduce
MKP-QA, a novel multi-product knowledge-augmented QA framework with
probabilistic federated search across domains and relevant knowledge. This
method enhances multi-domain search quality by aggregating query-domain and
query-passage probabilistic relevance. To address the lack of suitable
benchmarks for multi-product QAs, we also present new datasets focused on three
Adobe products: Adobe Experience Platform, Target, and Customer Journey
Analytics. Our experiments show that MKP-QA significantly boosts multi-product
RAG-QA performance in terms of both retrieval accuracy and response quality.; 83) Convolution-Based Converter : A Weak-Prior Approach For Modeling
  Stochastic Processes Based On Conditional Density Estimation; In this paper, a Convolution-Based Converter (CBC) is proposed to develop a
methodology for removing the strong or fixed priors in estimating the
probability distribution of targets based on observations in the stochastic
process. Traditional approaches, e.g., Markov-based and Gaussian process-based
methods, typically leverage observations to estimate targets based on strong or
fixed priors (such as Markov properties or Gaussian prior). However, the
effectiveness of these methods depends on how well their prior assumptions
align with the characteristics of the problem. When the assumed priors are not
satisfied, these approaches may perform poorly or even become unusable. To
overcome the above limitation, we introduce the Convolution-Based converter
(CBC), which implicitly estimates the conditional probability distribution of
targets without strong or fixed priors, and directly outputs the expected
trajectory of the stochastic process that satisfies the constraints from
observations. This approach reduces the dependence on priors, enhancing
flexibility and adaptability in modeling stochastic processes when addressing
different problems. Experimental results demonstrate that our method
outperforms existing baselines across multiple metrics.; 84) Combinatorial and Computational Insights about Patient-to-room
  Assignment under Consideration of Roommate Compatibility; During a hospital stay, a roommate can significantly influence a patient's
overall experience both positivly and negatively. Therefore, hospital staff
tries to assign patients together to a room that are likely to be compatible.
However, there are more conditions and objectives to be respected by the
patient-to-room assignment (PRA), e.g., ensuring gender separated rooms and
avoiding transfers. In this paper, we review the literature for reasons why
roommate compatibility is important as well as for criteria that can help to
increase the probability that two patients are suitable roommates. We further
present combinatorial insights about computing patient-to-room assignments with
optimal overall roommate compatibility. We then compare different
IP-formulations for PRA as well as the influence of different scoring functions
for patient compatibility on the runtime of PRA integer programming (IP)
optimisation. Using these results and real-world data, we conclude this paper
by developing and evaluating a fast IP-based solution approach for the dynamic
PRA.; 85) Embedding of a Discrete Lattice Structure in a Smooth Manifold; I propose a mathematical framework for embedding an unshaped discrete lattice
$L$ on a smooth manifold $M$. This framework simplifies complex concepts in
pure mathematics and physics by connecting discrete lattice structures with
continuous geometric interpretations through practical embeddings.; 86) Parametric Hypersensitivity and Transport in the Steady-State
  Open-System Holstein Model; We demonstrate that the nonequilibrium steady state (NESS) of an open-system
Holstein model with linear bias displays extreme sensitivity to the closed
system parameters. This sensitivity is shown to correspond to avoided crossings
in the closed system spectrum, as previously demonstrated in the Rabi model. We
then develop a kinetic model to analyze the effects of environmental parameters
on NESS hypersensitivity. This reveals that hypersensitivity only exists in
intermediate environmental parameter regimes, a prediction that is verified
numerically. The inherent spatial character of the Holstein model offers a
natural connection to transport, revealing that transport properties in the
steady-state regime can be optimized by simultaneously coordinating the closed-
and open-system parameters.; 87) A Peanut-hull-PLA based 3D printing filament with antimicrobial effect; Peanut hulls, also known as Arachis hypogaea L. particles (AHL), are an
abundant biomass source with a long shelf life. In this study, we incorporate
peanut hull powder into PLA polymer, imparting recyclability, biodegradability,
and biocompatibility, along with the antimicrobial properties of AHL particles.
In particular, we treat AHL particles as a reinforcement for PLA polymer to
produce 3D printing filament compatible with the fused filament fabrication
(FFF) 3D printing method. We provide a step-by-step method for preparing AHL
particles, incorporating them into PLA, and ultimately forming high-quality
filaments. We assess the quality of the filaments in terms of extruded
dimensions, mechanical strength, and elastic modulus, along with physical
properties such as porosity and melt flow index. We evaluate the printability
and wettability of the filaments as well. Notably, and unlike other
biomass-based reinforcements in PLA, AHL preserves the filament's strength and
enhances its elastic modulus. 3D-printed components fabricated using our
PLA-AHL filaments successfully retain their antimicrobial properties and
exhibit increased overall hardness. However, this comes at the expense of
forming more microvoids and a rougher surface, making the material more prone
to fracture and leading to a slight reduction in fracture toughness with
increasing AHL mass fraction.; 88) Higher-order multiscale method and its convergence analysis for
  nonlinear thermo-electric coupling problems of composite structures; This paper proposes a higher-order multiscale computational method for
nonlinear thermo-electric coupling problems of composite structures, which
possess temperature-dependent material properties and nonlinear Joule heating.
The innovative contributions of this work are the novel multiscale formulation
with the higher-order correction terms for periodic composite structures and
the global error estimation with an explicit rate for higher-order multiscale
solutions. By employing the multiscale asymptotic approach and the Taylor
series technique, the higher-order multiscale method is established for
time-dependent nonlinear thermo-electric coupling problems, which can keep the
local balance of heat flux and electric charge for high-accuracy multiscale
simulation. Furthermore, an efficient numerical algorithm with off-line and
on-line stages is presented in detail, and corresponding convergent analysis is
also obtained. Two- and three-dimensional numerical experiments are conducted
to showcase the competitive advantages of the proposed method for simulating
the time-dependent nonlinear thermo-electric coupling problems in composite
structures, not only exceptional numerical accuracy, but also less
computational cost.; 89) A Non-Relativistic Limit for Heterotic Supergravity and its Gauge
  Lagrangian; We show that the non-relativistic (NR) limit of $D=10$ heterotic supergravity
has a finite gauge Lagrangian due to non-trivial cancellations of divergent
parts coming from the Chern-Simons terms in the curvature of the $\hat B$-field
and the Yang-Mills Lagrangian. This is similar to what happens in bosonic
supergravity between the Ricci scalar, $\hat R$, and the $- \frac{1}{12} \hat
H^2$ term after taking the same limit. In this work we present the explicit
form of the gauge transformations and curvatures after considering the NR limit
and we compute the finite gauge Lagrangian in its covariant form. As an
interesting property, the Green-Schwarz mechanism for the two-form can be
trivialized in this limit. Terms equivalent to Chern-Simons contributions
naturally arise from the previous property.; 90) Heterogenous Macro-Finance Model: A Mean-field Game Approach; We investigate the full dynamics of capital allocation and wealth
distribution of heterogeneous agents in a frictional economy during booms and
busts using tools from mean-field games. Two groups in our models, namely the
expert and the household, are interconnected within and between their classes
through the law of capital processes and are bound by financial constraints.
Such a mean-field interaction explains why experts accumulate a lot of capital
in the good times and reverse their behavior quickly in the bad times even in
the absence of aggregate macro-shocks. When common noises from the market are
involved, financial friction amplifies the mean-field effect and leads to
capital fire sales by experts. In addition, the implicit interlink between and
within heterogeneous groups demonstrates the slow economic recovery and
characterizes the deviating and fear-of-missing-out (FOMO) behaviors of
households compared to their counterparts. Our model also gives a fairly
explicit representation of the equilibrium solution without exploiting
complicated numerical approaches.; 91) The distribution of negative eigenvalues of Schr\""odinger operators on
  asymptotically hyperbolic manifolds; We study the asymptotic behavior of the counting function of negative
eigenvalues of Schr\""odinger operators with real valued potentials on
asymptotically hyperbolic manifolds. We establish conditions on the potential
that determine if there are finitely or infinitely many negative eigenvalues.
In the latter case, they may only accumulate at zero and we obtain the
asymptotic behavior of the counting function of eigenvalues in an interval
$(-\infty,-E)$ as $E\rightarrow 0$.; 92) SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance
  Software Engineering?; We introduce SWE-Lancer, a benchmark of over 1,400 freelance software
engineering tasks from Upwork, valued at \$1 million USD total in real-world
payouts. SWE-Lancer encompasses both independent engineering tasks--ranging
from \$50 bug fixes to \$32,000 feature implementations--and managerial tasks,
where models choose between technical implementation proposals. Independent
tasks are graded with end-to-end tests triple-verified by experienced software
engineers, while managerial decisions are assessed against the choices of the
original hired engineering managers. We evaluate model performance and find
that frontier models are still unable to solve the majority of tasks. To
facilitate future research, we open-source a unified Docker image and a public
evaluation split, SWE-Lancer Diamond
(https://github.com/openai/SWELancer-Benchmark). By mapping model performance
to monetary value, we hope SWE-Lancer enables greater research into the
economic impact of AI model development.; 93) A multi-purpose reciprocating probe drive system for studying the effect
  of gas-puffs on edge plasma dynamics in the ADITYA-U tokamak; This article reports the development of a versatile high-speed reciprocating
drive system (HRDS) with interchangeable probe heads to characterize the edge
plasma region of ADITYA-U tokamak. This reciprocating probe drive system
consisting of Langmuir and magnetic probe heads, is designed, fabricated,
installed, and operated for studying the extent of fuel/impurity gas
propagation and its influence on plasma dynamics in the far-edge region inside
the last closed magnetic flux surface (LCFS). The HRDS is driven by a highly
accurate, easy-to-control, dynamic, brushless, permanently excited synchronous
servo motor operated by a PXI-commanded controller. The system is remotely
operated and allows for precise control of the speed, acceleration, and
distance traveled of the probe head on a shot-to-shot basis, facilitating
seamless control of operations according to experimental requirements. Using
this system, consisting of a linear array of Langmuir probes, measurements of
plasma density, temperature, potential, and their fluctuations revealed that
the fuel gas-puff impact these mean and fluctuating parameters up to three to
four cm inside the LCFS. Attaching an array of magnetic probes to this system
led to measurements of magnetic fluctuations inside the LCFS. The HRDS system
is fully operational and serves as an important diagnostic tool for ADITYA-U
tokamak.; 94) Exploiting Non-uniform Quantization for Enhanced ILC in Wideband Digital
  Pre-distortion; In this paper, it is identified that lowering the reference level at the
vector signal analyzer can significantly improve the performance of iterative
learning control (ILC). We present a mathematical explanation for this
phenomenon, where the signals experience logarithmic transform prior to
analogue-to-digital conversion, resulting in non-uniform quantization. This
process reduces the quantization noise of low-amplitude signals that constitute
a substantial portion of orthogonal frequency division multiplexing (OFDM)
signals, thereby improving ILC performance. Measurement results show that
compared to setting the reference level to the peak amplitude, lowering the
reference level achieves 3 dB improvement on error vector magnitude (EVM) and
15 dB improvement on normalized mean square error (NMSE) for 320 MHz WiFi OFDM
signals.; 95) Theoretical Study of Terahertz Absorption Spectra and Neutron Inelastic
  Scattering in Frustrated Magnet $\text{Tb}_2\text{Ti}_2\text{O}_7$; Within the framework of the single-particle approximation, the envelopes of
the spectral lines of terahertz absorption and inelastic neutron scattering
corresponding to magnetic dipole transitions between the sublevels of Tb$^{3+}$
ions in the Tb$_2$Ti$_2$O$_7$ crystal, split by the field of random
deformations induced by point defects of the crystal lattice upon violation of
the stoichiometric composition of the crystal, were calculated.; 96) Censor Resistant Instruction Independent Obfuscation for Multiple
  Programs; This work builds upon and optimizes our prior research on obfuscation as
instruction decorrelation which achieves multiple program obfuscation.
Leveraging this infrastructure, we further achieve the property of
sensor-resistant computation.; 97) Minerva: A Programmable Memory Test Benchmark for Language Models; How effectively can LLM-based AI assistants utilize their memory (context) to
perform various tasks? Traditional data benchmarks, which are often manually
crafted, suffer from several limitations: they are static, susceptible to
overfitting, difficult to interpret, and lack actionable insights--failing to
pinpoint the specific capabilities a model lacks when it does not pass a test.
In this paper, we present a framework for automatically generating a
comprehensive set of tests to evaluate models' abilities to use their memory
effectively. Our framework extends the range of capability tests beyond the
commonly explored (passkey, key-value, needle in the haystack) search, a
dominant focus in the literature. Specifically, we evaluate models on atomic
tasks such as searching, recalling, editing, matching, comparing information in
context memory, and performing basic operations when inputs are structured into
distinct blocks, simulating real-world data. Additionally, we design composite
tests to investigate the models' ability to maintain state while operating on
memory. Our benchmark enables an interpretable, detailed assessment of memory
capabilities of LLMs.; 98) A proof of generic Green's conjecture in odd genus; In this note, we give a new proof of Voisin's theorem on Green's conjecture
for generic curves of odd genus resembling the first two sections of ""Universal
Secant Bundles and Syzygies of Canonical Curves"" by the author, and so avoiding
the need for difficult computations.; 99) Risk-Aware Distributional Intervention Policies for Language Models; Language models are prone to occasionally undesirable generations, such as
harmful or toxic content, despite their impressive capability to produce texts
that appear accurate and coherent. This paper presents a new two-stage approach
to detect and mitigate undesirable content generations by rectifying
activations. First, we train an ensemble of layerwise classifiers to detect
undesirable content using activations by minimizing a smooth surrogate of the
risk-aware score. Then, for contents that are detected as undesirable, we
propose layerwise distributional intervention policies that perturb the
attention heads minimally while guaranteeing probabilistically the
effectiveness of the intervention. Benchmarks on several language models and
datasets show that our method outperforms baselines in reducing the generation
of undesirable output.; 100) DEAL: Data-Efficient Adversarial Learning for High-Quality Infrared
  Imaging; Thermal imaging is often compromised by dynamic, complex degradations caused
by hardware limitations and unpredictable environmental factors. The scarcity
of high-quality infrared data, coupled with the challenges of dynamic,
intricate degradations, makes it difficult to recover details using existing
methods. In this paper, we introduce thermal degradation simulation integrated
into the training process via a mini-max optimization, by modeling these
degraded factors as adversarial attacks on thermal images. The simulation is
dynamic to maximize objective functions, thus capturing a broad spectrum of
degraded data distributions. This approach enables training with limited data,
thereby improving model performance.Additionally, we introduce a
dual-interaction network that combines the benefits of spiking neural networks
with scale transformation to capture degraded features with sharp spike signal
intensities. This architecture ensures compact model parameters while
preserving efficient feature representation. Extensive experiments demonstrate
that our method not only achieves superior visual quality under diverse single
and composited degradation, but also delivers a significant reduction in
processing when trained on only fifty clear images, outperforming existing
techniques in efficiency and accuracy. The source code will be available at
https://github.com/LiuZhu-CV/DEAL.",0.0,1.0
