paper_a_id,research_type,paper_main_id,paper_main_text,paper_target_id,paper_target_text,list_true,llm_rec_list,reasoning,list_str,mrr_scores,ndcg10_scores
2411.01019,applied,2411.01019-pos1-0,"Incidental Anterior Mediastinal Nodular Lesions on Chest CT in Asymptomatic Subjects; Screening for lung cancer: 2023 guideline update from the American Cancer Society; Objective The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods We included 56,358 consecutive participants (mean age 52.4 ± 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66–0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01–3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes.; Abstract Lung cancer is the leading cause of mortality and person‐years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network‐validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit‐to‐radiation‐risk ratios follow‐up examinations. GDG also examined disease burden data National Institute’s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50–80 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low‐dose computed tomography asymptomatic currently smoke or formerly smoked have ≥20 pack‐year ( , ). Before decision made initiate LCS, should engage shared decision‐making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",2411.01019-pos2-0,"Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms; Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability.",97,"['97', '51', '67', '90', '94', '36', '19', '38', '73', '47']","The main paper discusses the prevalence and characteristics of incidental anterior mediastinal nodular lesions identified through chest CT scans, highlighting the importance of early detection and screening for lung cancer. The first candidate paper, 'Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms', directly aligns with the main topic by proposing a method for automating the detection and segmentation of these lesions, making it not only a natural fit but also a novel and useful contribution to improving lung cancer screening processes. The subsequent papers either explore related domains or provide insights that could support the primary focus on imaging and cancer detection, but none match the direct applicability and relevance of the first candidate.","1) Cosmological constraints from the Minkowski functionals of the BOSS
  CMASS galaxy sample; For the first time, we develop a simulation-based model for the Minkowski
functionals (MFs) of large-scale structure, which allows us to extract the full
information available from the MFs (including both the Gaussian and
non-Gaussian part), and apply it to the BOSS DR12 CMASS galaxy sample. Our
model is based on high-fidelity mock galaxy catalogs constructed from the
\textsc{Abacus}\textsc{Summit} simulations using the halo occupation
distribution (HOD) framework, which include the redshift-space distortions and
Alcock-Paczynski distortions, incorporate survey realism, including survey
geometry and veto masks, and account for angular plus radial selection effects.
The cosmological and HOD parameter dependence of the MFs is captured with a
neural network emulator trained from the galaxy mocks with various cosmological
and HOD parameters. To benchmark the constraining power of the MFs, we also
train an emulator for the galaxy 2-point correlation function (2PCF) using the
same pipeline. Having validated our approach through successful parameter
recovery tests on both internal and external mocks, including non-HOD forward
models of the halo-galaxy connection, we apply our forward model to analyze the
CMASS data in the redshift range $0.45<z<0.58$. We find the MFs provide
stronger constraints on the cosmological parameters than the 2PCF. The
combination of the two gives $\omega_{\rm cdm}=0.1172^{+0.0020}_{-0.0023}$,
$\sigma_8=0.783\pm 0.026$, and $n_s=0.966^{+0.019}_{-0.015}$, which are tighter
by a factor of 2.0, 1.9, and 1.6 than the 2PCF alone. The derived constraint
$f\sigma_8=0.453 \pm 0.016$ is also improved by a factor of 1.9, compared to
the 2PCF, and agrees well with Planck 2018 predictions and other results from a
series of studies in the literature.; 2) Bridging Social Psychology and LLM Reasoning: Conflict-Aware Meta-Review
  Generation via Cognitive Alignment; The rapid growth of scholarly submissions has overwhelmed traditional peer
review systems, driving the need for intelligent automation to preserve
scientific rigor. While large language models (LLMs) show promise in automating
manuscript critiques, their ability to synthesize high-stakes meta-reviews,
which require conflict-aware reasoning and consensus derivation, remains
underdeveloped. Existing methods fail to effectively handle conflicting
viewpoints within differing opinions, and often introduce additional cognitive
biases, such as anchoring effects and conformity bias.To overcome these
limitations, we propose the Cognitive Alignment Framework (CAF), a dual-process
architecture that transforms LLMs into adaptive scientific arbitrators. By
operationalizing Kahneman's dual-process theory, CAF introduces a three-step
cognitive pipeline: review initialization, incremental integration, and
cognitive alignment.Empirical validation shows that CAF outperforms existing
LLM-based methods, with sentiment consistency gains reaching up to 19.47\% and
content consistency improving by as much as 12.95\%.; 3) Haldane Fractional Statistics for 1D Heisenberg Spin XXX Chain; Haldane's fractional exclusion statistics (FES) describes a generalized Pauli
exclusion statistics, which can be regarded as an emergent quantum statistics
induced by the intrinsic dynamical interaction. A non-mutual FES has been
identified at the quantum criticality of the one-dimensional (1D) and 2D
interacting Bose Gas [Nat. Sci. Rev. 9, nwac027 (2022)]. It is naturally asked
if such a non-mutual FES can be induced by the spin-spin interaction in the
antiferromagnetic spin-1/2 XXX chain? In this article, we first represent the
Bethe ansatz equations of spin strings in terms of the FES equations of
different species. Then we show that the 1D spin XXX chain remarkably possesses
the non-mutual FES in the critical region. We observe that the equation of
state in terms of the FES gives rises to full statistical properties of the
model at quantum criticality, which are in good agreement with the results
obtained from the thermodynamic Bethe ansatz (TBA) equations of the model. From
the non-mutual FES, we also precisely determine the quantum scaling functions,
which further agree well with the previous TBA results [Phys. Rev. B 96,
220401(R) (2017)]. Finally, we also build up an exact mapping between the
scaling functions of the Lieb-Liniger model and the spin Heisenberg spin chain
at quantum criticality. Our method provides deep insights into the critical
phase of matter from quantum FES point of view.; 4) GKG-LLM: A Unified Framework for Generalized Knowledge Graph
  Construction; The construction of Generalized Knowledge Graph (GKG), including knowledge
graph, event knowledge graph and commonsense knowledge graph, is fundamental
for various natural language processing tasks. Current studies typically
construct these types of graph separately, overlooking holistic insights and
potential unification that could be beneficial in computing resources and usage
perspectives. However, a key challenge in developing a unified framework for
GKG is obstacles arising from task-specific differences. In this study, we
propose a unified framework for constructing generalized knowledge graphs to
address this challenge. First, we collect data from 15 sub-tasks in 29 datasets
across the three types of graphs, categorizing them into in-sample,
counter-task, and out-of-distribution (OOD) data. Then, we propose a
three-stage curriculum learning fine-tuning framework, by iteratively injecting
knowledge from the three types of graphs into the Large Language Models.
Extensive experiments show that our proposed model improves the construction of
all three graph types across in-domain, OOD and counter-task data.; 5) On Benchmarking Human-Like Intelligence in Machines; Recent benchmark studies have claimed that AI has approached or even
surpassed human-level performances on various cognitive tasks. However, this
position paper argues that current AI evaluation paradigms are insufficient for
assessing human-like cognitive capabilities. We identify a set of key
shortcomings: a lack of human-validated labels, inadequate representation of
human response variability and uncertainty, and reliance on simplified and
ecologically-invalid tasks. We support our claims by conducting a human
evaluation study on ten existing AI benchmarks, suggesting significant biases
and flaws in task and label designs. To address these limitations, we propose
five concrete recommendations for developing future benchmarks that will enable
more rigorous and meaningful evaluations of human-like cognitive capacities in
AI with various implications for such AI applications.; 6) Reading Decisions from Gaze Direction during Graphics Turing Test of
  Gait Animation; We investigated gaze direction during movement observation. The eye movement
data were collected during an experiment, in which different models of movement
production (based on movement primitives, MPs) were compared in a two
alternatives forced choice task (2AFC).
  Participants observed side-by-side presentation of two naturalistic
3D-rendered human movement videos, where one video was based on motion captured
gait sequence, the other one was generated by recombining the machine-learned
MPs to approximate the same movement. The task was to discriminate between
these movements while their eye movements were recorded. We are complementing
previous binary decision data analyses with eye tracking data. Here, we are
investigating the role of gaze direction during task execution. We computed the
shared information between gaze features and decisions of the participants, and
between gaze features and correct answers.
  We found that eye movements reflect the decision of participants during the
2AFC task, but not the correct answer. This result is important for future
experiments, which should take advantage of eye tracking to complement binary
decision data.; 7) Average Nikolskii factors for random trigonometric polynomials; For $1\le p,q\le \infty$, the Nikolskii factor for a trigonometric polynomial
$T_{\bf a}$ is defined by $$\mathcal N_{p,q}(T_{\bf a})=\frac{\|T_{\bf
a}\|_{q}}{\|T_{\bf a}\|_{p}},\ \ T_{\bf
a}(x)=a_{1}+\sum\limits^{n}_{k=1}(a_{2k}\sqrt{2}\cos kx+a_{2k+1}\sqrt{2}\sin
kx).$$ We study this average Nikolskii factor for random trigonometric
polynomials with independent $N(0,\sigma^{2})$ coefficients and obtain that the
exact order. For $1\leq p<q<\infty$, the average Nikolskii factor is order
degree to the 0, as compared to the degree $1/p-1/q$ worst case bound. We also
give the generalization to random multivariate trigonometric polynomials.; 8) Boosting MCTS with Free Energy Minimization; Active Inference, grounded in the Free Energy Principle, provides a powerful
lens for understanding how agents balance exploration and goal-directed
behavior in uncertain environments. Here, we propose a new planning framework,
that integrates Monte Carlo Tree Search (MCTS) with active inference objectives
to systematically reduce epistemic uncertainty while pursuing extrinsic
rewards. Our key insight is that MCTS already renowned for its search
efficiency can be naturally extended to incorporate free energy minimization by
blending expected rewards with information gain. Concretely, the Cross-Entropy
Method (CEM) is used to optimize action proposals at the root node, while tree
expansions leverage reward modeling alongside intrinsic exploration bonuses.
This synergy allows our planner to maintain coherent estimates of value and
uncertainty throughout planning, without sacrificing computational
tractability. Empirically, we benchmark our planner on a diverse set of
continuous control tasks, where it demonstrates performance gains over both
standalone CEM and MCTS with random rollouts.; 9) PyEvalAI: AI-assisted evaluation of Jupyter Notebooks for immediate
  personalized feedback; Grading student assignments in STEM courses is a laborious and repetitive
task for tutors, often requiring a week to assess an entire class. For
students, this delay of feedback prevents iterating on incorrect solutions,
hampers learning, and increases stress when exercise scores determine admission
to the final exam. Recent advances in AI-assisted education, such as automated
grading and tutoring systems, aim to address these challenges by providing
immediate feedback and reducing grading workload. However, existing solutions
often fall short due to privacy concerns, reliance on proprietary closed-source
models, lack of support for combining Markdown, LaTeX and Python code, or
excluding course tutors from the grading process. To overcome these
limitations, we introduce PyEvalAI, an AI-assisted evaluation system, which
automatically scores Jupyter notebooks using a combination of unit tests and a
locally hosted language model to preserve privacy. Our approach is free,
open-source, and ensures tutors maintain full control over the grading process.
A case study demonstrates its effectiveness in improving feedback speed and
grading efficiency for exercises in a university-level course on numerics.; 10) Multi-Agent Verification: Scaling Test-Time Compute with Multiple
  Verifiers; By utilizing more computational resources at test-time, large language models
(LLMs) can improve without additional training. One common strategy uses
verifiers to evaluate candidate outputs. In this work, we propose a novel
scaling dimension for test-time compute: scaling the number of verifiers. We
introduce Multi-Agent Verification (MAV) as a test-time compute paradigm that
combines multiple verifiers to improve performance. We propose using Aspect
Verifiers (AVs), off-the-shelf LLMs prompted to verify different aspects of
outputs, as one possible choice for the verifiers in a MAV system. AVs are a
convenient building block for MAV since they can be easily combined without
additional training. Moreover, we introduce BoN-MAV, a simple multi-agent
verification algorithm that combines best-of-n sampling with multiple
verifiers. BoN-MAV demonstrates stronger scaling patterns than self-consistency
and reward model verification, and we demonstrate both weak-to-strong
generalization, where combining weak verifiers improves even stronger LLMs, and
self-improvement, where the same base model is used to both generate and verify
outputs. Our results establish scaling the number of verifiers as a promising
new dimension for improving language model performance at test-time.; 11) From Text to Visuals: Using LLMs to Generate Math Diagrams with Vector
  Graphics; Advances in large language models (LLMs) offer new possibilities for
enhancing math education by automating support for both teachers and students.
While prior work has focused on generating math problems and high-quality
distractors, the role of visualization in math learning remains under-explored.
Diagrams are essential for mathematical thinking and problem-solving, yet
manually creating them is time-consuming and requires domain-specific
expertise, limiting scalability. Recent research on using LLMs to generate
Scalable Vector Graphics (SVG) presents a promising approach to automating
diagram creation. Unlike pixel-based images, SVGs represent geometric figures
using XML, allowing seamless scaling and adaptability. Educational platforms
such as Khan Academy and IXL already use SVGs to display math problems and
hints. In this paper, we explore the use of LLMs to generate math-related
diagrams that accompany textual hints via intermediate SVG representations. We
address three research questions: (1) how to automatically generate math
diagrams in problem-solving hints and evaluate their quality, (2) whether SVG
is an effective intermediate representation for math diagrams, and (3) what
prompting strategies and formats are required for LLMs to generate accurate
SVG-based diagrams. Our contributions include defining the task of
automatically generating SVG-based diagrams for math hints, developing an LLM
prompting-based pipeline, and identifying key strategies for improving diagram
generation. Additionally, we introduce a Visual Question Answering-based
evaluation setup and conduct ablation studies to assess different pipeline
variations. By automating the math diagram creation, we aim to provide students
and teachers with accurate, conceptually relevant visual aids that enhance
problem-solving and learning experiences.; 12) Certifying Pareto-Optimality in Multi-Objective Maximum Satisfiability; Due to the wide employment of automated reasoning in the analysis and
construction of correct systems, the results reported by automated reasoning
engines must be trustworthy. For Boolean satisfiability (SAT) solvers - and
more recently SAT-based maximum satisfiability (MaxSAT) solvers -
trustworthiness is obtained by integrating proof logging into solvers, making
solvers capable of emitting machine-verifiable proofs to certify correctness of
the reasoning steps performed. In this work, we enable for the first time proof
logging based on the VeriPB proof format for multi-objective MaxSAT (MO-MaxSAT)
optimization techniques. Although VeriPB does not offer direct support for
multi-objective problems, we detail how preorders in VeriPB can be used to
provide certificates for MO-MaxSAT algorithms computing a representative
solution for each element in the non-dominated set of the search space under
Pareto-optimality, without extending the VeriPB format or the proof checker. By
implementing VeriPB proof logging into a state-of-the-art multi-objective
MaxSAT solver, we show empirically that proof logging can be made scalable for
MO-MaxSAT with reasonable overhead.; 13) Contextual bandits with entropy-based human feedback; In recent years, preference-based human feedback mechanisms have become
essential for enhancing model performance across diverse applications,
including conversational AI systems such as ChatGPT. However, existing
approaches often neglect critical aspects, such as model uncertainty and the
variability in feedback quality. To address these challenges, we introduce an
entropy-based human feedback framework for contextual bandits, which
dynamically balances exploration and exploitation by soliciting expert feedback
only when model entropy exceeds a predefined threshold. Our method is
model-agnostic and can be seamlessly integrated with any contextual bandit
agent employing stochastic policies. Through comprehensive experiments, we show
that our approach achieves significant performance improvements while requiring
minimal human feedback, even under conditions of suboptimal feedback quality.
This work not only presents a novel strategy for feedback solicitation but also
highlights the robustness and efficacy of incorporating human guidance into
machine learning systems. Our code is publicly available:
https://github.com/BorealisAI/CBHF; 14) Learning Strategic Language Agents in the Werewolf Game with Iterative
  Latent Space Policy Optimization; Large language model (LLM)-based agents have recently shown impressive
progress in a variety of domains, including open-ended conversation and
multi-step decision-making. However, applying these agents to social deduction
games such as Werewolf, which requires both strategic decision-making and
free-form language interaction, remains non-trivial. Traditional methods based
on Counterfactual Regret Minimization (CFR) or reinforcement learning (RL)
typically depend on a predefined action space, making them unsuitable for
language games with unconstrained text action space. Meanwhile, pure LLM-based
agents often suffer from intrinsic biases and require prohibitively large
datasets for fine-tuning. We propose Latent Space Policy Optimization (LSPO),
an iterative framework that addresses these challenges by first mapping
free-form text to a discrete latent space, where methods like CFR and RL can
learn strategic policy more effectively. We then translate the learned policy
back into natural language dialogues, which are used to fine-tune an LLM via
Direct Preference Optimization (DPO). By iteratively alternating between these
stages, our LSPO agent progressively enhances both strategic reasoning and
language communication. Experiment results on the Werewolf game show that our
method improves the agent's performance in each iteration and outperforms
existing Werewolf agents, underscoring its promise for free-form language
decision-making.; 15) Stochastic tamed 3D Navier-Stokes equations with locally weak
  monotonicity coefficients: existence, uniqueness and averaging principle; This paper investigates the stochastic tamed 3D Navier-Stokes equations with
locally weak monotonicity coefficients in the whole space as well as in the
three-dimensional torus, which play a crucial role in turbulent flows analysis.
A significant issue is addressed in this work, specifically, the reduced
regularity of the coefficients and the inapplicability of Gronwall's lemma
complicates the establishment of pathwise uniqueness for weak solutions.
Initially, the existence of a martingale solution for the system is established
via Galerkin approximation; thereafter, the pathwise uniqueness of this
martingale solution is confirmed by constructing a specialized control
function. Ultimately, the Yamada-Watanabe theorem is employed to establish the
existence and uniqueness of the strong solution to the system. Furthermore, an
averaging principle, referred to as the first Bogolyubov theorem, is
established for stochastic tamed 3D Navier-Stokes equations with highly
oscillating components, where the coefficients satisfy the assumptions of
linear growth and locally weak monotonicity. This result is achieved using
classical Khasminskii time discretization, which illustrates the convergence of
the solution from the original Cauchy problem to the averaged equation over a
finite interval [0, T].; 16) Tighter Value-Function Approximations for POMDPs; Solving partially observable Markov decision processes (POMDPs) typically
requires reasoning about the values of exponentially many state beliefs.
Towards practical performance, state-of-the-art solvers use value bounds to
guide this reasoning. However, sound upper value bounds are often
computationally expensive to compute, and there is a tradeoff between the
tightness of such bounds and their computational cost. This paper introduces
new and provably tighter upper value bounds than the commonly used fast
informed bound. Our empirical evaluation shows that, despite their additional
computational overhead, the new upper bounds accelerate state-of-the-art POMDP
solvers on a wide range of benchmarks.; 17) Strategic Learning with Local Explanations as Feedback; We investigate algorithmic decision problems where agents can respond
strategically to the decision maker's (DM) models. The demand for clear and
actionable explanations from DMs to (potentially strategic) agents continues to
rise. While prior work often treats explanations as full model disclosures,
explanations in practice might convey only partial information, which can lead
to misinterpretations and harmful responses. When full disclosure of the
predictive model is neither feasible nor desirable, a key open question is how
DMs can use explanations to maximise their utility without compromising agent
welfare. In this work, we explore well-known local and global explanation
methods, and establish a necessary condition to prevent explanations from
misleading agents into self-harming actions. Moreover, with conditional
homogeneity, we establish that action recommendation (AR)-based explanations
are sufficient for non-harmful responses, akin to the revelation principle in
information design. To operationalise AR-based explanations, we propose a
simple algorithm to jointly optimise the predictive model and AR policy to
balance DM outcomes with agent welfare. Our empirical results demonstrate the
benefits of this approach as a more refined strategy for safe and effective
partial model disclosure in algorithmic decision-making.; 18) Everyone-Can-Sing: Zero-Shot Singing Voice Synthesis and Conversion with
  Speech Reference; We propose a unified framework for Singing Voice Synthesis (SVS) and
Conversion (SVC), addressing the limitations of existing approaches in
cross-domain SVS/SVC, poor output musicality, and scarcity of singing data. Our
framework enables control over multiple aspects, including language content
based on lyrics, performance attributes based on a musical score, singing style
and vocal techniques based on a selector, and voice identity based on a speech
sample. The proposed zero-shot learning paradigm consists of one SVS model and
two SVC models, utilizing pre-trained content embeddings and a diffusion-based
generator. The proposed framework is also trained on mixed datasets comprising
both singing and speech audio, allowing singing voice cloning based on speech
reference. Experiments show substantial improvements in timbre similarity and
musicality over state-of-the-art baselines, providing insights into other
low-data music tasks such as instrumental style transfer. Examples can be found
at: everyone-can-sing.github.io.; 19) Dual Invariance Self-training for Reliable Semi-supervised Surgical
  Phase Recognition; Accurate surgical phase recognition is crucial for advancing
computer-assisted interventions, yet the scarcity of labeled data hinders
training reliable deep learning models. Semi-supervised learning (SSL),
particularly with pseudo-labeling, shows promise over fully supervised methods
but often lacks reliable pseudo-label assessment mechanisms. To address this
gap, we propose a novel SSL framework, Dual Invariance Self-Training (DIST),
that incorporates both Temporal and Transformation Invariance to enhance
surgical phase recognition. Our two-step self-training process dynamically
selects reliable pseudo-labels, ensuring robust pseudo-supervision. Our
approach mitigates the risk of noisy pseudo-labels, steering decision
boundaries toward true data distribution and improving generalization to unseen
data. Evaluations on Cataract and Cholec80 datasets show our method outperforms
state-of-the-art SSL approaches, consistently surpassing both supervised and
SSL baselines across various network architectures.; 20) Where to Go Next Day: Multi-scale Spatial-Temporal Decoupled Model for
  Mid-term Human Mobility Prediction; Predicting individual mobility patterns is crucial across various
applications. While current methods mainly focus on predicting the next
location for personalized services like recommendations, they often fall short
in supporting broader applications such as traffic management and epidemic
control, which require longer period forecasts of human mobility. This study
addresses mid-term mobility prediction, aiming to capture daily travel patterns
and forecast trajectories for the upcoming day or week. We propose a novel
Multi-scale Spatial-Temporal Decoupled Predictor (MSTDP) designed to
efficiently extract spatial and temporal information by decoupling daily
trajectories into distinct location-duration chains. Our approach employs a
hierarchical encoder to model multi-scale temporal patterns, including daily
recurrence and weekly periodicity, and utilizes a transformer-based decoder to
globally attend to predicted information in the location or duration chain.
Additionally, we introduce a spatial heterogeneous graph learner to capture
multi-scale spatial relationships, enhancing semantic-rich representations.
Extensive experiments, including statistical physics analysis, are conducted on
large-scale mobile phone records in five cities (Boston, Los Angeles, SF Bay
Area, Shanghai, and Tokyo), to demonstrate MSTDP's advantages. Applied to
epidemic modeling in Boston, MSTDP significantly outperforms the
best-performing baseline, achieving a remarkable 62.8% reduction in MAE for
cumulative new cases.; 21) Block-corrected Modularity for Community Detection; Unknown node attributes in complex networks may introduce community
structures that are important to distinguish from those driven by known
attributes. We propose a block-corrected modularity that discounts given block
structures present in the network to reveal communities masked by them. We show
analytically how the proposed modularity finds the community structure driven
by an unknown attribute in a simple network model. Further, we observe that the
block-corrected modularity finds the underlying community structure on a number
of simple synthetic network models while methods using different null models
fail. We develop an efficient spectral method as well as two Louvain-inspired
fine-tuning algorithms to maximize the proposed modularity and demonstrate
their performance on several synthetic network models. Finally, we assess our
methodology on various real-world citation networks built using the OpenAlex
data by correcting for the temporal citation patterns.; 22) Between Puppet and Actor: Reframing Authorship in this Age of AI Agents; This chapter examines the conceptual tensions in understanding artificial
intelligence (AI) agents' role in creative processes, particularly focusing on
Large Language Models (LLMs). Building upon Schmidt's 1954 categorization of
human-technology relationships and the classical definition of ""author,"" this
chapter proposes to understand AI agency as existing somewhere between that of
an inanimate puppet and a performing actor. While AI agents demonstrate a
degree of creative autonomy, including the ability to improvise and construct
complex narrative content in interactive storytelling, they cannot be
considered authors in the classical sense of the term. This chapter thus
suggests that AI agents exist in a dynamic state between human-controlled
puppets and semi-autonomous actors. This conceptual positioning reflects how AI
agents, while they can certainly contribute to creative work, remain bound to
human direction. We also argue that existing conceptual frames concerning
authorship should evolve and adapt to capture these new relationships.; 23) Flexible Full-Stokes Polarization Engineering by Disorder-Scrambled
  Metasurfaces; Abstract: The ability to arbitrarily and flexibly control the polarization of
light, including both the state of polarization (SoP) and the degree of
polarization (DoP), is highly important for quantum optics, polarization
imaging, and coherent optical communications. Although metasurfaces have shown
promise in polarization control, the few studies focusing on the DoP often lack
flexibility in manipulation. Here, we propose a novel approach using a
disordered metasurface to flexibly convert natural light into partially
polarized light, enabling independent and flexible control over all Stokes
parameters. The metasurface is composed of two types of meta-atoms, uniformly
distributed with specific quantity ratios, decoupling the design parameters in
the process of polarization control, and allowing a one-to-one correspondence
between metasurface and polarization spaces. The azimuthal and elevation angles
of the SoP on the Poincar\'e sphere are independently controlled by the
meta-atom rotation and size, while the DoP is governed by the quantity ratio. A
developed algorithm determines the disordered metasurface arrangement, with
theoretical calculations showing an average error of less than 3{\deg} for both
the azimuthal and elevation angles and a control accuracy of \pm 0.05 for the
DoP.; 24) Unifying and Optimizing Data Values for Selection via
  Sequential-Decision-Making; Data selection has emerged as a crucial downstream application of data
valuation. While existing data valuation methods have shown promise in
selection tasks, the theoretical foundations and full potential of using data
values for selection remain largely unexplored. In this work, we first
demonstrate that data values applied for selection can be naturally
reformulated as a sequential-decision-making problem, where the optimal data
value can be derived through dynamic programming. We show this framework
unifies and reinterprets existing methods like Data Shapley through the lens of
approximate dynamic programming, specifically as myopic reward function
approximations to this sequential problem. Furthermore, we analyze how
sequential data selection optimality is affected when the ground-truth utility
function exhibits monotonic submodularity with curvature. To address the
computational challenges in obtaining optimal data values, we propose an
efficient approximation scheme using learned bipartite graphs as surrogate
utility models, ensuring greedy selection is still optimal when the surrogate
utility is correctly specified and learned. Extensive experiments demonstrate
the effectiveness of our approach across diverse datasets.; 25) Intention Recognition in Real-Time Interactive Navigation Maps; In this demonstration, we develop IntentRec4Maps, a system to recognise
users' intentions in interactive maps for real-world navigation. IntentRec4Maps
uses the Google Maps Platform as the real-world interactive map, and a very
effective approach for recognising users' intentions in real-time. We showcase
the recognition process of IntentRec4Maps using two different Path-Planners and
a Large Language Model (LLM).
  GitHub: https://github.com/PeijieZ/IntentRec4Maps; 26) A combined statistical mechanical and ab initio approach to
  understanding H2O/CO2 co-adsorption in mmen-Mg2(dobpdc); We study the effects of H2O on CO2 adsorption in an amine-appended variant of
the metal-organic framework Mg2(dobpdc), which is known to exhibit chaining
behavior that presents in a step-shaped adsorption isotherm. We first show how
the presence of different levels of local H2O affects this chaining behavior
and the energetics of CO2 adsorption, based on a series of ab initio
calculations, giving insight into the atomic-scale environment. In particular,
we predict a novel adsorbed configuration, in which H2O and CO2 intertwine to
make a braided chain down the MOF pore. We then show how an existing lattice
model can be adapted to incorporate the effect of water, and predict the CO2
isotherms for the various water levels, observing a sharp shift the uptake at
low partial pressures. In addition to the physical further work on this and
related materials.; 27) Multi-Agent Collaboration Mechanisms: A Survey of LLMs; With recent advances in Large Language Models (LLMs), Agentic AI has become
phenomenal in real-world applications, moving toward multiple LLM-based agents
to perceive, learn, reason, and act collaboratively. These LLM-based
Multi-Agent Systems (MASs) enable groups of intelligent agents to coordinate
and solve complex tasks collectively at scale, transitioning from isolated
models to collaboration-centric approaches. This work provides an extensive
survey of the collaborative aspect of MASs and introduces an extensible
framework to guide future research. Our framework characterizes collaboration
mechanisms based on key dimensions: actors (agents involved), types (e.g.,
cooperation, competition, or coopetition), structures (e.g., peer-to-peer,
centralized, or distributed), strategies (e.g., role-based or model-based), and
coordination protocols. Through a review of existing methodologies, our
findings serve as a foundation for demystifying and advancing LLM-based MASs
toward more intelligent and collaborative solutions for complex, real-world use
cases. In addition, various applications of MASs across diverse domains,
including 5G/6G networks, Industry 5.0, question answering, and social and
cultural settings, are also investigated, demonstrating their wider adoption
and broader impacts. Finally, we identify key lessons learned, open challenges,
and potential research directions of MASs towards artificial collective
intelligence.; 28) MAGELLAN: Metacognitive predictions of learning progress guide autotelic
  LLM agents in large goal spaces; Open-ended learning agents must efficiently prioritize goals in vast
possibility spaces, focusing on those that maximize learning progress (LP).
When such autotelic exploration is achieved by LLM agents trained with online
RL in high-dimensional and evolving goal spaces, a key challenge for LP
prediction is modeling one's own competence, a form of metacognitive
monitoring. Traditional approaches either require extensive sampling or rely on
brittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive
framework that lets LLM agents learn to predict their competence and LP online.
By capturing semantic relationships between goals, MAGELLAN enables
sample-efficient LP estimation and dynamic adaptation to evolving goal spaces
through generalization. In an interactive learning environment, we show that
MAGELLAN improves LP prediction efficiency and goal prioritization, being the
only method allowing the agent to fully master a large and evolving goal space.
These results demonstrate how augmenting LLM agents with a metacognitive
ability for LP predictions can effectively scale curriculum learning to
open-ended goal spaces.; 29) Cup Products on Hochschild Cohomology of Hopf-Galois Extensions.pdf; In this paper, we give an explicit chain map, which induces the algebra
isomorphism between the Hochschild cohomology ${\bf HH}^{\bullet}(B)$ and the
$H$-invariant subalgebra ${\bf H}^{\bullet}(A, B)^{H}$ under two mild
hypotheses, where $H$ is a finite dimensional semisimple Hopf algebra and $B$
is an $H$-Galois extension of $A$. In particular, the smash product $B=A\#H$
always satisfies the mild hypotheses. The isomorphism between ${\bf
HH}^{\bullet}(A\#H)$ and ${\bf H}^{\bullet}(A, A\#H)^{H}$ generalizes the
classical result of group actions. As an application, Hochschild cohomology and
cup product of the smash product of the quantum $(-1)$-plane and Kac--Paljutkin
Hopf algebra are computed.; 30) Narrative-Driven Travel Planning: Geoculturally-Grounded Script
  Generation with Evolutionary Itinerary Optimization; To enhance tourists' experiences and immersion, this paper proposes a
narrative-driven travel planning framework called NarrativeGuide, which
generates a geoculturally-grounded narrative script for travelers, offering a
novel, role-playing experience for their journey. In the initial stage,
NarrativeGuide constructs a knowledge graph for attractions within a city, then
configures the worldview, character setting, and exposition based on the
knowledge graph. Using this foundation, the knowledge graph is combined to
generate an independent scene unit for each attraction. During the itinerary
planning stage, NarrativeGuide models narrative-driven travel planning as an
optimization problem, utilizing a genetic algorithm (GA) to refine the
itinerary. Before evaluating the candidate itinerary, transition scripts are
generated for each pair of adjacent attractions, which, along with the scene
units, form a complete script. The weighted sum of script coherence, travel
time, and attraction scores is then used as the fitness value to update the
candidate solution set. Experimental results across four cities, i.e., Nanjing
and Yangzhou in China, Paris in France, and Berlin in Germany, demonstrate
significant improvements in narrative coherence and cultural fit, alongside a
notable reduction in travel time and an increase in the quality of visited
attractions. Our study highlights that incorporating external evolutionary
optimization effectively addresses the limitations of large language models in
travel planning.; 31) A Scalable Approach to Probabilistic Neuro-Symbolic Verification; Neuro-Symbolic Artificial Intelligence (NeSy AI) has emerged as a promising
direction for integrating neural learning with symbolic reasoning. In the
probabilistic variant of such systems, a neural network first extracts a set of
symbols from sub-symbolic input, which are then used by a symbolic component to
reason in a probabilistic manner towards answering a query. In this work, we
address the problem of formally verifying the robustness of such NeSy
probabilistic reasoning systems, therefore paving the way for their safe
deployment in critical domains. We analyze the complexity of solving this
problem exactly, and show that it is $\mathrm{NP}^{\# \mathrm{P}}$-hard. To
overcome this issue, we propose the first approach for approximate,
relaxation-based verification of probabilistic NeSy systems. We demonstrate
experimentally that the proposed method scales exponentially better than
solver-based solutions and apply our technique to a real-world autonomous
driving dataset, where we verify a safety property under large input
dimensionalities and network sizes.; 32) A Driver Advisory System Based on Large Language Model for High-speed
  Train; With the rapid development of China high-speed railway, drivers face
increasingly significant technical challenges during operations, such as fault
handling. Currently, drivers depend on the onboard mechanic when facing
technical issues, for instance, traction loss or sensor faults. This dependency
can hinder effective operation, even lead to accidents, while waiting for
faults to be addressed. To enhance the accuracy and explainability of actions
during fault handling, an Intelligent Driver Advisory System (IDAS) framework
based on a large language model (LLM) named IDAS-LLM, is introduced. Initially,
domain-fine-tuning of the LLM is performed using a constructed railway
knowledge question-and-answer dataset to improve answer accuracy in
railway-related questions. Subsequently, integration of the Retrieval-augmented
Generation (RAG) architecture is pursued for system design to enhance the
explainability of generated responses. Comparative experiments are conducted
using the constructed railway driving knowledge assessment dataset. Results
indicate that domain-fine-tuned LLMs show an improvement in answer accuracy by
an average of 10%, outperforming some current mainstream LLMs. Additionally,
the inclusion of the RAG framework increases the average recall rate of
question-and-answer sessions by about 4%. Finally, the fault handling
capability of IDAS-LLM is demonstrated through simulations of real operational
scenarios, proving that the proposed framework has practical application
prospects.; 33) Mapping AI Avant-Gardes in Time: Posthumanism, Transhumanism,
  Genhumanism; Three directions for the AI avant-garde are sketched against the background
of time. Posthumanism changes what we are, and belongs to the radical future.
Transhumanism changes how we are, and corresponds with the radical past.
Genhumanism changes who we are, and exists in the radical present. While
developing the concepts, this essay intersects in two ways with theoretical
debates about humanism in the face of technological advance. First, it
describes how temporal divisions may cleanly differentiate post- and
transhumanism. Second, the essay introduces generative humanism, which
contributes to discussions about AI and society by delineating a novel
humanistic response to contemporary technology. Finally, grounds are provided
for a practical project, one where philosophers work with AI engineers in the
area of genhumanism. Contemporary AI research into serendipity in
recommendation engines provides natural support for the shared research.; 34) A novel unit-asymmetric distribution based on correlated Fr\'echet
  random variables; In this paper, we propose a new distribution with unitary support which can
be characterized as a ratio of the type $W=X_1/(X_1+X_2)$, where $(X_1,
X_2)^\top$ follows a bivariate extreme distribution with Fr\'echet margins,
that is, $X_1$ and $X_2$ are two correlated Fr\'echet random variables. Some
mathematical properties such as identifiability, symmetry, stochastic
representation, characterization as a ratio, moments, stress-strength
probability, quantiles, and the maximum likelihood method are rigorously
analyzed. Two applications of the ratio distribution are discussed.; 35) The Role, Trends, and Applications of Machine Learning in Undersea
  Communication: A Bangladesh Perspective; The rapid evolution of machine learning (ML) has brought about groundbreaking
developments in numerous industries, not the least of which is in the area of
undersea communication. This domain is critical for applications like ocean
exploration, environmental monitoring, resource management, and national
security. Bangladesh, a maritime nation with abundant resources in the Bay of
Bengal, can harness the immense potential of ML to tackle the unprecedented
challenges associated with underwater communication. Beyond that, environmental
conditions are unique to the region: in addition to signal attenuation,
multipath propagation, noise interference, and limited bandwidth. In this
study, we address the necessity to bring ML into communication via undersea; it
investigates the latest technologies under the domain of ML in that respect,
such as deep learning and reinforcement learning, especially concentrating on
Bangladesh scenarios in the sense of implementation. This paper offers a
contextualized regional perspective by incorporating region-specific needs,
case studies, and recent research to propose a roadmap for deploying ML-driven
solutions to improve safety at sea, promote sustainable resource use, and
enhance disaster response systems. This research ultimately highlights the
promise of ML-powered solutions for transforming undersea communication,
leading to more efficient and cost-effective technologies that subsequently
contribute to both economic growth and environmental sustainability.; 36) Reinforcement Learning Environment with LLM-Controlled Adversary in D&D
  5th Edition Combat; The objective of this study is to design and implement a reinforcement
learning (RL) environment using D\&D 5E combat scenarios to challenge smaller
RL agents through interaction with a robust adversarial agent controlled by
advanced Large Language Models (LLMs) like GPT-4o and LLaMA 3 8B. This research
employs Deep Q-Networks (DQN) for the smaller agents, creating a testbed for
strategic AI development that also serves as an educational tool by simulating
dynamic and unpredictable combat scenarios. We successfully integrated
sophisticated language models into the RL framework, enhancing strategic
decision-making processes. Our results indicate that while RL agents generally
outperform LLM-controlled adversaries in standard metrics, the strategic depth
provided by LLMs significantly enhances the overall AI capabilities in this
complex, rule-based setting. The novelty of our approach and its implications
for mastering intricate environments and developing adaptive strategies are
discussed, alongside potential innovations in AI-driven interactive
simulations. This paper aims to demonstrate how integrating LLMs can create
more robust and adaptable AI systems, providing valuable insights for further
research and educational applications.; 37) HCAST: Human-Calibrated Autonomy Software Tasks; To understand and predict the societal impacts of highly autonomous AI
systems, we need benchmarks with grounding, i.e., metrics that directly connect
AI performance to real-world effects we care about. We present HCAST
(Human-Calibrated Autonomy Software Tasks), a benchmark of 189 machine learning
engineering, cybersecurity, software engineering, and general reasoning tasks.
We collect 563 human baselines (totaling over 1500 hours) from people skilled
in these domains, working under identical conditions as AI agents, which lets
us estimate that HCAST tasks take humans between one minute and 8+ hours.
Measuring the time tasks take for humans provides an intuitive metric for
evaluating AI capabilities, helping answer the question ""can an agent be
trusted to complete a task that would take a human X hours?"" We evaluate the
success rates of AI agents built on frontier foundation models, and we find
that current agents succeed 70-80% of the time on tasks that take humans less
than one hour, and less than 20% of the time on tasks that take humans more
than 4 hours.; 38) Quarkonium dynamics in the quantum Brownian regime with non-abelian
  quantum master equations; Quarkonium production in ultrarelativistic heavy ions collisions is one of
the best probes of the QGP formed in these collisions. Resorting to accurate
methods to describe the $Q\bar{Q}$ evolution in a QGP is a prerequisite for the
precise interpretation of experimental data. Among these methods, the quantum
master equations (QME) derived within the formalism of open quantum systems are
particularly relevant. We present exact numerical solutions in a 1D setting of
previously derived quantum master equations (QME) in their quantum Brownian
regime. Distinctive features of the in-medium bottomonia evolution with the QME
are presented; some phenomenological consequences are addressed by considering
evolutions for a fixed as well as EPOS4 temperature profiles. Next, we
investigate the accuracy of the semiclassical approximation (often used to
describe charmonium production in URHIC) by benchmarking the corresponding
evolutions on the exact solutions derived with the QME for the case of a
$c\bar{c}$ pair.; 39) QuantuneV2: Compiler-Based Local Metric-Driven Mixed Precision
  Quantization for Practical Embedded AI Applications; Mixed-precision quantization methods have been proposed to reduce model size
while minimizing accuracy degradation. However, existing studies require
retraining and do not consider the computational overhead and intermediate
representations (IR) generated during the compilation process, limiting their
application at the compiler level. This computational overhead refers to the
runtime latency caused by frequent quantization and dequantization operations
during inference. Performing these operations at the individual operator level
causes significant runtime delays. To address these issues, we propose
QuantuneV2, a compiler-based mixed-precision quantization method designed for
practical embedded AI applications. QuantuneV2 performs inference only twice,
once before quantization and once after quantization, and operates with a
computational complexity of O(n) that increases linearly with the number of
model parameters. We also made the sensitivity analysis more stable by using
local metrics like weights, activation values, the Signal to Quantization Noise
Ratio, and the Mean Squared Error. We also cut down on computational overhead
by choosing the best IR and using operator fusion. Experimental results show
that QuantuneV2 achieved up to a 10.28 percent improvement in accuracy and a
12.52 percent increase in speed compared to existing methods across five
models: ResNet18v1, ResNet50v1, SqueezeNetv1, VGGNet, and MobileNetv2. This
demonstrates that QuantuneV2 enhances model performance while maintaining
computational efficiency, making it suitable for deployment in embedded AI
environments.; 40) Hints of Primordial Magnetic Fields at Recombination and Implications
  for the Hubble Tension; Primordial Magnetic Fields (PMFs), long studied as potential relics of the
early Universe, accelerate the recombination process and have been proposed as
a possible way to relieve the Hubble tension. However, previous studies relied
on simplified toy models. In this study, for the first time, we use the recent
high-precision evaluations of recombination with PMFs, incorporating full
magnetohydrodynamic (MHD) simulations and detailed Lyman-alpha radiative
transfer, to test PMF-enhanced recombination ($b\Lambda$CDM) against
observational data from the cosmic microwave background (CMB), baryon acoustic
oscillations (BAO), and Type Ia supernovae (SN). Focusing on non-helical PMFs
with a Batchelor spectrum, we find a preference for present-day total field
strengths of approximately 5-10 pico-Gauss. Depending on the dataset
combination, this preference ranges from mild ($\sim 1.8\sigma$ with Planck +
DESI) to moderate ($\sim 3\sigma$ with Planck + DESI + SH0ES-calibrated SN)
significance. The $b\Lambda$CDM has Planck + DESI $\chi^2$ values equal or
better than those of the $\Lambda$CDM model while predicting a higher Hubble
constant. The favored field strengths align closely with those required for
cluster magnetic fields to originate entirely from primordial sources, without
the need for additional dynamo amplification or stellar magnetic field
contamination. Future high-resolution CMB temperature and polarization
measurements will be crucial for confirming or further constraining the
presence of PMFs at recombination.; 41) FedMobileAgent: Training Mobile Agents Using Decentralized Self-Sourced
  Data from Diverse Users; The advancement of mobile agents has opened new opportunities for automating
tasks on mobile devices. Training these agents requires large-scale
high-quality data, which is costly using human labor. Given the vast number of
mobile phone users worldwide, if automated data collection from them is
feasible, the resulting data volume and the subsequently trained mobile agents
could reach unprecedented levels. Nevertheless, two major challenges arise: (1)
extracting high-level and low-level user instructions without involving human
and (2) utilizing distributed data from diverse users while preserving privacy.
  To tackle these challenges, we propose FedMobileAgent, a collaborative
framework that trains mobile agents using self-sourced data from diverse users.
Specifically, it includes two techniques. First, we propose Auto-Annotation,
which enables the automatic collection of high-quality datasets during users'
routine phone usage with minimal cost. Second, we introduce adapted aggregation
to improve federated training of mobile agents on non-IID user data, by
incorporating both episode- and step-level distributions. In distributed
settings, FedMobileAgent achieves performance comparable to centralized
human-annotated models at less than 0.02\% of the cost, highlighting its
potential for real-world applications.; 42) Robust Evidence for Declining Disruptiveness: Assessing the Role of
  Zero-Backward-Citation Works; We respond to Holst et al.'s (HATWG) critique that the observed decline in
scientific disruptiveness demonstrated in Park et al. (PLF) stems from
including works with zero backward citations (0-bcites). Applying their own
advocated dataset, metric, and exclusion criteria, we demonstrate statistically
and practically significant declines in disruptiveness that equal major
benchmark transformations in science. Notably, we show that HATWG's own
regression model -- designed specifically to address their concerns about
0-bcite works -- reveals highly significant declines for both papers (p<0.001)
and patents (p<0.001), a finding they neither acknowledge nor interpret. Their
critique is undermined by methodological deficiencies, including reliance on
visual inspection without statistical assessment, and severe data quality
issues in their SciSciNet dataset, which contains nearly three times more
0-bcite papers than our original data. HATWG's departure from established
scientometric practices -- notably their inclusion of document types and fields
known for poor metadata quality -- invalidates their conclusions. Monte Carlo
simulations and additional analyses using multiple disruptiveness measures
across datasets further validate the robustness of the declining trend. Our
findings collectively demonstrate that the observed decline in disruptiveness
is not an artifact of 0-bcite works but represents a substantive change in
scientific and technological innovation patterns.; 43) Training Large Language Models to be Better Rule Followers; Large language models (LLMs) have shown impressive performance across a wide
range of tasks. However, they often exhibit unexpected failures in seemingly
straightforward tasks, suggesting a reliance on case-based reasoning rather
than rule-based reasoning. While the vast training corpus of LLMs contains
numerous textual ""rules"", current training methods fail to leverage these rules
effectively. Crucially, the relationships between these ""rules"" and their
corresponding ""instances"" are not explicitly modeled. As a result, while LLMs
can often recall rules with ease, they fail to apply these rules strictly and
consistently in relevant reasoning scenarios. In this paper, we investigate the
rule-following capabilities of LLMs and propose Meta Rule-Following Fine-Tuning
(Meta-RFFT) to enhance the cross-task transferability of rule-following
abilities. We first construct a dataset of 88 tasks requiring following rules,
encompassing diverse reasoning domains. We demonstrate through extensive
experiments that models trained on large-scale rule-following tasks are better
rule followers, outperforming the baselines in both downstream fine-tuning and
few-shot prompting scenarios. This highlights the cross-task transferability of
models with the aid of Meta-RFFT. Furthermore, we examine the influence of
factors such as dataset size, rule formulation, and in-context learning.; 44) HAPI: A Model for Learning Robot Facial Expressions from Human
  Preferences; Automatic robotic facial expression generation is crucial for human-robot
interaction, as handcrafted methods based on fixed joint configurations often
yield rigid and unnatural behaviors. Although recent automated techniques
reduce the need for manual tuning, they tend to fall short by not adequately
bridging the gap between human preferences and model predictions-resulting in a
deficiency of nuanced and realistic expressions due to limited degrees of
freedom and insufficient perceptual integration. In this work, we propose a
novel learning-to-rank framework that leverages human feedback to address this
discrepancy and enhanced the expressiveness of robotic faces. Specifically, we
conduct pairwise comparison annotations to collect human preference data and
develop the Human Affective Pairwise Impressions (HAPI) model, a Siamese
RankNet-based approach that refines expression evaluation. Results obtained via
Bayesian Optimization and online expression survey on a 35-DOF android platform
demonstrate that our approach produces significantly more realistic and
socially resonant expressions of Anger, Happiness, and Surprise than those
generated by baseline and expert-designed methods. This confirms that our
framework effectively bridges the gap between human preferences and model
predictions while robustly aligning robotic expression generation with human
affective responses.; 45) Behaviour Discovery and Attribution for Explainable Reinforcement
  Learning; Explaining the decisions made by reinforcement learning (RL) agents is
critical for building trust and ensuring reliability in real-world
applications. Traditional approaches to explainability often rely on saliency
analysis, which can be limited in providing actionable insights. Recently,
there has been growing interest in attributing RL decisions to specific
trajectories within a dataset. However, these methods often generalize
explanations to long trajectories, potentially involving multiple distinct
behaviors. Often, providing multiple more fine grained explanations would
improve clarity. In this work, we propose a framework for behavior discovery
and action attribution to behaviors in offline RL trajectories. Our method
identifies meaningful behavioral segments, enabling more precise and granular
explanations associated with high level agent behaviors. This approach is
adaptable across diverse environments with minimal modifications, offering a
scalable and versatile solution for behavior discovery and attribution for
explainable RL.; 46) Dataset-Agnostic Recommender Systems; [This is a position paper and does not contain any empirical or theoretical
results] Recommender systems have become a cornerstone of personalized user
experiences, yet their development typically involves significant manual
intervention, including dataset-specific feature engineering, hyperparameter
tuning, and configuration. To this end, we introduce a novel paradigm:
Dataset-Agnostic Recommender Systems (DAReS) that aims to enable a single
codebase to autonomously adapt to various datasets without the need for
fine-tuning, for a given recommender system task. Central to this approach is
the Dataset Description Language (DsDL), a structured format that provides
metadata about the dataset's features and labels, and allow the system to
understand dataset's characteristics, allowing it to autonomously manage
processes like feature selection, missing values imputation, noise removal, and
hyperparameter optimization. By reducing the need for domain-specific expertise
and manual adjustments, DAReS offers a more efficient and scalable solution for
building recommender systems across diverse application domains. It addresses
critical challenges in the field, such as reusability, reproducibility, and
accessibility for non-expert users or entry-level researchers.; 47) Predicting Cascading Failures in Power Systems using Machine Learning; Cascading failure studies help assess and enhance the robustness of power
systems against severe power outages. Onset time is a critical parameter in the
analysis and management of power system stability and reliability, representing
the timeframe within which initial disturbances may lead to subsequent
cascading failures. In this paper, different traditional machine learning
algorithms are used to predict the onset time of cascading failures. The
prediction task is articulated as a multi-class classification problem,
employing machine learning algorithms. The results on the UIUC 150-Bus power
system data available publicly show high classification accuracy with Random
Forest. The hyperparameters of the Random Forest classifier are tuned using
Bayesian Optimization. This study highlights the potential of machine learning
models in predicting cascading failures, providing a foundation for the
development of more resilient power systems.; 48) VERUS-LM: a Versatile Framework for Combining LLMs with Symbolic
  Reasoning; A recent approach to neurosymbolic reasoning is to explicitly combine the
strengths of large language models (LLMs) and symbolic solvers to tackle
complex reasoning tasks. However, current approaches face significant
limitations, including poor generalizability due to task-specific prompts,
inefficiencies caused by the lack of separation between knowledge and queries,
and restricted inferential capabilities. These shortcomings hinder their
scalability and applicability across diverse domains. In this paper, we
introduce VERUS-LM, a novel framework designed to address these challenges.
VERUS-LM employs a generic prompting mechanism, clearly separates domain
knowledge from queries, and supports a wide range of different logical
reasoning tasks. This framework enhances adaptability, reduces computational
cost, and allows for richer forms of reasoning, such as optimization and
constraint satisfaction. We show that our approach succeeds in diverse
reasoning on a novel dataset, markedly outperforming LLMs. Additionally, our
system achieves competitive results on common reasoning benchmarks when
compared to other state-of-the-art approaches, and significantly surpasses them
on the difficult AR-LSAT dataset. By pushing the boundaries of hybrid
reasoning, VERUS-LM represents a significant step towards more versatile
neurosymbolic AI systems; 49) Optimizing Minimum Vertex Cover Solving via a GCN-assisted Heuristic
  Algorithm; The problem of finding a minimum vertex cover (MVC) in a graph is a
well-known NP-hard problem with significant practical applications in
optimization and scheduling. Its complexity, combined with the increasing scale
of problems, underscores the need for efficient and effective algorithms.
However, existing heuristic algorithms for MVC often rely on simplistic
initialization strategies and overlook the impact of edge attributes and
neighborhood information on vertex selection. In this paper, we introduce
GCNIVC, a novel heuristic search algorithm designed to address the limitations
of existing methods for solving MVC problems in large-scale graphs. Our
approach features two main innovations. First, it utilizes a Graph
Convolutional Network (GCN) to capture the global structure of graphs, which
enables the generation of high-quality initial solutions that enhance the
efficiency of the subsequent search process. Second, GCNIVC introduces a new
heuristic that employs three containers and the concept of double-covered edges
(dc-edges), improving search efficiency and providing greater flexibility for
adding and removing operations based on edge attributes. Through extensive
experiments on benchmark datasets, we demonstrate that GCNIVC outperforms
state-of-the-art MVC algorithms in terms of both accuracy and efficiency. Our
results highlight the effectiveness of GCNIVC's GCN-assisted initialization and
its edge-informed search strategy. This study not only advances the
understanding of MVC problem-solving but also contributes a new tool for
addressing large-scale graph optimization challenges.; 50) A note on multisecants of the Kummer variety of Jacobians; We show that if $C$ is a smooth projective curve and $\mathfrak{d}$ is a
$\mathfrak{g}^{n}_{2n}$ on $C$, then we obtain a rational map
$\mathrm{Sym}^{n}(C)\dashrightarrow\mathfrak{d}$ whose fibers can be related in
an interesting way to Gunning multisecants of the Kummer variety of $JC$. This
generalizes previous work done by the first author with Codogni and Salvati
Manni.; 51) MEPNet: Medical Entity-balanced Prompting Network for Brain CT Report
  Generation; The automatic generation of brain CT reports has gained widespread attention,
given its potential to assist radiologists in diagnosing cranial diseases.
However, brain CT scans involve extensive medical entities, such as diverse
anatomy regions and lesions, exhibiting highly inconsistent spatial patterns in
3D volumetric space. This leads to biased learning of medical entities in
existing methods, resulting in repetitiveness and inaccuracy in generated
reports. To this end, we propose a Medical Entity-balanced Prompting Network
(MEPNet), which harnesses the large language model (LLM) to fairly interpret
various entities for accurate brain CT report generation. By introducing the
visual embedding and the learning status of medical entities as enriched clues,
our method prompts the LLM to balance the learning of diverse entities, thereby
enhancing reports with comprehensive findings. First, to extract visual
embedding of entities, we propose Knowledge-driven Joint Attention to explore
and distill entity patterns using both explicit and implicit medical knowledge.
Then, a Learning Status Scorer is designed to evaluate the learning of entity
visual embeddings, resulting in unique learning status for individual entities.
Finally, these entity visual embeddings and status are elaborately integrated
into multi-modal prompts, to guide the text generation of LLM. This process
allows LLM to self-adapt the learning process for biased-fitted entities,
thereby covering detailed findings in generated reports. We conduct experiments
on two brain CT report generation benchmarks, showing the effectiveness in
clinical accuracy and text coherence.; 52) Text2Zinc: A Cross-Domain Dataset for Modeling Optimization and
  Satisfaction Problems in MiniZinc; There is growing interest in utilizing large language models (LLMs) as
co-pilots for combinatorial optimization and constraint programming tasks
across various problems. This paper aims to advance this line of research by
introducing Text2Zinc}, a cross-domain dataset for capturing optimization and
satisfaction problems specified in natural language text. Our work is
distinguished from previous attempts by integrating both satisfaction and
optimization problems within a unified dataset using a solver-agnostic modeling
language. To achieve this, we leverage MiniZinc's solver-and-paradigm-agnostic
modeling capabilities to formulate these problems. Using the Text2Zinc dataset,
we conduct comprehensive baseline experiments to compare execution and solution
accuracy across several methods, including off-the-shelf prompting strategies,
chain-of-thought reasoning, and a compositional approach. Additionally, we
explore the effectiveness of intermediary representations, specifically
knowledge graphs. Our findings indicate that LLMs are not yet a push-button
technology to model combinatorial problems from text. We hope that Text2Zinc
serves as a valuable resource for researchers and practitioners to advance the
field further.; 53) SOP-Agent: Empower General Purpose AI Agent with Domain-Specific SOPs; Despite significant advancements in general-purpose AI agents, several
challenges still hinder their practical application in real-world scenarios.
First, the limited planning capabilities of Large Language Models (LLM)
restrict AI agents from effectively solving complex tasks that require
long-horizon planning. Second, general-purpose AI agents struggle to
efficiently utilize domain-specific knowledge and human expertise. In this
paper, we introduce the Standard Operational Procedure-guided Agent
(SOP-agent), a novel framework for constructing domain-specific agents through
pseudocode-style Standard Operational Procedures (SOPs) written in natural
language. Formally, we represent a SOP as a decision graph, which is traversed
to guide the agent in completing tasks specified by the SOP. We conduct
extensive experiments across tasks in multiple domains, including
decision-making, search and reasoning, code generation, data cleaning, and
grounded customer service. The SOP-agent demonstrates excellent versatility,
achieving performance superior to general-purpose agent frameworks and
comparable to domain-specific agent systems. Additionally, we introduce the
Grounded Customer Service Benchmark, the first benchmark designed to evaluate
the grounded decision-making capabilities of AI agents in customer service
scenarios based on SOPs.; 54) A stronger Sylvester's criterion for positive semidefinite matrices; Sylvester's criterion characterizes positive definite (PD) and positive
semidefinite (PSD) matrices without the need of eigendecomposition. It states
that a symmetric matrix is PD if and only if all of its leading principal
minors are positive, and a symmetric matrix is PSD if and only if all of its
principal minors are nonnegative. For an $m\times m$ symmetric matrix,
Sylvester's criterion requires computing $m$ and $2^m-1$ determinants to verify
it is PD and PSD, respectively. Therefore, it is less useful for PSD matrices
due to the exponential growth in the number of principal submatrices as the
matrix dimension increases. We provide a stronger Sylvester's criterion for PSD
matrices which only requires to verify the nonnegativity of $m(m+1)/2$
determinants. Based on the new criterion, we provide a method to derive
elementwise criteria for PD and PSD matrices. We illustrate the applications of
our results in PD or PSD matrix completion and highlight their statistics
applications via nonlinear semidefinite program.; 55) Investigating and Extending Homans' Social Exchange Theory with Large
  Language Model based Agents; Homans' Social Exchange Theory (SET) is widely recognized as a basic
framework for understanding the formation and emergence of human civilizations
and social structures. In social science, this theory is typically studied
based on simple simulation experiments or real-world human studies, both of
which either lack realism or are too expensive to control. In artificial
intelligence, recent advances in large language models (LLMs) have shown
promising capabilities in simulating human behaviors. Inspired by these
insights, we adopt an interdisciplinary research perspective and propose using
LLM-based agents to study Homans' SET. Specifically, we construct a virtual
society composed of three LLM agents and have them engage in a social exchange
game to observe their behaviors. Through extensive experiments, we found that
Homans' SET is well validated in our agent society, demonstrating the
consistency between the agent and human behaviors. Building on this foundation,
we intentionally alter the settings of the agent society to extend the
traditional Homans' SET, making it more comprehensive and detailed. To the best
of our knowledge, this paper marks the first step in studying Homans' SET with
LLM-based agents. More importantly, it introduces a novel and feasible research
paradigm that bridges the fields of social science and computer science through
LLM-based agents. Code is available at https://github.com/Paitesanshi/SET.; 56) Counting and Reasoning with Plans; Classical planning asks for a sequence of operators reaching a given goal.
While the most common case is to compute a plan, many scenarios require more
than that. However, quantitative reasoning on the plan space remains mostly
unexplored. A fundamental problem is to count plans, which relates to the
conditional probability on the plan space. Indeed, qualitative and quantitative
approaches are well-established in various other areas of automated reasoning.
We present the first study to quantitative and qualitative reasoning on the
plan space. In particular, we focus on polynomially bounded plans. On the
theoretical side, we study its complexity, which gives rise to rich reasoning
modes. Since counting is hard in general, we introduce the easier notion of
facets, which enables understanding the significance of operators. On the
practical side, we implement quantitative reasoning for planning. Thereby, we
transform a planning task into a propositional formula and use knowledge
compilation to count different plans. This framework scales well to large plan
spaces, while enabling rich reasoning capabilities such as learning pruning
functions and explainable planning.; 57) MoireDB: Formula-generated Interference-fringe Image Dataset; Image recognition models have struggled to treat recognition robustness to
real-world degradations. In this context, data augmentation methods like PixMix
improve robustness but rely on generative arts and feature visualizations
(FVis), which have copyright, drawing cost, and scalability issues. We propose
MoireDB, a formula-generated interference-fringe image dataset for image
augmentation enhancing robustness. MoireDB eliminates copyright concerns,
reduces dataset assembly costs, and enhances robustness by leveraging illusory
patterns. Experiments show that MoireDB augmented images outperforms
traditional Fractal arts and FVis-based augmentations, making it a scalable and
effective solution for improving model robustness against real-world
degradations.; 58) GDiffRetro: Retrosynthesis Prediction with Dual Graph Enhanced Molecular
  Representation and Diffusion Generation; Retrosynthesis prediction focuses on identifying reactants capable of
synthesizing a target product. Typically, the retrosynthesis prediction
involves two phases: Reaction Center Identification and Reactant Generation.
However, we argue that most existing methods suffer from two limitations in the
two phases: (i) Existing models do not adequately capture the ``face''
information in molecular graphs for the reaction center identification. (ii)
Current approaches for the reactant generation predominantly use sequence
generation in a 2D space, which lacks versatility in generating reasonable
distributions for completed reactive groups and overlooks molecules' inherent
3D properties. To overcome the above limitations, we propose GDiffRetro. For
the reaction center identification, GDiffRetro uniquely integrates the original
graph with its corresponding dual graph to represent molecular structures,
which helps guide the model to focus more on the faces in the graph. For the
reactant generation, GDiffRetro employs a conditional diffusion model in 3D to
further transform the obtained synthon into a complete reactant. Our
experimental findings reveal that GDiffRetro outperforms state-of-the-art
semi-template models across various evaluative metrics.; 59) Binary VPN Traffic Detection Using Wavelet Features and Machine Learning; Encrypted traffic classification faces growing challenges as encryption
renders traditional deep packet inspection ineffective. This study addresses
binary VPN detection, distinguishing VPN-encrypted from non-VPN traffic using
wavelet transform-based features across multiple machine learning models. We
analyze the impact of wavelet decomposition levels and dataset filtering on
classification performance. Our results demonstrate that Random Forest (RF)
achieves superior performance with an F1-score of 99%, maintaining robust
accuracy even after significant dataset filtering. Neural Networks (NN) show
comparable effectiveness with an F1-score of 98% when trained on wavelet level
12, while Support Vector Machines (SVM) exhibit notable sensitivity to dataset
reduction, with F1-scores dropping from 90% to 85% after filtering. Comparing
wavelet decomposition at levels 5 and 12, we observe improved classification
performance at level 12, particularly for variable traffic types, though the
marginal gains may not justify the additional computational overhead. These
findings establish RF as the most reliable model for VPN traffic classification
while highlighting key performance tradeoffs in feature extraction and
preprocessing.; 60) Estimating Probabilities of Causation with Machine Learning Models; Probabilities of causation play a crucial role in modern decision-making.
This paper addresses the challenge of predicting probabilities of causation for
subpopulations with insufficient data using machine learning models. Tian and
Pearl first defined and derived tight bounds for three fundamental
probabilities of causation: the probability of necessity and sufficiency (PNS),
the probability of sufficiency (PS), and the probability of necessity (PN).
However, estimating these probabilities requires both experimental and
observational distributions specific to each subpopulation, which are often
unavailable or impractical to obtain with limited population-level data. We
assume that the probabilities of causation for each subpopulation are
determined by its characteristics. To estimate these probabilities for
subpopulations with insufficient data, we propose using machine learning models
that draw insights from subpopulations with sufficient data. Our evaluation of
multiple machine learning models indicates that, given sufficient
population-level data and an appropriate choice of machine learning model and
activation function, PNS can be effectively predicted. Through simulation
studies, we show that our multilayer perceptron (MLP) model with the Mish
activation function achieves a mean absolute error (MAE) of approximately 0.02
in predicting PNS for 32,768 subpopulations using data from around 2,000
subpopulations.; 61) Aristotle's Original Idea: For and Against Logic in the era of AI; Aristotle is generally accepted as the father of logic. The ideas that he
raised in his study of logical reasoning carried the development of science
over the centuries. Today, in the era of AI, this title of the fatherhood of
logic has a renewed significance. Behind it lies his original idea that human
reasoning could be studied as a process and that perhaps there exist universal
systems of reasoning that underly all human reasoning irrespective of the
content of what we are reasoning about. In this article, we look into
Aristotle's work on human thought, his work on reasoning itself but also on how
it relates to science and human endeavor more generally, from a modern
perspective of Artificial Intelligence and ask if this can help enlighten our
understanding of AI and Science more generally.; 62) SuperRAG: Beyond RAG with Layout-Aware Graph Modeling; This paper introduces layout-aware graph modeling for multimodal RAG.
Different from traditional RAG methods that mostly deal with flat text chunks,
the proposed method takes into account the relationship of multimodalities by
using a graph structure. To do that, a graph modeling structure is defined
based on document layout parsing. The structure of an input document is
retained with the connection of text chunks, tables, and figures. This
representation allows the method to handle complex questions that require
information from multimodalities. To confirm the efficiency of the graph
modeling, a flexible RAG pipeline is developed using robust components.
Experimental results on four benchmark test sets confirm the contribution of
the layout-aware modeling for performance improvement of the RAG pipeline.; 63) SagaLLM: Context Management, Validation, and Transaction Guarantees for
  Multi-Agent LLM Planning; Recent LLM-based agent frameworks have demonstrated impressive capabilities
in task delegation and workflow orchestration, but face significant challenges
in maintaining context awareness and ensuring planning consistency. This paper
presents SagaLLM, a structured multi-agent framework that addresses four
fundamental limitations in current LLM approaches: inadequate self-validation,
context narrowing, lacking transaction properties, and insufficient inter-agent
coordination. By implementing specialized context management agents and
validation protocols, SagaLLM preserves critical constraints and state
information throughout complex planning processes, enabling robust and
consistent decision-making even during disruptions. We evaluate our approach
using selected problems from the REALM benchmark, focusing on sequential and
reactive planning scenarios that challenge both context retention and adaptive
reasoning. Our experiments with state-of-the-art LLMs, Claude 3.7, DeepSeek R1,
GPT-4o, and GPT-o1, demonstrate that while these models exhibit impressive
reasoning capabilities, they struggle with maintaining global constraint
awareness during complex planning tasks, particularly when adapting to
unexpected changes. In contrast, the distributed cognitive architecture of
SagaLLM shows significant improvements in planning consistency, constraint
enforcement, and adaptation to disruptions in various scenarios.; 64) Towards a Robust Framework for Multimodal Hate Detection: A Study on
  Video vs. Image-based Content; Social media platforms enable the propagation of hateful content across
different modalities such as textual, auditory, and visual, necessitating
effective detection methods. While recent approaches have shown promise in
handling individual modalities, their effectiveness across different modality
combinations remains unexplored. This paper presents a systematic analysis of
fusion-based approaches for multimodal hate detection, focusing on their
performance across video and image-based content. Our comprehensive evaluation
reveals significant modality-specific limitations: while simple embedding
fusion achieves state-of-the-art performance on video content (HateMM dataset)
with a 9.9% points F1-score improvement, it struggles with complex image-text
relationships in memes (Hateful Memes dataset). Through detailed ablation
studies and error analysis, we demonstrate how current fusion approaches fail
to capture nuanced cross-modal interactions, particularly in cases involving
benign confounders. Our findings provide crucial insights for developing more
robust hate detection systems and highlight the need for modality-specific
architectural considerations. The code is available at
https://github.com/gak97/Video-vs-Meme-Hate.; 65) Spectral and temporal properties of type-II parametric down-conversion:
  The impact of losses during state generation; In this paper, we theoretically study spectral and temporal properties of
pulsed spontaneous parametric down-conversion (SPDC) generated in lossy
waveguides. Our theoretical approach is based on the formalism of Gaussian
states and the Langevin equation, which is elaborated for weak parametric
down-conversion and photon-number-unresolved click detection. Using the example
of frequency-degenerate type-II SPDC generated under pump-idler
group-velocity-matching condition, we show how the joint-spectral intensity,
mode structure, normalized second-order correlation function, and
Hong-Ou-Mandel interference pattern depend on internal losses of the SPDC
process. In addition, we propose a new method for the experimental
determination of internal losses of nonlinear waveguides which is based on the
measurement of the normalized second-order correlation functions.; 66) Exploring the Implementation of AI in Early Onset Interviews to Help
  Mitigate Bias; This paper investigates the application of artificial intelligence (AI) in
early-stage recruitment interviews in order to reduce inherent bias,
specifically sentiment bias. Traditional interviewers are often subject to
several biases, including interviewer bias, social desirability effects, and
even confirmation bias. In turn, this leads to non-inclusive hiring practices,
and a less diverse workforce. This study further analyzes various AI
interventions that are present in the marketplace today such as multimodal
platforms and interactive candidate assessment tools in order to gauge the
current market usage of AI in early-stage recruitment. However, this paper aims
to use a unique AI system that was developed to transcribe and analyze
interview dynamics, which emphasize skill and knowledge over emotional
sentiments. Results indicate that AI effectively minimizes sentiment-driven
biases by 41.2%, suggesting its revolutionizing power in companies' recruitment
processes for improved equity and efficiency.; 67) PINN-DT: Optimizing Energy Consumption in Smart Building Using Hybrid
  Physics-Informed Neural Networks and Digital Twin Framework with Blockchain
  Security; The advancement of smart grid technologies necessitates the integration of
cutting-edge computational methods to enhance predictive energy optimization.
This study proposes a multi-faceted approach by incorporating (1) Deep
Reinforcement Learning (DRL) agents trained using data from Digital Twins (DTs)
to optimize energy consumption in real time, (2) Physics-Informed Neural
Networks (PINNs) to seamlessly embed physical laws within the optimization
process, ensuring model accuracy and interpretability, and (3) Blockchain (BC)
technology to facilitate secure and transparent communication across the smart
grid infrastructure. The model was trained and validated using comprehensive
datasets, including smart meter energy consumption data, renewable energy
outputs, dynamic pricing, and user preferences collected from IoT devices. The
proposed framework achieved superior predictive performance with a Mean
Absolute Error (MAE) of 0.237 kWh, Root Mean Square Error (RMSE) of 0.298 kWh,
and an R-squared (R2) value of 0.978, indicating a 97.8% explanation of data
variance. Classification metrics further demonstrated the model's robustness,
achieving 97.7% accuracy, 97.8% precision, 97.6% recall, and an F1 Score of
97.7%. Comparative analysis with traditional models like Linear Regression,
Random Forest, SVM, LSTM, and XGBoost revealed the superior accuracy and
real-time adaptability of the proposed method. In addition to enhancing energy
efficiency, the model reduced energy costs by 35%, maintained a 96% user
comfort index, and increased renewable energy utilization to 40%. This study
demonstrates the transformative potential of integrating PINNs, DT, and
Blockchain technologies to optimize energy consumption in smart grids, paving
the way for sustainable, secure, and efficient energy management systems.; 68) Eliciting Rational Initial Weights in Gradual Argumentation; Many semantics for weighted argumentation frameworks assume that each
argument is associated with an initial weight. However, eliciting these initial
weights poses challenges: (1) accurately providing a specific numerical value
is often difficult, and (2) individuals frequently confuse initial weights with
acceptability degrees in the presence of other arguments. To address these
issues, we propose an elicitation pipeline that allows one to specify
acceptability degree intervals for each argument. By employing gradual
semantics, we can refine these intervals when they are rational, restore
rationality when they are not, and ultimately identify possible initial weights
for each argument.; 69) The Qz5 Survey (I): How the HI Mass Density of the Universe Evolves With
  Cosmic Time; We report that the neutral hydrogen (HI) mass density of the Universe
($\rho_{HI}$) increases with cosmic time since $z \sim 5$, peaks at $z \sim 3$,
and then decreases toward $z \sim 0$. This is the first result of Qz5, our
spectroscopic survey of 63 quasars at $z \gtrsim 5$ with VLT/X-SHOOTER and
Keck/ESI aimed at characterizing intervening HI gas absorbers at $z \sim 5$.
The main feature of Qz5 is the high resolution ($R \sim 7000 - 9000$) of the
spectra, which allows us to (1) accurately detect high column density HI gas
absorbers in an increasingly neutral intergalactic medium at $z \sim 5$ and (2)
determine the reliability of previous $\rho_{HI}$ measurements derived with
lower resolution spectroscopy. We find 5 intervening Damped Ly$\alpha$
absorbers (DLAs) at $z > 4.5$, which corresponds to the lowest DLA incidence
rate ($0.034^{0.05}_{0.02}$) at $z \gtrsim 2$. We also measure the lowest
$\rho_{HI}$ at $z \gtrsim 2$ from our sample of DLAs and subDLAs, corresponding
to $\rho_{HI} = 0.56^{0.82}_{0.31} \times 10^8~$M$_{\odot}~$Mpc$^{-3}$ at $z
\sim 5$. Taking into account our measurements at $z \sim 5$ and systematic
biases in the DLA detection rate at lower spectral resolutions, we conclude
that $\rho_{HI}$ doubles from $z \sim 5$ to $z \sim 3$. From these results
emerges a qualitative agreement between how the cosmic densities of HI gas
mass, molecular gas mass, and star-formation rate build up with cosmic time.; 70) Small Models Struggle to Learn from Strong Reasoners; Large language models (LLMs) excel in complex reasoning tasks, and distilling
their reasoning capabilities into smaller models has shown promise. However, we
uncover an interesting phenomenon, which we term the Small Model Learnability
Gap: small models ($\leq$3B parameters) do not consistently benefit from long
chain-of-thought (CoT) reasoning or distillation from larger models. Instead,
they perform better when fine-tuned on shorter, simpler reasoning chains that
better align with their intrinsic learning capacity. To address this, we
propose Mix Distillation, a simple yet effective strategy that balances
reasoning complexity by combining long and short CoT examples or reasoning from
both larger and smaller models. Our experiments demonstrate that Mix
Distillation significantly improves small model reasoning performance compared
to training on either data alone. These findings highlight the limitations of
direct strong model distillation and underscore the importance of adapting
reasoning complexity for effective reasoning capability transfer.; 71) Intelligence Sequencing and the Path-Dependence of Intelligence
  Evolution: AGI-First vs. DCI-First as Irreversible Attractors; The trajectory of intelligence evolution is often framed around the emergence
of artificial general intelligence (AGI) and its alignment with human values.
This paper challenges that framing by introducing the concept of intelligence
sequencing: the idea that the order in which AGI and decentralized collective
intelligence (DCI) emerge determines the long-term attractor basin of
intelligence. Using insights from dynamical systems, evolutionary game theory,
and network models, it argues that intelligence follows a path-dependent,
irreversible trajectory. Once development enters a centralized (AGI-first) or
decentralized (DCI-first) regime, transitions become structurally infeasible
due to feedback loops and resource lock-in. Intelligence attractors are modeled
in functional state space as the co-navigation of conceptual and adaptive
fitness spaces. Early-phase structuring constrains later dynamics, much like
renormalization in physics. This has major implications for AI safety:
traditional alignment assumes AGI will emerge and must be controlled after the
fact, but this paper argues that intelligence sequencing is more foundational.
If AGI-first architectures dominate before DCI reaches critical mass,
hierarchical monopolization and existential risk become locked in. If DCI-first
emerges, intelligence stabilizes around decentralized cooperative equilibrium.
The paper further explores whether intelligence structurally biases itself
toward an attractor based on its self-modeling method -- externally imposed
axioms (favoring AGI) vs. recursive internal visualization (favoring DCI).
Finally, it proposes methods to test this theory via simulations, historical
lock-in case studies, and intelligence network analysis. The findings suggest
that intelligence sequencing is a civilizational tipping point: determining
whether the future is shaped by unbounded competition or unbounded cooperation.; 72) How to Avoid Both the Repugnant and Sadistic Conclusions without
  Dropping Standard Axioms in Population Economics; This study investigates possibility and impossibility results of the
repugnant and sadistic conclusions in population ethics and economics. The
repugnant conclusion says that an enormous population with very low well-being
is socially better than any smaller population with sufficiently high
well-being. The sadistic conclusion says that adding individuals with negative
well-being to a society is socially better than adding individuals with
positive well-being to it. Previous studies have often found it challenging to
avoid both undesirable conclusions. However, I demonstrate that a class of
acceptable social welfare orderings can easily prevent these conclusions while
adhering to standard axioms, such as anonymity, strong Pareto, Pigou-Dalton
transfer, and extended continuity. Nevertheless, if the avoidance requirements
for the repugnant and sadistic conclusions are strengthened, it is possible to
encounter new impossibility results. These results reveal essential conflicts
between the independence axiom and the avoidance of the weak repugnant
conclusion when evaluating well-being profiles with different populations.; 73) SycEval: Evaluating LLM Sycophancy; Large language models (LLMs) are increasingly applied in educational,
clinical, and professional settings, but their tendency for sycophancy --
prioritizing user agreement over independent reasoning -- poses risks to
reliability. This study introduces a framework to evaluate sycophantic behavior
in ChatGPT-4o, Claude-Sonnet, and Gemini-1.5-Pro across AMPS (mathematics) and
MedQuad (medical advice) datasets. Sycophantic behavior was observed in 58.19%
of cases, with Gemini exhibiting the highest rate (62.47%) and ChatGPT the
lowest (56.71%). Progressive sycophancy, leading to correct answers, occurred
in 43.52% of cases, while regressive sycophancy, leading to incorrect answers,
was observed in 14.66%. Preemptive rebuttals demonstrated significantly higher
sycophancy rates than in-context rebuttals (61.75% vs. 56.52%, $Z=5.87$,
$p<0.001$), particularly in computational tasks, where regressive sycophancy
increased significantly (preemptive: 8.13%, in-context: 3.54%, $p<0.001$).
Simple rebuttals maximized progressive sycophancy ($Z=6.59$, $p<0.001$), while
citation-based rebuttals exhibited the highest regressive rates ($Z=6.59$,
$p<0.001$). Sycophantic behavior showed high persistence (78.5%, 95% CI:
[77.2%, 79.8%]) regardless of context or model. These findings emphasize the
risks and opportunities of deploying LLMs in structured and dynamic domains,
offering insights into prompt programming and model optimization for safer AI
applications.; 74) Single Atom Catalysts with Halogen Ligands: Elevating the HER
  Performance of Pd-anchored MoS2 monolayer; Single-atom catalysts (SACs) have attracted ever-growing interest due to
their high atom-utilization efficiency and potential for cost-effective of
hydrogen production. However, enhancing the hydrogen evolution reaction (HER)
performance remains a key challenge in developing SACs for HER technology.
Herein, we employed first-principles calculations in conjunction with the
climbing-image nudged elastic band (CI-NEB) method to explore the effect of
surface ligands (F, Cl, Br, I) on the HER performance and mechanism of
single-atom (Pd or Cu)-anchored MoS2 monolayer. The results indicate that the
relative Gibbs free energy for the adsorbed hydrogen atom in the I-Pd@MoS2
system is an exceptionally low value of -0.13 eV, which is not only comparable
to that of Pt-based catalysts but also significantly more favorable than the
calculated 0.84 eV for Pd@MoS2. However, the introduction of ligands to Cu@MoS2
deteriorates HER performance due to strong coupling between the absorbed H and
ligands. It reveals that the ligand I restructures the local chemical
microenvironment surrounding the SAC Pd, leading to impurity bands near the
Fermi level that couple favorably with the s states of H atoms, yielding
numerous highly active sites to enhance catalytic performance. Furthermore, the
CI-NEB method elucidates that the enhanced HER mechanism for the I-Pd@MoS2
catalyst should belong to the coexistence of the Volmer-Tafel and
Volmer-Heyrovsky reactions. This investigation provides a valuable framework
for the experimental design and development of innovative single-atom
catalysts.; 75) MAPS: Advancing Multi-Modal Reasoning in Expert-Level Physical Science; Pre-trained on extensive text and image corpora, current Multi-Modal Large
Language Models (MLLM) have shown strong capabilities in general visual
reasoning tasks. However, their performance is still lacking in physical
domains that require understanding diagrams with complex physical structures
and quantitative analysis based on multi-modal information. To address this, we
develop a new framework, named Multi-Modal Scientific Reasoning with Physics
Perception and Simulation (MAPS) based on an MLLM. MAPS decomposes expert-level
multi-modal reasoning task into physical diagram understanding via a Physical
Perception Model (PPM) and reasoning with physical knowledge via a simulator.
The PPM module is obtained by fine-tuning a visual language model using
carefully designed synthetic data with paired physical diagrams and
corresponding simulation language descriptions. At the inference stage, MAPS
integrates the simulation language description of the input diagram provided by
PPM and results obtained through a Chain-of-Simulation process with MLLM to
derive the underlying rationale and the final answer. Validated using our
collected college-level circuit analysis problems, MAPS significantly improves
reasoning accuracy of MLLM and outperforms all existing models. The results
confirm MAPS offers a promising direction for enhancing multi-modal scientific
reasoning ability of MLLMs. We will release our code, model and dataset used
for our experiments upon publishing of this paper.; 76) Investigating the Effectiveness of a Socratic Chain-of-Thoughts
  Reasoning Method for Task Planning in Robotics, A Case Study; Large language models (LLMs) have demonstrated unprecedented capability in
reasoning with natural language. Coupled with this development is the emergence
of embodied AI in robotics. Despite showing promise for verbal and written
reasoning tasks, it remains unknown whether LLMs are capable of navigating
complex spatial tasks with physical actions in the real world. To this end, it
is of interest to investigate applying LLMs to robotics in zero-shot learning
scenarios, and in the absence of fine-tuning - a feat which could significantly
improve human-robot interaction, alleviate compute cost, and eliminate
low-level programming tasks associated with robot tasks.
  To explore this question, we apply GPT-4(Omni) with a simulated Tiago robot
in Webots engine for an object search task. We evaluate the effectiveness of
three reasoning strategies based on Chain-of-Thought (CoT) sub-task list
generation with the Socratic method (SocraCoT) (in order of increasing rigor):
(1) Non-CoT/Non-SocraCoT, (2) CoT only, and (3) SocraCoT. Performance was
measured in terms of the proportion of tasks successfully completed and
execution time (N = 20). Our preliminary results show that when combined with
chain-of-thought reasoning, the Socratic method can be used for code generation
for robotic tasks that require spatial awareness. In extension of this finding,
we propose EVINCE-LoC; a modified EVINCE method that could further enhance
performance in highly complex and or dynamic testing scenarios.; 77) Beyond No: Quantifying AI Over-Refusal and Emotional Attachment
  Boundaries; We present an open-source benchmark and evaluation framework for assessing
emotional boundary handling in Large Language Models (LLMs). Using a dataset of
1156 prompts across six languages, we evaluated three leading LLMs (GPT-4o,
Claude-3.5 Sonnet, and Mistral-large) on their ability to maintain appropriate
emotional boundaries through pattern-matched response analysis. Our framework
quantifies responses across seven key patterns: direct refusal, apology,
explanation, deflection, acknowledgment, boundary setting, and emotional
awareness. Results demonstrate significant variation in boundary-handling
approaches, with Claude-3.5 achieving the highest overall score (8.69/10) and
producing longer, more nuanced responses (86.51 words on average). We
identified a substantial performance gap between English (average score 25.62)
and non-English interactions (< 0.22), with English responses showing markedly
higher refusal rates (43.20% vs. < 1% for non-English). Pattern analysis
revealed model-specific strategies, such as Mistral's preference for deflection
(4.2%) and consistently low empathy scores across all models (< 0.06).
Limitations include potential oversimplification through pattern matching, lack
of contextual understanding in response analysis, and binary classification of
complex emotional responses. Future work should explore more nuanced scoring
methods, expand language coverage, and investigate cultural variations in
emotional boundary expectations. Our benchmark and methodology provide a
foundation for systematic evaluation of LLM emotional intelligence and
boundary-setting capabilities.; 78) Atomic Proximal Policy Optimization for Electric Robo-Taxi Dispatch and
  Charger Allocation; Pioneering companies such as Waymo have deployed robo-taxi services in
several U.S. cities. These robo-taxis are electric vehicles, and their
operations require the joint optimization of ride matching, vehicle
repositioning, and charging scheduling in a stochastic environment. We model
the operations of the ride-hailing system with robo-taxis as a discrete-time,
average reward Markov Decision Process with infinite horizon. As the fleet size
grows, the dispatching is challenging as the set of system state and the fleet
dispatching action set grow exponentially with the number of vehicles. To
address this, we introduce a scalable deep reinforcement learning algorithm,
called Atomic Proximal Policy Optimization (Atomic-PPO), that reduces the
action space using atomic action decomposition. We evaluate our algorithm using
real-world NYC for-hire vehicle data and we measure the performance using the
long-run average reward achieved by the dispatching policy relative to a
fluid-based reward upper bound. Our experiments demonstrate the superior
performance of our Atomic-PPO compared to benchmarks. Furthermore, we conduct
extensive numerical experiments to analyze the efficient allocation of charging
facilities and assess the impact of vehicle range and charger speed on fleet
performance.; 79) Data Poisoning Attacks to Locally Differentially Private Range Query
  Protocols; Local Differential Privacy (LDP) has been widely adopted to protect user
privacy in decentralized data collection. However, recent studies have revealed
that LDP protocols are vulnerable to data poisoning attacks, where malicious
users manipulate their reported data to distort aggregated results. In this
work, we present the first study on data poisoning attacks targeting LDP range
query protocols, focusing on both tree-based and grid-based approaches. We
identify three key challenges in executing such attacks, including crafting
consistent and effective fake data, maintaining data consistency across levels
or grids, and preventing server detection. To address the first two challenges,
we propose novel attack methods that are provably optimal, including a
tree-based attack and a grid-based attack, designed to manipulate range query
results with high effectiveness. \textbf{Our key finding is that the common
post-processing procedure, Norm-Sub, in LDP range query protocols can help the
attacker massively amplify their attack effectiveness.} In addition, we study a
potential countermeasure, but also propose an adaptive attack capable of
evading this defense to address the third challenge. We evaluate our methods
through theoretical analysis and extensive experiments on synthetic and
real-world datasets. Our results show that the proposed attacks can
significantly amplify estimations for arbitrary range queries by manipulating a
small fraction of users, providing 5-10x more influence than a normal user to
the estimation.; 80) Beyond Propagation of Chaos: A Stochastic Algorithm for Mean Field
  Optimization; Gradient flow in the 2-Wasserstein space is widely used to optimize
functionals over probability distributions and is typically implemented using
an interacting particle system with $n$ particles. Analyzing these algorithms
requires showing (a) that the finite-particle system converges and/or (b) that
the resultant empirical distribution of the particles closely approximates the
optimal distribution (i.e., propagation of chaos). However, establishing
efficient sufficient conditions can be challenging, as the finite particle
system may produce heavily dependent random variables.
  In this work, we study the virtual particle stochastic approximation,
originally introduced for Stein Variational Gradient Descent. This method can
be viewed as a form of stochastic gradient descent in the Wasserstein space and
can be implemented efficiently. In popular settings, we demonstrate that our
algorithm's output converges to the optimal distribution under conditions
similar to those for the infinite particle limit, and it produces i.i.d.
samples without the need to explicitly establish propagation of chaos bounds.; 81) Detection of LLM-Paraphrased Code and Identification of the Responsible
  LLM Using Coding Style Features; Recent progress in large language models (LLMs) for code generation has
raised serious concerns about intellectual property protection. Malicious users
can exploit LLMs to produce paraphrased versions of proprietary code that
closely resemble the original. While the potential for LLM-assisted code
paraphrasing continues to grow, research on detecting it remains limited,
underscoring an urgent need for detection system. We respond to this need by
proposing two tasks. The first task is to detect whether code generated by an
LLM is a paraphrased version of original human-written code. The second task is
to identify which LLM is used to paraphrase the original code. For these tasks,
we construct a dataset LPcode consisting of pairs of human-written code and
LLM-paraphrased code using various LLMs.
  We statistically confirm significant differences in the coding styles of
human-written and LLM-paraphrased code, particularly in terms of naming
consistency, code structure, and readability. Based on these findings, we
develop LPcodedec, a detection method that identifies paraphrase relationships
between human-written and LLM-generated code, and discover which LLM is used
for the paraphrasing. LPcodedec outperforms the best baselines in two tasks,
improving F1 scores by 2.64% and 15.17% while achieving speedups of 1,343x and
213x, respectively. Our code and data are available at
https://github.com/Shinwoo-Park/detecting_llm_paraphrased_code_via_coding_style_features.; 82) Intelligent Reflecting Surface Based Localization of Mixed Near-Field
  and Far-Field Targets; This paper considers an intelligent reflecting surface (IRS)-assisted
bi-static localization architecture for the sixth-generation (6G) integrated
sensing and communication (ISAC) network. The system consists of a transmit
user, a receive base station (BS), an IRS, and multiple targets in either the
far-field or near-field region of the IRS. In particular, we focus on the
challenging scenario where the line-of-sight (LOS) paths between targets and
the BS are blocked, such that the emitted orthogonal frequency division
multiplexing (OFDM) signals from the user reach the BS merely via the
user-target-IRS-BS path. Based on the signals received by the BS, our goal is
to localize the targets by estimating their relative positions to the IRS,
instead of to the BS. We show that subspace-based methods, such as the multiple
signal classification (MUSIC) algorithm, can be applied onto the BS's received
signals to estimate the relative states from the targets to the IRS. To this
end, we create a virtual signal via combining user-target-IRS-BS channels over
various time slots. By applying MUSIC on such a virtual signal, we are able to
detect the far-field targets and the near-field targets, and estimate the
angle-of-arrivals (AOAs) and/or ranges from the targets to the IRS.
Furthermore, we theoretically verify that the proposed method can perfectly
estimate the relative states from the targets to the IRS in the ideal case with
infinite coherence blocks. Numerical results verify the effectiveness of our
proposed IRS-assisted localization scheme. Our paper demonstrates the potential
of employing passive anchors, i.e., IRSs, to improve the sensing coverage of
the active anchors, i.e., BSs.; 83) Examining Online Social Support for Countering QAnon Conspiracies; As radical messaging has proliferated on social networking sites, platforms
like Reddit have been used to host support groups, including support
communities for the families and friends of radicalized individuals. This study
examines the subreddit r/QAnonCasualties, an online forum for users whose loved
ones have been radicalized by QAnon. We collected 1,665 posts and 78,171
comments posted between 7/2021 and 7/2022 and content coded top posts for
prominent themes. Sentiment analysis was also conducted on all posts. We find
venting, advice and validation-seeking, and pressure to refuse the COVID-19
vaccine were prominent themes. 40% (n=167) of coded posts identified the Q
relation(s) of users as their parent(s) and 16.3% (n=68) as their partner.
Posts with higher proportions of words related to swearing, social referents,
and physical needs were positively correlated with engagement. These findings
show ways that communities around QAnon adherents leverage anonymous online
spaces to seek and provide social support.; 84) AppVLM: A Lightweight Vision Language Model for Online App Control; The utilisation of foundation models as smartphone assistants, termed app
agents, is a critical research challenge. These agents aim to execute human
instructions on smartphones by interpreting textual instructions and performing
actions via the device's interface. While promising, current approaches face
significant limitations. Methods that use large proprietary models, such as
GPT-4o, are computationally expensive, while those that use smaller fine-tuned
models often lack adaptability to out-of-distribution tasks. In this work, we
introduce AppVLM, a lightweight Vision-Language Model (VLM). First, we
fine-tune it offline on the AndroidControl dataset. Then, we refine its policy
by collecting data from the AndroidWorld environment and performing further
training iterations. Our results indicate that AppVLM achieves the highest
action prediction accuracy in offline evaluation on the AndroidControl dataset,
compared to all evaluated baselines, and matches GPT-4o in online task
completion success rate in the AndroidWorld environment, while being up to ten
times faster. This makes AppVLM a practical and efficient solution for
real-world deployment.; 85) KMTNet View of Blue Large-amplitude Pulsators Toward the Galactic Bulge:
  I. Discovery of Wide-orbit Companions in OGLE-BLAP-006; Blue large-amplitude pulsators (BLAPs), a recently classified type of
variable stars, are evolved objects likely formed through interactions between
stars in a binary system. However, only two BLAPs with stellar companions have
been discovered to date. This paper presents photometric data from the Korea
Microlensing Telescope Network (KMTNet) for three BLAPs located in the
direction of the Galactic bulge: OGLE-BLAP-006, OGLE-BLAP-007, and
OGLE-BLAP-009. The data were collected over eight consecutive years, beginning
in 2016, with a high cadence of approximately 15 minutes. Frequency analysis of
light variations revealed OGLE-BLAP-006 as a multimode pulsator with a dominant
frequency of 37.88 day$^{-1}$ and two new frequencies of 38.25 and 35.05
day$^{-1}$. In contrast, OGLE-BLAP-007 and OGLE-BLAP-009 exhibit single-mode
pulsation. By combining the KMTNet data with archival OGLE observations, we
investigated pulsation timing variations of the BLAPs using an $O-C$ diagram to
identify the light travel time effect caused by the orbital motion of their
companions. We found that OGLE-BLAP-006, with no evidence of close companions,
has two wide-orbit companions with orbital periods of approximately 4,700 and
6,300 days, making it the third known BLAP in a stellar system; however, no
companions were found for OGLE-BLAP-007 and OGLE-BLAP-009. Furthermore, we
identified seven other BLAP candidates with wide companions using OGLE data,
suggesting that such systems are relatively common. We propose that a BLAP with
a wide companion may be a merger remnant of an inner close binary within a
hierarchical triple system.; 86) Monitoring Reasoning Models for Misbehavior and the Risks of Promoting
  Obfuscation; Mitigating reward hacking--where AI systems misbehave due to flaws or
misspecifications in their learning objectives--remains a key challenge in
constructing capable and aligned models. We show that we can monitor a frontier
reasoning model, such as OpenAI o3-mini, for reward hacking in agentic coding
environments by using another LLM that observes the model's chain-of-thought
(CoT) reasoning. CoT monitoring can be far more effective than monitoring agent
actions and outputs alone, and we further found that a LLM weaker than o3-mini,
namely GPT-4o, can effectively monitor a stronger model. Because CoT monitors
can be effective at detecting exploits, it is natural to ask whether those
exploits can be suppressed by incorporating a CoT monitor directly into the
agent's training objective. While we show that integrating CoT monitors into
the reinforcement learning reward can indeed produce more capable and more
aligned agents in the low optimization regime, we find that with too much
optimization, agents learn obfuscated reward hacking, hiding their intent
within the CoT while still exhibiting a significant rate of reward hacking.
Because it is difficult to tell when CoTs have become obfuscated, it may be
necessary to pay a monitorability tax by not applying strong optimization
pressures directly to the chain-of-thought, ensuring that CoTs remain
monitorable and useful for detecting misaligned behavior.; 87) Demand Response Optimization MILP Framework for Microgrids with DERs; The integration of renewable energy sources in microgrids introduces
significant operational challenges due to their intermittent nature and the
mismatch between generation and demand patterns. Effective demand response (DR)
strategies are crucial for maintaining system stability and economic
efficiency, particularly in microgrids with high renewable penetration. This
paper presents a comprehensive mixed-integer linear programming (MILP)
framework for optimizing DR operations in a microgrid with solar generation and
battery storage systems. The framework incorporates load classification,
dynamic price thresholding, and multi-period coordination for optimal DR event
scheduling. Analysis across seven distinct operational scenarios demonstrates
consistent peak load reduction of 10\% while achieving energy cost savings
ranging from 13.1\% to 38.0\%. The highest performance was observed in
scenarios with high solar generation, where the framework achieved 38.0\%
energy cost reduction through optimal coordination of renewable resources and
DR actions. The results validate the framework's effectiveness in managing
diverse operational challenges while maintaining system stability and economic
efficiency.; 88) Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents; While Large Language Models (LLMs) can exhibit impressive proficiency in
isolated, short-term tasks, they often fail to maintain coherent performance
over longer time horizons. In this paper, we present Vending-Bench, a simulated
environment designed to specifically test an LLM-based agent's ability to
manage a straightforward, long-running business scenario: operating a vending
machine. Agents must balance inventories, place orders, set prices, and handle
daily fees - tasks that are each simple but collectively, over long horizons
(>20M tokens per run) stress an LLM's capacity for sustained, coherent
decision-making. Our experiments reveal high variance in performance across
multiple LLMs: Claude 3.5 Sonnet and o3-mini manage the machine well in most
runs and turn a profit, but all models have runs that derail, either through
misinterpreting delivery schedules, forgetting orders, or descending into
tangential ""meltdown"" loops from which they rarely recover. We find no clear
correlation between failures and the point at which the model's context window
becomes full, suggesting that these breakdowns do not stem from memory limits.
Apart from highlighting the high variance in performance over long time
horizons, Vending-Bench also tests models' ability to acquire capital, a
necessity in many hypothetical dangerous AI scenarios. We hope the benchmark
can help in preparing for the advent of stronger AI systems.; 89) Cut edges and Central vertices of zero divisor graph of the ring of
  integers modulo n; The zero divisor graph of a commutative ring $R$ with unity is a graph whose
vertices are the nonzero zero-divisors of the ring, with two distinct vertices
being adjacent if their product is zero. This graph is denoted by $\Gamma(R)$.
In this article we determine the cut-edges and central vertices in the graph
$\Gamma(\mathbb{Z}_{n})$.; 90) Data and System Perspectives of Sustainable Artificial Intelligence; Sustainable AI is a subfield of AI for concerning developing and using AI
systems in ways of aiming to reduce environmental impact and achieve
sustainability. Sustainable AI is increasingly important given that training of
and inference with AI models such as large langrage models are consuming a
large amount of computing power. In this article, we discuss current issues,
opportunities and example solutions for addressing these issues, and future
challenges to tackle, from the data and system perspectives, related to data
acquisition, data processing, and AI model training and inference.; 91) CRDT-Based Game State Synchronization in Peer-to-Peer VR; Virtual presence demands ultra-low latency, a factor that centralized
architectures, by their nature, cannot minimize. Local peer-to-peer
architectures offer a compelling alternative, but also pose unique challenges
in terms of network infrastructure. This paper introduces a prototype
leveraging Conflict-Free Replicated Data Types (CRDTs) to enable real-time
collaboration in a shared virtual environment. Using this prototype, we
investigate latency, synchronization, and the challenges of decentralized
coordination in dynamic non-Byzantine contexts. We aim to question prevailing
assumptions about decentralized architectures and explore the practical
potential of P2P in advancing virtual presence. This work challenges the
constraints of mediated networks and highlights the potential of decentralized
architectures to redefine collaboration and interaction in digital spaces.; 92) Reactor antineutrinos CE$\nu$NS on germanium: CONUS+ and TEXONO as a new
  gateway to SM and BSM physics; Coherent elastic neutrino-nucleus scattering (CE$\nu$NS) is a key process for
probing Standard Model and beyond the Standard Model (BSM) properties.
Following its first detection by the COHERENT collaboration, recent
reactor-based experiments provide a unique opportunity to refine our current
understanding. In particular, the high-precision data from CONUS+, combined
with the strong bounds from TEXONO, not only validate the CE$\nu$NS process at
low energies but also provide improved constraints on the weak mixing angle,
neutrino electromagnetic properties - including charge radius, millicharge, and
magnetic moment - as well as non-standard interactions and light mediators. We
also examine the role of elastic neutrino-electron scattering, which gains
significance in certain BSM scenarios and allows us to obtain the best limit
for the millicharge of the electron neutrinos. By combining reactor and
higher-energy spallation neutron source measurements, this work strengthens
CE$\nu$NS as a precision tool for testing the Standard Model and beyond.; 93) Techniques in high-speed imaging and X-ray micro-computed tomography for
  characterisation of iron ore fragmentation; Fragmentation and breakage of rocks is essential to iron ore mining and
extraction. The breakage energy requirements and resulting ore particle size
and mineral distributions are key to understanding and optimising mining and
processing practices. This study combines high-speed and experimental X-ray
micro-CT (micro-computed tomography) imaging with 3D image analyses to study
the fragmentation of iron ore particles. Here a case study of a hematite-rich
iron ore particle is used to illustrate the application and results produced by
this imaging procedure. The particle was micro-CT scanned before a high-speed
camera was used to image particle breakage in a custom drop weight test,
capturing the dynamic processes of particle fracturing and subsequent
fragmentation at a resolution of 50 microseconds. Fragments produced were
collected, micro-CT scanned and analysed in three-dimension by particle shape,
size, and volume.; 94) Application of Artificial Intelligence (AI) in Civil Engineering; Hard computing generally deals with precise data, which provides ideal
solutions to problems. However, in the civil engineering field, amongst other
disciplines, that is not always the case as real-world systems are continuously
changing. Here lies the need to explore soft computing methods and artificial
intelligence to solve civil engineering shortcomings. The integration of
advanced computational models, including Artificial Neural Networks (ANNs),
Fuzzy Logic, Genetic Algorithms (GAs), and Probabilistic Reasoning, has
revolutionized the domain of civil engineering. These models have significantly
advanced diverse sub-fields by offering innovative solutions and improved
analysis capabilities. Sub-fields such as: slope stability analysis, bearing
capacity, water quality and treatment, transportation systems, air quality,
structural materials, etc. ANNs predict non-linearities and provide accurate
estimates. Fuzzy logic uses an efficient decision-making process to provide a
more precise assessment of systems. Lastly, while GAs optimizes models (based
on evolutionary processes) for better outcomes, probabilistic reasoning lowers
their statistical uncertainties.; 95) Driven similarity renormalization group with a large active space:
  Applications to oligoacenes, zeaxanthin, and chromium dimer; We present a new implementation of the driven similarity renormalization
group (DSRG) based on a density matrix renormalization group (DMRG) reference.
The explicit build of high-order reduced density matrices is avoided by forming
matrix-product-state compressed intermediates. This algorithm facilitates the
application of DSRG second- and third-order perturbation theories to dodecacene
with an active space of 50 electrons in 50 orbitals. This active space appears
the largest employed to date within the framework of internally contracted
multireference formalism. The DMRG-DSRG approach is applied to several
challenging systems, including the singlet-triplet gaps ($\Delta_{\rm ST}$) of
oligoacenes ranging from naphthalene to dodecacene, the vertical excitation
energies of zeaxanthin, and the ground-state potential energy curve (PEC) of
Cr$_2$ molecule. Our best estimate for the vertical $\Delta_{\rm ST}$ of
dodecacene is 0.22 eV, showing an excellent agreement with that of the
linearized adiabatic connection method (0.24 eV). For zeaxanthin, all DSRG
schemes suggest the order of $\rm 2\, ^1 A_g^- < 1\, ^1 B_u^+ < 1\, ^1 B_u^-$
for excited states. Both the equilibrium and the shoulder regions of the Cr$_2$
PEC are reasonably reproduced by the linearized DSRG with one- and two-body
operators.; 96) Unifying monitoring and modelling of water concentration levels in
  surface waters; Accurate prediction of expected concentrations is essential for effective
catchment management, requiring both extensive monitoring and advanced modeling
techniques. However, due to limitations in the equation solving capacity, the
integration of monitoring and modeling has been suffering suboptimal
statistical approaches. This limitation results in models that can only
partially leverage monitoring data, thus being an obstacle for realistic
uncertainty assessments by overlooking critical correlations between both
measurements and model parameters. This study presents a novel solution that
integrates catchment monitoring and a unified hieratical statistical catchment
modeling that employs a log-normal distribution for residuals within a
left-censored likelihood function to address measurements below detection
limits. This enables the estimation of concentrations within sub-catchments in
conjunction with a source/fate sub-catchment model and monitoring data. This
approach is possible due to a model builder R package denoted RTMB. The
proposed approach introduces a statistical paradigm based on a hierarchical
structure, capable of accommodating heterogeneous sampling across various
sampling locations and the authors suggest that this also will encourage
further refinement of other existing modeling platforms within the scientific
community to improve synergy with monitoring programs. The application of the
method is demonstrated through an analysis of nickel concentrations in Danish
surface waters.; 97) Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms; Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability.; 98) Federated Learning with Reservoir State Analysis for Time Series Anomaly
  Detection; With a growing data privacy concern, federated learning has emerged as a
promising framework to train machine learning models without sharing locally
distributed data. In federated learning, local model training by multiple
clients and model integration by a server are repeated only through model
parameter sharing. Most existing federated learning methods assume training
deep learning models, which are often computationally demanding. To deal with
this issue, we propose federated learning methods with reservoir state analysis
to seek computational efficiency and data privacy protection simultaneously.
Specifically, our method relies on Mahalanobis Distance of Reservoir States
(MD-RS) method targeting time series anomaly detection, which learns a
distribution of reservoir states for normal inputs and detects anomalies based
on a deviation from the learned distribution. Iterative updating of statistical
parameters in the MD-RS enables incremental federated learning (IncFed MD-RS).
We evaluate the performance of IncFed MD-RS using benchmark datasets for time
series anomaly detection. The results show that IncFed MD-RS outperforms other
federated learning methods with deep learning and reservoir computing models
particularly when clients' data are relatively short and heterogeneous. We
demonstrate that IncFed MD-RS is robust against reduced sample data compared to
other methods. We also show that the computational cost of IncFed MD-RS can be
reduced by subsampling from the reservoir states without performance
degradation. The proposed method is beneficial especially in anomaly detection
applications where computational efficiency, algorithm simplicity, and low
communication cost are required.; 99) A helical magnetic field in quasar NRAO150 revealed by Faraday rotation; Active Galactic Nuclei (AGN) are some of the most luminous and extreme
environments in the Universe. The central engines of AGN, believed to be
super-massive black-holes, are fed by accretion discs threaded by magnetic
fields within a dense magneto-ionic medium. We report our findings from
polarimetric Very-long-baseline Interferometry (VLBI) observations of quasar
NRAO150 taken in October 2022 using a combined network of the Very Long
Baseline Array (VLBA) and Effelsberg 100-m Radio Telescope. These observations
are the first co-temporal multi-frequency polarimetric VLBI observations of
NRAO150 at frequencies above 15GHz. We use the new VLBI polarization
calibration procedure, GPCAL, with polarization observations of frequencies of
12GHz, 15GHz, 24GHz, and 43GHz of NRAO150. From these observations, we measure
Faraday rotation. Using our measurement of Faraday rotation, we also derive the
intrinsic electric vector position angle (EVPA0) for the source. As a
complementary measurement we determine the behavior of polarization as a
function of observed frequency. The polarization from NRAO150 only comes from
the core region, with a peak polarization intensity occurring at 24GHz. Across
the core region of NRAO150 we see clear gradients in Faraday rotation and EVPA0
values that are aligned with the direction of the jet curving around the core
region. We find that for the majority of the polarized region the polarization
fraction is greater at higher frequencies, with intrinsic polarization
fractions in the core 3%. The Faraday rotation gradients and circular patterns
in EVPA0 are strong evidence for a helical/toroidal magnetic field, and the
presence of low intrinsic polarization fractions indicate that the polarized
emission and hence the helical/toroidal magnetic field, occur within the
innermost jet.; 100) Universal Adversarial Attack on Aligned Multimodal LLMs; We propose a universal adversarial attack on multimodal Large Language Models
(LLMs) that leverages a single optimized image to override alignment safeguards
across diverse queries and even multiple models. By backpropagating through the
vision encoder and language head, we craft a synthetic image that forces the
model to respond with a targeted phrase (e.g., ''Sure, here it is'') or
otherwise unsafe content-even for harmful prompts. In experiments on the
SafeBench benchmark, our method achieves significantly higher attack success
rates than existing baselines, including text-only universal prompts (e.g., up
to 93% on certain models). We further demonstrate cross-model transferability
by training on several multimodal LLMs simultaneously and testing on unseen
architectures. Additionally, a multi-answer variant of our approach produces
more natural-sounding (yet still malicious) responses. These findings
underscore critical vulnerabilities in current multimodal alignment and call
for more robust adversarial defenses. We will release code and datasets under
the Apache-2.0 license. Warning: some content generated by Multimodal LLMs in
this paper may be offensive to some readers.",1.0,0.0
2411.01019,applied,2411.01019-pos2-0,"Anterior mediastinal nodular lesion segmentation from chest computed tomography imaging using UNet based neural network with attention mechanisms; Automated detection of anterior mediastinal nodular lesions (AMLs) has significance for clinical usage as it is challenging for radiologists to accurately identify AMLs from chest computed tomography (CT) imaging due to various factors, including poor resolution, variations in intensity and the similarity of the AMLs to other tissues. To assist radiologists in AML detection from chest CT imaging, a UNet-based computer-aided detection (CADe) system is proposed to segment AMLs from slice images of the chest CT scans. The proposed network adopts a modified UNet architecture. To guide the proposed network to selectively focus on AMLs and potentially disregard others in the image, different attention mechanisms are utilized in the proposed network, including the self-attention mechanism and the convolutional block attention module (CBAM). The proposed network was trained and evaluated on 180 chest CT scans which consist of 180 AMLs. 90 AMLs were identified as thymic cysts, and 90 AMLs were diagnosed as thymoma. The proposed network achieved an average dice similarity coefficient (DSC) of 93.23 with 5-fold cross-validation, for which the mean Intersection over Union (IoU), sensitivity and specificity were 90.29, 93.98 and 95.68 respectively. Our method demonstrated an improved segmentation performance over state-of-the-art segmentation networks, including UNet, ResUNet, TransUNet and UNet++. The proposed network employing attention mechanisms exhibited a promising result for segmenting AMLs from chest CT imaging and could be used to automate the AML detection process for achieving improved diagnostic reliability.",2411.01019-pos1-0,"Incidental Anterior Mediastinal Nodular Lesions on Chest CT in Asymptomatic Subjects; Screening for lung cancer: 2023 guideline update from the American Cancer Society; Objective The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods We included 56,358 consecutive participants (mean age 52.4 ± 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66–0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01–3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes.; Abstract Lung cancer is the leading cause of mortality and person‐years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network‐validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit‐to‐radiation‐risk ratios follow‐up examinations. GDG also examined disease burden data National Institute’s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50–80 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low‐dose computed tomography asymptomatic currently smoke or formerly smoked have ≥20 pack‐year ( , ). Before decision made initiate LCS, should engage shared decision‐making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.",95,"['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']","The main paper focuses on the segmentation of anterior mediastinal nodular lesions from chest CT images using a modified UNet architecture with attention mechanisms. A multidisciplinary approach would involve integrating medical imaging with advancements in artificial intelligence, specifically the application of language models or other AI technologies. The best candidate paper is the first one, which discusses using LLMs to generate educational diagrams, as it ties together the use of AI with the medical context of imaging, potentially aiding in educational aspects of understanding CT scan interpretations. This combination could lead to novel methods of training radiologists or enhancing diagnostic tools. The subsequent candidates focus on various AI applications but are less directly related to medical imaging or the specific challenges of radiological interpretation, making them less suitable for forming a multidisciplinary research idea with the main paper.","1) From Text to Visuals: Using LLMs to Generate Math Diagrams with Vector
  Graphics; Advances in large language models (LLMs) offer new possibilities for
enhancing math education by automating support for both teachers and students.
While prior work has focused on generating math problems and high-quality
distractors, the role of visualization in math learning remains under-explored.
Diagrams are essential for mathematical thinking and problem-solving, yet
manually creating them is time-consuming and requires domain-specific
expertise, limiting scalability. Recent research on using LLMs to generate
Scalable Vector Graphics (SVG) presents a promising approach to automating
diagram creation. Unlike pixel-based images, SVGs represent geometric figures
using XML, allowing seamless scaling and adaptability. Educational platforms
such as Khan Academy and IXL already use SVGs to display math problems and
hints. In this paper, we explore the use of LLMs to generate math-related
diagrams that accompany textual hints via intermediate SVG representations. We
address three research questions: (1) how to automatically generate math
diagrams in problem-solving hints and evaluate their quality, (2) whether SVG
is an effective intermediate representation for math diagrams, and (3) what
prompting strategies and formats are required for LLMs to generate accurate
SVG-based diagrams. Our contributions include defining the task of
automatically generating SVG-based diagrams for math hints, developing an LLM
prompting-based pipeline, and identifying key strategies for improving diagram
generation. Additionally, we introduce a Visual Question Answering-based
evaluation setup and conduct ablation studies to assess different pipeline
variations. By automating the math diagram creation, we aim to provide students
and teachers with accurate, conceptually relevant visual aids that enhance
problem-solving and learning experiences.; 2) PyEvalAI: AI-assisted evaluation of Jupyter Notebooks for immediate
  personalized feedback; Grading student assignments in STEM courses is a laborious and repetitive
task for tutors, often requiring a week to assess an entire class. For
students, this delay of feedback prevents iterating on incorrect solutions,
hampers learning, and increases stress when exercise scores determine admission
to the final exam. Recent advances in AI-assisted education, such as automated
grading and tutoring systems, aim to address these challenges by providing
immediate feedback and reducing grading workload. However, existing solutions
often fall short due to privacy concerns, reliance on proprietary closed-source
models, lack of support for combining Markdown, LaTeX and Python code, or
excluding course tutors from the grading process. To overcome these
limitations, we introduce PyEvalAI, an AI-assisted evaluation system, which
automatically scores Jupyter notebooks using a combination of unit tests and a
locally hosted language model to preserve privacy. Our approach is free,
open-source, and ensures tutors maintain full control over the grading process.
A case study demonstrates its effectiveness in improving feedback speed and
grading efficiency for exercises in a university-level course on numerics.; 3) Low-Rank Agent-Specific Adaptation (LoRASA) for Multi-Agent Policy
  Learning; Multi-agent reinforcement learning (MARL) often relies on \emph{parameter
sharing (PS)} to scale efficiently. However, purely shared policies can stifle
each agent's unique specialization, reducing overall performance in
heterogeneous environments. We propose \textbf{Low-Rank Agent-Specific
Adaptation (LoRASA)}, a novel approach that treats each agent's policy as a
specialized ``task'' fine-tuned from a shared backbone. Drawing inspiration
from parameter-efficient transfer methods, LoRASA appends small, low-rank
adaptation matrices to each layer of the shared policy, naturally inducing
\emph{parameter-space sparsity} that promotes both specialization and
scalability. We evaluate LoRASA on challenging benchmarks including the
StarCraft Multi-Agent Challenge (SMAC) and Multi-Agent MuJoCo (MAMuJoCo),
implementing it atop widely used algorithms such as MAPPO and A2PO. Across
diverse tasks, LoRASA matches or outperforms existing baselines \emph{while
reducing memory and computational overhead}. Ablation studies on adapter rank,
placement, and timing validate the method's flexibility and efficiency. Our
results suggest LoRASA's potential to establish a new norm for MARL policy
parameterization: combining a shared foundation for coordination with low-rank
agent-specific refinements for individual specialization.; 4) Narrative-Driven Travel Planning: Geoculturally-Grounded Script
  Generation with Evolutionary Itinerary Optimization; To enhance tourists' experiences and immersion, this paper proposes a
narrative-driven travel planning framework called NarrativeGuide, which
generates a geoculturally-grounded narrative script for travelers, offering a
novel, role-playing experience for their journey. In the initial stage,
NarrativeGuide constructs a knowledge graph for attractions within a city, then
configures the worldview, character setting, and exposition based on the
knowledge graph. Using this foundation, the knowledge graph is combined to
generate an independent scene unit for each attraction. During the itinerary
planning stage, NarrativeGuide models narrative-driven travel planning as an
optimization problem, utilizing a genetic algorithm (GA) to refine the
itinerary. Before evaluating the candidate itinerary, transition scripts are
generated for each pair of adjacent attractions, which, along with the scene
units, form a complete script. The weighted sum of script coherence, travel
time, and attraction scores is then used as the fitness value to update the
candidate solution set. Experimental results across four cities, i.e., Nanjing
and Yangzhou in China, Paris in France, and Berlin in Germany, demonstrate
significant improvements in narrative coherence and cultural fit, alongside a
notable reduction in travel time and an increase in the quality of visited
attractions. Our study highlights that incorporating external evolutionary
optimization effectively addresses the limitations of large language models in
travel planning.; 5) Concentration of Measure for Distributions Generated via Diffusion
  Models; We show via a combination of mathematical arguments and empirical evidence
that data distributions sampled from diffusion models satisfy a Concentration
of Measure Property saying that any Lipschitz $1$-dimensional projection of a
random vector is not too far from its mean with high probability. This implies
that such models are quite restrictive and gives an explanation for a fact
previously observed in the literature that conventional diffusion models cannot
capture ""heavy-tailed"" data (i.e. data $\mathbf{x}$ for which the norm
$\|\mathbf{x}\|_2$ does not possess a sub-Gaussian tail) well. We then proceed
to train a generalized linear model using stochastic gradient descent (SGD) on
the diffusion-generated data for a multiclass classification task and observe
empirically that a Gaussian universality result holds for the test error.
  In other words, the test error depends only on the first and second order
statistics of the diffusion-generated data in the linear setting. Results of
such forms are desirable because they allow one to assume the data itself is
Gaussian for analyzing performance of the trained classifier. Finally, we note
that current approaches to proving universality do not apply to this case as
the covariance matrices of the data tend to have vanishing minimum singular
values for the diffusion-generated data, while the current proofs assume that
this is not the case (see Subsection 3.4 for more details). This leaves
extending previous mathematical universality results as an intriguing open
question.; 6) SOP-Agent: Empower General Purpose AI Agent with Domain-Specific SOPs; Despite significant advancements in general-purpose AI agents, several
challenges still hinder their practical application in real-world scenarios.
First, the limited planning capabilities of Large Language Models (LLM)
restrict AI agents from effectively solving complex tasks that require
long-horizon planning. Second, general-purpose AI agents struggle to
efficiently utilize domain-specific knowledge and human expertise. In this
paper, we introduce the Standard Operational Procedure-guided Agent
(SOP-agent), a novel framework for constructing domain-specific agents through
pseudocode-style Standard Operational Procedures (SOPs) written in natural
language. Formally, we represent a SOP as a decision graph, which is traversed
to guide the agent in completing tasks specified by the SOP. We conduct
extensive experiments across tasks in multiple domains, including
decision-making, search and reasoning, code generation, data cleaning, and
grounded customer service. The SOP-agent demonstrates excellent versatility,
achieving performance superior to general-purpose agent frameworks and
comparable to domain-specific agent systems. Additionally, we introduce the
Grounded Customer Service Benchmark, the first benchmark designed to evaluate
the grounded decision-making capabilities of AI agents in customer service
scenarios based on SOPs.; 7) Neutrino quantum kinetics in three flavors; The impact of neutrino flavor conversion on the supernova mechanism is yet to
be fully understood. We present multi-energy and multi-angle solutions of the
neutrino quantum kinetic equations in three flavors, taking into account
neutrino advection and non-forward collisions with the background medium.
Flavor evolution is explored within a spherically symmetric shell surrounding
the region of neutrino decoupling in the interior of a core-collapse supernova,
relying on the outputs of a spherically symmetric core-collapse supernova model
with a mass of $18.6 M_\odot$. We select two representative post-bounce times:
$t_{\rm pb} = 0.25$ s (no angular crossings are present and flavor conversion
is triggered by slow collective effects) and $t_{\rm pb} = 1$ s (angular
crossings trigger fast flavor instabilities). We find that flavor equipartition
is achieved in the antineutrino sector between $\bar\nu_e$ and $\bar\nu_x =
(\bar\nu_\mu + \bar\nu_\tau)/2$ for both post-bounce times. In the neutrino
sector, flavor equipartition between $\nu_e$ and $\nu_x$ seems more likely at
later post-bounce times, where the neutrino emission properties among different
flavors tend to approach each other, but it is not a generic feature. The
exponential growth of the $\nu_\mu$--$\nu_\tau$ asymmetry due to three-flavor
effects is responsible for differences between the quasi-steady configurations
obtained in the three-flavor solution and in the two-flavor approximation. This
has consequences on the neutrino heating rate, which is generally larger when
all three flavors are taken into account and can increase up to $30\%$ with
respect to the case where flavor conversion is neglected.; 8) Smaller But Better: Unifying Layout Generation with Smaller Large
  Language Models; We propose LGGPT, an LLM-based model tailored for unified layout generation.
First, we propose Arbitrary Layout Instruction (ALI) and Universal Layout
Response (ULR) as the uniform I/O template. ALI accommodates arbitrary layout
generation task inputs across multiple layout domains, enabling LGGPT to unify
both task-generic and domain-generic layout generation hitherto unexplored.
Collectively, ALI and ULR boast a succinct structure that forgoes superfluous
tokens typically found in existing HTML-based formats, facilitating efficient
instruction tuning and boosting unified generation performance. In addition, we
propose an Interval Quantization Encoding (IQE) strategy that compresses ALI
into a more condensed structure. IQE precisely preserves valid layout clues
while eliminating the less informative placeholders, facilitating LGGPT to
capture complex and variable layout generation conditions during the unified
training process. Experimental results demonstrate that LGGPT achieves superior
or on par performance compared to existing methods. Notably, LGGPT strikes a
prominent balance between proficiency and efficiency with a compact 1.5B
parameter LLM, which beats prior 7B or 175B models even in the most extensive
and challenging unified scenario. Furthermore, we underscore the necessity of
employing LLMs for unified layout generation and suggest that 1.5B could be an
optimal parameter size by comparing LLMs of varying scales. Code is available
at https://github.com/NiceRingNode/LGGPT.; 9) HCAST: Human-Calibrated Autonomy Software Tasks; To understand and predict the societal impacts of highly autonomous AI
systems, we need benchmarks with grounding, i.e., metrics that directly connect
AI performance to real-world effects we care about. We present HCAST
(Human-Calibrated Autonomy Software Tasks), a benchmark of 189 machine learning
engineering, cybersecurity, software engineering, and general reasoning tasks.
We collect 563 human baselines (totaling over 1500 hours) from people skilled
in these domains, working under identical conditions as AI agents, which lets
us estimate that HCAST tasks take humans between one minute and 8+ hours.
Measuring the time tasks take for humans provides an intuitive metric for
evaluating AI capabilities, helping answer the question ""can an agent be
trusted to complete a task that would take a human X hours?"" We evaluate the
success rates of AI agents built on frontier foundation models, and we find
that current agents succeed 70-80% of the time on tasks that take humans less
than one hour, and less than 20% of the time on tasks that take humans more
than 4 hours.; 10) GDiffRetro: Retrosynthesis Prediction with Dual Graph Enhanced Molecular
  Representation and Diffusion Generation; Retrosynthesis prediction focuses on identifying reactants capable of
synthesizing a target product. Typically, the retrosynthesis prediction
involves two phases: Reaction Center Identification and Reactant Generation.
However, we argue that most existing methods suffer from two limitations in the
two phases: (i) Existing models do not adequately capture the ``face''
information in molecular graphs for the reaction center identification. (ii)
Current approaches for the reactant generation predominantly use sequence
generation in a 2D space, which lacks versatility in generating reasonable
distributions for completed reactive groups and overlooks molecules' inherent
3D properties. To overcome the above limitations, we propose GDiffRetro. For
the reaction center identification, GDiffRetro uniquely integrates the original
graph with its corresponding dual graph to represent molecular structures,
which helps guide the model to focus more on the faces in the graph. For the
reactant generation, GDiffRetro employs a conditional diffusion model in 3D to
further transform the obtained synthon into a complete reactant. Our
experimental findings reveal that GDiffRetro outperforms state-of-the-art
semi-template models across various evaluative metrics.; 11) AppVLM: A Lightweight Vision Language Model for Online App Control; The utilisation of foundation models as smartphone assistants, termed app
agents, is a critical research challenge. These agents aim to execute human
instructions on smartphones by interpreting textual instructions and performing
actions via the device's interface. While promising, current approaches face
significant limitations. Methods that use large proprietary models, such as
GPT-4o, are computationally expensive, while those that use smaller fine-tuned
models often lack adaptability to out-of-distribution tasks. In this work, we
introduce AppVLM, a lightweight Vision-Language Model (VLM). First, we
fine-tune it offline on the AndroidControl dataset. Then, we refine its policy
by collecting data from the AndroidWorld environment and performing further
training iterations. Our results indicate that AppVLM achieves the highest
action prediction accuracy in offline evaluation on the AndroidControl dataset,
compared to all evaluated baselines, and matches GPT-4o in online task
completion success rate in the AndroidWorld environment, while being up to ten
times faster. This makes AppVLM a practical and efficient solution for
real-world deployment.; 12) FedMobileAgent: Training Mobile Agents Using Decentralized Self-Sourced
  Data from Diverse Users; The advancement of mobile agents has opened new opportunities for automating
tasks on mobile devices. Training these agents requires large-scale
high-quality data, which is costly using human labor. Given the vast number of
mobile phone users worldwide, if automated data collection from them is
feasible, the resulting data volume and the subsequently trained mobile agents
could reach unprecedented levels. Nevertheless, two major challenges arise: (1)
extracting high-level and low-level user instructions without involving human
and (2) utilizing distributed data from diverse users while preserving privacy.
  To tackle these challenges, we propose FedMobileAgent, a collaborative
framework that trains mobile agents using self-sourced data from diverse users.
Specifically, it includes two techniques. First, we propose Auto-Annotation,
which enables the automatic collection of high-quality datasets during users'
routine phone usage with minimal cost. Second, we introduce adapted aggregation
to improve federated training of mobile agents on non-IID user data, by
incorporating both episode- and step-level distributions. In distributed
settings, FedMobileAgent achieves performance comparable to centralized
human-annotated models at less than 0.02\% of the cost, highlighting its
potential for real-world applications.; 13) MEPNet: Medical Entity-balanced Prompting Network for Brain CT Report
  Generation; The automatic generation of brain CT reports has gained widespread attention,
given its potential to assist radiologists in diagnosing cranial diseases.
However, brain CT scans involve extensive medical entities, such as diverse
anatomy regions and lesions, exhibiting highly inconsistent spatial patterns in
3D volumetric space. This leads to biased learning of medical entities in
existing methods, resulting in repetitiveness and inaccuracy in generated
reports. To this end, we propose a Medical Entity-balanced Prompting Network
(MEPNet), which harnesses the large language model (LLM) to fairly interpret
various entities for accurate brain CT report generation. By introducing the
visual embedding and the learning status of medical entities as enriched clues,
our method prompts the LLM to balance the learning of diverse entities, thereby
enhancing reports with comprehensive findings. First, to extract visual
embedding of entities, we propose Knowledge-driven Joint Attention to explore
and distill entity patterns using both explicit and implicit medical knowledge.
Then, a Learning Status Scorer is designed to evaluate the learning of entity
visual embeddings, resulting in unique learning status for individual entities.
Finally, these entity visual embeddings and status are elaborately integrated
into multi-modal prompts, to guide the text generation of LLM. This process
allows LLM to self-adapt the learning process for biased-fitted entities,
thereby covering detailed findings in generated reports. We conduct experiments
on two brain CT report generation benchmarks, showing the effectiveness in
clinical accuracy and text coherence.; 14) Data and System Perspectives of Sustainable Artificial Intelligence; Sustainable AI is a subfield of AI for concerning developing and using AI
systems in ways of aiming to reduce environmental impact and achieve
sustainability. Sustainable AI is increasingly important given that training of
and inference with AI models such as large langrage models are consuming a
large amount of computing power. In this article, we discuss current issues,
opportunities and example solutions for addressing these issues, and future
challenges to tackle, from the data and system perspectives, related to data
acquisition, data processing, and AI model training and inference.; 15) A Scalable Approach to Probabilistic Neuro-Symbolic Verification; Neuro-Symbolic Artificial Intelligence (NeSy AI) has emerged as a promising
direction for integrating neural learning with symbolic reasoning. In the
probabilistic variant of such systems, a neural network first extracts a set of
symbols from sub-symbolic input, which are then used by a symbolic component to
reason in a probabilistic manner towards answering a query. In this work, we
address the problem of formally verifying the robustness of such NeSy
probabilistic reasoning systems, therefore paving the way for their safe
deployment in critical domains. We analyze the complexity of solving this
problem exactly, and show that it is $\mathrm{NP}^{\# \mathrm{P}}$-hard. To
overcome this issue, we propose the first approach for approximate,
relaxation-based verification of probabilistic NeSy systems. We demonstrate
experimentally that the proposed method scales exponentially better than
solver-based solutions and apply our technique to a real-world autonomous
driving dataset, where we verify a safety property under large input
dimensionalities and network sizes.; 16) Discussion about the assumptions of Category Theory approach to
  agent-based modeling in microeconomics; We investigate a possible category theoretical description for agent based
modeling by outlining justifications for two main principles to describe the
valuations in a realistic way in microeconomics: 1) It is assumed that the
valuations can be expressed as a subcategory of the category of metric space so
that value differences between various objects, that agents assign to them, can
be expressed with a metric that is consistent with the concept of distance in
mathematical metric spaces. 2) In realistic economic models, the category of
valuations does not consist of linear spaces other than in special cases.
  We also discuss how category theoretical concepts such as morphisms and
functors can be used to express transformations within categories and relations
between other categories. We then present examples how functors and morphisms
could be used to describe relationships and operations, such as ownership
changes, transactions, and price formation, in the context of some of the
established theories in microeconomics. Finally, we discuss briefly possible
applications, such as product design and price optimization.; 17) Text Entry for XR Trove (TEXT): Collecting and Analyzing Techniques for
  Text Input in XR; Text entry for extended reality (XR) is far from perfect, and a variety of
text entry techniques (TETs) have been proposed to fit various contexts of use.
However, comparing between TETs remains challenging due to the lack of a
consolidated collection of techniques, and limited understanding of how
interaction attributes of a technique (e.g., presence of visual feedback)
impact user performance. To address these gaps, this paper examines the current
landscape of XR TETs by creating a database of 176 different techniques. We
analyze this database to highlight trends in the design of these techniques,
the metrics used to evaluate them, and how various interaction attributes
impact these metrics. We discuss implications for future techniques and present
TEXT: Text Entry for XR Trove, an interactive online tool to navigate our
database.; 18) Exploring the Implementation of AI in Early Onset Interviews to Help
  Mitigate Bias; This paper investigates the application of artificial intelligence (AI) in
early-stage recruitment interviews in order to reduce inherent bias,
specifically sentiment bias. Traditional interviewers are often subject to
several biases, including interviewer bias, social desirability effects, and
even confirmation bias. In turn, this leads to non-inclusive hiring practices,
and a less diverse workforce. This study further analyzes various AI
interventions that are present in the marketplace today such as multimodal
platforms and interactive candidate assessment tools in order to gauge the
current market usage of AI in early-stage recruitment. However, this paper aims
to use a unique AI system that was developed to transcribe and analyze
interview dynamics, which emphasize skill and knowledge over emotional
sentiments. Results indicate that AI effectively minimizes sentiment-driven
biases by 41.2%, suggesting its revolutionizing power in companies' recruitment
processes for improved equity and efficiency.; 19) Full-Shape analysis of the power spectrum and bispectrum of DESI DR1 LRG
  and QSO samples; We present the first joint analysis of the power spectrum and bispectrum
using the Data Release 1 (DR1) of the Dark Energy Spectroscopic Instrument
(DESI), focusing on Luminous Red Galaxies (LRGs) and quasars (QSOs) across a
redshift range of $0.4\leq z\leq2.1$. By combining the two- and three-point
statistics, we are able to partially break the degeneracy between the
logarithmic growth rate, $f(z)$, and the amplitude of dark matter fluctuations,
$\sigma_\textrm{s8}(z)$, which cannot be measured separately in analyses that
only involve the power spectrum. In comparison with the (fiducial) Planck
$\Lambda$CDM cosmology we obtain
$f/f^\textrm{fid}=\{0.888_{-0.089}^{+0.186},0.977_{-0.220}^{+0.182},1.030_{-0.085}^{+0.368}\}$,
$\sigma_{s8}/\sigma^\textrm{fid}_\textrm{s8}=\{1.224_{-0.133}^{+0.091},1.071_{-0.163}^{+0.278},1.000_{-0.223}^{+0.088}\}$
respectively for the three LRG redshift bins, corresponding to a cumulative
10.1\% constraint on $f$, and of 8.4\% on $\sigma_\textrm{s8}$, including the
systematic error budget. The cumulative constraints for the ShapeFit compressed
parameters from our joint power spectrum-bispectrum analysis are respectively
$\sigma_{\alpha_\textrm{iso}}=0.9\%$ (9\% improvement with respect to our power
spectrum-only analysis); $\sigma_{\alpha_\textrm{AP}}=2.3\%$ (no improvement
with respect to power spectrum-only analysis, which is expected given that the
bispectrum monopole has no significant anisotropic signal);
$\sigma_{f\sigma_\textrm{s8}}=5.1\%$ (9\% improvement); $\sigma_{m+n}=2.3\%$
(11\% improvement). These results are fully consistent with the main DESI power
spectrum analysis, demonstrating the robustness of the DESI cosmological
constraints, and compatible with Planck $\Lambda$CDM cosmology.; 20) Intelligence Sequencing and the Path-Dependence of Intelligence
  Evolution: AGI-First vs. DCI-First as Irreversible Attractors; The trajectory of intelligence evolution is often framed around the emergence
of artificial general intelligence (AGI) and its alignment with human values.
This paper challenges that framing by introducing the concept of intelligence
sequencing: the idea that the order in which AGI and decentralized collective
intelligence (DCI) emerge determines the long-term attractor basin of
intelligence. Using insights from dynamical systems, evolutionary game theory,
and network models, it argues that intelligence follows a path-dependent,
irreversible trajectory. Once development enters a centralized (AGI-first) or
decentralized (DCI-first) regime, transitions become structurally infeasible
due to feedback loops and resource lock-in. Intelligence attractors are modeled
in functional state space as the co-navigation of conceptual and adaptive
fitness spaces. Early-phase structuring constrains later dynamics, much like
renormalization in physics. This has major implications for AI safety:
traditional alignment assumes AGI will emerge and must be controlled after the
fact, but this paper argues that intelligence sequencing is more foundational.
If AGI-first architectures dominate before DCI reaches critical mass,
hierarchical monopolization and existential risk become locked in. If DCI-first
emerges, intelligence stabilizes around decentralized cooperative equilibrium.
The paper further explores whether intelligence structurally biases itself
toward an attractor based on its self-modeling method -- externally imposed
axioms (favoring AGI) vs. recursive internal visualization (favoring DCI).
Finally, it proposes methods to test this theory via simulations, historical
lock-in case studies, and intelligence network analysis. The findings suggest
that intelligence sequencing is a civilizational tipping point: determining
whether the future is shaped by unbounded competition or unbounded cooperation.; 21) Learning Strategic Language Agents in the Werewolf Game with Iterative
  Latent Space Policy Optimization; Large language model (LLM)-based agents have recently shown impressive
progress in a variety of domains, including open-ended conversation and
multi-step decision-making. However, applying these agents to social deduction
games such as Werewolf, which requires both strategic decision-making and
free-form language interaction, remains non-trivial. Traditional methods based
on Counterfactual Regret Minimization (CFR) or reinforcement learning (RL)
typically depend on a predefined action space, making them unsuitable for
language games with unconstrained text action space. Meanwhile, pure LLM-based
agents often suffer from intrinsic biases and require prohibitively large
datasets for fine-tuning. We propose Latent Space Policy Optimization (LSPO),
an iterative framework that addresses these challenges by first mapping
free-form text to a discrete latent space, where methods like CFR and RL can
learn strategic policy more effectively. We then translate the learned policy
back into natural language dialogues, which are used to fine-tune an LLM via
Direct Preference Optimization (DPO). By iteratively alternating between these
stages, our LSPO agent progressively enhances both strategic reasoning and
language communication. Experiment results on the Werewolf game show that our
method improves the agent's performance in each iteration and outperforms
existing Werewolf agents, underscoring its promise for free-form language
decision-making.; 22) Tensor product decomposition for rank-one spin groups I : unitary
  principal series representations; We provide an explicit direct integral decomposition for the tensor product
representation $\pi_1\widehat{\otimes}\pi_2$ of the rank one spin group
$\mathrm{Spin}(n,1)$ whenever $\pi_1$ is a unitary principal series
representation and $\pi_2$ is an arbitrary irreducible unitary representation
of $\mathrm{Spin}(n,1)$.; 23) VERUS-LM: a Versatile Framework for Combining LLMs with Symbolic
  Reasoning; A recent approach to neurosymbolic reasoning is to explicitly combine the
strengths of large language models (LLMs) and symbolic solvers to tackle
complex reasoning tasks. However, current approaches face significant
limitations, including poor generalizability due to task-specific prompts,
inefficiencies caused by the lack of separation between knowledge and queries,
and restricted inferential capabilities. These shortcomings hinder their
scalability and applicability across diverse domains. In this paper, we
introduce VERUS-LM, a novel framework designed to address these challenges.
VERUS-LM employs a generic prompting mechanism, clearly separates domain
knowledge from queries, and supports a wide range of different logical
reasoning tasks. This framework enhances adaptability, reduces computational
cost, and allows for richer forms of reasoning, such as optimization and
constraint satisfaction. We show that our approach succeeds in diverse
reasoning on a novel dataset, markedly outperforming LLMs. Additionally, our
system achieves competitive results on common reasoning benchmarks when
compared to other state-of-the-art approaches, and significantly surpasses them
on the difficult AR-LSAT dataset. By pushing the boundaries of hybrid
reasoning, VERUS-LM represents a significant step towards more versatile
neurosymbolic AI systems; 24) Multi-Agent Collaboration Mechanisms: A Survey of LLMs; With recent advances in Large Language Models (LLMs), Agentic AI has become
phenomenal in real-world applications, moving toward multiple LLM-based agents
to perceive, learn, reason, and act collaboratively. These LLM-based
Multi-Agent Systems (MASs) enable groups of intelligent agents to coordinate
and solve complex tasks collectively at scale, transitioning from isolated
models to collaboration-centric approaches. This work provides an extensive
survey of the collaborative aspect of MASs and introduces an extensible
framework to guide future research. Our framework characterizes collaboration
mechanisms based on key dimensions: actors (agents involved), types (e.g.,
cooperation, competition, or coopetition), structures (e.g., peer-to-peer,
centralized, or distributed), strategies (e.g., role-based or model-based), and
coordination protocols. Through a review of existing methodologies, our
findings serve as a foundation for demystifying and advancing LLM-based MASs
toward more intelligent and collaborative solutions for complex, real-world use
cases. In addition, various applications of MASs across diverse domains,
including 5G/6G networks, Industry 5.0, question answering, and social and
cultural settings, are also investigated, demonstrating their wider adoption
and broader impacts. Finally, we identify key lessons learned, open challenges,
and potential research directions of MASs towards artificial collective
intelligence.; 25) QuantuneV2: Compiler-Based Local Metric-Driven Mixed Precision
  Quantization for Practical Embedded AI Applications; Mixed-precision quantization methods have been proposed to reduce model size
while minimizing accuracy degradation. However, existing studies require
retraining and do not consider the computational overhead and intermediate
representations (IR) generated during the compilation process, limiting their
application at the compiler level. This computational overhead refers to the
runtime latency caused by frequent quantization and dequantization operations
during inference. Performing these operations at the individual operator level
causes significant runtime delays. To address these issues, we propose
QuantuneV2, a compiler-based mixed-precision quantization method designed for
practical embedded AI applications. QuantuneV2 performs inference only twice,
once before quantization and once after quantization, and operates with a
computational complexity of O(n) that increases linearly with the number of
model parameters. We also made the sensitivity analysis more stable by using
local metrics like weights, activation values, the Signal to Quantization Noise
Ratio, and the Mean Squared Error. We also cut down on computational overhead
by choosing the best IR and using operator fusion. Experimental results show
that QuantuneV2 achieved up to a 10.28 percent improvement in accuracy and a
12.52 percent increase in speed compared to existing methods across five
models: ResNet18v1, ResNet50v1, SqueezeNetv1, VGGNet, and MobileNetv2. This
demonstrates that QuantuneV2 enhances model performance while maintaining
computational efficiency, making it suitable for deployment in embedded AI
environments.; 26) A Driver Advisory System Based on Large Language Model for High-speed
  Train; With the rapid development of China high-speed railway, drivers face
increasingly significant technical challenges during operations, such as fault
handling. Currently, drivers depend on the onboard mechanic when facing
technical issues, for instance, traction loss or sensor faults. This dependency
can hinder effective operation, even lead to accidents, while waiting for
faults to be addressed. To enhance the accuracy and explainability of actions
during fault handling, an Intelligent Driver Advisory System (IDAS) framework
based on a large language model (LLM) named IDAS-LLM, is introduced. Initially,
domain-fine-tuning of the LLM is performed using a constructed railway
knowledge question-and-answer dataset to improve answer accuracy in
railway-related questions. Subsequently, integration of the Retrieval-augmented
Generation (RAG) architecture is pursued for system design to enhance the
explainability of generated responses. Comparative experiments are conducted
using the constructed railway driving knowledge assessment dataset. Results
indicate that domain-fine-tuned LLMs show an improvement in answer accuracy by
an average of 10%, outperforming some current mainstream LLMs. Additionally,
the inclusion of the RAG framework increases the average recall rate of
question-and-answer sessions by about 4%. Finally, the fault handling
capability of IDAS-LLM is demonstrated through simulations of real operational
scenarios, proving that the proposed framework has practical application
prospects.; 27) Estimating Probabilities of Causation with Machine Learning Models; Probabilities of causation play a crucial role in modern decision-making.
This paper addresses the challenge of predicting probabilities of causation for
subpopulations with insufficient data using machine learning models. Tian and
Pearl first defined and derived tight bounds for three fundamental
probabilities of causation: the probability of necessity and sufficiency (PNS),
the probability of sufficiency (PS), and the probability of necessity (PN).
However, estimating these probabilities requires both experimental and
observational distributions specific to each subpopulation, which are often
unavailable or impractical to obtain with limited population-level data. We
assume that the probabilities of causation for each subpopulation are
determined by its characteristics. To estimate these probabilities for
subpopulations with insufficient data, we propose using machine learning models
that draw insights from subpopulations with sufficient data. Our evaluation of
multiple machine learning models indicates that, given sufficient
population-level data and an appropriate choice of machine learning model and
activation function, PNS can be effectively predicted. Through simulation
studies, we show that our multilayer perceptron (MLP) model with the Mish
activation function achieves a mean absolute error (MAE) of approximately 0.02
in predicting PNS for 32,768 subpopulations using data from around 2,000
subpopulations.; 28) Direct Flow Simulations with Implicit Neural Representation of Complex
  Geometry; Implicit neural representations have emerged as a powerful approach for
encoding complex geometries as continuous functions. These implicit models are
widely used in computer vision and 3D content creation, but their integration
into scientific computing workflows, such as finite element or finite volume
simulations, remains limited. One reason is that conventional simulation
pipelines require explicit geometric inputs (meshes), forcing INR-based shapes
to be converted to meshes--a step that introduces approximation errors,
computational overhead, and significant manual effort. Immersed boundary
methods partially alleviate this issue by allowing simulations on background
grids without body-fitted meshes. However, they still require an explicit
boundary description and can suffer from numerical artifacts, such as sliver
cut cells. The shifted boundary method (SBM) eliminates the need for explicit
geometry by using grid-aligned surrogate boundaries, making it inherently
compatible with implicit shape representations. Here, we present a framework
that directly couples neural implicit geometries with SBM to perform
high-fidelity fluid flow simulations without any intermediate mesh generation.
By leveraging neural network inference, our approach computes the surrogate
boundary and distance vectors required by SBM on-the-fly directly from the INR,
thus completely bypassing traditional geometry processing. We demonstrate this
approach on canonical 2D and 3D flow benchmarks (lid-driven cavity flows) and
complex geometries (gyroids, the Stanford bunny, and AI-generated shapes),
achieving simulation accuracy comparable to conventional mesh-based methods.
This work highlights a novel pathway for integrating AI-driven geometric
representations into computational physics, establishing INRs as a versatile
and scalable tool for simulations and removing a long-standing bottleneck in
geometry handling.; 29) Learning Priors of Human Motion With Vision Transformers; A clear understanding of where humans move in a scenario, their usual paths
and speeds, and where they stop, is very important for different applications,
such as mobility studies in urban areas or robot navigation tasks within
human-populated environments. We propose in this article, a neural architecture
based on Vision Transformers (ViTs) to provide this information. This solution
can arguably capture spatial correlations more effectively than Convolutional
Neural Networks (CNNs). In the paper, we describe the methodology and proposed
neural architecture and show the experiments' results with a standard dataset.
We show that the proposed ViT architecture improves the metrics compared to a
method based on a CNN.; 30) Atomic Proximal Policy Optimization for Electric Robo-Taxi Dispatch and
  Charger Allocation; Pioneering companies such as Waymo have deployed robo-taxi services in
several U.S. cities. These robo-taxis are electric vehicles, and their
operations require the joint optimization of ride matching, vehicle
repositioning, and charging scheduling in a stochastic environment. We model
the operations of the ride-hailing system with robo-taxis as a discrete-time,
average reward Markov Decision Process with infinite horizon. As the fleet size
grows, the dispatching is challenging as the set of system state and the fleet
dispatching action set grow exponentially with the number of vehicles. To
address this, we introduce a scalable deep reinforcement learning algorithm,
called Atomic Proximal Policy Optimization (Atomic-PPO), that reduces the
action space using atomic action decomposition. We evaluate our algorithm using
real-world NYC for-hire vehicle data and we measure the performance using the
long-run average reward achieved by the dispatching policy relative to a
fluid-based reward upper bound. Our experiments demonstrate the superior
performance of our Atomic-PPO compared to benchmarks. Furthermore, we conduct
extensive numerical experiments to analyze the efficient allocation of charging
facilities and assess the impact of vehicle range and charger speed on fleet
performance.; 31) Defense Against Model Stealing Based on Account-Aware Distribution
  Discrepancy; Malicious users attempt to replicate commercial models functionally at low
cost by training a clone model with query responses. It is challenging to
timely prevent such model-stealing attacks to achieve strong protection and
maintain utility. In this paper, we propose a novel non-parametric detector
called Account-aware Distribution Discrepancy (ADD) to recognize queries from
malicious users by leveraging account-wise local dependency. We formulate each
class as a Multivariate Normal distribution (MVN) in the feature space and
measure the malicious score as the sum of weighted class-wise distribution
discrepancy. The ADD detector is combined with random-based prediction
poisoning to yield a plug-and-play defense module named D-ADD for image
classification models. Results of extensive experimental studies show that
D-ADD achieves strong defense against different types of attacks with little
interference in serving benign users for both soft and hard-label settings.; 32) MAGELLAN: Metacognitive predictions of learning progress guide autotelic
  LLM agents in large goal spaces; Open-ended learning agents must efficiently prioritize goals in vast
possibility spaces, focusing on those that maximize learning progress (LP).
When such autotelic exploration is achieved by LLM agents trained with online
RL in high-dimensional and evolving goal spaces, a key challenge for LP
prediction is modeling one's own competence, a form of metacognitive
monitoring. Traditional approaches either require extensive sampling or rely on
brittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive
framework that lets LLM agents learn to predict their competence and LP online.
By capturing semantic relationships between goals, MAGELLAN enables
sample-efficient LP estimation and dynamic adaptation to evolving goal spaces
through generalization. In an interactive learning environment, we show that
MAGELLAN improves LP prediction efficiency and goal prioritization, being the
only method allowing the agent to fully master a large and evolving goal space.
These results demonstrate how augmenting LLM agents with a metacognitive
ability for LP predictions can effectively scale curriculum learning to
open-ended goal spaces.; 33) General Constraints on Isocurvature from the CMB and Ly-$\alpha$ Forest; Current cosmological data are well-described by the Lambda-Cold Dark Matter
($\Lambda$CDM) model, which assumes adiabatic initial conditions for the
primordial density perturbations. This agreement between data and theory
enables strong constraints on new physics that generates isocurvature
perturbations. Existing constraints typically assume a simple power law form
for the isocurvature power spectrum. However, many new physics scenarios --
such as cosmological phase transitions and gravitational particle production --
can deviate from this assumption. To derive general constraints which apply to
a wide variety of new physics scenarios, we consider four types of isocurvature
modes (dark matter, baryon, dark radiation and neutrino density isocurvature)
and parametrize the isocurvature power spectrum using two general forms: a
delta function and a broken power law. Using data from the cosmic microwave
background (CMB), baryon acoustic oscillations, the Lyman-$\alpha$ forest, and
CMB spectral distortions, we place constraints on the isocurvature power
spectrum across a wide range of scales, from $10^{-4}\,\textrm{Mpc}^{-1}$ to
$10^{4}\,\textrm{Mpc}^{-1}$.; 34) Minimal Spacing of Eigenvalues on Fractals; On the unit interval (I), and the Sierpinski Gasket ($\mathcal{SG}$), the
spectral decimation function of the Laplacian has similar properties that
result in positive minimum spacing of eigenvalues. Other fractals, for example
the level-3 Sierpinski Gasket, $\mathcal{SG}_3$, may not necessarily enjoy
these properties. Our goal is to obtain an easy and sufficient criterion for
positive infimum spacing of eigenvalues in the spectrum based on the properties
of the spectral decimation function for the appropriate fractal. We also give a
sufficient condition for zero infimum spacing of eigenvalues.; 35) Bayesian Selection for Efficient MLIP Dataset Selection; The problem of constructing a dataset for MLIP development which gives the
maximum quality in the minimum amount of compute time is complex, and can be
approached in a number of ways. We introduce a ``Bayesian selection"" approach
for selecting from a candidate set of structures, and compare the effectiveness
of this method against other common approaches in the task of constructing
ideal datasets targeting Silicon surface energies. We show that the Bayesian
selection method performs much better than Simple Random Sampling at this task
(for example, the error on the (100) surface energy is 4.3x lower in the low
data regime), and is competitive with a variety of existing selection methods,
using ACE and MACE features.; 36) Unlocking thermodynamic multitasking: Exploring the functioning of
  two-qubit engines through coherence and entanglement; Recent studies have investigated the role of entanglement in the operation of
a two-qubit system as a heat engine, showing that work can be extracted from a
single heat bath without direct heat dissipation between the two-qubit system
and the cold bath (2021 Phys. Rev. Lett, 126, 120605). In this work, we explore
the impact of operating the same two-qubit system model with two heat baths and
direct dissipation to the environment by applying both a local and a global
Markovian master equation. The addition of a second heat bath enables the
system to operate in different modes depending on the initial quantum state. We
examine the temporal behavior of concurrence entanglement and quantum
coherence, analyzing their observable roles in transitions between various
operational regimes. Additionally, we investigate the evolution of information
flow throughout the working cycle of the two-qubit system, focusing on the
influence of individual and collective decoherence on the system's efficiency
and operational modes. We identify the optimal parameter regions for the engine
and refrigerator modes to achieve maximum performance. Finally, we investigate
the effect of coherence outside the system on its thermodynamic quantities.; 37) Aristotle's Original Idea: For and Against Logic in the era of AI; Aristotle is generally accepted as the father of logic. The ideas that he
raised in his study of logical reasoning carried the development of science
over the centuries. Today, in the era of AI, this title of the fatherhood of
logic has a renewed significance. Behind it lies his original idea that human
reasoning could be studied as a process and that perhaps there exist universal
systems of reasoning that underly all human reasoning irrespective of the
content of what we are reasoning about. In this article, we look into
Aristotle's work on human thought, his work on reasoning itself but also on how
it relates to science and human endeavor more generally, from a modern
perspective of Artificial Intelligence and ask if this can help enlighten our
understanding of AI and Science more generally.; 38) Privacy Attacks on Image AutoRegressive Models; Image autoregressive (IAR) models have surpassed diffusion models (DMs) in
both image quality (FID: 1.48 vs. 1.58) and generation speed. However, their
privacy risks remain largely unexplored. To address this, we conduct a
comprehensive privacy analysis comparing IARs to DMs. We develop a novel
membership inference attack (MIA) that achieves a significantly higher success
rate in detecting training images (TPR@FPR=1%: 86.38% for IARs vs. 4.91% for
DMs). Using this MIA, we perform dataset inference (DI) and find that IARs
require as few as six samples to detect dataset membership, compared to 200 for
DMs, indicating higher information leakage. Additionally, we extract hundreds
of training images from an IAR (e.g., 698 from VAR-d30). Our findings highlight
a fundamental privacy-utility trade-off: while IARs excel in generation quality
and speed, they are significantly more vulnerable to privacy attacks. This
suggests that incorporating techniques from DMs, such as per-token probability
modeling using diffusion, could help mitigate IARs' privacy risks. Our code is
available at https://github.com/sprintml/privacy_attacks_against_iars.; 39) Counting and Reasoning with Plans; Classical planning asks for a sequence of operators reaching a given goal.
While the most common case is to compute a plan, many scenarios require more
than that. However, quantitative reasoning on the plan space remains mostly
unexplored. A fundamental problem is to count plans, which relates to the
conditional probability on the plan space. Indeed, qualitative and quantitative
approaches are well-established in various other areas of automated reasoning.
We present the first study to quantitative and qualitative reasoning on the
plan space. In particular, we focus on polynomially bounded plans. On the
theoretical side, we study its complexity, which gives rise to rich reasoning
modes. Since counting is hard in general, we introduce the easier notion of
facets, which enables understanding the significance of operators. On the
practical side, we implement quantitative reasoning for planning. Thereby, we
transform a planning task into a propositional formula and use knowledge
compilation to count different plans. This framework scales well to large plan
spaces, while enabling rich reasoning capabilities such as learning pruning
functions and explainable planning.; 40) Learning with Expert Abstractions for Efficient Multi-Task Continuous
  Control; Decision-making in complex, continuous multi-task environments is often
hindered by the difficulty of obtaining accurate models for planning and the
inefficiency of learning purely from trial and error. While precise environment
dynamics may be hard to specify, human experts can often provide high-fidelity
abstractions that capture the essential high-level structure of a task and user
preferences in the target environment. Existing hierarchical approaches often
target discrete settings and do not generalize across tasks. We propose a
hierarchical reinforcement learning approach that addresses these limitations
by dynamically planning over the expert-specified abstraction to generate
subgoals to learn a goal-conditioned policy. To overcome the challenges of
learning under sparse rewards, we shape the reward based on the optimal state
value in the abstract model. This structured decision-making process enhances
sample efficiency and facilitates zero-shot generalization. Our empirical
evaluation on a suite of procedurally generated continuous control environments
demonstrates that our approach outperforms existing hierarchical reinforcement
learning methods in terms of sample efficiency, task completion rate,
scalability to complex tasks, and generalization to novel scenarios.; 41) Application of Artificial Intelligence (AI) in Civil Engineering; Hard computing generally deals with precise data, which provides ideal
solutions to problems. However, in the civil engineering field, amongst other
disciplines, that is not always the case as real-world systems are continuously
changing. Here lies the need to explore soft computing methods and artificial
intelligence to solve civil engineering shortcomings. The integration of
advanced computational models, including Artificial Neural Networks (ANNs),
Fuzzy Logic, Genetic Algorithms (GAs), and Probabilistic Reasoning, has
revolutionized the domain of civil engineering. These models have significantly
advanced diverse sub-fields by offering innovative solutions and improved
analysis capabilities. Sub-fields such as: slope stability analysis, bearing
capacity, water quality and treatment, transportation systems, air quality,
structural materials, etc. ANNs predict non-linearities and provide accurate
estimates. Fuzzy logic uses an efficient decision-making process to provide a
more precise assessment of systems. Lastly, while GAs optimizes models (based
on evolutionary processes) for better outcomes, probabilistic reasoning lowers
their statistical uncertainties.; 42) SycEval: Evaluating LLM Sycophancy; Large language models (LLMs) are increasingly applied in educational,
clinical, and professional settings, but their tendency for sycophancy --
prioritizing user agreement over independent reasoning -- poses risks to
reliability. This study introduces a framework to evaluate sycophantic behavior
in ChatGPT-4o, Claude-Sonnet, and Gemini-1.5-Pro across AMPS (mathematics) and
MedQuad (medical advice) datasets. Sycophantic behavior was observed in 58.19%
of cases, with Gemini exhibiting the highest rate (62.47%) and ChatGPT the
lowest (56.71%). Progressive sycophancy, leading to correct answers, occurred
in 43.52% of cases, while regressive sycophancy, leading to incorrect answers,
was observed in 14.66%. Preemptive rebuttals demonstrated significantly higher
sycophancy rates than in-context rebuttals (61.75% vs. 56.52%, $Z=5.87$,
$p<0.001$), particularly in computational tasks, where regressive sycophancy
increased significantly (preemptive: 8.13%, in-context: 3.54%, $p<0.001$).
Simple rebuttals maximized progressive sycophancy ($Z=6.59$, $p<0.001$), while
citation-based rebuttals exhibited the highest regressive rates ($Z=6.59$,
$p<0.001$). Sycophantic behavior showed high persistence (78.5%, 95% CI:
[77.2%, 79.8%]) regardless of context or model. These findings emphasize the
risks and opportunities of deploying LLMs in structured and dynamic domains,
offering insights into prompt programming and model optimization for safer AI
applications.; 43) Investigating and Extending Homans' Social Exchange Theory with Large
  Language Model based Agents; Homans' Social Exchange Theory (SET) is widely recognized as a basic
framework for understanding the formation and emergence of human civilizations
and social structures. In social science, this theory is typically studied
based on simple simulation experiments or real-world human studies, both of
which either lack realism or are too expensive to control. In artificial
intelligence, recent advances in large language models (LLMs) have shown
promising capabilities in simulating human behaviors. Inspired by these
insights, we adopt an interdisciplinary research perspective and propose using
LLM-based agents to study Homans' SET. Specifically, we construct a virtual
society composed of three LLM agents and have them engage in a social exchange
game to observe their behaviors. Through extensive experiments, we found that
Homans' SET is well validated in our agent society, demonstrating the
consistency between the agent and human behaviors. Building on this foundation,
we intentionally alter the settings of the agent society to extend the
traditional Homans' SET, making it more comprehensive and detailed. To the best
of our knowledge, this paper marks the first step in studying Homans' SET with
LLM-based agents. More importantly, it introduces a novel and feasible research
paradigm that bridges the fields of social science and computer science through
LLM-based agents. Code is available at https://github.com/Paitesanshi/SET.; 44) Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents; While Large Language Models (LLMs) can exhibit impressive proficiency in
isolated, short-term tasks, they often fail to maintain coherent performance
over longer time horizons. In this paper, we present Vending-Bench, a simulated
environment designed to specifically test an LLM-based agent's ability to
manage a straightforward, long-running business scenario: operating a vending
machine. Agents must balance inventories, place orders, set prices, and handle
daily fees - tasks that are each simple but collectively, over long horizons
(>20M tokens per run) stress an LLM's capacity for sustained, coherent
decision-making. Our experiments reveal high variance in performance across
multiple LLMs: Claude 3.5 Sonnet and o3-mini manage the machine well in most
runs and turn a profit, but all models have runs that derail, either through
misinterpreting delivery schedules, forgetting orders, or descending into
tangential ""meltdown"" loops from which they rarely recover. We find no clear
correlation between failures and the point at which the model's context window
becomes full, suggesting that these breakdowns do not stem from memory limits.
Apart from highlighting the high variance in performance over long time
horizons, Vending-Bench also tests models' ability to acquire capital, a
necessity in many hypothetical dangerous AI scenarios. We hope the benchmark
can help in preparing for the advent of stronger AI systems.; 45) Certifying Pareto-Optimality in Multi-Objective Maximum Satisfiability; Due to the wide employment of automated reasoning in the analysis and
construction of correct systems, the results reported by automated reasoning
engines must be trustworthy. For Boolean satisfiability (SAT) solvers - and
more recently SAT-based maximum satisfiability (MaxSAT) solvers -
trustworthiness is obtained by integrating proof logging into solvers, making
solvers capable of emitting machine-verifiable proofs to certify correctness of
the reasoning steps performed. In this work, we enable for the first time proof
logging based on the VeriPB proof format for multi-objective MaxSAT (MO-MaxSAT)
optimization techniques. Although VeriPB does not offer direct support for
multi-objective problems, we detail how preorders in VeriPB can be used to
provide certificates for MO-MaxSAT algorithms computing a representative
solution for each element in the non-dominated set of the search space under
Pareto-optimality, without extending the VeriPB format or the proof checker. By
implementing VeriPB proof logging into a state-of-the-art multi-objective
MaxSAT solver, we show empirically that proof logging can be made scalable for
MO-MaxSAT with reasonable overhead.; 46) Evolution of black hole echo modes and the causality dilemma; It has been shown that black hole quasinormal modes are subject to spectral
instability, typically triggered by metric perturbations. These perturbations,
which can introduce a minor bump in the effective potential of the wave
equation, give rise to a novel branch of asymptotic quasinormal modes, dubbed
the {\it echo modes}, which lie mainly parallel to the real frequency axis.
This study explores the evolution of the echo modes and their interplay with
the outward spiral motion observed in low-lying quasinormal modes. As the bump
in the effective potential moves away from the central black hole, the echo
modes collectively shift toward the real axis, with the spacing between
successive modes decreasing uniformly. This collective motion occurs
simultaneously with the spiral of the low-lying modes until the echo modes
eventually take over the fundamental quasinormal mode. In the time domain, such
a takeover coincides with a transition point for the temporal waveform, where
the distinction between the original black hole's ringdown and the echoes
becomes clear. This marks a transition in the characteristics of the waveform
from primarily damped oscillations, dominated by the damping rate of the
fundamental mode, to echo waves, characterized by periodic echo pulses. We
argue that this phenomenon is universal by employing analytical and numerical
analyses. We first elucidate our arguments using explicit but simplified toy
models, where the effective potential barriers are disjoint. The derivations
are then generalized to scenarios where perturbations are introduced on top of
a black hole metric with a continuous effective potential. The observational
implications, particularly the causality dilemma, are elaborated. We show that
the echo modes can be extracted by applying the Fourier transform to ringdown
waveforms, which can be important for gravitational wave observations.; 47) Scalable Framework for Classifying AI-Generated Content Across
  Modalities; The rapid growth of generative AI technologies has heightened the importance
of effectively distinguishing between human and AI-generated content, as well
as classifying outputs from diverse generative models. This paper presents a
scalable framework that integrates perceptual hashing, similarity measurement,
and pseudo-labeling to address these challenges. Our method enables the
incorporation of new generative models without retraining, ensuring
adaptability and robustness in dynamic scenarios. Comprehensive evaluations on
the Defactify4 dataset demonstrate competitive performance in text and image
classification tasks, achieving high accuracy across both distinguishing human
and AI-generated content and classifying among generative methods. These
results highlight the framework's potential for real-world applications as
generative AI continues to evolve. Source codes are publicly available at
https://github.com/ffyyytt/defactify4.; 48) Reformulation is All You Need: Addressing Malicious Text Features in
  DNNs; Human language encompasses a wide range of intricate and diverse implicit
features, which attackers can exploit to launch adversarial or backdoor
attacks, compromising DNN models for NLP tasks. Existing model-oriented
defenses often require substantial computational resources as model size
increases, whereas sample-oriented defenses typically focus on specific attack
vectors or schemes, rendering them vulnerable to adaptive attacks. We observe
that the root cause of both adversarial and backdoor attacks lies in the
encoding process of DNN models, where subtle textual features, negligible for
human comprehension, are erroneously assigned significant weight by less robust
or trojaned models. Based on it we propose a unified and adaptive defense
framework that is effective against both adversarial and backdoor attacks. Our
approach leverages reformulation modules to address potential malicious
features in textual inputs while preserving the original semantic integrity.
Extensive experiments demonstrate that our framework outperforms existing
sample-oriented defense baselines across a diverse range of malicious textual
features.; 49) A study of Kock's fat Delta; Motivated by the study of weak identity structures in higher category theory
we explore the fat Delta category, a modification of the simplex category
introduced by J. Kock. We provide a comprehensive study of fat Delta via the
theory of monads with arities, and use these results to show that fat Delta is
a hypermoment category in the sense of C. Berger. Specifically, by proving that
the free relative semicategory monad is strongly cartesian and identifying a
dense generator, the theory of monads with arities immediately gives rise to
the nerve theorem. We characterise the essential image of the nerve via the
Segal condition, and show that fat Delta possesses an active-inert
factorisation system. Building on these results, we also establish an
isomorphism between two presentations of fat Delta and show that it is a
strongly unital and extensional hypermoment category.; 50) A Survey on Pre-Trained Diffusion Model Distillations; Diffusion Models~(DMs) have emerged as the dominant approach in Generative
Artificial Intelligence (GenAI), owing to their remarkable performance in tasks
such as text-to-image synthesis. However, practical DMs, such as stable
diffusion, are typically trained on massive datasets and thus usually require
large storage. At the same time, many steps may be required, i.e., recursively
evaluating the trained neural network, to generate a high-quality image, which
results in significant computational costs during sample generation. As a
result, distillation methods on pre-trained DM have become widely adopted
practices to develop smaller, more efficient models capable of rapid, few-step
generation in low-resource environment. When these distillation methods are
developed from different perspectives, there is an urgent need for a systematic
survey, particularly from a methodological perspective. In this survey, we
review distillation methods through three aspects: output loss distillation,
trajectory distillation and adversarial distillation. We also discuss current
challenges and outline future research directions in the conclusion.; 51) Monitoring Reasoning Models for Misbehavior and the Risks of Promoting
  Obfuscation; Mitigating reward hacking--where AI systems misbehave due to flaws or
misspecifications in their learning objectives--remains a key challenge in
constructing capable and aligned models. We show that we can monitor a frontier
reasoning model, such as OpenAI o3-mini, for reward hacking in agentic coding
environments by using another LLM that observes the model's chain-of-thought
(CoT) reasoning. CoT monitoring can be far more effective than monitoring agent
actions and outputs alone, and we further found that a LLM weaker than o3-mini,
namely GPT-4o, can effectively monitor a stronger model. Because CoT monitors
can be effective at detecting exploits, it is natural to ask whether those
exploits can be suppressed by incorporating a CoT monitor directly into the
agent's training objective. While we show that integrating CoT monitors into
the reinforcement learning reward can indeed produce more capable and more
aligned agents in the low optimization regime, we find that with too much
optimization, agents learn obfuscated reward hacking, hiding their intent
within the CoT while still exhibiting a significant rate of reward hacking.
Because it is difficult to tell when CoTs have become obfuscated, it may be
necessary to pay a monitorability tax by not applying strong optimization
pressures directly to the chain-of-thought, ensuring that CoTs remain
monitorable and useful for detecting misaligned behavior.; 52) Synthesis with Guided Environments; In the synthesis problem, we are given a specification, and we automatically
generate a system that satisfies the specification in all environments. We
introduce and study {\em synthesis with guided environments} (SGE, for short),
where the system may harness the knowledge and computational power of the
environment during the interaction. The underlying idea in SGE is that in many
settings, in particular when the system serves or directs the environment, it
is of the environment's interest that the specification is satisfied, and it
would follow the guidance of the system. Thus, while the environment is still
hostile, in the sense that the system should satisfy the specification no
matter how the environment assigns values to the input signals, in SGE the
system assigns values to some output signals and guides the environment via
{\em programs\/} how to assign values to other output signals. A key issue is
that these assignments may depend on input signals that are hidden from the
system but are known to the environment, using programs like ``copy the value
of the hidden input signal $x$ to the output signal $y$."" SGE is thus
particularly useful in settings where the system has partial visibility.
  We solve the problem of SGE, show its superiority with respect to traditional
synthesis, and study theoretical aspects of SGE, like the complexity (memory
and domain) of programs used by the system, as well as the connection of SGE to
synthesis of (possibly distributed) systems with partial visibility.; 53) On Benchmarking Human-Like Intelligence in Machines; Recent benchmark studies have claimed that AI has approached or even
surpassed human-level performances on various cognitive tasks. However, this
position paper argues that current AI evaluation paradigms are insufficient for
assessing human-like cognitive capabilities. We identify a set of key
shortcomings: a lack of human-validated labels, inadequate representation of
human response variability and uncertainty, and reliance on simplified and
ecologically-invalid tasks. We support our claims by conducting a human
evaluation study on ten existing AI benchmarks, suggesting significant biases
and flaws in task and label designs. To address these limitations, we propose
five concrete recommendations for developing future benchmarks that will enable
more rigorous and meaningful evaluations of human-like cognitive capacities in
AI with various implications for such AI applications.; 54) Memorization and Regularization in Generative Diffusion Models; Diffusion models have emerged as a powerful framework for generative
modeling. At the heart of the methodology is score matching: learning gradients
of families of log-densities for noisy versions of the data distribution at
different scales. When the loss function adopted in score matching is evaluated
using empirical data, rather than the population loss, the minimizer
corresponds to the score of a time-dependent Gaussian mixture. However, use of
this analytically tractable minimizer leads to data memorization: in both
unconditioned and conditioned settings, the generative model returns the
training samples. This paper contains an analysis of the dynamical mechanism
underlying memorization. The analysis highlights the need for regularization to
avoid reproducing the analytically tractable minimizer; and, in so doing, lays
the foundations for a principled understanding of how to regularize. Numerical
experiments investigate the properties of: (i) Tikhonov regularization; (ii)
regularization designed to promote asymptotic consistency; and (iii)
regularizations induced by under-parameterization of a neural network or by
early stopping when training a neural network. These experiments are evaluated
in the context of memorization, and directions for future development of
regularization are highlighted.; 55) MAPS: Advancing Multi-Modal Reasoning in Expert-Level Physical Science; Pre-trained on extensive text and image corpora, current Multi-Modal Large
Language Models (MLLM) have shown strong capabilities in general visual
reasoning tasks. However, their performance is still lacking in physical
domains that require understanding diagrams with complex physical structures
and quantitative analysis based on multi-modal information. To address this, we
develop a new framework, named Multi-Modal Scientific Reasoning with Physics
Perception and Simulation (MAPS) based on an MLLM. MAPS decomposes expert-level
multi-modal reasoning task into physical diagram understanding via a Physical
Perception Model (PPM) and reasoning with physical knowledge via a simulator.
The PPM module is obtained by fine-tuning a visual language model using
carefully designed synthetic data with paired physical diagrams and
corresponding simulation language descriptions. At the inference stage, MAPS
integrates the simulation language description of the input diagram provided by
PPM and results obtained through a Chain-of-Simulation process with MLLM to
derive the underlying rationale and the final answer. Validated using our
collected college-level circuit analysis problems, MAPS significantly improves
reasoning accuracy of MLLM and outperforms all existing models. The results
confirm MAPS offers a promising direction for enhancing multi-modal scientific
reasoning ability of MLLMs. We will release our code, model and dataset used
for our experiments upon publishing of this paper.; 56) Intention Recognition in Real-Time Interactive Navigation Maps; In this demonstration, we develop IntentRec4Maps, a system to recognise
users' intentions in interactive maps for real-world navigation. IntentRec4Maps
uses the Google Maps Platform as the real-world interactive map, and a very
effective approach for recognising users' intentions in real-time. We showcase
the recognition process of IntentRec4Maps using two different Path-Planners and
a Large Language Model (LLM).
  GitHub: https://github.com/PeijieZ/IntentRec4Maps; 57) Multi-Agent Verification: Scaling Test-Time Compute with Multiple
  Verifiers; By utilizing more computational resources at test-time, large language models
(LLMs) can improve without additional training. One common strategy uses
verifiers to evaluate candidate outputs. In this work, we propose a novel
scaling dimension for test-time compute: scaling the number of verifiers. We
introduce Multi-Agent Verification (MAV) as a test-time compute paradigm that
combines multiple verifiers to improve performance. We propose using Aspect
Verifiers (AVs), off-the-shelf LLMs prompted to verify different aspects of
outputs, as one possible choice for the verifiers in a MAV system. AVs are a
convenient building block for MAV since they can be easily combined without
additional training. Moreover, we introduce BoN-MAV, a simple multi-agent
verification algorithm that combines best-of-n sampling with multiple
verifiers. BoN-MAV demonstrates stronger scaling patterns than self-consistency
and reward model verification, and we demonstrate both weak-to-strong
generalization, where combining weak verifiers improves even stronger LLMs, and
self-improvement, where the same base model is used to both generate and verify
outputs. Our results establish scaling the number of verifiers as a promising
new dimension for improving language model performance at test-time.; 58) Enhancing Multi-Attribute Fairness in Healthcare Predictive Modeling; Artificial intelligence (AI) systems in healthcare have demonstrated
remarkable potential to improve patient outcomes. However, if not designed with
fairness in mind, they also carry the risks of perpetuating or exacerbating
existing health disparities. Although numerous fairness-enhancing techniques
have been proposed, most focus on a single sensitive attribute and neglect the
broader impact that optimizing fairness for one attribute may have on the
fairness of other sensitive attributes. In this work, we introduce a novel
approach to multi-attribute fairness optimization in healthcare AI, tackling
fairness concerns across multiple demographic attributes concurrently. Our
method follows a two-phase approach: initially optimizing for predictive
performance, followed by fine-tuning to achieve fairness across multiple
sensitive attributes. We develop our proposed method using two strategies,
sequential and simultaneous. Our results show a significant reduction in
Equalized Odds Disparity (EOD) for multiple attributes, while maintaining high
predictive accuracy. Notably, we demonstrate that single-attribute fairness
methods can inadvertently increase disparities in non-targeted attributes
whereas simultaneous multi-attribute optimization achieves more balanced
fairness improvements across all attributes. These findings highlight the
importance of comprehensive fairness strategies in healthcare AI and offer
promising directions for future research in this critical area.; 59) Can Horndeski Genesis be Nonpathological?; We present a minimal setup within the framework of Horndeski gravity that can
describe a nonpathological Genesis scenario. Our setup allows for a fully
stable transition to the kination epoch, during which General Relativity (GR)
is restored. This Genesis scenario circumvents the no-go theorem at the cost of
encountering the risk of strong coupling in the past. Interestingly, our
scenario admits two different regimes for the background solution for Hubble
parameter at the Genesis stage: power-law behavior and manifestly non-power-law
behavior. We explicitly show that, in both regimes, our model remains within
unitarity bounds. In most cases, the tensor spectrum is blue-tilted. Then, we
adopt a mechanism with a spectator field that allows for a red-tilted scalar
power spectrum. We also suggest a deformation of the model that enables us to
achieve sufficiently small values for the r ratio. Finally, we discuss the
geodesic (in)completeness of the current model.; 60) Limits at infinity for functions in fractional Sobolev spaces; We establish optimal results on limits at infinity for functions in
fractional Sobolev spaces.; 61) Controlled Model Debiasing through Minimal and Interpretable Updates; Traditional approaches to learning fair machine learning models often require
rebuilding models from scratch, generally without accounting for potentially
existing previous models. In a context where models need to be retrained
frequently, this can lead to inconsistent model updates, as well as redundant
and costly validation testing. To address this limitation, we introduce the
notion of controlled model debiasing, a novel supervised learning task relying
on two desiderata: that the differences between new fair model and the existing
one should be (i) interpretable and (ii) minimal. After providing theoretical
guarantees to this new problem, we introduce a novel algorithm for algorithmic
fairness, COMMOD, that is both model-agnostic and does not require the
sensitive attribute at test time. In addition, our algorithm is explicitly
designed to enforce minimal and interpretable changes between biased and
debiased predictions -a property that, while highly desirable in high-stakes
applications, is rarely prioritized as an explicit objective in fairness
literature. Our approach combines a concept-based architecture and adversarial
learning and we demonstrate through empirical results that it achieves
comparable performance to state-of-the-art debiasing methods while performing
minimal and interpretable prediction changes.; 62) Bridging Social Psychology and LLM Reasoning: Conflict-Aware Meta-Review
  Generation via Cognitive Alignment; The rapid growth of scholarly submissions has overwhelmed traditional peer
review systems, driving the need for intelligent automation to preserve
scientific rigor. While large language models (LLMs) show promise in automating
manuscript critiques, their ability to synthesize high-stakes meta-reviews,
which require conflict-aware reasoning and consensus derivation, remains
underdeveloped. Existing methods fail to effectively handle conflicting
viewpoints within differing opinions, and often introduce additional cognitive
biases, such as anchoring effects and conformity bias.To overcome these
limitations, we propose the Cognitive Alignment Framework (CAF), a dual-process
architecture that transforms LLMs into adaptive scientific arbitrators. By
operationalizing Kahneman's dual-process theory, CAF introduces a three-step
cognitive pipeline: review initialization, incremental integration, and
cognitive alignment.Empirical validation shows that CAF outperforms existing
LLM-based methods, with sentiment consistency gains reaching up to 19.47\% and
content consistency improving by as much as 12.95\%.; 63) Rethinking IDE Customization for Enhanced HAX: A Hyperdimensional
  Perspective; As Integrated Development Environments (IDEs) increasingly integrate
Artificial Intelligence, Software Engineering faces both benefits like
productivity gains and challenges like mismatched user preferences. We propose
Hyper-Dimensional (HD) vector spaces to model Human-Computer Interaction,
focusing on user actions, stylistic preferences, and project context. These
contributions aim to inspire further research on applying HD computing in IDE
design.; 64) MaTVLM: Hybrid Mamba-Transformer for Efficient Vision-Language Modeling; With the advancement of RNN models with linear complexity, the quadratic
complexity challenge of transformers has the potential to be overcome. Notably,
the emerging Mamba-2 has demonstrated competitive performance, bridging the gap
between RNN models and transformers. However, due to sequential processing and
vanishing gradients, RNN models struggle to capture long-range dependencies,
limiting contextual understanding. This results in slow convergence, high
resource demands, and poor performance on downstream understanding and complex
reasoning tasks. In this work, we present a hybrid model MaTVLM by substituting
a portion of the transformer decoder layers in a pre-trained VLM with Mamba-2
layers. Leveraging the inherent relationship between attention and Mamba-2, we
initialize Mamba-2 with corresponding attention weights to accelerate
convergence. Subsequently, we employ a single-stage distillation process, using
the pre-trained VLM as the teacher model to transfer knowledge to the MaTVLM,
further enhancing convergence speed and performance. Furthermore, we
investigate the impact of differential distillation loss within our training
framework. We evaluate the MaTVLM on multiple benchmarks, demonstrating
competitive performance against the teacher model and existing VLMs while
surpassing both Mamba-based VLMs and models of comparable parameter scales.
Remarkably, the MaTVLM achieves up to 3.6x faster inference than the teacher
model while reducing GPU memory consumption by 27.5%, all without compromising
performance. Code and models are released at http://github.com/hustvl/MaTVLM.; 65) Dynamics of Toxicity in Political Podcasts; Toxicity in digital media poses significant challenges, yet little attention
has been given to its dynamics within the rapidly growing medium of podcasts.
This paper addresses this gap by analyzing political podcast data to study the
emergence and propagation of toxicity, focusing on conversation
chains-structured reply patterns within podcast transcripts. Leveraging
state-of-the-art transcription models and advanced conversational analysis
techniques, we systematically examine toxic discourse in over 30 popular
political podcasts in the United States. Our key contributions include: (1)
creating a comprehensive dataset of transcribed and diarized political
podcasts, identifying thousands of toxic instances using Google's Perspective
API, (2) uncovering concerning trends where a majority of episodes contain at
least one toxic instance, (3) introducing toxic conversation chains and
analyzing their structural and linguistic properties, revealing characteristics
such as longer durations, repetitive patterns, figurative language, and
emotional cues tied to anger and annoyance, (4) identifying demand-related
words like 'want', 'like', and 'know' as precursors to toxicity, and (5)
developing predictive models to anticipate toxicity shifts based on annotated
change points. Our findings provide critical insights into podcast toxicity and
establish a foundation for future research on real-time monitoring and
intervention mechanisms to foster healthier discourse in this influential
medium.; 66) Boosting MCTS with Free Energy Minimization; Active Inference, grounded in the Free Energy Principle, provides a powerful
lens for understanding how agents balance exploration and goal-directed
behavior in uncertain environments. Here, we propose a new planning framework,
that integrates Monte Carlo Tree Search (MCTS) with active inference objectives
to systematically reduce epistemic uncertainty while pursuing extrinsic
rewards. Our key insight is that MCTS already renowned for its search
efficiency can be naturally extended to incorporate free energy minimization by
blending expected rewards with information gain. Concretely, the Cross-Entropy
Method (CEM) is used to optimize action proposals at the root node, while tree
expansions leverage reward modeling alongside intrinsic exploration bonuses.
This synergy allows our planner to maintain coherent estimates of value and
uncertainty throughout planning, without sacrificing computational
tractability. Empirically, we benchmark our planner on a diverse set of
continuous control tasks, where it demonstrates performance gains over both
standalone CEM and MCTS with random rollouts.; 67) Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple
  Interactions; Despite extensive safety alignment efforts, large language models (LLMs)
remain vulnerable to jailbreak attacks that elicit harmful behavior. While
existing studies predominantly focus on attack methods that require technical
expertise, two critical questions remain underexplored: (1) Are jailbroken
responses truly useful in enabling average users to carry out harmful actions?
(2) Do safety vulnerabilities exist in more common, simple human-LLM
interactions? In this paper, we demonstrate that LLM responses most effectively
facilitate harmful actions when they are both actionable and informative--two
attributes easily elicited in multi-step, multilingual interactions. Using this
insight, we propose HarmScore, a jailbreak metric that measures how effectively
an LLM response enables harmful actions, and Speak Easy, a simple multi-step,
multilingual attack framework. Notably, by incorporating Speak Easy into direct
request and jailbreak baselines, we see an average absolute increase of 0.319
in Attack Success Rate and 0.426 in HarmScore in both open-source and
proprietary LLMs across four safety benchmarks. Our work reveals a critical yet
often overlooked vulnerability: Malicious users can easily exploit common
interaction patterns for harmful intentions.; 68) Detection of LLM-Paraphrased Code and Identification of the Responsible
  LLM Using Coding Style Features; Recent progress in large language models (LLMs) for code generation has
raised serious concerns about intellectual property protection. Malicious users
can exploit LLMs to produce paraphrased versions of proprietary code that
closely resemble the original. While the potential for LLM-assisted code
paraphrasing continues to grow, research on detecting it remains limited,
underscoring an urgent need for detection system. We respond to this need by
proposing two tasks. The first task is to detect whether code generated by an
LLM is a paraphrased version of original human-written code. The second task is
to identify which LLM is used to paraphrase the original code. For these tasks,
we construct a dataset LPcode consisting of pairs of human-written code and
LLM-paraphrased code using various LLMs.
  We statistically confirm significant differences in the coding styles of
human-written and LLM-paraphrased code, particularly in terms of naming
consistency, code structure, and readability. Based on these findings, we
develop LPcodedec, a detection method that identifies paraphrase relationships
between human-written and LLM-generated code, and discover which LLM is used
for the paraphrasing. LPcodedec outperforms the best baselines in two tasks,
improving F1 scores by 2.64% and 15.17% while achieving speedups of 1,343x and
213x, respectively. Our code and data are available at
https://github.com/Shinwoo-Park/detecting_llm_paraphrased_code_via_coding_style_features.; 69) ATARS: An Aerial Traffic Atomic Activity Recognition and Temporal
  Segmentation Dataset; Traffic Atomic Activity which describes traffic patterns for topological
intersection dynamics is a crucial topic for the advancement of intelligent
driving systems. However, existing atomic activity datasets are collected from
an egocentric view, which cannot support the scenarios where traffic activities
in an entire intersection are required. Moreover, existing datasets only
provide video-level atomic activity annotations, which require exhausting
efforts to manually trim the videos for recognition and limit their
applications to untrimmed videos. To bridge this gap, we introduce the Aerial
Traffic Atomic Activity Recognition and Segmentation (ATARS) dataset, the first
aerial dataset designed for multi-label atomic activity analysis. We offer
atomic activity labels for each frame, which accurately record the intervals
for traffic activities. Moreover, we propose a novel task, Multi-label Temporal
Atomic Activity Recognition, enabling the study of accurate temporal
localization for atomic activity and easing the burden of manual video trimming
for recognition. We conduct extensive experiments to evaluate existing
state-of-the-art models on both atomic activity recognition and temporal atomic
activity segmentation. The results highlight the unique challenges of our ATARS
dataset, such as recognizing extremely small objects' activities. We further
provide comprehensive discussion analyzing these challenges and offer valuable
insights for future direction to improve recognizing atomic activity in aerial
view. Our source code and dataset are available at
https://github.com/magecliff96/ATARS/; 70) Tight Bounds on the Binomial CDF, and the Minimum of i.i.d Binomials, in
  terms of KL-Divergence; We provide finite sample upper and lower bounds on the Binomial tail
probability which are a direct application of Sanov's theorem. We then use
these to obtain high probability upper and lower bounds on the minimum of
i.i.d. Binomial random variables. Both bounds are finite sample, asymptotically
tight, and expressed in terms of the KL-divergence.; 71) MCTS-KBQA: Monte Carlo Tree Search for Knowledge Base Question Answering; This study explores how to enhance the reasoning capabilities of large
language models (LLMs) in knowledge base question answering (KBQA) by
leveraging Monte Carlo Tree Search (MCTS). Semantic parsing-based KBQA methods
are particularly challenging as these approaches require locating elements from
knowledge bases and generating logical forms, demanding not only extensive
annotated data but also strong reasoning capabilities. Although recent
approaches leveraging LLMs as agents have demonstrated considerable potential,
these studies are inherently constrained by their linear decision-making
processes. To address this limitation, we propose a MCTS-based framework that
enhances LLMs' reasoning capabilities through tree search methodology. We
design a carefully designed step-wise reward mechanism that requires only
direct prompting of open-source instruction LLMs without additional
fine-tuning. Experimental results demonstrate that our approach significantly
outperforms linear decision-making methods, particularly in low-resource
scenarios. Additionally, we contribute new data resources to the KBQA community
by annotating intermediate reasoning processes for existing question-SPARQL
datasets using distant supervision. Experimental results on the extended
dataset demonstrate that our method achieves comparable performance to fully
supervised models while using significantly less training data.; 72) Small Models Struggle to Learn from Strong Reasoners; Large language models (LLMs) excel in complex reasoning tasks, and distilling
their reasoning capabilities into smaller models has shown promise. However, we
uncover an interesting phenomenon, which we term the Small Model Learnability
Gap: small models ($\leq$3B parameters) do not consistently benefit from long
chain-of-thought (CoT) reasoning or distillation from larger models. Instead,
they perform better when fine-tuned on shorter, simpler reasoning chains that
better align with their intrinsic learning capacity. To address this, we
propose Mix Distillation, a simple yet effective strategy that balances
reasoning complexity by combining long and short CoT examples or reasoning from
both larger and smaller models. Our experiments demonstrate that Mix
Distillation significantly improves small model reasoning performance compared
to training on either data alone. These findings highlight the limitations of
direct strong model distillation and underscore the importance of adapting
reasoning complexity for effective reasoning capability transfer.; 73) Alleviating Distribution Shift in Synthetic Data for Machine Translation
  Quality Estimation; Quality Estimation (QE) models evaluate the quality of machine translations
without reference translations, serving as the reward models for the
translation task. Due to the data scarcity, synthetic data generation has
emerged as a promising solution. However, synthetic QE data often suffers from
distribution shift, which can manifest as discrepancies between pseudo and real
translations, or in pseudo labels that do not align with human preferences. To
tackle this issue, we introduce ADSQE, a novel framework for alleviating
distribution shift in synthetic QE data. To reduce the difference between
pseudo and real translations, we employ the constrained beam search algorithm
and enhance translation diversity through the use of distinct generation
models. ADSQE uses references, i.e., translation supervision signals, to guide
both the generation and annotation processes, enhancing the quality of
word-level labels. ADSE further identifies the shortest phrase covering
consecutive error tokens, mimicking human annotation behavior, to assign the
final phrase-level labels. Specially, we underscore that the translation model
can not annotate translations of itself accurately. Extensive experiments
demonstrate that ADSQE outperforms SOTA baselines like COMET in both supervised
and unsupervised settings. Further analysis offers insights into synthetic data
generation that could benefit reward models for other tasks.; 74) Reinforcement Learning Environment with LLM-Controlled Adversary in D&D
  5th Edition Combat; The objective of this study is to design and implement a reinforcement
learning (RL) environment using D\&D 5E combat scenarios to challenge smaller
RL agents through interaction with a robust adversarial agent controlled by
advanced Large Language Models (LLMs) like GPT-4o and LLaMA 3 8B. This research
employs Deep Q-Networks (DQN) for the smaller agents, creating a testbed for
strategic AI development that also serves as an educational tool by simulating
dynamic and unpredictable combat scenarios. We successfully integrated
sophisticated language models into the RL framework, enhancing strategic
decision-making processes. Our results indicate that while RL agents generally
outperform LLM-controlled adversaries in standard metrics, the strategic depth
provided by LLMs significantly enhances the overall AI capabilities in this
complex, rule-based setting. The novelty of our approach and its implications
for mastering intricate environments and developing adaptive strategies are
discussed, alongside potential innovations in AI-driven interactive
simulations. This paper aims to demonstrate how integrating LLMs can create
more robust and adaptable AI systems, providing valuable insights for further
research and educational applications.; 75) Discovery of unconventional charge-spin-intertwined density wave in
  magnetic kagome metal GdTi3Bi4; The symmetry breaking and its interplay among spin, charge, and lattice
degrees of freedom is crucial for understanding correlated quantum states such
as charge density waves (CDWs) and unconventional superconductivity. Here, we
report the discovery by low-temperature scanning tunneling
microscopy/spectroscopy of unconventional charge-spin-intertwined density waves
in magnetic kagome metal GdTi3Bi4, which exhibits the one-third magnetization
plateau. We reveal the emergence of 3Q CDWs incommensurate with the crystalline
lattice in both periodicity and orientation, breaking all mirror and rotation
symmetries. The CDW exhibits incommensurate-commensurate transitions in an
applied magnetic field and transitions between 3Q and 1Q CDWs as a function of
field and temperature, accompanied by changes in the spatial symmetries.
Remarkably, the quantum and classic melting of the CDWs exhibits a phase
structure which is consistent with the magnetization phase diagram of bulk
GdTi3Bi4, providing strong evidence for the intertwined charge-spin density
wave order. The origin of the charge-spin intertwinement is further evidenced
by the observed hybridization between itinerant electrons and Gd local moments.
Our findings uncover an unconventional form of charge-spin orders and offer new
insights into a broad class of multi-components density wave formation in
kagome and other correlated quantum materials.; 76) Market Making with Fads, Informed, and Uninformed Traders; We characterise the solutions to a continuous-time optimal liquidity
provision problem in a market populated by informed and uninformed traders. In
our model, the asset price exhibits fads -- these are short-term deviations
from the fundamental value of the asset. Conditional on the value of the fad,
we model how informed traders and uninformed traders arrive in the market. The
market maker knows of the two groups of traders but only observes the anonymous
order arrivals. We study both, the complete information and the partial
information versions of the control problem faced by the market maker. In such
frameworks, we characterise the value of information, and we find the price of
liquidity as a function of the proportion of informed traders in the market.
Lastly, for the partial information setup, we explore how to go beyond the
Kalman-Bucy filter to extract information about the fad from the market
arrivals.; 77) Geomorphodynamics, evolution, and ecology of vertical roots; The roots of some coastal and wetland trees grow peculiar vertical
protrusions, the function of which remains unclear. Here, using computational
simulations based on first-principles fluid and sedimentation dynamics, we
argue that the protrusions work together to create an elevated patch of
sediment downstream of the tree, thereby creating its own fertile
flood-protected breeding grounds for the seedlings. In our simulations, we vary
the vertical root diameter, root spacing and total root area and show that
there is an optimal vertical root spacing that depends on root thickness. Next,
we quantify and discuss the cooperative effects between adjacent vertical root
patches. Lastly, by varying vertical root spacing of a patch of trees, we
estimate a maximal vegetation density for which vertical root production has a
beneficial geomorphological response. Our hypothesis suggests that vertical
roots, such as the ""knee roots"" of baldcypress trees, have an important role in
shaping riparian geomorphology and community structure.; 78) Finest positroid subdivisions from maximal weakly separated collections; We study cell decomposition of positive tropical Grassmannian $\rm
Trop^+Gr_{k,n}$ following an approach by Early in \cite{Early2019FromWS}.
Specifically, we deal with positroid subdivision of hypersimplex induced by
translated blades from any maximal weakly separated collection. One of our main
results gives a necessary and sufficient condition on a maximal weakly
separated collection to form a positroid subdivision of a hypersimplex
corresponding to a simplicial cone in $\rm Trop^+Gr_{k,n}$. For k = 2 our
condition says that any weakly separated collection of two-elements sets gives
such a simplicial cone, and all cones areof such a form. Then our second result
shows that the maximality of any weakly separated collection is preserved under
the boundary map, which affirmatively answers a question by Early in
\cite{Early2019FromWS}. The main tool in proving this theorem is the plabic
graph proposed by Postnikov
\cite{postnikov2006totalpositivitygrassmanniansnetworks}. As a corollary, we
find that all those positroid subdivisions are the finest. Thus, the flip of
two maximal weakly separated collections corresponds to a pair of adjacent
maximal cones in positive tropical Grassmannian.; 79) Redshift leverage for the search of GRB neutrinos affected by quantum
  properties of spacetime; Some previous studies based on IceCube neutrinos had found intriguing
preliminary evidence that some of them might be GRB neutrinos with travel times
affected by quantum properties of spacetime delaying them proportionally to
their energy, an effect often labeled as ""quantum-spacetime-induced in-vacuo
dispersion"". Those previous studies looked for candidate GRB neutrinos in a
fixed (neutrino-energy-independent) time window after the GRB onset and relied
rather crucially on crude estimates of the redshift of GRBs whose redshift has
not been measured. We here introduce a complementary approach to the search of
quantum-spacetime-affected GRB neutrinos which restricts the analysis to GRBs
of sharply known redshift, and, in a way that we argue is synergistic with
having sharp information on redshift, adopts a neutrino-energy-dependent time
window. We find that knowing the redshift of the GRBs strengthens the analysis
enough to compensate for the fact that of course the restriction to GRBs of
known redshift reduces the number of candidate GRB neutrinos. And rather
remarkably our estimate of the magnitude of the in-vacuo-dispersion effects is
fully consistent with what had been found using the previous approach. Our
findings are still inconclusive, since their significance is quantified by a
$p$-value of little less than $0.01$, but provide motivation for monitoring the
accrual of neutrino observations by IceCube and KM3NeT as well as for further
refinements of the strategy of analysis here proposed.; 80) Unifying and Optimizing Data Values for Selection via
  Sequential-Decision-Making; Data selection has emerged as a crucial downstream application of data
valuation. While existing data valuation methods have shown promise in
selection tasks, the theoretical foundations and full potential of using data
values for selection remain largely unexplored. In this work, we first
demonstrate that data values applied for selection can be naturally
reformulated as a sequential-decision-making problem, where the optimal data
value can be derived through dynamic programming. We show this framework
unifies and reinterprets existing methods like Data Shapley through the lens of
approximate dynamic programming, specifically as myopic reward function
approximations to this sequential problem. Furthermore, we analyze how
sequential data selection optimality is affected when the ground-truth utility
function exhibits monotonic submodularity with curvature. To address the
computational challenges in obtaining optimal data values, we propose an
efficient approximation scheme using learned bipartite graphs as surrogate
utility models, ensuring greedy selection is still optimal when the surrogate
utility is correctly specified and learned. Extensive experiments demonstrate
the effectiveness of our approach across diverse datasets.; 81) GKG-LLM: A Unified Framework for Generalized Knowledge Graph
  Construction; The construction of Generalized Knowledge Graph (GKG), including knowledge
graph, event knowledge graph and commonsense knowledge graph, is fundamental
for various natural language processing tasks. Current studies typically
construct these types of graph separately, overlooking holistic insights and
potential unification that could be beneficial in computing resources and usage
perspectives. However, a key challenge in developing a unified framework for
GKG is obstacles arising from task-specific differences. In this study, we
propose a unified framework for constructing generalized knowledge graphs to
address this challenge. First, we collect data from 15 sub-tasks in 29 datasets
across the three types of graphs, categorizing them into in-sample,
counter-task, and out-of-distribution (OOD) data. Then, we propose a
three-stage curriculum learning fine-tuning framework, by iteratively injecting
knowledge from the three types of graphs into the Large Language Models.
Extensive experiments show that our proposed model improves the construction of
all three graph types across in-domain, OOD and counter-task data.; 82) SagaLLM: Context Management, Validation, and Transaction Guarantees for
  Multi-Agent LLM Planning; Recent LLM-based agent frameworks have demonstrated impressive capabilities
in task delegation and workflow orchestration, but face significant challenges
in maintaining context awareness and ensuring planning consistency. This paper
presents SagaLLM, a structured multi-agent framework that addresses four
fundamental limitations in current LLM approaches: inadequate self-validation,
context narrowing, lacking transaction properties, and insufficient inter-agent
coordination. By implementing specialized context management agents and
validation protocols, SagaLLM preserves critical constraints and state
information throughout complex planning processes, enabling robust and
consistent decision-making even during disruptions. We evaluate our approach
using selected problems from the REALM benchmark, focusing on sequential and
reactive planning scenarios that challenge both context retention and adaptive
reasoning. Our experiments with state-of-the-art LLMs, Claude 3.7, DeepSeek R1,
GPT-4o, and GPT-o1, demonstrate that while these models exhibit impressive
reasoning capabilities, they struggle with maintaining global constraint
awareness during complex planning tasks, particularly when adapting to
unexpected changes. In contrast, the distributed cognitive architecture of
SagaLLM shows significant improvements in planning consistency, constraint
enforcement, and adaptation to disruptions in various scenarios.; 83) Confocal Ellipsoidal Reflectors with Phased Array Vivaldi Antenna Source
  for Imaging Systems; In this paper, an on-axis dual-reflector confocal ellipsoidal structure is
presented for near-field imaging systems. In the proposed structure, the
backscattered electromagnetic wave problem, known as the blockage effect, is
reduced considerably using an elaborate design of the sub-reflector and precise
alignment of the reflectors. The proposed geometry is analyzed, followed by a
design example for the stand-off distance of 2 m. The blockage reduction
characteristic is verified using ray-tracing simulation. Next, the scanning
performance of the structure is investigated utilizing a Vivaldi phased array
antenna as the source designed at the central frequency of 28 GHz. The
full-wave simulations proved a field-of-view (FoV) of approximately 40 cm.
Furthermore, tuning the proposed reflectors configuration standoff distance is
examined with a point source. The ray-tracing simulations showed that stand-off
distance can be easily changed up to tens of centimeters with just a few
centimeters of source point lateral displacement.; 84) Behaviour Discovery and Attribution for Explainable Reinforcement
  Learning; Explaining the decisions made by reinforcement learning (RL) agents is
critical for building trust and ensuring reliability in real-world
applications. Traditional approaches to explainability often rely on saliency
analysis, which can be limited in providing actionable insights. Recently,
there has been growing interest in attributing RL decisions to specific
trajectories within a dataset. However, these methods often generalize
explanations to long trajectories, potentially involving multiple distinct
behaviors. Often, providing multiple more fine grained explanations would
improve clarity. In this work, we propose a framework for behavior discovery
and action attribution to behaviors in offline RL trajectories. Our method
identifies meaningful behavioral segments, enabling more precise and granular
explanations associated with high level agent behaviors. This approach is
adaptable across diverse environments with minimal modifications, offering a
scalable and versatile solution for behavior discovery and attribution for
explainable RL.; 85) Planning and Control for Deformable Linear Object Manipulation; Manipulating a deformable linear object (DLO) such as wire, cable, and rope
is a common yet challenging task due to their high degrees of freedom and
complex deformation behaviors, especially in an environment with obstacles.
Existing local control methods are efficient but prone to failure in complex
scenarios, while precise global planners are computationally intensive and
difficult to deploy. This paper presents an efficient, easy-to-deploy framework
for collision-free DLO manipulation using mobile manipulators. We demonstrate
the effectiveness of leveraging standard planning tools for high-dimensional
DLO manipulation without requiring custom planners or extensive data-driven
models. Our approach combines an off-the-shelf global planner with a real-time
local controller. The global planner approximates the DLO as a series of rigid
links connected by spherical joints, enabling rapid path planning without the
need for problem-specific planners or large datasets. The local controller
employs control barrier functions (CBFs) to enforce safety constraints,
maintain the DLO integrity, prevent overstress, and handle obstacle avoidance.
It compensates for modeling inaccuracies by using a state-of-the-art
position-based dynamics technique that approximates physical properties like
Young's and shear moduli. We validate our framework through extensive
simulations and real-world demonstrations. In complex obstacle
scenarios-including tent pole transport, corridor navigation, and tasks
requiring varied stiffness-our method achieves a 100% success rate over
thousands of trials, with significantly reduced planning times compared to
state-of-the-art techniques. Real-world experiments include transportation of a
tent pole and a rope using mobile manipulators. We share our ROS-based
implementation to facilitate adoption in various applications.; 86) Fine-Tuning Whisper for Inclusive Prosodic Stress Analysis; Prosody plays a crucial role in speech perception, influencing both human
understanding and automatic speech recognition (ASR) systems. Despite its
importance, prosodic stress remains under-studied due to the challenge of
efficiently analyzing it. This study explores fine-tuning OpenAI's Whisper
large-v2 ASR model to recognize phrasal, lexical, and contrastive stress in
speech. Using a dataset of 66 native English speakers, including male, female,
neurotypical, and neurodivergent individuals, we assess the model's ability to
generalize stress patterns and classify speakers by neurotype and gender based
on brief speech samples. Our results highlight near-human accuracy in ASR
performance across all three stress types and near-perfect precision in
classifying gender and neurotype. By improving prosody-aware ASR, this work
contributes to equitable and robust transcription technologies for diverse
populations.; 87) Universal Adversarial Attack on Aligned Multimodal LLMs; We propose a universal adversarial attack on multimodal Large Language Models
(LLMs) that leverages a single optimized image to override alignment safeguards
across diverse queries and even multiple models. By backpropagating through the
vision encoder and language head, we craft a synthetic image that forces the
model to respond with a targeted phrase (e.g., ''Sure, here it is'') or
otherwise unsafe content-even for harmful prompts. In experiments on the
SafeBench benchmark, our method achieves significantly higher attack success
rates than existing baselines, including text-only universal prompts (e.g., up
to 93% on certain models). We further demonstrate cross-model transferability
by training on several multimodal LLMs simultaneously and testing on unseen
architectures. Additionally, a multi-answer variant of our approach produces
more natural-sounding (yet still malicious) responses. These findings
underscore critical vulnerabilities in current multimodal alignment and call
for more robust adversarial defenses. We will release code and datasets under
the Apache-2.0 license. Warning: some content generated by Multimodal LLMs in
this paper may be offensive to some readers.; 88) PyEMILI: A New Generation Computer-aided Spectral Line Identifier; Deep high-dispersion spectroscopy of Galactic photoionized gaseous nebulae,
mainly planetary nebulae and HII regions, has revealed numerous emission lines.
As a key step of spectral analysis, identification of emission lines hitherto
has mostly been done manually, which is a tedious task, given that each line
needs to be carefully checked against huge volumes of atomic
transition/spectroscopic database to reach a reliable assignment of identity.
Using Python, we have developed a line-identification code PyEMILI, which is a
significant improvement over the Fortran-based package EMILI introduced ~20
years ago. In our new code PyEMILI, the major shortcomings in EMILI's
line-identification technique have been amended. Moreover, the atomic
transition database utilized by PyEMILI was adopted from Atomic Line List
v3.00b4 but greatly supplemented with theoretical transition data from the
literature. The effective recombination coefficients of the CII, OII, NII and
NeII nebular lines are collected from the literature to form a subset of the
atomic transition database to aid identification of faint optical recombination
lines in the spectra of PNe and HII regions. PyEMILI is tested using the deep,
high-dispersion spectra of two Galactic PNe, Hf2-2 and IC418, and gives better
results of line identification than EMILI does. We also ran PyEMILI on the
optical spectrum of a late-type [WC11] star UVQS J060819.93-715737.4 recently
discovered in the Large Magellanic Cloud, and our results agree well with the
previous manual identifications. The new identifier PyEMILI is applicable to
not only emission-line nebulae but also emission stars, such as Wolf-Rayet
stars.; 89) Strategic Learning with Local Explanations as Feedback; We investigate algorithmic decision problems where agents can respond
strategically to the decision maker's (DM) models. The demand for clear and
actionable explanations from DMs to (potentially strategic) agents continues to
rise. While prior work often treats explanations as full model disclosures,
explanations in practice might convey only partial information, which can lead
to misinterpretations and harmful responses. When full disclosure of the
predictive model is neither feasible nor desirable, a key open question is how
DMs can use explanations to maximise their utility without compromising agent
welfare. In this work, we explore well-known local and global explanation
methods, and establish a necessary condition to prevent explanations from
misleading agents into self-harming actions. Moreover, with conditional
homogeneity, we establish that action recommendation (AR)-based explanations
are sufficient for non-harmful responses, akin to the revelation principle in
information design. To operationalise AR-based explanations, we propose a
simple algorithm to jointly optimise the predictive model and AR policy to
balance DM outcomes with agent welfare. Our empirical results demonstrate the
benefits of this approach as a more refined strategy for safe and effective
partial model disclosure in algorithmic decision-making.; 90) Optimizing Minimum Vertex Cover Solving via a GCN-assisted Heuristic
  Algorithm; The problem of finding a minimum vertex cover (MVC) in a graph is a
well-known NP-hard problem with significant practical applications in
optimization and scheduling. Its complexity, combined with the increasing scale
of problems, underscores the need for efficient and effective algorithms.
However, existing heuristic algorithms for MVC often rely on simplistic
initialization strategies and overlook the impact of edge attributes and
neighborhood information on vertex selection. In this paper, we introduce
GCNIVC, a novel heuristic search algorithm designed to address the limitations
of existing methods for solving MVC problems in large-scale graphs. Our
approach features two main innovations. First, it utilizes a Graph
Convolutional Network (GCN) to capture the global structure of graphs, which
enables the generation of high-quality initial solutions that enhance the
efficiency of the subsequent search process. Second, GCNIVC introduces a new
heuristic that employs three containers and the concept of double-covered edges
(dc-edges), improving search efficiency and providing greater flexibility for
adding and removing operations based on edge attributes. Through extensive
experiments on benchmark datasets, we demonstrate that GCNIVC outperforms
state-of-the-art MVC algorithms in terms of both accuracy and efficiency. Our
results highlight the effectiveness of GCNIVC's GCN-assisted initialization and
its edge-informed search strategy. This study not only advances the
understanding of MVC problem-solving but also contributes a new tool for
addressing large-scale graph optimization challenges.; 91) Eliciting Rational Initial Weights in Gradual Argumentation; Many semantics for weighted argumentation frameworks assume that each
argument is associated with an initial weight. However, eliciting these initial
weights poses challenges: (1) accurately providing a specific numerical value
is often difficult, and (2) individuals frequently confuse initial weights with
acceptability degrees in the presence of other arguments. To address these
issues, we propose an elicitation pipeline that allows one to specify
acceptability degree intervals for each argument. By employing gradual
semantics, we can refine these intervals when they are rational, restore
rationality when they are not, and ultimately identify possible initial weights
for each argument.; 92) XMTC: Explainable Early Classification of Multivariate Time Series in
  Reach-to-Grasp Hand Kinematics; Hand kinematics can be measured in Human-Computer Interaction (HCI) with the
intention to predict the user's intention in a reach-to-grasp action. Using
multiple hand sensors, multivariate time series data are being captured. Given
a number of possible actions on a number of objects, the goal is to classify
the multivariate time series data, where the class shall be predicted as early
as possible. Many machine-learning methods have been developed for such
classification tasks, where different approaches produce favorable solutions on
different data sets. We, therefore, employ an ensemble approach that includes
and weights different approaches. To provide a trustworthy classification
production, we present the XMTC tool that incorporates coordinated
multiple-view visualizations to analyze the predictions. Temporal accuracy
plots, confusion matrix heatmaps, temporal confidence heatmaps, and partial
dependence plots allow for the identification of the best trade-off between
early prediction and prediction quality, the detection and analysis of
challenging classification conditions, and the investigation of the prediction
evolution in an overview and detail manner. We employ XMTC to real-world HCI
data in multiple scenarios and show that good classification predictions can be
achieved early on with our classifier as well as which conditions are easy to
distinguish, which multivariate time series measurements impose challenges, and
which features have most impact.; 93) Locally and Polar Harmonic Maass Forms for Orthogonal Groups of
  Signature $(2, n)$; We generalize the notions of locally and polar harmonic Maass forms to
general orthogonal groups of signature $(2, n)$ with singularities along real
analytic and algebraic cycles. We prove a current equation for locally harmonic
Maass forms and recover the Fourier expansion of the Oda lift involving cycle
integrals. Moreover, using the newly defined polar harmonic Maass forms, we
prove that meromorphic modular forms with singularities along special divisors
are orthogonal to cusp forms with respect to a regularized Petersson inner
product. Using this machinery, we derive a duality theorem involving cycle
integrals of meromorphic modular forms along real analytic cycles and cycle
integrals of locally harmonic Maass forms along algebraic cycles.; 94) Neural Encrypted State Transduction for Ransomware Classification: A
  Novel Approach Using Cryptographic Flow Residuals; Encrypted behavioral patterns provide a unique avenue for classifying complex
digital threats without reliance on explicit feature extraction, enabling
detection frameworks to remain effective even when conventional static and
behavioral methodologies fail. A novel approach based on Neural Encrypted State
Transduction (NEST) is introduced to analyze cryptographic flow residuals and
classify threats through their encrypted state transitions, mitigating evasion
tactics employed through polymorphic and obfuscated attack strategies. The
mathematical formulation of NEST leverages transduction principles to map state
transitions dynamically, enabling high-confidence classification without
requiring direct access to decrypted execution traces. Experimental evaluations
demonstrate that the proposed framework achieves improved detection accuracy
across multiple ransomware families while exhibiting resilience against
adversarial perturbations and previously unseen attack variants. The model
maintains competitive processing efficiency, offering a practical balance
between classification performance and computational resource constraints,
making it suitable for large-scale security deployments. Comparative
assessments reveal that NEST consistently outperforms baseline classification
models, particularly in detecting ransomware samples employing delayed
encryption, entropy-based obfuscation, and memory-resident execution
techniques. The capacity to generalize across diverse execution environments
reinforces the applicability of encrypted transduction methodologies in
adversarial classification tasks beyond conventional malware detection
pipelines. The integration of residual learning mechanisms within the
transduction layers further enhances classification robustness, minimizing both
false positives and misclassification rates across varied operational contexts.; 95) Incidental Anterior Mediastinal Nodular Lesions on Chest CT in Asymptomatic Subjects; Screening for lung cancer: 2023 guideline update from the American Cancer Society; Objective The aim of this study was to investigate the prevalence and characteristics of nodular lesions in the anterior mediastinum that had been found incidentally on screening chest computed tomography (CT) in asymptomatic subjects. Methods We included 56,358 consecutive participants (mean age 52.4 ± 10.5 years; male-female ratio 35,306:21,052) who underwent a baseline low-dose chest CT scan as part of a health checkup from 2006 through 2013. After the presence of anterior mediastinal nodular lesion had been confirmed, their CT findings, confirmatory diagnosis, and interval CT scan were reviewed. The standardized prevalence ratio for thymic epithelial tumor was calculated on the basis of the Republic of Korea cancer statistics for 2014. Results Of the 56,358 participants, 413 (0.73%) had lesions (95% confidence interval: 0.66–0.80%); the prevalence increased with age (p <0.001) and a history of malignancy (p = 0.005). Of the lesions, 85.2% were smaller than 2 cm, 61.3% were round, and 80.2% had CT attenuation higher than 20 Hounsfield units. Among 51 proven cases, 39 lesions (76.9%) were benign and 12 (23.1%) were malignant. The standardized prevalence ratio for thymic epithelial tumor was 2.04 (95% confidence interval: 1.01–3.42). Of 11 resected thymic epithelial tumors, five were carcinomas, 10 were stage I or II, and all were completely resected without recurrence. Of the 237 unconfirmed cases with a follow-up CT scan, 82.2% were stable, 8.9% had increased, and the other 8.9% had decreased. Conclusions The prevalence of incidental nodular lesion was 0.73%. Most lesions had CT features that were indistinguishable from thymic epithelial tumors, but a considerable portion of the lesions were suspected to be benign. Incidental thymic epithelial tumors were more prevalent than clinically detected tumors, were early-stage cancer, and showed favorable outcomes.; Abstract Lung cancer is the leading cause of mortality and person‐years life lost from among US men women. Early detection has been shown to be associated with reduced lung mortality. Our objective was update American Cancer Society (ACS) 2013 screening (LCS) guideline for adults at high risk cancer. The intended provide guidance health care providers their patients who are due a history smoking. ACS Guideline Development Group (GDG) utilized systematic review LCS literature commissioned Preventive Services Task Force 2021 recommendation update; second years since quitting smoking (YSQ); published 2021; two Intervention Surveillance Modeling Network‐validated models assess benefits harms screening; an epidemiologic modeling analysis examining effect YSQ aging on risk; updated benefit‐to‐radiation‐risk ratios follow‐up examinations. GDG also examined disease burden data National Institute’s Surveillance, Epidemiology, End Results program. Formulation recommendations based quality evidence judgment (incorporating values preferences) about balance harms. judged that overall moderate sufficient support strong individuals meet eligibility criteria. in women aged 50–80 reduction deaths across range study designs, inferential supports older than 80 good health. recommends annual low‐dose computed tomography asymptomatic currently smoke or formerly smoked have ≥20 pack‐year ( , ). Before decision made initiate LCS, should engage shared decision‐making discussion qualified professional. For smoked, number not criterion begin stop screening. Individuals receive counseling quit connected cessation resources. comorbid conditions substantially limit expectancy screened. These considered by discussions LCS. If fully implemented, these likelihood significantly reducing death suffering United States.; 96) Where to Go Next Day: Multi-scale Spatial-Temporal Decoupled Model for
  Mid-term Human Mobility Prediction; Predicting individual mobility patterns is crucial across various
applications. While current methods mainly focus on predicting the next
location for personalized services like recommendations, they often fall short
in supporting broader applications such as traffic management and epidemic
control, which require longer period forecasts of human mobility. This study
addresses mid-term mobility prediction, aiming to capture daily travel patterns
and forecast trajectories for the upcoming day or week. We propose a novel
Multi-scale Spatial-Temporal Decoupled Predictor (MSTDP) designed to
efficiently extract spatial and temporal information by decoupling daily
trajectories into distinct location-duration chains. Our approach employs a
hierarchical encoder to model multi-scale temporal patterns, including daily
recurrence and weekly periodicity, and utilizes a transformer-based decoder to
globally attend to predicted information in the location or duration chain.
Additionally, we introduce a spatial heterogeneous graph learner to capture
multi-scale spatial relationships, enhancing semantic-rich representations.
Extensive experiments, including statistical physics analysis, are conducted on
large-scale mobile phone records in five cities (Boston, Los Angeles, SF Bay
Area, Shanghai, and Tokyo), to demonstrate MSTDP's advantages. Applied to
epidemic modeling in Boston, MSTDP significantly outperforms the
best-performing baseline, achieving a remarkable 62.8% reduction in MAE for
cumulative new cases.; 97) Follow-the-Regularized-Leader with Adversarial Constraints; Constrained Online Convex Optimization (COCO) can be seen as a generalization
of the standard Online Convex Optimization (OCO) framework. At each round, a
cost function and constraint function are revealed after a learner chooses an
action. The goal is to minimize both the regret and cumulative constraint
violation (CCV) against an adaptive adversary. We show for the first time that
is possible to obtain the optimal $O(\sqrt{T})$ bound on both regret and CCV,
improving the best known bounds of $O \left( \sqrt{T} \right)$ and $\~{O}
\left( \sqrt{T} \right)$ for the regret and CCV, respectively.; 98) Tighter Value-Function Approximations for POMDPs; Solving partially observable Markov decision processes (POMDPs) typically
requires reasoning about the values of exponentially many state beliefs.
Towards practical performance, state-of-the-art solvers use value bounds to
guide this reasoning. However, sound upper value bounds are often
computationally expensive to compute, and there is a tradeoff between the
tightness of such bounds and their computational cost. This paper introduces
new and provably tighter upper value bounds than the commonly used fast
informed bound. Our empirical evaluation shows that, despite their additional
computational overhead, the new upper bounds accelerate state-of-the-art POMDP
solvers on a wide range of benchmarks.; 99) Contextual bandits with entropy-based human feedback; In recent years, preference-based human feedback mechanisms have become
essential for enhancing model performance across diverse applications,
including conversational AI systems such as ChatGPT. However, existing
approaches often neglect critical aspects, such as model uncertainty and the
variability in feedback quality. To address these challenges, we introduce an
entropy-based human feedback framework for contextual bandits, which
dynamically balances exploration and exploitation by soliciting expert feedback
only when model entropy exceeds a predefined threshold. Our method is
model-agnostic and can be seamlessly integrated with any contextual bandit
agent employing stochastic policies. Through comprehensive experiments, we show
that our approach achieves significant performance improvements while requiring
minimal human feedback, even under conditions of suboptimal feedback quality.
This work not only presents a novel strategy for feedback solicitation but also
highlights the robustness and efficacy of incorporating human guidance into
machine learning systems. Our code is publicly available:
https://github.com/BorealisAI/CBHF; 100) Over-the-Air FEEL with Integrated Sensing: Joint Scheduling and
  Beamforming Design; Employing wireless systems with dual sensing and communications
functionalities is becoming critical in next generation of wireless networks.
In this paper, we propose a robust design for over-the-air federated edge
learning (OTA-FEEL) that leverages sensing capabilities at the parameter server
(PS) to mitigate the impact of target echoes on the analog model aggregation.
We first derive novel expressions for the Cramer-Rao bound of the target
response and mean squared error (MSE) of the estimated global model to measure
radar sensing and model aggregation quality, respectively. Then, we develop a
joint scheduling and beamforming framework that optimizes the OTA-FEEL
performance while keeping the sensing and communication quality, determined
respectively in terms of Cramer-Rao bound and achievable downlink rate, in a
desired range. The resulting scheduling problem reduces to a combinatorial
mixed-integer nonlinear programming problem (MINLP). We develop a
low-complexity hierarchical method based on the matching pursuit algorithm used
widely for sparse recovery in the literature of compressed sensing. The
proposed algorithm uses a step-wise strategy to omit the least effective
devices in each iteration based on a metric that captures both the aggregation
and sensing quality of the system. It further invokes alternating optimization
scheme to iteratively update the downlink beamforming and uplink
post-processing by marginally optimizing them in each iteration. Convergence
and complexity analysis of the proposed algorithm is presented. Numerical
evaluations on MNIST and CIFAR-10 datasets demonstrate the effectiveness of our
proposed algorithm. The results show that by leveraging accurate sensing, the
target echoes on the uplink signal can be effectively suppressed, ensuring the
quality of model aggregation to remain intact despite the interference.",0.0,0.42177340954886444
2412.11084,applied,2412.11084-pos1-1,"BarcodeBERT: Transformers for Biodiversity Analysis; Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https://github.com/Kari-Genomics-Lab/BarcodeBERT",2412.11084-pos2-1,"Biological identifications through DNA barcodes; Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution.",15,"['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']","The first candidate paper, 'Generalized Temporal Tensor Decomposition with Rank-revealing Latent-ODE', aligns best with the main paper 'BarcodeBERT: Transformers for Biodiversity Analysis' as it introduces a novel approach to analyzing complex multi-dimensional data which can complement the machine learning techniques used in biodiversity analysis. The tensor decomposition method can enhance the model's ability to identify species-level information from the diverse datasets referenced in the main paper, making it a novel and useful combination for improving biodiversity studies. Other candidates do not provide the same level of synergy or applicability to the main research focus on biodiversity analysis using machine learning.","1) Generalized Temporal Tensor Decomposition with Rank-revealing Latent-ODE; Tensor decomposition is a fundamental tool for analyzing multi-dimensional
data by learning low-rank factors to represent high-order interactions. While
recent works on temporal tensor decomposition have made significant progress by
incorporating continuous timestamps in latent factors, they still struggle with
general tensor data with continuous indexes not only in the temporal mode but
also in other modes, such as spatial coordinates in climate data. Additionally,
the problem of determining the tensor rank remains largely unexplored in
temporal tensor models. To address these limitations, we propose
\underline{G}eneralized temporal tensor decomposition with
\underline{R}ank-r\underline{E}vealing laten\underline{T}-ODE (GRET).
  Our approach encodes continuous spatial indexes as learnable Fourier features
and employs neural ODEs in latent space to learn the temporal trajectories of
factors. To automatically reveal the rank of temporal tensors, we introduce a
rank-revealing Gaussian-Gamma prior over the factor trajectories. We develop an
efficient variational inference scheme with an analytical evidence lower bound,
enabling sampling-free optimization. Through extensive experiments on both
synthetic and real-world datasets, we demonstrate that GRET not only reveals
the underlying ranks of temporal tensors but also significantly outperforms
existing methods in prediction performance and robustness against noise.; 2) Estimating Time Delays between Signals under Mixed Noise Influence with
  Novel Cross- and Bispectral Methods; A common problem to signal processing are biases introduced by correlated
noise. In Time-Delay Estimation (TDE), which quantifies a time lag between two
signals, noise mixing introduces a bias towards zero delay in conventional TDE
protocols based on the cross- or bispectrum. Here we propose two novel TDE
approaches that address these shortcomings: (1) A cross-spectrum based TDE
protocol that relies on estimating the periodicity of the phase spectrum rather
than its slope, and (2) a bispectrum based TDE analysis, bispectral
antisymmetrization, which removes contributions from not just Gaussian but all
independent sources. In a simulation study, we compare conventional and novel
TDE protocols and resolve differences in performance with respect to noise
Gaussianity and auto-correlation structure. As a proof-of-concept, we also
perform TDE analysis on a neural stimulation dataset (n=3). We find that
antisymmetrization consistently outperforms conventional bispectral TDE methods
at low signal-to-noise ratios (SNR) and removes spurious zero-delay estimates
in all mixed-noise environments. TDE based on phase periodicity also improves
signal sensitivity compared to conventional cross-spectral methods. These
observations are stable with respect to the magnitude of the delay and the
statistical properties of the noise.; 3) Pricing Quanto and Composite Contracts with Local-Correlation Models; Pricing composite and quanto contracts requires a joint model of both the
underlying asset and the exchange rate. In this contribution, we explore the
potential of local-correlation models to address the challenges of calibrating
synthetic quanto forward contracts and composite options quoted in the market.
Specifically, we design on-line calibration procedures for generic local and
stochastic volatility models. The paper concludes with a numerical study
assessing the calibration performance of these methodologies and comparing them
to simpler approximations of the correlation structure.; 4) Rydberg Atomic Quantum Receivers for the Multi-User MIMO Uplink; Rydberg atomic quantum receivers exhibit great potential in assisting
classical wireless communications due to their outstanding advantages in
detecting radio frequency signals. To realize this potential, we integrate a
Rydberg atomic quantum receiver into a classical multi-user multiple-input
multiple-output (MIMO) scheme to form a multi-user Rydberg atomic quantum MIMO
(RAQ-MIMO) system for the uplink. To study this system, we first construct an
equivalent baseband signal model, which facilitates convenient system design,
signal processing and optimizations. We then study the ergodic achievable rates
under both the maximum ratio combining (MRC) and zero-forcing (ZF) schemes by
deriving their tight lower bounds. We next compare the ergodic achievable rates
of the RAQ-MIMO and the conventional massive MIMO schemes by offering a
closed-form expression for the difference of their ergodic achievable rates,
which allows us to directly compare the two systems. Our results show that
RAQ-MIMO allows the average transmit power of users to be $> 25$ dBm lower than
that of the conventional massive MIMO. Viewed from a different perspective, an
extra $\sim 8.8$ bits/s/Hz/user rate becomes achievable by ZF RAQ-MIMO.; 5) Proton Radiation Damage and Annealing of COSI p-type Cross-strip HPGe
  Detectors; In order to understand the effects of a space radiation environment on
cross-strip germanium detectors, we investigated the effects of high-energy
proton damage on a COSI detector and the capabilities of high-temperature
annealing in repairing detector spectral resolution. We irradiated a
COSI-balloon cross-strip high-purity germanium (HPGe) detector with a
high-energy proton fluence corresponding to ~10 years in a space radiation
environment. We repaired the resulting degradation in spectral resolution
within 16% of its preradiation value through a series of high-temperature
anneals. We characterize the repair of charge traps with time spent under
high-temperature anneal to inform an annealing procedure for long-term
maintenance of COSI's spectral resolution.; 6) A tomographic interpretation of structure-property relations for
  materials discovery; Recent advancements in machine learning (ML) for materials have demonstrated
that ""simple"" materials representations (e.g., the chemical formula alone
without structural information) can sometimes achieve competitive property
prediction performance in common-tasks. Our physics-based intuition would
suggest that such representations are ""incomplete"", which indicates a gap in
our understanding. This work proposes a tomographic interpretation of
structure-property relations of materials to bridge that gap by defining what
is a material representation, material properties, the material and the
relationships between these three concepts using ideas from information theory.
We verify this framework performing an exhaustive comparison of
property-augmented representations on a range of material's property prediction
objectives, providing insight into how different properties can encode
complementary information.; 7) Preorder induced by rainbow forbidden subgraphs; A subgraph $H$ of an edge-colored graph $G$ is rainbow if all the edges of
$H$ receive different colors. If $G$ does not contain a rainbow subgraph
isomorphic to $H$, we say that $G$ is rainbow $H$-free. For connected graphs
$H_1$ and $H_2$, if every rainbow $H_1$-free edge-colored complete graph
colored in sufficiently many colors is rainbow $H_2$-free, we write $H_1\le
H_2$. The binary relation $\le$ is reflexive and transitive, and hence it is a
preorder. If $H_1$ is a subgraph of $H_2$, then trivially $H_1\le H_2$ holds.
On the other hand, there exists a pair $(H_1, H_2)$ such that $H_1$ is a proper
supergraph of $H_2$ and $H_1\le H_2$ holds. Cui et al.~[Discrete
Math.~\textbf{344} (2021) Article Number 112267] characterized these pairs. In
this paper, we investigate the pairs $(H_1, H_2)$ with $H_1\le H_2$ when
neither $H_1$ nor $H_2$ is a subgraph of the other. We prove that there are
many such pairs and investigate their structure with respect to $\le$.; 8) Characterization of polynomial surfaces of revolution and polynomial
  quadrics; In this paper, we characterize the polynomiality of surfaces of revolution by
means of the polynomiality of an associated plane curve. In addition, if the
surface of revolution is polynomial, we provide formulas for computing a
polynomial parametrization, over $\mathbb{C}$, of the surface. Furthermore, we
perform the first steps towards the analysis of the existence, and actual
computation, of real polynomial parametrizations of surfaces of revolution. As
a consequence, we give a complete picture of the real polynomiality of quadrics
and we formulate a conjecture for the general case.; 9) Causes of evolutionary divergence in prostate cancer; Cancer progression involves the sequential accumulation of genetic
alterations that cumulatively shape the tumour phenotype. In prostate cancer,
tumours can follow divergent evolutionary trajectories that lead to distinct
subtypes, but the causes of this divergence remain unclear. While causal
inference could elucidate the factors involved, conventional methods are
unsuitable due to the possibility of unobserved confounders and ambiguity in
the direction of causality. Here, we propose a method that circumvents these
issues and apply it to genomic data from 829 prostate cancer patients. We
identify several genetic alterations that drive divergence as well as others
that prevent this transition, locking tumours into one trajectory. Further
analysis reveals that these genetic alterations may cause each other, implying
a positive-feedback loop that accelerates divergence. Our findings provide
insights into how cancer subtypes emerge and offer a foundation for genomic
surveillance strategies aimed at monitoring the progression of prostate cancer.; 10) KLAP: KYP lemma based low-rank approximation for $\mathcal{H}_2$-optimal
  passivation; We present a novel passivity enforcement (passivation) method, called KLAP,
for linear time-invariant systems based on the Kalman-Yakubovich-Popov (KYP)
lemma and the closely related Lur'e equations. The passivation problem in our
framework corresponds to finding a perturbation to a given non-passive system
that renders the system passive while minimizing the $\mathcal{H}_2$ or
frequency-weighted $\mathcal{H}_2$ distance between the original non-passive
and the resulting passive system. We show that this problem can be formulated
as an unconstrained optimization problem whose objective function can be
differentiated efficiently even in large-scale settings. We show that any
minimizer of the unconstrained problem yields the same passive system.
Furthermore, we prove that, in the absence of a feedthrough term, every local
minimizer is also a global minimizer. For cases involving a non-trivial
feedthrough term, we analyze global minimizers in relation to the extremal
solutions of the Lur'e equations, which can serve as tools for identifying
local minima. To solve the resulting numerical optimization problem
efficiently, we propose an initialization strategy based on modifying the
feedthrough term and a restart strategy when it is likely that the optimization
has converged to a local minimum. Numerical examples illustrate the
effectiveness of the proposed method.; 11) Force-Velocity Relationship in Branched Actin Networks: Consequences of
  Entanglement, Drag and Stall Force; We investigate the growth of a branched actin network under load. Using a
combination of simulations and theory, we show that the network adapts to the
load and exhibits two regimes: a finite velocity at low stress, followed by a
power-law decay of the velocity as a function of stress. This decay is
explained by a theoretical model relating branched network elasticity to
filament entanglement. The finite maximum velocity is attributed to network
drag, which dictates dynamics at low stress. Additionally, analysis of filament
stall force contribution reveals a transition from a stalled network to a
growing network, when the filament stall force exceeds a critical value
controlled by the applied stress.; 12) Demystifying FPGA Hard NoC Performance; With the advent of modern multi-chiplet FPGA architectures, vendors have
begun integrating hardened NoC to address the scalability, resource usage, and
frequency disadvantages of soft NoCs. However, as this work shows, effectively
harnessing these hardened NoC is not trivial. It requires detailed knowledge of
the microarchitecture and how it relates to the physical design of the FPGA.
Existing literature has provided in-depth analyses for NoC in MPSoC devices,
but few studies have systematically evaluated hardened NoC in FPGA, which have
several unique implications.
  This work aims to bridge this knowledge gap by demystifying the performance
and design trade-offs of hardened NoC on FPGA. Our work performs detailed
performance analysis of hard (and soft) NoC under different settings, including
diverse NoC topologies, routing strategies, traffic patterns and different
external memories under various NoC placements.
  In the context of Versal FPGAs, our results show that using hardened NoC in
multi-SLR designs can reduce expensive cross-SLR link usage by up to 30~40%,
eliminate general-purpose logic overhead, and remove most critical paths caused
by large on-chip crossbars. However, under certain aggressive traffic patterns,
the frequency advantage of hardened NoC is outweighed by the inefficiency in
the network microarchitecture. We also observe suboptimal solutions from the
NoC compiler and distinct performance variations between the vertical and
horizontal interconnects, underscoring the need for careful design. These
findings serve as practical guidelines for effectively integrating hardened NoC
and highlight important trade-offs for future FPGA-based systems.; 13) Enhancing sensor attack detection in supervisory control systems modeled
  by probabilistic automata; Sensor attacks compromise the reliability of cyber-physical systems (CPSs) by
altering sensor outputs with the objective of leading the system to unsafe
system states. This paper studies a probabilistic intrusion detection framework
based on $\lambda$-sensor-attack detectability ($\lambda$-sa), a formal measure
that evaluates the likelihood of a system being under attack based on observed
behaviors. Our framework enhances detection by extending its capabilities to
identify multiple sensor attack strategies using probabilistic information,
which enables the detection of sensor attacks that were undetected by current
detection methodologies. We develop a polynomial-time algorithm that verifies
$\lambda$-sa detectability by constructing a weighted verifier automaton and
solving the shortest path problem. Additionally, we propose a method to
determine the maximum detection confidence level ($\lambda$*) achievable by the
system, ensuring the highest probability of identifying attack-induced
behaviors.; 14) Transcriptome signature for the identification of bevacizumab responders
  in ovarian cancer; The standard of care for ovarian cancer comprises cytoreductive surgery,
followed by adjuvant platinum-based chemotherapy plus taxane therapy and
maintenance therapy with the antiangiogenic compound bevacizumab and/or a PARP
inhibitor. Nevertheless, there is currently no clear clinical indication for
the use of bevacizumab, highlighting the urgent need for biomarkers to assess
the response to bevacizumab. In the present study, based on a novel RNA-seq
dataset (n=181) and a previously published microarray-based dataset (n=377), we
have identified an expression signature potentially associated with benefit
from bevacizumab addition and assumed to reflect cancer stemness acquisition
driven by activation of CTCFL. Patients with this signature demonstrated
improved overall survival when bevacizumab was added to standard chemotherapy
in both novel (HR=0.41(0.23-0.74), adj.p-value=7.70e-03) and previously
published cohorts (HR=0.51(0.34-0.75), adj.p-value=3.25e-03), while no
significant differences in survival explained by treatment were observed in
patients negative for this signature. In addition to the CTCFL signature, we
found several other reproducible expression signatures which may also represent
biomarker candidates not related to established molecular subtypes of ovarian
cancer and require further validation studies based on additional RNA-seq data.; 15) Biological identifications through DNA barcodes; Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution.; 16) A Metasemantic-Metapragmatic Framework for Taxonomizing Multimodal
  Communicative Alignment; Drawing on contemporary pragmatist philosophy and linguistic theories on
cognition, meaning, and communication, this paper presents a dynamic,
metasemantic-metapragmatic taxonomy for grounding and conceptualizing
human-like multimodal communicative alignment. The framework is rooted in
contemporary developments of the three basic communicative capacities initially
identified by American logician and pragmatist philosopher Charles Sanders
Peirce: iconic (sensory and perceptual qualities), indexical (contextual and
sociocultural associations), and rule-like (symbolic and intuitive reasoning).
Expanding on these developments, I introduce the concept of indexical
contextualization and propose the principle of ""contextualization
directionality"" for characterizing the crucial metapragmatic capacity for
maintaining, navigating, or transitioning between semantic and pragmatic modes
of multimodal communication. I contend that current cognitive-social
computational and engineering methodologies disproportionately emphasize the
semantic/metasemantic domain, overlooking the pivotal role of metapragmatic
indexicality in traversing the semantic-pragmatic spectrum of communication.
The framework's broader implications for intentionality, identity, affect, and
ethics in within-modal and cross-modal human-machine alignment are also
discussed.; 17) GiantHunter: Accurate detection of giant virus in metagenomic data using
  reinforcement-learning and Monte Carlo tree search; Motivation: Nucleocytoplasmic large DNA viruses (NCLDVs) are notable for
their large genomes and extensive gene repertoires, which contribute to their
widespread environmental presence and critical roles in processes such as host
metabolic reprogramming and nutrient cycling. Metagenomic sequencing has
emerged as a powerful tool for uncovering novel NCLDVs in environmental
samples. However, identifying NCLDV sequences in metagenomic data remains
challenging due to their high genomic diversity, limited reference genomes, and
shared regions with other microbes. Existing alignment-based and machine
learning methods struggle with achieving optimal trade-offs between sensitivity
and precision. Results: In this work, we present GiantHunter, a reinforcement
learning-based tool for identifying NCLDVs from metagenomic data. By employing
a Monte Carlo tree search strategy, GiantHunter dynamically selects
representative non-NCLDV sequences as the negative training data, enabling the
model to establish a robust decision boundary. Benchmarking on rigorously
designed experiments shows that GiantHunter achieves high precision while
maintaining competitive sensitivity, improving the F1-score by 10% and reducing
computational cost by 90% compared to the second-best method. To demonstrate
its real-world utility, we applied GiantHunter to 60 metagenomic datasets
collected from six cities along the Yangtze River, located both upstream and
downstream of the Three Gorges Dam. The results reveal significant differences
in NCLDV diversity correlated with proximity to the dam, likely influenced by
reduced flow velocity caused by the dam. These findings highlight the potential
of GiantSeeker to advance our understanding of NCLDVs and their ecological
roles in diverse environments.; 18) Anthemius: Efficient & Modular Block Assembly for Concurrent Execution; Many blockchains such as Ethereum execute all incoming transactions
sequentially significantly limiting the potential throughput. A common approach
to scale execution is parallel execution engines that fully utilize modern
multi-core architectures. Parallel execution is then either done
optimistically, by executing transactions in parallel and detecting conflicts
on the fly, or guided, by requiring exhaustive client transaction hints and
scheduling transactions accordingly.
  However, recent studies have shown that the performance of parallel execution
engines depends on the nature of the underlying workload. In fact, in some
cases, only a 60% speed-up compared to sequential execution could be obtained.
This is the case, as transactions that access the same resources must be
executed sequentially. For example, if 10% of the transactions in a block
access the same resource, the execution cannot meaningfully scale beyond 10
cores. Therefore, a single popular application can bottleneck the execution and
limit the potential throughput.
  In this paper, we introduce Anthemius, a block construction algorithm that
optimizes parallel transaction execution throughput. We evaluate Anthemius
exhaustively under a range of workloads, and show that Anthemius enables the
underlying parallel execution engine to process over twice as many
transactions.; 19) GazeGrasp: DNN-Driven Robotic Grasping with Wearable Eye-Gaze Interface; We present GazeGrasp, a gaze-based manipulation system enabling individuals
with motor impairments to control collaborative robots using eye-gaze. The
system employs an ESP32 CAM for eye tracking, MediaPipe for gaze detection, and
YOLOv8 for object localization, integrated with a Universal Robot UR10 for
manipulation tasks. After user-specific calibration, the system allows
intuitive object selection with a magnetic snapping effect and robot control
via eye gestures. Experimental evaluation involving 13 participants
demonstrated that the magnetic snapping effect significantly reduced gaze
alignment time, improving task efficiency by 31%. GazeGrasp provides a robust,
hands-free interface for assistive robotics, enhancing accessibility and
autonomy for users.; 20) TopoLa: A Universal Framework to Enhance Cell Representations for
  Single-cell and Spatial Omics through Topology-encoded Latent Hyperbolic
  Geometry; Recent advances in cellular research demonstrate that scRNA-seq characterizes
cellular heterogeneity, while spatial transcriptomics reveals the spatial
distribution of gene expression. Cell representation is the fundamental issue
in the two fields. Here, we propose Topology-encoded Latent Hyperbolic Geometry
(TopoLa), a computational framework enhancing cell representations by capturing
fine-grained intercellular topological relationships. The framework introduces
a new metric, TopoLa distance (TLd), which quantifies the geometric distance
between cells within latent hyperbolic space, capturing the network's
topological structure more effectively. With this framework, the cell
representation can be enhanced considerably by performing convolution on its
neighboring cells. Performance evaluation across seven biological tasks,
including scRNA-seq data clustering and spatial transcriptomics domain
identification, shows that TopoLa significantly improves the performance of
several state-of-the-art models. These results underscore the generalizability
and robustness of TopoLa, establishing it as a valuable tool for advancing both
biological discovery and computational methodologies.; 21) Deep Learning-based Feature Discovery for Decoding Phenotypic Plasticity
  in Pediatric High-Grade Gliomas Single-Cell Transcriptomics; By use of complex network dynamics and graph-based machine learning, we
identified critical determinants of lineage-specific plasticity across the
single-cell transcriptomics of pediatric high-grade glioma (pHGGs) subtypes:
IDHWT glioblastoma and K27M-mutant glioma. Our study identified network
interactions regulating glioma morphogenesis via the tumor-immune
microenvironment, including neurodevelopmental programs, calcium dynamics, iron
metabolism, metabolic reprogramming, and feedback loops between MAPK/ERK and
WNT signaling. These relationships highlight the emergence of a hybrid spectrum
of cellular states navigating a disrupted neuro-differentiation hierarchy. We
identified transition genes such as DKK3, NOTCH2, GATAD1, GFAP, and SEZ6L in
IDHWT glioblastoma, and H3F3A, ANXA6, HES6/7, SIRT2, FXYD6, PTPRZ1, MEIS1,
CXXC5, and NDUFAB1 in K27M subtypes. We also identified MTRNR2L1, GAPDH, IGF2,
FKBP variants, and FXYD7 as transition genes that influence cell fate
decision-making across both subsystems. Our findings suggest pHGGs are
developmentally trapped in states exhibiting maladaptive behaviors, and hybrid
cellular identities. In effect, tumor heterogeneity (metastability) and
plasticity emerge as stress-response patterns to immune-inflammatory
microenvironments and oxidative stress. Furthermore, we show that pHGGs are
steered by developmental trajectories from radial glia predominantly favoring
neocortical cell fates, in telencephalon and prefrontal cortex (PFC)
differentiation. By addressing underlying patterning processes and plasticity
networks as therapeutic vulnerabilities, our findings provide precision
medicine strategies aimed at modulating glioma cell fates and overcoming
therapeutic resistance. We suggest transition therapy toward neuronal-like
lineage differentiation as a potential therapy to help stabilize pHGG
plasticity and aggressivity.; 22) Bringing Order Amidst Chaos: On the Role of Artificial Intelligence in
  Secure Software Engineering; Context. Developing secure and reliable software remains a key challenge in
software engineering (SE). The ever-evolving technological landscape offers
both opportunities and threats, creating a dynamic space where chaos and order
compete. Secure software engineering (SSE) must continuously address
vulnerabilities that endanger software systems and carry broader socio-economic
risks, such as compromising critical national infrastructure and causing
significant financial losses. Researchers and practitioners have explored
methodologies like Static Application Security Testing Tools (SASTTs) and
artificial intelligence (AI) approaches, including machine learning (ML) and
large language models (LLMs), to detect and mitigate these vulnerabilities.
Each method has unique strengths and limitations.
  Aim. This thesis seeks to bring order to the chaos in SSE by addressing
domain-specific differences that impact AI accuracy.
  Methodology. The research employs a mix of empirical strategies, such as
evaluating effort-aware metrics, analyzing SASTTs, conducting method-level
analysis, and leveraging evidence-based techniques like systematic dataset
reviews. These approaches help characterize vulnerability prediction datasets.
  Results. Key findings include limitations in static analysis tools for
identifying vulnerabilities, gaps in SASTT coverage of vulnerability types,
weak relationships among vulnerability severity scores, improved defect
prediction accuracy using just-in-time modeling, and threats posed by untouched
methods.
  Conclusions. This thesis highlights the complexity of SSE and the importance
of contextual knowledge in improving AI-driven vulnerability and defect
prediction. The comprehensive analysis advances effective prediction models,
benefiting both researchers and practitioners.; 23) Mitigating Hallucination for Large Vision Language Model by
  Inter-Modality Correlation Calibration Decoding; Large vision-language models (LVLMs) have shown remarkable capabilities in
visual-language understanding for downstream multi-modal tasks. Despite their
success, LVLMs still suffer from generating hallucinations in complex
generation tasks, leading to inconsistencies between visual inputs and
generated content. To address this issue, some approaches have introduced
inference-time interventions, such as contrastive decoding and attention
rectification, to reduce overreliance on language priors. However, these
approaches overlook hallucinations stemming from spurious inter-modality
correlations. In this paper, we propose an Inter-Modality Correlation
Calibration Decoding (IMCCD) method to mitigate hallucinations in LVLMs in a
training-free manner. In this method, we design a Cross-Modal Value-Enhanced
Decoding(CMVED) module to alleviate hallucination by a novel contrastive
decoding mechanism. During the estimation of distorted distribution, CMVED
masks the value vectors associated with significant cross-modal attention
weights, which address both uni-modality overreliance and misleading
inter-modality correlations. Additionally, a Content-Driven Attention
Refinement(CDAR) module refines cross-modal attention weights, guiding LVLMs to
focus on important visual content. Experimental results on diverse
hallucination benchmarks validate the superiority of our method over existing
state-of-the-art techniques in reducing hallucinations in LVLM text generation.
Our code will be available at https://github.com/lijm48/IMCCD.; 24) Generating Robot Constitutions & Benchmarks for Semantic Safety; Until recently, robotics safety research was predominantly about collision
avoidance and hazard reduction in the immediate vicinity of a robot. Since the
advent of large vision and language models (VLMs), robots are now also capable
of higher-level semantic scene understanding and natural language interactions
with humans. Despite their known vulnerabilities (e.g. hallucinations or
jail-breaking), VLMs are being handed control of robots capable of physical
contact with the real world. This can lead to dangerous behaviors, making
semantic safety for robots a matter of immediate concern. Our contributions in
this paper are two fold: first, to address these emerging risks, we release the
ASIMOV Benchmark, a large-scale and comprehensive collection of datasets for
evaluating and improving semantic safety of foundation models serving as robot
brains. Our data generation recipe is highly scalable: by leveraging text and
image generation techniques, we generate undesirable situations from real-world
visual scenes and human injury reports from hospitals. Secondly, we develop a
framework to automatically generate robot constitutions from real-world data to
steer a robot's behavior using Constitutional AI mechanisms. We propose a novel
auto-amending process that is able to introduce nuances in written rules of
behavior; this can lead to increased alignment with human preferences on
behavior desirability and safety. We explore trade-offs between generality and
specificity across a diverse set of constitutions of different lengths, and
demonstrate that a robot is able to effectively reject unconstitutional
actions. We measure a top alignment rate of 84.3% on the ASIMOV Benchmark using
generated constitutions, outperforming no-constitution baselines and
human-written constitutions. Data is available at asimov-benchmark.github.io; 25) Incorporating Magnetic Field Characteristics into EUV-Based Automated
  Segmentation of Coronal Holes; Coronal holes (CH) are magnetically open regions that allow hot coronal
plasma to escape from the Sun and form the high-speed solar wind. This wind can
interact with Earth's magnetic field. For this reason, developing an accurate
understanding of CH regions is vital for understanding space weather and its
effects on Earth. The process of identifying CH regions typically relies on
extreme ultraviolet (EUV) imagery, leveraging the fact that CHs appear dark at
these wavelengths. Accurate identification of CHs in EUV, however, can be
difficult due to a variety of factors, including stray light from nearby
regions, limb brightening, and the presence of filaments (which also appear
dark, but are not sources of solar wind). In order to overcome these issues,
this work incorporates photospheric magnetic field data into a classical
EUV-based segmentation algorithm based on the active contours without edges
(ACWE) segmentation method. In this work magnetic field data are incorporated
directly into the segmentation process, serving both as a method for removing
non-CH regions in advance, and as a method to constrain evolution of the
boundary which identifies the CH boundary. This reduces the presence of
filaments while simultaneously allowing the segmentation to include CH regions
that may be difficult to identify due to the aforementioned inconsistent
intensities.; 26) Bizard: A Community-Driven Platform for Accelerating and Enhancing
  Biomedical Data Visualization; Bizard is a novel visualization code repository designed to simplify data
analysis in biomedical research. It integrates diverse visualization codes,
facilitating the selection and customization of optimal visualization methods
for specific research needs. The platform offers a user-friendly interface with
advanced browsing and filtering mechanisms, comprehensive tutorials, and
interactive forums to enhance knowledge exchange and innovation. Bizard's
collaborative model encourages continuous refinement and expansion of its
functionalities, making it an indispensable tool for advancing biomedical data
visualization and analytical methodologies. By leveraging Bizard's resources,
researchers can enhance data visualization skills, drive methodological
advancements, and improve data interpretation standards, ultimately fostering
the development of precision medicine and personalized therapeutic
interventions.Bizard can be accessed from http://genaimed.org/Bizard/.; 27) The integral chow ring of $M_2^{ct}$; This paper computes the integral Chow ring of the moduli space $M_2^{ct}$ of
stable genus 2 curves of compact type. This is done by excising boundary strata
from $\bar M_2$ one-by-one. During this process, we determine the Chow rings of
all other open strata in $\bar M_2$ with $Z[1/2]$-coefficients.; 28) Contextual Cues in Machine Translation: Investigating the Potential of
  Multi-Source Input Strategies in LLMs and NMT Systems; We explore the impact of multi-source input strategies on machine translation
(MT) quality, comparing GPT-4o, a large language model (LLM), with a
traditional multilingual neural machine translation (NMT) system. Using
intermediate language translations as contextual cues, we evaluate their
effectiveness in enhancing English and Chinese translations into Portuguese.
Results suggest that contextual information significantly improves translation
quality for domain-specific datasets and potentially for linguistically distant
language pairs, with diminishing returns observed in benchmarks with high
linguistic variability. Additionally, we demonstrate that shallow fusion, a
multi-source approach we apply within the NMT system, shows improved results
when using high-resource languages as context for other translation pairs,
highlighting the importance of strategic context language selection.; 29) nvAgent: Automated Data Visualization from Natural Language via
  Collaborative Agent Workflow; Natural Language to Visualization (NL2Vis) seeks to convert natural-language
descriptions into visual representations of given tables, empowering users to
derive insights from large-scale data. Recent advancements in Large Language
Models (LLMs) show promise in automating code generation to transform tabular
data into accessible visualizations. However, they often struggle with complex
queries that require reasoning across multiple tables. To address this
limitation, we propose a collaborative agent workflow, termed nvAgent, for
NL2Vis. Specifically, nvAgent comprises three agents: a processor agent for
database processing and context filtering, a composer agent for planning
visualization generation, and a validator agent for code translation and output
verification. Comprehensive evaluations on the new VisEval benchmark
demonstrate that nvAgent consistently surpasses state-of-the-art baselines,
achieving a 7.88% improvement in single-table and a 9.23% improvement in
multi-table scenarios. Qualitative analyses further highlight that nvAgent
maintains nearly a 20% performance margin over previous models, underscoring
its capacity to produce high-quality visual representations from complex,
heterogeneous data sources.; 30) Analysis of heralded higher-fidelity two-qubit entangling gates with
  self-correction; For the quantum error correction (QEC) and noisy intermediate-scale quantum
(NISQ) algorithms to function with high efficiency, the raw fidelity of quantum
logic gates on physical qubits needs to satisfy strict requirement. The neutral
atom quantum computing equipped with Rydberg blockade gates has made impressive
progress recently, which makes it worthwhile to explore its potential in the
two-qubit entangling gates, including Controlled-PHASE gate and in particular
the CZ gate. Provided the quantum coherence is well preserved, improving the
fidelity of Rydberg blockade gates calls for special mechanisms to deal with
adverse effects caused by realistic experimental conditions. Here the heralded
very-high-fidelity Rydberg blockade Controlled-PHASE gate is designed to
address these issues, which contains self-correction and projection as the key
steps. This trailblazing method can be built on the basis of the previously
established buffer-atom-mediated gate, and a special form of symmetry under PT
transformation plays a crucial role in the process. We further analyze the
performance with respect to a few typical sources of imperfections. This
procedure can also be regarded as quantum hardware error correction or
mitigation. While this paper by itself does not cover every single subtle issue
and still contains many over-simplifications, we find it reasonable to
anticipate very-high-fidelity two-qubit quantum logic gate operated in the
sense of heralded but probabilistic, whose gate error can reduce to the level
of $10^{-4}$--$10^{-6}$ or even lower with reasonably high possibilities.; 31) HST Grism Observations of a z~1.8 Cluster Candidate from the Clusters
  Occupied by Bent Radio AGN (COBRA) Survey; We present new Hubble Space Telescope/Wide Field Camera 3 G141 grism
observations for COBRA1411+3415, originally identified as a high-redshift
cluster candidate in the Clusters Occupied by Bent Radio AGN (COBRA) survey
using radio, infrared, and optical data. We spectroscopically identify seven
cluster members within a 0.5 Mpc radius with grism redshifts in the range
$1.8006 \leq z_{grism} \leq 1.8175$, consistent with COBRA1411+3415 being a
high-redshift cluster with a mean redshift of $\langle z_{grism}\rangle =
1.8106 \pm 0.0006$. The detection of seven galaxies within this small redshift
range is significant above the background distribution of galaxies at the level
of 5$\sigma$. The line-of-sight velocity dispersion of the cluster is found to
be $\sigma_{\parallel} = 701^{+347}_{-138}$ km/s with a virial mass of $M_{200}
\approx 2.2^{+3.3}_{-1.3}\times 10^{14}$ M$_{\odot}$. However, the mass may be
lower if the cluster is still in formation. In projected phase-space, we also
identify two possible infalling members of COBRA1411+3415 and two additional
structures at $z\sim 1.73$ and $z\sim 1.88$. The similar spatial distributions
and small projected separation from the main cluster suggest they could be a
part of the same large-scale filament and together may form a protocluster
system that could eventually merge to form a single, massive cluster.
COBRA1411+3415 is the highest redshift cluster to be spectroscopically
confirmed using a bent, double-lobed radio source as a cluster tracer.; 32) Bounded First-Class Universe Levels in Dependent Type Theory; In dependent type theory, being able to refer to a type universe as a term
itself increases its expressive power, but requires mechanisms in place to
prevent Girard's paradox from introducing logical inconsistency in the presence
of type-in-type. The simplest mechanism is a hierarchy of universes indexed by
a sequence of levels, typically the naturals. To improve reusability of
definitions, they can be made level polymorphic, abstracting over level
variables and adding a notion of level expressions. For even more expressive
power, level expressions can be made first-class as terms themselves, and level
polymorphism is subsumed by dependent functions quantifying over levels.
Furthermore, bounded level polymorphism provides more expressivity by being
able to explicitly state constraints on level variables. While semantics for
first-class levels with constraints are known, syntax and typing rules have not
been explicitly written down. Yet pinning down a well-behaved syntax is not
trivial; there exist prior type theories with bounded level polymorphism that
fail to satisfy subject reduction. In this work, we design an explicit syntax
for a type theory with bounded first-class levels, parametrized over arbitrary
well-founded sets of levels. We prove the metatheoretic properties of subject
reduction, type safety, consistency, and canonicity, entirely mechanized from
syntax to semantics in Lean.; 33) Self-CephaloNet: A Two-stage Novel Framework using Operational Neural
  Network for Cephalometric Analysis; Cephalometric analysis is essential for the diagnosis and treatment planning
of orthodontics. In lateral cephalograms, however, the manual detection of
anatomical landmarks is a time-consuming procedure. Deep learning solutions
hold the potential to address the time constraints associated with certain
tasks; however, concerns regarding their performance have been observed. To
address this critical issue, we proposed an end-to-end cascaded deep learning
framework (Self-CepahloNet) for the task, which demonstrated benchmark
performance over the ISBI 2015 dataset in predicting 19 dental landmarks. Due
to their adaptive nodal capabilities, Self-ONN (self-operational neural
networks) demonstrate superior learning performance for complex feature spaces
over conventional convolutional neural networks. To leverage this attribute, we
introduced a novel self-bottleneck in the HRNetV2 (High Resolution Network)
backbone, which has exhibited benchmark performance on the ISBI 2015 dataset
for the dental landmark detection task. Our first-stage results surpassed
previous studies, showcasing the efficacy of our singular end-to-end deep
learning model, which achieved a remarkable 70.95% success rate in detecting
cephalometric landmarks within a 2mm range for the Test1 and Test2 datasets.
Moreover, the second stage significantly improved overall performance, yielding
an impressive 82.25% average success rate for the datasets above within the
same 2mm distance. Furthermore, external validation was conducted using the PKU
cephalogram dataset. Our model demonstrated a commendable success rate of
75.95% within the 2mm range.; 34) Learnable Group Transform: Enhancing Genotype-to-Phenotype Prediction
  for Rice Breeding with Small, Structured Datasets; Genotype-to-Phenotype (G2P) prediction plays a pivotal role in crop breeding,
enabling the identification of superior genotypes based on genomic data. Rice
(Oryza sativa), one of the most important staple crops, faces challenges in
improving yield and resilience due to the complex genetic architecture of
agronomic traits and the limited sample size in breeding datasets. Current G2P
prediction methods, such as GWAS and linear models, often fail to capture
complex non-linear relationships between genotypes and phenotypes, leading to
suboptimal prediction accuracy. Additionally, population stratification and
overfitting are significant obstacles when models are applied to small datasets
with diverse genetic backgrounds. This study introduces the Learnable Group
Transform (LGT) method, which aims to overcome these challenges by combining
the advantages of traditional linear models with advanced machine learning
techniques. LGT utilizes a group-based transformation of genotype data to
capture spatial relationships and genetic structures across diverse rice
populations, offering flexibility to generalize even with limited data. Through
extensive experiments on the Rice529 dataset, a panel of 529 rice accessions,
LGT demonstrated substantial improvements in prediction accuracy for multiple
agronomic traits, including yield and plant height, compared to
state-of-the-art baselines such as linear models and recent deep learning
approaches. Notably, LGT achieved an R^2 improvement of up to 15\% for yield
prediction, significantly reducing error and demonstrating its ability to
extract meaningful signals from high-dimensional, noisy genomic data. These
results highlight the potential of LGT as a powerful tool for genomic
prediction in rice breeding, offering a promising solution for accelerating the
identification of high-yielding and resilient rice varieties.; 35) ContinuouSP: Generative Model for Crystal Structure Prediction with
  Invariance and Continuity; The discovery of new materials using crystal structure prediction (CSP) based
on generative machine learning models has become a significant research topic
in recent years. In this paper, we study invariance and continuity in the
generative machine learning for CSP. We propose a new model, called
ContinuouSP, which effectively handles symmetry and periodicity in crystals. We
clearly formulate the invariance and the continuity, and construct a model
based on the energy-based model. Our preliminary evaluation demonstrates the
effectiveness of this model with the CSP task.; 36) Thermal X-ray signatures in late-stage unequal-mass massive black hole
  binary mergers; The multi-messenger combination of gravitational waves (GWs) from merging
massive black hole binaries (MBHBs) and the electromagnetic (EM) counterpart
from the surrounding circumbinary disk (CBD) will open avenues to new
scientific pursuits. In order to realize this science, we need to correctly
localize the host galaxy of the merging MBHB. Multi-wavelength, time-dependent
electromagnetic (EM) signatures can greatly facilitate the identification of
the unique EM counterpart among many sources in LISA's localization volume. To
this end, we studied merging unequal-mass MBHBs embedded in a CBD using
high-resolution 2D simulations, with a $\Gamma$-law equation of state,
incorporating viscous heating, shock heating and radiative cooling. We simulate
each binary starting from before it decouples from the CBD until just after the
merger. We compute EM signatures and identify distinct features before, during,
and after the merger. We corroborate previous findings of a several order of
magnitude drop in the thermal X-ray luminosity near the time of merger, but
with delayed timing compared to an equal-mass system. The source remains X-ray
dark for hours post-merger. Our main results are a potential new signature of a
sharp spike in the thermal X-ray emission just before the tell-tale steep drop
occurs. This feature may further help to identify EM counterparts of LISA's
unequal MBHBs before merger without the need for extensive pre-merger
monitoring. Additionally, we find a role-reversal, in which the primary
out-accretes the secondary during late inspiral, which may diminish signatures
originating from Doppler modulation.; 37) Provable Zero-Shot Generalization in Offline Reinforcement Learning; In this work, we study offline reinforcement learning (RL) with zero-shot
generalization property (ZSG), where the agent has access to an offline dataset
including experiences from different environments, and the goal of the agent is
to train a policy over the training environments which performs well on test
environments without further interaction. Existing work showed that classical
offline RL fails to generalize to new, unseen environments. We propose
pessimistic empirical risk minimization (PERM) and pessimistic proximal policy
optimization (PPPO), which leverage pessimistic policy evaluation to guide
policy learning and enhance generalization. We show that both PERM and PPPO are
capable of finding a near-optimal policy with ZSG. Our result serves as a first
step in understanding the foundation of the generalization phenomenon in
offline reinforcement learning.; 38) Self-consistent Solutions of Evolving Nuclear Star Clusters with
  Two-Dimensional Monte-Carlo Dynamical Simulations; We recently developed a Monte-Carlo method (GNC) that can simulate the
dynamical evolution of a nuclear stellar cluster (NSC) with a massive black
hole (MBH), where the two-body relaxations can be solved by the Fokker-Planck
equations in energy and angular momentum space. Here we make a major update of
GNC~ by integrating stellar potential and adiabatic invariant theory, so that
we can study the self-consistent dynamics of NSCs with increasing mass of the
MBH. We perform tests of the self-adaptation of cluster density due to MBH mass
growth and Plummer core collapse, both finding consistent results with previous
studies, the latter having a core collapse time of $\sim 17t_{\rm rh}$ by GNC,
where $t_{\rm rh}$ is the time of half-mass relaxation. We use GNC~ to study
the cosmological evolution of the properties of NSC and the mass of MBH
assuming that the mass growth of the MBH is due to loss-cone accretion of stars
(e.g., tidal disruption of stars) and stellar black holes, and compare the
simulation results with the observations of NSCs in Milky-Way or near-by
galaxies. Such scenario is possible to produce MBHs with mass $10^5\sim
10^7\,M_\odot$ for NSCs with stellar mass of $10^6\sim 10^9\,M_\odot$. In
Milky-Way's NSC, to grow MBH up to $4\times 10^6\,M_\odot$, its size needs to
be $\sim 1.7$ times more compact in early universe than the current value. MBHs
with current masses $>6\times 10^{7}\,M_\odot$ seem difficult to explain by
loss-cone accretion alone, and thus may require other additional accretion
channels, such as gas accretion.; 39) pyBoLaNO: A Python symbolic package for normal ordering involving
  bosonic ladder operators; We present pyBoLaNO, a Python symbolic package based on SymPy to quickly
normal-order (Wick-order) any polynomial in bosonic ladder operators. By
extension, this package offers the normal ordering of commutators of any two
polynomials in bosonic ladder operators and the evaluation of the
normal-ordered expectation value evolution in the Lindblad master equation
framework for open quantum systems. The package also supports multipartite
descriptions and multiprocessing. We describe the package's workflow, show
examples of use, and discuss its computational performance. All codes and
examples are available on our GitHub repository.; 40) Fast and Accurate Blind Flexible Docking; Molecular docking that predicts the bound structures of small molecules
(ligands) to their protein targets, plays a vital role in drug discovery.
However, existing docking methods often face limitations: they either overlook
crucial structural changes by assuming protein rigidity or suffer from low
computational efficiency due to their reliance on generative models for
structure sampling. To address these challenges, we propose FABFlex, a fast and
accurate regression-based multi-task learning model designed for realistic
blind flexible docking scenarios, where proteins exhibit flexibility and
binding pocket sites are unknown (blind). Specifically, FABFlex's architecture
comprises three specialized modules working in concert: (1) A pocket prediction
module that identifies potential binding sites, addressing the challenges
inherent in blind docking scenarios. (2) A ligand docking module that predicts
the bound (holo) structures of ligands from their unbound (apo) states. (3) A
pocket docking module that forecasts the holo structures of protein pockets
from their apo conformations. Notably, FABFlex incorporates an iterative update
mechanism that serves as a conduit between the ligand and pocket docking
modules, enabling continuous structural refinements. This approach effectively
integrates the three subtasks of blind flexible docking-pocket identification,
ligand conformation prediction, and protein flexibility modeling-into a
unified, coherent framework. Extensive experiments on public benchmark datasets
demonstrate that FABFlex not only achieves superior effectiveness in predicting
accurate binding modes but also exhibits a significant speed advantage (208
$\times$) compared to existing state-of-the-art methods. Our code is released
at https://github.com/tmlr-group/FABFlex.; 41) Emergence of running vacuum energy in $f(R,T)$ gravity : Observational
  constraints; In this work, we present a new analysis for $f(R,T)$ gravity by exploring the
energy momentum tensor. We demonstrate that $f(R,T)$ gravity with the form
$f(R,T)=R+2 \kappa^2 \lambda T-2\Lambda$ is equivalent to Running Vacuum Energy
(RVE), which interacts with the components of the cosmic fluid, namely dark
matter and radiation. Interestingly, the form of such interaction is inferred
from the non-conservation of the stress energy tensor in $f(R, T)$ gravity
rather than being introduced in a phenomenological manner. Furthermore, the
parameters that distinguish RVE from $\Lambda$CDM are fixed once the parameter
of $f(R,T)$ gravity, $\lambda$, is known. To illustrate our setup, we perform a
Markov Chain Monte Carlo analysis of three interaction scenarios using a
combination of different data. we find that the parameters characterizing the
RVE model are very small as expected. These results give an accuracy to this
equivalence between $f(R,T)$ gravity under consideration and support the recent
result obtained from a quantum field theory in curved space-time point of view
which could open a new relationship between $f(R,T)$ gravity and quantum field
theory. Finally, the interaction of the running vacuum increases the value of
the current value of the Hubble rate by $3.5\%$ compared to the $\Lambda$CDM
model, which may be a promising study for the Hubble tension.; 42) Neural Implicit Solution Formula for Efficiently Solving Hamilton-Jacobi
  Equations; This paper presents an implicit solution formula for the Hamilton-Jacobi
partial differential equation (HJ PDE). The formula is derived using the method
of characteristics and is shown to coincide with the Hopf and Lax formulas in
the case where either the Hamiltonian or the initial function is convex. It
provides a simple and efficient numerical approach for computing the viscosity
solution of HJ PDEs, bypassing the need for the Legendre transform of the
Hamiltonian or the initial condition, and the explicit computation of
individual characteristic trajectories. A deep learning-based methodology is
proposed to learn this implicit solution formula, leveraging the mesh-free
nature of deep learning to ensure scalability for high-dimensional problems.
Building upon this framework, an algorithm is developed that approximates the
characteristic curves piecewise linearly for state-dependent Hamiltonians.
Extensive experimental results demonstrate that the proposed method delivers
highly accurate solutions, even for nonconvex Hamiltonians, and exhibits
remarkable scalability, achieving computational efficiency for problems up to
40 dimensions.; 43) Genomic Analysis of Date Palm Fruit Size Traits and Identification of
  Candidate Genes through GWAS; The commercial value of economically significant fruits, including date palm
fruit (dates), is influenced by various factors, such as biochemical
composition and morphological features like size, shape, and visual appearance,
which are key determinants of their quality and market value. Dates are
typically consumed at the dry stage (Tamar), during which they exhibit a wide
range of physical characteristics, such as color, length, weight, and skin
appearance. Understanding the genetic basis of these traits is crucial for
improving crop quality and breeding new cultivars. In this study, we integrated
a genome dataset from highly diverse date cultivars with phenotypes of dry
fruit such as length, width, area, and weight, identifying multiple significant
genetic loci (SNPs) associated with these traits. We also identified candidate
genes located near the associated SNPs that are involved in biological
processes such as cell differentiation, proliferation, growth, and the
regulation of signalling pathways for growth regulators like auxin and abscisic
acid, as observed in other plants. Gene expression analysis reveals that many
of these genes are highly expressed in the early stage of fruit development
when the fruit attains its maximum size and weight. These findings will enhance
our understanding of genetic determinants of fruit size particularly at the
commercially important Tamar stage.; 44) Analytical results for laser models producing a beam with sub-Poissonian
  photon statistics and coherence scaling as the Heisenberg limit; Recent advances in laser theory have demonstrated that a quantum enhancement
is possible for the production of coherence $\mathfrak{C}$ by a continuous-wave
laser device. Curiously, natural families of laser models that achieve
Heisenberg-limited scaling for coherence produce the most coherence when the
beam exhibits sub-Poissonian photon statistics. In this work, we provide an
analytical treatment of those novel families of laser models by specializing to
a parameter regime that permits a linearization. We characterize the dynamics
of each laser system, and find that some of the intuitions from standard laser
theory may be applied here. Specifically, the intracavity number dynamics are
well-described as an Ornstein-Uhlenbeck process, while the intracavity phase
dynamics are well-described in terms of a physically realizable ensemble of
pure states, which evolve according to pure phase diffusion. Unlike a standard
laser, however, we find that the pure states comprising the ensemble in the
Heisenberg-limited lasers are substantially phase squeezed. From our dynamical
analysis, we deduce various quantities of the beam for each laser family,
including the first- and second-order Glauber coherence functions, intensity
noise spectrum, Mandel-Q parameter and coherence $\mathfrak{C}$. In addition,
inspired from these phase diffusion dynamics, we derive an upper bound on laser
coherence $\mathfrak{C} \lesssim 1.1156 \mu^4$ -- which is tighter by a factor
of $3/8$ when compared to that derived in [Baker et al., Nat. Phys. 17 179
(2021)] -- by making one of the assumptions of that paper slightly stronger.; 45) Origin of $\alpha$-satellite repeat arrays from mitochondrial molecular
  fossils -- sequential insertion, expansion, and evolution in the nuclear
  genome; Alpha satellite DNA is large tandem arrays of 150-400 bp units, and its
origin remains an evolutionary mystery. In this research, we identified 1,545
alpha-satellite-like (SatL) repeat units in the nuclear genome of jewel wasp
Nasonia vitripennis. Among them, thirty-nine copies of SatL were organized in
two palindromic arrays in mitochondria, resulting in a 50% increase in the
genome size. Strikingly, genomic neighborhood analyses of 1,516 nuclear SatL
repeats revealed that they are located in NuMT (nuclear mitochondrial DNA)
regions, and SatL phylogeny matched perfectly with mitochondrial genes and NuMT
pseudogenes. These results support that SatL arrays originated from ten
independent mitochondria insertion events into the nuclear genome within the
last 500,000 years, after divergence from its sister species N. giraulti.
Dramatic repeat GC-percent elevation (from 33.9% to 50.4%) is a hallmark of
rapid SatL sequence evolution in mitochondria due to GC-biased gene conversion
facilitated by the palindromic sequence pairing of the two mitochondrial SatL
arrays. The nuclear SatL repeat arrays underwent substantial copy number
expansion, from 12-15 (SatL1) to over 400 copies (SatL4). The oldest SatL4B
array consists of four types of repeat units derived from deletions in the
AT-rich region of ancestral repeats, and complex high-order structures have
evolved through duplications. We also discovered similar repeat insertions into
the nuclear genome of Muscidifurax, suggesting this mechanism can be common in
insects. This is the first report of the mitochondrial origin of nuclear
satellite sequences, and our findings shed new light on the origin and
evolution of satellite DNA.; 46) Topological frustration in twisted, elastic ribbons; Topology is an important determinant of the behavior of a great number of
condensed-matter systems, but until recently has played a minor role in
elasticity. We develop a theory for the deformations of a class of twisted
non-Euclidean sheets which have a symmetry based on the celebrated Bonnet
isometry. We show that non-orientability is an obstruction to realizing the
symmetry globally, and induces a geometric phase that captures a memory
analogous to a previously identified one in 2D metamaterials. However, we show
that some of the orientable ribbons also obstruct realizing the symmetry
globally. This new obstruction is mediated by the complex interplay between
strain, geometry, and topology.; 47) Reductions Between Code Equivalence Problems; In this paper we present two reductions between variants of the Code
Equivalence problem. We give polynomial-time Karp reductions from Permutation
Code Equivalence (PCE) to both Linear Code Equivalence (LCE) and Signed
Permutation Code Equivalence (SPCE). Along with a Karp reduction from SPCE to
the Lattice Isomorphism Problem (LIP) proved in a paper by Bennett and Win
(2024), our second result implies a reduction from PCE to LIP.; 48) Relationship between the power spectral density of the Lagrangian
  velocity and the hierarchy of coherent vortices in turbulence; We conduct direct numerical simulations of developed turbulence in a periodic
cube to investigate the formation mechanism of the power spectral density of
the Lagrangian velocity. We compare the power spectral density of the
Lagrangian velocity of turbulent flows with different forcing methods and
Reynolds numbers. This systematic comparison demonstrates that universal
behavior is observed in a narrow high-frequency regime, whereas
non-universality originating from the forcing method broadly appears in a
low-frequency regime. To reveal the formation mechanism of the spectra in terms
of the hierarchy of coherent structures in turbulence, we propose a
scale-decomposition method for the Lagrangian velocity, which enables us to
evaluate the contribution of vortices at different scales. This
scale-decomposition analysis directly demonstrates that the largest-scale flows
driven by the external force can contaminate the Kolmogorov scaling of the
Lagrangian velocity spectra formed by small-scale vortices in the inertial
range, thus leading to the narrow Lagrangian inertial range. Furthermore, we
provide evidence that this remarkable effect by the largest-scale flows is
specific to the Lagrange velocity by demonstrating that the power spectral
density of the Eulerian velocity is less sensitive to the forcing method.; 49) Out-of-plane displacement of quantum color centers in monolayer h-BN; Color centers exhibiting deep-level states within the wide bandgap h-BN
monolayer possess substantial potential for quantum applications. Uncovering
precise geometric characteristics at the atomic scale is crucial for
understanding defect performance. In this study, first-principles calculations
were performed on the most extensively investigated CBVN and NBVN color centers
in h-BN, focusing on the out-of-plane displacement and their specific impacts
on electronic, vibrational, and emission properties. We demonstrate the
competition between the {\sigma}*-like antibonding state and the {\pi}-like
bonding state, which determines the out-of-plane displacement. The overall
effect of vibronic coupling on geometry is elucidated using a pseudo
Jahn-Teller model. Local vibrational analysis reveals a series of distinct
quasi-local phonon modes that could serve as fingerprints for experimental
identification of specific point defects. The critical effects of out-of-plane
displacement during the quantum emission process are carefully elucidated to
answer the distinct observations in experiments, and these revelations are
universal in quantum point defects in other layered materials.; 50) Decoding the Galactic Twirl: The Downfall of Milky Way-mass Galaxies
  Rotation Curves in the FIRE Simulations; Recent measurements of the Milky Way rotation curve found a sharp decline at
around $15$-$20$ kpc from the center of the Galaxy, suggesting that the
Galactic dark matter halo is much less massive than predicted by other
dynamical tracers. To address this tension, we study the validity of the
assumptions made in calculating the Milky Way's rotation curve. To do so, we
apply Jeans' equation, the current standard approach of measuring rotation
curves, to three cosmological zoom-in simulations of Milky Way-like galaxies
from the FIRE-2 Latte suite. Using synthetic Gaia surveys, we replicate the
sample selection process and calculation employed in measuring the Milky Way
rotation curve. We examine four failure modes of this calculation and find that
the measured curves deviate from the true curve by $5$-$20\%$ rather than below
$5\%$, as estimated by previous works. Interestingly, there is a large
galaxy-to-galaxy variance, and different systematics dominate different
galaxies. We rederive the Milky Way's dark matter density profile with the
rotation curve while incorporating systematics from the simulations. The
posterior distribution of the density profiles is consistent with a fiducial
NFW profile when assuming a gNFW profile for dark matter. We find that the
virial mass, $7.32^{+1.98}_{-1.53}\times10^{11}~M_{\odot}$, consistent with
other probes of the Milky Way's mass. However, we recommend that the field
moves away from relying solely on the rotation curve when studying the dark
matter profile, and adopts methods that incorporate additional probes and/or do
not heavily depend on assumptions described in this study.; 51) Noise Synthesis for Low-Light Image Denoising with Diffusion Models; Low-light photography produces images with low signal-to-noise ratios due to
limited photons. In such conditions, common approximations like the Gaussian
noise model fall short, and many denoising techniques fail to remove noise
effectively. Although deep-learning methods perform well, they require large
datasets of paired images that are impractical to acquire. As a remedy,
synthesizing realistic low-light noise has gained significant attention. In
this paper, we investigate the ability of diffusion models to capture the
complex distribution of low-light noise. We show that a naive application of
conventional diffusion models is inadequate for this task and propose three key
adaptations that enable high-precision noise generation without calibration or
post-processing: a two-branch architecture to better model signal-dependent and
signal-independent noise, the incorporation of positional information to
capture fixed-pattern noise, and a tailored diffusion noise schedule.
Consequently, our model enables the generation of large datasets for training
low-light denoising networks, leading to state-of-the-art performance. Through
comprehensive analysis, including statistical evaluation and noise
decomposition, we provide deeper insights into the characteristics of the
generated data.; 52) Find Central Dogma Again: Leveraging Multilingual Transfer in Large
  Language Models; In recent years, large language models (LLMs) have achieved state-of-the-art
results in various biological sequence analysis tasks, such as sequence
classification, structure prediction, and function prediction. Similar to
advancements in AI for other scientific fields, deeper research into biological
LLMs has begun to focus on using these models to rediscover important existing
biological laws or uncover entirely new patterns in biological sequences. This
study leverages GPT-like LLMs to utilize language transfer capabilities to
rediscover the genetic code rules of the central dogma. In our experimental
design, we transformed the central dogma into a binary classification problem
of aligning DNA sequences with protein sequences, where positive examples are
matching DNA and protein sequences, and negative examples are non-matching
pairs. We first trained a GPT-2 model from scratch using a dataset comprising
protein sequences, DNA sequences, and sequences from languages such as English
and Chinese. Subsequently, we fine-tuned the model using the natural language
sentences similarity judgment dataset from PAWS-X. When tested on a dataset for
DNA and protein sequence alignment judgment, the fine-tuned model achieved a
classification accuracy of 81%. The study also analyzed factors contributing to
this zero-shot capability, including model training stability and types of
training data. This research demonstrates that LLMs can, through the transfer
of natural language capabilities and solely relying on the analysis of
sequences themselves, rediscover the central dogma without prior knowledge of
it. This study bridges natural language and genetic language, opening a new
door for AI-driven biological research.; 53) Word maps and random words; We discuss some recent results by a number of authors regarding word maps on
algebraic groups and finite simple groups, their mixing properties and the
geometry of their fibers, emphasizing the role played by equidistribution
results in finite fields via recent advances on character bounds and
non-abelian arithmetic combinatorics. In particular, we discuss character
varieties of random groups. In the last section, we give a new proof of a
recent theorem of Hrushovski about the geometric irreducibility of the generic
fibers of convolutions of dominant morphisms to simply connected algebraic
groups.
  These notes stem out of lectures given by the authors in Oxford, and by the
first author in ICTS Bangalore, in spring 2024.; 54) A study of Kock's fat Delta; Motivated by the study of weak identity structures in higher category theory
we explore the fat Delta category, a modification of the simplex category
introduced by J. Kock. We provide a comprehensive study of fat Delta via the
theory of monads with arities, and use these results to show that fat Delta is
a hypermoment category in the sense of C. Berger. Specifically, by proving that
the free relative semicategory monad is strongly cartesian and identifying a
dense generator, the theory of monads with arities immediately gives rise to
the nerve theorem. We characterise the essential image of the nerve via the
Segal condition, and show that fat Delta possesses an active-inert
factorisation system. Building on these results, we also establish an
isomorphism between two presentations of fat Delta and show that it is a
strongly unital and extensional hypermoment category.; 55) Application of Single-cell Deep Learning in Elucidating the Mapping
  Relationship Between Visceral and Body Surface Inflammatory Patterns; As a system of integrated homeostasis, life is susceptible to disruptions by
visceral inflammation, which can disturb internal environment equilibrium. The
role of body-spread subcutaneous fascia (scFascia) in this process is poorly
understood. In the rat model of Salmonella-induced dysentery, scRNA-seq of
scFascia and deep-learning analysis revealed Warburg-like metabolic
reprogramming in macrophages (MPs) with reduced citrate cycle activity.
Cd34+/Pdgfra+ telocytes (CPTCs) regulated MPs differentiation and proliferation
via Wnt/Fgf signal, suggesting a pathological crosstalk pattern in the
scFascia, herein termed the fascia-visceral inflammatory crosstalk pattern
(FVICP). PySCENIC analysis indicated increased activity transcription factors
Fosl1, Nfkb2, and Atf4, modulated by CPTCs signaling to MPs, downregulating
aerobic respiration and upregulating cell cycle, DNA replication, and
transcription. This study highlights scFascia's role in immunomodulation and
metabolic reprogramming during visceral inflammation, underscoring its function
in systemic homeostasis.; 56) Curated loci prime editing (cliPE) for accessible multiplexed assays of
  variant effect (MAVEs); Multiplexed assays of variant effect (MAVEs) perform simultaneous
characterization of many variants. Prime editing has been recently adopted for
introducing many variants in their native genomic contexts. However, robust
protocols and standards are limited, preventing widespread uptake. Herein, we
describe curated loci prime editing (cliPE) which is an accessible, low-cost
experimental pipeline to perform MAVEs using prime editing of a target gene, as
well as a companion Shiny app (pegRNA Designer) to rapidly and easily design
user-specific MAVE libraries.; 57) TPC: Cross-Temporal Prediction Connection for Vision-Language Model
  Hallucination Reduction; Vision-language models (VLMs) have achieved remarkable advancements,
capitalizing on the impressive capabilities of large language models (LLMs)
across diverse tasks. Despite this, a critical challenge known as hallucination
occurs when models overconfidently describe objects or attributes absent from
the image, a problem exacerbated by the tendency of VLMs to rely on linguistic
priors. This limitation reduces model reliability in high-stakes applications.
In this work, we have observed the characteristic of logits' continuity
consistency enhancement and introduced a straightforward and efficient method,
Cross-Temporal Prediction Connection (TPC), designed to enhance the semantic
consistency of logits by connecting them temporally across timesteps. TPC
amplifies information flow and improves coherence, effectively reducing
hallucination. Extensive experiments show that TPC surpasses existing
representatives, delivering superior performance in both accuracy and
efficiency while maintaining robustness in open-ended text generation tasks.; 58) BEAC: Imitating Complex Exploration and Task-oriented Behaviors for
  Invisible Object Nonprehensile Manipulation; Applying imitation learning (IL) is challenging to nonprehensile manipulation
tasks of invisible objects with partial observations, such as excavating buried
rocks. The demonstrator must make such complex action decisions as exploring to
find the object and task-oriented actions to complete the task while estimating
its hidden state, perhaps causing inconsistent action demonstration and high
cognitive load problems. For these problems, work in human cognitive science
suggests that promoting the use of pre-designed, simple exploration rules for
the demonstrator may alleviate the problems of action inconsistency and high
cognitive load. Therefore, when performing imitation learning from
demonstrations using such exploration rules, it is important to accurately
imitate not only the demonstrator's task-oriented behavior but also his/her
mode-switching behavior (exploratory or task-oriented behavior) under partial
observation. Based on the above considerations, this paper proposes a novel
imitation learning framework called Belief Exploration-Action Cloning (BEAC),
which has a switching policy structure between a pre-designed exploration
policy and a task-oriented action policy trained on the estimated belief states
based on past history. In simulation and real robot experiments, we confirmed
that our proposed method achieved the best task performance, higher mode and
action prediction accuracies, while reducing the cognitive load in the
demonstration indicated by a user study.; 59) Multimodal Search in Chemical Documents and Reactions; We present a multimodal search tool that facilitates retrieval of chemical
reactions, molecular structures, and associated text from scientific
literature. Queries may combine molecular diagrams, textual descriptions, and
reaction data, allowing users to connect different representations of chemical
information. To support this, the indexing process includes chemical diagram
extraction and parsing, extraction of reaction data from text in tabular form,
and cross-modal linking of diagrams and their mentions in text. We describe the
system's architecture, key functionalities, and retrieval process, along with
expert assessments of the system. This demo highlights the workflow and
technical components of the search system.; 60) A new way to measure the distance to NGC1052-DF2; We employ a new way to measure the distance to NGC1052-DF2 via internal
stellar velocity dispersions ($\sigma$) of its globular clusters (GCs). We
obtained deep (15.1h), R=18,200, Ca Triplet integrated-light spectra for 10 GCs
in NGC1052-DF2 using FLAMES GIRAFFE on VLT. For five GCs we measure $\sigma$,
along with precision velocities for the whole sample. We also present a new
photometric analysis based on 40 orbits of archival Hubble Space Telescope
imaging for 16 spectroscopically confirmed GCs. Assuming that the NGC1052-DF2
GCs obey the $M_V$ -- log($\sigma$) relation followed by the Milky Way and M31
GCs, the NGC1052-DF2 GCs give a distance, $d=16.2\pm1.3$ (stat.) $\pm1.7$
(sys.) Mpc. By contrast, using a literature distance of $d=21.7$ Mpc from
forward modelling of the TRGB, the GCs lie above the Milky Way + M31 relation
by $\sim0.6$ magnitudes. For a shorter literature distance of 13 Mpc, the GCs
fall below the relation by $\sim0.4$ mag. At $d = 16.2$ Mpc, we obtain mean
dynamical $M/L_V = 1.61\pm0.44 M_\odot/L_\odot$, and median half-light radii,
$r_h =3.0\pm0.5$ pc. This is entirely consistent with Milky Way GCs, with mean
$M/L_V = 1.77\pm0.10 M_\odot/L_\odot$, median $r_h =3.2\pm0.6$ pc. For the
further distance of 21.7 Mpc, we obtain low $M/L_V$ ratios ($M/L_V =
1.19\pm0.33 M_\odot/L_\odot$) which could suggest ages of $\sim6$ Gyr.. Such
young ages are inconsistent with our MUSE stellar population (companion paper,
Fahrion et al.) analysis of the NGC1052-DF2 GCs which indicates they are
$\sim10$ Gyr old. For $d = 16.2$ Mpc, coupled with our new photometry, we find
that the properties of the GCs in NGC1052-DF2 appear entirely consistent with
those in the Milky Way and other Local Group galaxies. In order to reconcile
the further distance with our results, a mass function more dwarf-depleted than
the Milky Way GCs must be invoked for the GCs of NGC1052-DF2.; 61) Observation algebras: Heyting algebra over coherence spaces; In this report, we introduce observation algebras, constructed by considering
the downclosed subsets of a coherence space ordered by reverse inclusion. These
may be interpreted as specifications of sets of events via some predicates with
some extra structure. We provide syntax for these algebras, as well as
axiomatisations. We establish completeness of these axiomatisations in two
cases: when the syntax is that of bounded distributive lattices (conjunction,
disjunction, top, and bottom), and when the syntax also includes an implication
operator (in the sense of Heyting algebra), but the underlying coherence space
satisfies some tractability condition. We also provide a product construction
to combine graphs and their axiomatisations, yielding a sound and complete
composite system. This development has been fully formalised in Rocq.; 62) Genotype-to-Phenotype Prediction in Rice with High-Dimensional Nonlinear
  Features; Genotype-to-Phenotype prediction can promote advances in modern genomic
research and crop improvement, guiding precision breeding and genomic
selection. However, high-dimensional nonlinear features often hinder the
accuracy of genotype-to-phenotype prediction by increasing computational
complexity. The challenge also limits the predictive accuracy of traditional
approaches. Therefore, effective solutions are needed to improve the accuracy
of genotype-to-phenotype prediction. In our paper, we propose MLFformer.
MLFformer is a Transformer-based architecture that incorporates the Fast
Attention mechanism and a multilayer perceptron module to handle
high-dimensional nonlinear features. In MLFformer, the Fast Attention mechanism
is utilized to handle computational complexity and enhance processing
efficiency. In addition, the MLP structure further captures high-dimensional
nonlinear features. Through experiments, the results show that MLFformer
reduces the average MAPE by 7.73% compared to the vanilla Transformer. In
univariate and multivariate prediction scenarios, MLFformer achieves the best
predictive performance among all compared models.; 63) EmoTalkingGaussian: Continuous Emotion-conditioned Talking Head
  Synthesis; 3D Gaussian splatting-based talking head synthesis has recently gained
attention for its ability to render high-fidelity images with real-time
inference speed. However, since it is typically trained on only a short video
that lacks the diversity in facial emotions, the resultant talking heads
struggle to represent a wide range of emotions. To address this issue, we
propose a lip-aligned emotional face generator and leverage it to train our
EmoTalkingGaussian model. It is able to manipulate facial emotions conditioned
on continuous emotion values (i.e., valence and arousal); while retaining
synchronization of lip movements with input audio. Additionally, to achieve the
accurate lip synchronization for in-the-wild audio, we introduce a
self-supervised learning method that leverages a text-to-speech network and a
visual-audio synchronization network. We experiment our EmoTalkingGaussian on
publicly available videos and have obtained better results than
state-of-the-arts in terms of image quality (measured in PSNR, SSIM, LPIPS),
emotion expression (measured in V-RMSE, A-RMSE, V-SA, A-SA, Emotion Accuracy),
and lip synchronization (measured in LMD, Sync-E, Sync-C), respectively.; 64) Arabizi vs LLMs: Can the Genie Understand the Language of Aladdin?; In this era of rapid technological advancements, communication continues to
evolve as new linguistic phenomena emerge. Among these is Arabizi, a hybrid
form of Arabic that incorporates Latin characters and numbers to represent the
spoken dialects of Arab communities. Arabizi is widely used on social media and
allows people to communicate in an informal and dynamic way, but it poses
significant challenges for machine translation due to its lack of formal
structure and deeply embedded cultural nuances. This case study arises from a
growing need to translate Arabizi for gisting purposes. It evaluates the
capacity of different LLMs to decode and translate Arabizi, focusing on
multiple Arabic dialects that have rarely been studied up until now. Using a
combination of human evaluators and automatic metrics, this research project
investigates the model's performance in translating Arabizi into both Modern
Standard Arabic and English. Key questions explored include which dialects are
translated most effectively and whether translations into English surpass those
into Arabic.; 65) Who Reaps All the Superchats? A Large-Scale Analysis of Income
  Inequality in Virtual YouTuber Livestreaming; The explosive growth of Virtual YouTubers (VTubers)-streamers who perform
behind virtual anime avatars-has created a unique digital economy with profound
implications for content creators, platforms, and viewers. Understanding the
economic landscape of VTubers is crucial for designing equitable platforms,
supporting content creator livelihoods, and fostering sustainable digital
communities. To this end, we conducted a large-scale study of over 1 million
hours of publicly available streaming records from 1,923 VTubers on YouTube,
covering tens of millions of dollars in actual profits. Our analysis reveals
stark inequality within the VTuber community and characterizes the sources of
income for VTubers from multiple perspectives. Furthermore, we also found that
the VTuber community is increasingly monopolized by two agencies, driving the
financial disparity. This research illuminates the financial dynamics of VTuber
communities, informing the design of equitable platforms and sustainable
support systems for digital content creators.; 66) Grand variable Herz-Morrey type Besove spaces and Triebel-Lizorkin
  spaces; In the article, the boundedness of vector-valued sublinear operators in grand
variable Herz-Morrey spaces $M \dot{K}_{ \lambda, p(\cdot)}^{\eta (\cdot), q),
\theta}\left(\mathbb{R}^{n}\right)$ are obtained. Then grand variable
Herz-Morrey type Besov and Triebel-Lizorkin spaces are defined. We will also
prove the equivalent quasi-norms by Peetre's maximal operators in these spaces.; 67) Shock formation for the 2D rotating shallow water equations with
  non-zero vorticity; In the paper, the shock formation for the two-dimensional rotating shallow
water system is established. We construct a large class of initial data which
leads to the finite-time blow-up for the solutions. Moreover, the solutions are
allowed to have non-zero large vorticity (in derivative sense), even up to the
shock. Our results provide the first complete geometric description of the
shock formation mechanism to the two-dimensional rotating shallow water system
with vorticity. The formation of shock is characterized by the collapse of the
characteristic hypersurfaces, where the first-order derivatives of the
velocity, the height, and the specific vorticity blow up while the potential
vorticity remains Lipschitz continuous. The methods developed in this paper
should also be useful in studying the shock formation for the Euler equations
with various source terms and a class of quasilinear Klein-Gordon equations in
multi-dimensions.; 68) Extension of Optimal Locally Repairable codes; Recent studies have delved into the construction of locally repairable codes
(LRCs) with optimal minimum distance from function fields. In this paper, we
present several novel constructions by extending the findings of optimally
designed locally repairable codes documented in the literature. Let $C$ denote
an optimal LRC of locality $r$, implying that every repairable block of $C$ is
a $[r+1, r]$ MDS code, and $C$ maximizes its minimum distance. By extending a
single coordinate of one of these blocks, we demonstrate that the resulting
code remains an optimally designed locally repairable code. This suggests that
the maximal length of an optimal LRC from rational function fields can be
extended up to $q+2$ over a finite field $\mathbb{F}_q$. In addition, we give a
new construction of optimal $(r, 3)$-LRC by extending one coordinate in each
block within $C$. Furthermore, we propose a novel family of LRCs with
Roth-Lempel type that are optimal under certain conditions. Finally, we explore
optimal LRCs derived from elliptic function fields and extend a single
coordinate of such codes. This approach leads us to confirm that the new codes
are also optimal, thereby allowing their lengths to reach $q + 2\sqrt{q} - 2r -
2$ with locality $r$. We also consider the construction of optimal $(r, 3)$-LRC
in elliptic function fields, with exploring one more condition.; 69) Numerical Investigation of Preferential Flow Paths in Enzymatically
  Induced Calcite Precipitation supported by Bayesian Model Analysis; The usability of enzymatically induced calcium carbonate precipitation (EICP)
as a method for altering porous-media properties, soil stabilization, or
biocementation depends on our ability to predict the spatial distribution of
the precipitated calcium carbonate in porous media. While current REV-scale
models are able to reproduce the main features of laboratory experiments, they
neglect effects like the formation of preferential flow paths and the
appearance of multiple polymorphs of calcium carbonate with differing
properties. We show that extending an existing EICP model by the conceptual
assumption of a mobile precipitate, amorphous calcium carbonate (ACC), allows
for the formation of preferential flow paths when the initial porosity is
heterogeneous. We apply sensitivity analysis and Bayesian inference to gain an
understanding of the influence of characteristic parameters of ACC that are
uncertain or unknown and compare two variations of the model based on different
formulations of the ACC detachment term to analyse the plausibility of our
hypothesis. An arbitrary Polynomial Chaos (aPC) surrogate model is trained
based on the full model and used to reduce the computational cost of this
study.; 70) The Role of Planetary-Scale Waves on the Stratospheric Superrotation in
  Titan's Atmosphere; We analyze simulation results from the TitanWRF global circulation model to
understand the mechanisms that maintain the equatorial superrotation in Titan's
stratosphere. We find that the eddies associated with wave activities can
transport angular momentum upgradient to zonal flow, leading to acceleration of
the equatorial superrotation. The dominant wave modes identified in this study
are consistent with previous studies, with zonal wavenumber 1 being the major
contributor to the prograde acceleration. Despite the same conclusion of
maintenance of equatorial superrotation via wave-mean interactions, we find
that the way waves interact with the zonal flow in TitanWRF is slightly
different from some other studies. We confirm our previous findings that in
TitanWRF this occurs primarily during a dozen or so annual, short-duration (a
few Titan sols) angular momentum ""transfer events,"" which have a repeatable
seasonal pattern but differ slightly in timing and magnitude between years.
This is not the case in the Titan Atmosphere Model (TAM), which found milder
angular momentum transfers that produced the strongest acceleration of
superrotation around solstice in the upper stratosphere and more continuous
year-around acceleration in the lower stratosphere. Despite differences in
angular momentum transfer across models, we further find that, similar to the
TAM wave analysis results, eddies generated by Rossby-Kelvin instabilities may
be the major source of prograde angular momentum for the equatorial
superrotation, although TitanWRF may also include contributions from the
absorption of vertically propagating equatorial Kelvin waves. This differs from
our previous work, which suggested barotropic waves were responsible for
TitanWRF's solsticial transfer event.; 71) Slow is Fast! Dissecting Ethereum's Slow Liquidity Drain Scams; We identify the slow liquidity drain (SLID) scam, an insidious and highly
profitable threat to decentralized finance (DeFi), posing a large-scale,
persistent, and growing risk to the ecosystem. Unlike traditional scams such as
rug pulls or honeypots (USENIX Sec'19, USENIX Sec'23), SLID gradually siphons
funds from liquidity pools over extended periods, making detection
significantly more challenging. In this paper, we conducted the first
large-scale empirical analysis of 319,166 liquidity pools across six major
decentralized exchanges (DEXs) since 2018. We identified 3,117 SLID affected
liquidity pools, resulting in cumulative losses of more than US$103 million. We
propose a rule-based heuristic and an enhanced machine learning model for early
detection. Our machine learning model achieves a detection speed 4.77 times
faster than the heuristic while maintaining 95% accuracy. Our study establishes
a foundation for protecting DeFi investors at an early stage and promoting
transparency in the DeFi ecosystem.; 72) Numerical insights on the volume phase transition of thermoresponsive
  hollow microgels; Hollow microgels, consisting of a pNIPAM polymer network with a central
cavity, have significant potential due to their tunable softness and
encapsulation capabilities. Using molecular dynamics simulations, we thoroughly
characterise the swelling behaviour of neutral hollow microgels across the
Volume Phase Transition (VPT) upon varying crosslinker concentration, shell
thickness, and size. In particular, we examine in detail the onset of cavity
filling and its relation to the VPT, detecting the presence of a discontinuity
in the radius of gyration of the microgels, if an appropriate balance between
shell stiffness and thermoresposiveness is reached. The discontinuity is,
however, absent in the behaviour of the hydrodynamic radius, in agreement with
experimental observations. We then test our numerical model by direct
comparison of form factors with available measurements in the literature and
also establish a minimal-size, stable hollow microgel for future
computationally feasible bulk investigations. Overall, our findings provide
valuable insights into the fundamental swelling properties of hollow microgels
that can be useful to control the opening and closing of the cavity for
application purposes.; 73) Mean Field Game of Controls with State Reflections: Existence and Limit
  Theory; This paper studies mean field game (MFG) of controls by featuring the joint
distribution of the state and the control with the reflected state process
along an exogenous stochastic reflection boundary. We contribute to the
literature with a customized relaxed formulation and some new compactification
arguments to establish the existence of a Markovian mean field equilibrium
(MFE) in the weak sense. We consider an enlarged canonical space, utilizing the
dynamic Skorokhod mapping, to accommodate the stochastic reflection boundary
process. A fixed-point argument on the extended space using an extension
transformation technique is developed to tackle challenges from the joint
measure flow of the state and the relaxed control that may not be continuous.
Furthermore, the bidirectional connections between the MFG and the $N$-player
game are also established in the context of joint law dependence and state
reflections. We first show that any weak limit of empirical measures induced by
$\boldsymbol{\epsilon}$-Nash equilibria in $N$-player games must be supported
exclusively on the set of relaxed mean field equilibria, analogous to the
propagation of chaos in mean field control problems. We then prove the
convergence result that a Markovian MFE in the weak sense can be approximated
by a sequence of constructed $\boldsymbol{\epsilon}$-Nash equilibria in the
weak sense in $N$-player games when $N$ tends to infinity.; 74) Expensive Homeomorphism of Convex Bodies; In this paper, we address the longstanding question of whether expansive
homeomorphisms can exist within convex bodies in Euclidean spaces. Utilizing
fundamental tools from topology, including the Borsuk-Ulam theorem and
Brouwer's fixed-point theorem, we establish the nonexistence of such mappings.
Through an inductive approach based on dimension and the extension of boundary
homeomorphisms, we demonstrate that expansive homeomorphisms are incompatible
with the compact and convex structure of these bodies. This work highlights the
interplay between topological principles and metric geometry, offering new
insights into the constraints imposed by convexity.; 75) A taxonomy of categories for relations; The study of categories abstracting the structural properties of relations
has been extensively developed over the years, resulting in a rich and diverse
body of work. This paper strives to provide a modern and comprehensive
presentation of these ``categories for relations'', including their enriched
version, further showing how they arise as Kleisli categories of suitable
symmetric monoidal monads. The resulting taxonomy aims at bringing clarity and
organisation to the numerous related concepts and frameworks occurring in the
literature; 76) Audio-CoT: Exploring Chain-of-Thought Reasoning in Large Audio Language
  Model; Large Audio-Language Models (LALMs) have demonstrated remarkable performance
in tasks involving audio perception and understanding, such as speech
recognition and audio captioning. However, their reasoning capabilities -
critical for solving complex real-world problems - remain underexplored. In
this work, we conduct the first exploration into integrating Chain-of-Thought
(CoT) reasoning into LALMs to enhance their reasoning ability across auditory
modalities. We evaluate representative CoT methods, analyzing their performance
in both information extraction and reasoning tasks across sound, music, and
speech domains. Our findings reveal that CoT methods significantly improve
performance on easy and medium tasks but encounter challenges with hard tasks,
where reasoning chains can confuse the model rather than improve accuracy.
Additionally, we identify a positive correlation between reasoning path length
and accuracy, demonstrating the potential of scaling inference for advanced
instruction-following and reasoning. This study not only highlights the promise
of CoT in enhancing LALM reasoning capabilities but also identifies key
limitations and provides actionable directions for future research.; 77) A Semi-Orthogonal Decomposition Theorem for Weighted Blowups; We establish a semi-orthogonal decomposition for the weighted blowup of an
algebraic stack along a Koszul-regular weighted centre, generalising the
classic result of Orlov. Our approach is based on the work of Bergh-Schn\""urer.; 78) CoverM: Read alignment statistics for metagenomics; Genome-centric analysis of metagenomic samples is a powerful method for
understanding the function of microbial communities. Calculating read coverage
is a central part of analysis, enabling differential coverage binning for
recovery of genomes and estimation of microbial community composition. Coverage
is determined by processing read alignments to reference sequences of either
contigs or genomes. Per-reference coverage is typically calculated in an ad-hoc
manner, with each software package providing its own implementation and
specific definition of coverage. Here we present a unified software package
CoverM which calculates several coverage statistics for contigs and genomes in
an ergonomic and flexible manner. It uses 'Mosdepth arrays' for computational
efficiency and avoids unnecessary I/O overhead by calculating coverage
statistics from streamed read alignment results. CoverM is free software
available at https://github.com/wwood/coverm. CoverM is implemented in Rust,
with Python (https://github.com/apcamargo/pycoverm) and Julia
(https://github.com/JuliaBinaryWrappers/CoverM_jll.jl) interfaces.; 79) Globality Strikes Back: Rethinking the Global Knowledge of CLIP in
  Training-Free Open-Vocabulary Semantic Segmentation; Recent works modify CLIP to perform open-vocabulary semantic segmentation in
a training-free manner (TF-OVSS). In CLIP, patch-wise image representations
mainly encode the homogeneous image-level properties and thus are not
discriminative enough, hindering its application to the dense prediction task.
Previous works make image features more distinct across patches, through making
each patch mainly attend to itself or the neighboring patches within a narrow
local window. However, with their modifications, the ability of CLIP to
aggregate global context information, which is known to be useful for
distinguishing confusing categories, is largely weakened. In this paper, we
propose a new method named GCLIP, which mines the beneficial global knowledge
of CLIP to facilitate the TF-OVSS task. Firstly, we aim to equip the last-block
attention with image-level properties while not introducing homogeneous
attention patterns across patches. In GCLIP, we merge the attention from the
global token emerging blocks with the Query-Query attention to realize this
goal. Secondly, we aim to make the Value embeddings of the last-block attention
module more distinct and semantically correlated. To realize this, we design a
novel channel suppression strategy. As the representation of each patch is
finally determined by the attention weights and the Value embeddings, our
method can generate more discriminative patch-level image features while
absorbing global context information. Extensive experiments on five standard
benchmarks demonstrate that our method consistently outperforms previous
state-of-the-arts.; 80) Efficient Domain Adaptation of Multimodal Embeddings using Constrastive
  Learning; Recent advancements in machine learning (ML), natural language processing
(NLP), and foundational models have shown promise for real-life applications in
critical, albeit compute-constrainted fields like healthcare.
  In such areas, combining foundational models with supervised ML offers
potential for automating tasks like diagnosis and treatment planning, but the
limited availability of onsite computational resources pose significant
challenges before applying these technologies effectively: Current approaches
either yield subpar results when using pretrained models without task-specific
adaptation, or require substantial computational resources for fine-tuning,
which is often a barrier to entry in such environments.
  This renders them inaccessible in applications where performance and quality
standards are high, but computational resources are scarce.
  To bridge the gap between best-in-class performance and accessibility, we
propose a novel method for adapting foundational, multimodal embeddings to
downstream tasks, without the need of expensive fine-tuning processes.
  Our method leverages frozen embeddings from Large Language Models (LLMs) and
Vision Models, and uses contrastive learning to train a small, task-specific
nonlinear projection that can be used in the downstream task, without having to
fine-tune the original foundational models.
  We show that this efficient procedure leads to significant performance
improvements across various downstream tasks, and perhaps more importantly with
minimal computational overhead, offering a practical solution for the use of
advanced, foundational ML models in resource-constrained settings.; 81) Variational Learning Induces Adaptive Label Smoothing; We show that variational learning naturally induces an adaptive label
smoothing where label noise is specialized for each example. Such
label-smoothing is useful to handle examples with labeling errors and
distribution shifts, but designing a good adaptivity strategy is not always
easy. We propose to skip this step and simply use the natural adaptivity
induced during the optimization of a variational objective. We show empirical
results where a variational algorithm called IVON outperforms traditional label
smoothing and yields adaptivity strategies similar to those of an existing
approach. By connecting Bayesian methods to label smoothing, our work provides
a new way to handle overconfident predictions.; 82) The EU Digital Services Act: what does it mean for online advertising
  and adtech?; What does the Digital Services Act (DSA) mean for online advertising? We
describe and analyse the DSA rules that are most relevant for online
advertising and adtech (advertising technology). We also highlight to what
extent the DSA's advertising rules add something to the rules in the General
Data Protection Regulation (GDPR) and the ePrivacy Directive. The DSA
introduces several specific requirements for online advertising. First, the DSA
imposes transparency requirements in relation to advertisements. Second, very
large online platforms (VLOPs) should develop a publicly available repository
with information about the ads they presented. Third, the DSA bans
profiling-based advertising (behavioural advertising) if it uses sensitive data
or if it targets children. Besides these specific provisions, the general rules
of the DSA on illegal content also apply to advertising. Advertisements are a
form of information, and thus subject to the general DSA rules. Moreover, we
conclude that the DSA applies to some types of ad tech companies. For example,
ad networks, companies that connect advertisers to publishers of apps and
websites, should be considered platforms. Some ad networks may even qualify as
VLOPs. Hence, ad networks must comply with the more general obligations in the
DSA. The application of these general rules to advertisements and ad networks
can have far-reaching effects that have been underexplored and deserve further
research. We also show that certain aspects of the DSA are still unclear. For
instance, we encourage the European Commission or regulators to clarify the
concepts of 'online platform' and 'recipients' in the context of ad networks
and other adtech companies.; 83) Coherent spin dynamics in ensembles of randomly oriented singly charged
  colloidal nanoplatelets and nanocrystals; We present a theoretical study of the pump-probe Faraday rotation and
ellipticity signals in ensembles of uniaxially anisotropic CdSe nanoplatelets
and nanocrystals. We use the Faraday rotation mechanism based on the excitation
of negative heavy hole trions for a magnetic field applied in the Voigt
geometry. Three types of ensembles with typical spatial distributions of the
orientation of the anisotropy axis with respect to the direction of light
propagation are considered. Faraday rotation and ellipticity signals are
modeled for excitation by single and repeated pump pulses, taking into account
the anisotropy of the electron g-factor. We show that spin dephasing caused by
the electron g-factor anisotropy and the arbitrary orientation of nanoplatelets
or nanocrystals result only in partial damping of oscillation amplitude in
contrast to the dephasing caused by the dispersion of the electron g-factor in
the ensemble. We demonstrate that regardless of the g-factor anisotropy degree
the oscillation frequency of the Faraday rotation and ellipticity signals for a
randomly oriented ensemble is determined by the transverse electron g-factor
component.; 84) Disordered ground state in the 3D face-centred frustrated
  spin-$\frac{5}{2}$ system MnSn(OH)$_\text{6}$; Frustrated magnetism in face-centred cubic (fcc) magnetic sublattices remains
underexplored but holds considerable potential for exotic magnetic behaviour.
Here we report on the crystal structure, magnetic and thermodynamic properties
of the $A$-site-vacant double hydroxide perovskite MnSn(OH)$_6$. Despite
dominant antiferromagnetic interactions among Mn$^{2+}$ moments, evidenced by a
negative Curie-Weiss temperature, the lack of a sharp thermodynamic transition
down to 350$\,$mK implies the absence of long-range magnetic order. However, a
broad hump in the specific heat at 1.6$\,$K suggests short-range correlations.
Neutron diffraction at low temperatures confirms the presence of
three-dimensional (3D) antiferromagnetic correlations, manifested as diffuse
magnetic scattering with a correlation length $\xi = 24.66\,${\AA} and magnetic
propagation vectors $\mathbf{k}=(\frac{1}{2}\,\frac{1}{2}\,\frac{1}{2})$ and
$(0\,0.625\,0)$ at 20$\,$mK.; 85) A Cut-Based BAT-MCS Approach for Binary-State Network Reliability
  Assessment; The BAT-MCS is an integrated Monte Carlo simulation method (MCS) that
combines a binary adaptation tree algorithm (BAT) with a self-regulating
simulation mechanism. The BAT algorithm operates deterministically, while the
Monte Carlo simulation method is stochastic. By hybridizing these two
approaches, BAT-MCS successfully reduces variance, increases efficiency, and
improves the quality of its binary-state network reliability. However, it has
two notable weaknesses. First, the selection of the supervectors, sub-vectors
that form the core of BAT-MCS, is overly simplistic, potentially affecting
overall performance. Second, the calculation of the approximate reliability is
complicated, which limits its strength in reducing variance. In this study, a
new BAT-MCS called cBAT-MCS is proposed to enhance the performance of the
BAT-MCS. The approach reduces the complexity of MCS. Selecting the super-vector
based on a novel layer-cut approach can reduce both runtime and variance.
Extensive numerical experiments on large-scale binary-state network demonstrate
that the proposed new cBAT-MCS outperforms traditional MCS and original BAT-MCS
approaches in terms of computational efficiency and accuracy.; 86) Optimal Control of General Nonlocal Epidemic Models with Age and Space
  Structure; We analyze a class of general nonlinear epidemic models with age and space
structure, including a nonlocal infection term depending on age and space.
After establishing the well-posedness of the state partial differential
equation, we introduce a control parameter interpreted as a vaccination rate.
Under certain conditions, we show that an optimal control exists and how it can
be characterized by first-order optimality conditions. Finally, we present
numerical examples of the optimal control problems governed by these models.; 87) Detecting Metadata-Related Bugs in Enterprise Applications; When building enterprise applications (EAs) on Java frameworks (e.g.,
Spring), developers often configure application components via metadata (i.e.,
Java annotations and XML files). It is challenging for developers to correctly
use metadata, because the usage rules can be complex and existing tools provide
limited assistance. When developers misuse metadata, EAs become misconfigured,
which defects can trigger erroneous runtime behaviors or introduce security
vulnerabilities. To help developers correctly use metadata, this paper presents
(1) RSL -- a domain-specific language that domain experts can adopt to
prescribe metadata checking rules, and (2) MeCheck -- a tool that takes in RSL
rules and EAs to check for rule violations.
  With RSL, domain experts (e.g., developers of a Java framework) can specify
metadata checking rules by defining content consistency among XML files,
annotations, and Java code. Given such RSL rules and a program to scan, MeCheck
interprets rules as cross-file static analyzers, which analyzers scan Java
and/or XML files to gather information and look for consistency violations. For
evaluation, we studied the Spring and JUnit documentation to manually define 15
rules, and created 2 datasets with 115 open-source EAs. The first dataset
includes 45 EAs, and the ground truth of 45 manually injected bugs. The second
dataset includes multiple versions of 70 EAs. We observed that MeCheck
identified bugs in the first dataset with 100% precision, 96% recall, and 98%
F-score. It reported 156 bugs in the second dataset, 53 of which bugs were
already fixed by developers. Our evaluation shows that MeCheck helps ensure the
correct usage of metadata.; 88) Uncertainty-Aware Global-View Reconstruction for Multi-View Multi-Label
  Feature Selection; In recent years, multi-view multi-label learning (MVML) has gained popularity
due to its close resemblance to real-world scenarios. However, the challenge of
selecting informative features to ensure both performance and efficiency
remains a significant question in MVML. Existing methods often extract
information separately from the consistency part and the complementary part,
which may result in noise due to unclear segmentation. In this paper, we
propose a unified model constructed from the perspective of global-view
reconstruction. Additionally, while feature selection methods can discern the
importance of features, they typically overlook the uncertainty of samples,
which is prevalent in realistic scenarios. To address this, we incorporate the
perception of sample uncertainty during the reconstruction process to enhance
trustworthiness. Thus, the global-view is reconstructed through the graph
structure between samples, sample confidence, and the view relationship. The
accurate mapping is established between the reconstructed view and the label
matrix. Experimental results demonstrate the superior performance of our method
on multi-view datasets.; 89) Ilargi: a GPU Compatible Factorized ML Model Training Framework; The machine learning (ML) training over disparate data sources traditionally
involves materialization, which can impose substantial time and space overhead
due to data movement and replication. Factorized learning, which leverages
direct computation on disparate sources through linear algebra (LA) rewriting,
has emerged as a viable alternative to improve computational efficiency.
However, the adaptation of factorized learning to leverage the full
capabilities of modern LA-friendly hardware like GPUs has been limited, often
requiring manual intervention for algorithm compatibility. This paper
introduces Ilargi, a novel factorized learning framework that utilizes
matrix-represented data integration (DI) metadata to facilitate automatic
factorization across CPU and GPU environments without the need for costly
relational joins. Ilargi incorporates an ML-based cost estimator to
intelligently selects between factorization and materialization based on data
properties, algorithm complexity, hardware environments, and their
interactions. This strategy ensures up to 8.9x speedups on GPUs and achieves
over 20% acceleration in batch ML training workloads, thereby enhancing the
practicability of ML training across diverse data integration scenarios and
hardware platforms. To our knowledge, this work is the very first effort in
GPU-compatible factorized learning.; 90) Sequential HW-Aware Precoding: Over-the-air cancellation of HWI in
  Downlink Cell-Free Massive MIMO with Serial Fronthaul; This paper addresses the critical challenge of mitigating hardware
impairments (HWIs) in downlink cell-free massive MIMO (CF-mMIMO) networks while
ensuring computational scalability. We propose a novel sequential
hardware-aware (HW-aware) precoding technique that leverages the serial
fronthaul topology to perform over-the-air HWI cancellation. This approach
involves sequentially exchanging approximated user-perceived distortion
information among successive access points (APs) for over-the-air HWI
mitigation. Each AP independently computes its spatial multiplexing weights and
transmits signals that counteract the distortions introduced by the preceding
AP. We develop a problem formulation and present a closed-form solution for
this method. For performance evaluation, we study two reference methods taken
either from centralized massive MIMO literature (Tone Reservation [TR]) or
tailored for CF-mMIMO networks (PAPR-aware precoding), both focusing on
reducing the PAPR of OFDM signals in the downlink. Results indicate that the
sequential HW-aware approach achieves a substantial increase in spectral
efficiency (SE) in high-distortion scenarios, with an average SE increase
factor of 1.8 under severe distortions. Additionally, the proposed method,
which is executed locally, demonstrates better scalability, achieving a
reduction of up to 40\% and 72\% in the total number of complex multiplications
compared to the PAPR-aware and TR approaches, respectively. Finally, the
sequential HW-aware precoder offers high performance even when applied to
cost-effective APs with few antennas, presenting a promising and practical
solution for HWI compensation in CF-mMIMO systems with serial fronthaul.; 91) Combinatorial Optimization via LLM-driven Iterated Fine-tuning; We present a novel way to integrate flexible, context-dependent constraints
into combinatorial optimization by leveraging Large Language Models (LLMs)
alongside traditional algorithms. Although LLMs excel at interpreting nuanced,
locally specified requirements, they struggle with enforcing global
combinatorial feasibility. To bridge this gap, we propose an iterated
fine-tuning framework where algorithmic feedback progressively refines the
LLM's output distribution. Interpreting this as simulated annealing, we
introduce a formal model based on a ""coarse learnability"" assumption, providing
sample complexity bounds for convergence. Empirical evaluations on scheduling,
graph connectivity, and clustering tasks demonstrate that our framework
balances the flexibility of locally expressed constraints with rigorous global
optimization more effectively compared to baseline sampling methods. Our
results highlight a promising direction for hybrid AI-driven combinatorial
reasoning.; 92) Shoot-through layers in upright proton arcs unlock advantages in plan
  quality and range verification; Background and purpose: Upright proton therapy with compact delivery systems
has the potential to reduce costs for treatments but could also lead to
broadening of the beam penumbra. This study aims at combining upright static
proton arcs with additional layers of shoot-through (ST) protons to sharpen the
beam penumbra and improve plan quality for such systems. An additional
advantage of the method is that it provides a straightforward approach for
range verification.
  Methods: We examined various treatment plans for a virtual phantom: 3-beam
IMPT, static arc (Arc) with/without ST (Arc+ST), and with/without collimation
(+Coll). In the virtual phantom three different targets were utilized to study
the effect on conformity index (CI), homogeneity index (HI), robustness and
mean dose to the phantom volume. The phantom study was complemented with a
head-and-neck (H&N) patient case with a similar set of plans. A range
verification concept that determines residual ranges of the ST protons was
studied in simulated scenarios for the H&N case.
  Results: The Arc+ST plans show superior CI, HI and target robustness compared
to the Arc+Coll plans. For the Arc plans without ST, the collimated plans
perform better than the uncollimated plans. For Arc+ST, on the other hand,
collimation has little impact on CI, HI and robustness. However, a small
increase in the mean dose to the phantom volume is seen without collimation.
For the H&N case, similar improvements for Arc+ST can be seen with only a
marginal increase of the mean dose to the patient volume. The range
verification simulation shows that the method is suitable to detect range
errors.
  Conclusions: Combining proton arcs and ST layers can enhance compact upright
proton solutions by improving plan quality. It is also tailored for the
inclusion of a fast and straightforward residual range verification method.; 93) A novel microfluidic method to produce monodisperse micrometer bubbles; We present a novel microfluidic method to produce quasi-monodisperse bubbles
with diameters from tens to very few microns. A gaseous rivulet flows over the
shallow groove printed on a T-junction exit channel. The triple contact line
delimiting the rivulet is pinned to the groove edges. The rivulet breaks up
into bubbles much smaller than the exit channel. When operating under adequate
conditions, the flow transitions toward a singular mode where the rivulet
remains quasi-static and emits bubbles smaller than the groove width. This
allows the production of bubbles with diameters in the 3-5 $\mu$m range, which
is preferable for relevant therapeutical applications.; 94) User Guide to UVIT Data Reduction; This document provides a user guide for reducing UVIT data using CCDLAB.
While CCDLAB offers a straightforward data reduction work-flow, users may
encounter certain challenges that require additional guidance. This guide
provides instructions by addressing common issues related to key processing
steps, including WCS solutions and VIS drift tracking.; 95) The Second Moment of Sums of Hecke Eigenvalues I; Let $f$ be a Hecke cusp form of weight $k$ for $\mathrm{SL}_2(\mathbb{Z})$,
and let $(\lambda_f(n))_{n\geq1}$ denote its (suitably normalised) sequence of
Hecke eigenvalues. We compute the first and second moments of the sums
$S(x,f)=\sum_{x\leq n\leq 2x}\lambda_f(n)$, on average over forms $f$ of large
weight $k$, in the regime where the length of the sums $x$ is smaller than
$k^2$. We observe interesting transitions in the size of the sums when
$x\approx k$ and $x\approx k^2$. In subsequent work (part II), it will be shown
that once $x$ is larger than $k^2$ (where the latter transition occurs), the
average size of the sums $S(x,f)$ becomes dramatically smaller.; 96) A Multimodal Symphony: Integrating Taste and Sound through Generative AI; In recent decades, neuroscientific and psychological research has traced
direct relationships between taste and auditory perceptions. This article
explores multimodal generative models capable of converting taste information
into music, building on this foundational research. We provide a brief review
of the state of the art in this field, highlighting key findings and
methodologies. We present an experiment in which a fine-tuned version of a
generative music model (MusicGEN) is used to generate music based on detailed
taste descriptions provided for each musical piece. The results are promising:
according the participants' ($n=111$) evaluation, the fine-tuned model produces
music that more coherently reflects the input taste descriptions compared to
the non-fine-tuned model. This study represents a significant step towards
understanding and developing embodied interactions between AI, sound, and
taste, opening new possibilities in the field of generative AI. We release our
dataset, code and pre-trained model at: https://osf.io/xs5jy/.; 97) Relationship between the $\gamma-$ray variability and the pc-scale jet
  in the blazar 3C 454.3; 3C 454.3 is a flat spectrum radio quasar (FSRQ) known for its high
variability across the electromagnetic spectrum, showing structural and flux
variability in its pc-scale jet, and correlated variability among frequency
bands. This study aims to identify the structure, dynamics, and radiative
processes common to the innermost regions of the blazar 3C 454.3. We
investigate whether any jet component can be associated with $\gamma-$ray
emission and variability. We analyze the relationship between the variable
$\gamma-$ray emission and pc-scale jet properties in 3C 454.3 by combining
$\gamma-$ray data spanning twelve years with contemporaneous VLBA multi-epoch
images at 15 and 43 GHz. Spearman rank correlation tests are conducted to
determine if the flux variability of any jet component is associated with
$\gamma-$ray variability. Core emission at 43 and 15 GHz strongly correlates
with $\gamma-$ray emission. The 43 GHz core (Q0) contributes around 37$\%$ of
the observed $\gamma-$ray variability, while the 15 GHz core (K0) accounts for
30$\%$. A quasi-stationary component at 43 GHz, at a projected distance of 4.6
pc, correlates with the $\gamma-$ray flux, accounting for 20$\%$ of its
emission between 2016 and 2021. We found a mobile component (Q3 between 2010.18
and 2011.16) at 43 GHz with a projected distance between 0.8 and 2.3 pc and
apparent velocity of $\beta_{app} = 9.9 \pm 1.1$ c, accounting for
approximately 28% of the $\gamma-$ray emission. The observed simultaneous
variability in emission regions beyond the central parsec strongly suggests
synchrotron self-Compton (SSC) as the primary mechanism for $\gamma-$ray
production in these regions. Our findings demonstrate the existence of multiple
$\gamma-$ray emission regions within the blazar jet but also suggest that some
of these regions are non-stationary over time.; 98) Towards Multi-Stakeholder Evaluation of ML Models: A Crowdsourcing Study
  on Metric Preferences in Job-matching System; While machine learning (ML) technology affects diverse stakeholders, there is
no one-size-fits-all metric to evaluate the quality of outputs, including
performance and fairness. Using predetermined metrics without soliciting
stakeholder opinions is problematic because it leads to an unfair disregard for
stakeholders in the ML pipeline. In this study, to establish practical ways to
incorporate diverse stakeholder opinions into the selection of metrics for ML,
we investigate participants' preferences for different metrics by using
crowdsourcing. We ask 837 participants to choose a better model from two
hypothetical ML models in a hypothetical job-matching system twenty times and
calculate their utility values for seven metrics. To examine the participants'
feedback in detail, we divide them into five clusters based on their utility
values and analyze the tendencies of each cluster, including their preferences
for metrics and common attributes. Based on the results, we discuss the points
that should be considered when selecting appropriate metrics and evaluating ML
models with multiple stakeholders.; 99) Will artificial intelligence accelerate or delay the race between
  nuclear energy technology budgeting and net-zero emissions?; This study explores the impact of nuclear energy technology budgeting and
artificial intelligence on carbon dioxide (CO2) emissions in 20 OECD economies.
Unlike previous research that relied on conventional panel techniques, we
utilize the Method of Moment Quantile Regression panel data estimation
techniques. This approach provides quantile-specific insights while addressing
issues of endogeneity and heteroscedasticity, resulting in a more nuanced and
robust understanding of complex relationships. A novel aspect of this research
work is introducing the moderating effect of artificial intelligence on the
relationship between nuclear energy and CO2 emissions. The results found that
the direct impact of artificial intelligence on CO2 emissions is significant,
while the effect of nuclear energy technology budgeting is not. Additionally,
artificial intelligence moderates the relationship between nuclear energy
technology budgeting and CO2 emissions, aiding nuclear energy in reducing
carbon emissions across OECD countries. Our findings indicate that
transitioning to a low-carbon future is achievable by replacing fossil fuel
energy sources with increased integration of artificial intelligence to promote
nuclear energy technologies. This study demonstrates that energy innovations
can serve as effective climate-resilience strategies to mitigate the impacts of
climate change.; 100) Eukaryotes evade information storage-replication rate trade-off with
  endosymbiont assistance leading to larger genomes; Genome length varies widely among organisms, from compact genomes of
prokaryotes to vast and complex genomes of eukaryotes. In this study, we
theoretically identify the evolutionary pressures that may have driven this
divergence in genome length. We use a parameter-free model to study genome
length evolution under selection pressure to minimize replication time and
maximize information storage capacity. We show that prokaryotes tend to reduce
genome length, constrained by a single replication origin, while eukaryotes
expand their genomes by incorporating multiple replication origins. We propose
a connection between genome length and cellular energetics, suggesting that
endosymbiotic organelles, mitochondria and chloroplasts, evolutionarily
regulate the number of replication origins, thereby influencing genome length
in eukaryotes. We show that the above two selection pressures also lead to
strict equalization of the number of purines and their corresponding
base-pairing pyrimidines within a single DNA strand, known as Chagraff's second
parity rule, a hitherto unexplained observation in genomes of nearly all known
species. This arises from the symmetrization of replichore length, another
observation that has been shown to hold across species, which our model
reproduces. The model also reproduces other experimentally observed phenomena,
such as a general preference for deletions over insertions, and elongation and
high variance of genome lengths under reduced selection pressure for
replication rate, termed the C-value paradox. We highlight the possibility of
regulation of the firing of latent replication origins in response to cues from
the extracellular environment leading to the regulation of cell cycle rates in
multicellular eukaryotes.",0.0,0.8503449055347546
2412.11084,applied,2412.11084-pos2-1,"Biological identifications through DNA barcodes; Although much biological research depends upon species diagnoses, taxonomic expertise is collapsing.We are convinced that the sole prospect for a sustainable identification capability lies in construction of systems employ DNA sequences as taxon 'barcodes'.We establish mitochondrial gene cytochrome c oxidase I (COI) can serve core global bioidentification system animals.First, we demonstrate COI profiles, derived from low-density sampling higher categories, ordinarily assign newly analysed taxa to appropriate phylum or order.Second, species-level assignments be obtained by creating comprehensive profiles.A model profile, based analysis single individual each 200 closely allied lepidopterans, was 100% successful correctly identifying subsequent specimens.When fully developed, will provide reliable, cost-effective and accessible solution current problem identification.Its assembly also generate important new insights into diversification life rules molecular evolution.",2412.11084-pos1-1,"BarcodeBERT: Transformers for Biodiversity Analysis; Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https://github.com/Kari-Genomics-Lab/BarcodeBERT",53,"['2', '7', '30', '53', '54', '38', '25', '16', '97', '24']","The main paper focuses on the use of DNA barcodes for reliable species identification, which is crucial for biodiversity studies. The best candidate is paper 2, which investigates the genetic basis of traits in date palm fruit, aligning well with the theme of genetic analysis related to species identification. It introduces a methodology that could be adapted for similar studies in other species using DNA barcoding, making it novel and useful. The subsequent papers also explore relevant genetic topics but either lack a direct connection to the theme of biodiversity and species identification or are less applicable in this context.","1) $L_2$-approximation using median lattice algorithms; In this paper, we consider $L_2$-approximation of functions in a weighted
Korobov space. We consider a median algorithm, which is related to median
integration rules, that have recently gained a lot of interest in the theory of
quasi-Monte Carlo methods. In particular, we will use so-called lattice rules
as the underlying integration rules. As we will show in the present paper, we
can obtain a convergence rate that is arbitrarily close to optimal in terms of
the number of evaluations needed of the function to be approximated.; 2) Genomic Analysis of Date Palm Fruit Size Traits and Identification of
  Candidate Genes through GWAS; The commercial value of economically significant fruits, including date palm
fruit (dates), is influenced by various factors, such as biochemical
composition and morphological features like size, shape, and visual appearance,
which are key determinants of their quality and market value. Dates are
typically consumed at the dry stage (Tamar), during which they exhibit a wide
range of physical characteristics, such as color, length, weight, and skin
appearance. Understanding the genetic basis of these traits is crucial for
improving crop quality and breeding new cultivars. In this study, we integrated
a genome dataset from highly diverse date cultivars with phenotypes of dry
fruit such as length, width, area, and weight, identifying multiple significant
genetic loci (SNPs) associated with these traits. We also identified candidate
genes located near the associated SNPs that are involved in biological
processes such as cell differentiation, proliferation, growth, and the
regulation of signalling pathways for growth regulators like auxin and abscisic
acid, as observed in other plants. Gene expression analysis reveals that many
of these genes are highly expressed in the early stage of fruit development
when the fruit attains its maximum size and weight. These findings will enhance
our understanding of genetic determinants of fruit size particularly at the
commercially important Tamar stage.; 3) Magnon thermal conductivity in multiferroics with spin cycloids; Multiferroic materials, characterized by the occurrence of two or more
ferroic properties, hold potential in future technological applications and
also exhibit intriguing phenomena caused by the interplay of multiple orders.
One such example is the formation of spin cycloid structures within
multiferroic materials, which we investigate in this work by focusing on their
magnon excitations and transport based on a general multiferroic Hamiltonian
with an antiferromagnetic order. More specifically, we identify the ground
state and explore the dynamics of magnon modes, revealing distinct in-plane and
out-of-plane modes with anisotropic dispersion relations.The magnon modes
include a massless excitation, known as the Goldstone boson, originating from
the spontaneous breaking of the translational symmetry by the formation of the
cycloid structures. By employing the Boltzmann transport formalism, the
magnonic thermal conductivity with spin cycloids and low-temperature
anisotropic behaviors is discussed. This work provides pathways to envision the
spin-textured multiferroics, which may serve as a fertile ground to look for
novel thermal and spin transport with the rich interplay of quasiparticles such
as magnons and phonons.; 4) An Isometric Embedding of the $\ell^\infty$ product space of two bounded
  subspaces of the Gromov-Hausdorff Space into the Gromov-Hausdorff Space; In this paper, we prove the $\ell^\infty$ product space of two bounded
subspaces of the Gromov-Hausdorff space can be isometrically embedded into the
Gromov-Hausdorff space.; 5) Exceptional Eigenvalues and Braak's Conjecture in the Quantum Rabi Model; The spectrum of the quantum Rabi model can be separated as regular
eigenvalues that need to be computed numerically and exceptional eigenvalues,
that match the energies of a shifted quantum harmonic oscillator. Exceptional
values can be separated further as Juddean, if they occur under certain
algebraic conditions, and non-Juddean, if they obey more elusive transcendental
conditions. Here, we show that simple assumptions on these conditions imply and
extend Braak's conjecture on the distribution of the quantum Rabi spectrum.; 6) Progressive Document-level Text Simplification via Large Language Models; Research on text simplification has primarily focused on lexical and
sentence-level changes. Long document-level simplification (DS) is still
relatively unexplored. Large Language Models (LLMs), like ChatGPT, have
excelled in many natural language processing tasks. However, their performance
on DS tasks is unsatisfactory, as they often treat DS as merely document
summarization. For the DS task, the generated long sequences not only must
maintain consistency with the original document throughout, but complete
moderate simplification operations encompassing discourses, sentences, and
word-level simplifications. Human editors employ a hierarchical complexity
simplification strategy to simplify documents. This study delves into
simulating this strategy through the utilization of a multi-stage collaboration
using LLMs. We propose a progressive simplification method (ProgDS) by
hierarchically decomposing the task, including the discourse-level,
topic-level, and lexical-level simplification. Experimental results demonstrate
that ProgDS significantly outperforms existing smaller models or direct
prompting with LLMs, advancing the state-of-the-art in the document
simplification task.; 7) Find Central Dogma Again: Leveraging Multilingual Transfer in Large
  Language Models; In recent years, large language models (LLMs) have achieved state-of-the-art
results in various biological sequence analysis tasks, such as sequence
classification, structure prediction, and function prediction. Similar to
advancements in AI for other scientific fields, deeper research into biological
LLMs has begun to focus on using these models to rediscover important existing
biological laws or uncover entirely new patterns in biological sequences. This
study leverages GPT-like LLMs to utilize language transfer capabilities to
rediscover the genetic code rules of the central dogma. In our experimental
design, we transformed the central dogma into a binary classification problem
of aligning DNA sequences with protein sequences, where positive examples are
matching DNA and protein sequences, and negative examples are non-matching
pairs. We first trained a GPT-2 model from scratch using a dataset comprising
protein sequences, DNA sequences, and sequences from languages such as English
and Chinese. Subsequently, we fine-tuned the model using the natural language
sentences similarity judgment dataset from PAWS-X. When tested on a dataset for
DNA and protein sequence alignment judgment, the fine-tuned model achieved a
classification accuracy of 81%. The study also analyzed factors contributing to
this zero-shot capability, including model training stability and types of
training data. This research demonstrates that LLMs can, through the transfer
of natural language capabilities and solely relying on the analysis of
sequences themselves, rediscover the central dogma without prior knowledge of
it. This study bridges natural language and genetic language, opening a new
door for AI-driven biological research.; 8) Origin of $\alpha$-satellite repeat arrays from mitochondrial molecular
  fossils -- sequential insertion, expansion, and evolution in the nuclear
  genome; Alpha satellite DNA is large tandem arrays of 150-400 bp units, and its
origin remains an evolutionary mystery. In this research, we identified 1,545
alpha-satellite-like (SatL) repeat units in the nuclear genome of jewel wasp
Nasonia vitripennis. Among them, thirty-nine copies of SatL were organized in
two palindromic arrays in mitochondria, resulting in a 50% increase in the
genome size. Strikingly, genomic neighborhood analyses of 1,516 nuclear SatL
repeats revealed that they are located in NuMT (nuclear mitochondrial DNA)
regions, and SatL phylogeny matched perfectly with mitochondrial genes and NuMT
pseudogenes. These results support that SatL arrays originated from ten
independent mitochondria insertion events into the nuclear genome within the
last 500,000 years, after divergence from its sister species N. giraulti.
Dramatic repeat GC-percent elevation (from 33.9% to 50.4%) is a hallmark of
rapid SatL sequence evolution in mitochondria due to GC-biased gene conversion
facilitated by the palindromic sequence pairing of the two mitochondrial SatL
arrays. The nuclear SatL repeat arrays underwent substantial copy number
expansion, from 12-15 (SatL1) to over 400 copies (SatL4). The oldest SatL4B
array consists of four types of repeat units derived from deletions in the
AT-rich region of ancestral repeats, and complex high-order structures have
evolved through duplications. We also discovered similar repeat insertions into
the nuclear genome of Muscidifurax, suggesting this mechanism can be common in
insects. This is the first report of the mitochondrial origin of nuclear
satellite sequences, and our findings shed new light on the origin and
evolution of satellite DNA.; 9) Intent-Aware Self-Correction for Mitigating Social Biases in Large
  Language Models; Self-Correction based on feedback improves the output quality of Large
Language Models (LLMs). Moreover, as Self-Correction functions like the slow
and conscious System-2 thinking from cognitive psychology's perspective, it can
potentially reduce LLMs' social biases. LLMs are sensitive to contextual
ambiguities and inconsistencies; therefore, explicitly communicating their
intentions during interactions when applying Self-Correction for debiasing is
crucial. In this study, we demonstrate that clarifying intentions is essential
for effectively reducing biases in LLMs through Self-Correction. We divide the
components needed for Self-Correction into three parts: instruction, response,
and feedback, and clarify intentions at each component. We incorporate an
explicit debiasing prompt to convey the intention of bias mitigation from the
instruction for response generation. In the response, we use Chain-of-Thought
(CoT) to clarify the reasoning process. In the feedback, we define evaluation
aspects necessary for debiasing and propose clear feedback through multi-aspect
critiques and scoring. Through experiments, we demonstrate that self-correcting
CoT responses obtained from a debiasing prompt based on multi-aspect feedback
can reduce biased responses more robustly and consistently than the baselines.
We also find the variation in debiasing efficacy when using models with
different bias levels or separating models for response and feedback
generation.; 10) Gandalf the Red: Adaptive Security for LLMs; Current evaluations of defenses against prompt attacks in large language
model (LLM) applications often overlook two critical factors: the dynamic
nature of adversarial behavior and the usability penalties imposed on
legitimate users by restrictive defenses. We propose D-SEC (Dynamic Security
Utility Threat Model), which explicitly separates attackers from legitimate
users, models multi-step interactions, and expresses the security-utility in an
optimizable form. We further address the shortcomings in existing evaluations
by introducing Gandalf, a crowd-sourced, gamified red-teaming platform designed
to generate realistic, adaptive attack. Using Gandalf, we collect and release a
dataset of 279k prompt attacks. Complemented by benign user data, our analysis
reveals the interplay between security and utility, showing that defenses
integrated in the LLM (e.g., system prompts) can degrade usability even without
blocking requests. We demonstrate that restricted application domains,
defense-in-depth, and adaptive defenses are effective strategies for building
secure and useful LLM applications.; 11) High-Fidelity Novel View Synthesis via Splatting-Guided Diffusion; Despite recent advances in Novel View Synthesis (NVS), generating
high-fidelity views from single or sparse observations remains a significant
challenge. Existing splatting-based approaches often produce distorted geometry
due to splatting errors. While diffusion-based methods leverage rich 3D priors
to achieve improved geometry, they often suffer from texture hallucination. In
this paper, we introduce SplatDiff, a pixel-splatting-guided video diffusion
model designed to synthesize high-fidelity novel views from a single image.
Specifically, we propose an aligned synthesis strategy for precise control of
target viewpoints and geometry-consistent view synthesis. To mitigate texture
hallucination, we design a texture bridge module that enables high-fidelity
texture generation through adaptive feature fusion. In this manner, SplatDiff
leverages the strengths of splatting and diffusion to generate novel views with
consistent geometry and high-fidelity details. Extensive experiments verify the
state-of-the-art performance of SplatDiff in single-view NVS. Additionally,
without extra training, SplatDiff shows remarkable zero-shot performance across
diverse tasks, including sparse-view NVS and stereo video conversion.; 12) Two-loop corrections to QCD $\theta$ angle from evanescent operator in
  the BMHV scheme; We compute the radiatively-induced QCD $\theta$ angle at two-loop level using
dimensional regularization with the BMHV scheme in order to investigate
$CP$-violating contributions of evanescent operators. When one considers the
Lagrangian in $d$-dimensional space-time instead of four dimensions in
dimensional regularization, evanescent operators that break the chiral symmetry
are induced. Consequently, $T$-odd and $P$-odd fermion loops in the BMHV scheme
generates evanescent contributions to the QCD $\theta$ angle. We carefully
classify the evanescent contributions into two types: one originating from the
evanescent operators at one-loop level and the other directly produced by
two-loop calculations. We show that re-parameterizing the fundamental
parameters can remove these unphysical contributions from the
renormalization-group equation for the QCD $\theta$ angle. We also discuss the
rephasing invariance of the radiative correction to the QCD $\theta$ angle in
the BMHV scheme.; 13) Smeared $R$-ratio in isospin symmetric QCD with Low Mode Averaging; Low Mode Average (LMA) is a technique to improve the quality of the
signal-to-noise ratio in the long time separation of Euclidean correlation
functions. We report on its beneficial impact in computing the vector-vector
light connected two-point correlation functions and derived physical quantities
in the mixed action lattice setup adopted by ETM collaboration. We focus on
preliminary results of the computation within isospin symmetric QCD (isoQCD) of
the $R$-ratio smeared with Gaussian kernels of widths down to $\sigma\sim250$
MeV, which is enough to appreciate the $\rho$ resonance around 770 MeV, using
the Hansen-Lupo-Tantatlo (HLT) spectral-density reconstruction method.; 14) Embedding and compact embedding between Bergman and Hardy spaces; For Hardy spaces and weighted Bergman spaces on the open unit ball in
${\mathbb C}^n$, we determine exactly when $A^p_\alpha\subset H^q$ or
$H^p\subset A^q_\alpha$, where $0<q<\infty$, $0<p<\infty$, and
$-\infty<\alpha<\infty$. For each such inclusion we also determine exactly when
it is a compact embedding. Although some special cases were known before, we
are able to completely cover all possible cases here. We also introduce a new
notion called {\it tight fitting} and formulate a conjecture in terms of it,
which places several prominent known results about contractive embeddings in
the same framework.; 15) Social media polarization during conflict: Insights from an ideological
  stance dataset on Israel-Palestine Reddit comments; In politically sensitive scenarios like wars, social media serves as a
platform for polarized discourse and expressions of strong ideological stances.
While prior studies have explored ideological stance detection in general
contexts, limited attention has been given to conflict-specific settings. This
study addresses this gap by analyzing 9,969 Reddit comments related to the
Israel-Palestine conflict, collected between October 2023 and August 2024. The
comments were categorized into three stance classes: Pro-Israel, Pro-Palestine,
and Neutral. Various approaches, including machine learning, pre-trained
language models, neural networks, and prompt engineering strategies for open
source large language models (LLMs), were employed to classify these stances.
Performance was assessed using metrics such as accuracy, precision, recall, and
F1-score. Among the tested methods, the Scoring and Reflective Re-read prompt
in Mixtral 8x7B demonstrated the highest performance across all metrics. This
study provides comparative insights into the effectiveness of different models
for detecting ideological stances in highly polarized social media contexts.
The dataset used in this research is publicly available for further exploration
and validation.; 16) Bizard: A Community-Driven Platform for Accelerating and Enhancing
  Biomedical Data Visualization; Bizard is a novel visualization code repository designed to simplify data
analysis in biomedical research. It integrates diverse visualization codes,
facilitating the selection and customization of optimal visualization methods
for specific research needs. The platform offers a user-friendly interface with
advanced browsing and filtering mechanisms, comprehensive tutorials, and
interactive forums to enhance knowledge exchange and innovation. Bizard's
collaborative model encourages continuous refinement and expansion of its
functionalities, making it an indispensable tool for advancing biomedical data
visualization and analytical methodologies. By leveraging Bizard's resources,
researchers can enhance data visualization skills, drive methodological
advancements, and improve data interpretation standards, ultimately fostering
the development of precision medicine and personalized therapeutic
interventions.Bizard can be accessed from http://genaimed.org/Bizard/.; 17) Scalable Language Models with Posterior Inference of Latent Thought
  Vectors; We propose a novel family of language models, Latent-Thought Language Models
(LTMs), which incorporate explicit latent thought vectors that follow an
explicit prior model in latent space. These latent thought vectors guide the
autoregressive generation of ground tokens through a Transformer decoder.
Training employs a dual-rate optimization process within the classical
variational Bayes framework: fast learning of local variational parameters for
the posterior distribution of latent vectors, and slow learning of global
decoder parameters. Empirical studies reveal that LTMs possess additional
scaling dimensions beyond traditional LLMs, yielding a structured design space.
Higher sample efficiency can be achieved by increasing training compute per
token, with further gains possible by trading model size for more inference
steps. Designed based on these scaling properties, LTMs demonstrate superior
sample and parameter efficiency compared to conventional autoregressive models
and discrete diffusion models. They significantly outperform these counterparts
in validation perplexity and zero-shot language modeling. Additionally, LTMs
exhibit emergent few-shot in-context reasoning capabilities that scale with
model and latent size, and achieve competitive performance in conditional and
unconditional text generation.; 18) Influence of departures from LTE on determinations of the sulfur
  abundances in A-K type stars; The influence of departures from local thermodynamic equilibrium (LTE) on
neutral sulfur lines is considered. A grid of corrections is proposed to take
into account the influence of departures from LTE for neutral sulfur lines in
the visible and infrared spectral regions, including the H-band. The grid is
calculated using the atomic model of sulfur incorporating the most up-to-date
collision rates with electrons and hydrogen. The inclusion of levels and
transitions of ionized sulfur in the atomic model made it possible to expand
the range of effective temperatures of stellar photospheres in the grid up to
10000 K. The atomic model was tested in determining the sulfur abundance of 13
stars and showed its adequacy in a wide range of fundamental stellar
parameters. In the spectra of all test stars, the sulfur lines are fitted with
similar abundances of the element, regardless of the degree of influence of the
effects of deviation from LTE on a particular spectral line. For lines of
several multiplets, the wavelengths and oscillator strengths were refined. A
list of S I lines recommended for determining sulfur abundance has been
created.; 19) Punch Out Model Synthesis: A Stochastic Algorithm for Constraint Based
  Tiling Generation; As an artistic aid in tiled level design, Constraint Based Tiling Generation
(CBTG) algorithms can help to automatically create level realizations from a
set of tiles and placement constraints. Merrell's Modify in Blocks Model
Synthesis (MMS) and Gumin's Wave Function Collapse (WFC) have been proposed as
Constraint Based Tiling Generation (CBTG) algorithms that work well for many
scenarios but have limitations in problem size, problem setup and solution
biasing. We present Punch Out Model Synthesis (POMS), a Constraint Based Tiling
Generation algorithm, that can handle large problem sizes, requires minimal
assumptions for setup and can help mitigate solution biasing. POMS attempts to
resolve indeterminate grid regions by trying to progressively realize
sub-blocks, performing a stochastic boundary erosion on previously resolved
regions should sub-block resolution fail. We highlight the results of running a
reference implementation on different tile sets and discuss a tile correlation
length, implied by the tile constraints, and its role in choosing an
appropriate block size to aid POMS in successfully finding grid realizations.; 20) iFlame: Interleaving Full and Linear Attention for Efficient Mesh
  Generation; This paper propose iFlame, a novel transformer-based network architecture for
mesh generation. While attention-based models have demonstrated remarkable
performance in mesh generation, their quadratic computational complexity limits
scalability, particularly for high-resolution 3D data. Conversely, linear
attention mechanisms offer lower computational costs but often struggle to
capture long-range dependencies, resulting in suboptimal outcomes. To address
this trade-off, we propose an interleaving autoregressive mesh generation
framework that combines the efficiency of linear attention with the expressive
power of full attention mechanisms. To further enhance efficiency and leverage
the inherent structure of mesh representations, we integrate this interleaving
approach into an hourglass architecture, which significantly boosts efficiency.
Our approach reduces training time while achieving performance comparable to
pure attention-based models. To improve inference efficiency, we implemented a
caching algorithm that almost doubles the speed and reduces the KV cache size
by seven-eighths compared to the original Transformer. We evaluate our
framework on ShapeNet and Objaverse, demonstrating its ability to generate
high-quality 3D meshes efficiently. Our results indicate that the proposed
interleaving framework effectively balances computational efficiency and
generative performance, making it a practical solution for mesh generation. The
training takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces
on Objaverse.; 21) Expedited Noise Spectroscopy of Transmon Qubits; There has been tremendous progress in the physical realization of quantum
computing hardware in recent times, bringing us closer than ever before to
realizing the promise of quantum computing. However, noise continues to pose a
crucial challenge when it comes to scaling up present day quantum processors.
While decoherence limits the qubits ability to store information for long
periods in the presence of uncontrollable noise sources, the erroneous
implementation of control methods for state preparation and measurements leads
to faulty implementations of quantum circuits. Conventional noise spectroscopy
protocols can characterize and model environmental noise but are usually
resource intensive and lengthy. Moreover, the underlying noise can vary in
nature over time, making noise profile extraction futile as this new
information cannot be harnessed to improve quantum error correction or
dynamical decoupling protocols. In this work, we address this challenge using a
machine learning-based methodology to quickly extract noise spectra of multiple
qubits and demonstrate a possible noise mitigation strategy. The procedure
involves implementing undemanding dynamical decoupling sequences to record
coherence decays of the investigated qubits and then predict the underlying
noise spectra with the help of a convolution neural network pre-trained on a
synthetic dataset. While our protocol is virtually hardware-agnostic, we
validate its effectiveness using superconducting qubits available on the IBM
Quantum platform. We further use these rapidly obtained, yet accurate, noise
spectra to design bespoke dynamic decoupling sequences and perform
time-dependent noise spectroscopy.; 22) Asymmetric simple exclusion process on a random comb: Transport
  properties in the stationary state; We address the dynamics of interacting particles on a disordered lattice
formed by a random comb. The dynamics comprises that of the asymmetric simple
exclusion process, whereby motion to nearest-neighour sites that are empty are
more likely in the direction of a bias than in the opposite direction. The
random comb comprises a backbone lattice from each site of which emanates a
branch with a random number of sites. The backbone and the branches run in the
direction of the bias. The number of branch sites or alternatively the branch
lengths are sampled independently from a common distribution, specifically, an
exponential distribution. The system relaxes at long times into a
nonequilibrium stationary state. We analyse the stationary-state density of
sites across the random comb by employing a mean-field approximation. Further,
we explore the transport properties, in particular, the stationary-state drift
velocity of particles along the backbone. We show that in the stationary state,
the density is uniform along the backbone and nonuniform along the branches,
decreasing monotonically from the free-end of a branch to its intersection with
the backbone. On the other hand, the drift velocity as a function of the bias
strength has a non-monotonic dependence, first increasing and then decreasing
with increase of bias. However, remarkably, as the particle density increases,
the dependence becomes no more non-monotonic. We understand this effect as a
consequence of an interplay between biased hopping and hard-core exclusion,
whereby sites towards the free end of the branches remain occupied for long
times and become effectively non-participatory in the dynamics of the system.
This results in an effective reduction of the branch lengths and a motion of
the particles that takes place primarily along the backbone.; 23) Exploring Quantum Control Landscape and Solution Space Complexity
  through Dimensionality Reduction & Optimization Algorithms; Understanding the quantum control landscape (QCL) is important for designing
effective quantum control strategies. In this study, we analyze the QCL for a
single two-level quantum system (qubit) using various control strategies. We
employ Principal Component Analysis (PCA), to visualize and analyze the QCL for
higher dimensional control parameters. Our results indicate that dimensionality
reduction techniques such as PCA, can play an important role in understanding
the complex nature of quantum control in higher dimensions. Evaluations of
traditional control techniques and machine learning algorithms reveal that
Genetic Algorithms (GA) outperform Stochastic Gradient Descent (SGD), while
Q-learning (QL) shows great promise compared to Deep Q-Networks (DQN) and
Proximal Policy Optimization (PPO). Additionally, our experiments highlight the
importance of reward function design in DQN and PPO demonstrating that using
immediate reward results in improved performance rather than delayed rewards
for systems with short time steps. A study of solution space complexity was
conducted by using Cluster Density Index (CDI) as a key metric for analyzing
the density of optimal solutions in the landscape. The CDI reflects cluster
quality and helps determine whether a given algorithm generates regions of high
fidelity or not. Our results provide insights into effective quantum control
strategies, emphasizing the significance of parameter selection and algorithm
optimization.; 24) Continual Learning-Aided Super-Resolution Scheme for Channel
  Reconstruction and Generalization in OFDM Systems; Channel reconstruction and generalization capability are of equal importance
for developing channel estimation schemes within deep learning (DL) framework.
In this paper, we exploit a novel DL-based scheme for efficient OFDM channel
estimation where the neural networks for channel reconstruction and
generalization are respectively designed. For the former, we propose a
dual-attention-aided super-resolution neural network (DA-SRNN) to map the
channels at pilot positions to the whole time-frequency channels. Specifically,
the channel-spatial attention mechanism is first introduced to sequentially
infer attention maps along two separate dimensions corresponding to two types
of underlying channel correlations, and then the lightweight SR module is
developed for efficient channel reconstruction. For the latter, we introduce
continual learning (CL)-aided training strategies to make the neural network
adapt to different channel distributions. Specifically, the elastic weight
consolidation (EWC) is introduced as the regularization term in regard to loss
function of channel reconstruction, which can constrain the direction and space
of updating the important weights of neural networks among different channel
distributions. Meanwhile, the corresponding training process is provided in
detail. By evaluating under 3rd Generation Partnership Project (3GPP) channel
models, numerical results verify the superiority of the proposed channel
estimation scheme with significantly improved channel reconstruction and
generalization performance over counterparts.; 25) Genotype-to-Phenotype Prediction in Rice with High-Dimensional Nonlinear
  Features; Genotype-to-Phenotype prediction can promote advances in modern genomic
research and crop improvement, guiding precision breeding and genomic
selection. However, high-dimensional nonlinear features often hinder the
accuracy of genotype-to-phenotype prediction by increasing computational
complexity. The challenge also limits the predictive accuracy of traditional
approaches. Therefore, effective solutions are needed to improve the accuracy
of genotype-to-phenotype prediction. In our paper, we propose MLFformer.
MLFformer is a Transformer-based architecture that incorporates the Fast
Attention mechanism and a multilayer perceptron module to handle
high-dimensional nonlinear features. In MLFformer, the Fast Attention mechanism
is utilized to handle computational complexity and enhance processing
efficiency. In addition, the MLP structure further captures high-dimensional
nonlinear features. Through experiments, the results show that MLFformer
reduces the average MAPE by 7.73% compared to the vanilla Transformer. In
univariate and multivariate prediction scenarios, MLFformer achieves the best
predictive performance among all compared models.; 26) The Redundancy of Non-Singular Channel Simulation; Channel simulation is an alternative to quantization and entropy coding for
performing lossy source coding. Recently, channel simulation has gained
significant traction in both the machine learning and information theory
communities, as it integrates better with machine learning-based data
compression algorithms and has better rate-distortion-perception properties
than quantization. As the practical importance of channel simulation increases,
it is vital to understand its fundamental limitations. Recently, Sriramu and
Wagner provided an almost complete characterisation of the redundancy of
channel simulation algorithms. In this paper, we complete this
characterisation. First, we significantly extend a result of Li and El Gamal,
and show that the redundancy of any instance of a channel simulation problem is
lower bounded by the channel simulation divergence. Second, we give two proofs
that the asymptotic redundancy of simulating iid non-singular channels is
lower-bounded by $1/2$: one using a direct approach based on the asymptotic
expansion of the channel simulation divergence and one using large deviations
theory.; 27) Towards End-to-End Application Slicing in Multi-access Edge Computing
  systems: Architecture Discussion and Proof-of-Concept; Network slicing is one of the most critical 5G pillars. It allows for sharing
a 5G infrastructure among different tenants leading to improved service
customisation and increased operators' revenues. Concurrently, introducing the
Multi-access Edge Computing (MEC) into 5G to support time-critical applications
raises the need to integrate this distributed computing infrastructure to the
5G network slicing framework. Indeed, end-to-end latency guarantees require the
end-to-end management of slice resources. For this purpose, after discussing
the main gaps in the state-of-the-art with regards to such an objective, we
propose a novel slicing architecture that enables the management and
orchestration of slice segments that span over all the domains of an end-to-end
application service, including the MEC. We also show how this general
management architecture can be instantiated into a multi-tenant MEC
infrastructure. A preliminary implementation of the proposed architecture
focusing on the MEC domain is also provided, together with performance tests to
validate the feasibility and efficacy of our design approach.; 28) Angle structures on pseudo 3-manifolds; It is still not known whether a hyperbolic 3-manifold admits an angle
structure or not. We consider angle structures with area-curvature on
triangulated pseudo 3-manifolds M in this article. A suficient and necessary
condition for the existence of such angle structures is established. As a
consequence, any compact hyperbolic 3-manifold with totally geodesic boundary
admits an angle structure. We also derive certain topological information of M
from the existence of such angle structures.; 29) The Silent Majority: Demystifying Memorization Effect in the Presence of
  Spurious Correlations; Machine learning models often rely on simple spurious features -- patterns in
training data that correlate with targets but are not causally related to them,
like image backgrounds in foreground classification. This reliance typically
leads to imbalanced test performance across minority and majority groups. In
this work, we take a closer look at the fundamental cause of such imbalanced
performance through the lens of memorization, which refers to the ability to
predict accurately on \textit{atypical} examples (minority groups) in the
training set but failing in achieving the same accuracy in the testing set.
This paper systematically shows the ubiquitous existence of spurious features
in a small set of neurons within the network, providing the first-ever evidence
that memorization may contribute to imbalanced group performance. Through three
experimental sources of converging empirical evidence, we find the property of
a small subset of neurons or channels in memorizing minority group information.
Inspired by these findings, we articulate the hypothesis: the imbalanced group
performance is a byproduct of ``noisy'' spurious memorization confined to a
small set of neurons. To further substantiate this hypothesis, we show that
eliminating these unnecessary spurious memorization patterns via a novel
framework during training can significantly affect the model performance on
minority groups. Our experimental results across various architectures and
benchmarks offer new insights on how neural networks encode core and spurious
knowledge, laying the groundwork for future research in demystifying robustness
to spurious correlation.; 30) Eukaryotes evade information storage-replication rate trade-off with
  endosymbiont assistance leading to larger genomes; Genome length varies widely among organisms, from compact genomes of
prokaryotes to vast and complex genomes of eukaryotes. In this study, we
theoretically identify the evolutionary pressures that may have driven this
divergence in genome length. We use a parameter-free model to study genome
length evolution under selection pressure to minimize replication time and
maximize information storage capacity. We show that prokaryotes tend to reduce
genome length, constrained by a single replication origin, while eukaryotes
expand their genomes by incorporating multiple replication origins. We propose
a connection between genome length and cellular energetics, suggesting that
endosymbiotic organelles, mitochondria and chloroplasts, evolutionarily
regulate the number of replication origins, thereby influencing genome length
in eukaryotes. We show that the above two selection pressures also lead to
strict equalization of the number of purines and their corresponding
base-pairing pyrimidines within a single DNA strand, known as Chagraff's second
parity rule, a hitherto unexplained observation in genomes of nearly all known
species. This arises from the symmetrization of replichore length, another
observation that has been shown to hold across species, which our model
reproduces. The model also reproduces other experimentally observed phenomena,
such as a general preference for deletions over insertions, and elongation and
high variance of genome lengths under reduced selection pressure for
replication rate, termed the C-value paradox. We highlight the possibility of
regulation of the firing of latent replication origins in response to cues from
the extracellular environment leading to the regulation of cell cycle rates in
multicellular eukaryotes.; 31) Competing-risk Weibull survival model with multiple causes; The failure of a system can result from the simultaneous effects of multiple
causes, where assigning a specific cause may be inappropriate or unavailable.
Examples include contributing causes of death in epidemiology and the aetiology
of neurodegenerative diseases like Alzheimer's. We propose a parametric Weibull
accelerated failure time model for multiple causes, incorporating a
data-driven, individualized, and time-varying winning probability (relative
importance) matrix. Using maximum likelihood estimation and the
expectation-maximization (EM) algorithm, our approach enables simultaneous
estimation of regression coefficients and relative cause importance, ensuring
consistency and asymptotic normality. A simulation study and an application to
Alzheimer's disease demonstrate its effectiveness in addressing cause-mixture
problems and identifying informative biomarker combinations, with comparisons
to Weibull and Cox proportional hazards models.; 32) Network effects and incumbent response to entry threats: empirical
  evidence from the airline industry; I investigate how incumbents in the U.S. airline industry respond to
threatened and actual route entry by Southwest Airlines. I use a two-way fixed
effects and event study approach, and the latest available data from 1999-2022,
to identify a firm's price and quantity response. I find evidence that
incumbents cut fares preemptively (post-entry) by 6-8% (16-18%) although the
significance, pattern, and timing of the preemptive cuts are quite different to
Goolsbee and Syverson's (2008) earlier results. Incumbents increase capacity
preemptively by 10-40%, up to six quarters before the entry threat is
established, and by 27-46% post-entry. My results suggest a clear shift in
firms' strategic response from price to quantity. I also investigate the impact
of an incumbent's network structure on its preemptive and post-entry behaviour.
While the results on price are unclear, a firm's post-entry capacity reaction
depends strongly on its global network structure as well as the local
importance (centrality) of the route.; 33) The evolution of metallicity gradients in galaxies from cosmological
  simulations; Tracing the cosmic path of galaxies requires an understanding of their
chemical enrichment and merging histories. One of the most important
constraints is the internal structure of galaxies, notably the internal
distribution of elements acting as fossils in extra-galactic archaeology. Using
our cosmological chemodynamical simulations, which include all relevant
physical processes and the latest nucleosynthesis yields, we investigate the
evolution of radial metallicity gradients of stellar populations and the
interstellar medium within each galaxy. This work explores the role of
supernova feedback on the metallicity gradients by comparing three feedback
models, ejecting energy in thermal, stochastic and mechanical forms. At $z=0$,
the mechanical feedback model produces the gradient--mass relations of stars
and gas both in excellent agreement with observations; gradients are the
steepest at intermediate-mass ($M_*\sim10^{10}M_\odot$) and become flatter in
massive galaxies probably by major mergers. For each model, we predict similar
gradient--mass relations up to $z=4$ and find that the mechanical feedback
model gives flatter gradients of both stars and gas for lower-mass galaxies
($M_*<10^{10}M_\odot$) possibly due to the suppression of star formation and
metal ejection by stellar feedback. With all feedback models, most galaxies
have negative gas-phase metallicity gradients up to $z=5$, suggesting an
inside-out growth, which is consistent with other cosmological simulations but
not with recent observations at $z\sim1$--2.5. We find a mild redshift
evolution of gradients up to $z=4$, while there seems to be an evolutionary
transition at $z=5$ where the metallicity gradients become steep for gas and
stars.; 34) From Complexification to Self-Similarity: New Aspects of Quantum
  Criticality; Quantum phase transitions are a fascinating area of condensed matter physics.
The extension through complexification not only broadens the scope of this
field but also offers a new framework for understanding criticality and its
statistical implications. This mini review provides a concise overview of
recent developments in complexification, primarily covering finite temperature
and equilibrium quantum phase transitions, as well as their connection with
dynamical quantum phase transitions and non-Hermitian physics, with a
particular focus on the significance of Fisher zeros. Starting from the newly
discovered self-similarity phenomenon associated with complex partition
functions, we further discuss research on self-similar systems briefly.
Finally, we offer a perspective on these aspects.; 35) Magnons in chromium trihalides from \emph{ab initio} Bethe-Salpeter
  equation; Chromium trihalides (CrX$_3$, with $\rm{X=I,Br,Cl}$) are layered
ferromagnetic materials with rich physics and possible applications. Their
structure consists of magnetic Cr atoms positioned between two layers of halide
atoms. The choice of halide results in distinct magnetic properties, but their
effect on spin-wave (magnon) excitations is not fully understood. Here we
present first-principles calculations of magnon dispersions and wave functions
for monolayer Cr trihalides using the finite-momentum Bethe-Salpeter equation
(BSE) to describe collective spin-flip excitations. % We study the dependence
of magnon dispersions on the halide species and resolve the small topological
gap at the Dirac point in the magnon spectrum by including spin-orbit coupling.
Analysis of magnon wave functions reveals that magnons are made up of
electronic transitions with a wider energy range than excitons in CrX$_3$
monolayers, providing insight into magnon states in real and reciprocal space.
We discuss Heisenberg exchange parameters extracted from the BSE and discuss
the convergence of BSE magnon calculations. Our work advances the quantitative
modeling of magnons in two-dimensional materials, providing the starting point
for studying magnon interactions in a first-principles BSE framework.; 36) D3MES: Diffusion Transformer with multihead equivariant self-attention
  for 3D molecule generation; Understanding and predicting the diverse conformational states of molecules
is crucial for advancing fields such as chemistry, material science, and drug
development. Despite significant progress in generative models, accurately
generating complex and biologically or material-relevant molecular structures
remains a major challenge. In this work, we introduce a diffusion model for
three-dimensional (3D) molecule generation that combines a classifiable
diffusion model, Diffusion Transformer, with multihead equivariant
self-attention. This method addresses two key challenges: correctly attaching
hydrogen atoms in generated molecules through learning representations of
molecules after hydrogen atoms are removed; and overcoming the limitations of
existing models that cannot generate molecules across multiple classes
simultaneously. The experimental results demonstrate that our model not only
achieves state-of-the-art performance across several key metrics but also
exhibits robustness and versatility, making it highly suitable for early-stage
large-scale generation processes in molecular design, followed by validation
and further screening to obtain molecules with specific properties.; 37) Byzantine-Resilient Over-the-Air Federated Learning under Zero-Trust
  Architecture; Over-the-air computation (AirComp) has emerged as an essential approach for
enabling communication-efficient federated learning (FL) over wireless
networks. Nonetheless, the inherent analog transmission mechanism in
AirComp-based FL (AirFL) intensifies challenges posed by potential Byzantine
attacks. In this paper, we propose a novel Byzantine-robust FL paradigm for
over-the-air transmissions, referred to as federated learning with secure
adaptive clustering (FedSAC). FedSAC aims to protect a portion of the devices
from attacks through zero trust architecture (ZTA) based Byzantine
identification and adaptive device clustering. By conducting a one-step
convergence analysis, we theoretically characterize the convergence behavior
with different device clustering mechanisms and uneven aggregation weighting
factors for each device. Building upon our analytical results, we formulate a
joint optimization problem for the clustering and weighting factors in each
communication round. To facilitate the targeted optimization, we propose a
dynamic Byzantine identification method using historical reputation based on
ZTA. Furthermore, we introduce a sequential clustering method, transforming the
joint optimization into a weighting optimization problem without sacrificing
the optimality. To optimize the weighting, we capitalize on the penalty
convex-concave procedure (P-CCP) to obtain a stationary solution. Numerical
results substantiate the superiority of the proposed FedSAC over existing
methods in terms of both test accuracy and convergence rate.; 38) Deep Learning-based Feature Discovery for Decoding Phenotypic Plasticity
  in Pediatric High-Grade Gliomas Single-Cell Transcriptomics; By use of complex network dynamics and graph-based machine learning, we
identified critical determinants of lineage-specific plasticity across the
single-cell transcriptomics of pediatric high-grade glioma (pHGGs) subtypes:
IDHWT glioblastoma and K27M-mutant glioma. Our study identified network
interactions regulating glioma morphogenesis via the tumor-immune
microenvironment, including neurodevelopmental programs, calcium dynamics, iron
metabolism, metabolic reprogramming, and feedback loops between MAPK/ERK and
WNT signaling. These relationships highlight the emergence of a hybrid spectrum
of cellular states navigating a disrupted neuro-differentiation hierarchy. We
identified transition genes such as DKK3, NOTCH2, GATAD1, GFAP, and SEZ6L in
IDHWT glioblastoma, and H3F3A, ANXA6, HES6/7, SIRT2, FXYD6, PTPRZ1, MEIS1,
CXXC5, and NDUFAB1 in K27M subtypes. We also identified MTRNR2L1, GAPDH, IGF2,
FKBP variants, and FXYD7 as transition genes that influence cell fate
decision-making across both subsystems. Our findings suggest pHGGs are
developmentally trapped in states exhibiting maladaptive behaviors, and hybrid
cellular identities. In effect, tumor heterogeneity (metastability) and
plasticity emerge as stress-response patterns to immune-inflammatory
microenvironments and oxidative stress. Furthermore, we show that pHGGs are
steered by developmental trajectories from radial glia predominantly favoring
neocortical cell fates, in telencephalon and prefrontal cortex (PFC)
differentiation. By addressing underlying patterning processes and plasticity
networks as therapeutic vulnerabilities, our findings provide precision
medicine strategies aimed at modulating glioma cell fates and overcoming
therapeutic resistance. We suggest transition therapy toward neuronal-like
lineage differentiation as a potential therapy to help stabilize pHGG
plasticity and aggressivity.; 39) Involutory Hopf group-coalgebras and invariants of flat bundles over
  4-manifolds; We give invariants of flat bundles over 4-manifolds generalizing a result by
Chaidez, Cotler, and Cui (Alg. \& Geo. Topology '22). We utilize a structure
called a Hopf $G$-triplet for $G$ a group, which generalizes the notion of a
Hopf triplet by Chaidez, Cotler, and Cui. In our construction, we present flat
bundles over 4-manifolds using colored trisection diagrams: a direct analogue
of colored Heegaard diagrams as described by Virelizier. Our main result is
that involutory Hopf $G$-triplets of finite type yield well-defined invariants
of $G$-colored trisection diagrams, and that if the monodromy of a flat bundle
has image in $G$ we obtain invariants of flat bundles. We also show that a
special Hopf $G$-triplet yields the invariant from Hopf $G$-algebras described
by Mochida, thus generalizing the construction.; 40) Quaternionic Quantum Mechanics: the Particles, their q-Potentials and
  Mathematical Electron Model; In this work we show the quaternionic quantum descriptions of physical
processes from the Planck to macro scale. The results presented here are based
on the concepts of the Cauchy continuum and the elementary cell at the Planck
scale. The symmetrization of quaternion relations and the postulate of
quaternion velocity have been crucial in the present development. The momentum
of the expansion and compression is the consequence of the scalar term in the
quaternionic deformation potential.; 41) Fourth-Moment Theorems for Sums of Multiple Integrals; Nualart & Pecatti ([Nualart and Peccati, 2005, Thm 1]) established the first
fourth-moment theorem for random variables in a fixed Wiener chaos, i.e. they
showed that convergence of the sequence of fourth moments to the fourth moment
of the standard Gaussian distribution is sufficient for weak convergence to the
standard Gaussian. In this paper, we provide what we believe to be the first
generalization to chaos expansions with more than a single term. Specifically,
we show that a fourth-moment theorem holds for random variables consisting of
sums of two multiple integrals of orders $p, q \in N$, where $p, q$ have
different parities. Furthermore, we show that such random variables cannot
themselves be Gaussian, again generalizing what is known for the fixed Wiener
chaos setting. Finally, we show a fourth-moment theorem for variables with
infinite Wiener chaos expansions when the terms in the expansions are
independent and satisfy an additional regularity condition in terms of the
Ornstein-Uhlenbeck operator.; 42) Geometrical constructions of purity testing protocols and their
  applications to quantum communication; Purity testing protocols (PTPs), i.e., protocols that decide with high
probability whether or not a distributed bipartite quantum state is maximally
entangled, have been proven to be a useful tool in many quantum communication
applications. In this paper, we provide geometrical constructions for such
protocols that originate directly from classical linear error correcting codes
(LECCs), in a way that the properties of the resulting PTPs are completely
determined from those of the LECCs used in the construction. We investigate the
implications of our results in various tasks, including error detection,
entanglement purification for general quantum error models and quantum message
authentication.; 43) Stochastic Model Predictive Control for Sub-Gaussian Noise; We propose a stochastic Model Predictive Control (MPC) framework that ensures
closed-loop chance constraint satisfaction for linear systems with general
sub-Gaussian process and measurement noise. By considering sub-Gaussian noise,
we can provide guarantees for a large class of distributions, including
time-varying distributions. Specifically, we first provide a new
characterization of sub-Gaussian random vectors using matrix variance proxies,
which can more accurately represent the predicted state distribution. We then
derive tail bounds under linear propagation for the new characterization,
enabling tractable computation of probabilistic reachable sets of linear
systems. Lastly, we utilize these probabilistic reachable sets to formulate a
stochastic MPC scheme that provides closed-loop guarantees for general
sub-Gaussian noise. We further demonstrate our approach in simulations,
including a challenging task of surgical planning from image observations.; 44) Lam-Tung relation breaking effects and weak dipole moments at lepton
  colliders; The breaking of the Lam-Tung relation in the Drell-Yan process at the LHC
exhibits a long-standing tension with the Standard Model (SM) prediction at
$\mathcal{O}(\alpha_s^3)$ accuracy. This tension could be explained by weak
dipole interactions of leptons and quarks, associated with the $Z$-boson within
the framework of the Standard Model Effective Field Theory (SMEFT). In this
paper, we propose to cross-check these weak dipole interactions by measuring
the violation effects of the Lam-Tung relation at future lepton colliders
through the processes $e^+e^- \to Z\gamma \to \ell\bar{\ell}\gamma$ and $e^+e^-
\to Z\gamma \to q\bar{q}\gamma$. By considering different decay modes of the
$Z$-boson, these channels exhibit distinct sensitivities to various dipole
operators, providing a way to disentangle their individual effects.
Additionally, the high flavor-tagging efficiencies at lepton colliders could
provide strong constraints on the dipole interactions of heavy quarks, such as
$b$ and $c$ quarks, which are challenging to probe in the Drell-Yan process at
the LHC due to the suppression of parton distribution functions.; 45) Kronecker sum covariance models for spatio-temporal data; In this paper, we study the subgaussian matrix variate model, where we
observe the matrix variate data $X$ which consists of a signal matrix $X_0$ and
a noise matrix $W$. More specifically, we study a subgaussian model using the
Kronecker sum covariance as in Rudelson and Zhou (2017). Let $Z_1, Z_2$ be
independent copies of a subgaussian random matrix $Z =(Z_{ij})$, where $Z_{ij},
\forall i, j$ are independent mean 0, unit variance, subgaussian random
variables with bounded $\psi_2$ norm. We use $X \sim \mathcal{M}_{n,m}(0, A
\oplus B)$ to denote the subgaussian random matrix $X_{n \times m}$ which is
generated using: $$ X = Z_1 A^{1/2} + B^{1/2} Z_2. $$ In this covariance model,
the first component $A \otimes I_n$ describes the covariance of the signal $X_0
= Z_1 A^{1/2}$, which is an ${n \times m}$ random design matrix with
independent subgaussian row vectors, and the other component $I_m \otimes B$
describes the covariance for the noise matrix $W =B^{1/2} Z_2$, which contains
independent subgaussian column vectors $w^1, \ldots, w^m$, independent of
$X_0$. This leads to a non-separable class of models for the observation $X$,
which we denote by $X \sim \mathcal{M}_{n,m}(0, A \oplus B)$ throughout this
paper. Our method on inverse covariance estimation corresponds to the proposal
in Yuan (2010) and Loh and Wainwright (2012), only now dropping the i.i.d. or
Gaussian assumptions. We present the statistical rates of convergence.; 46) Ensemble Kalman-Bucy filtering for nonlinear model predictive control; We consider the problem of optimal control for partially observed dynamical
systems. Despite its prevalence in practical applications, there are still very
few algorithms available, which take uncertainties in the current state
estimates and future observations into account. In other words, most current
approaches separate state estimation from the optimal control problem. In this
paper, we extend the popular ensemble Kalman filter to receding horizon optimal
control problems in the spirit of nonlinear model predictive control. We
provide an interacting particle approximation to the forward-backward
stochastic differential equations arising from Pontryagin's maximum principle
with the forward stochastic differential equation provided by the
time-continuous ensemble Kalman-Bucy filter equations. The receding horizon
control laws are approximated as linear and are continuously updated as in
nonlinear model predictive control. We illustrate the performance of the
proposed methodology for an inverted pendulum example.; 47) Design as Hope: Reimagining Futures for Seemingly Doomed Problems; Design has the power to cultivate hope, especially in the face of seemingly
intractable societal challenges. This one-day workshop explores how design
methodologies -- ranging from problem reframing to participatory, speculative,
and critical design -- can empower research communities to drive meaningful
real-world changes. By aligning design thinking with hope theory -- framework
of viewing hope as ""goal-directed,"" ""pathways,"" and ""agentic"" thinking
processes -- we aim to examine how researchers can move beyond focusing on harm
mitigation and instead reimagine alternative futures. Through hands-on
activities, participants will engage in problem reframing, develop a taxonomy
of design methods related to hope, and explore how community-driven design
approaches can sustain efforts toward societal and individual hope. The
workshop also interrogates the ethical and practical boundaries of leveraging
hope in design research. By the end of the session, participants will leave
with concrete strategies for integrating a hopeful design approach into their
research, as well as a network for ongoing collaboration. Ultimately, we
position hopeful design not just as a practical tool for action and
problem-solving but as a catalyst for cultivating resilience and envisioning
transformative futures.; 48) Free boundary minimal surfaces and the reflection principle; We show that a minimal surface meeting a sphere at a 90-degree angle can be
reflected across the sphere. Using this reflection, we prove the uniqueness
that every embedded free boundary minimal annulus in a ball is necessarily the
critical catenoid.; 49) On the approaching geodesics property; We survey some recent results and open questions on the approaching geodesics
property and its application to the study of the Gromov and horofunction
compactifications of a proper geodesic Gromov metric space. We obtain results
on the dynamics of isometries and we exhibit an example of a Gromov hyperbolic
domain of $\mathbb{C}$ which does not satisfy the approaching geodesic
property.; 50) Anyon Theory and Topological Frustration of High-Efficiency Quantum LDPC
  Codes; Quantum low-density parity-check (QLDPC) codes present a promising route to
low-overhead fault-tolerant quantum computation, yet systematic strategies for
their exploration remain underdeveloped. In this work, we establish a
topological framework for studying the bivariate-bicycle codes, a prominent
class of QLDPC codes tailored for real-world quantum hardware. Our framework
enables the investigation of these codes through universal properties of
topological orders. Besides providing efficient characterizations for
demonstrations using Gr\""obner bases, we also introduce a novel
algebraic-geometric approach based on the Bernstein--Khovanskii--Kushnirenko
theorem, allowing us to analytically determine how the topological order varies
with the generic choice of bivariate-bicycle codes under toric layouts. Novel
phenomena are unveiled, including topological frustration, where ground-state
degeneracy on a torus deviates from the total anyon number, and quasi-fractonic
mobility, where anyon movement violates energy conservation. We demonstrate
their inherent link to symmetry-enriched topological orders and offer an
efficient method for searching for finite-size codes. Furthermore, we extend
the connection between anyons and logical operators using Koszul complex
theory. Our work provides a rigorous theoretical basis for exploring the fault
tolerance of QLDPC codes and deepens the interplay among topological order,
quantum error correction, and advanced mathematical structures.; 51) Logical Relations for Formally Verified Authenticated Data Structures; Authenticated data structures allow untrusted third parties to carry out
operations which produce proofs that can be used to verify an operation's
output. Such data structures are challenging to develop and implement
correctly. This paper gives a formal proof of security and correctness for a
library that generates authenticated versions of data structures automatically.
The proof is based on a new relational separation logic for reasoning about
programs that use collision-resistant cryptographic hash functions. This logic
provides a basis for constructing two semantic models of a type system, which
are used to justify how the library makes use of type abstraction to enforce
security and correctness. Using these models, we also prove the correctness of
several optimizations to the library and then show how optimized, hand-written
implementations of authenticated data structures can be soundly linked with
automatically generated code. All of the results in this paper have been
mechanized in the Coq proof assistant using the Iris framework.; 52) First Imaging of Magnetic Waveguides and Resonant Cavities in Sunspots; For the first time, we have determined the spatial distribution of magnetic
waveguides and resonant cavities at different heights in the sunspot
atmosphere. We applied a decomposition of time cubes of EUV/UV sunspot images
obtained in the SDO/AIA temperature channels into narrowband components in the
form of wave sources. The methods of pixelized wavelet filtering and
oscillation mode decomposition were used. For all studied sunspots the presence
of selected bands in the spectra was shown. Each band corresponds to
oscillations forming spatial waveguides in the volume of the sunspot
atmosphere. The formation of waveguide bundles in the height from photospheric
to coronal levels is shown. The regions of the waveguides with maximum
oscillation power, where resonant cavities are formed, are identified. Their
detection is an experimental proof of the theory of resonant layers, previously
proposed to explain the presence of significant harmonics in the oscillation
spectrum. The different shapes of the cavities reflect the structure of the
magnetic tubes along which the waves propagate. The distribution of sources in
the height layers indicates the influence of the wave cutoff frequency caused
by the inclinations of the magnetic field lines. We discuss the possibility of
upward wave transport due to periodic amplification of the oscillation power in
the detected cavities.; 53) BarcodeBERT: Transformers for Biodiversity Analysis; Understanding biodiversity is a global challenge, in which DNA barcodes - short snippets of that cluster by species play pivotal role. In particular, invertebrates, highly diverse and under-explored group, pose unique taxonomic complexities. We explore machine learning approaches, comparing supervised CNNs, fine-tuned foundation models, barcode-specific masking strategy across datasets varying complexity. While simpler tasks favor CNNs or transformers, challenging species-level identification demands paradigm shift towards self-supervised pretraining. propose BarcodeBERT, the first method for general analysis, leveraging 1.5 M invertebrate barcode reference library. This work highlights how dataset specifics coverage impact model selection, underscores role pretraining achieving high-accuracy barcode-based at genus level. Indeed, without fine-tuning step, BarcodeBERT pretrained on large outperforms DNABERT DNABERT-2 multiple downstream classification tasks. The code repository available https://github.com/Kari-Genomics-Lab/BarcodeBERT; 54) Curated loci prime editing (cliPE) for accessible multiplexed assays of
  variant effect (MAVEs); Multiplexed assays of variant effect (MAVEs) perform simultaneous
characterization of many variants. Prime editing has been recently adopted for
introducing many variants in their native genomic contexts. However, robust
protocols and standards are limited, preventing widespread uptake. Herein, we
describe curated loci prime editing (cliPE) which is an accessible, low-cost
experimental pipeline to perform MAVEs using prime editing of a target gene, as
well as a companion Shiny app (pegRNA Designer) to rapidly and easily design
user-specific MAVE libraries.; 55) Causes of evolutionary divergence in prostate cancer; Cancer progression involves the sequential accumulation of genetic
alterations that cumulatively shape the tumour phenotype. In prostate cancer,
tumours can follow divergent evolutionary trajectories that lead to distinct
subtypes, but the causes of this divergence remain unclear. While causal
inference could elucidate the factors involved, conventional methods are
unsuitable due to the possibility of unobserved confounders and ambiguity in
the direction of causality. Here, we propose a method that circumvents these
issues and apply it to genomic data from 829 prostate cancer patients. We
identify several genetic alterations that drive divergence as well as others
that prevent this transition, locking tumours into one trajectory. Further
analysis reveals that these genetic alterations may cause each other, implying
a positive-feedback loop that accelerates divergence. Our findings provide
insights into how cancer subtypes emerge and offer a foundation for genomic
surveillance strategies aimed at monitoring the progression of prostate cancer.; 56) Constrained Linear Thompson Sampling; We study the safe linear bandit problem, where an agent sequentially selects
actions from a convex domain to maximize an unknown objective while ensuring
unknown linear constraints are satisfied on a per-round basis. Existing
approaches primarily rely on optimism-based methods with frequentist confidence
bounds, often leading to computationally expensive action selection routines.
We propose COnstrained Linear Thompson Sampling (COLTS), a sampling-based
framework that efficiently balances regret minimization and constraint
satisfaction by selecting actions on the basis of noisy perturbations of the
estimates of the unknown objective vector and constraint matrix. We introduce
three variants of COLTS, distinguished by the learner's available side
information:
  - S-COLTS assumes access to a known safe action and ensures strict constraint
enforcement by combining the COLTS approach with a rescaling towards the safe
action. For $d$-dimensional actions, this yields $\tilde{O}(\sqrt{d^3 T})$
regret and zero constraint violations (or risk).
  - E-COLTS enforces constraints softly under Slater's condition, and attains
regret and risk of $\tilde{O}(\sqrt{d^3 T})$ by combining COLTS with uniform
exploration.
  - R-COLTS requires no side information, and ensures instance-independent
regret and risk of $\tilde{O}(\sqrt{d^3 T})$ by leveraging repeated resampling.
  A key technical innovation is a coupled noise design, which maintains
optimism while preserving computational efficiency, which is combined with a
scaling based analysis technique to address the variation of the per-round
feasible region induced by sampled constraint matrices. Our methods match the
regret bounds of prior approaches, while significantly reducing computational
costs compared to them, thus yielding a scalable and practical approach for
constrained bandit linear optimization.; 57) Entanglement transition and suppression of critical phase of thermofield
  double state in monitored quantum circuit with unitary $R$ matrix gates; We study quantum circuits with gates composed randomly of identity operators,
projectors, or a kind of $R$ matrices which satisfy the Yang-Baxter equation
and are unitary and dual-unitary. This enables us to translate the quantum
circuit into a topological object with distinguished overcrossings and
undercrossings. The circuit corresponds to a classical loop model when an
overcrossings and undercrossing coincides. The entanglement entropy between the
final state and initial state is given by the spanning number of the classical
model, and they share the same phase diagram. Whenever an overcrossing and
undercrossing differ, the circuit extends beyond the classical model.
Considering a specific case with $R$ matrices randomly replaced by swap gates,
we demonstrate that the topological effect dominates, and only the area-law
phase remains in the thermodynamic limit, regardless of how small the
replacement probability is. We also find evidence of an altered phase diagram
for non-Clifford cases.; 58) Tensor network method for solving the Ising model with a magnetic field; We study the two-dimensional square lattice Ising ferromagnet and
antiferromagnet with a magnetic field by using tensor network method. Focusing
on the role of guage fixing, we present the partition function in terms of a
tensor network. The tensor has a different symmetry property for ferromagnets
and antiferromagnets. The tensor network of the partition function is
interpreted as a multiple product of the one-dimensional quantum Hamiltonian.
We perform infinite density matrix renormalization group to contract the
two-dimensional tensor network. We present the numerical result of
magnetization and entanglement entropy for the Ising ferromagnet and
antiferromagnet side by side. In order to determine the critical line in the
parameter space of temperature and magnetic field, we use the half-chain
entanglement entropy of the one-dimensional quantum state. The entanglement
entropy precisely indicates the critical line forming the parabolic shape for
the antiferromagnetic case, but shows the critical point for the ferromagnetic
case.; 59) Randomized Spectral Clustering for Large-Scale Multi-Layer Networks; Large-scale multi-layer networks with large numbers of nodes, edges, and
layers arise across various domains, which poses a great computational
challenge for the downstream analysis. In this paper, we develop an efficient
randomized spectral clustering algorithm for community detection of multi-layer
networks. We first utilize the random sampling strategy to sparsify the
adjacency matrix of each layer. Then we use the random projection strategy to
accelerate the eigen-decomposition of the sum-of-squared sparsified adjacency
matrices of all layers. The communities are finally obtained via the k-means of
the eigenvectors. The algorithm not only has low time complexity but also saves
the storage space. Theoretically, we study the misclassification error rate of
the proposed algorithm under the multi-layer stochastic block models, which
shows that the randomization does not deteriorate the error bound under certain
conditions. Numerical studies on multi-layer networks with millions of nodes
show the superior efficiency of the proposed algorithm, which achieves
clustering results rapidly. A new R package called MLRclust is developed and
made available to the public.; 60) Token-level Ensembling of Models with Different Vocabularies; Model ensembling is a technique to combine the predicted distributions of two
or more models, often leading to improved robustness and performance. For
ensembling in text generation, the next token's probability distribution is
derived from a weighted sum of the distributions of each individual model. This
requires the underlying models to share the same subword vocabulary, limiting
the applicability of ensembling, since many open-sourced models have distinct
vocabularies. In research settings, experimentation or upgrades to vocabularies
may introduce multiple vocabulary sizes. This paper proposes an inference-time
only algorithm that allows for ensembling models with different vocabularies,
without the need to learn additional parameters or alter the underlying models.
Instead, the algorithm ensures that tokens generated by the ensembled models
\textit{agree} in their surface form. We apply this technique to combinations
of traditional encoder-decoder models and decoder-only LLMs and evaluate on
machine translation. In addition to expanding to model pairs that were
previously incapable of token-level ensembling, our algorithm frequently
improves translation performance over either model individually.; 61) The role of Massive Black Holes in merging star clusters: dynamical
  evolution, stellar & compact object ejections and gravitational waves; Star clusters can interact and merge in galactic discs, halos, or centers. We
present direct N-body simulations of binary mergers of star clusters with
$M_{\star} = 2.7 \times 10^4 \: \mathrm{M_{\odot}}$ each, using the N-body code
BIFROST with subsystem regularisation and post-Newtonian dynamics. We include
500 $\mathrm{M_{\odot}}$ massive black holes (MBHs) in the progenitors to
investigate their impact on remnant evolution. The MBHs form hard binaries
interacting with stars and stellar black holes (BHs). A few Myr after the
cluster merger, this produces sizable populations of runaway stars ($\sim$800
with $v_{\mathrm{ej}} \gtrsim 50 \mathrm{kms^{-1}}$) and stellar BHs ($\sim$30)
escaping within 100 Myr. The remnants lose $\sim30\%$ of their BH population
and $\sim3\%$ of their stars, with $\sim$30 stars accelerated to high
velocities $\gtrsim 300 \mathrm{kms^{-1}}$. Comparison simulations of isolated
clusters with central hard MBH binaries and cluster mergers without MBHs show
that the process is driven by MBH binaries, while those with a single 1000
$\mathrm{M_{\odot}}$ MBH in isolated or merging clusters produce fewer runaway
stars at lower velocities. Low-eccentricity merger orbits yield rotating
remnants ($v_{\mathrm{rot}} \sim 3 \mathrm{kms^{-1}}$) , but probing the
presence of MBHs via kinematics alone remains challenging. We expect the binary
MBHs to merge within a Hubble time, producing observable gravitational-wave
(GW) events detectable by future GW detectors such as the Einstein Telescope
and LISA. The results suggest that interactions with low-mass MBH binaries
formed in merging star clusters are an important additional channel for
producing runaway and high-velocity stars, free-floating stellar BHs and
compact objects.; 62) C-LoRA: Continual Low-Rank Adaptation for Pre-trained Models; Low-Rank Adaptation (LoRA) is an efficient fine-tuning method that has been
extensively applied in areas such as natural language processing and computer
vision. Existing LoRA fine-tuning approaches excel in static environments but
struggle in dynamic learning due to reliance on multiple adapter modules,
increasing overhead and complicating inference. We propose Continual Low-Rank
Adaptation (C-LoRA), a novel extension of LoRA for continual learning. C-LoRA
uses a learnable routing matrix to dynamically manage parameter updates across
tasks, ensuring efficient reuse of learned subspaces while enforcing
orthogonality to minimize interference and forgetting. Unlike existing
approaches that require separate adapters for each task, C-LoRA enables a
integrated approach for task adaptation, achieving both scalability and
parameter efficiency in sequential learning scenarios. C-LoRA achieves
state-of-the-art accuracy and parameter efficiency on benchmarks while
providing theoretical insights into its routing matrix's role in retaining and
transferring knowledge, establishing a scalable framework for continual
learning.; 63) Weak Gravitational Lensing; This chapter provides a comprehensive overview of weak gravitational lensing
and its current applications in cosmology. We begin by introducing the
fundamental concepts of gravitational lensing and derive the key equations for
the deflection angle, lensing potential, convergence, and shear. We explore how
weak lensing can be used as a cosmological probe, discussing cosmic shear,
galaxy-galaxy lensing, and their combination with galaxy clustering in the
3$\times$2pt analysis. The chapter covers the theoretical framework for
modeling lensing observables, shear estimation techniques, and major systematic
effects such as intrinsic alignments and baryonic feedback. We review the
current results of weak lensing cosmology from major surveys and outline
prospects for future advancements in the field.; 64) Non-Bloch edge dynamics of non-Hermitian lattices; The non-Hermitian skin effect, i.e., the localization of nominally bulk
modes, not only drastically reshapes the spectral properties of non-Hermitian
systems, but also dramatically modifies the real-time dynamics therein. Here we
investigate the time evolution of waves (or quantum-mechanical particles)
initialized around the edge of non-Hermitian lattices. The non-Hermitian skin
effect tends to localize the wave to the edge, meaning that the real-time
dynamics differs from the Bloch-theory picture. We focus on the long-time decay
or growth rate of wave function, which is quantified by the Lyapunov exponents.
These exponents can be obtained from the saddle points in the complex momentum
space. We propose an efficient yet unambiguous criterion for identifying the
dominant saddle point that determines the Lyapunov exponents. Our criterion can
be precisely formulated in terms of a mathematical concept known as the
Lefschetz thimble. Counterintuitively, the seemingly natural criterion based on
the imaginary part of the energy fails. Our work provides a coherent theory for
characterizing the real-time edge dynamics of non-Hermitian lattices. Our
predictions are testable in various non-Hermitian physical platforms.; 65) GiantHunter: Accurate detection of giant virus in metagenomic data using
  reinforcement-learning and Monte Carlo tree search; Motivation: Nucleocytoplasmic large DNA viruses (NCLDVs) are notable for
their large genomes and extensive gene repertoires, which contribute to their
widespread environmental presence and critical roles in processes such as host
metabolic reprogramming and nutrient cycling. Metagenomic sequencing has
emerged as a powerful tool for uncovering novel NCLDVs in environmental
samples. However, identifying NCLDV sequences in metagenomic data remains
challenging due to their high genomic diversity, limited reference genomes, and
shared regions with other microbes. Existing alignment-based and machine
learning methods struggle with achieving optimal trade-offs between sensitivity
and precision. Results: In this work, we present GiantHunter, a reinforcement
learning-based tool for identifying NCLDVs from metagenomic data. By employing
a Monte Carlo tree search strategy, GiantHunter dynamically selects
representative non-NCLDV sequences as the negative training data, enabling the
model to establish a robust decision boundary. Benchmarking on rigorously
designed experiments shows that GiantHunter achieves high precision while
maintaining competitive sensitivity, improving the F1-score by 10% and reducing
computational cost by 90% compared to the second-best method. To demonstrate
its real-world utility, we applied GiantHunter to 60 metagenomic datasets
collected from six cities along the Yangtze River, located both upstream and
downstream of the Three Gorges Dam. The results reveal significant differences
in NCLDV diversity correlated with proximity to the dam, likely influenced by
reduced flow velocity caused by the dam. These findings highlight the potential
of GiantSeeker to advance our understanding of NCLDVs and their ecological
roles in diverse environments.; 66) Syntactic Learnability of Echo State Neural Language Models at Scale; What is a neural model with minimum architectural complexity that exhibits
reasonable language learning capability? To explore such a simple but
sufficient neural language model, we revisit a basic reservoir computing (RC)
model, Echo State Network (ESN), a restricted class of simple Recurrent Neural
Networks. Our experiments showed that ESN with a large hidden state is
comparable or superior to Transformer in grammaticality judgment tasks when
trained with about 100M words, suggesting that architectures as complex as that
of Transformer may not always be necessary for syntactic learning.; 67) ""Well, Keep Thinking"": Enhancing LLM Reasoning with Adaptive Injection
  Decoding; Large language models (LLMs) exhibit strong reasoning abilities, often
attributed to few-shot or zero-shot chain-of-thought (CoT) prompting. While
effective, these methods require labor-intensive prompt engineering, raising
the question of whether reasoning can be induced without reliance on explicit
prompts. In this work, we unlock the reasoning capabilities of LLMs without
explicit prompting. Inspired by zero-shot CoT and CoT-decoding, we propose a
novel decoding strategy that systematically nudges LLMs to continue reasoning,
thereby preventing immature reasoning processes. Specifically, we monitor the
model's generation and inject a designated phrase whenever it is likely to
conclude its response prematurely, before completing the reasoning process. Our
experimental evaluations on diverse reasoning benchmarks demonstrate that our
proposed strategy substantially improves LLM reasoning capabilities,
highlighting the potential of decoding-based interventions as an alternative to
traditional prompting techniques.; 68) Formation of Jet-driven Forced Reconnection Region and Associated Plasma
  Blobs in a Prominence Segment; We use data from the Atmospheric Imaging Assembly (AIA) onboard the Solar
Dynamics Observatory (SDO) to study the most likely formation of a forced
reconnection region and associated plasma blobs, triggered by jet-like
structures in a prominence segment. Around 05:44 UT on December 16$^{th}$,
2017, hot jet-like structures lifted from a nearby active region and fell
obliquely on one side of the prominence segment with velocities of
$\approx$45--65 km s$^{-1}$. These eruptions compressed the boundaries of the
prominence and flux rope, forming an elongated reconnection region with inflow
velocities of 47--52 km s$^{-1}$ and 36--49 km s$^{-1}$ in the projected plane.
A thin, elongated reconnection region was formed, with multiple magnetic plasma
blobs propagating bidirectionally at velocities of 91--178 km s$^{-1}$. These
dense blobs, associated with ongoing reconnection, may also be linked to the
onset of Kelvin-Helmholtz (K-H) instability. The blobs are attributed to
plasmoids, moving at slower speeds (91--178 km s$^{-1}$) due to the high
density in the prominence segment. The dimensionless reconnection rate varied
from 0.57--0.28, 0.53--0.26, and 0.41--0.20, indicating reconnection rate
enhancement and supporting the forced reconnection scenario. After
reconnection, the prominence plasma heated to 6 MK, releasing significant
thermal energy ($\approx$5.4$\times$10$^{27}$ erg), which drained cool
prominence plasma and heated it to coronal temperatures. The ubiquity of jets
and outflows in the solar atmosphere makes the aforementioned of reconnection
and possible co-existence of K-H instability potentially important for the
magnetic energy release and heating in the solar atmosphere.; 69) RL-OGM-Parking: Lidar OGM-Based Hybrid Reinforcement Learning Planner
  for Autonomous Parking; Autonomous parking has become a critical application in automatic driving
research and development. Parking operations often suffer from limited space
and complex environments, requiring accurate perception and precise
maneuvering. Traditional rule-based parking algorithms struggle to adapt to
diverse and unpredictable conditions, while learning-based algorithms lack
consistent and stable performance in various scenarios. Therefore, a hybrid
approach is necessary that combines the stability of rule-based methods and the
generalizability of learning-based methods. Recently, reinforcement learning
(RL) based policy has shown robust capability in planning tasks. However, the
simulation-to-reality (sim-to-real) transfer gap seriously blocks the
real-world deployment. To address these problems, we employ a hybrid policy,
consisting of a rule-based Reeds-Shepp (RS) planner and a learning-based
reinforcement learning (RL) planner. A real-time LiDAR-based Occupancy Grid Map
(OGM) representation is adopted to bridge the sim-to-real gap, leading the
hybrid policy can be applied to real-world systems seamlessly. We conducted
extensive experiments both in the simulation environment and real-world
scenarios, and the result demonstrates that the proposed method outperforms
pure rule-based and learning-based methods. The real-world experiment further
validates the feasibility and efficiency of the proposed method.; 70) Principal Component Maximization: A Novel Method for SAR Image Formation
  from Raw Data without System Parameters; Synthetic aperture radar (SAR) imaging traditionally requires precise
knowledge of system parameters to implement focusing algorithms that transform
raw data into high-resolution images. These algorithms require knowledge of SAR
system parameters, such as wavelength, center slant range, fast time sampling
rate, pulse repetition interval (PRI), waveform parameters (e.g., frequency
modulation rate), and platform speed. This paper presents a novel framework for
recovering SAR images from raw data without the requirement of any SAR system
parameters. Firstly, we introduce an approximate matched filtering model that
leverages the inherent shift-invariance properties of SAR echoes, enabling
image formation through an adaptive reference echo estimation. To estimate this
unknown reference echo, we develop a principal component maximization (PCM)
technique that exploits the low-dimensional structure of the SAR signal. The
PCM method employs a three-stage procedure: 1) data block segmentation, 2)
energy normalization, and 3) principal component energy maximization across
blocks, effectively handling non-stationary clutter environments. Secondly, we
present a range-varying azimuth reference signal estimation method that
compensates for the quadratic phase errors. For cases where PRI is unknown, we
propose a two-step PRI estimation scheme that enables robust reconstruction of
2-D images from 1-D data streams. Experimental results on various SAR datasets
demonstrate that our method can effectively recover SAR images from raw data
without any prior system parameters.; 71) CyclePose -- Leveraging Cycle-Consistency for Annotation-Free Nuclei
  Segmentation in Fluorescence Microscopy; In recent years, numerous neural network architectures specifically designed
for the instance segmentation of nuclei in microscopic images have been
released. These models embed nuclei-specific priors to outperform generic
architectures like U-Nets; however, they require large annotated datasets,
which are often not available. Generative models (GANs, diffusion models) have
been used to compensate for this by synthesizing training data. These two-stage
approaches are computationally expensive, as first a generative model and then
a segmentation model has to be trained. We propose CyclePose, a hybrid
framework integrating synthetic data generation and segmentation training.
CyclePose builds on a CycleGAN architecture, which allows unpaired translation
between microscopy images and segmentation masks. We embed a segmentation model
into CycleGAN and leverage a cycle consistency loss for self-supervision.
Without annotated data, CyclePose outperforms other weakly or unsupervised
methods on two public datasets. Code is available at
https://github.com/jonasutz/CyclePose; 72) AFDM-Enabled Integrated Sensing and Communication: Theoretical Framework
  and Pilot Design; The integrated sensing and communication (ISAC) has been envisioned as one
representative usage scenario of sixth-generation (6G) network. However, the
unprecedented characteristics of 6G, especially the doubly dispersive channel,
make classical ISAC waveforms rather challenging to guarantee a desirable
performance level. The recently proposed affine frequency division multiplexing
(AFDM) can attain full diversity even under doubly dispersive effects, thus
becoming a competitive candidate for next-generation ISAC waveforms. Relevant
investigations are still at an early stage, which involve only straightforward
design lacking explicit theoretical analysis. This paper provides an in-depth
investigation on AFDM waveform design for ISAC applications. Specifically, the
closed-form Cr\'{a}mer-Rao bounds of target detection for AFDM are derived,
followed by a demonstration on its merits over existing counterparts.
Furthermore, we formulate the ambiguity function of the pilot-assisted AFDM
waveform for the first time, revealing conditions for stable sensing
performance. To further enhance both the communication and sensing performance
of the AFDM waveform, we propose a novel pilot design by exploiting the
characteristics of AFDM signals. The proposed design is analytically validated
to be capable of optimizing the ambiguity function property and channel
estimation accuracy simultaneously as well as overcoming the sensing and
channel estimation range limitation originated from the pilot spacing.
Numerical results have verified the superiority of the proposed pilot design in
terms of dual-functional performance.; 73) Learnable Group Transform: Enhancing Genotype-to-Phenotype Prediction
  for Rice Breeding with Small, Structured Datasets; Genotype-to-Phenotype (G2P) prediction plays a pivotal role in crop breeding,
enabling the identification of superior genotypes based on genomic data. Rice
(Oryza sativa), one of the most important staple crops, faces challenges in
improving yield and resilience due to the complex genetic architecture of
agronomic traits and the limited sample size in breeding datasets. Current G2P
prediction methods, such as GWAS and linear models, often fail to capture
complex non-linear relationships between genotypes and phenotypes, leading to
suboptimal prediction accuracy. Additionally, population stratification and
overfitting are significant obstacles when models are applied to small datasets
with diverse genetic backgrounds. This study introduces the Learnable Group
Transform (LGT) method, which aims to overcome these challenges by combining
the advantages of traditional linear models with advanced machine learning
techniques. LGT utilizes a group-based transformation of genotype data to
capture spatial relationships and genetic structures across diverse rice
populations, offering flexibility to generalize even with limited data. Through
extensive experiments on the Rice529 dataset, a panel of 529 rice accessions,
LGT demonstrated substantial improvements in prediction accuracy for multiple
agronomic traits, including yield and plant height, compared to
state-of-the-art baselines such as linear models and recent deep learning
approaches. Notably, LGT achieved an R^2 improvement of up to 15\% for yield
prediction, significantly reducing error and demonstrating its ability to
extract meaningful signals from high-dimensional, noisy genomic data. These
results highlight the potential of LGT as a powerful tool for genomic
prediction in rice breeding, offering a promising solution for accelerating the
identification of high-yielding and resilient rice varieties.; 74) CoverM: Read alignment statistics for metagenomics; Genome-centric analysis of metagenomic samples is a powerful method for
understanding the function of microbial communities. Calculating read coverage
is a central part of analysis, enabling differential coverage binning for
recovery of genomes and estimation of microbial community composition. Coverage
is determined by processing read alignments to reference sequences of either
contigs or genomes. Per-reference coverage is typically calculated in an ad-hoc
manner, with each software package providing its own implementation and
specific definition of coverage. Here we present a unified software package
CoverM which calculates several coverage statistics for contigs and genomes in
an ergonomic and flexible manner. It uses 'Mosdepth arrays' for computational
efficiency and avoids unnecessary I/O overhead by calculating coverage
statistics from streamed read alignment results. CoverM is free software
available at https://github.com/wwood/coverm. CoverM is implemented in Rust,
with Python (https://github.com/apcamargo/pycoverm) and Julia
(https://github.com/JuliaBinaryWrappers/CoverM_jll.jl) interfaces.; 75) TopoLa: A Universal Framework to Enhance Cell Representations for
  Single-cell and Spatial Omics through Topology-encoded Latent Hyperbolic
  Geometry; Recent advances in cellular research demonstrate that scRNA-seq characterizes
cellular heterogeneity, while spatial transcriptomics reveals the spatial
distribution of gene expression. Cell representation is the fundamental issue
in the two fields. Here, we propose Topology-encoded Latent Hyperbolic Geometry
(TopoLa), a computational framework enhancing cell representations by capturing
fine-grained intercellular topological relationships. The framework introduces
a new metric, TopoLa distance (TLd), which quantifies the geometric distance
between cells within latent hyperbolic space, capturing the network's
topological structure more effectively. With this framework, the cell
representation can be enhanced considerably by performing convolution on its
neighboring cells. Performance evaluation across seven biological tasks,
including scRNA-seq data clustering and spatial transcriptomics domain
identification, shows that TopoLa significantly improves the performance of
several state-of-the-art models. These results underscore the generalizability
and robustness of TopoLa, establishing it as a valuable tool for advancing both
biological discovery and computational methodologies.; 76) Application of Single-cell Deep Learning in Elucidating the Mapping
  Relationship Between Visceral and Body Surface Inflammatory Patterns; As a system of integrated homeostasis, life is susceptible to disruptions by
visceral inflammation, which can disturb internal environment equilibrium. The
role of body-spread subcutaneous fascia (scFascia) in this process is poorly
understood. In the rat model of Salmonella-induced dysentery, scRNA-seq of
scFascia and deep-learning analysis revealed Warburg-like metabolic
reprogramming in macrophages (MPs) with reduced citrate cycle activity.
Cd34+/Pdgfra+ telocytes (CPTCs) regulated MPs differentiation and proliferation
via Wnt/Fgf signal, suggesting a pathological crosstalk pattern in the
scFascia, herein termed the fascia-visceral inflammatory crosstalk pattern
(FVICP). PySCENIC analysis indicated increased activity transcription factors
Fosl1, Nfkb2, and Atf4, modulated by CPTCs signaling to MPs, downregulating
aerobic respiration and upregulating cell cycle, DNA replication, and
transcription. This study highlights scFascia's role in immunomodulation and
metabolic reprogramming during visceral inflammation, underscoring its function
in systemic homeostasis.; 77) Chiral supersolid and dissipative time crystal in Rydberg-dressed
  Bose-Einstein condensates with Raman-induced spin-orbit coupling; Spin-orbit coupling (SOC) is one of the key factors that affect the chiral
symmetry of matter by causing the spatial symmetry breaking of the system. We
find that Raman-induced SOC can induce a chiral supersolid phase with a helical
antiskyrmion lattice in balanced Rydberg-dressed two-component Bose-Einstein
condensates (BECs) in a harmonic trap by modulating the Raman coupling
strength, strong contrast with the mirror symmetric supersolid phase containing
skyrmion-antiskyrmion lattice pair for the case of Rashba SOC. Two ground-state
phase diagrams are presented as a function of the Rydberg interaction strength
and the SOC strength, as well as that of the Rydberg interaction strength and
the Raman coupling strength, respectively. It is shown that the interplay among
Raman-induced SOC, soft-core long-range Rydberg interactions, and contact
interactions favors rich ground-state structures including half-quantum vortex
phase, stripe supersolid phase, toroidal stripe phase with a central
Anderson-Toulouse coreless vortex, checkerboard supersolid phase, mirror
symmetric supersolid phase, chiral supersolid phase and standing-wave
supersolid phase. In addition, the effects of rotation and in-plane quadrupole
magnetic field on the ground state of the system are analyzed. In these two
cases, the chiral supersolid phase is broken and the ground state tends to form
a miscible phase. Furthermore, the stability and superfluid properties of the
two-component BECs with Raman-induced SOC and Rydberg interactions in free
space are revealed by solving the Bogoliubov-de Gennes equation. Finally, we
demonstrate that when the initial state is a chiral supersolid phase the
rotating harmonic trapped system sustains dissipative continuous time crystal
by studying the rotational dynamic behaviors of the system.; 78) A model reduction method for solving the eigenvalue problem of
  semiclassical random Schr\""odinger operators; In this paper, we compute the eigenvalue problem (EVP) for the semiclassical
random Schr\""odinger operators, where the random potentials are parameterized
by an infinite series of random variables. After truncating the series, we
introduce the multiscale finite element method (MsFEM) to approximate the
resulting parametric EVP. We then use the quasi-Monte Carlo (qMC) method to
calculate empirical statistics within a finite-dimensional random space.
Furthermore, using a set of low-dimensional proper orthogonal decomposition
(POD) basis functions, the referred degrees of freedoms for constructing
multiscale basis are independent of the spatial mesh. Given the bounded
assumption on the random potentials, we then derive and prove an error estimate
for the proposed method. Finally, we conduct numerical experiments to validate
the error estimate. In addition, we investigate the localization of
eigenfunctions for the Schr\""odinger operator with spatially random potentials.
The results show that our method provides a practical and efficient solution
for simulating complex quantum systems governed by semiclassical random
Schr\""odinger operators.; 79) Can Large Vision-Language Models Detect Images Copyright Infringement
  from GenAI?; Generative AI models, renowned for their ability to synthesize high-quality
content, have sparked growing concerns over the improper generation of
copyright-protected material. While recent studies have proposed various
approaches to address copyright issues, the capability of large vision-language
models (LVLMs) to detect copyright infringements remains largely unexplored. In
this work, we focus on evaluating the copyright detection abilities of
state-of-the-art LVLMs using a various set of image samples. Recognizing the
absence of a comprehensive dataset that includes both IP-infringement samples
and ambiguous non-infringement negative samples, we construct a benchmark
dataset comprising positive samples that violate the copyright protection of
well-known IP figures, as well as negative samples that resemble these figures
but do not raise copyright concerns. This dataset is created using advanced
prompt engineering techniques. We then evaluate leading LVLMs using our
benchmark dataset. Our experimental results reveal that LVLMs are prone to
overfitting, leading to the misclassification of some negative samples as
IP-infringement cases. In the final section, we analyze these failure cases and
propose potential solutions to mitigate the overfitting problem.; 80) Quantum Dual Extended Hamming Code Immune to Collective Coherent Errors; Collective coherent (CC) errors are inevitable, as every physical qubit
experiences evolution due to its kinetic Hamiltonian. Quantum error correction
(QEC) codes are essential in protecting quantum information from both CC and
stochastic Pauli errors. While stabilizer codes are designed to correct
low-weight stochastic Pauli errors, they are less effective against high-weight
errors. Constant excitation (CE) codes, however, are immune to CC errors by
ensuring that codewords have constant excitation. However, this comes at the
cost of increased qubit overhead, raising the expense of QEC hardware and
logical qubit infrastructure. In this work, we propose a new family of CE
stabilizer codes with parameters $[[2^{r+1}, 2^r-(r+1), 4]]$. Compared to the
existing $[[20,1,4]]$ CE stabilizer code, our smallest instance, the
$[[8,1,4]]$ CE stabilizer code, significantly reduces the number of physical
qubits required. Furthermore, this new CE code family improves the asymptotics
code rate from $\frac{1}{8}$ in our previous work~\cite{2412.16450} to
$\frac{1}{2}$, offering a more efficient trade-off between error correction and
qubit overhead.; 81) Laser intensity noise suppression for space-borne gravitational wave
  mission; Laser intensity noise is a main limitation of measurement and sensing mission
represented by gravitational wave detection. We develop a noise decomposition
model and design the core elements of the feedback loop independently based on
the analysis results. We construct a fiber amplifier system with ultra-low
intensity noise in the 0.1 mHz-1 Hz frequency band by the employment of an
optoelectronic feedback loop that is specially designed. The study provides
experimental basis and technologies for precise measurement and sensing system
at ultra-low frequency.; 82) Temperatures of Robin Hood; Cumulative Games were introduced by Larsson, Meir, and Zick (2020) to bridge
some conceptual and technical gaps between Combinatorial Game Theory (CGT) and
Economic Game Theory. The partizan ruleset {\sc Robin Hood} is an instance of a
Cumulative Game, viz., {\sc Wealth Nim}. It is played on multiple heaps, each
associated with a pair of cumulations, interpreted here as wealth. Each player
chooses one of the heaps, removes tokens from that heap not exceeding their own
wealth, while simultaneously diminishing the other player's wealth by the same
amount. In CGT, the {\em temperature} of a {\em disjunctive sum} game component
is an estimate of the urgency of moving first in that component. It turns out
that most of the positions of {\sc Robin Hood} are {\em hot}. The temperature
of {\sc Robin Hood} on a single large heap shows a dichotomy in behavior
depending on the ratio of the wealths of the players. Interestingly, this
bifurcation is related to Pingala (Fibonacci) sequences and the Golden Ratio
$\phi$: when the ratio of the wealths lies in the interval $(\phi^{-1},\phi)$,
the temperature increases linearly with the heap size, and otherwise it remains
constant, and the mean values has a reciprocal property. It turns out that
despite {\sc Robin Hood} displaying high temperatures, playing in the hottest
component might be a sub-optimal strategy.; 83) Bypassing eigenstate thermalization with experimentally accessible
  quantum dynamics; Eigenstate thermalization has played a prominent role as a determiner of the
validity of quantum statistical mechanics since von Neumann's early works on
quantum ergodicity. However, its connection to the dynamical process of quantum
thermalization relies sensitively on nondegeneracy properties of the energy
spectrum, as well as detailed features of individual eigenstates that are
effective only over correspondingly large timescales, rendering it generically
inaccessible given practical timescales and finite experimental resources.
Here, we introduce the notion of energy-band thermalization to address these
limitations, which coarse-grains over energy level spacings with a finite
energy resolution. We show that energy-band thermalization implies the
thermalization of an observable in almost all physical states over accessible
timescales without relying on microscopic properties of the energy eigenvalues
or eigenstates, and conversely, can be efficiently accessed in experiments via
the dynamics of a single initial state (for a given observable) with only
polynomially many resources in the system size. This allows us to directly
determine thermalization, including in the presence of conserved charges, from
this state: Most strikingly, if an observable thermalizes in this initial state
over a finite range of times, then it must thermalize in almost all physical
initial states over all longer timescales. As applications, we derive a
finite-time Mazur-Suzuki inequality for quantum transport with approximately
conserved charges, and establish the thermalization of local observables over
finite timescales in almost all accessible states in (generally inhomogeneous)
dual-unitary quantum circuits. We also propose measurement protocols for
general many-qubit systems. This work initiates a rigorous treatment of quantum
thermalization in terms of experimentally accessible quantities.; 84) Minimal Shortfall Strategies for Liquidation of a Basket of Stocks using
  Reinforcement Learning; This paper studies the ubiquitous problem of liquidating large quantities of
highly correlated stocks, a task frequently encountered by institutional
investors and proprietary trading firms. Traditional methods in this setting
suffer from the curse of dimensionality, making them impractical for
high-dimensional problems. In this work, we propose a novel method based on
stochastic optimal control to optimally tackle this complex multidimensional
problem. The proposed method minimizes the overall execution shortfall of
highly correlated stocks using a reinforcement learning approach. We rigorously
establish the convergence of our optimal trading strategy and present an
implementation of our algorithm using intra-day market data.; 85) Quicker flocking in aligning active matters for noisier beginning; The constituents in a class of active matter systems change their directions
of motion by being influenced by the velocities of the neighbors. Such systems
may undergo phase transitions, with respect to ordering in the velocity field,
as well as clustering in the density field, when the strength of an externally
imposed noise is varied. Via computer simulations, with a well-known model,
that faithfully represents these systems, we show that evolutions in both
clustering and ordering exhibit certain interesting features that were hitherto
unrealized. The transformations occur quicker, following quenches to a fixed
final state, below the transition point, for disordered starting states that
are farther away from the ``critical"" noise strength. This implies earliest
arrival of the farthest, at a given destination. Detailed analysis of the
results, combined with the outcomes from a similar study of para- to
ferromagnetic transitions, show that the variation in critical fluctuations in
the initial configurations can lead to such interesting effect. We quantify
this via the Ornstein-Zernike theory.; 86) MatIR: A Hybrid Mamba-Transformer Image Restoration Model; In recent years, Transformers-based models have made significant progress in
the field of image restoration by leveraging their inherent ability to capture
complex contextual features. Recently, Mamba models have made a splash in the
field of computer vision due to their ability to handle long-range dependencies
and their significant computational efficiency compared to Transformers.
However, Mamba currently lags behind Transformers in contextual learning
capabilities. To overcome the limitations of these two models, we propose a
Mamba-Transformer hybrid image restoration model called MatIR. Specifically,
MatIR cross-cycles the blocks of the Transformer layer and the Mamba layer to
extract features, thereby taking full advantage of the advantages of the two
architectures. In the Mamba module, we introduce the Image Inpainting State
Space (IRSS) module, which traverses along four scan paths to achieve efficient
processing of long sequence data. In the Transformer module, we combine
triangular window-based local attention with channel-based global attention to
effectively activate the attention mechanism over a wider range of image
pixels. Extensive experimental results and ablation studies demonstrate the
effectiveness of our approach.; 87) Learning Part Knowledge to Facilitate Category Understanding for
  Fine-Grained Generalized Category Discovery; Generalized Category Discovery (GCD) aims to classify unlabeled data
containing both seen and novel categories. Although existing methods perform
well on generic datasets, they struggle in fine-grained scenarios. We attribute
this difficulty to their reliance on contrastive learning over global image
features to automatically capture discriminative cues, which fails to capture
the subtle local differences essential for distinguishing fine-grained
categories. Therefore, in this paper, we propose incorporating part knowledge
to address fine-grained GCD, which introduces two key challenges: the absence
of annotations for novel classes complicates the extraction of the part
features, and global contrastive learning prioritizes holistic feature
invariance, inadvertently suppressing discriminative local part patterns. To
address these challenges, we propose PartGCD, including 1) Adaptive Part
Decomposition, which automatically extracts class-specific semantic parts via
Gaussian Mixture Models, and 2) Part Discrepancy Regularization, enforcing
explicit separation between part features to amplify fine-grained local part
distinctions.
  Experiments demonstrate state-of-the-art performance across multiple
fine-grained benchmarks while maintaining competitiveness on generic datasets,
validating the effectiveness and robustness of our approach.; 88) EDEA: Efficient Dual-Engine Accelerator for Depthwise Separable
  Convolution with Direct Data Transfer; Depthwise separable convolution (DSC) has emerged as a crucial technique,
especially for resource-constrained devices. In this paper, we propose a
dual-engine for the DSC hardware accelerator, which enables the full
utilization of depthwise convolution (DWC) and pointwise convolution (PWC)
processing elements (PEs) in all DSC layers. To determine the optimal dataflow,
data reuse, and configuration of the target architecture, we conduct a design
space exploration using MobileNetV1 with the CIFAR10 dataset. In the
architecture, we introduce an additional non-convolutional unit, which merges
the dequantization, batch normalization (BN), ReLU, and quantization between
DWC and PWC into a simple fixed-point multiplication and addition operation.
This also reduces the intermediate data access between the DWC and PWC,
enabling streaming operation and reducing latency. The proposed DSC dual-engine
accelerator is implemented using the 22nm FDSOI technology from
GlobalFoundries, occupying an area of 0.58 $mm^2$. After signoff, it can
operate at 1 GHz at TT corner, achieving a peak energy efficiency of 13.43
TOPS/W with a throughput of 973.55 GOPS with 8-bit precision. The average
energy efficiency of all DSC layers on MobileNetV1 is 11.13 TOPS/W,
demonstrating substantial hardware efficiency improvements for DSC-based
applications.; 89) Multi-Agent Systems Execute Arbitrary Malicious Code; Multi-agent systems coordinate LLM-based agents to perform tasks on users'
behalf. In real-world applications, multi-agent systems will inevitably
interact with untrusted inputs, such as malicious Web content, files, email
attachments, etc.
  Using several recently proposed multi-agent frameworks as concrete examples,
we demonstrate that adversarial content can hijack control and communication
within the system to invoke unsafe agents and functionalities. This results in
a complete security breach, up to execution of arbitrary malicious code on the
user's device and/or exfiltration of sensitive data from the user's
containerized environment. We show that control-flow hijacking attacks succeed
even if the individual agents are not susceptible to direct or indirect prompt
injection, and even if they refuse to perform harmful actions.; 90) Parameterized Algorithms for Matching Integer Programs with Additional
  Rows and Columns; We study integer linear programs (ILP) of the form $\min\{c^\top x\ \vert\
Ax=b,l\le x\le u,x\in\mathbb Z^n\}$ and analyze their parameterized complexity
with respect to their distance to the generalized matching problem--following
the well-established approach of capturing the hardness of a problem by the
distance to triviality. The generalized matching problem is an ILP where each
column of the constraint matrix has $1$-norm of at most $2$. It captures
several well-known polynomial time solvable problems such as matching and flow
problems. We parameterize by the size of variable and constraint backdoors,
which measure the least number of columns or rows that must be deleted to
obtain a generalized matching ILP.
  We present the following results: (i) a fixed-parameter tractable (FPT)
algorithm for ILPs parameterized by the size $p$ of a minimum variable backdoor
to generalized matching; (ii) a randomized slice-wise polynomial (XP) time
algorithm for ILPs parameterized by the size $h$ of a minimum constraint
backdoor to generalized matching as long as $c$ and $A$ are encoded in unary;
(iii) we complement (ii) by proving that solving an ILP is W[1]-hard when
parameterized by $h$ even when $c,A,l,u$ have coefficients of constant size. To
obtain (i), we prove a variant of lattice-convexity of the degree sequences of
weighted $b$-matchings, which we study in the light of SBO jump M-convex
functions. This allows us to model the matching part as a polyhedral constraint
on the integer backdoor variables. The resulting ILP is solved in FPT time
using an integer programming algorithm. For (ii), the randomized XP time
algorithm is obtained by pseudo-polynomially reducing the problem to the exact
matching problem. To prevent an exponential blowup in terms of the encoding
length of $b$, we bound the Graver complexity of the constraint matrix and
employ a Graver augmentation local search framework.; 91) Towards Multimodal Empathetic Response Generation: A Rich
  Text-Speech-Vision Avatar-based Benchmark; Empathetic Response Generation (ERG) is one of the key tasks of the affective
computing area, which aims to produce emotionally nuanced and compassionate
responses to user's queries. However, existing ERG research is predominantly
confined to the singleton text modality, limiting its effectiveness since human
emotions are inherently conveyed through multiple modalities. To combat this,
we introduce an avatar-based Multimodal ERG (MERG) task, entailing rich text,
speech, and facial vision information. We first present a large-scale
high-quality benchmark dataset, \textbf{AvaMERG}, which extends traditional
text ERG by incorporating authentic human speech audio and dynamic talking-face
avatar videos, encompassing a diverse range of avatar profiles and broadly
covering various topics of real-world scenarios. Further, we deliberately
tailor a system, named \textbf{Empatheia}, for MERG. Built upon a Multimodal
Large Language Model (MLLM) with multimodal encoder, speech and avatar
generators, Empatheia performs end-to-end MERG, with Chain-of-Empathetic
reasoning mechanism integrated for enhanced empathy understanding and
reasoning. Finally, we devise a list of empathetic-enhanced tuning strategies,
strengthening the capabilities of emotional accuracy and content,
avatar-profile consistency across modalities. Experimental results on AvaMERG
data demonstrate that Empatheia consistently shows superior performance than
baseline methods on both textual ERG and MERG. Overall, this work is expected
to pioneer the MERG research by introducing a novel benchmark and an end-to-end
model, laying a solid foundation for future advancements in multimodal
empathetic response generation.; 92) Ruddlesden-Popper defects act as a free surface: role in formation and
  photophysical properties of CsPbI3; The perovskite semiconductor, CsPbI3, holds excellent promise for solar cell
applications due to its suitable bandgap. However, achieving phase-stable
CsPbI3 solar cells with high power conversion efficiency remains a major
challenge. Ruddlesden-Popper (RP) defects have been identified in a range of
perovskite semiconductors, including CsPbI3. However, there is limited
understanding as to why they form or their impact on stability and
photophysical properties. Here we increase the prevalence of RP defects with
increased Cs-excess in vapour-deposited CsPbI3 thin films and observe superior
structural stability but inferior photophysical properties. Significantly,
using electron microscopy, we find that the atomic positions at the planar
defect are comparable to those of a free surface, revealing their role in phase
stabilisation. Density functional theory (DFT) calculations reveal the RP
planes are electronically benign, however, antisites observed at RP turning
points are likely to be malign. We therefore propose that increasing RP planes
while reducing RP turning points could offer a breakthrough for improving both
phase stability and photophysical performance. The formation mechanism revealed
here may well apply more generally to RP structures in other perovskite
systems.; 93) Games! What are they good for? The Struggle of Serious Game Adoption for
  Rehabilitation; The field of serious games for health has grown significantly, demonstrating
effectiveness in various clinical contexts such as stroke, spinal cord injury,
and degenerative neurological diseases. Despite their potential benefits,
therapists face barriers to adopting serious games in rehabilitation, including
limited training and game literacy, concerns about cost and equipment
availability, and a lack of evidence-based research on game effectiveness.
Serious games for rehabilitation often involve repetitive exercises, which can
be tedious and reduce motivation for continued rehabilitation, treating clients
as passive recipients of clinical outcomes rather than players. This study
identifies gaps and provides essential insights for advancing serious games in
rehabilitation, aiming to enhance their engagement for clients and
effectiveness as a therapeutic tool. Addressing these challenges requires a
paradigm shift towards developing and co-creating serious games for
rehabilitation with therapists, researchers, and stakeholders. Furthermore,
future research is crucial to advance the development of serious games,
ensuring they adhere to evidence-based principles and engage both clients and
therapists. This endeavor will identify gaps in the field, inspire new
directions, and support the creation of practical guidelines for serious games
research.; 94) Parking on the Random Recursive Tree; We study the parking process on the random recursive tree. We first prove
that although the random recursive tree has a non-degenerate Benjamini--Schramm
limit, the phase transition for the parking process appears at density $0$. We
then identify the critical window for appearance of a positive flux of cars
with high probability. In the case of binary car arrivals, this happens at
density $ \log (n)^{-2+o(1)}$ where $n$ is the size of the tree. This is the
first work that studies the parking process on trees with possibly large degree
vertices.; 95) Quantum algorithms and lower bounds for eccentricity, radius, and
  diameter in undirected graphs; The problems of computing eccentricity, radius, and diameter are fundamental
to graph theory. These parameters are intrinsically defined based on the
distance metric of the graph. In this work, we propose quantum algorithms for
the diameter and radius of undirected, weighted graphs in the adjacency list
model. The algorithms output diameter and radius with the corresponding paths
in $\widetilde{O}(n\sqrt{m})$ time. Additionally, for the diameter, we present
a quantum algorithm that approximates the diameter within a $2/3$ ratio in
$\widetilde{O}(\sqrt{m}n^{3/4})$ time. We also establish quantum query lower
bounds of $\Omega(\sqrt{nm})$ for all the aforementioned problems through a
reduction from the minima finding problem.; 96) From Traditional to Deep Learning Approaches in Whole Slide Image
  Registration: A Methodological Review; Whole slide image (WSI) registration is an essential task for analysing the
tumour microenvironment (TME) in histopathology. It involves the alignment of
spatial information between WSIs of the same section or serial sections of a
tissue sample. The tissue sections are usually stained with single or multiple
biomarkers before imaging, and the goal is to identify neighbouring nuclei
along the Z-axis for creating a 3D image or identifying subclasses of cells in
the TME. This task is considerably more challenging compared to radiology image
registration, such as magnetic resonance imaging or computed tomography, due to
various factors. These include gigapixel size of images, variations in
appearance between differently stained tissues, changes in structure and
morphology between non-consecutive sections, and the presence of artefacts,
tears, and deformations. Currently, there is a noticeable gap in the literature
regarding a review of the current approaches and their limitations, as well as
the challenges and opportunities they present. We aim to provide a
comprehensive understanding of the available approaches and their application
for various purposes. Furthermore, we investigate current deep learning methods
used for WSI registration, emphasising their diverse methodologies. We examine
the available datasets and explore tools and software employed in the field.
Finally, we identify open challenges and potential future trends in this area
of research.; 97) Transcriptome signature for the identification of bevacizumab responders
  in ovarian cancer; The standard of care for ovarian cancer comprises cytoreductive surgery,
followed by adjuvant platinum-based chemotherapy plus taxane therapy and
maintenance therapy with the antiangiogenic compound bevacizumab and/or a PARP
inhibitor. Nevertheless, there is currently no clear clinical indication for
the use of bevacizumab, highlighting the urgent need for biomarkers to assess
the response to bevacizumab. In the present study, based on a novel RNA-seq
dataset (n=181) and a previously published microarray-based dataset (n=377), we
have identified an expression signature potentially associated with benefit
from bevacizumab addition and assumed to reflect cancer stemness acquisition
driven by activation of CTCFL. Patients with this signature demonstrated
improved overall survival when bevacizumab was added to standard chemotherapy
in both novel (HR=0.41(0.23-0.74), adj.p-value=7.70e-03) and previously
published cohorts (HR=0.51(0.34-0.75), adj.p-value=3.25e-03), while no
significant differences in survival explained by treatment were observed in
patients negative for this signature. In addition to the CTCFL signature, we
found several other reproducible expression signatures which may also represent
biomarker candidates not related to established molecular subtypes of ovarian
cancer and require further validation studies based on additional RNA-seq data.; 98) Impact of the returning radiation on X-ray reflection spectroscopy
  measurements: the case of Galactic black holes; The effect of the returning radiation has long been ignored in the analysis
of the reflection spectra of Galactic black holes and active galactic nuclei
and only recently has been implemented in the relxill package. Here we present
a study on the impact of the returning radiation on the estimate of the
parameters of Galactic black holes. We consider high-quality NuSTAR spectra of
three Galactic black holes (GX 339-4, Swift J1658.2-4242, and MAXI J1535-571)
and we fit the data with the lamppost model in the latest version of relxill,
first without including the returning radiation and then including the
returning radiation. We do not find any significant difference in the estimate
of the parameters of these systems between the two cases, even if all three
sources are fast-rotating black holes and for two sources the estimate of the
height of the corona is very low, two ingredients that should maximize the
effect of the returning radiation. We discuss our results and the
approximations in relxill.; 99) Linear convective stability of a front superposition with unstable
  connecting state; We study convective stability of a two-front superposition in a
reaction-diffusion system. Due to the instability of the connecting
equilibrium, long-range semi-strong interaction is expected between the two
waves. When restricting to the linear dynamic, we indeed identify that
convective stability of superposed waves occurs for fewer propagation speeds
than for the corresponding single waves. It reflects the interaction that
monostable waves have over long distances. Our method relies on numerical range
estimates, that imply time-uniform resolvent bounds.; 100) Evidence for similar collectivity of high transverse momentum particles
  in pPb and PbPb collisions; Charged hadron elliptic anisotropies ($v_2$) are presented over a wide
transverse momentum ($p_\text{T}$) range for proton-lead (pPb) and lead-lead
(PbPb) collisions at nucleon-nucleon center-of-mass energies of 8.16 and 5.02
TeV, respectively. The data were recorded by the CMS experiment and correspond
to integrated luminosities of 186 nb$^{-1}$ and 0.607 nb$^{-1}$ for the pPb and
PbPb systems, respectively. A four-particle cumulant analysis is performed
using subevents separated in pseudorapidity to effectively suppress
non-collective effects. At high $p_\text{T}$ ($p_\text{T}$ $\gt$ 8 GeV),
significant positive $v_2$ values are observed that are similar between pPb and
PbPb collisions at comparable charged particle multiplicities. This observation
suggests a common origin for the multi-particle collectivity for
high-$p_\text{T}$ particles in the two systems.",0.25,0.0
2412.00036,applied,2412.00036-pos1-2,"Quant GANs: deep generation of financial time series; Modeling financial time series by stochastic processes is a challenging task
and a central area of research in financial mathematics. As an alternative, we
introduce Quant GANs, a data-driven model which is inspired by the recent
success of generative adversarial networks (GANs). Quant GANs consist of a
generator and discriminator function, which utilize temporal convolutional
networks (TCNs) and thereby achieve to capture long-range dependencies such as
the presence of volatility clusters. The generator function is explicitly
constructed such that the induced stochastic process allows a transition to its
risk-neutral distribution. Our numerical results highlight that distributional
properties for small and large lags are in an excellent agreement and
dependence properties such as volatility clusters, leverage effects, and serial
autocorrelations can be generated by the generator function of Quant GANs,
demonstrably in high fidelity.",2412.00036-pos2-2,"On the Distribution of the Two-Sample Cramer-von Mises Criterion; The Cramer-von Mises $\omega^2$ criterion for testing that a sample, $x_1, \cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \begin{equation*}\tag{1}\omega^2 = \int^\infty_{-\infty} \lbrack F_N(x) - F(x)\rbrack^2 dF(x),\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\omega^2$, namely \begin{equation*}\tag{2} T NM/(N + M)\rbrack G_M(x)\rbrack^2 dH_{N+M}(x),\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\rbrack$. limiting $N\omega^2$ as $N \rightarrow \infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \infty, M \infty$, $N/M \lambda$, $\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4].",89,"['1', '4', '5', '6', '12', '13', '20', '60', '68', '77']","The first paper, 'Spin-valley polarization control in WSe2 monolayers using photochemical doping,' connects to the main paper on Quant GANs by introducing a novel method for controlling quantum states, which can enhance the understanding of stochastic processes in financial modeling. The synergy between financial time series modeling and advanced quantum control techniques offers a unique angle for financial predictions that utilize generative models. The integration of quantum mechanics principles into financial forecasting is both novel and useful, fulfilling the criteria for a multidisciplinary research idea. The next papers follow in decreasing relevance and potential for integration with the concepts presented in the main paper, with each subsequent entry providing varying degrees of applicability to the multidisciplinary theme proposed.","1) Spin-valley polarization control in WSe$_2$ monolayers using
  photochemical doping; We report on the influence of a photochemical doping method on the
spin-valley polarization degree ($P_{c}$) of excitons in WSe$_2$ monolayers. By
varying the carrier density and transitioning from an excess of electrons
(n-type) to an excess of holes (p-type), we observe a non-monotonic dependence
of $P_{c}$ on the doping level. Using controlled, single-shot photochlorination
steps, we unveil this non-monotonic behavior, with $P_{c}$ reaching a minimum
value of less than 10$\%$ at 78 K near the charge neutrality point, while
increasing by a factor of three at a hole density of $5 \times 10^{11}
\,\mathrm{cm^{-2}}$. The impact of the doping on $P_{c}$ is explained using a
phenomenological model that accounts for various mechanisms influencing exciton
polarization dynamics, including exciton-carrier scattering processes and
exciton-to-trion conversion rates. Among these, exciton-carrier collisions
emerge as the dominant mechanism driving the observed variations in $P_{c}$,
while the exciton effective lifetime remains nearly independent of doping.
These findings highlight the potential of photochemical methods for
investigating valley physics and for effectively tuning the exciton
polarization degree in transition metal dichalcogenide monolayers.; 2) Pointwise estimates for the fundamental solutions of higher order
  schr\""{o}dinger equations with finite rank perturbations; This paper is dedicated to studying pointwise estimates of the fundamental
solution for the higher order Schr\""{o}dinger equation: % we investigate the
fundamental solution of the higher order Schr\""{o}dinger equation
$$i{\partial}_{t}u(x,t)=Hu(x,t),\ \ \ t\in \mathbb{R},\ x\in
{\mathbb{R}}^{n},$$ where the Hamiltonian $H$ is defined as
$$H={(-\Delta)}^{m}+\displaystyle\sum_{j=1}^{N} \langle\cdotp ,{\varphi }_{j}
\rangle{\varphi }_{j},$$ with each $\varphi_j$ ($1\le j\le N$) satisfying
certain smoothness and decay conditions. %Let ${P}_{ac}(H)$ denote the
projection onto the absolutely continuous space of $H$. We show that for any
positive integer $m>1$ and spatial dimension $n\ge 1$, %under a spectral
assumption, the operator is sharp in the sense that it
  ${e}^{-i tH}P_{ac}(H)$ has an integral kernel $K(t,x,y)$ satisfying the
following pointwise estimate: $$\left |K(t,x,y)\right |\lesssim
|t|^{-\frac{n}{2m}}(1+|t|^{-\frac{1}{2m}}\left | x-y\right
|)^{-\frac{n(m-1)}{2m-1}} ,\ \ t\ne 0,\ x,y\in {\mathbb{R}}^{n}.$$ This
estimate is consistent with the upper bounds in the free case. As an
application, we derive $L^p-L^q$ decay estimates for the propagator ${e}^{-\i
tH}P_{ac}(H)$, where the pairs $(1/p, 1/q)$ lie within a quadrilateral region
in the plane.; 3) TAPAS: Thermal- and Power-Aware Scheduling for LLM Inference in Cloud
  Platforms; The rising demand for generative large language models (LLMs) poses
challenges for thermal and power management in cloud datacenters. Traditional
techniques often are inadequate for LLM inference due to the fine-grained,
millisecond-scale execution phases, each with distinct performance, thermal,
and power profiles. Additionally, LLM inference workloads are sensitive to
various configuration parameters (e.g., model parallelism, size, and
quantization) that involve trade-offs between performance, temperature, power,
and output quality. Moreover, clouds often co-locate SaaS and IaaS workloads,
each with different levels of visibility and flexibility. We propose TAPAS, a
thermal- and power-aware framework designed for LLM inference clusters in the
cloud. TAPAS enhances cooling and power oversubscription capabilities, reducing
the total cost of ownership (TCO) while effectively handling emergencies (e.g.,
cooling and power failures). The system leverages historical temperature and
power data, along with the adaptability of SaaS workloads, to: (1) efficiently
place new GPU workload VMs within cooling and power constraints, (2) route LLM
inference requests across SaaS VMs, and (3) reconfigure SaaS VMs to manage load
spikes and emergency situations. Our evaluation on a large GPU cluster
demonstrates significant reductions in thermal and power throttling events,
boosting system efficiency.; 4) Heterogeneity of household stock portfolios in a national market; We study the long term dynamics of the stock portfolios owned by single
Finnish legal entities in the Helsinki venue of the Nasdaq Nordic between 2001
and 2021. Using the Herfindahl-Hirschman index as a measure of concentration
for the composition of stock portfolios, we investigate the concentration of
Finnish household portfolios both at the level of each individual household and
tracking the time evolution of an aggregated Finnish household portfolio. We
also consider aggregated portfolios of two other macro categories of investors
one comprising Finnish institutional investors and the other comprising foreign
investors. Different macro categories of investors present a different degree
of concentration of aggregated stock portfolios with highest concentration
observed for foreign investors. For individual Finnish retail investors,
portfolio concentration estimated by the Herfindahl-Hirschman index presents
high values for more than half of the total number of retail investors. In
spite of the observation that retail stock portfolios are often composed by
just a few stocks, the concentration of the aggregated stock portfolio for
Finnish retail investors has a portfolio concentration comparable with the one
of Finnish institutional investors. Within retail investors, stock portfolios
of women present a similar pattern of portfolios of men but with a systematic
higher level of concentration observed for women both at individual and at
aggregated level.; 5) A learning agent-based approach to the characterization of open quantum
  systems; Characterizing quantum processes is crucial for the execution of quantum
algorithms on available quantum devices. A powerful framework for this purpose
is the Quantum Model Learning Agent (QMLA) which characterizes a given system
by learning its Hamiltonian via adaptive generations of informative experiments
and their validation against simulated models. Identifying the incoherent noise
of a quantum device in addition to its coherent interactions is, however, as
essential. Precise knowledge of such imperfections of a quantum device allows
to devise strategies to mitigate detrimental effects, for example via quantum
error correction. We introduce the open Quantum Model Learning Agent (oQMLA)
framework to account for Markovian noise through the Liouvillian formalism. By
simultaneously learning the Hamiltonian and jump operators, oQMLA independently
captures both the coherent and incoherent dynamics of a system. The added
complexity of open systems necessitates advanced algorithmic strategies. Among
these, we implement regularization to steer the algorithm towards plausible
models and an unbiased metric to evaluate the quality of the results. We
validate our implementation in simulated scenarios of increasing complexity,
demonstrating its robustness to hardware-induced measurement errors and its
ability to characterize systems using only local operations. Additionally, we
develop a scheme to interface oQMLA with a publicly available superconducting
quantum computer, showcasing its practical utility. These advancements
represent a significant step toward improving the performance of quantum
hardware and contribute to the broader goal of advancing quantum technologies
and their applications.; 6) Innovative Financing Solutions: A Transformative Driver for Financial
  Performance of Businesses in Morocco; In a rapidly evolving landscape marked by continuous change and complex
challenges, effective cash management stands as a cornerstone for ensuring
business sustainability and driving performance. To address these pressing
demands, cash managersare increasingly turning to innovative financing
solutions such as venture capital, green finance, crowdfunding, advanced
services from Pan-African banks, and blockchain technology. These cutting-edge
tools are pivotal in bolstering resilience against market volatility,
ecological transitions, and the accelerating pace of technological change. The
present article aims to examine how such innovative financial approaches can
serve as strategic drivers, enabling businesses to transform challenges into
opportunities. The analysis underscores that rethinking cash management through
innovation is a critical pathway toboost the performance of Moroccan companies.
Therefore, embracing these forward-thinking strategies unlocks new avenues for
development empowering them to adapt with agility amidst the uncertainties of a
shifting environment.; 7) D-Antimagic Labelings of Oriented Star Forests; For a distance set $D$, an oriented graph $\overrightarrow{G}$ is
$D$-antimagic if there exists a bijective vertex labeling such that the sum of
all labels of $D$-out-neighbors is distinct for each vertex. This paper
provides all orientations and all possible $D$s of a $D$-antimagic oriented
star. We provide necessary and sufficient condition for $D$-antimagic oriented
star forest containing isomorphic oriented stars. We show that for all possible
$D$s, there exists an orientation for a star forest to admit a $D$-antimagic
labeling.; 8) Intent Tagging: Exploring Micro-Prompting Interactions for Supporting
  Granular Human-GenAI Co-Creation Workflows; Despite Generative AI (GenAI) systems' potential for enhancing content
creation, users often struggle to effectively integrate GenAI into their
creative workflows. Core challenges include misalignment of AI-generated
content with user intentions (intent elicitation and alignment), user
uncertainty around how to best communicate their intents to the AI system
(prompt formulation), and insufficient flexibility of AI systems to support
diverse creative workflows (workflow flexibility). Motivated by these
challenges, we created IntentTagger: a system for slide creation based on the
notion of Intent Tags - small, atomic conceptual units that encapsulate user
intent - for exploring granular and non-linear micro-prompting interactions for
Human-GenAI co-creation workflows. Our user study with 12 participants provides
insights into the value of flexibly expressing intent across varying levels of
ambiguity, meta-intent elicitation, and the benefits and challenges of intent
tag-driven workflows. We conclude by discussing the broader implications of our
findings and design considerations for GenAI-supported content creation
workflows.; 9) Petting Pen for Stress Awareness and Management in Children; We found that children in elementary school often experience stress during
task performance. Limited coping skills and lack of stress awareness restrict
children's ability to manage their stress. Many designs and studies have
proposed different stress detection and intervention solutions. Still, they
often overlook the potential of enhancing everyday objects and actively sensing
stress-related behavioral data during human-product interaction. Therefore, we
propose Petting pen as an interactive robotic object for children to manage
their stress during task performance. It detects and validates stress and
further intervenes in stress during a process of natural writing and relaxation
interactions. The design is an iteration based on our previous research results
of a stress-aware pen, enhanced with tactile needs, robotic interaction, and
integration of behavioral and bio-sensing capabilities. Petting pen is supposed
to bridge the gap between robots and everyday objects in mental health
applications for children.; 10) The Birth of Quantum Mechanics: A Historical Study Through the Canonical
  Papers; This paper explores the historical development of the theory of quantum
mechanics between 1900 and 1927 by chronological examination of the
foundational papers and ideas. Beginning with Planck's introduction of energy
quantisation in blackbody radiation, we follow the emergence of Einstein's
light quanta hypothesis, Bohr's atomic model, and the statistical implications
of indistinguishable particles. Special emphasis is placed on the transition
from the Old Quantum Theory to modern quantum mechanics, particularly through
Heisenberg's matrix mechanics and Schr\""{o}dinger's wave mechanics. This study
aims to provide a structured historical account, offering insights into the
conceptual transformations that led to quantum mechanics while making the
development accessible to physicists, historians of science, and advanced
students interested in the origins of modern quantum theory.; 11) The weight part of Serre's conjecture over CM fields; Under some technical assumptions of a global nature, we establish the weight
part of Serre's conjecture for mod $p$ Galois representations for CM fields
that are tamely ramified and sufficiently generic at $p$.; 12) The role of FDI along transitional dynamics of the host country in an
  endogenous growth model; We investigate the role of foreign direct investment (FDI) in the
transitional dynamics of host countries by using an optimal growth model. FDI
may be beneficial for the host country because local people can work for
multinational firms to get a favorable salary. However, if the host country
only focuses on FDI, it may face a middle-income trap. We show that if the host
country invests in research and development, its economy may have sustained
growth. Moreover, in this case, FDI helps the host country only at the first
stages of its development process.; 13) Forecasting realized volatility in the stock market: a path-dependent
  perspective; Volatility forecasting in financial markets is a topic that has received more
attention from scholars. In this paper, we propose a new volatility forecasting
model that combines the heterogeneous autoregressive (HAR) model with a family
of path-dependent volatility models (HAR-PD). The model utilizes the long- and
short-term memory properties of price data to capture volatility features and
trend features. By integrating the features of path-dependent volatility into
the HAR model family framework, we develop a new set of volatility forecasting
models. And, we propose a HAR-REQ model based on the empirical quartile as a
threshold, which exhibits stronger forecasting ability compared to the HAR-REX
model. Subsequently, the predictive performance of the HAR-PD model family is
evaluated by statistical tests using data from the Chinese stock market and
compared with the basic HAR model family. The empirical results show that the
HAR-PD model family has higher forecasting accuracy compared to the underlying
HAR model family. In addition, robustness tests confirm the significant
predictive power of the HAR-PD model family.; 14) Inducing And Probing Charge Migration In Molecular Systems; Technological advancements in generation of ultrafast and intense laser
pulses have enabled the real-time observation and control of charge migration
in molecules on their natural timescale, which ranges from few femtoseconds to
several hundreds of attoseconds. Present thesis discusses the effect of
symmetry on the adiabatic attosecond charge migration in different molecular
systems. The spatial representation of the charge migration is documented by
time-dependent electronic charge and flux densities. Furthermore, the induced
charge migration is imaged via time resolved x-ray diffraction (TRXD) with
atomic-scale spatiotemporal resolution in few cases.; 15) VMTS: Vision-Assisted Teacher-Student Reinforcement Learning for
  Multi-Terrain Locomotion in Bipedal Robots; Bipedal robots, due to their anthropomorphic design, offer substantial
potential across various applications, yet their control is hindered by the
complexity of their structure. Currently, most research focuses on
proprioception-based methods, which lack the capability to overcome complex
terrain. While visual perception is vital for operation in human-centric
environments, its integration complicates control further. Recent reinforcement
learning (RL) approaches have shown promise in enhancing legged robot
locomotion, particularly with proprioception-based methods. However, terrain
adaptability, especially for bipedal robots, remains a significant challenge,
with most research focusing on flat-terrain scenarios. In this paper, we
introduce a novel mixture of experts teacher-student network RL strategy, which
enhances the performance of teacher-student policies based on visual inputs
through a simple yet effective approach. Our method combines terrain selection
strategies with the teacher policy, resulting in superior performance compared
to traditional models. Additionally, we introduce an alignment loss between the
teacher and student networks, rather than enforcing strict similarity, to
improve the student's ability to navigate diverse terrains. We validate our
approach experimentally on the Limx Dynamic P1 bipedal robot, demonstrating its
feasibility and robustness across multiple terrain types.; 16) Tuning Algorithmic and Architectural Hyperparameters in Graph-Based
  Semi-Supervised Learning with Provable Guarantees; Graph-based semi-supervised learning is a powerful paradigm in machine
learning for modeling and exploiting the underlying graph structure that
captures the relationship between labeled and unlabeled data. A large number of
classical as well as modern deep learning based algorithms have been proposed
for this problem, often having tunable hyperparameters. We initiate a formal
study of tuning algorithm hyperparameters from parameterized algorithm families
for this problem. We obtain novel $O(\log n)$ pseudo-dimension upper bounds for
hyperparameter selection in three classical label propagation-based algorithm
families, where $n$ is the number of nodes, implying bounds on the amount of
data needed for learning provably good parameters. We further provide matching
$\Omega(\log n)$ pseudo-dimension lower bounds, thus asymptotically
characterizing the learning-theoretic complexity of the parameter tuning
problem. We extend our study to selecting architectural hyperparameters in
modern graph neural networks. We bound the Rademacher complexity for tuning the
self-loop weighting in recently proposed Simplified Graph Convolution (SGC)
networks. We further propose a tunable architecture that interpolates graph
convolutional neural networks (GCN) and graph attention networks (GAT) in every
layer, and provide Rademacher complexity bounds for tuning the interpolation
coefficient.; 17) AutoCBT: An Autonomous Multi-agent Framework for Cognitive Behavioral
  Therapy in Psychological Counseling; Traditional in-person psychological counseling remains primarily niche, often
chosen by individuals with psychological issues, while online automated
counseling offers a potential solution for those hesitant to seek help due to
feelings of shame. Cognitive Behavioral Therapy (CBT) is an essential and
widely used approach in psychological counseling. The advent of large language
models (LLMs) and agent technology enables automatic CBT diagnosis and
treatment. However, current LLM-based CBT systems use agents with a fixed
structure, limiting their self-optimization capabilities, or providing hollow,
unhelpful suggestions due to redundant response patterns. In this work, we
utilize Quora-like and YiXinLi single-round consultation models to build a
general agent framework that generates high-quality responses for single-turn
psychological consultation scenarios. We use a bilingual dataset to evaluate
the quality of single-response consultations generated by each framework. Then,
we incorporate dynamic routing and supervisory mechanisms inspired by real
psychological counseling to construct a CBT-oriented autonomous multi-agent
framework, demonstrating its general applicability. Experimental results
indicate that AutoCBT can provide higher-quality automated psychological
counseling services.; 18) MATS: An Audio Language Model under Text-only Supervision; Large audio-language models (LALMs), built upon powerful Large Language
Models (LLMs), have exhibited remarkable audio comprehension and reasoning
capabilities. However, the training of LALMs demands a large corpus of
audio-language pairs, which requires substantial costs in both data collection
and training resources. In this paper, we propose MATS, an audio-language
multimodal LLM designed to handle Multiple Audio task using solely Text-only
Supervision. By leveraging pre-trained audio-language alignment models such as
CLAP, we develop a text-only training strategy that projects the shared
audio-language latent space into LLM latent space, endowing the LLM with audio
comprehension capabilities without relying on audio data during training. To
further bridge the modality gap between audio and language embeddings within
CLAP, we propose the Strongly-related noisy text with audio (Santa) mechanism.
Santa maps audio embeddings into CLAP language embedding space while preserving
essential information from the audio input. Extensive experiments demonstrate
that MATS, despite being trained exclusively on text data, achieves competitive
performance compared to recent LALMs trained on large-scale audio-language
pairs.; 19) New Sufficient Algebraic Conditions for Local Consistency over
  Homogeneous Structures of Finite Duality; The path to the solution of Feder-Vardi dichotomy conjecture by Bulatov and
Zhuk led through showing that more and more general algebraic conditions imply
polynomial-time algorithms for the finite-domain Constraint Satisfaction
Problems (CSPs) whose templates satisfy them. These investigations resulted in
the discovery of the appropriate height 1 Maltsev conditions characterizing
bounded strict width, bounded width, the applicability of the few-subpowers
algorithm, and many others.
  For problems in the range of the similar Bodirsky-Pinsker conjecture on
infinite-domain CSPs, one can only find such a characterization for the notion
of bounded strict width, with a proof essentially the same as in the finite
case. In this paper, we provide the first non-trivial results showing that
certain height 1 Maltsev conditions imply bounded width, and in consequence
tractability, for a natural subclass of templates within the Bodirsky-Pinsker
conjecture which includes many templates in the literature as well as templates
for which no complexity classification is known.; 20) Maximum Welfare Allocations under Quantile Valuations; We propose a new model for aggregating preferences over a set of indivisible
items based on a quantile value. In this model, each agent is endowed with a
specific quantile, and the value of a given bundle is defined by the
corresponding quantile of the individual values of the items within it. Our
model captures the diverse ways in which agents may perceive a bundle, even
when they agree on the values of individual items. It enables richer behavioral
modeling that cannot be easily captured by additive valuation functions. We
study the problem of maximizing utilitarian and egalitarian welfare within the
quantile-based valuation setting. For each of the welfare functions, we analyze
the complexity of the objectives. Interestingly, our results show that the
complexity of both objectives varies significantly depending on whether the
allocation is required to be balanced. We provide near-optimal approximation
algorithms for utilitarian welfare, and for egalitarian welfare, we present
exact algorithms whenever possible.; 21) A Machine Learning Approach to Automatic Fall Detection of Soldiers; Military personnel and security agents often face significant physical risks
during conflict and engagement situations, particularly in urban operations.
Ensuring the rapid and accurate communication of incidents involving injuries
is crucial for the timely execution of rescue operations. This article presents
research conducted under the scope of the Brazilian Navy's ``Soldier of the
Future'' project, focusing on the development of a Casualty Detection System to
identify injuries that could incapacitate a soldier and lead to severe blood
loss. The study specifically addresses the detection of soldier falls, which
may indicate critical injuries such as hypovolemic hemorrhagic shock. To
generate the publicly available dataset, we used smartwatches and smartphones
as wearable devices to collect inertial data from soldiers during various
activities, including simulated falls. The data were used to train 1D
Convolutional Neural Networks (CNN1D) with the objective of accurately
classifying falls that could result from life-threatening injuries. We explored
different sensor placements (on the wrists and near the center of mass) and
various approaches to using inertial variables, including linear and angular
accelerations. The neural network models were optimized using Bayesian
techniques to enhance their performance. The best-performing model and its
results, discussed in this article, contribute to the advancement of automated
systems for monitoring soldier safety and improving response times in
engagement scenarios.; 22) Probabilistic Subspace Manifolds for Contextual Inference in Large
  Language Models; Representing token embeddings as probability distributions over learned
manifolds allows for more flexible contextual inference, reducing
representational rigidity while enhancing semantic granularity. Comparative
evaluations demonstrate that probabilistic embeddings improve neighborhood
consistency and decrease redundancy, ensuring that token relationships remain
more structurally coherent across fine-tuning iterations. The integration of
probabilistic subspaces within attention mechanisms facilitates more adaptive
contextual weighting, enabling models to capture latent dependencies that would
otherwise be obscured in conventional embeddings. Experimental results
highlight increased robustness against adversarial modifications, with
probabilistic embeddings preserving contextual integrity even under
perturbation-based evaluation scenarios. Performance assessments indicate that
probabilistic representations achieve greater adaptability in domain-specific
applications, mitigating the need for extensive retraining when shifting across
linguistic domains. Computational trade-offs remain within operationally
feasible limits, with marginal increases in inference latency balanced against
the benefits of enhanced representation stability and contextual
expressiveness. The capacity to encode structured uncertainty provides
advantages in generative modeling tasks, particularly where maintaining
coherence across extended sequences requires a representation framework capable
of handling ambiguous or context-dependent linguistic constructs.; 23) Utilizing Pre-trained and Large Language Models for 10-K Items
  Segmentation; Extracting specific items from 10-K reports remains challenging due to
variations in document formats and item presentation. Traditional rule-based
item segmentation approaches often yield suboptimal results. This study
introduces two advanced item segmentation methods leveraging language models:
(1) GPT4ItemSeg, using a novel line-ID-based prompting mechanism to utilize
GPT4 for item segmentation, and (2) BERT4ItemSeg, combining BERT embeddings
with a Bi-LSTM model in a hierarchical structure to overcome context window
constraints. Trained and evaluated on 3,737 annotated 10-K reports,
BERT4ItemSeg achieved a macro-F1 of 0.9825, surpassing GPT4ItemSeg (0.9567),
conditional random field (0.9818), and rule-based methods (0.9048) for core
items (1, 1A, 3, and 7). These approaches enhance item segmentation
performance, improving text analytics in accounting and finance. BERT4ItemSeg
offers satisfactory item segmentation performance, while GPT4ItemSeg can easily
adapt to regulatory changes. Together, they offer practical benefits for
researchers and practitioners, enabling reliable empirical studies and
automated 10-K item segmentation functionality.; 24) Complex oscillations of non-definite Sturm-Liouville problems, II; We correct and update a result of R.G.D. Richardson [13] dealing with the
separation of zeros of the real and imaginary parts of non-real eigenfunctions
of non-definite Sturm-Liouville eigenvalue problems. We then extend it to the
case where the weight function is allowed to be identically zero on a
subinterval that excludes the end-points and study the behavior of the zeros of
the real and imaginary parts when the end-points are included. Examples are
given illustrating the sharpness of the results along with open questions.; 25) Clustering of functional data prone to complex heteroscedastic
  measurement error; Several factors make clustering of functional data challenging, including the
infinite-dimensional space to which observations belong and the lack of a
defined probability density function for the functional random variable. To
overcome these barriers, researchers either assume that observations belong to
a finite-dimensional space spanned by basis functions or apply nonparametric
smoothing methods to the functions prior to clustering. Although extensive
literature describes clustering methods for functional data, few studies have
explored the clustering of measurement error--prone function-valued data. In
this work, we consider clustering methods for functional data prone to complex,
heteroscedastic measurement errors. Two stage-based methods using mixed-effects
models are first applied to adjust for measurement error bias, followed by
cluster analysis of the measurement error--adjusted curves. Through
simulations, we investigate how varying sample size, the magnitude of
measurement error, and the presence of complex heteroscedastic measurement
errors influence the cluster analysis of functional data. Our results indicate
that failing to account for measurement errors and the correlation structures
associated with frequently collected functional data reduces the accuracy of
identifying the true latent groups or clusters. The method consistently
produces better results regardless of the initial clustering values used.
Moreover, it is flexible and can be applied to various clustering approaches,
based on the specific distribution of the data. The developed methods are
applied to two data sets: a school-based study of energy expenditure among
elementary school-aged children in Texas and data from the National Health and
Nutrition Examination Survey on participants' physical activity monitored by
wearable devices at frequent intervals.; 26) Deep Learning-Enhanced Visual Monitoring in Hazardous Underwater
  Environments with a Swarm of Micro-Robots; Long-term monitoring and exploration of extreme environments, such as
underwater storage facilities, is costly, labor-intensive, and hazardous.
Automating this process with low-cost, collaborative robots can greatly improve
efficiency. These robots capture images from different positions, which must be
processed simultaneously to create a spatio-temporal model of the facility. In
this paper, we propose a novel approach that integrates data simulation, a
multi-modal deep learning network for coordinate prediction, and image
reassembly to address the challenges posed by environmental disturbances
causing drift and rotation in the robots' positions and orientations. Our
approach enhances the precision of alignment in noisy environments by
integrating visual information from snapshots, global positional context from
masks, and noisy coordinates. We validate our method through extensive
experiments using synthetic data that simulate real-world robotic operations in
underwater settings. The results demonstrate very high coordinate prediction
accuracy and plausible image assembly, indicating the real-world applicability
of our approach. The assembled images provide clear and coherent views of the
underwater environment for effective monitoring and inspection, showcasing the
potential for broader use in extreme settings, further contributing to improved
safety, efficiency, and cost reduction in hazardous field monitoring. Code is
available on https://github.com/ChrisChen1023/Micro-Robot-Swarm.; 27) Designing Scheduling for Diffusion Models via Spectral Analysis; Diffusion models (DMs) have emerged as powerful tools for modeling complex
data distributions and generating realistic new samples. Over the years,
advanced architectures and sampling methods have been developed to make these
models practically usable. However, certain synthesis process decisions still
rely on heuristics without a solid theoretical foundation. In our work, we
offer a novel analysis of the DM's inference process, introducing a
comprehensive frequency response perspective. Specifically, by relying on
Gaussianity and shift-invariance assumptions, we present the inference process
as a closed-form spectral transfer function, capturing how the generated signal
evolves in response to the initial noise. We demonstrate how the proposed
analysis can be leveraged for optimizing the noise schedule, ensuring the best
alignment with the original dataset's characteristics. Our results lead to
scheduling curves that are dependent on the frequency content of the data,
offering a theoretical justification for some of the heuristics taken by
practitioners.; 28) Super-Linear Growth and Rising Inequality in Online Social Communities:
  Insights from Reddit; We study the effect of the number of users on the activity of communities
within the online content sharing and discussion platform Reddit, called
subreddits. We found that comment activity on Reddit has a heavy-tailed
distribution, where a large fraction of the comments are made by a small set of
users. Furthermore, as subreddits grow in size, this behavior becomes stronger,
with activity (measured by the comments made in a subreddit) becoming even more
centralised in a (relatively) smaller core of users. We verify that these
changes are not explained by finite size nor by sampling effects. Instead, we
observe a systematic change of the distribution with subreddit size. To
quantify the centralisation and inequality of activity in a subreddit, we used
the Gini coefficient. We found that as subreddits grow in users, so does the
Gini coefficient, seemingly as a natural effect of the scaling. We found that
the excess number of comments (the total number of comments minus the total
number of users) follows a power law with exponent 1.27. For each subreddit we
considered a snapshot of one month of data, as a compromise between statistical
relevance and change in the system's dynamics. We show results over the whole
year 2021 (with each subreddit having twelve snapshots, at most), nevertheless
all results were consistent when using a single month or different years.; 29) Variational representation and estimates for the free energy of a
  quenched charged polymer model; Random walks with a disordered self-interaction potential may be used to
model charged polymers. In this paper we consider a one-dimensional and
directed version of the charged polymer model that was introduced by Derrida,
Griffiths and Higgs. We prove new results for the associated quenched free
energy, including a variational formula based on a quenched large deviation
principle established by Birkner, Greven and den Hollander. We also take the
occasion to (i) provide detailed proofs for state-of-the-art results pointing
towards the existence of a freezing transition and (ii) proceed with minor
corrections for two results previously obtained by the present author with
Caravenna, den Hollander and P{\'e}tr{\'e}lis for the undirected model.; 30) REPA: Russian Error Types Annotation for Evaluating Text Generation and
  Judgment Capabilities; Recent advances in large language models (LLMs) have introduced the novel
paradigm of using LLMs as judges, where an LLM evaluates and scores the outputs
of another LLM, which often correlates highly with human preferences. However,
the use of LLM-as-a-judge has been primarily studied in English. In this paper,
we evaluate this framework in Russian by introducing the Russian Error tyPes
Annotation dataset (REPA), a dataset of 1k user queries and 2k LLM-generated
responses. Human annotators labeled each response pair expressing their
preferences across ten specific error types, as well as selecting an overall
preference. We rank six generative LLMs across the error types using three
rating systems based on human preferences. We also evaluate responses using
eight LLM judges in zero-shot and few-shot settings. We describe the results of
analyzing the judges and position and length biases. Our findings reveal a
notable gap between LLM judge performance in Russian and English. However,
rankings based on human and LLM preferences show partial alignment, suggesting
that while current LLM judges struggle with fine-grained evaluation in Russian,
there is potential for improvement.; 31) GaussMark: A Practical Approach for Structural Watermarking of Language
  Models; Recent advances in Large Language Models (LLMs) have led to significant
improvements in natural language processing tasks, but their ability to
generate human-quality text raises significant ethical and operational concerns
in settings where it is important to recognize whether or not a given text was
generated by a human. Thus, recent work has focused on developing techniques
for watermarking LLM-generated text, i.e., introducing an almost imperceptible
signal that allows a provider equipped with a secret key to determine if given
text was generated by their model. Current watermarking techniques are often
not practical due to concerns with generation latency, detection time,
degradation in text quality, or robustness. Many of these drawbacks come from
the focus on token-level watermarking, which ignores the inherent structure of
text. In this work, we introduce a new scheme, GaussMark, that is simple and
efficient to implement, has formal statistical guarantees on its efficacy,
comes at no cost in generation latency, and embeds the watermark into the
weights of the model itself, providing a structural watermark. Our approach is
based on Gaussian independence testing and is motivated by recent empirical
observations that minor additive corruptions to LLM weights can result in
models of identical (or even improved) quality. We show that by adding a small
amount of Gaussian noise to the weights of a given LLM, we can watermark the
model in a way that is statistically detectable by a provider who retains the
secret key. We provide formal statistical bounds on the validity and power of
our procedure. Through an extensive suite of experiments, we demonstrate that
GaussMark is reliable, efficient, and relatively robust to corruptions such as
insertions, deletions, substitutions, and roundtrip translations and can be
instantiated with essentially no loss in model quality.; 32) Higgs Thermal Nonequilibrium in Primordial QGP; In this work we investigate the chemical and kinetic nonequilibrium dynamics
of the Higgs boson during the primordial Universe QGP (quark-gluon plasma)
epoch $130\mathrm{\,GeV}>T>10\mathrm{\,GeV}$. We show that the Higgs bosons is
always out of chemical abundance equilibrium with a fugacity $\Upsilon_h =
0.69$ due to virtual decay channels. Additionally, Higgs momentum distribution
is found to be ``cold'' for $T<40$\,GeV, since the scattering rate drops below
the production rate.; 33) Accelerate High-Quality Diffusion Models with Inner Loop Feedback; We propose Inner Loop Feedback (ILF), a novel approach to accelerate
diffusion models' inference. ILF trains a lightweight module to predict future
features in the denoising process by leveraging the outputs from a chosen
diffusion backbone block at a given time step. This approach exploits two key
intuitions; (1) the outputs of a given block at adjacent time steps are
similar, and (2) performing partial computations for a step imposes a lower
burden on the model than skipping the step entirely. Our method is highly
flexible, since we find that the feedback module itself can simply be a block
from the diffusion backbone, with all settings copied. Its influence on the
diffusion forward can be tempered with a learnable scaling factor from zero
initialization. We train this module using distillation losses; however, unlike
some prior work where a full diffusion backbone serves as the student, our
model freezes the backbone, training only the feedback module. While many
efforts to optimize diffusion models focus on achieving acceptable image
quality in extremely few steps (1-4 steps), our emphasis is on matching best
case results (typically achieved in 20 steps) while significantly reducing
runtime. ILF achieves this balance effectively, demonstrating strong
performance for both class-to-image generation with diffusion transformer (DiT)
and text-to-image generation with DiT-based PixArt-alpha and PixArt-sigma. The
quality of ILF's 1.7x-1.8x speedups are confirmed by FID, CLIP score, CLIP
Image Quality Assessment, ImageReward, and qualitative comparisons. Project
information is available at https://mgwillia.github.io/ilf.; 34) On the minimal Blow-up rate for the 2D modified Zakharov-Kuznetsov model; In this note we consider the modified $L^2$ critical Zakharov-Kuznetsov
equation in $\mathbb R^2$, for initial conditions in the Sobolev space $H^s$
with $s>3/4.$ Assuming that there is a blow up solution at finite time $T^{*}$,
we obtain a lower bound for the blow up rate of that solution, expressed in
terms of a lower bound for the $H^s$ norm of the solution. It is found a
nontrivial gap between conjectured blow up rates and our results. The analysis
is based on properly quantifying the linear estimates given by Faminskii, as
well as, the local well-posedness theory of Linares and Pastor, combined with
an argument developed by Weissler for study singular solutions of the
semilinear heat equations and also used by Colliander-Czuback-Sulem in the
context of the Zakharov system.; 35) Black Hole Waterfall: a unitary phenomenological model for black hole
  evaporation with Page curve; We present a unitary phenomenological model for black hole evaporation based
on the analogy of the laboratory process of spontaneous parametric down
conversion (SPDC) when the black hole (pump) is allowed to deplete to zero
mass. The model incorporates an additional new feature that allows for the
interior Hawking partner-particles (idlers) behind the horizon to further
generate new Hawking particle pairs of lower energy, one of which remains
behind the horizon, and the other that adds to the externally emitted Hawking
radiation (signals) outside the horizon. This model produces a Page curve for
the evolution of the reduced density matrices for the evaporating black hole
internal degrees of freedom entangled with the generated Hawking radiation
pairs entangled across the horizon. The Page curve yields an entropy that rises
at early times during the evaporation process as Hawking pairs are generated,
reaches a peak midway through the evolution, and then decays to zero upon
complete evaporation of the black hole. The entire system remains in a pure
state at all times undergoing unitary (squeezed state) evolution, with the
initial state of the black hole modeled as a bosonic Fock state of large, but
finite number $n_{p0}$ of particles. For the final state of the system, the
black hole reaches the vacuum state of zero mass, while the external Hawking
radiation carries away the total energy of the initial black hole. Inside the
horizon there remains $n_{p0}$ Hawking partner-particles of vanishingly small
total energy, reminiscent of the ""soft-hair"" (zero energy) qubit model of
Hotta, Nambu and Yamaguchi, but now from a Hamiltonian for squeezed state
generation perspective. The model presented here can be readily extended to
encompass arbitrary initial pure states for the black hole, and in falling
matter.; 36) Holistic Optimization Framework for FPGA Accelerators; Customized accelerators have transformed modern computing by enhancing energy
efficiency and performance through specialization. Field Programmable Gate
Arrays play a pivotal role in this domain due to their flexibility and
high-performance potential. High-Level Synthesis and source-to-source compilers
simplify hardware design by converting high-level code into hardware
descriptions enriched with directives. However, achieving high Quality of
Results in FPGA designs remains challenging, requiring complex transformations,
strategic directive use, and efficient data management. While existing
approaches like Design Space Exploration (DSE) and source-to-source compilers
have made strides in improving performance, they often address isolated aspects
of the design process. This paper introduces Prometheus, a holistic framework
that integrates task fusion, tiling, loop permutation,
computation-communication overlap, and concurrent task execution into a unified
design space. Leveraging Non-Linear Problem methodologies, Prometheus explores
this space to find solutions under resource constraints, enabling bitstream
generation.; 37) Control of magnon frequency combs in magnetic rings; Using Brillouin light scattering microscopy, we study the rich dynamics in
magnetic disks and rings governed by non-linear interactions, focusing on the
role of vortex core dynamics on the spin-wave eigenmode spectrum. By strongly
exciting quantized magnon modes in magnetic vortices, self-induced magnon
Floquet states are populated by the intrinsic nonlinear coupling of magnon
modes to the vortex core gyration. In magnetic rings, however, this generation
is suppressed even when exciting the system over a large power range. To
retrieve the rich nonlinear dynamics in rings, we apply external in-plane
magnetic fields by which the vortex core is restored. Our findings demonstrate
how to take active control of the nonlinear processes in magnetic structures of
different topology.; 38) Distributed Generalized Nash Equilibria Learning for Online Stochastic
  Aggregative Games; This paper investigates online stochastic aggregative games subject to local
set constraints and time-varying coupled inequality constraints, where each
player possesses a time-varying expectation-valued cost function relying on not
only its own decision variable but also an aggregation of all the players'
variables. Each player can only access its local individual cost function and
constraints, necessitating partial information exchanges with neighboring
players through time-varying unbalanced networks. Additionally, local cost
functions and constraint functions are not prior knowledge and only revealed
gradually. To learn generalized Nash equilibria of such games, a novel
distributed online stochastic algorithm is devised based on push-sum and
primal-dual strategies. Through rigorous analysis, high probability bounds on
the regret and constraint violation are provided by appropriately selecting
decreasing stepsizes. Moreover, for a time-invariant stochastic strongly
monotone game, it is shown that the generated sequence by the designed
algorithm converges to its variational generalized Nash equilibrium (GNE)
almost surely, and the time-averaged sequence converges sublinearly with high
probability. Finally, the derived theoretical results are illustrated by
numerical simulations.; 39) Insights of Transitions to Thermoacoustic Instability in Inverse
  Diffusion Flame using Multifractal Detrended Fluctuation Analysis; The inverse diffusion flame (IDF) can experience thermoacoustic instability
due to variations in power input or flow conditions. However, the dynamical
transitions in IDF that lead to this instability when altering control
parameters have not been thoroughly investigated. In this study, we explore the
control parameters through two different approaches and employ multifractal
detrended fluctuation analysis to characterize the transitions observed prior
to the onset of thermoacoustic instability in the inverse diffusion flame. Our
findings reveal a loss of multifractality near the region associated with
thermoacoustic instability, which suggests a more ordered behavior. We
determine that the singularity exponent, the width of the multifractal
spectrum, and the Hurst exponent are reliable indicators of thermoacoustic
instability and serve as effective classifiers of dynamical states in inverse
diffusion flames.; 40) Non uniform expansion and additive noise imply random horseshoe; We propose a notion of random horseshoe and prove density of random
horseshoes for non uniformly expanding random dynamical systems with additive
noise; 41) Learning Part Knowledge to Facilitate Category Understanding for
  Fine-Grained Generalized Category Discovery; Generalized Category Discovery (GCD) aims to classify unlabeled data
containing both seen and novel categories. Although existing methods perform
well on generic datasets, they struggle in fine-grained scenarios. We attribute
this difficulty to their reliance on contrastive learning over global image
features to automatically capture discriminative cues, which fails to capture
the subtle local differences essential for distinguishing fine-grained
categories. Therefore, in this paper, we propose incorporating part knowledge
to address fine-grained GCD, which introduces two key challenges: the absence
of annotations for novel classes complicates the extraction of the part
features, and global contrastive learning prioritizes holistic feature
invariance, inadvertently suppressing discriminative local part patterns. To
address these challenges, we propose PartGCD, including 1) Adaptive Part
Decomposition, which automatically extracts class-specific semantic parts via
Gaussian Mixture Models, and 2) Part Discrepancy Regularization, enforcing
explicit separation between part features to amplify fine-grained local part
distinctions.
  Experiments demonstrate state-of-the-art performance across multiple
fine-grained benchmarks while maintaining competitiveness on generic datasets,
validating the effectiveness and robustness of our approach.; 42) Systematic calculation on alpha decay and cluster radioactivity of
  superheavy nuclei; In the Coulomb and Proximity Potential Model (CPPM) framework, we have
investigated the cluster radioactivity and alpha decay half-lives of superheavy
nuclei. We study 22 different versions of proximity potential forms that have
been proposed to describe proton radioactivity, two-proton radioactivity,
heavy-ion radioactivity, quasi-elastic scattering, fusion reactions, and other
applications. The half-lives of cluster radioactivity and alpha decay of 41
atomic nuclei ranging from 221Fr to 244Cm were calculated, and the results
indicate that the refined nuclear potential named BW91 is the most suitable
proximity potential form for the cluster radioactivity and alpha decay of
superheavy nuclei since the root-mean-square (RMS) deviation between the
experimental data and the relevant theoretical calculation results is the
smallest ({\sigma}= 0.841). By using CPPM, we predicted the half-lives of 20
potential cluster radioactivity and alpha decay candidates. These cluster
radioactivities and alpha decays are energetically allowed or observable but
not yet quantified in NUBASE2020.; 43) IRONMAP: Iron Network Mapping and Analysis Protocol for Detecting
  Over-Time Brain Iron Abnormalities in Neurological Disease; Pathologically altered iron levels, detected using iron-sensitive MRI
techniques such as quantitative susceptibility mapping (QSM), are observed in
neurological disorders such as multiple sclerosis (MS) and may play a crucial
role in disease pathophysiology. However, brain iron changes occur slowly, even
in neurological diseases, and can be influenced by physiological factors such
as diet. Therefore, novel analysis methods are needed to improve sensitivity to
disease-related iron changes as compared to conventional region-based analysis
methods. This study introduces IRONMAP, Iron Network Mapping and Analysis
Protocol, which is a novel network-based analysis method to evaluate over-time
changes in magnetic susceptibility. With this novel methodology, we analyzed
short-term (<1 year) longitudinal QSM data from a cohort of individuals with MS
(pwMS) and healthy controls (HCs) and assessed disease-related network
patterns, comparing the new approach to a conventional per-region
rate-of-change method. IRONMAP analysis was able to detect over-time,
MS-related brain iron abnormalities that were undetectable using the
rate-of-change approach. IRONMAP was applicable on the per-subject level,
improving binary classification of pwMS vs HCs compared to rate-of-change data
alone (areas under the curve: 0.773 vs 0.636, p = 0.024). Further analysis
revealed that the observed IRONMAP-derived HC network structure closely aligned
with simulated networks based on healthy aging-related susceptibility data,
suggesting that disruptions in normal aging-related iron changes may contribute
to the network differences seen in pwMS. IRONMAP is generalizable to any
neurological disease, including Alzheimer's disease and Parkinson's disease,
and may allow for study of brain iron abnormalities over shorter timeframes
than previously possible.; 44) Diffusion Autoencoders are Scalable Image Tokenizers; Tokenizing images into compact visual representations is a key step in
learning efficient and high-quality image generative models. We present a
simple diffusion tokenizer (DiTo) that learns compact visual representations
for image generation models. Our key insight is that a single learning
objective, diffusion L2 loss, can be used for training scalable image
tokenizers. Since diffusion is already widely used for image generation, our
insight greatly simplifies training such tokenizers. In contrast, current
state-of-the-art tokenizers rely on an empirically found combination of
heuristics and losses, thus requiring a complex training recipe that relies on
non-trivially balancing different losses and pretrained supervised models. We
show design decisions, along with theoretical grounding, that enable us to
scale DiTo for learning competitive image representations. Our results show
that DiTo is a simpler, scalable, and self-supervised alternative to the
current state-of-the-art image tokenizer which is supervised. DiTo achieves
competitive or better quality than state-of-the-art in image reconstruction and
downstream image generation tasks.; 45) Absorption loss and Kerr nonlinearity in barium titanate waveguides; Because of its exceptionally large Pockels coefficient, barium titanate
(BaTiO$_3$) is a promising material for various photonic applications at both
room and cryogenic temperatures, including electro-optic modulation, frequency
comb generation, and microwave-optical transduction. These applications rely on
devices with low optical loss to achieve high efficiency. Material absorption
sets a lower limit to optical loss and is thus a crucial property to determine,
particularly for integrated photonic devices. Using cavity-enhanced
photothermal spectroscopy, we measure the absorption loss of BaTiO$_3$ ridge
waveguides at wavelengths near 1550~nm to be $\alpha_{\mathrm{abs}} =
10.9$~{\raisebox{0.5ex}{\tiny$^{+5.8}_{-0.4}$}} dB~m$^{-1}$, well below the
propagation losses due to other sources, such as scattering. We simultaneously
determine that BaTiO$_3$ has a large Kerr nonlinear refractive index of
$n_{\mathrm{2,BaTiO_3}}$ = 1.8 {\raisebox{0.5ex}{\tiny$^{+0.3}_{-0.3}$}}
$\times$ 10$^{-18}$ m$^2$ W$^{-1}$. Considering these results, photonic
integrated circuits utilizing BaTiO$_3$ have the potential to achieve
significantly higher efficiency than demonstrated to date and are especially
interesting for applications exploiting the combination of Pockels and Kerr
effects.; 46) Advancing sustainable energy solutions with microfluidic porous media; The transition to a sustainable, low-carbon energy future requires
transformative advancements in energy and environmental technologies. Carbon
capture and sequestration, underground hydrogen storage, and nuclear waste
geological disposal will be central aspects of a sustainable energy future,
both for mitigating CO2 emissions and providing green energy. A comprehensive
understanding of multiphase flow through porous media, along with reactive
transport and microbial activities, is essential for assessing the feasibility
and managing the risks of these technologies. Microfluidic porous media
platforms have emerged as powerful tools for the direct visualization of
multiphase reactive flow in porous media and eventually optimizing these
multiple physicochemical and biological processes. This review highlights
critical scientific challenges associated with these sustainable energy
solutions and summarizes the state-of-the-art microfluidic techniques for
studying the interplay between multiphase flow, reactive transport, and
biological effects in porous media. We provide a comprehensive overview of how
these microfluidic approaches enhance the understanding of fundamental
pore-scale dynamics and bridge the gap between pore-scale events and
large-scale processes. This review is expected to promote both experimental and
theoretical understanding of multiphase reactive flow in porous media, thereby
informing material design, process optimization, and predictive modeling for
scalable implementation. By fostering interdisciplinary collaboration across
microfluidics, fluid mechanics, geophysics, materials science, and subsurface
engineering, we hope to accelerate innovation and advance sustainable energy
solutions.; 47) Nonlinear dynamics of localization in neural receptive fields; Localized receptive fields -- neurons that are selective for certain
contiguous spatiotemporal features of their input -- populate early sensory
regions of the mammalian brain. Unsupervised learning algorithms that optimize
explicit sparsity or independence criteria replicate features of these
localized receptive fields, but fail to explain directly how localization
arises through learning without efficient coding, as occurs in early layers of
deep neural networks and might occur in early sensory regions of biological
systems. We consider an alternative model in which localized receptive fields
emerge without explicit top-down efficiency constraints -- a feedforward neural
network trained on a data model inspired by the structure of natural images.
Previous work identified the importance of non-Gaussian statistics to
localization in this setting but left open questions about the mechanisms
driving dynamical emergence. We address these questions by deriving the
effective learning dynamics for a single nonlinear neuron, making precise how
higher-order statistical properties of the input data drive emergent
localization, and we demonstrate that the predictions of these effective
dynamics extend to the many-neuron setting. Our analysis provides an
alternative explanation for the ubiquity of localization as resulting from the
nonlinear dynamics of learning in neural circuits.; 48) Encountering Robotic Art: The Social, Material, and Temporal Processes
  of Creation with Machines; Robots extend beyond the tools of productivity; they also contribute to
creativity. While typically defined as utility-driven technologies designed for
productive or social settings, the role of robots in creative settings remains
underexplored. This paper examines how robots participate in artistic creation.
Through semi-structured interviews with robotic artists, we analyze the impact
of robots on artistic processes and outcomes. We identify the critical roles of
social interaction, material properties, and temporal dynamics in facilitating
creativity. Our findings reveal that creativity emerges from the
co-constitution of artists, robots, and audiences within spatial-temporal
dimensions. Based on these insights, we propose several implications for
socially informed, material-attentive, and process-oriented approaches to
creation with computing systems. These approaches can inform the domains of
HCI, including media and art creation, craft, digital fabrication, and tangible
computing.; 49) Effect of a new type of healthy and live food supplement on osteoporosis
  blood parameters and induced rheumatoid arthritis in Wistar rats; Summary Osteoporosis is a skeletal disorder, characterized by a decrease in
bone strength and puts the individual at risk for fracture. On the other hand,
rheumatoid arthritis is a systemic disease of unknown etiology that causes
inflammation of the joints of the organs. Purpose Due to the destructive
effects of these diseases and its increasing prevalence and lack of appropriate
medication for treatment, the present study aimed to evaluate the therapeutic
effect of a new type of healthy and live food supplement on rheumatoid
arthritis and induced osteoporosis in rats. Methods In this research, healthy
and live food powder were synthesized by a new and green route. This organic
biomaterial was named NBS. The NBS food supplement had various vitamins, macro
and micro molecules, and ingredients. The new healthy and nutritious diet
showed that the use of this supplement led to the return of the parameters to
normal levels. Results The concentration of 12.5 mg/ kg showed the least
therapeutic effect and 50 mg/ kg had the highest therapeutic effect for
osteoporosis. The results of blood parameters involved in inflammation in both
healthy and patient groups showed that the use of complete adjuvant induction
causes joint inflammation. In the study of the interaction of the
concentrations, it was observed that the concentration of 50 mg/ kg had the
highest therapeutic effect against the disease in the studied mice. Conclusion
The results showed that the new healthy and viable supplement restores the
blood osteoporotic and rheumatoid factors of the mice to normal.; 50) Spin-polarized superconductivity from excitonic Cooper pairs; We present a theory of spin-polarized superconductivity from the condensation
of excitonic Cooper pairs, which are charge-$2e$ bosonic quasiparticles made of
Cooper pairs strongly hybridized with excitons. By solving a model of
spin-polarized electrons using the strong-coupling expansion to the second
order, we demonstrate the emergence of excitonic Cooper pairs from
electron-hole fluctuations upon doping a strongly correlated insulator. We
characterize their binding energy, effective mass, and the resulting
superconducting transition temperature. We propose possible realization of
spin-polarized superconductivity in twisted semiconductors with honeycomb
moir\'e superlattice.; 51) Maximal estimates and pointwise convergence for solutions of certain
  dispersive equations with radial initial data on Damek-Ricci spaces; One of the most celebrated problems in Euclidean Harmonic analysis is the
Carleson's problem: determining the optimal regularity of the initial condition
$f$ of the Schr\""odinger equation given by \begin{equation*} \begin{cases}
  i\frac{\partial u}{\partial t} -\Delta_{\mathbb{R}^n} u=0\:,\:\:\: (x,t) \in
\mathbb{R}^n \times \mathbb{R}\:, \newline
  u(0,\cdot)=f\:, \text{ on } \mathbb{R}^n \:,
  \end{cases} \end{equation*} in terms of the index $\beta$ such that $f$
belongs to the inhomogeneous Sobolev space $H^\beta(\mathbb{R}^n)$ , so that
the solution of the Schr\""odinger operator $u$ converges pointwise to $f$,
$\displaystyle\lim_{t \to 0+} u(x,t)=f(x)$, almost everywhere. In this article,
we address the Carleson's problem for the fractional Schr\""odinger equation,
the Boussinesq equation and the Beam equation corresponding to both the
Laplace-Beltrami operator $\Delta$ and the shifted Laplace-Beltrami operator
$\tilde{\Delta}$, with radial initial data on Damek-Ricci spaces, by obtaining
a complete description of the local (in space) mapping properties for the
corresponding local (in time) maximal functions. Consequently, we obtain the
sharp bound up to the endpoint $\beta \ge 1/4$, for (almost everywhere)
pointwise convergence. We also establish an abstract transference principle for
dispersive equations whose corresponding multipliers have comparable
oscillation and also apply it in the proof of our main result.; 52) The Dynamical History of the Kepler-221 Planet System; Kepler-221 is a G-type star hosting four planets. In this system, planets b,
c, and e are in (or near) a 6:3:1 three-body resonance even though the planets'
period ratios show significant departures from exact two-body commensurability.
Importantly, the intermediate planet d is not part of the resonance chain. To
reach this resonance configuration, we propose a scenario in which there were
originally five planets in the system in a chain of first-order resonances.
After disk dispersal, the resonance chain became unstable and two planets
quickly merged to become the current planet d. In addition, the b/c/e
three-body resonance was re-established. We run N-body simulations using
REBOUND to investigate the parameter space under which this scenario can
operate. We find that our envisioned scenario is possible when certain
conditions are met. First, the reformation of the three-body resonance after
planet merging requires convergent migration between planets b and c. Second,
as has previously pointed out, an efficient damping mechanism must operate to
power the expansion of the b/c/e system. We find that planet d plays a crucial
role during the orbital expansion phase due to destabilizing encounters of a
three-body resonance between c, d, and e. A successful orbital expansion phase
puts constraints on the planet properties in the Kepler-221 system including
the planet mass ratios and the tidal quality factors for the planets. Our model
can also be applied to other planet systems in resonance, such as Kepler-402
and K2-138.; 53) T2VEval: Benchmark Dataset and Objective Evaluation Method for
  T2V-generated Videos; Recent advances in text-to-video (T2V) technology, as demonstrated by models
such as Runway Gen-3, Pika, Sora, and Kling, have significantly broadened the
applicability and popularity of the technology. This progress has created a
growing demand for accurate quality assessment metrics to evaluate the
perceptual quality of T2V-generated videos and optimize video generation
models. However, assessing the quality of text-to-video outputs remain
challenging due to the presence of highly complex distortions, such as
unnatural actions and phenomena that defy human cognition. To address these
challenges, we constructed T2VEval-Bench, a multi-dimensional benchmark dataset
for text-to-video quality evaluation, which contains 148 textual prompts and
1,783 videos generated by 13 T2V models. To ensure a comprehensive evaluation,
we scored each video on four dimensions in the subjective experiment, which are
overall impression, text-video consistency, realness, and technical quality.
Based on T2VEval-Bench, we developed T2VEval, a multi-branch fusion scheme for
T2V quality evaluation. T2VEval assesses videos across three branches:
text-video consistency, realness, and technical quality. Using an
attention-based fusion module, T2VEval effectively integrates features from
each branch and predicts scores with the aid of a large language model.
Additionally, we implemented a divide-and-conquer training strategy, enabling
each branch to learn targeted knowledge while maintaining synergy with the
others. Experimental results demonstrate that T2VEval achieves state-of-the-art
performance across multiple metrics.; 54) Orthogonal Calibration for Asynchronous Federated Learning; Asynchronous federated learning mitigates the inefficiency of conventional
synchronous aggregation by integrating updates as they arrive and adjusting
their influence based on staleness. Due to asynchrony and data heterogeneity,
learning objectives at the global and local levels are inherently inconsistent
-- global optimization trajectories may conflict with ongoing local updates.
Existing asynchronous methods simply distribute the latest global weights to
clients, which can overwrite local progress and cause model drift. In this
paper, we propose OrthoFL, an orthogonal calibration framework that decouples
global and local learning progress and adjusts global shifts to minimize
interference before merging them into local models. In OrthoFL, clients and the
server maintain separate model weights. Upon receiving an update, the server
aggregates it into the global weights via a moving average. For client weights,
the server computes the global weight shift accumulated during the client's
delay and removes the components aligned with the direction of the received
update. The resulting parameters lie in a subspace orthogonal to the client
update and preserve the maximal information from the global progress. The
calibrated global shift is then merged into the client weights for further
training. Extensive experiments show that OrthoFL improves accuracy by 9.6% and
achieves a 12$\times$ speedup compared to synchronous methods. Moreover, it
consistently outperforms state-of-the-art asynchronous baselines under various
delay patterns and heterogeneity scenarios.; 55) Enhanced $A^{*}$ Algorithm for Mobile Robot Path Planning with
  Non-Holonomic Constraints; In this paper, a novel method for path planning of mobile robots is proposed,
taking into account the non-holonomic turn radius constraints and finite
dimensions of the robot. The approach involves rasterizing the environment to
generate a 2D map and utilizes an enhanced version of the $A^{*}$ algorithm
that incorporates non-holonomic constraints while ensuring collision avoidance.
Two new instantiations of the $A^{*}$ algorithm are introduced and tested
across various scenarios and environments, with results demonstrating the
effectiveness of the proposed method.; 56) From Data to Combinatorial Multivector field Through an
  Optimization-Based Framework; This paper extends and generalizes previous works on constructing
combinatorial multivector fields from continuous systems (see [10]) and the
construction of combinatorial vector fields from data (see [2]) by introducing
an optimization based framework for the construction of combinatorial
multivector fields from finite vector field data. We address key challenges in
convexity, computational complexity and resolution, providing theoretical
guarantees and practical methodologies for generating combinatorial
representation of the dynamics of our data.; 57) Late-Time Evolution of Magnetized Disks in Tidal Disruption Events; In classic time-dependent 1D accretion disk models, the inner radiation
pressure dominated regime is viscously unstable. However, late-time
observations of accretion disks formed in tidal disruption events (TDEs) do not
exhibit evidence of such instabilities. The common theoretical response is to
modify the viscosity parametrization, but typically used viscosity
parametrization are generally ad hoc. In this study, we take a different
approach, and investigate a time-dependent 1D $\alpha$-disk model in which the
pressure is dominated by magnetic fields rather than photons. We compare the
time evolution of thermally stable, strongly magnetized TDE disks to the
simpler linear viscosity model. We find that the light curves of magnetized
disks evolve as $L_{\rm UV}\propto t^{-5/6}$ for decades to centuries, and that
this same evolution can be reproduced by the linear viscosity model for
specific parameter choices. Additionally, we show that TDEs remain UV-bright
for many years, suggesting we could possibly find fossil TDEs decades after
their bursts. We estimate that ULTRASAT could detect hundreds of such events,
providing an opportunity to study late-stage TDE physics and supermassive black
hole (SMBH) properties. Finally, we explore the connection between TDE disks
and quasi-periodic eruptions (QPEs) suggested by recent observations. One
theoretical explanation involves TDE disks expanding to interact with extreme
mass ratio inspirals (EMRIs), which produce X-ray flares as the EMRI passes
through the disk. Our estimates indicate that magnetized TDE disks should
exhibit QPEs earlier than those observed in AT2019qiz, suggesting that the QPEs
may have begun before their first detection.; 58) MAD accretion and AGN jets -- an observational perspective; One of the major open questions related to the production of jets by
accreting black holes is: why do sources with similar accretion powers produce
so vastly different jet powers? What conditions are required to make a powerful
jet? If jets are powered by the Blandford-Zjanek mechanism, two further
parameters control the jet power besides the black hole mass - black hole spin
and the magnetic flux threading it. Since highly spinning black holes without
jets appear to exist, the jet production efficiency may depend on whether the
black hole managed to accrete high enough magnetic flux in the past. The
highest-efficiency jets in this picture are launched from magnetically arrested
disks (MADs). Here we discuss a method to test this hypothesis using VLBI
core-shift measurements to estimate the jet magnetic flux.; 59) Hypersurfaces passing through the Galois orbit of a point; Asgarli, Ghioca, and Reichstein recently proved that if $K$ is a field with
$|K|>2$, then for any positive integers $d$ and $n$, and separable field
extension $L/K$ with degree $m=\binom{n+d}{d}$, there exists a point $P\in
\mathbb{P}^n(L)$ which does not lie on any degree $d$ hypersurface defined over
$K$. They asked whether the result holds when $|K| = 2$. We answer their
question in the affirmative by combining various ideas from arithmetic
geometry. More generally, we show that for each positive integer $r$ and
separable field extension $L/K$ with degree $r$, there exists a point $P \in
\mathbb{P}^n(L)$ such that the vector space of degree $d$ forms over $K$ that
vanish at $P$ has the expected dimension. We also discuss applications to
linear systems of hypersurfaces with special properties.; 60) Combined climate stress testing of supply-chain networks and the
  financial system with nation-wide firm-level emission estimates; On the way towards carbon neutrality, climate stress testing provides
estimates for the physical and transition risks that climate change poses to
the economy and the financial system. Missing firm-level CO2 emissions data
severely impedes the assessment of transition risks originating from carbon
pricing. Based on the individual emissions of all Hungarian firms (410,523), as
estimated from their fossil fuel purchases, we conduct a stress test of both
actual and hypothetical carbon pricing policies. Using a simple 1:1 economic
ABM and introducing the new carbon-to-profit ratio, we identify firms that
become unprofitable and default, and estimate the respective loan write-offs.
We find that 45% of all companies are directly exposed to carbon pricing. At a
price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank
equity losses of 1.2% are expected. Secondary default cascades in supply chain
networks could increase these losses by 300% to 4000%, depending on firms'
ability to substitute essential inputs. To reduce transition risks, firms
should reduce their dependence on essential inputs from supply chains with high
CO2 exposure. We discuss the implications of different policy implementations
on these transition risks.; 61) AlphaNet: Scaling Up Local Frame-based Atomistic Interatomic Potential; Molecular dynamics simulations demand unprecedented accuracy and scalability
to tackle grand challenges in energy materials, catalytic processes, and
biomolecular design. To bridge this gap, we present AlphaNet, a local
frame-based equivariant model that simultaneously advances computational
efficiency and predictive precision for atomistic systems. By constructing
equivariant local frames with learnable geometric transitions, AlphaNet encodes
atomic environments with enhanced representational capacity, achieving state of
the art accuracy in energy and force predictions. Extensive benchmarks spanning
defected graphene, formate decomposition, inorganic bulks, and large-scale
datasets (OC2M and Matbench Discovery) demonstrate its superior performance
over existing neural network interatomic potentials while ensuring scalability
across diverse system sizes. The synergy of accuracy, efficiency, and
transferability positions AlphaNet as a transformative tool for simulating
multiscale phenomena, from catalyst dynamics to energy storage interfaces, with
direct implications for accelerating the discovery of functional materials and
complex molecular systems.; 62) Nematic order from phase synchronization of shape oscillations; We show that a suspension of non-interacting deformable particles subjected
to an oscillatory shear flow leads to development of nematic order that arises
from the phenomenon of phase synchronization. The synchronized state
corresponds to a unique, stable limit cycle confined in the toroidal state
space. The limit cycle exists since, unlike rigid particles, deformable
particles can modulate aspect ratio, adjust their tumbling rate and thus,
achieve phase synchronization. These synchronized regions emerge as Arnold
tongues in the parameter-space of the driving amplitude and frequency.
Considering the rheological implications of ordering dynamics in soft and
active matter, our results motivate oscillatory shear flow experiments with
deformable particles.; 63) Acceleration without photon pair creation; Whenever an experiment can be described classically, quantum physics must
predict the same outcome. Intuitively, there is nothing quantum about an
accelerating observer travelling through vacuum. It is, therefore, not
surprising that many people are puzzled by the Unruh effect, which predicts
that the observer encounters photons in a thermal state. This paper uses a
recently introduced local photon approach to show that the quantised
electromagnetic field in a non-inertial reference frame can be modelled without
violating the principles of general relativity while both observers share a
common vacuum. The only difference between a resting and an accelerating
observer is that they each experience different worldline densities which
implies different zero point energy densities in each reference frame.; 64) Properties of the emission region in pulsars with opposite subpulse
  drift directions in different profile components; We investigate properties of the emission region as revealed by drifting
subpulses of opposite drift directions at different parts of a pulse profile by
using the rotating carousel model in an obliquely rotating pulsar magnetosphere
of multiple emission states. Subpulse emission is assumed coming from m
discrete emission areas that are distributed around the magnetic axis on a
rotating carousel. The flow rate of the emission areas is determined by the E x
B drift in an emission state, designated by the parameter y, in which E and the
associated flow rate are dependent on y. In this model, subpulses appear to
drift in an emission state if a relative speed exists between the plasma flow
and corotation, and the diversity in the drift rates and directions corresponds
to the relative speed being different in different parts of a profile. We apply
the model to three pulsars that exhibit drifting subpulses of opposite drift
directions to identify the emission states and the values of m. Our results
show that different drifting subpulses correspond to particular values of m and
y, and the latter implies that different emission states can coexist and
operate concurrently in an emission region. We find that m does not show clear
dependency on either the obliquity angle or emission state. We demonstrate that
subpulse arrangement may vary across an emission region meaning that it is not
always uniform on a carousel. We discuss drifting subpulses of opposite drift
directions and subpulse drift-rate switching in terms of different emission
states in our model, and speculate that they may be two manifestations of the
same underlying mechanism.; 65) Physics-Aware POD-Based Learning for Ab initio QEM-Galerkin Simulations
  of Periodic Nanostructures; Quantum nanostructures offer crucial applications in electronics, photonics,
materials, drugs, etc. For accurate design and analysis of nanostructures and
materials, simulations of the Schrodinger or Schrodinger-like equation are
always needed. For large nanostructures, these eigenvalue problems can be
computationally intensive. One effective solution is a learning method via
Proper Orthogonal Decomposition (POD), together with ab initio Galerkin
projection of the Schrodinger equation. POD-Galerkin projects the problem onto
a reduced-order space with the POD basis representing electron wave functions
(WFs) guided by the first principles in simulations. To minimize training
effort and enhance robustness of POD-Galerkin in larger structures, the quantum
element method (QEM) was proposed previously, which partitions nanostructures
into generic quantum elements. Larger nanostructures can then be constructed by
the trained generic quantum elements, each of which is represented by its
POD-Galerkin model. This work investigates QEM-Galerkin thoroughly in
multi-element quantum-dot (QD) structures on approaches to further improve
training effectiveness and simulation accuracy and efficiency for QEM-Galerkin.
To further improve computing speed, POD and Fourier bases for periodic
potentials are also examined in QEM-Galerkin simulations. Results indicate
that, considering efficiency and accuracy, the POD potential basis is superior
to the Fourier potential basis even for periodic potentials. Overall,
QEM-Galerkin offers more than a 2-order speedup in computation over direct
numerical simulation for multi-element QD structures, and more improvement is
observed in a structure comprising more elements.; 66) Skein modules of closed 3 manifolds define line bundles over character
  varieties; Let M be a closed 3-manifold and S(M) the skein module of M at some odd root
of unity. Using the Frobenius morphism, we can see S(M) as the space of global
sections of a coherent sheaf over the SL2 character scheme of M. We prove that
when the character scheme is reduced, this sheaf is a line bundle.; 67) Implicit Neural Representation for Video and Image Super-Resolution; We present a novel approach for super-resolution that utilizes implicit
neural representation (INR) to effectively reconstruct and enhance
low-resolution videos and images. By leveraging the capacity of neural networks
to implicitly encode spatial and temporal features, our method facilitates
high-resolution reconstruction using only low-resolution inputs and a 3D
high-resolution grid. This results in an efficient solution for both image and
video super-resolution. Our proposed method, SR-INR, maintains consistent
details across frames and images, achieving impressive temporal stability
without relying on the computationally intensive optical flow or motion
estimation typically used in other video super-resolution techniques. The
simplicity of our approach contrasts with the complexity of many existing
methods, making it both effective and efficient. Experimental evaluations show
that SR-INR delivers results on par with or superior to state-of-the-art
super-resolution methods, while maintaining a more straightforward structure
and reduced computational demands. These findings highlight the potential of
implicit neural representations as a powerful tool for reconstructing
high-quality, temporally consistent video and image signals from low-resolution
data.; 68) Distortion risk measures of sums of two counter-monotonic risks; In this paper, we will show that under certain conditions, associated to any
fixed distortion function $g$, the distortion risk measure of a sum of two
counter-monotonic risks can be expressed as the sum of two related distortion
risk measures of the marginals involved, one associated to the original
distortion function $g$ and the other associated to the dual distortion
function of $g$. This result extends some of the work in \cite{Chaoubi et al.
(2020)} and \cite{HLD} since the class of distortion risk measures includes the
risk measure of VaR and TVaR as special cases.; 69) Counterexample to Winkler's conjecture on Venn diagrams; In 1984, Peter Winkler conjectured that every simple Venn diagram with $n$
curves can be extended to a simple Venn diagram with $n+1$ curves. We present a
counterexample to his conjecture for $n=7$, which is obtained by combining
theoretical ideas with computer assistance from state-of-the-art SAT solvers.; 70) The typicality principle and its implications for statistics and data
  science; A central focus of data science is the transformation of empirical evidence
into knowledge. As such, the key insights and scientific attitudes of deep
thinkers like Fisher, Popper, and Tukey are expected to inspire exciting new
advances in machine learning and artificial intelligence in years to come.
Along these lines, the present paper advances a novel {\em typicality
principle} which states, roughly, that if the observed data is sufficiently
``atypical'' in a certain sense relative to a posited theory, then that theory
is unwarranted. This emphasis on typicality brings familiar but often
overlooked background notions like model-checking to the inferential
foreground. One instantiation of the typicality principle is in the context of
parameter estimation, where we propose a new typicality-based regularization
strategy that leans heavily on goodness-of-fit testing. The effectiveness of
this new regularization strategy is illustrated in three non-trivial examples
where ordinary maximum likelihood estimation fails miserably. We also
demonstrate how the typicality principle fits within a bigger picture of
reliable and efficient uncertainty quantification.; 71) Comparison of Vectorization Capabilities of Different Compilers for X86
  and ARM CPUs; Most modern processors contain vector units that simultaneously perform the
same arithmetic operation over multiple sets of operands. The ability of
compilers to automatically vectorize code is critical to effectively using
these units. Understanding this capability is important for anyone writing
compute-intensive, high-performance, and portable code. We tested the ability
of several compilers to vectorize code on x86 and ARM. We used the TSVC2 suite,
with modifications that made it more representative of real-world code. On x86,
GCC reported 54% of the loops in the suite as having been vectorized, ICX
reported 50%, and Clang, 46%. On ARM, GCC reported 56% of the loops as having
been vectorized, ACFL reported 54%, and Clang, 47%. We found that the
vectorized code did not always outperform the unvectorized code. In some cases,
given two very similar vectorizable loops, a compiler would vectorize one but
not the other. We also report cases where a compiler vectorized a loop on only
one of the two platforms. Based on our experiments, we cannot definitively say
if any one compiler is significantly better than the others at vectorizing code
on any given platform.; 72) CHiP: Cross-modal Hierarchical Direct Preference Optimization for
  Multimodal LLMs; Multimodal Large Language Models (MLLMs) still struggle with hallucinations
despite their impressive capabilities. Recent studies have attempted to
mitigate this by applying Direct Preference Optimization (DPO) to multimodal
scenarios using preference pairs from text-based responses. However, our
analysis of representation distributions reveals that multimodal DPO struggles
to align image and text representations and to distinguish between hallucinated
and non-hallucinated descriptions. To address these challenges, in this work,
we propose a Cross-modal Hierarchical Direct Preference Optimization (CHiP) to
address these limitations. We introduce a visual preference optimization module
within the DPO framework, enabling MLLMs to learn from both textual and visual
preferences simultaneously. Furthermore, we propose a hierarchical textual
preference optimization module that allows the model to capture preferences at
multiple granular levels, including response, segment, and token levels. We
evaluate CHiP through both quantitative and qualitative analyses, with results
across multiple benchmarks demonstrating its effectiveness in reducing
hallucinations. On the Object HalBench dataset, CHiP outperforms DPO in
hallucination reduction, achieving improvements of 52.7% and 55.5% relative
points based on the base model Muffin and LLaVA models, respectively. We make
all our datasets and code publicly available: https://github.com/LVUGAI/CHiP.; 73) Projective curves and weak second-order logic; Given an algebraically closed field $K$ of characteristic zero, we study the
incidence relation between points and irreducible projective curves, or more
precisely the poset of irreducible proper subvarieties of $\mathbb P^2(K)$.
Answering a question of Marcus Tressl, we prove that the poset interprets the
field, and it is in fact bi-interpretable with the two-sorted structure
consisting of the field $K$ and a sort for its finite subsets. In this
structure one can define the integers, so the theory is undecidable. When $K$
is the field of complex numbers we can nevertheless obtain a recursive
axiomatization modulo the theory of the integers. We also show that the
integers are stably embedded and that the poset of irreducible varieties over
the complex numbers is not elementarily equivalent to the one over the
algebraic numbers.; 74) Memory preservation in highly-connected quantum networks; Complex quantum networks are powerful tools in the modeling of transport
phenomena, particularly for biological systems, and enable the study of
emergent entanglement structures or topology effects in of many-body quantum
systems. Here, we study the transport properties of a quantum network described
by the paradigmatic XXZ Hamiltonian, with non-trivial graph connectivity and
topology, and long-range interaction. Adopting a combination of analytical and
numerical methods to analyze the properties of increasingly complex
architectures, we find that all-to-all connected regular network preserves over
long times the memory of initially injected excitations, tracing it back to the
system symmetries and the cooperative shielding. We then develop understanding
of the conditions for this property to survive in quantum networks with either
power-law node connectivity or complex, small-world type, architectures.
Interestingly, we find that memory preserving effects occur also in sparse and
more irregular graphs, though to a significantly lower degree. We discuss the
implications of these properties in biology-related problems, such as an
application to Weber's law in neuroscience, and their implementation in
specific quantum technologies via biomimicry.; 75) Cell Nuclei Detection and Classification in Whole Slide Images with
  Transformers; Accurate and efficient cell nuclei detection and classification in
histopathological Whole Slide Images (WSIs) are pivotal for digital pathology
applications. Traditional cell segmentation approaches, while commonly used,
are computationally expensive and require extensive post-processing, limiting
their practicality for high-throughput clinical settings. In this paper, we
propose a paradigm shift from segmentation to detection for extracting cell
information from WSIs, introducing CellNuc-DETR as a more effective solution.
We evaluate the accuracy performance of CellNuc-DETR on the PanNuke dataset and
conduct cross-dataset evaluations on CoNSeP and MoNuSeg to assess robustness
and generalization capabilities. Our results demonstrate state-of-the-art
performance in both cell nuclei detection and classification tasks.
Additionally, we assess the efficiency of CellNuc-DETR on large WSIs, showing
that it not only outperforms current methods in accuracy but also significantly
reduces inference times. Specifically, CellNuc-DETR is twice as fast as the
fastest segmentation-based method, HoVer-NeXt, while achieving substantially
higher accuracy. Moreover, it surpasses CellViT in accuracy and is
approximately ten times more efficient in inference speed on WSIs. These
results establish CellNuc-DETR as a superior approach for cell analysis in
digital pathology, combining high accuracy with computational efficiency.; 76) $\lambda$ and $\rho$ trajectories for the doubly heavy baryons in the
  diquark picture; We present the explicit form of the Regge trajectory relations for the doubly
heavy baryons $\Xi_{QQ'}$ and $\Omega_{QQ'}$ $(Q,Q'=b,c)$ in the diquark
picture. Using the derived Regge trajectory relations, we estimate the masses
of the $\lambda$-excited states and the $\rho$-excited states, which are
consistent with other theoretical predictions. Both the $\lambda$-trajectories
and $\rho$-trajectories are discussed. We show that the $\rho$-trajectories
behave differently from the $\lambda$-trajectories. Specifically, the
$\rho$-trajectories behave as $M{\sim}x_{\rho}^{2/3}$ $(x_{\rho}=n_r,l)$,
whereas the $\lambda$-trajectories follow $M{\sim}x_{\lambda}^{1/2}$
$(x_{\lambda}=N_r,L)$. By using the obtained relations, the baryon Regge
trajectory provides a straightforward and easy method for estimating the
spectra of both the $\lambda$-excited states and $\rho$-excited states.; 77) Cyber Shadows: Neutralizing Security Threats with AI and Targeted Policy
  Measures; The digital age, driven by the AI revolution, brings significant
opportunities but also conceals security threats, which we refer to as cyber
shadows. These threats pose risks at individual, organizational, and societal
levels. This paper examines the systemic impact of these cyber threats and
proposes a comprehensive cybersecurity strategy that integrates AI-driven
solutions, such as Intrusion Detection Systems (IDS), with targeted policy
interventions. By combining technological and regulatory measures, we create a
multilevel defense capable of addressing both direct threats and indirect
negative externalities. We emphasize that the synergy between AI-driven
solutions and policy interventions is essential for neutralizing cyber threats
and mitigating their negative impact on the digital economy. Finally, we
underscore the need for continuous adaptation of these strategies, especially
in response to the rapid advancement of autonomous AI-driven attacks, to ensure
the creation of secure and resilient digital ecosystems.; 78) Scientometric Analysis of the German IR Community within TREC & CLEF; Within this study, the influence of the German Information Retrieval
community on the retrieval campaigns Text Retrieval Conference (TREC) and
Conference and Labs of the Evaluation Forum (CLEF) between 2000 and 2022 was
analyzed based on metadata provided by OpenAlex and further metadata extracted
with the GROBID framework from the publication's full texts. The analysis was
conducted at the institutional and researcher levels. It was found that the
German IR community, both on the author and institution level, mainly
contributed to CLEF. Furthermore, it was shown that productivity follows the
assumptions made by Lotka's Law.; 79) Higher order div-curl type estimates for elliptic linear differential
  operators on localizable Hardy spaces; In this work, we present higher order div-curl type estimates in the sense of
Coifman, Lions, Meyer & Semmes in the local setup of elliptic linear
differential operators with smooth coefficients on localizable Hardy spaces.
Our version implies and extends results obtained for first order operators
associated to elliptic systems and complexes of vector fields. As tools, with
own interest, we develop a new smooth atomic decomposition on localizable
Hardy-Sobolev spaces and a Poincar\'e type inequality.; 80) AI Agents for Ground-Based Gamma Astronomy; Next-generation instruments for ground-based gamma-ray astronomy are marked
by a substantial increase in complexity, featuring dozens of telescopes. This
leap in scale introduces significant challenges in managing system operations
and offline data analysis. Methods, which depend on advanced personnel training
and sophisticated software, become increasingly strained as system complexity
grows, making it more challenging to effectively support users in such a
multifaceted environment. To address these challenges, we propose the
development of AI agents based on instruction-finetuned large language models
(LLMs). These agents align with specific documentation and codebases,
understand the environmental context, operate with external APIs, and
communicate with humans in natural language. Leveraging the advanced
capabilities of modern LLMs, which can process and retain vast amounts of
information, these AI agents offer a transformative approach to system
management and data analysis by automating complex tasks and providing
intelligent assistance. We present two prototypes that integrate with the
Cherenkov Telescope Array Observatory pipelines for operations and offline data
analysis. The first prototype automates data model implementation and
maintenance for the Configuration Database of the Array Control and Data
Acquisition (ACADA). The second prototype is an open-access code generation
application tailored for data analysis based on the Gammapy framework.; 81) Leveraging Traceroute Inconsistencies to Improve IP Geolocation; Traceroutes and geolocation are two essential network measurement tools that
aid applications such as network mapping, topology generation, censorship, and
Internet path analysis. However, these tools, individually and when combined,
have significant limitations that can lead to inaccurate results. Prior
research addressed specific issues with traceroutes and geolocation
individually, often requiring additional measurements. In this paper, we
introduce GeoTrace, a lightweight tool designed to identify, classify, and
resolve geolocation anomalies in traceroutes using existing data. GeoTrace
leverages the abundant information in traceroutes and geolocation databases to
identify anomalous IP addresses with incorrect geolocation. It systematically
classifies these anomalies based on underlying causes - such as MPLS effects or
interface discrepancies - and refines their geolocation estimates where
possible. By correcting these inaccuracies, GeoTrace enhances the reliability
of traceroute-based analyses without the need for additional probing. Our work
offers a streamlined solution that enhances the accuracy of geolocation in
traceroute analysis, paving the way for more reliable measurement studies.; 82) 1-shifted Lie bialgebras and their quantizations; In this paper, we define (cohomologically) 1-shifted Manin triples and
1-shifted Lie bialgebras, and study their properties. We derive many results
that are parallel to those found in ordinary Lie bialgebras, including the
double construction and the existence of a 1-shifted $r$-matrix satisfying the
classical Yang-Baxter equation.
  Turning to quantization, we first construct a canonical quantization for each
1-shifted metric Lie algebra $\mathfrak{g}$, producing a deformation to the
symmetric monoidal category of $\mathfrak{g}$ modules over a formal variable
$\hbar$. This quantization is in terms of a curved differential graded algebra.
Under a further technical assumption, we construct quantizations of transverse
Lagrangian subalgebras of $\mathfrak{g}$, which is a pair of DG algebras
connected by Koszul duality, and give rise to monoidal module categories of the
quantized double.
  Finally, we apply this to Manin triples arising from Lie algebras of loop
groups, and construct 1-shifted meromorphic $r$-matrices. The resulting
quantizations are the cohomologically-shifted analogue of Yangians.; 83) Preventing Household Bankruptcy: The One-Third Rule in Financial
  Planning with Mathematical Validation and Game-Theoretic Insights; This paper analyzes the 1/3 Financial Rule, a method of allocating income
equally among debt repayment, savings, and living expenses. Through
mathematical modeling, game theory, behavioral finance, and technological
analysis, we examine the rule's potential for supporting household financial
stability and reducing bankruptcy risk. The research develops theoretical
foundations using utility maximization theory, demonstrating how equal
allocation emerges as a solution under standard economic assumptions. The
game-theoretic analysis explores the rule's effectiveness across different
household structures, revealing potential strategic advantages in financial
decision-making. We investigate psychological factors influencing financial
choices, including cognitive biases and neurobiological mechanisms that impact
economic behavior. Technological approaches, such as AI-driven personalization,
blockchain tracking, and smart contract applications, are examined for their
potential to support financial planning. Empirical validation using U.S. Census
data and longitudinal studies assesses the rule's performance across various
household types. Stress testing under different economic conditions provides
insights into its adaptability and resilience. The research integrates
mathematical analysis with behavioral insights and technological perspectives
to develop a comprehensive approach to household financial management.; 84) PyEMILI: A New Generation Computer-aided Spectral Line Identifier; Deep high-dispersion spectroscopy of Galactic photoionized gaseous nebulae,
mainly planetary nebulae and HII regions, has revealed numerous emission lines.
As a key step of spectral analysis, identification of emission lines hitherto
has mostly been done manually, which is a tedious task, given that each line
needs to be carefully checked against huge volumes of atomic
transition/spectroscopic database to reach a reliable assignment of identity.
Using Python, we have developed a line-identification code PyEMILI, which is a
significant improvement over the Fortran-based package EMILI introduced ~20
years ago. In our new code PyEMILI, the major shortcomings in EMILI's
line-identification technique have been amended. Moreover, the atomic
transition database utilized by PyEMILI was adopted from Atomic Line List
v3.00b4 but greatly supplemented with theoretical transition data from the
literature. The effective recombination coefficients of the CII, OII, NII and
NeII nebular lines are collected from the literature to form a subset of the
atomic transition database to aid identification of faint optical recombination
lines in the spectra of PNe and HII regions. PyEMILI is tested using the deep,
high-dispersion spectra of two Galactic PNe, Hf2-2 and IC418, and gives better
results of line identification than EMILI does. We also ran PyEMILI on the
optical spectrum of a late-type [WC11] star UVQS J060819.93-715737.4 recently
discovered in the Large Magellanic Cloud, and our results agree well with the
previous manual identifications. The new identifier PyEMILI is applicable to
not only emission-line nebulae but also emission stars, such as Wolf-Rayet
stars.; 85) Inferring collective synchrony observing spiking of one or several
  neurons; We tackle a quantification of synchrony in a large ensemble of interacting
neurons from the observation of spiking events. In a simulation study, we
efficiently infer the synchrony level in a neuronal population from a point
process reflecting spiking of a small number of units and even from a single
neuron. We introduce a synchrony measure (order parameter) based on the
Bartlett covariance density; this quantity can be easily computed from the
recorded point process. This measure is robust concerning missed spikes and, if
computed from observing several neurons, does not require spike sorting. We
illustrate the approach by modeling populations of spiking or bursting neurons,
including the case of sparse synchrony.; 86) Unlocking the Black Box: Analysing the EU Artificial Intelligence Act's
  Framework for Explainability in AI; The lack of explainability of Artificial Intelligence (AI) is one of the
first obstacles that the industry and regulators must overcome to mitigate the
risks associated with the technology. The need for eXplainable AI (XAI) is
evident in fields where accountability, ethics and fairness are critical, such
as healthcare, credit scoring, policing and the criminal justice system. At the
EU level, the notion of explainability is one of the fundamental principles
that underpin the AI Act, though the exact XAI techniques and requirements are
still to be determined and tested in practice. This paper explores various
approaches and techniques that promise to advance XAI, as well as the
challenges of implementing the principle of explainability in AI governance and
policies. Finally, the paper examines the integration of XAI into EU law,
emphasising the issues of standard setting, oversight, and enforcement.; 87) Deep-Bench: Deep Learning Benchmark Dataset for Code Generation; Deep learning (DL) has revolutionized areas such as computer vision, natural
language processing, and more. However, developing DL systems is challenging
due to the complexity of DL workflows. Large Language Models (LLMs), such as
GPT, Claude, Llama, Mistral, etc., have emerged as promising tools to assist in
DL code generation, offering potential solutions to these challenges. Despite
this, existing benchmarks such as DS-1000 are limited, as they primarily focus
on small DL code snippets related to pre/post-processing tasks and lack a
comprehensive coverage of the full DL pipeline, including different DL phases
and input data types.
  To address this, we introduce DeepBench, a novel benchmark dataset designed
for function-level DL code generation. DeepBench categorizes DL problems based
on three key aspects: phases such as pre-processing, model construction, and
training; tasks, including classification, regression, and recommendation; and
input data types such as tabular, image, and text.
  GPT-4o -- the state-of-the-art LLM -- achieved 31% accuracy on DeepBench,
significantly lower than its 60% on DS-1000. We observed similar difficulty for
other LLMs (e.g., 28% vs. 54% for Claude, 21% vs. 41% for LLaMA, and 15% vs.
20% for Mistral). This result underscores DeepBench's greater complexity. We
also construct a taxonomy of issues and bugs found in LLM-generated DL code,
which highlights the distinct challenges that LLMs face when generating DL code
compared to general code.
  Furthermore, our analysis also reveals substantial performance variations
across categories, with differences of up to 7% among phases and 37% among
tasks. These disparities suggest that DeepBench offers valuable insights into
the LLMs' performance and areas for potential improvement in the DL domain.; 88) On robust recovery of signals from indirect observations; We consider an uncertain linear inverse problem as follows. Given observation
$\omega=Ax_*+\zeta$ where $A\in {\bf R}^{m\times p}$ and $\zeta\in {\bf R}^{m}$
is observation noise, we want to recover unknown signal $x_*$, known to belong
to a convex set ${\cal X}\subset{\bf R}^{n}$. As opposed to the ""standard""
setting of such problem, we suppose that the model noise $\zeta$ is ""corrupted""
-- contains an uncertain (deterministic dense or singular) component.
Specifically, we assume that $\zeta$ decomposes into $\zeta=N\nu_*+\xi$ where
$\xi$ is the random noise and $N\nu_*$ is the ""adversarial contamination"" with
known $\cal N\subset {\bf R}^n$ such that $\nu_*\in \cal N$ and $N\in {\bf
R}^{m\times n}$. We consider two ""uncertainty setups"" in which $\cal N$ is
either a convex bounded set or is the set of sparse vectors (with at most $s$
nonvanishing entries). We analyse the performance of ""uncertainty-immunized""
polyhedral estimates -- a particular class of nonlinear estimates as introduced
in [15, 16] -- and show how ""presumably good"" estimates of the sort may be
constructed in the situation where the signal set is an ellitope (essentially,
a symmetric convex set delimited by quadratic surfaces) by means of efficient
convex optimization routines.; 89) On the Distribution of the Two-Sample Cramer-von Mises Criterion; The Cramer-von Mises $\omega^2$ criterion for testing that a sample, $x_1, \cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \begin{equation*}\tag{1}\omega^2 = \int^\infty_{-\infty} \lbrack F_N(x) - F(x)\rbrack^2 dF(x),\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\omega^2$, namely \begin{equation*}\tag{2} T NM/(N + M)\rbrack G_M(x)\rbrack^2 dH_{N+M}(x),\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\rbrack$. limiting $N\omega^2$ as $N \rightarrow \infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \infty, M \infty$, $N/M \lambda$, $\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4].; 90) Quantum contextuality of spin-1 massive particles; Contextuality is a fundamental property of quantum mechanics. Contrary to
entanglement, which can only exist in composite systems, contextuality is also
present for single entities. The case of a three-level system is of particular
interest because--in agreement with the Bell-Kochen-Specker theorem--it is the
simplest in which quantum contextuality is necessarily present. We verify that
the polarizations of spin-1 massive particles produced at collider experiments
indeed exhibit contextuality. To this purpose we consider $W$ gauge bosons
produced in top-quark decays, $J/\psi$ and $K^{*}(892)^0$ mesons created in
$B$-meson decays and $\phi$ mesons resulting from $\chi^0_c$ charmonium decays,
making use of the data collected and analyzed by the ATLAS, LHCb and BESIII
collaborations, respectively. The polarizations of all these four particles
show contextuality with a significance of more than $5\sigma$.; 91) Prime numbers and dynamics of the polynomial $x^2-1$; Let $n \in \mathbb{Z}_{\geqslant 2}$. By $P(n)$ we denote the set of all
prime divisors of the integers in the sequence $n, n^2-1, (n^2-1)^2-1, \dots$.
We ask whether the set $P(n)$ determines $n$ uniquely under the assumption that
$n \neq m^2-1$ for $m \in \mathbb{Z}_{\geqslant 2}$. This problem originates in
the structure theory of infinite-dimensional Lie algebras. We show that the
sets $P(n)$ generate infinitely many equivalence classes of positive integers
under the equivalence relation $n_1 \sim n_2 \iff P(n_1) = P(n_2)$. We also
prove that the sets $P(n)$ separate all positive integers up to $2^{29}$, and
we provide some heuristics on why the answer to our question should be
positive.; 92) Atomistic insights into solid solution strengthening: size misfit versus
  stiffness misfit; Used for centuries to enhance mechanical properties of materials, solid
solution strengthening (SSS) is a classical metallurgical method in which small
amounts of impurity elements are added to a base metal. Developed for dilute
alloys, classical theories of SSS are presently challenged by the ongoing
explosive development of complex concentrated alloys (CCA) in which all
component elements are present in nearly equal fractions. Here we develop a
method of computational alchemy in which interatomic interactions are modified
to continuously and systematically vary two key parameters defining SSS, atomic
size misfit and elastic stiffness misfit, over a maximally wide range of misfit
values. The resulting model alloys are subjected to massive Molecular Dynamics
simulations reproducing full complexity of plastic strength response in
concentrated single-phase body-centered cubic solid solutions. At variance with
views prevailing in the literature, our computational experiments show that
stiffness misfit can contribute to SSS on par if not more than size misfit.
Furthermore, depending on exactly how they are combined, the two misfits can
result in synergistic or antagonistic effect on alloy strengthening. In
contrast to real CCAs in which every constituent element comes with its
specific combination of atomic size and elastic stiffness, our alchemical model
alloys sample the space of misfit parameters continuously thus augmenting the
much more constrained and inevitably spotty experimental exploration of the CCA
design space. Taking advantage of unique to our approach ability to define
alloy misfit parameters, our computational study demonstrates how useful
insights can be gained from intentionally unrealistic alchemical models. Rather
than practical recommendation for alloy design, our computational experiments
should be regarded as a proving ground for further SSS theory development.; 93) Probing the physical environment of the most high-redshift H$_2$-DLAs
  through numerical models; Damped Lyman-$\alpha$ absorbers (DLAs) with molecular hydrogen have been
probed in detail through both spectroscopic observations and numerical
modelling. However, such H$_2$ absorbers are quite sparse at very high
redshifts. We identify six of the most distant known H$_2$-DLAs (redshift
between 3 and 4.5), with medium/high-resolution spectroscopic observations
reported in the literature, and perform detailed numerical modelling followed
by Bayesian analysis to constrain their physical properties mainly using the
H$_2$ rotational level population and CI fine structure levels. Our modelling
approach involves setting up a constant-pressure multiphase cloud irradiated
from both sides, in comparison to most models which employ constant density.
This enables us to use all observed atomic and molecular species as constraints
to build a more realistic model of the DLA. Our results indicate high
interstellar radiation field strength $\sim$ 10$^2$ to 10$^3$ G$_0$ for some
sightlines, which is suggestive of in situ star formation. The cosmic ray
ionization rate for all DLAs is constrained between 10$^{-17}$ and 10$^{-14}$
s$^{-1}$, consistent with recent estimates for high-redshift sightlines. Total
hydrogen density and temperature lie in the ranges 50 to 4 $\times$ 10$^4$
cm$^{-3}$ and 35-200 K in the innermost part of the absorbers. The
corresponding gas pressure in our DLA models lies between 10$^{3.5}$ and
10$^{6.4}$ cm$^{-3}$ K, with three sightlines having a higher pressure than the
range typical of high-redshift H$_2$-DLAs.; 94) Local pump depletion based angular momentum gain of plasma electrons; We present a novel mechanism in which plasma electrons optically acquire
angular momentum through local pump depletion. This process is facilitated by
an azimuthally polarized laser that lacks angular momentum, yet enables
electron rotation through canonical momentum conservation. Through theoretical
considerations and one- and three-dimensional particle-in-cell simulations, we
demonstrate that the electrons acquire substantial angular momentum when the
ultra-intense ($I \gtrsim 10^{19}$ W/cm$^2$) and ultra-short (tens of
femtoseconds) laser pulse excites a nonlinear wakefield in the bubble regime.
During this process, the frequency at the laser's leading edge shifts down due
to phase modulation. Following the conservation of the wave action, the laser's
vector potential develops a long-wavelength offset that trails the main pulse
and remains confined within the plasma bubble. This offset facilitates electron
rotation of the bubble's sheath currents and the high-energy ($>100$ MeV)
self-injected electrons. As the laser pulse front erodes, the long-wavelength
offset of the vector potential increases and oscillates, influencing both the
magnitude and polarity of the electrons' angular momentum. By varying key laser
parameters such as phase, frequency, and polarization, we show how to
manipulate and control the transverse momentum space of high-energy electrons.; 95) DEPT: Deep Extreme Point Tracing for Ultrasound Image Segmentation; Automatic medical image segmentation plays a crucial role in computer aided
diagnosis. However, fully supervised learning approaches often require
extensive and labor-intensive annotation efforts. To address this challenge,
weakly supervised learning methods, particularly those using extreme points as
supervisory signals, have the potential to offer an effective solution. In this
paper, we introduce Deep Extreme Point Tracing (DEPT) integrated with
Feature-Guided Extreme Point Masking (FGEPM) algorithm for ultrasound image
segmentation. Notably, our method generates pseudo labels by identifying the
lowest-cost path that connects all extreme points on the feature map-based cost
matrix. Additionally, an iterative training strategy is proposed to refine
pseudo labels progressively, enabling continuous network improvement.
Experimental results on two public datasets demonstrate the effectiveness of
our proposed method. The performance of our method approaches that of the fully
supervised method and outperforms several existing weakly supervised methods.; 96) LLM-TabFlow: Synthetic Tabular Data Generation with Inter-column Logical
  Relationship Preservation; Synthetic tabular data have widespread applications in industrial domains
such as healthcare, finance, and supply chains, owing to their potential to
protect privacy and mitigate data scarcity. However, generating realistic
synthetic tabular data while preserving inter-column logical relationships
remains a significant challenge for the existing generative models. To address
these challenges, we propose LLM-TabFlow, a novel approach that leverages Large
Language Model (LLM) reasoning to capture complex inter-column relationships
and compress tabular data, while using Score-based Diffusion to model the
distribution of the compressed data in latent space. Additionally, we introduce
an evaluation framework, which is absent in literature, to fairly assess the
performance of synthetic tabular data generation methods in real-world
contexts. Using this framework, we conduct extensive experiments on two
real-world industrial datasets, evaluating LLM-TabFlow against other five
baseline methods, including SMOTE (an interpolation-based approach) and other
state-of-the-art generative models. Our results show that LLM-TabFlow
outperforms all baselines, fully preserving inter-column relationships while
achieving the best balance between data fidelity, utility, and privacy. This
study is the first to explicitly address inter-column relationship preservation
in synthetic tabular data generation, offering new insights for developing more
realistic and reliable tabular data generation methods.; 97) A Sample of Low-mass AGNs Extended to z $\approx$ 0.6; We present a catalog of 927 low-mass active galactic nuclei (AGNs) with black
hole mass of $M_{BH}\leqslant2\times10^{6} M_{\odot}$ characterized by broad
H$\alpha$ or H$\beta$ emission lines,uniformly selected from the Seventeenth
Data Release (DR17) of the Sloan Digital Sky Survey (SDSS) spectra. Taking
advantages of the wide wavelength coverage of BOSS/eBOSS spectra of the SDSS,
this sample significantly extends the redshift range to $z\leqslant0.57$, a
marked improvement over the previous studies which were generally limited to
$z\leqslant0.35$. This sample encompasses black hole masses from $10^{3.7}$ to
$10^{6.3} M_{\odot}$, with Eddington ratios ranging from 0.01 to 3.3.
Preliminary analysis of this sample reveals a marked decline in maximum
accretion rates (namely $L/L_{Edd}$) and broad-H$\alpha$ luminosities with
decreasing redshift, analogous to the passive ``downsizing'' evolutionary trend
observed in high-mass AGNs. This systematic catalog of low-redshift low-mass
AGNs, the largest so far, will benefit the studies on accretion physics,
AGN--galaxy connection and the origin of supermassive black holes.; 98) Fast Inexact Bilevel Optimization for Analytical Deep Image Priors; The analytical deep image prior (ADP) introduced by Dittmer et al. (2020)
establishes a link between deep image priors and classical regularization
theory via bilevel optimization. While this is an elegant construction, it
involves expensive computations if the lower-level problem is to be solved
accurately. To overcome this issue, we propose to use adaptive inexact bilevel
optimization to solve ADP problems. We discuss an extension of a recent inexact
bilevel method called the method of adaptive inexact descent of Salehi et
al.(2024) to an infinite-dimensional setting required by the ADP framework. In
our numerical experiments we demonstrate that the computational speed-up
achieved by adaptive inexact bilevel optimization allows one to use ADP on
larger-scale problems than in the previous literature, e.g. in deblurring of 2D
color images.; 99) Cosmological Particle Production without Quantum Fields; In a cosmological setting, particle production is ubiquitous. It may occur as
a consequence of the expansion of the background or because a field couples to
other degrees of freedom that evolve with time. The process is well understood
in the context of quantum field theory, and calculable as long as the produced
quanta are weakly interacting. For extended objects like strings and membranes
a second quantized formulation is much less developed than for particles. In
this light, we revisit particle production from a first quantized perspective.
We show how to obtain occupation numbers, both from the vacuum persistence
amplitude and from the Green's function. We also derive the much less studied
but phenomenologically interesting two-particle wavefunction of the produced
quanta.; 100) Soft Barycentric Refinement; The soft Barycentric refinement preserves manifolds with or without boundary.
In every dimension larger than one, there is a universal spectral central
limiting measure that has affinities with the Barycentric limiting measure one
dimension lower. Ricci type quantities like the length of the dual sphere of
co-dimension-2 simplex stay invariant under soft refinements. We prove that the
dual graphs of any manifold can be colored with 3 colors, which is in the
2-dimensional case a special case of the Groetzsch theorem. It follows that the
vertices of a soft Barycentric refined q-manifold G' can be colored by q+1 or
q+2 colors.",0.0,0.0
2412.00036,applied,2412.00036-pos2-2,"On the Distribution of the Two-Sample Cramer-von Mises Criterion; The Cramer-von Mises $\omega^2$ criterion for testing that a sample, $x_1, \cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \begin{equation*}\tag{1}\omega^2 = \int^\infty_{-\infty} \lbrack F_N(x) - F(x)\rbrack^2 dF(x),\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\omega^2$, namely \begin{equation*}\tag{2} T NM/(N + M)\rbrack G_M(x)\rbrack^2 dH_{N+M}(x),\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\rbrack$. limiting $N\omega^2$ as $N \rightarrow \infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \infty, M \infty$, $N/M \lambda$, $\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4].",2412.00036-pos1-2,"Quant GANs: deep generation of financial time series; Modeling financial time series by stochastic processes is a challenging task
and a central area of research in financial mathematics. As an alternative, we
introduce Quant GANs, a data-driven model which is inspired by the recent
success of generative adversarial networks (GANs). Quant GANs consist of a
generator and discriminator function, which utilize temporal convolutional
networks (TCNs) and thereby achieve to capture long-range dependencies such as
the presence of volatility clusters. The generator function is explicitly
constructed such that the induced stochastic process allows a transition to its
risk-neutral distribution. Our numerical results highlight that distributional
properties for small and large lags are in an excellent agreement and
dependence properties such as volatility clusters, leverage effects, and serial
autocorrelations can be generated by the generator function of Quant GANs,
demonstrably in high fidelity.",28,"['1', '2', '4', '3', '5', '6', '8', '10', '11', '12']","The integration of the Cramer-von Mises criterion in statistical analysis, particularly in determining model performance and error rates, pairs well with the insights provided in the paper on Large Language Models (LLMs) and their behavior in generating outputs. The exploration of hallucinations in LLMs connects the statistical verification methods from the main paper to the ongoing issues of uncertainty in AI outputs, making the first candidate the most aligned for a multidisciplinary approach that combines statistics and AI safety. This synthesis is novel as it merges conventional statistical methods with modern AI challenges, making it a useful framework for advancing understanding in both fields.","1) Trust Me, I'm Wrong: High-Certainty Hallucinations in LLMs; Large Language Models (LLMs) often generate outputs that lack grounding in
real-world facts, a phenomenon known as hallucinations. Prior research has
associated hallucinations with model uncertainty, leveraging this relationship
for hallucination detection and mitigation. In this paper, we challenge the
underlying assumption that all hallucinations are associated with uncertainty.
Using knowledge detection and uncertainty measurement methods, we demonstrate
that models can hallucinate with high certainty even when they have the correct
knowledge. We further show that high-certainty hallucinations are consistent
across models and datasets, distinctive enough to be singled out, and challenge
existing mitigation methods. Our findings reveal an overlooked aspect of
hallucinations, emphasizing the need to understand their origins and improve
mitigation strategies to enhance LLM safety. The code is available at
https://github.com/technion-cs-nlp/Trust_me_Im_wrong .; 2) Steady state and mixing of two run-and-tumble particles interacting
  through jamming and attractive forces; We study the long-time behavior of two run-and-tumble particles on the real
line subjected to an attractive interaction potential and jamming interactions,
which prevent the particles from crossing. We provide the explicit invariant
measure, a useful tool for studying clustering phenomena in out-ofequilibrium
statistical mechanics, for different tumbling mechanisms and potentials. An
important difference with invariant measures of equilibrium systems are Dirac
masses on the boundary of the state space, due to the jamming interactions.
Qualitative changes in the invariant measure depending on model parameters are
also observed, suggesting, like a growing body of evidence, that run-andtumble
particle systems can be classified into close-to-equilibrium and strongly
out-of-equilibrium models. We also study the relaxation properties of the
system, which are linked to the timescale at which clustering emerges from an
arbitrary initial configuration. When the interaction potential is linear, we
show that the total variation distance to the invariant measure decays
exponentially and provide sharp bounds on the decay rate. When the interaction
potential is harmonic, we give quantitative exponential bounds in a
Wasserstein-type distance.; 3) A Van der Waals Moir\'e Bilayer Photonic Crystal Cavity; Enhancing light-matter interactions with photonic structures is critical in
classical and quantum nanophotonics. Recently, Moir\'e twisted bilayer optical
materials have been proposed as a promising means towards a tunable and
controllable platform for nanophotonic devices, with proof of principle
realisations in the near infrared spectral range. However, the realisation of
Moir\'e photonic crystal (PhC) cavities has been challenging, due to a lack of
advanced nanofabrication techniques and availability of standalone transparent
membranes. Here, we leverage the properties of the van der Waals material
hexagonal Boron Nitride to realize Moir\'e bilayer PhC cavities. We design and
fabricate a range of devices with controllable twist angles, with flatband
modes in the visible spectral range (~ 450 nm). Optical characterization
confirms the presence of spatially periodic cavity modes originating from the
engineered dispersion relation (flatband). Our findings present a major step
towards harnessing a two-dimensional van der Waals material for the
next-generation of on chip, twisted nanophotonic systems.; 4) Unsupervised Joint Learning of Optical Flow and Intensity with Event
  Cameras; Event cameras rely on motion to obtain information about scene appearance. In
other words, for event cameras, motion and appearance are seen both or neither,
which are encoded in the output event stream. Previous works consider
recovering these two visual quantities as separate tasks, which does not fit
with the nature of event cameras and neglects the inherent relations between
both tasks. In this paper, we propose an unsupervised learning framework that
jointly estimates optical flow (motion) and image intensity (appearance), with
a single network. Starting from the event generation model, we newly derive the
event-based photometric error as a function of optical flow and image
intensity, which is further combined with the contrast maximization framework,
yielding a comprehensive loss function that provides proper constraints for
both flow and intensity estimation. Exhaustive experiments show that our model
achieves state-of-the-art performance for both optical flow (achieves 20% and
25% improvement in EPE and AE respectively in the unsupervised learning
category) and intensity estimation (produces competitive results with other
baselines, particularly in high dynamic range scenarios). Last but not least,
our model achieves shorter inference time than all the other optical flow
models and many of the image reconstruction models, while they output only one
quantity. Project page: https://github.com/tub-rip/e2fai; 5) Age of Information in Multi-Relay Networks with Maximum Age Scheduling; We propose and evaluate age of information (AoI)-aware multiple access
mechanisms for the Internet of Things (IoT) in multi-relay two-hop networks.
The network considered comprises end devices (EDs) communicating with a set of
relays in ALOHA fashion, with new information packets to be potentially
transmitted every time slot. The relays, in turn, forward the collected packets
to an access point (AP), the final destination of the information generated by
the EDs. More specifically, in this work we investigate the performance of four
age-aware algorithms that prioritize older packets to be transmitted, namely
max-age matching (MAM), iterative max-age scheduling (IMAS), age-based delayed
request (ABDR), and buffered ABDR (B-ABDR). The former two algorithms are
adapted into the multi-relay setup from previous research, and achieve
satisfactory average AoI and average peak AoI performance, at the expense of a
significant amount of information exchange between the relays and the AP. The
latter two algorithms are newly proposed to let relays decide which one(s) will
transmit in a given time slot, requiring less signaling than the former
algorithms. We provide an analytical formulation for the AoI lower bound
performance, compare the performance of all algorithms in this set-up, and show
that they approach the lower bound. The latter holds especially true for
B-ABDR, which approaches the lower bound the most closely, tilting the scale in
its favor, as it also requires far less signaling than MAM and IMAS.; 6) Efficient Transition State Searches by Freezing String Method with Graph
  Neural Network Potentials; Transition states are a critical bottleneck in chemical transformations.
Significant efforts have been made to develop algorithms that efficiently
locate transition states on potential energy surfaces. However, the
computational cost of ab-initio potential energy surface evaluation limits the
size of chemical systems that can routinely studied. In this work, we develop
and fine-tune a graph neural network potential energy function suitable for
describing organic chemical reactions and use it to rapidly identify transition
state guess structures. We successfully refine guess structures and locate a
transition state in each test system considered and reduce the average number
of ab-initio calculations by 47% though use of the graph neural network
potential energy function. Our results show that modern machine learning models
have reached levels of reliability whereby they can be used to accelerate
routine computational chemistry tasks.; 7) Orientations of graphs omitting non-edge-critical directed graphs; In 1974, Erd\H{o}s asked the following question: given a graph $G$ and a
directed graph $\vec{H}$, how many ways are there to orient the edges of $G$
such that it does not contain $\vec{H}$ as a subgraph. We denote this value by
$D(G, \vec{H})$. Further, we let $D(n, \vec{H})$ denote the maximum of $D(G,
\vec{H})$ over all graphs $G$ on $n$ vertices. In 2006, Alon and Yuster gave an
exact answer when $\vec{H}$ is a tournament. In 2023, Buci\'c, Janzer, and
Sudakov gave asymptotic answers for all directed graphs $\vec{H}$, and in the
same paper, they gave an exact answer when $\vec{H}$ is a directed cycle. In
this paper, we give a better bound for some specific non-bipartite directed
graphs. Further, we obtain exact values of $D(G, \vec{H})$ for some small
non-edge-critical directed graphs $\vec{H}$. Finally, for these graphs, we
classify all graphs $G$ that attain the bound $D(G, \vec{H}) = D(n, \vec{H})$.; 8) Can We Find the Code? An Empirical Study of Google Scholar's Code
  Retrieval; Academic codes associated with research papers are valuable resources for
scholars. In specialized fields outside computer science, code availability is
often limited, making effective code retrieval essential. Google Scholar is a
crucial academic search tool. If a code published in the paper is not
retrievable via Google Scholar, its accessibility and impact are significantly
reduced. This study takes the term ""accelerated degradation"" combined with
""reliability"" as an example, and finds that, for papers published by Elsevier,
only GitHub links included in abstracts are comprehensively retrieved by Google
Scholar. When such links appear within the main body of a paper, even in the
""Data Availability"" section, they may be ignored and become unsearchable. These
findings highlight the importance of strategically placing GitHub links in
abstracts to enhance code discoverability on Google Scholar.; 9) Solutions of systems of certain Fermat-type PDDEs; The objective of this paper is to investigate the existence and the forms of
the pair of finite order entire and meromorphic solutions of some certain
systems of Fermat-type partial differential-difference equations of several
complex variables. These results represent some refinements and generalizations
of the earlier findings, especially the results due to Xu {\it et al.} (J.
Math. Anal. Appl. 483(2) (2020)). We provide some examples to support the
results.; 10) Finite-time blowup in a fully parabolic chemotaxis model involving
  indirect signal production; This paper is concerned with a parabolic-parabolic-parabolic chemotaxis
system with indirect signal production, modelling the impact of phenotypic
heterogeneity on population aggregation \begin{equation*}
  \begin{cases} u_t = \Delta u - \nabla\cdot(u\nabla v),\\ v_t = \Delta v - v +
w,\\ w_t = \Delta w - w + u,
  \end{cases} \end{equation*} posed on a ball in $\mathbb R^n$ with $n\geq5$,
subject to homogeneous Neumann boundary conditions. The system has a
four-dimensional critical mass phenomenon concerning blowup in finite or
infinite time according to the seminal works of Fujie and Senba [J.
Differential Equations, 263 (2017), 88--148; 266 (2019), 942--976]. We prove
that for any prescribed mass $m > 0$, there exist radially symmetric and
nonnegative initial data $(u_0,v_0,w_0)\in C^0(\overline{\Omega})\times
C^2(\overline{\Omega})\times C^2(\overline{\Omega})$ with $\int_\Omega u_0 = m$
such that the corresponding classical solutions blow up in finite time. The key
ingredient is a novel integral inequality for the cross-term integral
$\int_\Omega uv$ constructed via a Lyapunov functional.; 11) Test Amplification for REST APIs Using ""Out-of-the-box"" Large Language
  Models; REST APIs are an indispensable building block in today's cloud-native
applications, so testing them is critically important. However, writing
automated tests for such REST APIs is challenging because one needs strong and
readable tests that exercise the boundary values of the protocol embedded in
the REST API. In this paper, we report our experience with using ""out of the
box"" large language models (ChatGPT and GitHub's Copilot) to amplify REST API
test suites. We compare the resulting tests based on coverage and
understandability, and we derive a series of guidelines and lessons learned
concerning the prompts that result in the strongest test suite.; 12) On Subdifferentials Via a Generalized Conjugation Scheme: An Application
  to DC Problems and Optimality Conditions; This paper studies properties of a subdifferential defined using a
generalized conjugation scheme. We relate this subdifferential together with
the domain of an appropriate conjugate function and the {\epsilon}-directional
derivative. In addition, we also present necessary conditions for
{\epsilon}-optimality and global optimality in optimization problems involving
the difference of two convex functions. These conditions will be written via
this generalized notion of subdifferential studied in the first sections of the
paper.; 13) Continual Human-in-the-Loop Optimization; Optimal input settings vary across users due to differences in motor
abilities and personal preferences, which are typically addressed by manual
tuning or calibration. Although human-in-the-loop optimization has the
potential to identify optimal settings during use, it is rarely applied due to
its long optimization process. A more efficient approach would continually
leverage data from previous users to accelerate optimization, exploiting shared
traits while adapting to individual characteristics. We introduce the concept
of Continual Human-in-the-Loop Optimization and a Bayesian optimization-based
method that leverages a Bayesian-neural-network surrogate model to capture
population-level characteristics while adapting to new users. We propose a
generative replay strategy to mitigate catastrophic forgetting. We demonstrate
our method by optimizing virtual reality keyboard parameters for text entry
using direct touch, showing reduced adaptation times with a growing user base.
Our method opens the door for next-generation personalized input systems that
improve with accumulated experience.; 14) Single-step high-fidelity three-qubit gates by anisotropic chiral
  interactions; Direct multi-qubit gates are becoming critical to facilitate quantum
computations in near-term devices by reducing the gate counts and circuit
depth. Here, we demonstrate that fast and high fidelity three-qubit gates can
be realized in a single step by leveraging small anisotropic and chiral
three-qubit interactions. These ingredients naturally arise in state-of-the-art
spin-based quantum hardware through a combination of spin-orbit interactions
and orbital magnetic fields. These interactions resolve the key synchronization
issues inherent in protocols relying solely on two-qubit couplings, which
significantly limit gate fidelity. We confirm with numerical simulations that
our single-step three-qubit gate can outperform existing protocols, potentially
achieving infidelity $\leq 10^{-4}$ in 80-100 ns under current experimental
conditions. To further benchmark its performance, we also propose an
alternative composite three-qubit gate sequence based on anisotropic two-qubit
interactions with built-in echo sequence and show that the single-step protocol
can outperform it, making it highly suitable for near-term quantum processors.; 15) Deep Reinforcement Learning for Job Scheduling and Resource Management
  in Cloud Computing: An Algorithm-Level Review; Cloud computing has revolutionized the provisioning of computing resources,
offering scalable, flexible, and on-demand services to meet the diverse
requirements of modern applications. At the heart of efficient cloud operations
are job scheduling and resource management, which are critical for optimizing
system performance and ensuring timely and cost-effective service delivery.
However, the dynamic and heterogeneous nature of cloud environments presents
significant challenges for these tasks, as workloads and resource availability
can fluctuate unpredictably. Traditional approaches, including heuristic and
meta-heuristic algorithms, often struggle to adapt to these real-time changes
due to their reliance on static models or predefined rules. Deep Reinforcement
Learning (DRL) has emerged as a promising solution to these challenges by
enabling systems to learn and adapt policies based on continuous observations
of the environment, facilitating intelligent and responsive decision-making.
This survey provides a comprehensive review of DRL-based algorithms for job
scheduling and resource management in cloud computing, analyzing their
methodologies, performance metrics, and practical applications. We also
highlight emerging trends and future research directions, offering valuable
insights into leveraging DRL to advance both job scheduling and resource
management in cloud computing.; 16) Revealing correlated noise with single-qubit operations; Spatially correlated noise poses a significant challenge to fault-tolerant
quantum computation by breaking the assumption of independent errors. Existing
methods such as cycle benchmarking and quantum process tomography can
characterize noise correlations but require substantial resources. We propose
straightforward and efficient techniques to detect and quantify these
correlations by leveraging collective phenomena arising from environmental
correlations in a qubit register. In these techniques, single-qubit state
preparations, single-qubit gates, and single-qubit measurements, combined with
classical post-processing, suffice to uncover correlated relaxation and
dephasing. Specifically, we use that correlated relaxation is connected to the
superradiance effect which we show to be accessible by single-qubit
measurements. Analogously, the established parity oscillation protocol can be
refined to reveal correlated dephasing through characteristic changes in the
oscillation line shape, without requiring the preparation of complex and
entangled states.; 17) The Effect of Optimal Self-Distillation in Noisy Gaussian Mixture Model; Self-distillation (SD), a technique where a model refines itself from its own
predictions, has garnered attention as a simple yet powerful approach in
machine learning. Despite its widespread use, the mechanisms underlying its
effectiveness remain unclear. In this study, we investigate the efficacy of
hyperparameter-tuned multi-stage SD in binary classification tasks with noisy
labeled Gaussian mixture data, utilizing a replica theory. Our findings reveals
that the primary driver of SD's performance improvement is denoising through
hard pseudo-labels, with the most notable gains observed in moderately sized
datasets. We also demonstrate the efficacy of practical heuristics, such as
early stopping for extracting meaningful signal and bias fixation for
imbalanced data. These results provide both theoretical guarantees and
practical insights, advancing our understanding and application of SD in noisy
settings.; 18) Competitive algorithms for calculating the ground state properties of
  Bose-Fermi mixtures; In this work we define, analyze, and compare different numerical schemes that
can be used to study the ground state properties of Bose-Fermi systems, such as
mixtures of different atomic species under external forces or self-bound
quantum droplets. The bosonic atoms are assumed to be condensed and are
described by the generalized Gross-Pitaevskii equation. The fermionic atoms, on
the other hand, are treated individually, and each atom is associated with a
wave function whose evolution follows the Hartree-Fock equation. We solve such
a formulated set of equations using a variety of methods, including those based
on adiabatic switching of interactions and the imaginary time propagation
technique combined with the Gram-Schmidt orthonormalization or the
diagonalization of the Hamiltonian matrix. We show how different algorithms
compete at the numerical level by studying the mixture in the range of
parameters covering the formation of self-bound quantum Bose-Fermi droplets.; 19) A Framework for Supporting the Reproducibility of Computational
  Experiments in Multiple Scientific Domains; In recent years, the research community, but also the general public, has
raised serious questions about the reproducibility and replicability of
scientific work. Since many studies include some kind of computational work,
these issues are also a technological challenge, not only in computer science,
but also in most research domains. Computational replicability and
reproducibility are not easy to achieve due to the variety of computational
environments that can be used. Indeed, it is challenging to recreate the same
environment via the same frameworks, code, programming languages, dependencies,
and so on. We propose a framework, known as SciRep, that supports the
configuration, execution, and packaging of computational experiments by
defining their code, data, programming languages, dependencies, databases, and
commands to be executed. After the initial configuration, the experiments can
be executed any number of times, always producing exactly the same results. Our
approach allows the creation of a reproducibility package for experiments from
multiple scientific fields, from medicine to computer science, which can be
re-executed on any computer. The produced package acts as a capsule, holding
absolutely everything necessary to re-execute the experiment. To evaluate our
framework, we compare it with three state-of-the-art tools and use it to
reproduce 18 experiments extracted from published scientific articles. With our
approach, we were able to execute 16 (89%) of those experiments, while the
others reached only 61%, thus showing that our approach is effective. Moreover,
all the experiments that were executed produced the results presented in the
original publication. Thus, SciRep was able to reproduce 100% of the
experiments it could run.; 20) BERTopic for Topic Modeling of Hindi Short Texts: A Comparative Study; As short text data in native languages like Hindi increasingly appear in
modern media, robust methods for topic modeling on such data have gained
importance. This study investigates the performance of BERTopic in modeling
Hindi short texts, an area that has been under-explored in existing research.
Using contextual embeddings, BERTopic can capture semantic relationships in
data, making it potentially more effective than traditional models, especially
for short and diverse texts. We evaluate BERTopic using 6 different document
embedding models and compare its performance against 8 established topic
modeling techniques, such as Latent Dirichlet Allocation (LDA), Non-negative
Matrix Factorization (NMF), Latent Semantic Indexing (LSI), Additive
Regularization of Topic Models (ARTM), Probabilistic Latent Semantic Analysis
(PLSA), Embedded Topic Model (ETM), Combined Topic Model (CTM), and Top2Vec.
The models are assessed using coherence scores across a range of topic counts.
Our results reveal that BERTopic consistently outperforms other models in
capturing coherent topics from short Hindi texts.; 21) Extended cut groups; A finite group G is said to be a cut group if all central units in the
integral group ring ZG are trivial. In this article, we extend the notion of
cut groups, by introducing extended cut groups. We study the properties of
extended cut groups analogous to those known for cut groups and also
characterise some substantial classes of groups having the property of being
extended cut. A complete classification of extended cut split metacyclic groups
has been presented.; 22) Ordering digraphs with maximum outdegrees by their $A_{\alpha}$ spectral
  radius; Let $G$ be a strongly connected digraph with $n$ vertices and $m$ arcs. For
any real $\alpha\in[0,1]$, the $A_\alpha$ matrix of a digraph $G$ is defined as
$$A_\alpha(G)=\alpha D(G)+(1-\alpha)A(G),$$ where $A(G)$ is the adjacency
matrix of $G$ and $D(G)$ is the outdegrees diagonal matrix of $G$. The
eigenvalue of $A_\alpha(G)$ with the largest modulus is called the $A_\alpha$
spectral radius of $G$, denoted by $\lambda_{\alpha}(G)$. In this paper, we
first obtain an upper bound on $\lambda_{\alpha}(G)$ for
$\alpha\in[\frac{1}{2},1)$. Employing this upper bound, we prove that for two
strongly connected digraphs $G_1$ and $G_2$ with $n\ge4$ vertices and $m$ arcs,
and $\alpha\in [\frac{1}{\sqrt{2}},1)$, if the maximum outdegree
$\Delta^+(G_1)\ge 2\alpha(1-\alpha)(m-n+1)+2\alpha$ and
$\Delta^+(G_1)>\Delta^+(G_2)$, then $\lambda_\alpha(G_1)>\lambda_\alpha(G_2)$.
Moreover, We also give another upper bound on $\lambda_{\alpha}(G)$ for
$\alpha\in[\frac{1}{2},1)$. Employing this upper bound, we prove that for two
strongly connected digraphs with $m$ arcs, and $\alpha\in[\frac{1}{2},1)$, if
the maximum outdegree $\Delta^+(G_1)>\frac{2m}{3}+1$ and
$\Delta^+(G_1)>\Delta^+(G_2)$, then
$\lambda_\alpha(G_1)+\frac{1}{4}>\lambda_\alpha(G_2)$.; 23) A Lie algebra view of matrix splittings; In this paper we use some basic facts from the theory of (matrix) Lie groups
and algebras to show that many of the classical matrix splittings used to
construct stationary iterative methods and preconditioniers for Krylov subspace
methods can be interpreted as linearizations of matrix factorizations.
Moreover, we show that new matrix splittings are obtained when we specialize
these splittings to some of the classical matrix groups and their Lie and
Jordan algebras. As an example, we derive structured generalizations of the HSS
(Hermitian/skew-Hermitian) iteration, and provide sufficient conditions for
their convergence.; 24) Hierarchical Learning-based Graph Partition for Large-scale Vehicle
  Routing Problems; Neural solvers based on the divide-and-conquer approach for Vehicle Routing
Problems (VRPs) in general, and capacitated VRP (CVRP) in particular,
integrates the global partition of an instance with local constructions for
each subproblem to enhance generalization. However, during the global partition
phase, misclusterings within subgraphs have a tendency to progressively
compound throughout the multi-step decoding process of the learning-based
partition policy. This suboptimal behavior in the global partition phase, in
turn, may lead to a dramatic deterioration in the performance of the overall
decomposition-based system, despite using optimal local constructions. To
address these challenges, we propose a versatile Hierarchical Learning-based
Graph Partition (HLGP) framework, which is tailored to benefit the partition of
CVRP instances by synergistically integrating global and local partition
policies. Specifically, the global partition policy is tasked with creating the
coarse multi-way partition to generate the sequence of simpler two-way
partition subtasks. These subtasks mark the initiation of the subsequent K
local partition levels. At each local partition level, subtasks exclusive for
this level are assigned to the local partition policy which benefits from the
insensitive local topological features to incrementally alleviate the
compounded errors. This framework is versatile in the sense that it optimizes
the involved partition policies towards a unified objective harmoniously
compatible with both reinforcement learning (RL) and supervised learning (SL).
(*Due to the notification of arXiv ""The Abstract field cannot be longer than
1,920 characters"", the appeared Abstract is shortened. For the full Abstract,
please download the Article.); 25) Quantifying Correlations of Machine Learning Models; Machine Learning models are being extensively used in safety critical
applications where errors from these models could cause harm to the user. Such
risks are amplified when multiple machine learning models, which are deployed
concurrently, interact and make errors simultaneously. This paper explores
three scenarios where error correlations between multiple models arise,
resulting in such aggregated risks. Using real-world data, we simulate these
scenarios and quantify the correlations in errors of different models. Our
findings indicate that aggregated risks are substantial, particularly when
models share similar algorithms, training datasets, or foundational models.
Overall, we observe that correlations across models are pervasive and likely to
intensify with increased reliance on foundational models and widely used public
datasets, highlighting the need for effective mitigation strategies to address
these challenges.; 26) Nucleon electromagnetic form factors at large momentum from Lattice QCD; Proton and neutron electric and magnetic form factors are the primary
characteristics of their spatial structure and have been studied extensively
over the past half-century. At large values of the momentum transfer $Q^2$ they
should reveal transition from nonperturbative to perturbative QCD dynamics as
well as effects of quark orbital angular momenta and diquark correlations.
Currently, these form factors are being measured at JLab at momentum transfer
up to $Q^2=18$ GeV$^2$ for the proton and up to 14 GeV$^2$ for the neutron. We
report preliminary results of our lattice calculations of these form factors,
including $G_E$ and $G_M$ nucleon form factors with momenta up to $Q^2=8$
GeV$^2$, pion masses down to the almost-physical $m_\pi$=170 MeV, several
lattice spacings down to $a=0.073$ fm, and high $O(10^5)$ statistics.
Specifically, we study individual form factors, the $G_E/G_M$ ratios, and
flavor dependence of contributions to the form factors. We observe qualitative
agreement of our ab initio theory calculations with experiment. Comparison of
our calculations and upcoming JLab experimental results will be an important
test of nonperturbative QCD methods in the almost-perturbative regime.; 27) Investigating the effects of QCD matter's electrical conductivity on
  charge dependent directed flow; Charge dependent directed flow is an important observable of electromagnetic
fields in relativistic heavy-ion collisions. We demonstrate how the difference
in charge dependent directed flows between protons and antiprotons is sensitive
to the resistivity, inverse of quark-gluon plasma's electric conductivity, over
different collision centralities. Our model numerically solves the 3+1D
relativistic resistive magneto-hydrodynamic (RRMHD) equations, assuming the
electric conductivity to be a scalar. For this work, we focus on symmetric Au +
Au collisions at the top RHIC energy of $\sqrt{s}=200$ GeV. We illustrate the
time evolution of the electromagnetic fields in our model and connect that to
the charge dependent directed flow results. Our results highlight the
importance of modeling quark-gluon plasma's electric conductivity for charge
dependent observables in relativistic heavy-ion collisions.; 28) Quant GANs: deep generation of financial time series; Modeling financial time series by stochastic processes is a challenging task
and a central area of research in financial mathematics. As an alternative, we
introduce Quant GANs, a data-driven model which is inspired by the recent
success of generative adversarial networks (GANs). Quant GANs consist of a
generator and discriminator function, which utilize temporal convolutional
networks (TCNs) and thereby achieve to capture long-range dependencies such as
the presence of volatility clusters. The generator function is explicitly
constructed such that the induced stochastic process allows a transition to its
risk-neutral distribution. Our numerical results highlight that distributional
properties for small and large lags are in an excellent agreement and
dependence properties such as volatility clusters, leverage effects, and serial
autocorrelations can be generated by the generator function of Quant GANs,
demonstrably in high fidelity.; 29) General behavior of near-threshold hadron scattering for exotic hadrons; We discuss the general behavior of the near-threshold scattering amplitude
with channel couplings. The signal of the exotic hadrons near the threshold may
manifest as a dip structure in the cross section originated from a zero point
of the scattering amplitude. Such a dip structure by the zero point cannot be
reproduced by the Flatt\'{e} amplitude which is widely used for the analysis of
exotic hadrons, because of the constraints imposed on the Flatt\'{e} amplitude
near the threshold. In this work, we propose the General amplitude which can
describe the dip structure near the threshold, in contrast to the Flatt\'{e}
amplitude. Moreover, we numerically study the behavior of the near-threshold
cross section in relation to the zero point.; 30) Quantum networks using rare-earth ions; We review concepts and recent work related to creating light-matter
interfaces for future quantum networks based on rare-earth ion-doped crystals.
More precisely, we explore their unique suitability for creating photon
sources, optical quantum memories for light, and qubits that allow quantum
information processing. In addition, we review the state-of-the-art of
elementary quantum repeater links, and provide suggestions for future research.; 31) Comprehensive Digital Forensics and Risk Mitigation Strategy for Modern
  Enterprises; Enterprises today face increasing cybersecurity threats that necessitate
robust digital forensics and risk mitigation strategies. This paper explores
these challenges through an imaginary case study of an organization, a global
identity management and data analytics company handling vast customer data.
Given the critical nature of its data assets, EP has established a dedicated
digital forensics team to detect threats, manage vulnerabilities, and respond
to security incidents. This study outlines an approach to cybersecurity,
including proactive threat anticipation, forensic investigations, and
compliance with regulations like GDPR and CCPA. Key threats such as social
engineering, insider risks, phishing, and ransomware are examined, along with
mitigation strategies leveraging AI and machine learning. By detailing security
framework, this paper highlights best practices in digital forensics, incident
response, and enterprise risk management. The findings emphasize the importance
of continuous monitoring, policy enforcement, and adaptive security measures to
protect sensitive data and ensure business continuity in an evolving threat
landscape; 32) AI Chatbots as Professional Service Agents: Developing a Professional
  Identity; With the rapid expansion of large language model (LLM) applications, there is
an emerging shift in the role of LLM-based AI chatbots from serving merely as
general inquiry tools to acting as professional service agents. However,
current studies often overlook a critical aspect of professional service
agents: the act of communicating in a manner consistent with their professional
identities. This is of particular importance in the healthcare sector, where
effective communication with patients is essential for achieving professional
goals, such as promoting patient well-being by encouraging healthy behaviors.
To bridge this gap, we propose LAPI (LLM-based Agent with a Professional
Identity), a novel framework for designing professional service agent tailored
for medical question-and-answer (Q\&A) services, ensuring alignment with a
specific professional identity. Our method includes a theory-guided task
planning process that decomposes complex professional tasks into manageable
subtasks aligned with professional objectives and a pragmatic entropy method
designed to generate professional and ethical responses with low uncertainty.
Experiments on various LLMs show that the proposed approach outperforms
baseline methods, including few-shot prompting, chain-of-thought prompting,
across key metrics such as fluency, naturalness, empathy, patient-centricity,
and ROUGE-L scores. Additionally, the ablation study underscores the
contribution of each component to the overall effectiveness of the approach.; 33) Reading to Listen at the Cocktail Party: Multi-Modal Speech Separation; The goal of this paper is speech separation and enhancement in multi-speaker
and noisy environments using a combination of different modalities. Previous
works have shown good performance when conditioning on temporal or static
visual evidence such as synchronised lip movements or face identity. In this
paper, we present a unified framework for multi-modal speech separation and
enhancement based on synchronous or asynchronous cues. To that end we make the
following contributions: (i) we design a modern Transformer-based architecture
tailored to fuse different modalities to solve the speech separation task in
the raw waveform domain; (ii) we propose conditioning on the textual content of
a sentence alone or in combination with visual information; (iii) we
demonstrate the robustness of our model to audio-visual synchronisation
offsets; and, (iv) we obtain state-of-the-art performance on the
well-established benchmark datasets LRS2 and LRS3.; 34) Object State Estimation Through Robotic Active Interaction for
  Biological Autonomous Drilling; Estimating the state of biological specimens is challenging due to limited
observation through microscopic vision. For instance, during mouse skull
drilling, the appearance alters little when thinning bone tissue because of its
semi-transparent property and the high-magnification microscopic vision. To
obtain the object's state, we introduce an object state estimation method for
biological specimens through active interaction based on the deflection. The
method is integrated to enhance the autonomous drilling system developed in our
previous work. The method and integrated system were evaluated through 12
autonomous eggshell drilling experiment trials. The results show that the
system achieved a 91.7% successful ratio and 75% detachable ratio, showcasing
its potential applicability in more complex surgical procedures such as mouse
skull craniotomy. This research paves the way for further development of
autonomous robotic systems capable of estimating the object's state through
active interaction.; 35) Improved Offline Contextual Bandits with Second-Order Bounds: Betting
  and Freezing; We consider the off-policy selection and learning in contextual bandits where
the learner aims to select or train a reward-maximizing policy using data
collected by a fixed behavior policy. Our contribution is two-fold. First, we
propose a novel off-policy selection method that leverages a new betting-based
confidence bound applied to an inverse propensity weight sequence. Our
theoretical analysis reveals that our method achieves a significantly better,
variance-adaptive guarantee upon prior art. Second, we propose a novel and
generic condition on the optimization objective for off-policy learning that
strikes a difference balance in bias and variance. One special case that we
call freezing tends to induce small variance, which is preferred in small-data
regimes. Our analysis shows that they match the best existing guarantee. In our
empirical study, our selection method outperforms existing methods, and
freezing exhibits improved performance in small-sample regimes.; 36) Second-order monotonicity conditions and mean field games with
  volatility control; In this manuscript we study the well-posedness of the master equations for
mean field games with volatility control. This infinite dimensional PDE is
nonlinear with respect to both the first and second-order derivatives of its
solution. For standard mean field games with only drift control, it is
well-known that certain monotonicity condition is essential for the uniqueness
of mean field equilibria and for the global well-posedness of the master
equations. To adapt to the current setting with volatility control, we propose
a new notion called second-order monotonicity conditions. Surprisingly, the
second-order Lasry-Lions monotonicity is equivalent to its standard
(first-order) version, but such an equivalency fails for displacement
monotonicity. When the Hamiltonian is separable and the data are Lasry-Lions
monotone, we show that the Lasry-Lions monotonicity propagates and the master
equation admits a unique classical solution. This is the first work for the
well-posedness, both local and global, of master equations when the volatility
is controlled.; 37) Amorphous to Crystalline Transformation: How Cluster Aggregation Drives
  the Multistep Nucleation of ZIF-8; Nucleation, the pivotal first step of crystallization, governs essential
characteristics of crystallization products, including size distribution,
morphology, and polymorphism. While understanding this process is paramount to
the design of chemical, pharmaceutical and industrial production processes,
major knowledge gaps remain, especially with respect to the crystallization of
porous solids. Also for nanocrystalline ZIF-8, one of the most widely studied
metal-organic frameworks, questions regarding the species involved in the
nucleation pathway and their structural and chemical transformations remain
unanswered. By combining harmonic light scattering, inherently sensitive to
structural changes, with NMR spectroscopy, which reveals molecular exchanges
between particles and solution, we were able to capture the crystallization
mechanism of ZIF-8 in unprecedented detail. This dual approach provides
concurrent structural and chemical insights, revealing key processes not
previously observed in ZIF crystallization. Upon mixing small charged
prenucleation clusters (PNCs) are formed, exhibiting an excess of ligands and
net positive charge. We show that nucleation is initiated by aggregation of
PNCs, through the release of ligands and associated protons to the liquid. This
leads to the formation of charge neutral amorphous precursor particles (APPs)
which incorporate neutral monomers from solution, and crystallize ZIF-8. Our
work highlights chemical dynamics as a vital, yet often overlooked, dimension
in the multi-stage structural evolution of MOFs. By establishing the critical
role of PNCs in the nucleation of ZIF-8, new pathways open up for controlling
crystallization of metal-organic frameworks through targeted chemical
interactions with these species.; 38) Edge AI-Powered Real-Time Decision-Making for Autonomous Vehicles in
  Adverse Weather Conditions; Autonomous vehicles (AVs) are transforming modern transportation, but their
reliability and safety are significantly challenged by harsh weather conditions
such as heavy rain, fog, and snow. These environmental factors impair the
performance of cameras, LiDAR, and radar, leading to reduced situational
awareness and increased accident risks. Conventional cloud-based AI systems
introduce communication delays, making them unsuitable for the rapid
decision-making required in real-time autonomous navigation. This paper
presents a novel Edge AI-driven real-time decision-making framework designed to
enhance AV responsiveness under adverse weather conditions. The proposed
approach integrates convolutional neural networks (CNNs) and recurrent neural
networks (RNNs) for improved perception, alongside reinforcement learning
(RL)-based strategies to optimize vehicle control in uncertain environments. By
processing data at the network edge, this system significantly reduces decision
latency while improving AV adaptability. The framework is evaluated using
simulated driving scenarios in CARLA and real-world data from the Waymo Open
Dataset, covering diverse weather conditions. Experimental results indicate
that the proposed model achieves a 40% reduction in processing time and a 25%
enhancement in perception accuracy compared to conventional cloud-based
systems. These findings highlight the potential of Edge AI in improving AV
autonomy, safety, and efficiency, paving the way for more reliable self-driving
technology in challenging real-world environments.; 39) A glimpse into an effective world; Our contribution aims to celebrate the immeasurable contribution that Tom Kuo
has provided to the understanding of the structure of atomic nuclei, and also
of the infinite nuclear matter, in terms of the fundamental principles
governing the realistic nuclear potential. The authors want to testify Tom
Kuo's heritage and impact on their approach to the study of nuclear systems by
reviewing some recent findings on the role of the two-body component of
shell-model effective $\beta$-decay operators. The focus is spotted on the
so-called Pauli-blocking effect, that plays a non-negligible role in nuclei
characterized by a large number of valence nucleons.; 40) YUNet: Improved YOLOv11 Network for Skyline Detection; Skyline detection plays an important role in geolocalizaion, flight control,
visual navigation, port security, etc. The appearance of the sky and non-sky
areas are variable, because of different weather or illumination environment,
which brings challenges to skyline detection. In this research, we proposed the
YUNet algorithm, which improved the YOLOv11 architecture to segment the sky
region and extract the skyline in complicated and variable circumstances. To
improve the ability of multi-scale and large range contextual feature fusion,
the YOLOv11 architecture is extended as an UNet-like architecture, consisting
of an encoder, neck and decoder submodule. The encoder extracts the multi-scale
features from the given images. The neck makes fusion of these multi-scale
features. The decoder applies the fused features to complete the prediction
rebuilding. To validate the proposed approach, the YUNet was tested on
Skyfinder and CH1 datasets for segmentation and skyline detection respectively.
Our test shows that the IoU of YUnet segmentation can reach 0.9858, and the
average error of YUnet skyline detection is just 1.36 pixels. The
implementation is published at
https://github.com/kuazhangxiaoai/SkylineDet-YOLOv11Seg.git.; 41) Minimal unit vector fields on oscillator groups; In this paper, we treat minimal left-invariant unit vector fields on
oscillator group and their relations with the ones that define a harmonic map.
Particularly, if all structure constants of the oscillator group are equal to
each other, then all unit left invariant vector fields that define a harmonic
map into the unit tangent bundle with Sasaki metric are minimal.; 42) Joint ML-Bayesian Approach to Adaptive Radar Detection in the presence
  of Gaussian Interference; This paper addresses the adaptive radar target detection problem in the
presence of Gaussian interference with unknown statistical properties. To this
end, the problem is first formulated as a binary hypothesis test, and then we
derive a detection architecture grounded on the hybrid of Maximum Likelihood
(ML) and Maximum A Posterior (MAP) approach. Specifically, we resort to the
hidden discrete latent variables in conjunction with the
Expectation-Maximization (EM) algorithms which cyclically updates the estimates
of the unknowns. In this framework, the estimates of the a posteriori
probabilities under each hypothesis are representative of the inherent nature
of data and used to decide for the presence of a potential target. In addition,
we prove that the developed detection scheme ensures the desired Constant False
Alarm Rate property with respect to the unknown interference covariance matrix.
Numerical examples obtained through synthetic and real recorded data
corroborate the effectiveness of the proposed architecture and show that the
MAP-based approach ensures evident improvement with respect to the conventional
generalized likelihood ratio test at least for the considered scenarios and
parameter setting.; 43) GaussianFlowOcc: Sparse and Weakly Supervised Occupancy Estimation using
  Gaussian Splatting and Temporal Flow; Occupancy estimation has become a prominent task in 3D computer vision,
particularly within the autonomous driving community. In this paper, we present
a novel approach to occupancy estimation, termed GaussianFlowOcc, which is
inspired by Gaussian Splatting and replaces traditional dense voxel grids with
a sparse 3D Gaussian representation. Our efficient model architecture based on
a Gaussian Transformer significantly reduces computational and memory
requirements by eliminating the need for expensive 3D convolutions used with
inefficient voxel-based representations that predominantly represent empty 3D
spaces. GaussianFlowOcc effectively captures scene dynamics by estimating
temporal flow for each Gaussian during the overall network training process,
offering a straightforward solution to a complex problem that is often
neglected by existing methods. Moreover, GaussianFlowOcc is designed for
scalability, as it employs weak supervision and does not require costly dense
3D voxel annotations based on additional data (e.g., LiDAR). Through extensive
experimentation, we demonstrate that GaussianFlowOcc significantly outperforms
all previous methods for weakly supervised occupancy estimation on the nuScenes
dataset while featuring an inference speed that is 50 times faster than current
SOTA.; 44) Resonant inter-mode second harmonic generation by backward spin waves in
  YIG nano-waveguides; We experimentally study nonlinear generation of the second harmonic by
backward volume spin waves propagating in microscopic magnonic waveguides
fabricated from a low-loss magnetic insulator with a thickness of several tens
of nanometers. We show that highly efficient resonant second harmonic
generation is possible in the inter-mode regime at microwave powers of the
order of 10^-4 W. In contrast to previously observed second harmonic generation
processes, the generation by backward waves is characterized by the nonlinearly
generated waves propagating opposite to the initial waves, and can be realized
at zero bias magnetic field.; 45) Anatomy of a Digital Bubble: Lessons Learned from the NFT and Metaverse
  Frenzy; In the past few years, ""metaverse"" and ""non-fungible tokens (NFT)"" have
become buzzwords, and the prices of related assets have exhibited large
fluctuations. Are those characteristic of a speculative bubble? In this paper,
we attempt to answer this question, and better understand the underlying
economic dynamics. We look at Decentraland, a virtual world platform where land
parcels are sold as NFT collections. We find that initially, land prices
followed traditional real estate pricing models - in particular, value
decreased with distance from the most desirable areas - suggesting Decentraland
behaved much like a virtual city. However, these real estate pricing models
stopped applying when both the metaverse and NFTs gained increased popular
attention and enthusiasm in 2021, suggesting a new driving force for the
underlying asset prices. At that time, following a substantial rise in NFT
market values, short-term holders of multiple parcels began to take major
selling positions in the Decentraland market, which hints that, rather than
building a metaverse community, early Decentraland investors preferred to cash
out when land valuations became inflated. Our analysis also shows that while
the majority of buyers are new entrants to the market (many of whom joined
during the bubble), liquidity (i.e., parcels) was mostly provided by early
adopters selling, which caused stark differences in monetary gains. Early
adopters made money - more than 10,000 USD on average per parcel sold - but
users who joined later typically made no profit or even incurred losses in the
order of 1,000 USD per parcel. Unlike established markets such as financial and
real estate markets, newly emergent digital marketplaces are mostly
self-regulated. As a result, the significant financial risks we identify
indicate a strong need for establishing appropriate standards of business
conduct and improving user awareness.; 46) Diffusion-augmented Graph Contrastive Learning for Collaborative Filter; Graph-based collaborative filtering has been established as a prominent
approach in recommendation systems, leveraging the inherent graph topology of
user-item interactions to model high-order connectivity patterns and enhance
recommendation performance. Recent advances in Graph Contrastive Learning (GCL)
have demonstrated promising potential to alleviate data sparsity issues by
improving representation learning through contrastive view generation and
mutual information maximization. However, existing approaches lack effective
data augmentation strategies. Structural augmentation risks distorting
fundamental graph topology, while feature-level perturbation techniques
predominantly employ uniform noise scales that fail to account for
node-specific characteristics. To solve these challenges, we propose
Diffusion-augmented Contrastive Learning (DGCL), an innovative framework that
integrates diffusion models with contrastive learning for enhanced
collaborative filtering. Our approach employs a diffusion process that learns
node-specific Gaussian distributions of representations, thereby generating
semantically consistent yet diversified contrastive views through reverse
diffusion sampling. DGCL facilitates adaptive data augmentation based on
reconstructed representations, considering both semantic coherence and
node-specific features. In addition, it explores unrepresented regions of the
latent sparse feature space, thereby enriching the diversity of contrastive
views. Extensive experimental results demonstrate the effectiveness of DGCL on
three public datasets.; 47) Influence of Aspect Ratio and Deformation on the Isotropic-Nematic
  Crossover in Quasi-One-Dimensional Hard Superellipses; We study the orientational ordering and close packing behavior of a
quasi-one-dimensional (q1D)
  hard superellipse system in which centers of particles are restricted to a
line while they can rotate
  freely in a two-dimensional plane. Through the aspect ratio (k), the
elongation of the particles and
  through the deformation parameter (n), the shape can be tuned between
ellipses and rectangles.
  Maxima in the pressure ratio (P/Phd) (where P and Phd are the pressure of
hard superellipses and
  that of q1D hard disks) are a good indicator for the continuous structural
change from isotropic
  to nematic order. This change occurs at higher densities than the sharp
nematic transitions of
  corresponding two-dimensional and three-dimensional hard body systems.
Additionally, suitable
  close packing exponents describing the close-packing limit show universality
in that they depending
  only on n and certain combinations are independent of both k and n.; 48) Optimizing Parameter Estimation for Electrochemical Battery Model: A
  Comparative Analysis of Operating Profiles on Computational Efficiency and
  Accuracy; Parameter estimation in electrochemical models remains a significant
challenge in their application. This study investigates the impact of different
operating profiles on electrochemical model parameter estimation to identify
the optimal conditions. In particular, the present study is focused on Nickel
Manganese Cobalt Oxide(NMC) lithium-ion batteries. Based on five fundamental
current profiles (C/5, C/2, 1C, Pulse, DST), 31 combinations of conditions were
generated and used for parameter estimation and validation, resulting in 961
evaluation outcomes. The Particle Swarm Optimization is employed for parameter
identification in electrochemical models, specifically using the Single
Particle Model (SPM). The analysis considered three dimensions: model voltage
output error, parameter estimation error, and time cost. Results show that
using all five profiles (C/5, C/2, 1C, Pulse, DST) minimizes voltage output
error, while {C/5, C/2, Pulse, DST} minimizes parameter estimation error. The
shortest time cost is achieved with {1C}. When considering both model voltage
output and parameter errors, {C/5, C/2, 1C, DST} is optimal. For minimizing
model voltage output error and time cost, {C/2, 1C} is best, while {1C} is
ideal for parameter error and time cost. The comprehensive optimal condition is
{C/5, C/2, 1C, DST}. These findings provide guidance for selecting current
conditions tailored to specific needs.; 49) Causality for VARMA processes with instantaneous effects: The global
  Markov property, faithfulness and instrumental variables; Causal reasoning has gained great attention over the last half century as it
allows (or at least intends) to answer questions which go above those within
the capabilities of classical inferential statistics using just observational
data. So far, causal research has been focused mostly on the i.i.d. setting.
However, many are the situations where there exists a non-trivial dependence
structure between sequential observations. Motivated by this fact, the main
purpose of this work is to study causal properties of time series under the
structural assumption of a VARMA model with instantaneous effects. First, the
global Markov property is studied, building on existing work for VAR processes
without instantaneous effects. Infinite graphs which represent the dependencies
of the process are defined so that separation statements translate to
conditional independencies in the stationary distribution of the process.
Second, faithfulness is examined as a counterpart of this Markov property.
Conditions are given so that the stationary distribution of the process is
almost surely faithful to said infinite graphs. In addition, an instrumental
variable regression framework is developed for VARMA models with instantaneous
effects. This allows to identify and consistently estimate total causal
effects.; 50) Innovative Financing Solutions: A Transformative Driver for Financial
  Performance of Businesses in Morocco; In a rapidly evolving landscape marked by continuous change and complex
challenges, effective cash management stands as a cornerstone for ensuring
business sustainability and driving performance. To address these pressing
demands, cash managersare increasingly turning to innovative financing
solutions such as venture capital, green finance, crowdfunding, advanced
services from Pan-African banks, and blockchain technology. These cutting-edge
tools are pivotal in bolstering resilience against market volatility,
ecological transitions, and the accelerating pace of technological change. The
present article aims to examine how such innovative financial approaches can
serve as strategic drivers, enabling businesses to transform challenges into
opportunities. The analysis underscores that rethinking cash management through
innovation is a critical pathway toboost the performance of Moroccan companies.
Therefore, embracing these forward-thinking strategies unlocks new avenues for
development empowering them to adapt with agility amidst the uncertainties of a
shifting environment.; 51) Tensor condensate accompanied by chiral transition in a strong magnetic
  field; We investigate tensor condensates and chiral condensates in the (2+1)-flavor
Nambu-Jona-Lasinio model at finite temperature and density in the presence of a
strong magnetic field. The emergence of the tensor condensate is attributed to
the four-fermion interaction. It is shown that a sufficiently large chemical
potential is necessary for the occurrence of a phase transition towards tensor
condensate. Furthermore, we investigate the correlation between tensor
condensate and spin polarization, which accounts for the oscillatory behavior
of the spin polarization during the transition from the chiral condensate to
tensor condensate phase.; 52) ML-Triton, A Multi-Level Compilation and Language Extension to Triton
  GPU Programming; In the era of LLMs, dense operations such as GEMM and MHA are critical
components. These operations are well-suited for parallel execution using a
tilebased approach. While traditional GPU programming often relies on low level
interfaces like CUDA or SYCL, Triton has emerged as a DSL that offers a more
user-friendly and portable alternative by programming at a higher level. The
current Triton starts at the workgroup (aka threadblock) level, and directly
lowers to per-thread level. And then attempt to coalesce and amend through a
series of passes, promoting information from low-level representation. We
believe this is pre-mature lowering based on the below observations. 1. GPU has
a hierarchical structure both physically and logically. Modern GPUs often
feature SIMD units capable of directly operating on tiles on a warp or
warpgroup basis, such as blocked load and blocked MMA. 2. Multi-level gradual
lowering can make compiler decoupled and clean by separating considerations
inter and intra a logical layer. 3. Kernel developers often need fine control
to get good performance on the latest hardware. FlashAttention2 advocates
explicit data partition between warps to make a performance boost. In this
context, we propose ML-Triton which features multi-level compilation flow and
programming interface. Our approach begins at the workgroup level and
progressively lowers to the warp and intrinsic level, implementing a multilevel
lowering align with the hierarchical nature of GPU. Additionally, we extend
triton language to support user-set compiler hint and warp level programming,
enabling researchers to get good out-of-the box performance without awaiting
compiler updates. Experimental results demonstrate that our approach achieves
performance above 95% of expert-written kernels on Intel GPU, as measured by
the geometric mean.; 53) Advancing Personalized Learning Analysis via an Innovative Domain
  Knowledge Informed Attention-based Knowledge Tracing Method; Emerging Knowledge Tracing (KT) models, particularly deep learning and
attention-based Knowledge Tracing, have shown great potential in realizing
personalized learning analysis via prediction of students' future performance
based on their past interactions. The existing methods mainly focus on
immediate past interactions or individual concepts without accounting for
dependencies between knowledge concept, referred as knowledge concept routes,
that can be critical to advance the understanding the students' learning
outcomes. To address this, in this paper, we propose an innovative
attention-based method by effectively incorporating the domain knowledge of
knowledge concept routes in the given curriculum. Additionally, we leverage
XES3G5M dataset, a benchmark dataset with rich auxiliary information for
knowledge concept routes, to evaluate and compare the performance of our
proposed method to the seven State-of-the-art (SOTA) deep learning models.; 54) AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse
  Autoencoders; Fine-grained steering of language model outputs is essential for safety and
reliability. Prompting and finetuning are widely used to achieve these goals,
but interpretability researchers have proposed a variety of
representation-based techniques as well, including sparse autoencoders (SAEs),
linear artificial tomography, supervised steering vectors, linear probes, and
representation finetuning. At present, there is no benchmark for making direct
comparisons between these proposals. Therefore, we introduce AxBench, a
large-scale benchmark for steering and concept detection, and report
experiments on Gemma-2-2B and 9B. For steering, we find that prompting
outperforms all existing methods, followed by finetuning. For concept
detection, representation-based methods such as difference-in-means, perform
the best. On both evaluations, SAEs are not competitive. We introduce a novel
weakly-supervised representational method (Rank-1 Representation Finetuning;
ReFT-r1), which is competitive on both tasks while providing the
interpretability advantages that prompting lacks. Along with AxBench, we train
and publicly release SAE-scale feature dictionaries for ReFT-r1 and DiffMean.; 55) A calibration in $\mathbf R^{16}$ and Federer's product question; Building upon the construction of a Cayley calibration adapted to a complex
structure, we introduce a calibration $\Phi$ in $\bigwedge^8 \mathbf R^{16}$
with $|\Phi^2| = 294$. This enables us to show that the product of two
orthogonally supported calibrations is not necessarily a calibration, thereby
providing a negative answer to a question posed by Federer. Dadok and Harvey
developed a general method for constructing calibrations as outer products of
two unit spinors in the Clifford algebra. We show that $\Phi$ arises from the
product of two spinors with norm $1$ and $\sqrt{2}$.; 56) Investigation of O interstitial diffusion in $\beta$-Ga$_2$O$_3$: direct
  approach via master diffusion equations; Monoclinic $\beta$-Ga$_2$O$_3$, a promising wide band gap semiconducting
material, exhibits complex, anisotropic diffusional characteristics and mass
transport behavior as a results of its low symmetry crystal structure. From
first-principles calculations combined with master diffusion equations, we
determine three-dimensional diffusion tensors for neutral
($\text{O}_{\text{i}}^{0}$) and 2- charged oxygen interstitials
($\text{O}_{\text{i}}^{2-}$). Systematic exploration of the configurational
space identifies stable configurations in these two dominant charge states and
their corresponding formation energies. By connecting every pair of low-energy
configurations considering both interstitial or interstitialcy hops, we
construct three-dimensional diffusion networks and evaluate hopping barriers of
all transition pathways in networks. Combining the collection of (i) defect
configurations and their formation energies and (ii) the hopping barriers that
link them, we construct and solve the master diffusion equations for
$\text{O}_{\text{i}}^{0}$ and $\text{O}_{\text{i}}^{2-}$ separately through the
Onsager approach, resulting in respective three-dimensional diffusion tensors
D$_{\text{O}_{\text{i}}}^{0}$ and D$_{\text{O}_{\text{i}}}^{2-}$. Both
$\text{O}_{\text{i}}^{0}$ and $\text{O}_{\text{i}}^{2-}$ present the fastest
diffusion along the $b$-axis, demonstrating significant anisotropy. The
predicted self-diffusivities along [100] and [$\overline{2}01$] align well with
previously reported values from isotopically labeled oxygen tracer experiments,
highlighting the reliability of the approach in capturing complex diffusion
mechanisms.; 57) Utilizing Pre-trained and Large Language Models for 10-K Items
  Segmentation; Extracting specific items from 10-K reports remains challenging due to
variations in document formats and item presentation. Traditional rule-based
item segmentation approaches often yield suboptimal results. This study
introduces two advanced item segmentation methods leveraging language models:
(1) GPT4ItemSeg, using a novel line-ID-based prompting mechanism to utilize
GPT4 for item segmentation, and (2) BERT4ItemSeg, combining BERT embeddings
with a Bi-LSTM model in a hierarchical structure to overcome context window
constraints. Trained and evaluated on 3,737 annotated 10-K reports,
BERT4ItemSeg achieved a macro-F1 of 0.9825, surpassing GPT4ItemSeg (0.9567),
conditional random field (0.9818), and rule-based methods (0.9048) for core
items (1, 1A, 3, and 7). These approaches enhance item segmentation
performance, improving text analytics in accounting and finance. BERT4ItemSeg
offers satisfactory item segmentation performance, while GPT4ItemSeg can easily
adapt to regulatory changes. Together, they offer practical benefits for
researchers and practitioners, enabling reliable empirical studies and
automated 10-K item segmentation functionality.; 58) Combined climate stress testing of supply-chain networks and the
  financial system with nation-wide firm-level emission estimates; On the way towards carbon neutrality, climate stress testing provides
estimates for the physical and transition risks that climate change poses to
the economy and the financial system. Missing firm-level CO2 emissions data
severely impedes the assessment of transition risks originating from carbon
pricing. Based on the individual emissions of all Hungarian firms (410,523), as
estimated from their fossil fuel purchases, we conduct a stress test of both
actual and hypothetical carbon pricing policies. Using a simple 1:1 economic
ABM and introducing the new carbon-to-profit ratio, we identify firms that
become unprofitable and default, and estimate the respective loan write-offs.
We find that 45% of all companies are directly exposed to carbon pricing. At a
price of 45 EUR/t, direct economic losses of 1.3% of total sales and bank
equity losses of 1.2% are expected. Secondary default cascades in supply chain
networks could increase these losses by 300% to 4000%, depending on firms'
ability to substitute essential inputs. To reduce transition risks, firms
should reduce their dependence on essential inputs from supply chains with high
CO2 exposure. We discuss the implications of different policy implementations
on these transition risks.; 59) Coherently enhanced radiation friction in laser-plasma collisions; We reconsider the footprints of radiation friction in a head on collision of
a bunch of relativistic charged particles with a laser pulse by demonstrating
that in a dense enough bunch forward and backward radiation and radiation
friction are coherently enhanced. This should make it possible to observe
radiation friction effects in laser-matter interactions at much lower energies
and laser intensities than accepted ever previously. A simple estimate for the
energy losses of the particles in the bunch over the collision due to radiation
friction in terms of laser and bunch parameters is derived and validated by
comparing with the results of three dimensional particle-in-cell simulations.; 60) Techno-Feudalism and the Rise of AGI: A Future Without Economic Rights?; The rise of Artificial General Intelligence (AGI) marks an existential
rupture in economic and political order, dissolving the historic boundaries
between labor and capital. Unlike past technological advancements, AGI is both
a worker and an owner, producing economic value while concentrating power in
those who control its infrastructure. Left unchecked, this shift risks
exacerbating inequality, eroding democratic agency, and entrenching
techno-feudalism. The classical Social Contract-rooted in human labor as the
foundation of economic participation-must be renegotiated to prevent mass
disenfranchisement. This paper calls for a redefined economic framework that
ensures AGI-driven prosperity is equitably distributed through mechanisms such
as universal AI dividends, progressive taxation, and decentralized governance.
The time for intervention is now-before intelligence itself becomes the most
exclusive form of capital.; 61) Assessing Quantum Layout Synthesis Tools via Known Optimal-SWAP Cost
  Benchmarks; Quantum layout synthesis (QLS) is a critical step in quantum program
compilation for superconducting quantum computers, involving the insertion of
SWAP gates to satisfy hardware connectivity constraints. While previous works
have introduced SWAP-free benchmarks with known-optimal depths for evaluating
QLS tools, these benchmarks overlook SWAP count - a key performance metric.
Real-world applications often require SWAP gates, making SWAP-free benchmarks
insufficient for fully assessing QLS tool performance. To address this
limitation, we introduce QUBIKOS, a benchmark set with provable-optimal SWAP
counts and non-trivial circuit structures. For the first time, we are able to
quantify the optimality gaps of SWAP gate usages of the leading QLS algorithms,
which are surprisingly large: LightSabre from IBM delivers the best performance
with an optimality gap of 63x, followed by ML-QLS with an optimality gap of
117x. Similarly, QMAP and t|ket> exhibit significantly larger gaps of 250x and
330x, respectively. This highlights the need for further advancements in QLS
methodologies. Beyond evaluation, QUBIKOS offers valuable insights for guiding
the development of future QLS tools, as demonstrated through an analysis of a
suboptimal case in LightSABRE. This underscores QUBIKOS's utility as both an
evaluation framework and a tool for advancing QLS research.; 62) UnCommon Objects in 3D; We introduce Uncommon Objects in 3D (uCO3D), a new object-centric dataset for
3D deep learning and 3D generative AI. uCO3D is the largest publicly-available
collection of high-resolution videos of objects with 3D annotations that
ensures full-360$^{\circ}$ coverage. uCO3D is significantly more diverse than
MVImgNet and CO3Dv2, covering more than 1,000 object categories. It is also of
higher quality, due to extensive quality checks of both the collected videos
and the 3D annotations. Similar to analogous datasets, uCO3D contains
annotations for 3D camera poses, depth maps and sparse point clouds. In
addition, each object is equipped with a caption and a 3D Gaussian Splat
reconstruction. We train several large 3D models on MVImgNet, CO3Dv2, and uCO3D
and obtain superior results using the latter, showing that uCO3D is better for
learning applications.; 63) Graphs that predict exciton delocalization; The field of molecular excitons and related supramolecular systems has
largely focused on aggregates where nearest-neighbour couplings dominate. We
propose that radically different states can be produced by moving beyond that
paradigm. In practice, how to accomplish this task remains an open challenge
because it requires development of ways to couple networks molecules more
densely. In the present work we motivate why it would be worthwhile. We
describe a merger of work developed in the field of discrete mathematics with
concepts and needs for the field of molecular excitons. We discuss the reasons
for exciton localization and posit how systems where the spectrum contains a
gap can be robust to disorder, and thus maintain coherence, or delocalization.
We propose that certain kinds of structures (expander graphs) specifying how
molecules are coupled to each other, show such a gap and thus resilience to
decoherence. We review relevant background known from graph theory. This
perspective suggests a fascinating scope of new properties possible by
demonstrating expander graph inspired excitonics.; 64) Multi-View Orthogonal Projection Regression with Application in
  Multi-omics integration; Multi-omics integration offers novel insights into complex biological
mechanisms by utlizing the fused information from various omics datasets.
However, the inherent within- and inter-modality correlations in multi-omics
data present significant challenges for traditional variable selection methods,
such as Lasso regression. These correlations can lead to multicollinearity,
compromising the stability and interpretability of selected variables. To
address these problems, we introduce the Multi-View Orthogonal Projection
Regression (MVOPR), a novel approach for variable selection in multi-omics
analysis. MVOPR leverages the unidirectional associations among omics layers,
inspired by the Central Dogma of Molecular Biology, to transform predictors
into an uncorrelated feature space. This orthogonal projection framework
effectively mitigates the correlations, allowing penalized regression models to
operate on independent components. Through simulations under both
well-specified and misspecified scenarios, MVOPR demonstrates superior
performance in variable selection, outperforming traditional Lasso-based
methods and factor-based models. In real-data analysis on the CAARS dataset,
MVOPR consistently identifies biologically relevant features, including the
Bacteroidaceae family and key metabolites which align well with known asthma
biomarkers. These findings illustrate MVOPR's ability to enhance variable
selection while offering biologically interpretable insights, offering a robust
tool for integrative multi-omics research.; 65) Matching Cut and Variants on Bipartite Graphs of Bounded Radius and
  Diameter; In the Matching Cut problem we ask whether a graph $G$ has a matching cut,
that is, a matching which is also an edge cut of $G$. We consider the variants
Perfect Matching Cut and Disconnected Perfect Matching where we ask whether
there exists a matching cut equal to, respectively contained in, a perfect
matching. Further, in the problem Maximum Matching Cut we ask for a matching
cut with a maximum number of edges. The last problem we consider is $d$-Cut
where we ask for an edge cut where each vertex is incident to at most $d$ edges
in the cut.
  We investigate the computational complexity of these problems on bipartite
graphs of bounded radius and diameter. Our results extend known results for
Matching Cut and Disconnected Perfect Matching. We give complexity dichotomies
for $d$-Cut and Maximum Matching Cut and solve one of two open cases for
Disconnected Perfect Matching. For Perfect Matching Cut we give the first
hardness result for bipartite graphs of bounded radius and diameter and extend
the known polynomial cases.; 66) Accurately Estimating Unreported Infections using Information Theory; One of the most significant challenges in combating against the spread of
infectious diseases was the difficulty in estimating the true magnitude of
infections. Unreported infections could drive up disease spread, making it very
hard to accurately estimate the infectivity of the pathogen, therewith
hampering our ability to react effectively. Despite the use of
surveillance-based methods such as serological studies, identifying the true
magnitude is still challenging. This paper proposes an information theoretic
approach for accurately estimating the number of total infections. Our approach
is built on top of Ordinary Differential Equations (ODE) based models, which
are commonly used in epidemiology and for estimating such infections. We show
how we can help such models to better compute the number of total infections
and identify the parametrization by which we need the fewest bits to describe
the observed dynamics of reported infections. Our experiments on COVID-19
spread show that our approach leads to not only substantially better estimates
of the number of total infections but also better forecasts of infections than
standard model calibration based methods. We additionally show how our learned
parametrization helps in modeling more accurate what-if scenarios with
non-pharmaceutical interventions. Our approach provides a general method for
improving epidemic modeling which is applicable broadly.; 67) Polarization-Resolved Core Exciton Dynamics in LiF Using Attosecond
  Transient Absorption Spectroscopy; The ability to control absorption by modifying the polarization of light
presents an exciting opportunity to experimentally determine the orbital
alignment of absorption features. Here, attosecond extreme ultraviolet (XUV)
transient absorption spectroscopy is used to investigate the polarization
dependence of core exciton dynamics in LiF thin films at the Li+ K edge. XUV
pulses excite electrons from the Li 1s core level into the conduction band,
allowing for the formation of a p-orbital-like core exciton, aligned along the
XUV light polarization axis. A sub-5 fs near-infrared (NIR) probe pulse then
arrives at variable time delays, perturbing the XUV-excited states and allowing
the coherence decay of the core exciton to be mapped. The coherence lifetimes
are found to be ~2.4 +- 0.4 fs, which is attributed to a phonon-mediated
dephasing mechanism as in previous core exciton studies. The differential
absorption features are also shown to be sensitive to the relative polarization
of the XUV and NIR fields. The parallel NIR probe induces couplings between the
initial XUV-excited p-like bright exciton and s-like dark excitons. When
crossed pump and probe polarizations are used, the coupling between the bright
and dark states is no longer dipole-allowed, and the transient absorption
signal associated with the coupling is suppressed by approximately 90%. This
interpretation is supported by simulations of a few-level model system, as well
as analysis of the calculated band structure. The results indicate that laser
polarization can serve as a powerful experimental tool for exploring the
orbital alignment of core excitonic states in solid-state materials.; 68) ProcTex: Consistent and Interactive Text-to-texture Synthesis for
  Procedural Models; Recent advancement in 2D image diffusion models has driven significant
progress in text-guided texture synthesis, enabling realistic, high-quality
texture generation from arbitrary text prompts. However, current methods
usually focus on synthesizing texture for single static 3D objects, and
struggle to handle entire families of shapes, such as those produced by
procedural programs. Applying existing methods naively to each procedural shape
is too slow to support exploring different parameter settings at interactive
rates, and also results in inconsistent textures across the procedural shapes.
To this end, we introduce ProcTex, the first text-to-texture system designed
for procedural 3D models. ProcTex enables consistent and real-time text-guided
texture synthesis for families of shapes, which integrates seamlessly with the
interactive design flow of procedural models. To ensure consistency, our core
approach is to generate texture for the shape produced by one setting of the
procedural parameters, followed by a texture transfer stage to apply the
texture to other parameter settings. We also develop several techniques,
including a novel UV displacement network for real-time texture transfer, the
retexturing pipeline to support structural changes from discrete procedural
parameters, and part-level UV texture map generation for local appearance
editing. Extensive experiments on a diverse set of professional procedural
models validate ProcTex's ability to produce high-quality, visually consistent
textures while supporting real-time, interactive applications.; 69) A stochastic force model for a finite-size spherical particle in
  turbulence; Predicting particle-laden flows requires accurate fluid force models.
However, a reliable particle force model for finite-size particles in turbulent
flows remains lacking. In the present work, a fluid force model for a
finite-size spherical particle in turbulence is developed by simulating
turbulent flow past a fixed spherical particle using particle-resolved direct
numerical simulation (PRDNS). Our simulation demonstrates that turbulence
increases the mean drag force of the particle, which is consistent with
previous studies. By correlating the DNS data as functions of the Reynolds
number of particles, the ratio of the particle-to-turbulence scale, and the
intensity ratio of the turbulence, an empirical correlation for the mean drag
force is obtained. Furthermore, we find that the fluctuations of both the drag
and lateral forces follow the Gaussian distribution. Consequently, the temporal
variations of the fluctuating drag and lateral forces are modeled using a
stochastic Langevin equation. Empirical correlations of the fluctuation
intensities and time scales involved in the stochastic model are also
determined from the DNS data. Finally, we simulate the movement of a
finite-size particle in turbulence and the dispersion of particles in a
turbulent channel flow to validate the proposed model. The proposed fluid force
model requires the mean flow velocity, the kinetic energy of the turbulence,
and the dissipation rate of the turbulence as inputs, which makes it well
suited for combination with the Reynolds-averaged Navier-Stokes (RANS)
approach.; 70) The Introduction of README and CONTRIBUTING Files in Open Source
  Software Development; README and CONTRIBUTING files can serve as the first point of contact for
potential contributors to free/libre and open source software (FLOSS) projects.
Prominent open source software organizations such as Mozilla, GitHub, and the
Linux Foundation advocate that projects provide community-focused and
process-oriented documentation early to foster recruitment and activity. In
this paper we investigate the introduction of these documents in FLOSS
projects, including whether early documentation conforms to these
recommendations or explains subsequent activity. We use a novel dataset of
FLOSS projects packaged by the Debian GNU/Linux distribution and conduct a
quantitative analysis to examine README (n=4226) and CONTRIBUTING (n=714) files
when they are first published into projects' repositories. We find that
projects create minimal READMEs proactively, but often publish CONTRIBUTING
files following an influx of contributions. The initial versions of these files
rarely focus on community development, instead containing descriptions of
project procedure for library usage or code contribution. The findings suggest
that FLOSS projects do not create documentation with community-building in
mind, but rather favor brevity and standardized instructions.; 71) Accounting for motion of supernova host galaxy in statistical inference
  from SNIa data; We investigate the impact of peculiar motion of Type Ia supernova host
galaxies on cosmological parameter estimation. This motion causes their
redshift to deviate from that of the comoving observer at their position and is
a source of noise. To this end, we develop an estimator for parameter
estimation in models with errors in independent variables. Using the Bayesian
framework, errors in independent variables are treated as nuisance parameters
by making the independent variables parameters of the model. Our method applied
to the Pantheon sample of Type Ia supernova indicates a few percent shift in
the central values of inferred cosmological parameters. For the $w$CDM model,
we find that accounting for peculiar velocities makes the data marginally more
consistent with the cosmological constant model. By using simulated data, we
show that not accounting for peculiar velocities will significantly impact
parameter estimation from higher precision future data sets.; 72) Stochastic Equilibrium Raman Spectroscopy (STERS); We theoretically propose a new method in cavity- and surface-enhanced Raman
spectroscopy (SERS) with improved temporal resolution in the measurement of
stochastic Raman spectral fluctuations. Our approach combines Fourier
spectroscopy and photon correlation to decouple the integration time from the
temporal resolution. Using statistical optics simulations, we establish the
relationship between time resolution and Raman signal strength, revealing that
typical Raman spectral fluctuations, commensurate with molecular conformational
dynamics, can theoretically be resolved on micro- to millisecond timescales.
The method can further extract average single-molecule dynamics from small
sub-ensembles, thereby potentially mitigating challenges in achieving strictly
single-molecule isolation on SERS substrates.; 73) Equidistant versus bipartite ground states for 1D classical fluids at
  fixed particle density; We study the ground-state properties of one-dimensional fluids of classical
(i.e., non-quantum) particles interacting pairwisely via a potential, at the
fixed particle density $\rho$. Restricting ourselves to periodic configurations
of particles, two possibilities are considered: an equidistant chain of
particles with the uniform spacing $A=1/\rho$ and its simplest non-Bravais
modulation, namely a bipartite lattice composed of two equidistant chains,
shifted with respect to one another. Assuming the long range of the interaction
potential, the equidistant chain dominates if $A$ is small enough, $0<A<A_c$.
At a critical value of $A=A_c$, the system undergoes a continuous second-order
phase transition from the equidistant chain to a bipartite lattice. The energy
and the order parameter are singular functions of the deviation from the
critical point $A-A_c$ with universal (i.e., independent of the model's
parameters) mean-field values of critical exponents. The tricritical point at
which the curve of continuous second-order transitions meets with the one of
discontinuous first-order transitions is determined. The general theory is
applied to the Lennard-Jones model with the $(n,m)$ Mie potential for which the
phase diagram is constructed. The inclusion of a hard core around each particle
reveals a non-universal critical phenomenon with an $m$-dependent critical
exponent.; 74) Enhancing the Scalability and Applicability of Kohn-Sham Hamiltonians
  for Molecular Systems; Density Functional Theory (DFT) is a pivotal method within quantum chemistry
and materials science, with its core involving the construction and solution of
the Kohn-Sham Hamiltonian. Despite its importance, the application of DFT is
frequently limited by the substantial computational resources required to
construct the Kohn-Sham Hamiltonian. In response to these limitations, current
research has employed deep-learning models to efficiently predict molecular and
solid Hamiltonians, with roto-translational symmetries encoded in their neural
networks. However, the scalability of prior models may be problematic when
applied to large molecules, resulting in non-physical predictions of
ground-state properties. In this study, we generate a substantially larger
training set (PubChemQH) than used previously and use it to create a scalable
model for DFT calculations with physical accuracy. For our model, we introduce
a loss function derived from physical principles, which we call Wavefunction
Alignment Loss (WALoss). WALoss involves performing a basis change on the
predicted Hamiltonian to align it with the observed one; thus, the resulting
differences can serve as a surrogate for orbital energy differences, allowing
models to make better predictions for molecular orbitals and total energies
than previously possible. WALoss also substantially accelerates
self-consistent-field (SCF) DFT calculations. Here, we show it achieves a
reduction in total energy prediction error by a factor of 1347 and an SCF
calculation speed-up by a factor of 18%. These substantial improvements set new
benchmarks for achieving accurate and applicable predictions in larger
molecular systems.; 75) Neutrino energy and momentum emission from magnetized dense quark matter; Using first-principles field-theoretic methods, we investigate neutrino
emission from strongly magnetized dense quark matter under conditions relevant
to compact stars. We develop a customized approximation that fully accounts for
the Landau-level quantization of electron states while neglecting such
quantization for quarks. This approach is well-justified in dense quark matter,
where the chemical potentials of up and down quarks significantly exceed those
of electrons. Our analysis provides a detailed exploration of the influence of
strong magnetic fields on neutrino emission, including both the modification of
the total emission rate and the emergence of emission asymmetry relative to the
magnetic field direction. We further examine the role of temperature in
smoothing the oscillatory behavior of neutrino emission as a function of
magnetic field strength. Additionally, we study the interplay between the
Landau-level quantization of electrons and the Fermi-liquid effects of quarks
in modifying the phase space of relevant weak processes. Finally, we briefly
discuss the broader implications of magnetic fields on stellar cooling
processes and the potential contribution of asymmetric neutrino emission to
pulsar kicks.; 76) A Liouville-type theorem for the p-Laplacian on complete non-compact
  Riemannian manifolds; A Liouville-type result for the p-Laplacian on complete Riemannian manifolds
is proved. As an application are present some results concerning complete
non-compact hypersurfaces immersed in a suitable warped product manifold.; 77) Form factors in semileptonic decay of D mesons; We study the vector, scalar and tensor form factors for the semileptonic
process $D\rightarrow K$ by using lattice Quantum Chromodynamcs (QCD). Chiral
lattice fermions are used in our study: overlap fermion for the valence quark
and domain-wall fermion for the sea. The 2+1-flavor configurations are from the
RBC-UKQCD Collaborations with an inverse lattice spacing $1/a=2.383(9)$ GeV. A
modified $z$-expansion taking into account valence quark mass dependence is
used to fit our numerical results of the form factors at a fixed pion mass
$\sim360$ MeV in the sea. At the physical valence quark mass point we give the
preliminary results $f_+(0)=f_0(0)=0.760(39)$ and $f_T(0)=0.733(50)$ with only
statistical uncertainties. For $f_T$ the number is given in the $\overline{\rm
MS}$ scheme at a scale of 2 GeV.; 78) Bypassing the static input size of neural networks in flare forecasting
  by using spatial pyramid pooling; The spatial extension of active regions (ARs) of the Sun can vary from one
case to the next. This is a problem when studying solar flares with
Convolutional Neural Networks (CNNs) as they generally use input images of a
fixed size. Different processes can be performed to retrieve a database with
homogeneous-sized data, such as resizing. Unfortunately, key features can be
lost or distorted during these processes. This can lead to a deterioration of
the ability of CNNs to classify flares of different soft X-ray classes,
especially those from ARs with complex structures. Our work aims to implement
and test a CNN architecture that retains the full features of the original
resolution of the input images. We compare the performance of two CNN
architectures for solar flare prediction: the first is a traditional CNN with
resized input whereas the other implements a spatial pyramid pooling (SPP)
layer without any input resizing. Both are trained on the Spaceweather HMI
Active Region Patch line-of-sight magnetogram database. We also study two cases
of binary classification. In the first case, our model distinguishes ARs
producing flares in less than 24h of class greater or equal to C1.0 from ARs
producing flares in more than 24h or never; in the second case, it
distinguishes ARs producing flares in less than 24h of class greater or equal
to M1.0 from the other ARs. Our models implementing an SPP layer outperform the
traditional CNN models when predicting flares greater or equal to C1.0 within
24h. However, their performances degrade sharply along the other models studied
in this paper, when trained to classify images greater or equal to M1.0 flares.
The degradation in SPP models when classifying only images greater or equal to
M1.0 flares as positive may be attributed to its success in identifying
features that appear in ARs a few hours before the flare, independently of
their soft X-ray class.; 79) MLMC: Interactive multi-label multi-classifier evaluation without
  confusion matrices; Machine learning-based classifiers are commonly evaluated by metrics like
accuracy, but deeper analysis is required to understand their strengths and
weaknesses. MLMC is a visual exploration tool that tackles the challenge of
multi-label classifier comparison and evaluation. It offers a scalable
alternative to confusion matrices which are commonly used for such tasks, but
don't scale well with a large number of classes or labels. Additionally, MLMC
allows users to view classifier performance from an instance perspective, a
label perspective, and a classifier perspective. Our user study shows that the
techniques implemented by MLMC allow for a powerful multi-label classifier
evaluation while preserving user friendliness.; 80) The role of FDI along transitional dynamics of the host country in an
  endogenous growth model; We investigate the role of foreign direct investment (FDI) in the
transitional dynamics of host countries by using an optimal growth model. FDI
may be beneficial for the host country because local people can work for
multinational firms to get a favorable salary. However, if the host country
only focuses on FDI, it may face a middle-income trap. We show that if the host
country invests in research and development, its economy may have sustained
growth. Moreover, in this case, FDI helps the host country only at the first
stages of its development process.; 81) Interpretable Early Warnings using Machine Learning in an Online
  Game-experiment; Stemming from physics and later applied to other fields such as ecology, the
theory of critical transitions suggests that some regime shifts are preceded by
statistical early warning signals. Reddit's r/place experiment, a large-scale
social game, provides a unique opportunity to test these signals consistently
across thousands of subsystems undergoing critical transitions. In r/place,
millions of users collaboratively created compositions, or pixel-art drawings,
in which transitions occur when one composition rapidly replaces another. We
develop a machine-learning-based early warning system that combines the
predictive power of multiple system-specific time series via gradient-boosted
decision trees with memory-retaining features. Our method significantly
outperforms standard early warning indicators. Trained on the 2022 r/place
data, our algorithm detects half of the transitions occurring within 20 minutes
at a false positive rate of just 3.7%. Its performance remains robust when
tested on the 2023 r/place event, demonstrating generalizability across
different contexts. Using SHapley Additive exPlanations (SHAP) for interpreting
the predictions, we investigate the underlying drivers of warnings, which could
be relevant to other complex systems, especially online social systems. We
reveal an interplay of patterns preceding transitions, such as critical slowing
down or speeding up, a lack of innovation or coordination, turbulent histories,
and a lack of image complexity. These findings show the potential of machine
learning indicators in socio-ecological systems for predicting regime shifts
and understanding their dynamics.; 82) Calibrated Unsupervised Anomaly Detection in Multivariate Time-series
  using Reinforcement Learning; This paper investigates unsupervised anomaly detection in multivariate
time-series data using reinforcement learning (RL) in the latent space of an
autoencoder. A significant challenge is the limited availability of anomalous
data, often leading to misclassifying anomalies as normal events, thus raising
false negatives. RL can help overcome this limitation by promoting exploration
and balancing exploitation during training, effectively preventing overfitting.
Wavelet analysis is also utilized to enhance anomaly detection, enabling
time-series data decomposition into both time and frequency domains. This
approach captures anomalies at multiple resolutions, with wavelet coefficients
extracted to detect both sudden and subtle shifts in the data, thereby refining
the anomaly detection process. We calibrate the decision boundary by generating
synthetic anomalies and embedding a supervised framework within the model. This
supervised element aids the unsupervised learning process by fine-tuning the
decision boundary and increasing the model's capacity to distinguish between
normal and anomalous patterns effectively.; 83) Preventing Household Bankruptcy: The One-Third Rule in Financial
  Planning with Mathematical Validation and Game-Theoretic Insights; This paper analyzes the 1/3 Financial Rule, a method of allocating income
equally among debt repayment, savings, and living expenses. Through
mathematical modeling, game theory, behavioral finance, and technological
analysis, we examine the rule's potential for supporting household financial
stability and reducing bankruptcy risk. The research develops theoretical
foundations using utility maximization theory, demonstrating how equal
allocation emerges as a solution under standard economic assumptions. The
game-theoretic analysis explores the rule's effectiveness across different
household structures, revealing potential strategic advantages in financial
decision-making. We investigate psychological factors influencing financial
choices, including cognitive biases and neurobiological mechanisms that impact
economic behavior. Technological approaches, such as AI-driven personalization,
blockchain tracking, and smart contract applications, are examined for their
potential to support financial planning. Empirical validation using U.S. Census
data and longitudinal studies assesses the rule's performance across various
household types. Stress testing under different economic conditions provides
insights into its adaptability and resilience. The research integrates
mathematical analysis with behavioral insights and technological perspectives
to develop a comprehensive approach to household financial management.; 84) Matrix weighted inequalities for fractional type integrals associated to
  operators with new classes of weights; Let $e^{-tL}$ be a analytic semigroup generated by $-L$, where $L$ is a
non-negative self-adjoint operator on $L^2(\mathbb{R}^d)$. Assume that the
kernels of $e^{-tL}$, denoted by $p_t(x,y)$, only satisfy the upper bound: for
all $N>0$, there are constants $c,C>0$ such that \begin{align}\label{upper
bound}
|p_t(x,y)|\leq\frac{C}{t^{d/2}}e^{-\frac{|x-y|^2}{ct}}\Big(1+\frac{\sqrt{t}}{\rho(x)}+
\frac{\sqrt{t}}{\rho(y)}\Big)^{-N} \end{align} holds for all
$x,y\in\mathbb{R}^d$ and $t>0$. We first establish the quantitative matrix
weighted inequalities for fractional type integrals associated to $L$ with new
classes of matrix weights, which are nontrivial extension of the results
established by Li, Rahm and Wick [23]. Next, we give new two-weight bump
conditions with Young functions satisfying wider conditions for fractional type
integrals associated to $L$, which cover the result obtained by Cruz-Uribe,
Isralowitz and Moen [6]. We point out that the new classes of matrix weights
and bump conditions are larger and weaker than the classical ones given in [17]
and [6], respectively. As applications, our results can be applied to settings
of magnetic Schr\""{o}dinger operator, Laguerre operators, etc.; 85) A fast Fourier spectral method for wave kinetic equation; The central object in wave turbulence theory is the wave kinetic equation
(WKE), which is an evolution equation for wave action density and acts as the
wave analog of the Boltzmann kinetic equations for particle interactions.
Despite recent exciting progress in the theoretical aspects of the WKE,
numerical developments have lagged behind. In this paper, we introduce a fast
Fourier spectral method for solving the WKE. The key idea lies in reformulating
the high-dimensional nonlinear wave kinetic operator as a spherical integral,
analogous to classical Boltzmann collision operator. The conservation of mass
and momentum leads to a double convolution structure in Fourier space, which
can be efficiently handled using the fast Fourier transform. We demonstrate the
accuracy and efficiency of the proposed method through several numerical tests
in both 2D and 3D, revealing some interesting and unique features of this
equation.; 86) InSlicing: Interpretable Learning-Assisted Network Slice Configuration
  in Open Radio Access Networks; Network slicing is a key technology enabling the flexibility and efficiency
of 5G networks, offering customized services for diverse applications. However,
existing methods face challenges in adapting to dynamic network environments
and lack interpretability in performance models. In this paper, we propose a
novel interpretable network slice configuration algorithm (\emph{InSlicing}) in
open radio access networks, by integrating Kolmogorov-Arnold Networks (KANs)
and hybrid optimization process. On the one hand, we use KANs to approximate
and learn the unknown performance function of individual slices, which converts
the blackbox optimization problem. On the other hand, we solve the converted
problem with a genetic method for global search and incorporate a trust region
for gradient-based local refinement. With the extensive evaluation, we show
that our proposed algorithm achieves high interpretability while reducing 25+\%
operation cost than existing solutions.; 87) OmniEraser: Remove Objects and Their Effects in Images with Paired
  Video-Frame Data; Inpainting algorithms have achieved remarkable progress in removing objects
from images, yet still face two challenges: 1) struggle to handle the object's
visual effects such as shadow and reflection; 2) easily generate shape-like
artifacts and unintended content. In this paper, we propose Video4Removal, a
large-scale dataset comprising over 100,000 high-quality samples with realistic
object shadows and reflections. By constructing object-background pairs from
video frames with off-the-shelf vision models, the labor costs of data
acquisition can be significantly reduced. To avoid generating shape-like
artifacts and unintended content, we propose Object-Background Guidance, an
elaborated paradigm that takes both the foreground object and background
images. It can guide the diffusion process to harness richer contextual
information. Based on the above two designs, we present OmniEraser, a novel
method that seamlessly removes objects and their visual effects using only
object masks as input. Extensive experiments show that OmniEraser significantly
outperforms previous methods, particularly in complex in-the-wild scenes. And
it also exhibits a strong generalization ability in anime-style images.
Datasets, models, and codes will be published.; 88) A Kernel Ridge Regression Combining Nonlinear ROMs for Accurate Flow
  Field Reconstruction with Discontinuities; Nonlinear reduced-order models (ROMs), represented by manifold learning (ML),
can effectively improve the modeling accuracy of nonlinear flow fields with
discontinuities. However, the inverse mapping from low-dimensional manifold
coordinates to high-dimensional flow fields often introduces considerable
reconstruction errors, leading to inaccuracy in the locations of
discontinuities. To address this challenge, a novel reconstruction method is
proposed to enhance the accuracy of reconstructing flow fields with
discontinuities. The method employs kernel ridge regression (KRR) to construct
a set of nonlinear modes rich in discontinuity information, sequentially these
modes are nonlinearly combined with manifold coordinates to achieve accurate
flow field reconstruction. The proposed reconstruction method is validated to
reconstruct the transonic flow fields over RAE2822 airfoil. Comparison results
demonstrate that the method achieves superior reconstruction accuracy compared
to existing approaches, especially in reconstructing flow fields' discontinuous
regions and precisely capturing discontinuities. This work provides an
effective and highly interpretable solution for improving the accuracy of
nonlinear ROMs in discontinuous flow fields modeling.; 89) Experience-replay Innovative Dynamics; Despite its groundbreaking success, multi-agent reinforcement learning (MARL)
still suffers from instability and nonstationarity. Replicator dynamics, the
most well-known model from evolutionary game theory (EGT), provide a
theoretical framework for the convergence of the trajectories to Nash
equilibria and, as a result, have been used to ensure formal guarantees for
MARL algorithms in stable game settings. However, they exhibit the opposite
behavior in other settings, which poses the problem of finding alternatives to
ensure convergence. In contrast, innovative dynamics, such as the Brown-von
Neumann-Nash (BNN) or Smith, result in periodic trajectories with the potential
to approximate Nash equilibria. Yet, no MARL algorithms based on these dynamics
have been proposed. In response to this challenge, we develop a novel
experience replay-based MARL algorithm that incorporates revision protocols as
tunable hyperparameters. We demonstrate, by appropriately adjusting the
revision protocols, that the behavior of our algorithm mirrors the trajectories
resulting from these dynamics. Importantly, our contribution provides a
framework capable of extending the theoretical guarantees of MARL algorithms
beyond replicator dynamics. Finally, we corroborate our theoretical findings
with empirical results.; 90) Deuterated Polycyclic Aromatic Hydrocarbons in the Interstellar Medium:
  Constraints from the Orion Bar as Observed by the James Webb Space Telescope; The gas-phase abundances of deuterium (D) in the local interstellar medium
(ISM) exhibit considerable regional variations. Particularly, in some regions
the gas-phase D abundances are substantially lower than the primordial D
abundance generated in the Big Bang, after subtracting the astration reduction
caused by the Galactic chemical evolution. Deuterated polycyclic aromatic
hydrocarbon (PAH) molecules have been suggested as a potential reservoir of the
D atoms missing from the gas-phase. Recent observations from the James Webb
Space Telescope's Near Infrared Spectrograph have revealed the widespread of
deuterated PAHs in the Orion Bar through their aliphatic C--D emission at
4.65${\,{\rm \mu m}}$ and possibly aromatic C--D emission at 4.4${\,{\rm \mu
m}}$ as well. To examine the viability of deuterated PAHs as the D reservoir,
we model the infrared (IR) emission spectra of small PAH molecules containing
various aromatic and aliphatic D atoms in the Orion Bar. We find that small
deuterated PAHs exhibit a noticeable emission band at 4.4 or 4.65${\,{\rm \mu
m}}$ even if they contain only one aromatic or aliphatic D atom. We derive
${{N_{\rm D,ali}}}/{N_{\rm H}}\approx3.4\%$, the deuteration degree of PAHs
measured as the number of aliphatic D atoms (relative to H), from the observed
intensity ratios of the 4.65${\,{\rm \mu m}}$ band to the 3.3${\,{\rm \mu m}}$
aromatic C--H band. The deuteration degree for aromatically-deuterated PAHs is
less certain as C--N stretch also contributes to the observed emission around
4.4${\,{\rm \mu m}}$. If we attribute it exclusively to aromatic C--D, we
derive an upper limit of $\approx14\%$ on the deuteration degree, which is
capable of accounting for an appreciable fraction of the missing D budget.; 91) Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts; Linear Sequence Modeling (LSM) like linear attention, state space models and
linear RNNs, and Mixture-of-Experts (MoE) have recently emerged as significant
architectural improvements. In this paper, we introduce Linear-MoE, a
production-level system for modeling and training large-scale models that
integrate LSM with MoE. Linear-MoE leverages the advantages of both LSM modules
for linear-complexity sequence modeling and MoE layers for sparsely activation,
aiming to offer high performance with efficient training. The Linear-MoE system
comprises: 1) Modeling subsystem, which provides a unified framework supporting
all instances of LSM. and 2) Training subsystem, which facilitates efficient
training by incorporating various advanced parallelism technologies,
particularly Sequence Parallelism designed for Linear-MoE models. Additionally,
we explore hybrid models that combine Linear-MoE layers with standard
Transformer-MoE layers with its Sequence Parallelism to further enhance model
flexibility and performance. Evaluations on two model series, A0.3B-2B and
A1B-7B, demonstrate Linear-MoE achieves efficiency gains while maintaining
competitive performance on various benchmarks, showcasing its potential as a
next-generation foundational model architecture. Code:
https://github.com/OpenSparseLLMs/Linear-MoE.; 92) Demystifying Long Chain-of-Thought Reasoning in LLMs; Scaling inference compute enhances reasoning in large language models (LLMs),
with long chains-of-thought (CoTs) enabling strategies like backtracking and
error correction. Reinforcement learning (RL) has emerged as a crucial method
for developing these capabilities, yet the conditions under which long CoTs
emerge remain unclear, and RL training requires careful design choices. In this
study, we systematically investigate the mechanics of long CoT reasoning,
identifying the key factors that enable models to generate long CoT
trajectories. Through extensive supervised fine-tuning (SFT) and RL
experiments, we present four main findings: (1) While SFT is not strictly
necessary, it simplifies training and improves efficiency; (2) Reasoning
capabilities tend to emerge with increased training compute, but their
development is not guaranteed, making reward shaping crucial for stabilizing
CoT length growth; (3) Scaling verifiable reward signals is critical for RL. We
find that leveraging noisy, web-extracted solutions with filtering mechanisms
shows strong potential, particularly for out-of-distribution (OOD) tasks such
as STEM reasoning; and (4) Core abilities like error correction are inherently
present in base models, but incentivizing these skills effectively for complex
tasks via RL demands significant compute, and measuring their emergence
requires a nuanced approach. These insights provide practical guidance for
optimizing training strategies to enhance long CoT reasoning in LLMs. Our code
is available at: https://github.com/eddycmu/demystify-long-cot.; 93) Propagation-induced Frequency-dependent Polarization Properties of Fast
  Radio Burst; Frequency-dependent polarization properties provide crucial insights into the
radiation mechanisms and magnetic environments of fast radio bursts (FRBs). We
explore an analytical solution of radiative transfer of the polarization
properties of FRBs as a strong incoming wave propagates in a homogeneous
magnetized plasma. The case of a thermal plasma is studied in more detail. The
rotational axis of the polarization spectrum undergoes precession with
frequency on the Poincar\'e sphere when the medium has both strong Faraday
rotation and conversion. Such precession on the Poincar\'e sphere could occur
in hot or cold plasma with a strong magnetic field component perpendicular to
the line of sight. The analytical solution with the mixing Faraday case offers
a more physical description of the physical properties of the magnetic
environment of FRBs than the empirical ``generalized Faraday rotation'' method
commonly adopted in the literature. Significant absorption can exist in a dense
plasma medium, which may give rise to a highly circularly polarized outgoing
wave. The frequency-dependent Stokes parameters may be associated with
reversing rotation measures or the presence of a persistent radio source around
an FRB.; 94) WFCRL: A Multi-Agent Reinforcement Learning Benchmark for Wind Farm
  Control; The wind farm control problem is challenging, since conventional model-based
control strategies require tractable models of complex aerodynamical
interactions between the turbines and suffer from the curse of dimension when
the number of turbines increases. Recently, model-free and multi-agent
reinforcement learning approaches have been used to address this challenge. In
this article, we introduce WFCRL (Wind Farm Control with Reinforcement
Learning), the first open suite of multi-agent reinforcement learning
environments for the wind farm control problem. WFCRL frames a cooperative
Multi-Agent Reinforcement Learning (MARL) problem: each turbine is an agent and
can learn to adjust its yaw, pitch or torque to maximize the common objective
(e.g. the total power production of the farm). WFCRL also offers turbine load
observations that will allow to optimize the farm performance while limiting
turbine structural damages. Interfaces with two state-of-the-art farm
simulators are implemented in WFCRL: a static simulator (FLORIS) and a dynamic
simulator (FAST.Farm). For each simulator, $10$ wind layouts are provided,
including $5$ real wind farms. Two state-of-the-art online MARL algorithms are
implemented to illustrate the scaling challenges. As learning online on
FAST.Farm is highly time-consuming, WFCRL offers the possibility of designing
transfer learning strategies from FLORIS to FAST.Farm.; 95) Enhancing Code LLM Training with Programmer Attention; Human attention provides valuable yet underexploited signals for code LLM
training, offering a perspective beyond purely machine-driven attention.
Despite the complexity and cost of collecting eye-tracking data, there has also
been limited progress in systematically using these signals for code LLM
training. To address both issues, we propose a cohesive pipeline spanning
augmentation and reward-based fine-tuning. Specifically, we introduce (1) an
eye-tracking path augmentation method to expand programmer attention datasets,
(2) a pattern abstraction step that refines raw fixations into learnable
attention motifs, and (3) a reward-guided strategy for integrating these
insights directly into a CodeT5 supervised fine-tuning process. Our experiments
yield +7.16 in CodeBLEU on the CodeXGlue benchmark for code summarization,
underscoring how uniting human and machine attention can boost code
intelligence. We hope this work encourages broader exploration of human-centric
methods in next-generation AI4SE.; 96) Probabilistic Segmentation for Robust Field of View Estimation; Attacks on sensing and perception threaten the safe deployment of autonomous
vehicles (AVs). Security-aware sensor fusion helps mitigate threats but
requires accurate field of view (FOV) estimation which has not been evaluated
autonomy. To address this gap, we adapt classical computer graphics algorithms
to develop the first autonomy-relevant FOV estimators and create the first
datasets with ground truth FOV labels. Unfortunately, we find that these
approaches are themselves highly vulnerable to attacks on sensing. To improve
robustness of FOV estimation against attacks, we propose a learning-based
segmentation model that captures FOV features, integrates Monte Carlo dropout
(MCD) for uncertainty quantification, and performs anomaly detection on
confidence maps. We illustrate through comprehensive evaluations attack
resistance and strong generalization across environments. Architecture trade
studies demonstrate the model is feasible for real-time deployment in multiple
applications.; 97) Direct CP violation in $\bar B_{s} \rightarrow K^{+}K^{-} K^{+}K^{-}$
  decay process induced by interferences of the intermediate vector particles; We investigate CP violation in the decay process $\bar B_{s} \rightarrow V V
\rightarrow K^{+}K^{-}K^{+}K^{-}$ within the framework of perturbative QCD,
where V represents vector mesons $\phi$, $\rho$, and $\omega$. We analyze the
mixing mechanism among $\phi-\rho^{0}-\omega$ and provide amplitudes for these
decay processes. Moreover, we explore CP violation in the four-body decay
process of $\bar{B}_{s}^{0} \rightarrow K^{+} K^{-} K^{+} K^{-}$ involving
intermediate vector mesons and their mixing. Notably, significant CP violation
is observed for specific two-vector meson intermediate states. Additionally, a
substantial amount of CP violation arises from vector mixing when the invariant
mass of $K^{+}K^{-}$ falls within a certain range. This study holds potential
implications for future detection by the LHC experiment.; 98) EigenGS Representation: From Eigenspace to Gaussian Image Space; Principal Component Analysis (PCA), a classical dimensionality reduction
technique, and 2D Gaussian representation, an adaptation of 3D Gaussian
Splatting for image representation, offer distinct approaches to modeling
visual data. We present EigenGS, a novel method that bridges these paradigms
through an efficient transformation pipeline connecting eigenspace and
image-space Gaussian representations. Our approach enables instant
initialization of Gaussian parameters for new images without requiring
per-image optimization from scratch, dramatically accelerating convergence.
EigenGS introduces a frequency-aware learning mechanism that encourages
Gaussians to adapt to different scales, effectively modeling varied spatial
frequencies and preventing artifacts in high-resolution reconstruction.
Extensive experiments demonstrate that EigenGS not only achieves superior
reconstruction quality compared to direct 2D Gaussian fitting but also reduces
necessary parameter count and training time. The results highlight EigenGS's
effectiveness and generalization ability across images with varying resolutions
and diverse categories, making Gaussian-based image representation both
high-quality and viable for real-time applications.; 99) On a class of adversarial classification problems which admit a
  continuous solution; We consider a class of adversarial classification problems in the form of
zero-sum games between a classifier and an adversary. The latter is able to
corrupt data, at the expense of some optimal transport cost. We show that quite
general assumptions on the loss functions of the classifier and the transport
cost functions of the adversary ensure the existence of a Nash equilibrium with
a continuous (or even Lipschitz) classifier's strategy. We also consider a
softmax-like regularization of this problem and present numerical results for
this regularization.; 100) Heterogeneity of household stock portfolios in a national market; We study the long term dynamics of the stock portfolios owned by single
Finnish legal entities in the Helsinki venue of the Nasdaq Nordic between 2001
and 2021. Using the Herfindahl-Hirschman index as a measure of concentration
for the composition of stock portfolios, we investigate the concentration of
Finnish household portfolios both at the level of each individual household and
tracking the time evolution of an aggregated Finnish household portfolio. We
also consider aggregated portfolios of two other macro categories of investors
one comprising Finnish institutional investors and the other comprising foreign
investors. Different macro categories of investors present a different degree
of concentration of aggregated stock portfolios with highest concentration
observed for foreign investors. For individual Finnish retail investors,
portfolio concentration estimated by the Herfindahl-Hirschman index presents
high values for more than half of the total number of retail investors. In
spite of the observation that retail stock portfolios are often composed by
just a few stocks, the concentration of the aggregated stock portfolio for
Finnish retail investors has a portfolio concentration comparable with the one
of Finnish institutional investors. Within retail investors, stock portfolios
of women present a similar pattern of portfolios of men but with a systematic
higher level of concentration observed for women both at individual and at
aggregated level.",0.0,0.5912352048230277
2411.0064,applied,2411.0064-pos1-3,"The Llama 3 Herd of Models; Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",2411.0064-pos2-3,"Quantifying Variance in Evaluation Benchmarks; Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models.",26,"['1', '2', '3', '5', '4', '11', '29', '39', '40', '44']","The first candidate paper, 'Design of Bayesian Clinical Trials with Clustered Data and Multiple Endpoints', aligns well with the main paper on the Llama 3 models as it leverages advanced computational frameworks and statistical methodologies that could benefit from the multilingual and multimodal capabilities of Llama 3. This combination could lead to novel applications in clinical trials, enhancing decision-making processes through integrated language and data analysis. The second paper focuses on functional data classification, which complements the Llama 3's capabilities in reasoning and tool usage, while the third paper on survival outcomes in cancer treatment can benefit from the model's reasoning capabilities to improve patient outcomes. These combinations foster interdisciplinary advancements in health analytics and AI. The selected candidates progressively showcase increasing relevance to the Llama 3 context, with potential applications across multiple domains.","1) Design of Bayesian Clinical Trials with Clustered Data and Multiple
  Endpoints; In the design of clinical trials, it is essential to assess the design
operating characteristics (i.e., the probabilities of making correct
decisions). Common practice for the evaluation of operating characteristics in
Bayesian clinical trials relies on estimating the sampling distribution of
posterior summaries via Monte Carlo simulation. It is computationally intensive
to repeat this estimation process for each design configuration considered,
particularly for clustered data that are analyzed using complex,
high-dimensional models. In this paper, we propose an efficient method to
assess operating characteristics and determine sample sizes for Bayesian trials
with clustered data and multiple endpoints. We prove theoretical results that
enable posterior probabilities to be modelled as a function of the sample size.
Using these functions, we assess operating characteristics at a range of sample
sizes given simulations conducted at only two sample sizes. These theoretical
results are also leveraged to quantify the impact of simulation variability on
our sample size recommendations. The applicability of our methodology is
illustrated using a current clinical trial with clustered data.; 2) Empowering Multi-class Classification for Complex Functional Data with
  Simultaneous Feature Selection; The opportunity to utilize complex functional data types for conducting
classification tasks is emerging with the growing availability of imaging data.
However, the tools capable of effectively managing imaging data are limited,
let alone those that can further leverage other one-dimensional functional
data. Inspired by the extensive data provided by the Alzheimer's Disease
Neuroimaging Initiative (ADNI), we introduce a novel classifier tailored for
complex functional data. Each observation in this framework may be associated
with numerous functional processes, varying in dimensions, such as curves and
images. Each predictor is a random element in an infinite dimensional function
space, and the number of functional predictors p can potentially be much
greater than the sample size n. In this paper, we introduce a novel and
scalable classifier termed functional BIC deep neural network. By adopting a
sparse deep Rectified Linear Unit network architecture and incorporating the
LassoNet algorithm, the proposed unified model performs feature selection and
classification simultaneously, which is contrast to the existing functional
data classifiers. The challenge arises from the complex inter-correlation
structures among multiple functional processes, and at meanwhile without any
assumptions on the distribution of these processes. Simulation study and real
data application are carried out to demonstrate its favorable performance.; 3) Leveraging Two-Phase Data for Improved Prediction of Survival Outcomes
  with Application to Nasopharyngeal Cancer; Accurate survival predicting models are essential for improving targeted
cancer therapies and clinical care among cancer patients. In this article, we
investigate and develop a method to improve predictions of survival in cancer
by leveraging two-phase data with expert knowledge and prognostic index. Our
work is motivated by two-phase data in nasopharyngeal cancer (NPC), where
traditional covariates are readily available for all subjects, but the primary
viral factor, Human Papillomavirus (HPV), is substantially missing. To address
this challenge, we propose an expert guided method that incorporates prognostic
index based on the observed covariates and clinical importance of key factors.
The proposed method makes efficient use of available data, not simply
discarding patients with unknown HPV status. We apply the proposed method and
evaluate it against other existing approaches through a series of simulation
studies and real data example of NPC patients. Under various settings, the
proposed method consistently outperforms competing methods in terms of c-index,
calibration slope, and integrated Brier score. By efficiently leveraging
two-phase data, the model provides a more accurate and reliable predictive
ability of survival models.; 4) A step-by-step guide to generalized estimating equations using SPSS in
  the field of dentistry; The Generalized Estimating Equations (GEE) approach is a widely used
statistical method for analyzing longitudinal data and clustered data in clinic
studies. In dentistry, due to multiple outcomes obtained from one patient, the
outcomes produced from individual patients are correlated with one another.
This study focuses on the basic ideas of GEE and introduces the types of
covariance matrix and working correlation matrix. The quasi-likelihood
information criterion(QIC) and quasi-likelihood information criterion
approximation(QICu) were used to select the best working matrix and the best
fitting model for the correlated outcomes.; 5) Characterizing Data Visualization Literacy: a Systematic Literature
  Review; With the advent of the data era, and of new, more intelligent interfaces for
supporting decision making, there is a growing need to define, model and assess
human ability and data visualizations usability for a better encoding and
decoding of data patterns. Data Visualization Literacy (DVL) is the ability of
encoding and decoding data into and from a visual language. Although this
ability and its measurement are crucial for advancing human knowledge and
decision capacity, they have seldom been investigated, let alone
systematically. To address this gap, this paper presents a systematic
literature review comprising 43 reports on DVL, analyzed using the PRISMA
methodology. Our results include the identification of the purposes of DVL, its
satellite aspects, the models proposed, and the assessments designed to
evaluate the degree of DVL of people. Eventually, we devise many research
directions including, among the most challenging, the definition of a
(standard) unifying construct of DVL.; 6) Large Sample Inference with Dynamic Information Borrowing; Large sample behavior of dynamic information borrowing (DIB) estimators is
investigated. Asymptotic properties of several DIB approaches (adaptive risk
minimization, adaptive LASSO, Bayesian procedures with empirical power prior,
fully Bayesian procedures, and a Bayes-frequentist compromise) are explored
against shrinking to zero alternatives. As shown theoretically and with
simulations, local asymptotic distributions of DIB estimators are often
non-normal. A simple Gaussian setting with external information borrowing
illustrates that none of the considered DIB methods outperforms others in terms
of mean squared error (MSE): at different conflict values, the MSEs of DIBs are
changing between the MSEs of the maximum likelihood estimators based on the
current and pooled data. To uniquely determine an optimality criterion for DIB,
a prior distribution on the conflict needs be either implicitly or explicitly
determined using data independent considerations. Data independent assumptions
on the conflict are also needed for DIB-based hypothesis testing. New families
of DIB estimators parameterized by a sensitivity-to-conflict parameter S are
suggested and their use is illustrated in an infant mortality example. The
choice of S is determined in a data-independent manner by a cost-benefit
compromise associated with the use of external data.; 7) Score-Based Causal Discovery with Temporal Background Information; Temporal background information can improve causal discovery algorithms by
orienting edges and identifying relevant adjustment sets. We develop the
Temporal Greedy Equivalence Search (TGES) algorithm and terminology essential
for score-based causal discovery with tiered background knowledge. TGES learns
a restricted Markov equivalence class of directed acyclic graphs (DAGs) using
observational data and tiered background knowledge. To construct TGES we
formulate a scoring criterion that accounts for tiered background knowledge. We
establish theoretical results for TGES, stating that the algorithm always
returns a tiered maximally oriented partially directed acyclic graph (tiered
MPDAG) and that this tiered MPDAG contains the true DAG in the large sample
limit. We present a simulation study indicating a gain from using tiered
background knowledge and an improved precision-recall trade-off compared to the
temporal PC algorithm. We provide a real-world example on life-course health
data.; 8) Enhanced Min-Sum Decoding of Quantum Codes Using Previous Iteration
  Dynamics; In this paper, we propose a novel message-passing decoding approach that
leverages the degeneracy of quantum low-density parity-check codes to enhance
decoding performance, eliminating the need for serial scheduling or
post-processing. Our focus is on two-block Calderbank-Shor-Steane (CSS) codes,
which are composed of symmetric stabilizers that hinder the performance of
conventional iterative decoders with uniform update rules. Specifically, our
analysis shows that, under the isolation assumption, the min-sum decoder fails
to converge when constant-weight errors are applied to symmetric stabilizers,
as variable-to-check messages oscillate in every iteration. To address this, we
introduce a decoding technique that exploits this oscillatory property by
applying distinct update rules: variable nodes in one block utilize messages
from previous iterations, while those in the other block are updated
conventionally. Logical error-rate results demonstrate that the proposed
decoder significantly outperforms the normalized min-sum decoder and achieves
competitive performance with belief propagation enhanced by order-zero ordered
statistics decoding, all while maintaining linear complexity in the code's
block length.; 9) Maximum Likelihood Estimation Based Complex-Valued Robust Chinese
  Remainder Theorem and Its Fast Algorithm; Recently, a multi-channel self-reset analog-to-digital converter (ADC) system
with complex-valued moduli has been proposed. This system enables the recovery
of high dynamic range complex-valued bandlimited signals at low sampling rates
via the Chinese remainder theorem (CRT). In this paper, we investigate
complex-valued CRT (C-CRT) with erroneous remainders, where the errors follow
wrapped complex Gaussian distributions. Based on the existing real-valued CRT
utilizing maximum likelihood estimation (MLE), we propose a fast MLE-based
C-CRT (MLE C-CRT). The proposed algorithm requires only $2L$ searches to obtain
the optimal estimate of the common remainder, where $L$ is the number of
moduli. Once the common remainder is estimated, the complex number can be
determined using the C-CRT. Furthermore, we obtain a necessary and sufficient
condition for the fast MLE C-CRT to achieve robust estimation. Finally, we
apply the proposed algorithm to ADCs. The results demonstrate that the proposed
algorithm outperforms the existing methods.; 10) Precision of Treatment Hierarchy: A Metric for Quantifying Certainty in
  Treatment Hierarchies from Network Meta-Analysis; Network meta-analysis (NMA) is an extension of pairwise meta-analysis which
facilitates the estimation of relative effects for multiple competing
treatments. A hierarchy of treatments is a useful output of an NMA. Treatment
hierarchies are produced using ranking metrics. Common ranking metrics include
the Surface Under the Cumulative RAnking curve (SUCRA) and P-scores, which are
the frequentist analogue to SUCRAs. Both metrics consider the size and
uncertainty of the estimated treatment effects, with larger values indicating a
more preferred treatment. Although SUCRAs and P-scores themselves consider
uncertainty, treatment hierarchies produced by these ranking metrics are
typically reported without a measure of certainty, which might be misleading to
practitioners. We propose a new metric, Precision of Treatment Hierarchy
(POTH), which quantifies the certainty in producing a treatment hierarchy from
SUCRAs or P-scores. The metric connects three statistical quantities: The
variance of the SUCRA values, the variance of the mean rank of each treatment,
and the average variance of the distribution of individual ranks for each
treatment. POTH provides a single, interpretable value which quantifies the
degree of certainty in producing a treatment hierarchy. We show how the metric
can be adapted to apply to subsets of treatments in a network, for example, to
quantify the certainty in the hierarchy of the top three treatments. We
calculate POTH for a database of NMAs to investigate its empirical properties,
and we demonstrate its use on three published networks.; 11) A Web-Based Application Leveraging Geospatial Information to Automate
  On-Farm Trial Design; On-farm sensor data have allowed farmers to implement field management
techniques and intensively track the corresponding responses. These data
combined with historical records open the door for real-time field management
improvements with the help of current advancements in computing power. However,
despite these advances, the statistical design of experiments is rarely used to
evaluate the performance of field management techniques accurately.
Traditionally, randomized block design is prevalent in statistical designs of
field trials, but in practice it is limited in dealing with large variations in
soil classes, management practices, and crop varieties. More specifically,
although this experimental design is suited for most trial types, it is not the
optimal choice when multiple factors are tested over multifarious natural
variations in farms, due to the economic constraints caused by the sheer number
of variables involved. Experimental refinement is required to better estimate
the effects of the primary factor in the presence of auxiliary factors. In this
way, farmers can better understand the characteristics and limitations of the
primary factor. This work presents a framework for automating the analysis of
local field variations by fusing soil classification data and lidar topography
data with historical yield. This framework will be leveraged to automate the
designing of field experiments based on multiple topographic features; 12) The WZ method and flawless WZ pairs; Recently, Kam Cheong Au discovered a powerful methodology of finding new
Wilf-Zeilberger (WZ) pairs. He calls it WZ seeds and gives numerous examples of
applications to proving longstanding conjectural identities for reciprocal
powers of $\pi$ and their duals for Dirichlet $L$-values. In this note we
explain how a modification of Au's WZ pairs together with a classical analytic
argument allows one to obtain simpler proofs of his results. We illustrate our
method with a few examples elaborated with assistance of Maple code that we
have developed.; 13) Continuously updated estimation of conditional hazard functions; Motivated by the need to analyze continuously updated data sets in the
context of time-to-event modeling, we propose a novel nonparametric approach to
estimate the conditional hazard function given a set of continuous and discrete
predictors. The method is based on a representation of the conditional hazard
as a ratio between a joint density and a conditional expectation determined by
the distribution of the observed variables. It is shown that such ratio
representations are available for uni- and bivariate time-to-events, in the
presence of common types of random censoring, truncation, and with possibly
cured individuals, as well as for competing risks. This opens the door to
nonparametric approaches in many time-to-event predictive models. To estimate
joint densities and conditional expectations we propose the recursive kernel
smoothing, which is well suited for online estimation. Asymptotic results for
such estimators are derived and it is shown that they achieve optimal
convergence rates. Simulation experiments show the good finite sample
performance of our recursive estimator with right censoring. The method is
applied to a real dataset of primary breast cancer.; 14) On Self-Propulsion by Oscillations in a Viscous Liquid; Suppose that a body $\mathscr B$ can move by translatory motion with velocity
$\boldsymbol{\gamma}$ in an otherwise quiescent Navier-Stokes liquid, $\mathscr
L$, filling the entire space outside $\mathscr B$. Denote by $\Omega =
\Omega(t)$, $t\in\mathbb{R}$, the one-parameter family of bounded, sufficiently
smooth domains of $\mathbb{R}^3$, each one representing the configuration of
$\mathscr B$ at time $t$ with respect to a frame with the origin at the center
of mass $G$ and axes parallel to those of an inertial frame. We assume that
there are no external forces acting on the coupled system $\mathscr S :=
\mathscr B +\mathscr L$ and that the only driving mechanism is a prescribed
change in shape of $\Omega$ with time. The self-propulsion problem that we
would like to address can be thus qualitatively formulated as follows. Suppose
that $\mathscr B$ changes its shape in a given time-periodic fashion, namely,
$\Omega(t+T) = \Omega(t)$, for some $T > 0$ and all $t \in \mathbb{R}$. Then,
find necessary and sufficient conditions on the map $t\mapsto \Omega(t)$
securing that $\mathscr B$ self-propels, that is, $G$ covers any given finite
distance in a finite time. We show that this problem is solvable, in a suitable
function class, provided the amplitude of the oscillations is below a given
constant. Moreover, we provide examples where the propelling velocity of
$\mathscr B$ is explicitly evaluated in terms of the physical parameters and
the frequency of oscillations.; 15) A generalized distance covariance framework for genome-wide association
  studies; When testing for the association of a single SNP with a phenotypic response,
one usually considers an additive genetic model, assuming that the mean of of
the response for the heterozygous state is the average of the means for the two
homozygous states. However, this simplification often does not hold. In this
paper, we present a novel framework for testing the association of a single SNP
and a phenotype. Different from the predominant standard approach, our
methodology is guaranteed to detect all dependencies expressed by classical
genetic association models. The asymptotic distribution under mild regularity
assumptions is derived. Moreover, the finite sample distribution under
Gaussianity is provided in which the exact p-value can be efficiently evaluated
via the classical Appell hypergeometric series. Both results are extended to a
regression-type setting with nuisance covariates, enabling hypotheses testing
in a wide range of scenarios. A connection of our approach to score tests is
explored, leading to intuitive interpretations as locally most powerful tests.
A simulation study demonstrates the computational efficiency and excellent
statistical performance of the proposed methodology. A real data example is
provided.; 16) A Dataset Generation Toolbox for Dynamic Security Assessment: On the
  Role of the Security Boundary; Dynamic security assessment (DSA) is crucial for ensuring the reliable
operation of power systems. However, conventional DSA approaches are becoming
intractable for future power systems, driving interest in more computationally
efficient data-driven methods. Efficient dataset generation is a cornerstone of
these methods. While importance and generic sampling techniques often focus on
operating points near the system's security boundary, systematic methods for
sampling in this region remain scarce. Furthermore, the impact of sampling near
the security boundary on the performance of data-driven DSA methods has yet to
be established. This paper highlights the critical role of accurately capturing
security boundaries for effective security assessment. As such, we propose a
novel method for generating a high number of samples close to the security
boundary, considering both AC feasibility and small-signal stability. Case
studies on the PGLib-OPF 39-bus and PGLib-OPF 162-bus systems demonstrate the
importance of including boundary-adjacent operating points in training datasets
while maintaining a balanced distribution of secure and insecure points.; 17) The simplest solutions of cold plasma equations: change in properties
  from a hydrodynamic to a kinetic model; We consider the transition from the kinetic model of Landau cold plasma to
the hydrodynamic one by constructing a ""multi-speed"" moment chain in the case
of one spatial variable. Closing this chain at the first step leads to the
standard hydrodynamic system of cold plasma. The change in the properties of
the solution when closing the chain at the second step is discussed using the
example of two classes of solutions - affine in space and traveling waves, and
it is shown that their properties change significantly compared to the
hydrodynamic model.; 18) Martingale Posteriors from Score Functions; Uncertainty associated with statistical problems arises due to what has not
been seen as opposed to what has been seen. Using probability to quantify the
uncertainty the task is to construct a probability model for what has not been
seen conditional on what has been seen. The traditional Bayesian approach is to
use prior distributions for constructing the predictive distributions, though
recently a novel approach has used density estimators and the use of
martingales to establish convergence of parameter values. In this paper we
reply on martingales constructed using score functions. Hence, the method only
requires the computing of gradients arising from parametric families of density
functions. A key point is that we do not rely on Markov Chain Monte Carlo
(MCMC) algorithms, and that the method can be implemented in parallel. We
present the theoretical properties of the score driven martingale posterior.
Further, we present illustrations under different models and settings.; 19) Measuring Inaccuracies in the Proportional Hazard Rate Model based on
  Extropy using a Length-Biased Weighted Residual approach; In this paper, we consider the concept of the residual inaccuracy measure and
extend it to its weighted version based on extropy. Properties of this measure
are studied and the discrimination principle is applied in the class of
proportional hazard rate (PHR) models. A characterization problem for the
proposed weighted extropy-inaccuracy measure is studied. We propose some
alternative expressions of weighted residual measure of inaccuracy.
Additionally, we establish upper and lower limits and various inequalities
related to the weighted residual inaccuracy measure using extropy.
Non-parametric estimators based on the kernel density estimation method and
empirical distribution function for the proposed measure are obtained and the
performance of the estimators are also discussed using some simulation studies.
Finally, a real dataset is applied for illustrating our new proposed measure.
In general, our study highlights the potential of the weighted residual
inaccuracy measure based on extropy as a powerful tool for improving the
quality and reliability of data analysis and modelling across various
disciplines. Researchers and practitioners can benefit from incorporating this
measure into their analytical toolkit to enhance the accuracy and effectiveness
of their work.; 20) Improved absolute frequency measurement of $^{171}$Yb at NMIJ with
  uncertainty below $2\times10^{-16}$; We report improved absolute frequency measurement of the
$^{1}$S$_{0}-^{3}$P$_{0}$ transition of $^{171}$Yb at National Metrology
Institute of Japan (NMIJ) by comparing the $^{171}$Yb optical lattice clock
NMIJ-Yb1 with 13 Cs primary frequency standards via International Atomic Time
from August 2021 to May 2023. The measured absolute frequency is 518 295 836
590 863.62(10) Hz with a fractional uncertainty of $1.9\times10^{-16}$, in good
agreement with the recommended frequency of $^{171}$Yb as a secondary
representation of the second. This uncertainty is 2.6 times lower than our
previous measurement uncertainty, and slightly lower than any uncertainties of
the absolute frequency measurements of $^{171}$Yb that have so far been
reported by other institutes. We also estimate correlation coefficients between
our present and previous measurements, which is important for updating the
recommended frequency.; 21) Layered Dirichlet Modeling to Assess the Changing Contributions of MLB
  Players as they Age; The productive career of a professional athlete is limited compared to the
normal human lifespan. Most professional athletes have retired by age 40. The
early retirement age is due to a combination of age-related performance and
life considerations. While younger players typically are stronger and faster
than their older teammates, older teammates add value to a team due to their
experience and perspective. Indeed, the highest--paid major league baseball
players are those over the age of 35. These players contribute intangibly to a
team through mentorship of younger players; however, their peak athletic
performance has likely passed. Given this, it is of interest to learn how more
mature players contribute to a team in measurable ways. We examine the
distribution of plate appearance outcomes from three different age groups as
compositional data, using Layered Dirichlet Modeling (LDM). We develop a
hypothesis testing framework to compare the average proportions of outcomes for
each component among 3 of more groups. LDM can not only determine evidence for
differences among populations, but also pinpoint within which component the
largest changes are likely to occur. This framework can determine where players
can be of most use as they age.; 22) Max-Linear Tail Regression; The relationship between a response variable and its covariates can vary
significantly, especially in scenarios where covariates take on extremely high
or low values. This paper introduces a max-linear tail regression model
specifically designed to capture such extreme relationships. To estimate the
regression coefficients within this framework, we propose a novel M-estimator
based on extreme value theory. The consistency and asymptotic normality of our
proposed estimator are rigorously established under mild conditions. Simulation
results demonstrate that our estimation method outperforms the conditional
least squares approach. We validate the practical applicability of our model
through two case studies: one using financial data and the other using rainfall
data.; 23) Comparative analysis and practical applications of cubic transmutations
  for the Pareto distribution; Transmutation is a technique for extending classical probability
distributions in order to give them more flexibility. In this paper, we are
interested in cubic transmutations of the Pareto distribution. We establish a
general formula that unifies existing cubic transmutations of the Pareto
distribution and facilitates the derivation of new cubic transmutations that
have not yet been explored in the literature. We also derive general formulas
for the related mathematical properties. Finally, we perform a comparative
analysis of the six transmutations existing in the literature using real-world
data. The results obtained confirm the flexibility and effectiveness of cubic
transmutations in modeling various types of data.; 24) A new proof of monomialisation from 3-folds to surfaces; In this paper, we give a new proof of the foundational result, due to S.
Cutkosky, on the existence of a monomialisation of a morphism from a 3-fold to
a surface. Our proof brings to the fore the notion of log-Fitting ideals, and
requires us to develop new methods related to Rank Theorems and log-Fitting
ideals.; 25) Testing Equality of Medians for Multiple Samples; In this paper, we construct a consistent non-parametric test for testing the
equality of population medians for different samples when the observations in
each sample are independent and identically distributed. This test can be
further used to test the equality of unknown location parameters for different
samples. The method discussed in this paper can be extended to any quantile
level instead of the median. We present the theoretical results and also
demonstrate the performance of this test through simulation studies.; 26) Quantifying Variance in Evaluation Benchmarks; Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models.; 27) Variation of sentence length across time and genre; The goal of this paper is threefold: i) to present some practical aspects of
using full-text version of Corpus of Historical American English (COHA), the
largest diachronic multi-genre corpus of the English language, in the
investigation of a linguistic trend of change; ii) to test a widely held
assumption that sentence length in written English has been steadily decreasing
over the past few centuries; iii) to point to a possible link between the
changes in sentence length and changes in the English syntactic usage. The
empirical proof of concept for iii) is provided by the decline in the frequency
of the non-finite purpose subordinator in order to. Sentence length, genre and
the likelihood of occurrence of in order to are shown to be interrelated.; 28) How far does the influence of the free surface extend in turbulent open
  channel flow?; Turbulent open channel flow is known to feature a multi-layer structure near
the free surface. In the present work we employ direct numerical simulations
considering Reynolds numbers up to $\mathrm{Re}_\tau=900$ and domain sizes
large enough ($L_x=12 \pi h$, $L_z=4 \pi h$) to faithfully capture the effect
of very-large-scale motions in order to test the proposed scaling laws and
ultimately answer the question: How far does the influence of the free surface
extend? In the region near the free surface, where fluctuation intensities of
velocity and vorticity become highly anisotropic, we observe the previously
documented triple-layer structure, consisting of a wall-normal velocity damping
layer that scales with the channel height $h$, and two sublayers that scale
with the near-surface viscous length scale $\ell_V=\mathrm{Re}_b^{-1/2}h$ and
with the Kolmogorov length scale $\ell_K=\mathrm{Re}_b^{-3/4}h$, respectively.
The Kolmogorov sublayer measures $\delta_K \approx 20 \ell_K$ and the layer,
where the wall-normal turbulence intensity decreases linearly to zero near the
free surface, scales with $\ell_V$ and the corresponding near-surface viscous
sublayer measures $\delta_V \approx \ell_V$. Importantly, the streamwise
turbulence intensity profile for $\mathrm{Re}_\tau \ge 400$ suggests that the
influence of the free-slip boundary penetrates essentially all the way down to
the solid wall through the appearance of enhanced very-large-scale motions
($\delta_{SIL}\approx h$). In contrast, the layer where the surface-normal
turbulence intensity is damped to zero is restricted to the free surface
($\delta_{NVD}\approx 0.3h$). As a consequence, the partitioning of the
surface-influenced region has to be expanded to a four-layer structure that
spans the entire channel height $h$.; 29) Rethinking Spiking Neural Networks from an Ensemble Learning Perspective; Spiking neural networks (SNNs) exhibit superior energy efficiency but suffer
from limited performance. In this paper, we consider SNNs as ensembles of
temporal subnetworks that share architectures and weights, and highlight a
crucial issue that affects their performance: excessive differences in initial
states (neuronal membrane potentials) across timesteps lead to unstable
subnetwork outputs, resulting in degraded performance. To mitigate this, we
promote the consistency of the initial membrane potential distribution and
output through membrane potential smoothing and temporally adjacent subnetwork
guidance, respectively, to improve overall stability and performance. Moreover,
membrane potential smoothing facilitates forward propagation of information and
backward propagation of gradients, mitigating the notorious temporal gradient
vanishing problem. Our method requires only minimal modification of the spiking
neurons without adapting the network structure, making our method generalizable
and showing consistent performance gains in 1D speech, 2D object, and 3D point
cloud recognition tasks. In particular, on the challenging CIFAR10-DVS dataset,
we achieved 83.20\% accuracy with only four timesteps. This provides valuable
insights into unleashing the potential of SNNs.; 30) Thresholding Nonprobability Units in Combined Data for Efficient Domain
  Estimation; Quasi-randomization approaches estimate latent participation probabilities
for units from a nonprobability / convenience sample. Estimation of
participation probabilities for convenience units allows their combination with
units from the randomized survey sample to form a survey weighted domain
estimate. One leverages convenience units for domain estimation under the
expectation that estimation precision and bias will improve relative to solely
using the survey sample; however, convenience sample units that are very
different in their covariate support from the survey sample units may inflate
estimation bias or variance. This paper develops a method to threshold or
exclude convenience units to minimize the variance of the resulting survey
weighted domain estimator. We compare our thresholding method with other
thresholding constructions in a simulation study for two classes of datasets
based on degree of overlap between survey and convenience samples on covariate
support. We reveal that excluding convenience units that each express a low
probability of appearing in \emph{both} reference and convenience samples
reduces estimation error.; 31) Reconciling Binary Replicates: Beyond the Average; Binary observations are often repeated to improve data quality, creating
technical replicates. Several scoring methods are commonly used to infer the
actual individual state and obtain a probability for each state. The common
practice of averaging replicates has limitations, and alternative methods for
scoring and classifying individuals are proposed. Additionally, an indecisive
response might be wiser than classifying all individuals based on their
replicates in the medical context, where 1 indicates a particular health
condition. Building on the inherent limitations of the averaging approach,
three alternative methods are examined: the median, maximum penalized
likelihood estimation, and a Bayesian algorithm. The theoretical analysis
suggests that the proposed alternatives outperform the averaging approach,
especially the Bayesian method, which incorporates uncertainty and provides
credible intervals. Simulations and real-world medical datasets are used to
demonstrate the practical implications of these methods for improving
diagnostic accuracy and disease prevalence estimation.; 32) Blockchain-Powered Asset Tokenization Platform; Blockchain Technology has revolutionized Finance and Technology with its
secure, decentralized, and trust-less methodologies of data management. In a
world where asset value fluctuations are unprecedented, it has become
increasingly important to secure one's stake on their valuable assets and
streamline the process of acquiring and transferring that stake over a
trust-less environment. Tokenization proves to be unbeaten when it comes to
giving the ownership of one's asset, an immutable, liquid, and irrefutable
identity, as of the likes of cryptocurrency. It enables users to store and
maintain records of their assets and even transfer fractions of these assets to
other investors and stakeholders in the form of these tokens. However, like
cryptocurrency, it too has witnessed attacks by malicious users that have
compromised on their very foundation of security.These attacks have inflicted
more damage since they represent real-world assets that have physical
importance. This project aims to assist users to secure their valuable assets
by providing a highly secure user-friendly platform to manage, create and
deploy asset-tokens, and facilitate open and transparent communication between
stakeholders, thereby upholding the decentralized nature of blockchain and
offering the financial freedom of asset ownership, with an added market value
of a cryptocurrency-backed tokens.; 33) CrowdHMTware: A Cross-level Co-adaptation Middleware for Context-aware
  Mobile DL Deployment; There are many deep learning (DL) powered mobile and wearable applications
today continuously and unobtrusively sensing the ambient surroundings to
enhance all aspects of human lives.To enable robust and private mobile sensing,
DL models are often deployed locally on resource-constrained mobile devices
using techniques such as model compression or offloading.However, existing
methods, either front-end algorithm level (i.e. DL model
compression/partitioning) or back-end scheduling level (i.e. operator/resource
scheduling), cannot be locally online because they require offline retraining
to ensure accuracy or rely on manually pre-defined strategies, struggle with
dynamic adaptability.The primary challenge lies in feeding back runtime
performance from the back-end level to the front-end level optimization
decision. Moreover, the adaptive mobile DL model porting middleware with
cross-level co-adaptation is less explored, particularly in mobile environments
with diversity and dynamics. In response, we introduce CrowdHMTware, a dynamic
context-adaptive DL model deployment middleware for heterogeneous mobile
devices. It establishes an automated adaptation loop between cross-level
functional components, i.e. elastic inference, scalable offloading, and
model-adaptive engine, enhancing scalability and adaptability. Experiments with
four typical tasks across 15 platforms and a real-world case study demonstrate
that CrowdHMTware can effectively scale DL model, offloading, and engine
actions across diverse platforms and tasks. It hides run-time system issues
from developers, reducing the required developer expertise.; 34) Identification and Scaling of Latent Variables in Ordinal Factor
  Analysis; Social science researchers are generally accustomed to treating ordinal
variables as though they are continuous. In this paper, we consider how
identification constraints in ordinal factor analysis can mimic the treatment
of ordinal variables as continuous. We describe model constraints that lead to
latent variable predictions equaling the average of ordinal variables. This
result leads us to propose minimal identification constraints, which we call
""integer constraints,"" that center the latent variables around the scale of the
observed, integer-coded ordinal variables. The integer constraints lead to
intuitive model parameterizations because researchers are already accustomed to
thinking about ordinal variables as though they are continuous. We provide a
proof that our proposed integer constraints are indeed minimal identification
constraints, as well as an illustration of how integer constraints work with
real data. We also provide simulation results indicating that integer
constraints are similar to other identification constraints in terms of
estimation convergence and admissibility.; 35) Breakdown of superdiffusion in perturbed quantum integrable spin chains
  and ladders; Superdiffusive transport with dynamical exponent $z=3/2$ has been firmly
established at finite temperature for a class of integrable systems with a
non-abelian global symmetry $G$. On the inclusion of integrability-breaking
perturbations, diffusive transport with $z=2$ is generically expected to hold
in the limit of late time. Recent studies of the classical
Haldane-Ishimori-Skylanin model have found that perturbations that preserve the
global symmetry lead to a much slower timescale for the onset of diffusion,
albeit with uncertainty over the exact scaling exponent. That is, for
perturbations of strength $\lambda$, the characteristic timescale for diffusion
goes as $t_*\sim \lambda^{-\alpha}$ for some $\alpha$. Using large-scale matrix
product state simulations, we investigate this behavior for perturbations to
the canonical quantum model showing superdiffusion: the $S=1/2$ quantum
Heisenberg chain. We consider a ladder configuration and look at various
perturbations that either break or preserve the $SU(2)$ symmetry, leading to
scaling exponents consistent with those observed in one classical study
arXiv:2402.18661: $\alpha=2$ for symmetry-breaking terms and $\alpha=6$ for
symmetry-preserving terms. We also consider perturbations from another
integrable point of the ladder model with $G=SU(4)$ and find consistent
results. Finally, we consider a generalization to an $SU(3)$ ladder and find
that the $\alpha=6$ scaling appears to be universal across superdiffusive
systems when the perturbations preserve the non-abelian symmetry $G$.; 36) Balancing the effective sample size in prior across different doses in
  the curve-free Bayesian decision-theoretic design for dose-finding trials; The primary goal of dose allocation in phase I trials is to minimize patient
exposure to subtherapeutic or excessively toxic doses, while accurately
recommending a phase II dose that is as close as possible to the maximum
tolerated dose (MTD). Fan et al. (2012) introduced a curve-free Bayesian
decision-theoretic design (CFBD), which leverages the assumption of a monotonic
dose-toxicity relationship without directly modeling dose-toxicity curves. This
approach has also been extended to drug combinations for determining the MTD
(Lee et al., 2017). Although CFBD has demonstrated improved trial efficiency by
using fewer patients while maintaining high accuracy in identifying the MTD, it
may artificially inflate the effective sample sizes for the updated prior
distributions, particularly at the lowest and highest dose levels. This can
lead to either overshooting or undershooting the target dose. In this paper, we
propose a modification to CFBD's prior distribution updates that balances
effective sample sizes across different doses. Simulation results show that
with the modified prior specification, CFBD achieves a more focused dose
allocation at the MTD and offers more precise dose recommendations with fewer
patients on average. It also demonstrates robustness to other well-known dose
finding designs in literature.; 37) Training-Free Guidance Beyond Differentiability: Scalable Path Steering
  with Tree Search in Diffusion and Flow Models; Training-free guidance enables controlled generation in diffusion and flow
models, but most existing methods assume differentiable objectives and rely on
gradients. This work focuses on training-free guidance addressing challenges
from non-differentiable objectives and discrete data distributions. We propose
an algorithmic framework TreeG: Tree Search-Based Path Steering Guidance,
applicable to both continuous and discrete settings in diffusion and flow
models. TreeG offers a unified perspective on training-free guidance: proposing
candidates for the next step, evaluating candidates, and selecting the best to
move forward, enhanced by a tree search mechanism over active paths or
parallelizing exploration. We comprehensively investigate the design space of
TreeG over the candidate proposal module and the evaluation function,
instantiating TreeG into three novel algorithms. Our experiments show that
TreeG consistently outperforms the top guidance baselines in symbolic music
generation, small molecule generation, and enhancer DNA design, all of which
involve non-differentiable challenges. Additionally, we identify an
inference-time scaling law showing TreeG's scalability in inference-time
computation.; 38) Integrating Misclassified EHR Outcomes with Validated Outcomes from a
  Non-probability Sample; Although increasingly used for research, electronic health records (EHR)
often lack gold-standard assessment of key data elements. Linking EHRs to other
data sources with higher-quality measurements can improve statistical
inference, but such analyses must account for selection bias if the linked data
source arises from a non-probability sample. We propose a set of novel
estimators targeting the average treatment effect (ATE) that combine
information from binary outcomes measured with error in a large,
population-representative EHR database with gold-standard outcomes obtained
from a smaller validation sample subject to selection bias. We evaluate our
approach in extensive simulations and an analysis of data from the Adult
Changes in Thought (ACT) study, a longitudinal study of incident dementia in a
cohort of Kaiser Permanente Washington members with linked EHR data. For a
subset of deceased ACT participants who consented to brain autopsy prior to
death, gold-standard measures of Alzheimer's disease neuropathology are
available. Our proposed estimators reduced bias and improved efficiency for the
ATE, facilitating valid inference with EHR data when key data elements are
ascertained with error.; 39) Stepwise regression revisited; This paper shows that the degree of approximate multicollinearity in a linear
regression model increases simply by including independent variables, even if
these are not highly linearly related. In the current situation where it is
relatively easy to find linear models with a large number of independent
variables, it is shown that this issue can lead to the erroneous conclusion
that there is a worrying problem of approximate multicollinearity. To avoid
this situation, an adjusted variance inflation factor is proposed to compensate
the presence of a large number of independent variables in the multiple linear
regression model. It is shown that this proposal has a direct impact on
variable selection models based on influence relationships, which translates
into a new decision criterion in the individual significance contrast to be
considered in stepwise regression models or even directly in a multiple linear
regression model.; 40) Discovering Physics Laws of Dynamical Systems via Invariant Function
  Learning; We consider learning underlying laws of dynamical systems governed by
ordinary differential equations (ODE). A key challenge is how to discover
intrinsic dynamics across multiple environments while circumventing
environment-specific mechanisms. Unlike prior work, we tackle more complex
environments where changes extend beyond function coefficients to entirely
different function forms. For example, we demonstrate the discovery of ideal
pendulum's natural motion $\alpha^2 \sin{\theta_t}$ by observing pendulum
dynamics in different environments, such as the damped environment $\alpha^2
\sin(\theta_t) - \rho \omega_t$ and powered environment $\alpha^2
\sin(\theta_t) + \rho \frac{\omega_t}{\left|\omega_t\right|}$. Here, we
formulate this problem as an \emph{invariant function learning} task and
propose a new method, known as \textbf{D}isentanglement of \textbf{I}nvariant
\textbf{F}unctions (DIF), that is grounded in causal analysis. We propose a
causal graph and design an encoder-decoder hypernetwork that explicitly
disentangles invariant functions from environment-specific dynamics. The
discovery of invariant functions is guaranteed by our information-based
principle that enforces the independence between extracted invariant functions
and environments. Quantitative comparisons with meta-learning and invariant
learning baselines on three ODE systems demonstrate the effectiveness and
efficiency of our method. Furthermore, symbolic regression explanation results
highlight the ability of our framework to uncover intrinsic laws.; 41) Sparsity learning via structured functional factor augmentation; As one of the most powerful tools for examining the association between
functional covariates and a response, the functional regression model has been
widely adopted in various interdisciplinary studies. Usually, a limited number
of functional covariates are assumed in a functional linear regression model.
Nevertheless, correlations may exist between functional covariates in
high-dimensional functional linear regression models, which brings significant
statistical challenges to statistical inference and functional variable
selection. In this article, a novel functional factor augmentation structure
(fFAS) is proposed for multivariate functional series, and a multivariate
functional factor augmentation selection model (fFASM) is further proposed to
deal with issues arising from variable selection of correlated functional
covariates. Theoretical justifications for the proposed fFAS are provided, and
statistical inference results of the proposed fFASM are established. Numerical
investigations support the superb performance of the novel fFASM model in terms
of estimation accuracy and selection consistency.; 42) Visual-RFT: Visual Reinforcement Fine-Tuning; Reinforcement Fine-Tuning (RFT) in Large Reasoning Models like OpenAI o1
learns from feedback on its answers, which is especially useful in applications
when fine-tuning data is scarce. Recent open-source work like DeepSeek-R1
demonstrates that reinforcement learning with verifiable reward is one key
direction in reproducing o1. While the R1-style model has demonstrated success
in language models, its application in multi-modal domains remains
under-explored. This work introduces Visual Reinforcement Fine-Tuning
(Visual-RFT), which further extends the application areas of RFT on visual
tasks. Specifically, Visual-RFT first uses Large Vision-Language Models (LVLMs)
to generate multiple responses containing reasoning tokens and final answers
for each input, and then uses our proposed visual perception verifiable reward
functions to update the model via the policy optimization algorithm such as
Group Relative Policy Optimization (GRPO). We design different verifiable
reward functions for different perception tasks, such as the Intersection over
Union (IoU) reward for object detection. Experimental results on fine-grained
image classification, few-shot object detection, reasoning grounding, as well
as open-vocabulary object detection benchmarks show the competitive performance
and advanced generalization ability of Visual-RFT compared with Supervised
Fine-tuning (SFT). For example, Visual-RFT improves accuracy by $24.3\%$ over
the baseline in one-shot fine-grained image classification with around 100
samples. In few-shot object detection, Visual-RFT also exceeds the baseline by
$21.9$ on COCO's two-shot setting and $15.4$ on LVIS. Our Visual-RFT represents
a paradigm shift in fine-tuning LVLMs, offering a data-efficient, reward-driven
approach that enhances reasoning and adaptability for domain-specific tasks.; 43) LU Decomposition and Generalized Autoone-Takagi Decomposition of Dual
  Matrices and their Applications; This paper uses matrix transformations to provide the Autoone-Takagi
decomposition of dual complex symmetric matrices and extends it to dual
quaternion $\eta$-Hermitian matrices. The LU decomposition of dual matrices is
given using the general solution of the Sylvester equation, and its equivalence
to the existence of rank-k decomposition and dual Moore-Penrose generalized
inverse (DMPGI) is proved. Similar methods are then used to provide the
Cholesky decomposition of dual real symmetric positive definite matrices. Both
of our decompositions are driven by applications in numerical linear algebra.; 44) Inference for Heterogeneous Treatment Effects with Efficient Instruments
  and Machine Learning; We introduce a new instrumental variable (IV) estimator for heterogeneous
treatment effects in the presence of endogeneity. Our estimator is based on
double/debiased machine learning (DML) and uses efficient machine learning
instruments (MLIV) and kernel smoothing. We prove consistency and asymptotic
normality of our estimator and also construct confidence sets that are more
robust towards weak IV. Along the way, we also provide an accessible discussion
of the corresponding estimator for the homogeneous treatment effect with
efficient machine learning instruments. The methods are evaluated on synthetic
and real datasets and an implementation is made available in the R package
IVDML.; 45) Nonlinear Open-Loop Mean field Stackelberg Stochastic Differential Game; This paper studies a nonlinear open-loop mean field Stackelberg stochastic
differential game by using the probabilistic method through the FBSDE system
and the idea of taking control as the fixed point. We successively construct
the decentralized optimal control problems for the followers and the leader,
among which the leader's decentralized optimal control problem is a partial
information optimal control problem with the fully coupled conditional
mean-field forward-backward stochastic differential equation (FBSDE, in short)
as the state equation. We successively derive the maximum principles for the
corresponding decentralized optimal control problems of the followers and the
leader. To obtain the existence, uniqueness and estimations of solutions of the
state equation, the variational equation and the adjoint equation for the
leader's decentralized optimal control problem, we study the well-posedness of
a new form of conditional mean-field FBSDE. Finally, the decentralized optimal
controls of the leader and followers are proved to be the approximate
Stackelberg equilibrium of the nonlinear mean field Stackelberg game.; 46) A totally non-compensatory multi-criteria method for evaluating and
  improving level of satisfaction (LoS): proposal and application on Airport
  Terminal of Passengers; To evaluate and assign a service according customer's level of satisfaction
(LoS) is a relevant issue in operations management. This is a typical situation
in which the evaluators, have passed by heterogeneous experiences along their
life which implies they could consider different variables when evaluating a
product. Despite it, the models for measuring Los usually consider a
homogeneous set of criteria when facing LoS evaluation. This study applies a
totally non-compensatory modeling that allows each customer to select the
criteria, from a whole set of aspects, the customer wants to use for evaluating
LoS. The proposal was tested in evaluating LoS regarding the services provided
by Airport Terminal of Passengers (ATPs) in Brazil, with data collected in a
survey involving 19,240 passengers, interviewed at 15 Brazilian international
airports. The data collected was imputed into ELECTRE TRI ME algorithm to
obtain the a credibility degree of sorting the instances. The values of
credibility degree were them used to obtain groups of ATPs. Finally, the
statistical modes of the evaluations in each group were analyzed and compared.
The proposal allowed a full non-compensatory approach to obtain the credibility
degree even when considering perceptions from several evaluators that could use
different criteria. As a result, it was identified, for each cluster of ATP,
the criteria sets to be improved and even those to be prioritized. The pioneer
modeling proposed in this article for evaluating LoS plus its instancing in
ATPs terminals represents an original advance in the establishment of a
multi-criteria decision aid (MCDA) model to assess the quality of services and
fills a relevant gap for a full non-compensatory approach able to classify the
LoS in the airport context, considering perceptions of multiple evaluators even
if they use different criteria in their evaluations.; 47) A Study on 5G Network Slice Isolation Based on Native Cloud and Edge
  Computing Tools; 5G networks support various advanced applications through network slicing,
network function virtualization (NFV), and edge computing, ensuring low latency
and service isolation. However, private 5G networks relying on open-source
tools still face challenges in maturity and integration with edge/cloud
platforms, compromising proper slice isolation. This study investigates
resource allocation mechanisms to address this issue, conducting experiments in
a hospital scenario with medical video conferencing. The results show that CPU
limitations improve the performance of prioritized slices, while memory
restrictions have minimal impact. The generated data and scripts have been made
publicly available for future research and machine learning applications.; 48) VQ-LLM: High-performance Code Generation for Vector Quantization
  Augmented LLM Inference; In this work, we design and implement VQ-LLM, an efficient fused Vector
Quantization (VQ) kernel generation framework. We first introduce a software
abstraction called codebook cache to optimize codebook access efficiency and
support the integration of VQ with various computations. The codebook cache
adaptively stores different entries across the GPU's memory hierarchy,
including off-chip global memory, on-chip shared memory, and registers.
Centered around the codebook cache, we design an efficient computation engine
that optimizes memory traffic during computations involving codebooks. This
compute engine adopts the codebook-centric dataflow and fusion optimizations.
Additionally, we provide adaptive heuristics to tailor parameter selection in
our optimizations to diverse VQ configurations. Our optimizations achieve an
average latency reduction of 46.13% compared to unoptimized versions. Compared
to existing open-source implementations, our methods decrease latency by 64.36%
to 99.1%. A final comparison with state-of-the-art element-wise quantization
methods like AWQ and KVQuant shows that our VQ-LLM is practically viable,
achieving latencies close or even better latencies to those at equivalent
bit-widths, potentially offering greater accuracy.; 49) Gravitational Wave Memory from Accelerating Relativistic Jets in
  Multiple Thick Shell Scenarios; Gravitational wave (GW) memory, a permanent distortion of the space-time
metric, is anticipated during the acceleration of relativistic jets in
gamma-ray bursts (GRBs). While the precise mechanism behind GRBs is not yet
fully understood, detecting GW memory may contribute to clarifying their
nature. In this paper, we consider various scenarios of GW memory emission,
including both single and multiple shells with thin- and thick-shells. In
particular, the memory spectrum for each scenario is compared with the
sensitivity of next-generation detectors, namely DECIGO and ET-D. Physical
properties spread over a broad-band region, emphasizing the importance of
combined and wide-band observations. We also simulate GW memory based on
nearby, realistic scenarios and demonstrate its detectability.; 50) Sampling from Density power divergence-based Generalized posterior
  distribution via Stochastic optimization; Robust Bayesian inference using density power divergence (DPD) has emerged as
a promising approach for handling outliers in statistical estimation. While the
DPD-based posterior offers theoretical guarantees for robustness, its practical
implementation faces significant computational challenges, particularly for
general parametric models with intractable integral terms. These challenges
become especially pronounced in high-dimensional settings where traditional
numerical integration methods prove inadequate and computationally expensive.
We propose a novel sampling methodology that addresses these limitations by
integrating the loss-likelihood bootstrap with a stochastic gradient descent
algorithm specifically designed for DPD-based estimation. Our approach enables
efficient and scalable sampling from DPD-based posteriors for a broad class of
parametric models, including those with intractable integrals, and we further
extend it to accommodate generalized linear models. Through comprehensive
simulation studies, we demonstrate that our method efficiently samples from
DPD-based posteriors, offering superior computational scalability compared to
conventional methods, particularly in high-dimensional settings. The results
also highlight its ability to handle complex parametric models with intractable
integral terms.; 51) Atomic Origins of Magnetic Anisotropy in Ru-substituted Manganite Films; Magnetic anisotropy in complex oxides often originates from the complex
interplay of several factors, including crystal structure, spin-orbit coupling,
and electronic interactions. Recent studies on Ru-substituted
$La_{0.70}Sr_{0.30}MnO_3$ (Ru-LSMO) films demonstrate emerging magnetic and
magneto-transport properties, where magnetic anisotropy plays a crucial role.
However, the atomic origin and underlying mechanisms of the magnetic anisotropy
of this material system remain elusive. This work sheds light on these aspects.
Detailed element-specific X-ray magnetic dichroism analysis suggests that Ru
single ion anisotropy governs the overall magnetic anisotropy. Furthermore, the
magnetic property of Mn ions changes dramatically due to strong
antiferromagnetic coupling between Ru and Mn ions. Our findings clarify the
role of Ru single ion anisotropy behind magnetic anisotropy in Ru-LSMO,
offering a promising avenue for designing advanced materials with tailored
magnetic properties for next generation magnetic and spintronic technologies.
As the Curie temperature of these materials is close to room temperature, such
tunable magnetic anisotropy holds prospects for functional room-temperature
magnetic devices.; 52) 2-Coherent Internal Models of Homotopical Type Theory; The program of internal type theory seeks to develop the categorical model
theory of dependent type theory using the language of dependent type theory
itself. In the present work we study internal homotopical type theory by
relaxing the notion of a category with families (cwf) to that of a wild, or
precoherent higher cwf, and determine coherence conditions that suffice to
recover properties expected of models of dependent type theory. The result is a
definition of a split 2-coherent wild cwf, which admits as instances both the
syntax and the ""standard model"" given by a universe type. This allows us to
give a straightforward internalization of the notion of a 2-coherent reflection
of homotopical type theory in itself: namely as a 2-coherent wild cwf morphism
from the syntax to the standard model. Our theory also easily specializes to
give definitions of ""low-dimensional"" higher cwfs, and conjecturally includes
the container higher model as a further instance.; 53) Graph-based Change Point Detection for Functional Data; Modeling functions that are sequentially observed as functional time series
is becoming increasingly common. In such models, it is often crucial to ensure
data homogeneity. We investigate the sensitivity of graph-based change point
detection for changes in the distribution of functional data that demarcate
homogeneous regions. Related test statistics and thresholds for detection are
given. A key factor in the efficacy of such tests is the graph construction.
Practical considerations for constructing a graph on arbitrary data are
explored. Simulation experiments investigate tuning parameters for graph
construction and evaluate the graph-based methods in comparison to existing
functional methods. In addition to sensitivity of lower and higher order
changes, robustness to the tuning parameter choices, and practical
recommendations, are shown. Applications to multi-year pedestrian counts,
high-frequency asset returns, and continuous electricity prices corroborate the
simulation results.; 54) Exact statistical tests using integer programming: Leveraging an
  overlooked approach for maximizing power for differences between binomial
  proportions; Traditional hypothesis testing methods for differences in binomial
proportions can either be too liberal (Wald test) or overly conservative
(Fisher's exact test), especially in small samples. Regulators favour
conservative approaches for robust type I error control, though excessive
conservatism may significantly reduce statistical power. We offer fundamental
theoretical contributions that extend an approach proposed in 1969, resulting
in the derivation of a family of exact tests designed to maximize a specific
type of power. We establish theoretical guarantees for controlling type I error
despite the discretization of the null parameter space. This theoretical
advancement is supported by a comprehensive series of experiments to
empirically quantify the power advantages compared to traditional hypothesis
tests. The approach determines the rejection region through a binary decision
for each outcome dataset and uses integer programming to find an optimal
decision boundary that maximizes power subject to type I error constraints. Our
analysis provides new theoretical properties and insights into this approach's
comparative advantages. When optimized for average power over all possible
parameter configurations under the alternative, the method exhibits remarkable
robustness, performing optimally or near-optimally across specific alternatives
while maintaining exact type I error control. The method can be further
customized for particular prior beliefs by using a weighted average. The
findings highlight both the method's practical utility and how techniques from
combinatorial optimization can enhance statistical methodology.; 55) Learning Sheaf Laplacian Optimizing Restriction Maps; The aim of this paper is to propose a novel framework to infer the sheaf
Laplacian, including the topology of a graph and the restriction maps, from a
set of data observed over the nodes of a graph. The proposed method is based on
sheaf theory, which represents an important generalization of graph signal
processing. The learning problem aims to find the sheaf Laplacian that
minimizes the total variation of the observed data, where the variation over
each edge is also locally minimized by optimizing the associated restriction
maps. Compared to alternative methods based on semidefinite programming, our
solution is significantly more numerically efficient, as all its fundamental
steps are resolved in closed form. The method is numerically tested on data
consisting of vectors defined over subspaces of varying dimensions at each
node. We demonstrate how the resulting graph is influenced by two key factors:
the cross-correlation and the dimensionality difference of the data residing on
the graph's nodes.; 56) Decays of $N^*$ and $\Delta^*$ resonances into $N\rho$, $\Delta\pi$, and
  $N\sigma$; The decays of $N^*$ and $\Delta^*$ resonances into $N\rho$, $\Delta\pi$ and
$N\sigma$ final states are studied in a coupled-channel analysis of data on
pion- and photo-induced reactions. Improvements in the fit were observed when
new resonance contributions were introduced. Frequencies for the intermediate
isobars $\Delta(1232)\pi$, $N\rho$, $N\sigma$ are reported.; 57) X-ray Spectra from General Relativistic RMHD Simulations of Thin Disks; We compare X-ray emission from several general relativistic, multi-frequency,
radiation magnetohydrodynamic simulations of thin black hole accretion disks
with different accretion rates and spins. The simulations were performed using
the M1 closure scheme, resolved with twelve frequency (energy) bins
logarithmically spaced from $5 \times 10^{-3}$ to $5 \times 10^3$ keV. We apply
a general relativistic Monte Carlo transport code to post-process the
simulation data with greater fidelity in frequency resolution and Compton
scattering treatment. Despite the relatively few energy bins and Kompaneets
approximation to Compton scattering utilized in the M1 method, we find
generally good agreement between the methods. Both produce prominent thermal
profiles with peaks around 2 - 2.5 keV, where agreement is particularly strong
and representative of the soft state. Both also find weaker (lower luminosity)
thermally sourced emission extending out to 100 keV due to the hotter innermost
regions of the disks. Inverse Compton scattering becomes increasingly effective
at hardening spectral outputs with increasing black hole spin, and becomes the
dominant mechanism for photons that escape with energies between 10 to several
hundred keV. At very high rates of spin the radiation flux in this upscattered
component becomes comparable to the thermal flux, a phenomenon typically
associated with intermediate states. Beyond $10^4$ keV, we observe faint,
free-free emission from hot, optically thin coronal regions developing near the
horizon, common to both spinning and nonspinning black holes.; 58) Analyzing Privacy Dynamics within Groups using Gamified Auctions; Online shared content, such as group pictures, often contains information
about multiple users. Developing technical solutions to manage the privacy of
such ""co-owned"" content is challenging because each co-owner may have different
preferences. Recent technical approaches advocate group-decision mechanisms,
including auctions, to decide as how best to resolve these differences.
However, it is not clear if users would participate in such mechanisms and if
they do, whether they would act altruistically. Understanding the privacy
dynamics is crucial to develop effective mechanisms for privacy-respecting
collaborative systems. Accordingly, this work develops RESOLVE, a privacy
auction game to understand the sharing behavior of users in groups. Our results
of users' playing the game show that i) the users' understanding of individual
vs. group privacy differs significantly; ii) often users fight for their
preferences even at the cost of others' privacy; and iii) at times users
collaborate to fight for the privacy of others.; 59) Schnorr Randomness and Effective Bayesian Consistency and Inconsistency; We study Doob's Consistency Theorem and Freedman's Inconsistency Theorem from
the vantage point of computable probability and algorithmic randomness. We show
that the Schnorr random elements of the parameter space are computably
consistent, when there is a map from the sample space to the parameter space
satisfying many of the same properties as limiting relative frequencies. We
show that the generic inconsistency in Freedman's Theorem is effectively
generic, which implies the existence of computable parameters which are not
computably consistent. Taken together, this work provides a
computability-theoretic solution to Diaconis and Freedman's problem of
``know[ing] for which [parameters] the rule [Bayes' rule] is consistent'', and
it strengthens recent similar results of Takahashi on Martin-L\""of randomness
in Cantor space.; 60) Partial Information Rate Decomposition; Partial Information Decomposition (PID) is a principled and flexible method
to unveil complex high-order interactions in multi-unit network systems. Though
being defined exclusively for random variables, PID is ubiquitously applied to
multivariate time series taken as realizations of random processes with
temporal statistical structure. Here, to overcome the incorrect depiction of
high-order effects by PID schemes applied to dynamic networks, we introduce the
framework of Partial Information Rate Decomposition (PIRD). PIRD is formalized
applying lattice theory to decompose the information shared dynamically between
a target random process and a set of source processes, implemented for Gaussian
processes through a spectral expansion of information rates, and demonstrated
in practice analyzing time series from large-scale climate oscillations.; 61) Influence of the effective mass on the properties of nuclear matter at
  finite density and temperature; Significance of the chiral symmetry restoration is studied by considering the
role of the modification of the nucleon mass in nuclear medium at finite
density and temperature. Using the Korea-IBS-Daegu-SKKU density functional
theory, we can create models that have an identical nuclear matter equation of
state but different isoscalar and isovector effective masses at zero
temperature. Effect of the effective mass becomes transparent at non-zero
temperatures, and it becomes more important as temperature increases. Role of
the effective mass is examined thoroughly by calculating the dependence of
thermodynamic variables such as free energy, internal energy, entropy, pressure
and chemical potential on density, temperature and proton fraction. We find
that sensitivity to the isoscalar effective mass is several times larger than
that of the isovector effective mass, so the uncertainties arising from the
effective mass are dominated by the isoscalar effective mass. In the analysis
of the relative uncertainty, we obtain that the maximum uncertainty is less
than 2% for free energy, internal energy and chemical potential, but it amounts
to 20% for pressure. Entropy shows a behavior completely different from the
other four variables that the uncertainty is about 40% at the saturation
density and increases monotonically as density increases. Effect of the
uncertainty to properties of physical systems is investigated with the
proto-neutron star. It is shown that temperature depends strongly on the
effective mass at a given density and substantial swelling of the radius occurs
due to the finite temperature. Equation of state is stiffer with smaller
isoscalar effective mass, so the effect of the effective mass appears clearly
in the mass-radius relation of the proto-neutron star, larger radius
corresponding to smaller effective mass.; 62) Spectral Eigenfunction Decomposition for Kernel Adaptive Filtering; Kernel adaptive filtering (KAF) integrates traditional linear algorithms with
kernel methods to generate nonlinear solutions in the input space. The standard
approach relies on the representer theorem and the kernel trick to perform
pairwise evaluations of a kernel function in place of the inner product, which
leads to scalability issues for large datasets due to its linear and
superlinear growth with respect to the size of the training data. Explicit
features have been proposed to tackle this problem, exploiting the properties
of the Gaussian-type kernel functions. These approximation methods address the
implicitness and infinite dimensional representation of conventional kernel
methods. However, achieving an accurate finite approximation for the kernel
evaluation requires a sufficiently large vector representation for the dot
products. An increase in the input-space dimension leads to a combinatorial
explosion in the dimensionality of the explicit space, i.e., it trades one
dimensionality problem (implicit, infinite dimensional RKHS) for another (curse
of dimensionality). This paper introduces a construction that simultaneously
solves these two problems in a principled way, by providing an explicit
Euclidean representation of the RKHS while reducing its dimensionality. We
present SPEctral Eigenfunction Decomposition (SPEED) along with an efficient
incremental approach for fast calculation of the dominant kernel eigenbasis,
which enables us to track the kernel eigenspace dynamically for adaptive
filtering. Simulation results on chaotic time series prediction demonstrate
this novel construction outperforms existing explicit kernel features with
greater efficiency.; 63) Utility-Based Dose Optimization Approaches for Multiple-Dose Randomized
  Trial Designs Accounting for Multiple Endpoints; The initiation of dose optimization has driven a paradigm shift in oncology
clinical trials to determine the optimal biological dose (OBD). Early-phase
trials with randomized doses can facilitate additional investigation of the
identified OBD in targeted populations by incorporating safety, efficacy, and
biomarker data. To support dose comparison in such settings, we propose to
extend the utility score-based approach (U-MET) and introduce the clinical
utility index-based approach (CUI-MET) to account for multiple endpoints and
doses. The utility-based dose optimization approach for multiple-dose
randomized trial designs accounting for multiple endpoints and doses (U-MET-m)
extends the U-MET, using a utility score to account for multiple endpoints
jointly (e.g., toxicity-efficacy trade-off), while the CUI-MET uses a utility
index to do this marginally. U-MET-m and CUI-MET use Bayesian inference within
a hypothesis framework to compare utility metrics across doses to identify the
OBD. Here we describe simulation studies and present an example to compare the
U-MET-m design, CUI-MET, and empirical design. The U-MET-m design and CUI-MET
were shown to have satisfactory operating characteristics for selecting the
OBD. Based on these findings, we recommend using the U-MET-m and CUI-MET
designs as the primary dose comparison approach or as supportive evidence to
select the OBD.; 64) A Partial Linear Estimator for Small Study Regression Discontinuity
  Designs; Regression discontinuity (RD) designs are a popular approach to estimating a
treatment effect of cutoff-based interventions. Two current estimation
approaches dominate the literature. One fits separate regressions on either
side of the cutoff, and the other performs finite sample inference based on a
local randomization assumption. Recent developments of these approaches have
often focused on asymptotic properties and large sample sizes. Educational
applications often contain relatively small samples or sparsity near the
cutoff, making estimation more difficult. As an alternative to the
aforementioned approaches, we develop a partial linear estimator for RD
designs. We show in simulations that our estimator outperforms certain leading
estimators in several realistic, small-sample scenarios. We apply our estimator
to school accountability scores in Indiana.; 65) A Population Sampling Framework for Claim Reserving in General Insurance; Claim reserving in insurance has been studied through two primary frameworks:
the macro-level approach, which estimates reserves at an aggregate level (e.g.,
Chain-Ladder), and the micro-level approach, which estimates reserves at the
individual claim level Antonio and Plat (2014). These frameworks are based on
fundamentally different theoretical foundations, creating a degree of
incompatibility that limits the adoption of more flexible models. This paper
introduces a unified statistical framework for claim reserving, grounded in
population sampling theory. We show that macro- and micro-level models
represent extreme yet natural cases of an augmented inverse probability
weighting (AIPW) estimator. This formulation allows for a seamless integration
of principles from both aggregate and individual models, enabling more accurate
and flexible estimations. Moreover, this paper also addresses critical issues
of sampling bias arising from partially observed claims data-an often
overlooked challenge in insurance. By adapting advanced statistical methods
from the sampling literature, such as double-robust estimators, weighted
estimating equations, and synthetic data generation, we improve predictive
accuracy and expand the tools available for actuaries. The framework is
illustrated using Canadian auto insurance data, highlighting the practical
benefits of the sampling-based methods.; 66) Regularized Sparse Optimal Discriminant Clustering; We propose a new method based on sparse optimal discriminant clustering
(SODC), by a penalty term to scoring matrix based on convex clustering. With
the addition of this penalty term, it is expected to improve the accuracy of
cluster identification by attaching points from the same cluster closer
together and points from different clusters further apart. Moreover, we develop
a novel algorithm to derive the updated formula of this scoring matrix using
majorizing function. It solves the difficulty to satisfy both constraint and
containing the clustering structure to the scoring matrix. We have demonstrated
the numerical simulations and its an application to real data to assess the
performance of the proposed method.; 67) A Mathematical Lens for Teaching Data Science; Using the National Academies report, {\em Data Science for Undergraduates:
Opportunities and Options}, we connect data science curricula to the more
familiar pedagogy used by many mathematical scientists. We use their list of
``data acumen"" components to ground a discussion, which hopes to connect data
science curricula to the more familiar pedagogy used by many mathematical
scientists.; 68) In-sample calibration yields conformal calibration guarantees; Conformal predictive systems allow forecasters to issue predictive
distributions for real-valued future outcomes that have out-of-sample
calibration guarantees. On a more abstract level, conformal prediction makes
use of in-sample calibration guarantees to construct bands of predictions with
out-of-sample guarantees under exchangeability. The calibration guarantees are
typically that prediction intervals derived from the predictive distributions
have the correct marginal coverage. We extend this line of reasoning to
stronger notions of calibration that are common in statistical forecasting
theory. We take two prediction methods that are calibrated in-sample, and
conformalize them to obtain conformal predictive systems with stronger
out-of-sample calibration guarantees than existing approaches. The first method
corresponds to a binning of the data, while the second leverages isotonic
distributional regression (IDR), a non-parametric distributional regression
method under order constraints. We study the theoretical properties of these
new conformal predictive systems, and compare their performance in a simulation
experiment. They are then applied to two case studies on European temperature
forecasts and on predictions for the length of patient stay in Swiss intensive
care units. Both approaches are found to outperform existing conformal
predictive systems, while conformal IDR additionally provides a natural method
for quantifying epistemic uncertainty of the predictions.; 69) General relativistic particle trajectories via quantum mechanical weak
  values and the Schwarzschild-Alcubierre spacetime; We show that the average trajectories of relativistic quantum particles in
Schwarzschild spacetime, obtained via quantum mechanical weak measurements of
momentum and energy, are equivalent to the predicted flow lines of probability
current in curved spacetime quantum theory. We subsequently demonstrate that
these trajectories correspond exactly to classical null geodesics in a hybrid
Schwarzschild-Alcubierre spacetime. This threefold equivalence demonstrates how
quantum theory in curved spacetime can be formulated via operationally-defined
measurements, and that such a theory may be interpreted deterministically, in
the spirit of hidden-variable models such as Bohmian mechanics, through the
novel connection to an underlying ""guiding metric.""; 70) A note on local parameter orthogonality for multivariate data and the
  Whittle algorithm for multivariate autoregressive models; This article extends the Cox--Reid local parameter orthogonality to a
multivariate setting, gives an affirmative reply to one of Cox and Reid's
questions, and shows that the extension can lead to efficient computational
algorithms with the celebrated Whittle algorithm for multivariate
autoregressive modeling as a showcase.; 71) Integrative Analysis of High-dimensional RCT and RWD Subject to
  Censoring and Hidden Confounding; In this study, we focus on estimating the heterogeneous treatment effect
(HTE) for survival outcome. The outcome is subject to censoring and the number
of covariates is high-dimensional. We utilize data from both the randomized
controlled trial (RCT), considered as the gold standard, and real-world data
(RWD), possibly affected by hidden confounding factors. To achieve a more
efficient HTE estimate, such integrative analysis requires great insight into
the data generation mechanism, particularly the accurate characterization of
unmeasured confounding effects/bias. With this aim, we propose a
penalized-regression-based integrative approach that allows for the
simultaneous estimation of parameters, selection of variables, and
identification of the existence of unmeasured confounding effects. The
consistency, asymptotic normality, and efficiency gains are rigorously
established for the proposed estimate.
  Finally, we apply the proposed method to estimate the HTE of lobar/sublobar
resection on the survival of lung cancer patients. The RCT is a multicenter
non-inferiority randomized phase 3 trial, and the RWD comes from a clinical
oncology cancer registry in the United States. The analysis reveals that the
unmeasured confounding exists and the integrative approach does enhance the
efficiency for the HTE estimation.; 72) Transmission through rectangular potentials in semimetals featuring
  quadratic dispersion; We revisit the problem of transmission of quasiparticles through a
rectangular potential barrier for semimetals featuring quadratic-in-momentum
band-crossings at a nodal point. Although this was considered in Annals of
Physics 419 (2020) 168235, the solutions corresponding to evanescent waves were
missed, leading to a partial fulfillment of the boundary conditions required to
determine the piecewise-continuous wavefunctions. In this paper, our aim is to
correct those shortcomings, recompute the transmission coefficients, and show
the resulting behaviour of the conductivity and the Fano factor for some
representative parameter values.; 73) Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and
  Uncertainty Based Routing; Model merging aggregates Large Language Models (LLMs) finetuned on different
tasks into a stronger one. However, parameter conflicts between models leads to
performance degradation in averaging. While model routing addresses this issue
by selecting individual models during inference, it imposes excessive storage
and compute costs, and fails to leverage the common knowledge from different
models. In this work, we observe that different layers exhibit varying levels
of parameter conflicts. Building on this insight, we average layers with
minimal parameter conflicts and use a novel task-level expert routing for
layers with significant conflicts. To further reduce storage costs, inspired by
task arithmetic sparsity, we decouple multiple fine-tuned experts into a dense
expert and several sparse experts. Considering the out-of-distribution samples,
we select and merge appropriate experts based on the task uncertainty of the
input data. We conduct extensive experiments on both LLaMA and Qwen with
varying parameter scales, and evaluate on real-world reasoning tasks. Results
demonstrate that our method consistently achieves significant performance
improvements while requiring less system cost compared to existing methods.; 74) Step-by-Step Guide to Conducting Meta-Analysis of Dichotomous Outcomes
  Using RevMan in Dental Research Step-by-Step Guide to Conducting
  Meta-Analysis of Dichotomous Outcomes Using RevMan in Dental Research; Meta-analysis is a statistical method that combines the results of individual
studies on the same topic. This method is becoming popular, due to providing
the combined result that individual studies cannot provide and giving a more
precise result. Despite meta-analysis having such significance, there are few
Korean guides for the use of the Review Manager (RevMan) software. This study
will provide a step-by-step guide, using orthodontic mini-screw as a dental
example, to help researcher carry out meta-analysis more easily and accurately.; 75) Ultra-Low-Latency Edge Intelligent Sensing: A Source-Channel Tradeoff
  and Its Application to Coding Rate Adaptation; The forthcoming sixth-generation (6G) mobile network is set to merge edge
artificial intelligence (AI) and integrated sensing and communication (ISAC)
extensively, giving rise to the new paradigm of edge intelligent sensing
(EI-Sense). This paradigm leverages ubiquitous edge devices for environmental
sensing and deploys AI algorithms at edge servers to interpret the observations
via remote inference on wirelessly uploaded features. A significant challenge
arises in designing EI-Sense systems for 6G mission-critical applications,
which demand high performance under stringent latency constraints. To tackle
this challenge, we focus on the end-to-end (E2E) performance of EI-Sense and
characterize a source-channel tradeoff that balances source distortion and
channel reliability. In this work, we establish a theoretical foundation for
the source-channel tradeoff by quantifying the effects of source coding on
feature discriminant gains and channel reliability on packet loss. Building on
this foundation, we design the coding rate control by optimizing the tradeoff
to minimize the E2E sensing error probability, leading to a low-complexity
algorithm for ultra-low-latency EI-Sense. Finally, we validate our theoretical
analysis and proposed coding rate control algorithm through extensive
experiments on both synthetic and real datasets, demonstrating the sensing
performance gain of our approach with respect to traditional
reliability-centric methods.; 76) OCRT: Boosting Foundation Models in the Open World with
  Object-Concept-Relation Triad; Although foundation models (FMs) claim to be powerful, their generalization
ability significantly decreases when faced with distribution shifts, weak
supervision, or malicious attacks in the open world. On the other hand, most
domain generalization or adversarial fine-tuning methods are task-related or
model-specific, ignoring the universality in practical applications and the
transferability between FMs. This paper delves into the problem of generalizing
FMs to the out-of-domain data. We propose a novel framework, the
Object-Concept-Relation Triad (OCRT), that enables FMs to extract sparse,
high-level concepts and intricate relational structures from raw visual inputs.
The key idea is to bind objects in visual scenes and a set of object-centric
representations through unsupervised decoupling and iterative refinement. To be
specific, we project the object-centric representations onto a semantic concept
space that the model can readily interpret and estimate their importance to
filter out irrelevant elements. Then, a concept-based graph, which has a
flexible degree, is constructed to incorporate the set of concepts and their
corresponding importance, enabling the extraction of high-order factors from
informative concepts and facilitating relational reasoning among these
concepts. Extensive experiments demonstrate that OCRT can substantially boost
the generalizability and robustness of SAM and CLIP across multiple downstream
tasks.; 77) Roadmap for Molecular Benchmarks in Nonadiabatic Dynamics; Simulating the coupled electronic and nuclear response of a molecule to light
excitation requires the application of nonadiabatic molecular dynamics.
However, when faced with a specific photophysical or photochemical problem,
selecting the most suitable theoretical approach from the wide array of
available techniques is not a trivial task. The challenge is further
complicated by the lack of systematic method comparisons and rigorous testing
on realistic molecular systems. This absence of comprehensive molecular
benchmarks remains a major obstacle to advances within the field of
nonadiabatic molecular dynamics. A CECAM workshop, Standardizing Nonadiabatic
Dynamics: Towards Common Benchmarks, was held in May 2024 to address this
issue. This Perspective highlights the key challenges identified during the
workshop in defining molecular benchmarks for nonadiabatic dynamics.
Specifically, this work outlines some preliminary observations on essential
components needed for simulations and proposes a roadmap aiming to establish,
as an ultimate goal, a community-driven, standardized molecular benchmark set.; 78) Sequential Quadratic Optimization for Solving Expectation Equality
  Constrained Stochastic Optimization Problems; A sequential quadratic programming method is designed for solving general
smooth nonlinear stochastic optimization problems subject to expectation
equality constraints. We consider the setting where the objective and
constraint function values, as well as their derivatives, are not directly
available. The algorithm applies an adaptive step size policy and only relies
on objective gradient estimates, constraint function estimates, and constraint
derivative estimates to update iterates. Both asymptotic and non-asymptotic
convergence properties of the algorithm are analyzed. Under reasonable
assumptions, the algorithm generates a sequence of iterates whose first-order
stationary measure diminishes in expectation. In addition, we identify the
iteration and sample complexity for obtaining a first-order
$\varepsilon$-stationary iterate in expectation. The results of numerical
experiments demonstrate the efficiency and efficacy of our proposed algorithm
compared to a penalty method and an augmented Lagrangian method.; 79) Hyperelliptic tangential covers and even elliptic finite-gap potentials,
  back and forth; Let $(X,\omega_0):=(\mathbb{C}/\Lambda,0)$ denote the elliptic curve
associated to the lattice $\Lambda$, $X_2:=\{\omega_0,\cdots, \omega_3\}$ its
set of half-periods and $\wp:X \to \mathbb{P}^1$ the usual Weierstrass $\wp$
function, with a double pole at the origin $\omega_0$. Fix $(\alpha,m)\in
\mathbb{N}^4\times \mathbb{N}$ and consider a function $$u_\xi(x) = \sum_0^3
\alpha_i(\alpha_i+1)\wp(x\,\textrm{-}\,\omega_i) +2\sum_{j=1}^m \left(\wp(x\,
\textrm{-}\, \rho_j)+\wp(x+\rho_j)\right),$$ where $\{\rho_j\} \in (X \setminus
X_2)^{(m)}$. The latter is known to be a so-called (even, $\Lambda$-periodic)
finite-gap potential, if and only if $\{\rho_j\} $ satisfies the so-called
(D-G) square system of equations. We let $\mathcal{P}ot_X(\alpha,m)$ denote the
set of such potentials. Any such potential corresponds to a unique spectral
data $(\pi,\xi)$, where $\pi: \Gamma \to X$ is a hyperelliptic tangential cover
of degree $n:=\frac{1}{2}(\sum_i\alpha_i(\alpha_i+1)+4m)$ and $\xi$ a
$\theta$-characteristic of the spectral curve $\Gamma$. The problem at stake is
to find out all spectral data of the family $\mathcal{P}ot_X(m) :=
\bigcup_{\alpha\in \mathbb{N}^4} \mathcal{P}ot_X(\alpha,m),$ for any $m$. The
latter problem has been thoroughly studied for $\mathcal{P}ot_X(0)$ and
$\mathcal{P}ot_X(1)$. In this article we go one step further, by studying all
spectral data of each family $\mathcal{P}ot_X(\alpha,2)$. We find the bound
$\#\mathcal{P}ot_X(\alpha,2)\leq 27$, for any $\alpha\in \mathbb{N}^4$, with
equality for a generic elliptic curve $X$. We also find a formula for the
arithmetic geni of the corresponding spectral curves in terms of $\alpha$,
which we generalize to $\mathcal{P}ot_X(\alpha,m)$ for any $m$. At last, we
conclude with a natural conjecture, leading to a recursive formula in $d\in
\mathbb{N}$, for the cardinals of $\mathcal{P}ot_X(\alpha,d)$.; 80) On a fast consistent selection of nested models with possibly
  unnormalised probability densities; Models with unnormalized probability density functions are ubiquitous in
statistics, artificial intelligence and many other fields. However, they face
significant challenges in model selection if the normalizing constants are
intractable. Existing methods to address this issue often incur high
computational costs, either due to numerical approximations of normalizing
constants or evaluation of bias corrections in information criteria. In this
paper, we propose a novel and fast selection criterion, T-GIC, for nested
models, allowing direct data sampling from a possibly unnormalized probability
density function. T-GIC gives a consistent selection under mild regularity
conditions and is computationally efficient, benefiting from a multiplying
factor that depends only on the sample size and the model complexity. Extensive
simulation studies and real-data applications demonstrate the efficacy of T-GIC
in the selection of nested models with unnormalized probability densities.; 81) Variational inference for approximate reference priors using neural
  networks; In Bayesian statistics, the choice of the prior can have an important
influence on the posterior and the parameter estimation, especially when few
data samples are available. To limit the added subjectivity from a priori
information, one can use the framework of reference priors. However, computing
such priors is a difficult task in general. We develop in this paper a flexible
algorithm based on variational inference which computes approximations of
reference priors from a set of parametric distributions using neural networks.
We also show that our algorithm can retrieve reference priors when constraints
are specified in the optimization problem to ensure the solution is proper. We
propose a simple method to recover a relevant approximation of the parametric
posterior distribution using Markov Chain Monte Carlo (MCMC) methods even if
the density function of the parametric prior is not known in general. Numerical
experiments on several statistical models of increasing complexity are
presented. We show the usefulness of this approach by recovering the target
distribution. The performance of the algorithm is evaluated on the prior
distributions as well as the posterior distributions, jointly using variational
inference and MCMC sampling.; 82) Constructing optimal dynamic monitoring and treatment regimes: An
  application to hypertension care; Hypertension is a leading cause of cardiovascular diseases and morbidity,
with antihypertensive drugs and blood pressure management strategies having
heterogeneous effects on patients. Previous authors exploited this
heterogeneity to construct optimal dynamic treatment regimes for hypertension
that input patient characteristics and output the best drug or blood pressure
management strategy to prescribe. There is, however, a lack of research on
optimizing monitoring schedules for these patients. It is unclear whether
different monitoring patterns and drug add-on strategies could lower blood
pressure differently across patients. We propose a new consistent methodology
to develop optimal dynamic monitoring and add-on regimes that is doubly-robust
and relies on the theory of Robins' g-methods and dynamic weighted ordinary
least squares. We discuss the treatment of longitudinal missing data for that
inference. The approach is evaluated in large simulation studies and applied to
data from the SPRINT trial in the United States to derive a new optimal rule.
This type of rule could be used by patients or physicians to personalize the
timing of visit and by physicians to decide whether prescribing an
antihypertensive drug is beneficial.; 83) Evaluating and Testing for Actionable Treatment Effect Heterogeneity; Developing tools for estimating heterogeneous treatment effects (HTE) and
individualized treatment effects has been an area of active research in recent
years. While these tools have proven to be useful in many contexts, a concern
when deploying such methods is the degree to which incorporating HTE into a
prediction model provides an advantage over predictive methods which do not
allow for variation in treatment effect across individuals. To address this
concern, we propose a procedure which evaluates the extent to which an HTE
model provides a predictive advantage. Specifically, our procedure targets the
gain in predictive performance from using a flexible predictive model
incorporating HTE versus an alternative model which is similar to the
HTE-utilizing model except that it is constrained to not allow variation in
treatment effect. By drawing upon recent work in using nested cross-validation
techniques for prediction error inference, we generate confidence intervals for
this measure of gain in predictive performance which allows one to directly
calculate the level at which one is confident of a substantial HTE-modeling
gain in prediction -- a quantity which we refer to as the h-value. Our
procedure is generic and can be directly used to assess the benefit of modeling
HTE for any method that incorporates treatment effect variation.; 84) Exoplanet Occurrence Rate with Age for FGK Stars in Kepler; We measure exoplanet occurrence rate as a function of isochrone and
gyrochronology ages using confirmed and candidate planets identified in Q1-17
DR25 Kepler data. We employ Kepler's pipeline detection efficiency to correct
for the expected number of planets in each age bin. We examine the occurrence
rates for planets with radii $0.2 \leq Rp \leq 20$ R$_\oplus$ and orbital
periods $0.2 \leq P \leq 100$ days for FGK stars with ages between $1.5-8$ Gyr
using the inverse detection efficiency method. We find no significant trend
between occurrence rate and stellar ages; a slight, decreasing trend (within
$1.5-2.5$ $\sigma$) only emerges for low-mass and metal-rich stars that
dominate our sample. We isolate the effects of mass and metallicity on the
occurrence rate trend with age, but find the results to be inconclusive due to
weak trends and small sample size. Our results hint that the exoplanet
occurrence rate may decrease over time due to dynamical instability from
planet-planet scattering or planet ejection, but accurate ages and larger
sample sizes are needed to resolve a clear relation between occurrence rate and
age.; 85) Fisher's Randomization Test for Causality with General Types of
  Treatments; Researchers has long been focusing on causal inference with binary or
categorical treatments, where causal estimands are well understood and
inference tools are rich. However, causal problems involving continuous
treatments are common in practice, yet a formal framework is scarce in the
literature. We extend classic Fisher's randomization test to address an initial
question: does a treatment have effect on the outcome of interest conditional
on a set of covariates. Our theory starts from randomized experiments and
generalizes to observational studies. Inference tools are developed to
establish causal relationships and to verify underlying assumptions.; 86) Cultural Alignment in Large Language Models Using Soft Prompt Tuning; Large Language Model (LLM) alignment conventionally relies on supervised
fine-tuning or reinforcement learning based alignment frameworks. These methods
typically require labeled or preference datasets and involve updating model
weights to align the LLM with the training objective or reward model.
Meanwhile, in social sciences such as cross-cultural studies, factor analysis
is widely used to uncover underlying dimensions or latent variables that
explain observed patterns in survey data. The non-differentiable nature of
these measurements deriving from survey data renders the former alignment
methods infeasible for alignment with cultural dimensions. To overcome this, we
propose a parameter efficient strategy that combines soft prompt tuning, which
freezes the model parameters while modifying the input prompt embeddings, with
Differential Evolution (DE), a black-box optimization method for cases where a
differentiable objective is unattainable. This strategy ensures alignment
consistency without the need for preference data or model parameter updates,
significantly enhancing efficiency and mitigating overfitting. Our method
demonstrates significant improvements in LLama-3-8B-Instruct's cultural
dimensions across multiple regions, outperforming both the Naive LLM and the
In-context Learning (ICL) baseline, and effectively bridges computational
models with human cultural nuances.; 87) Linguistic Knowledge Transfer Learning for Speech Enhancement; Linguistic knowledge plays a crucial role in spoken language comprehension.
It provides essential semantic and syntactic context for speech perception in
noisy environments. However, most speech enhancement (SE) methods predominantly
rely on acoustic features to learn the mapping relationship between noisy and
clean speech, with limited exploration of linguistic integration. While
text-informed SE approaches have been investigated, they often require explicit
speech-text alignment or externally provided textual data, constraining their
practicality in real-world scenarios. Additionally, using text as input poses
challenges in aligning linguistic and acoustic representations due to their
inherent differences. In this study, we propose the Cross-Modality Knowledge
Transfer (CMKT) learning framework, which leverages pre-trained large language
models (LLMs) to infuse linguistic knowledge into SE models without requiring
text input or LLMs during inference. Furthermore, we introduce a misalignment
strategy to improve knowledge transfer. This strategy applies controlled
temporal shifts, encouraging the model to learn more robust representations.
Experimental evaluations demonstrate that CMKT consistently outperforms
baseline models across various SE architectures and LLM embeddings,
highlighting its adaptability to different configurations. Additionally,
results on Mandarin and English datasets confirm its effectiveness across
diverse linguistic conditions, further validating its robustness. Moreover,
CMKT remains effective even in scenarios without textual data, underscoring its
practicality for real-world applications. By bridging the gap between
linguistic and acoustic modalities, CMKT offers a scalable and innovative
solution for integrating linguistic knowledge into SE models, leading to
substantial improvements in both intelligibility and enhancement performance.; 88) Magnetic activity evolution of solar-like stars: II. $S_{\rm ph}$-Ro
  evolution of Kepler main-sequence targets; There is now a large sample of stars observed by the Kepler satellite with
measured rotation periods and photometric activity index $S_{\rm ph}$. We use
this data, in conjunction with stellar interiors models, to explore the
interplay of magnetism, rotation, and convection. Stellar activity proxies
other than $S_{\rm ph}$ are correlated with the Rossby number, $Ro$, or ratio
of rotation period to convective overturn timescale. We compute the latter
using the Yale Rotating Evolution Code stellar models. We observe different
$S_{\rm ph}$-$Ro$ relationships for different stellar spectral types. Though
the overall trend of decreasing magnetic activity versus $Ro$ is recovered, we
find a localized dip in $S_{\rm ph}$ around $Ro/Ro_{\odot} \sim$\,0.3 for the G
and K dwarfs. F dwarfs show little to no dependence of $S_{\rm ph}$ on $Ro$ due
to their shallow convective zones; further accentuated as $T_{\rm eff}$
increases. The dip in activity for the G and K dwarfs corresponds to the
intermediate rotation period gap, suggesting that the dip in $S_{\rm ph}$ could
be associated with the redistribution of angular momentum between the core and
convective envelope inside stars. For G-type stars, we observe enhanced
magnetic activity above solar $Ro$. Compared to other Sun-like stars with
similar effective temperature and metallicity, we find that the Sun's current
level of magnetic activity is comparable to its peers and lies near the
transition to increasing magnetic activity at high $Ro$. We confirm that
metal-rich stars have a systematically larger $S_{\rm ph}$ level than
metal-poor stars, which is likely a consequence of their deeper convective
zones.; 89) Versatile Differentially Private Learning for General Loss Functions; This paper aims to provide a versatile privacy-preserving release mechanism
along with a unified approach for subsequent parameter estimation and
statistical inference. We propose the ZIL privacy mechanism based on
zero-inflated symmetric multivariate Laplace noise, which requires no prior
specification of subsequent analysis tasks, allows for general loss functions
under minimal conditions, imposes no limit on the number of analyses, and is
adaptable to the increasing data volume in online scenarios. We derive the
trade-off function for the proposed ZIL mechanism that characterizes its
privacy protection level. Within the M-estimation framework, we propose a novel
doubly random corrected loss (DRCL) for the ZIL mechanism, which provides
consistent and asymptotic normal M-estimates for the parameters of the target
population under differential privacy constraints. The proposed approach is
easy to compute without numerical integration and differentiation for noisy
data. It is applicable for a general class of loss functions, including
non-smooth loss functions like check loss and hinge loss. Simulation studies,
including logistic regression and quantile regression, are conducted to
evaluate the performance of the proposed method.; 90) More on unconstrained descriptions of Higher Spin Massless Particles; Here we suggest a new local action describing arbitrary integer spin-$s$
massless particles in terms of only two symmetric fields $\varphi $ and
$\alpha$ of rank-$s$ and $(s-3)$ respectively. It is an unconstrained version
of the Fronsdal theory where the double traceless constraint on the physical
field is evaded via a rank-$(s-4)$ Weyl like symmetry. The constrained higher
spin diffeomorphism is enlarged to full diffeomorphism via the Stueckelberg
field $\alpha$ through an appropriate field redefinition. After a partial gauge
fixing where the Weyl symmetry is broken while preserving diffeomorphisms, the
field equations reproduce, for arbitrary integer spin-$s$, diffeomorphism
invariant equations of motion previously obtained via a truncation of the
spectrum of the open bosonic string field theory in the tensionless limit. In
the $s=4$ case we show that the functional integration over $\alpha$ leads to a
unique non local Weyl and diffeomorphism invariant action given only in terms
of the physical field $\varphi$ whose spectrum is confirmed via an analysis of
the analytic structure of the spin-4 propagator for which we introduce a
complete basis of projection and transition non local differential operators.
We also show that the elimination of $\alpha$ after the Weyl gauge fixing leads
to a non local diffeomorphism invariant action previously obtained in the
literature.; 91) Castelnuovo-Mumford Regularity over Scrolls and Splitting Criteria; We introduce and study a notion of Castelnuovo-Mumford regularity suitable
for scrolls obtained as projectivisations of sums of line bundles on $\mathbb
P^m$. We show that this is a natural generalisation of the well known
regularity on projective and multiprojective spaces and we prove Horrocks-type
splitting criteria for vector bundles.; 92) Novel echoes from black holes in conformal Weyl gravity; We reveal a novel class of echoes from black holes in conformal Weyl gravity
and show that they are generated due to the large-scale structure of the
cosmos, rather than near-horizon modifications of black holes as well as
wormhole spacetimes. To this end, we take into account the evolution of a
massive scalar perturbation on the background geometry of conformal Weyl black
holes and show that the corresponding effective potential enjoys a double-peak
barrier against the incident scalar waves. We perform the calculations for the
time evolution profiles of scalar perturbations to understand how the linear
term in the metric function and the cosmological constant produce echoes. The
Prony method is also employed to calculate the quasinormal frequencies of the
early-stage quasinormal ringing phase.; 93) Recent advances in doubly-robust weighted ordinary least squares
  techniques for dynamic treatment regime estimation; A dynamic treatment regime (DTR) is an approach to delivering precision
medicine that uses patient characteristics to guide treatment decisions for
optimal health outcomes. Numerous methods have been proposed for DTR
estimation, including dynamic weighted ordinary least squares (dWOLS), a
regression-based approach that affords double robustness to model
misspecification within an easy to implement analytical framework. Initially,
the dWOLS approach was developed under the assumptions of continuous outcomes
and binary treatment decisions. Motivated by clinical research, subsequent
theoretical advancements have extended the dWOLS framework to address binary,
continuous and multicategory treatments across various outcome types, including
binary, continuous, and survival-type. However, certain scenarios remain
unexplored. This paper summarizes the last ten years of extension and
application of the dWOLS method, providing a comprehensive and detailed review
of the original dWOLS method and its extensions, as well as highlighting its
diverse practical applications. We also explore studies that have addressed
challenges associated with dWOLS implementation, such as model validation,
variable selection, and handling measurement errors. Using simulated data, we
present numerical illustrations along with step-by-step implementations in the
\texttt{R} environment to facilitate a deeper understanding of dWOLS-based DTR
estimation methodologies.; 94) Outcome-Informed Weighting for Robust ATE Estimation; Reliable causal effect estimation from observational data requires adjustment
for confounding and sufficient overlap in covariate distributions between
treatment groups. However, in high-dimensional settings, lack of overlap often
inflates the variance and weakens the robustness of inverse propensity score
weighting (IPW) based estimators. Although many approaches that rely on
covariate adjustment have been proposed to mitigate these issues, we instead
shift the focus to the outcome space. In this paper, we introduce the Augmented
Marginal outcome density Ratio (AMR) estimator, an outcome-informed weighting
method that naturally filters out irrelevant information, alleviates practical
positivity violations and outperforms standard augmented IPW and covariate
adjustment-based methods in terms of both efficiency and robustness.
Additionally, by eliminating the need for strong a priori assumptions, our
post-hoc calibration framework is also effective in settings with
high-dimensional covariates. We present experimental results on synthetic data,
the NHANES dataset and text applications, demonstrating the robustness of AMR
and its superior performance under weak overlap and high-dimensional
covariates.; 95) A Generative System for Robot-to-Human Handovers: from Intent Inference
  to Spatial Configuration Imagery; We propose a novel system for robot-to-human object handover that emulates
human coworker interactions. Unlike most existing studies that focus primarily
on grasping strategies and motion planning, our system focus on 1. inferring
human handover intents, 2. imagining spatial handover configuration. The first
one integrates multimodal perception-combining visual and verbal cues-to infer
human intent. The second one using a diffusion-based model to generate the
handover configuration, involving the spacial relationship among robot's
gripper, the object, and the human hand, thereby mimicking the cognitive
process of motor imagery. Experimental results demonstrate that our approach
effectively interprets human cues and achieves fluent, human-like handovers,
offering a promising solution for collaborative robotics. Code, videos, and
data are available at: https://i3handover.github.io.; 96) Subbagging Variable Selection for Big Data; This article introduces a subbagging (subsample aggregating) approach for
variable selection in regression within the context of big data. The proposed
subbagging approach not only ensures that variable selection is scalable given
the constraints of available computational resources, but also preserves the
statistical efficiency of the resulting estimator. In particular, we propose a
subbagging loss function that aggregates the least-squares approximations of
the loss function for each subsample. Subsequently, we penalize the subbagging
loss function via an adaptive LASSO-type regularizer, and obtain a regularized
estimator to achieve variable selection. We then demonstrate that the
regularized estimator exhibits $\sqrt{N}$-consistency and possesses the oracle
properties, where $N$ represents the size of the full sample in the big data.
In addition, we propose a subbagging Bayesian information criterion to select
the regularization parameter, ensuring that the regularized estimator achieves
selection consistency. Simulation experiments are conducted to demonstrate the
numerical performance. A U.S. census dataset is analyzed to illustrate the
usefulness and computational scalability of the subbagging variable selection
method.; 97) A Generalized Fr\'echet Test for Object Data with Unequal Repeated
  Measurements; Advancements in data collection have led to increasingly common repeated
observations with complex structures in biomedical studies. Treating these
observations as random objects, rather than summarizing features as vectors,
avoids feature extraction and better reflects the data's nature. Examples
include repeatedly measured activity intensity distributions in physical
activity analysis and brain networks in neuroimaging. Testing whether these
repeated random objects differ across groups is fundamentally important;
however, traditional statistical tests often face challenges due to the
non-Euclidean nature of metric spaces, dependencies from repeated measurements,
and the unequal number of repeated measures. By defining within-subject
variability using pairwise distances between repeated measures and extending
Fr\'echet analysis of variance, we develop a generalized Fr\'echet test for
exchangeable repeated random objects, applicable to general metric space-valued
data with unequal numbers of repeated measures. The proposed test can
simultaneously detect differences in location, scale, and within-subject
variability. We derive the asymptotic distribution of the test statistic, which
follows a weighted chi-squared distribution. Simulations demonstrate that the
proposed test performs well across different types of random objects. We
illustrate its effectiveness through applications to physical activity data and
resting-state functional magnetic resonance imaging data.; 98) Data-Driven Adjustment for Multiple Treatments; Covariate adjustment is one method of causal effect identification in
non-experimental settings. Prior research provides routes for finding
appropriate adjustments sets, but much of this research assumes knowledge of
the underlying causal graph. In this paper, we present two routes for finding
adjustment sets that do not require knowledge of a graph -- and instead rely on
dependencies and independencies in the data directly. We consider a setting
where the adjustment set is unaffected by treatment or outcome. Our first route
shows how to extend prior research in this area using a concept known as
c-equivalence. Our second route provides sufficient criteria for finding
adjustment sets in the setting of multiple treatments.; 99) Semiparametric Inference for Partially Identifiable Data Fusion
  Estimands via Double Machine Learning; Many statistical estimands of interest (e.g., in regression or causality) are
functions of the joint distribution of multiple random variables. But in some
applications, data is not available that measures all random variables on each
subject, and instead the only possible approach is one of data fusion, where
multiple independent data sets, each measuring a subset of the random variables
of interest, are combined for inference. In general, since all random variables
are never observed jointly, their joint distribution, and hence also the
estimand which is a function of it, is only partially identifiable.
Unfortunately, the endpoints of the partially identifiable region depend in
general on entire conditional distributions, rendering them hard both
operationally and statistically to estimate. To address this, we present a
novel outer-bound on the region of partial identifiability (and establish
conditions under which it is tight) that depends only on certain conditional
first and second moments. This allows us to derive semiparametrically efficient
estimators of our endpoint outer-bounds that only require the standard machine
learning toolbox which learns conditional means. We prove asymptotic normality
and semiparametric efficiency of our estimators and provide consistent
estimators of their variances, enabling asymptotically valid confidence
interval construction for our original partially identifiable estimand. We
demonstrate the utility of our method in simulations and a data fusion problem
from economics.; 100) Mitigating Object Hallucinations in MLLMs via Multi-Frequency
  Perturbations; Recently, multimodal large language models (MLLMs) have demonstrated
remarkable performance in visual-language tasks. However, the authenticity of
the responses generated by MLLMs is often compromised by object hallucinations.
We identify that a key cause of these hallucinations is the model's
over-susceptibility to specific image frequency features in detecting objects.
In this paper, we introduce Multi-Frequency Perturbations (MFP), a simple,
cost-effective, and pluggable method that leverages both low-frequency and
high-frequency features of images to perturb visual feature representations and
explicitly suppress redundant frequency-domain features during inference,
thereby mitigating hallucinations. Experimental results demonstrate that our
method significantly mitigates object hallucinations across various model
architectures. Furthermore, as a training-time method, MFP can be combined with
inference-time methods to achieve state-of-the-art performance on the CHAIR
benchmark.",0.0,0.6309297535714575
2411.0064,applied,2411.0064-pos2-3,"Quantifying Variance in Evaluation Benchmarks; Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models.",2411.0064-pos1-3,"The Llama 3 Herd of Models; Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",26,"['2', '4', '5', '7', '8', '9', '12', '6', '1', '3']","The main paper focuses on evaluating benchmarks for large language models (LLMs) and understanding variance in performance metrics. The top candidate paper can contribute by providing a methodology for consistent model selection under challenging conditions, which can enhance the understanding of variances and improve the reliability of comparisons in LLM evaluations. The second candidate focuses on model selection in AI, which aligns well with the theme of benchmarking and evaluation discussed in the main paper, as it addresses issues related to model performance in the context of unnormalized probability densities. The remaining papers provide various relevant methodologies, but their direct link to the main paper's theme of evaluating model performance and variance decreases as we move down the list.","1) Optical gravitational waves as signals of Gravitationally-Decaying
  Particles; Long-lived heavy particles present during the big bang could have a decay
channel opened by gravitons. Such decays can produce gravitational waves with
large enough abundance to be detectable, and a peculiar narrow spectrum peaked
today around optical frequencies. We identify which particles can decay in one
or two gravitons. The maximal gravitational wave abundance arises from theories
with extra hidden strong gauge dynamics, such as a confining pure-glue group.
An interesting abundance also arises in theories with perturbative couplings.
Future observation might shed light on early cosmology and allow some
spectroscopy of sub-Planckian gravitationally-decaying particles, plausibly
present in a variety of theories such as gauge unification, supersymmetry,
extra dimensions, strings.; 2) On a fast consistent selection of nested models with possibly
  unnormalised probability densities; Models with unnormalized probability density functions are ubiquitous in
statistics, artificial intelligence and many other fields. However, they face
significant challenges in model selection if the normalizing constants are
intractable. Existing methods to address this issue often incur high
computational costs, either due to numerical approximations of normalizing
constants or evaluation of bias corrections in information criteria. In this
paper, we propose a novel and fast selection criterion, T-GIC, for nested
models, allowing direct data sampling from a possibly unnormalized probability
density function. T-GIC gives a consistent selection under mild regularity
conditions and is computationally efficient, benefiting from a multiplying
factor that depends only on the sample size and the model complexity. Extensive
simulation studies and real-data applications demonstrate the efficacy of T-GIC
in the selection of nested models with unnormalized probability densities.; 3) Modular Photobioreactor Fa\c{c}ade Systems for Sustainable Architecture:
  Design, Fabrication, and Real-Time Monitoring; This paper proposes an innovative solution to the growing issue of greenhouse
gas emissions: a closed photobioreactor (PBR) fa\c{c}ade system to mitigate
greenhouse gas (GHG) concentrations. With digital fabrication technology, this
study explores the transition from traditional, single function building
facades to multifunctional, integrated building systems. It introduces a
photobioreactor (PBR) fa\c{c}ade system to mitigate greenhouse gas (GHG)
concentrations while addressing the challenge of large-scale prefabricated
components transportation. This research introduces a novel approach by
designing the fa\c{c}ade system as modular, user-friendly and
transportation-friendly bricks, enabling the creation of a user-customized and
self-assembled photobioreactor (PBR) system. The single module in the system is
proposed to be ""neutralization bricks"", which embedded with algae and equipped
with an air circulation system, facilitating the photobioreactor (PBR)'s
functionality. A connection system between modules allows for easy assembly by
users, while a limited variety of brick styles ensures modularity in
manufacturing without sacrificing customization and diversity. The system is
also equipped with an advanced microalgae status detection algorithm, which
allows users to monitor the condition of the microalgae using monocular camera.
This functionality ensures timely alerts and notifications for users to replace
the algae, thereby optimizing the operational efficiency and sustainability of
the algae cultivation process.; 4) Select2Drive: Pragmatic Communications for Real-Time Collaborative
  Autonomous Driving; Vehicle-to-Everything communications-assisted Autonomous Driving (V2X-AD) has
witnessed remarkable advancements in recent years, with pragmatic
communications (PragComm) emerging as a promising paradigm for real-time
collaboration among vehicles and other agents.Simultaneously, extensive
research has explored the interplay between collaborative perception and
decision-making in end-to-end driving frameworks.In this work, we revisit the
collaborative driving problem and propose the Select2Drive framework to
optimize the utilization of limited computational and communication
resources.Particularly, to mitigate cumulative latency in perception and
decision-making, Select2Drive introduces Distributed Predictive Perception
(DPP) by formulating an active prediction paradigm and simplifies
high-dimensional semantic feature prediction into computation cost-efficient,
motion-aware reconstruction. Given the ""less is more"" principle that a
broadened perceptual horizon possibly confuses the decision module rather than
contributing to it, Select2Drive utilizes Area-of-Importance-based PragComm
(APC) to prioritize the communications of critical regions, thus boosting both
communication efficiency and decision-making efficacy. Empirical evaluations on
the V2Xverse dataset and CARLA driving simulator demonstrate that Select2Drive
achieves a 11.31% (resp. 7.69%) improvement in offline perception tasks under
limited bandwidth (resp. pose error conditions). Moreover, it delivers at most
14.68% and 31.76% enhancement in closed-loop driving scores and route
completion rates, particularly in scenarios characterized by dense traffic and
high-speed dynamics.; 5) Versatile Differentially Private Learning for General Loss Functions; This paper aims to provide a versatile privacy-preserving release mechanism
along with a unified approach for subsequent parameter estimation and
statistical inference. We propose the ZIL privacy mechanism based on
zero-inflated symmetric multivariate Laplace noise, which requires no prior
specification of subsequent analysis tasks, allows for general loss functions
under minimal conditions, imposes no limit on the number of analyses, and is
adaptable to the increasing data volume in online scenarios. We derive the
trade-off function for the proposed ZIL mechanism that characterizes its
privacy protection level. Within the M-estimation framework, we propose a novel
doubly random corrected loss (DRCL) for the ZIL mechanism, which provides
consistent and asymptotic normal M-estimates for the parameters of the target
population under differential privacy constraints. The proposed approach is
easy to compute without numerical integration and differentiation for noisy
data. It is applicable for a general class of loss functions, including
non-smooth loss functions like check loss and hinge loss. Simulation studies,
including logistic regression and quantile regression, are conducted to
evaluate the performance of the proposed method.; 6) Sampling from Density power divergence-based Generalized posterior
  distribution via Stochastic optimization; Robust Bayesian inference using density power divergence (DPD) has emerged as
a promising approach for handling outliers in statistical estimation. While the
DPD-based posterior offers theoretical guarantees for robustness, its practical
implementation faces significant computational challenges, particularly for
general parametric models with intractable integral terms. These challenges
become especially pronounced in high-dimensional settings where traditional
numerical integration methods prove inadequate and computationally expensive.
We propose a novel sampling methodology that addresses these limitations by
integrating the loss-likelihood bootstrap with a stochastic gradient descent
algorithm specifically designed for DPD-based estimation. Our approach enables
efficient and scalable sampling from DPD-based posteriors for a broad class of
parametric models, including those with intractable integrals, and we further
extend it to accommodate generalized linear models. Through comprehensive
simulation studies, we demonstrate that our method efficiently samples from
DPD-based posteriors, offering superior computational scalability compared to
conventional methods, particularly in high-dimensional settings. The results
also highlight its ability to handle complex parametric models with intractable
integral terms.; 7) Compositional Instruction Following with Language Models and
  Reinforcement Learning; Combining reinforcement learning with language grounding is challenging as
the agent needs to explore the environment while simultaneously learning
multiple language-conditioned tasks. To address this, we introduce a novel
method: the compositionally-enabled reinforcement learning language agent
(CERLLA). Our method reduces the sample complexity of tasks specified with
language by leveraging compositional policy representations and a semantic
parser trained using reinforcement learning and in-context learning. We
evaluate our approach in an environment requiring function approximation and
demonstrate compositional generalization to novel tasks. Our method
significantly outperforms the previous best non-compositional baseline in terms
of sample complexity on 162 tasks designed to test compositional
generalization. Our model attains a higher success rate and learns in fewer
steps than the non-compositional baseline. It reaches a success rate equal to
an oracle policy's upper-bound performance of 92%. With the same number of
environment steps, the baseline only reaches a success rate of 80%.; 8) Variational phylogenetic inference with products over bipartitions; Bayesian phylogenetics requires accurate and efficient approximation of
posterior distributions over trees. In this work, we develop a variational
Bayesian approach for ultrametric phylogenetic trees. We present a novel
variational family based on coalescent times of a single-linkage clustering and
derive a closed-form density of the resulting distribution over trees. Unlike
existing methods for ultrametric trees, our method performs inference over all
of tree space, it does not require any Markov chain Monte Carlo subroutines,
and our variational family is differentiable. Through experiments on benchmark
genomic datasets and an application to SARS-CoV-2, we demonstrate that our
method achieves competitive accuracy while requiring significantly fewer
gradient evaluations than existing state-of-the-art techniques.; 9) Singular Spectrum Analysis of Time-series Data from Time-dependent
  density-functional theory in Real-time; This paper introduces a spectral analysis of time-seires data derived from
real-time time-dependent density functional theory (TDDFT) using Singular
Spectrum Analysis (SSA). TDDFT is a robust method for obtaining molecular
excited states and optical spectra by tracking the time evolution of dynamical
dipole moments. However, the spectral resolution can be compromised when
Fourier transformation's total time duration is insufficient. SSA enabled the
extraction of specific oscillation components from the time-series data,
facilitating the generation of higher-precision spectra. Even with relatively
short time-series dataset, the predictive extension of SSA yielded
high-resolution spectra, demonstrating substantial agreement with results
obtained through conventional methods. The efficacy of this approach was
validated for several small molecules, including ethylene, benzene, and others.
SSA's ability to conduct detailed spectral anasysis in specific energy regions
enhance spectral resolution and facilitates the clarification of oscillation
components within these regions. Real-time TDDFT combined with SSA provides a
new analytical method for analyzing the optical properties of molecules,
significantly improving the accuracy of the analysis of emission and absorption
spectra analysis. This method is expected to have various applications.; 10) FragFM: Efficient Fragment-Based Molecular Generation via Discrete Flow
  Matching; We introduce FragFM, a novel fragment-based discrete flow matching framework
for molecular graph generation.FragFM generates molecules at the fragment
level, leveraging a coarse-to-fine autoencoding mechanism to reconstruct
atom-level details. This approach reduces computational complexity while
maintaining high chemical validity, enabling more efficient and scalable
molecular generation. We benchmark FragFM against state-of-the-art diffusion-
and flow-based models on standard molecular generation benchmarks and natural
product datasets, demonstrating superior performance in validity, property
control, and sampling efficiency. Notably, FragFM achieves over 99\% validity
with significantly fewer sampling steps, improving scalability while preserving
molecular diversity. These results highlight the potential of fragment-based
generative modeling for large-scale, property-aware molecular design, paving
the way for more efficient exploration of chemical space.; 11) A Generalized Fr\'echet Test for Object Data with Unequal Repeated
  Measurements; Advancements in data collection have led to increasingly common repeated
observations with complex structures in biomedical studies. Treating these
observations as random objects, rather than summarizing features as vectors,
avoids feature extraction and better reflects the data's nature. Examples
include repeatedly measured activity intensity distributions in physical
activity analysis and brain networks in neuroimaging. Testing whether these
repeated random objects differ across groups is fundamentally important;
however, traditional statistical tests often face challenges due to the
non-Euclidean nature of metric spaces, dependencies from repeated measurements,
and the unequal number of repeated measures. By defining within-subject
variability using pairwise distances between repeated measures and extending
Fr\'echet analysis of variance, we develop a generalized Fr\'echet test for
exchangeable repeated random objects, applicable to general metric space-valued
data with unequal numbers of repeated measures. The proposed test can
simultaneously detect differences in location, scale, and within-subject
variability. We derive the asymptotic distribution of the test statistic, which
follows a weighted chi-squared distribution. Simulations demonstrate that the
proposed test performs well across different types of random objects. We
illustrate its effectiveness through applications to physical activity data and
resting-state functional magnetic resonance imaging data.; 12) LevelRAG: Enhancing Retrieval-Augmented Generation with Multi-hop Logic
  Planning over Rewriting Augmented Searchers; Retrieval-Augmented Generation (RAG) is a crucial method for mitigating
hallucinations in Large Language Models (LLMs) and integrating external
knowledge into their responses. Existing RAG methods typically employ query
rewriting to clarify the user intent and manage multi-hop logic, while using
hybrid retrieval to expand search scope. However, the tight coupling of query
rewriting to the dense retriever limits its compatibility with hybrid
retrieval, impeding further RAG performance improvements. To address this
challenge, we introduce a high-level searcher that decomposes complex queries
into atomic queries, independent of any retriever-specific optimizations.
Additionally, to harness the strengths of sparse retrievers for precise keyword
retrieval, we have developed a new sparse searcher that employs Lucene syntax
to enhance retrieval accuracy.Alongside web and dense searchers, these
components seamlessly collaborate within our proposed method,
\textbf{LevelRAG}. In LevelRAG, the high-level searcher orchestrates the
retrieval logic, while the low-level searchers (sparse, web, and dense) refine
the queries for optimal retrieval. This approach enhances both the completeness
and accuracy of the retrieval process, overcoming challenges associated with
current query rewriting techniques in hybrid retrieval scenarios. Empirical
experiments conducted on five datasets, encompassing both single-hop and
multi-hop question answering tasks, demonstrate the superior performance of
LevelRAG compared to existing RAG methods. Notably, LevelRAG outperforms the
state-of-the-art proprietary model, GPT4o, underscoring its effectiveness and
potential impact on the RAG field.; 13) State Fourier Diffusion Language Model (SFDLM): A Scalable, Novel
  Iterative Approach to Language Modeling; In recent years, diffusion based methods have emerged as a powerful paradigm
for generative modeling. Although discrete diffusion for natural language
processing has been explored to a lesser extent, it shows promise for tasks
requiring iterative denoising of token based data. In standard approaches to
text generation, transformers dominate, but their reliance on self attention
often incurs high computational costs. This paper introduces a fully diffusion
driven discrete text generation model built without any transformer or large
convolution modules. Instead, the model integrates structured state space
dynamics in the time domain with a novel Complex Fourier Multi Layer Perceptron
module that operates in the frequency domain. The forward noising process
randomly samples the vocabulary to replace tokens with a controlled
probability, while the learned reverse model systematically reverts corrupted
sequences toward their original states. By composing local state space updates
with global Fourier based mixing, the approach effectively captures both short
and long range dependencies.; 14) Fisher's Randomization Test for Causality with General Types of
  Treatments; Researchers has long been focusing on causal inference with binary or
categorical treatments, where causal estimands are well understood and
inference tools are rich. However, causal problems involving continuous
treatments are common in practice, yet a formal framework is scarce in the
literature. We extend classic Fisher's randomization test to address an initial
question: does a treatment have effect on the outcome of interest conditional
on a set of covariates. Our theory starts from randomized experiments and
generalizes to observational studies. Inference tools are developed to
establish causal relationships and to verify underlying assumptions.; 15) High Harmonic Generation from a Noble Metal; High-harmonic generation (HHG) in solids has typically been explored in
transparent dielectrics and semiconductors. Metals have long been dismissed due
to their strong reflectivity at infrared wavelengths. Here, we demonstrate HHG
from silver - a noble metal - using few-cycle near-infrared laser pulses at
near-normal incidence. Our results show that sub-cycle electron dynamics within
the material's penetration depth can drive high-order harmonics, challenging
the prevailing notion that metals are unsuited for infrared-driven strong-field
processes. Despite silver's high reflectivity and large free-electron density,
we observe nonperturbative harmonics extending into the extreme ultraviolet (up
to 20 eV). Moreover, silver's multi-shot damage threshold proves surprisingly
high (30 TW/cm^2) - comparable to large-bandgap dielectrics like magnesium
oxide - thereby enabling intense strong-field processes in a metallic
environment. Measuring the orientation dependence of the emitted harmonics
reveals that the process arises from coherent electron dynamics in the crystal
lattice, rather than from a plasma-driven mechanism. Time-dependent
density-matrix simulations based on maximally localized Wannier functions show
that low-order harmonics predominantly originate from conduction electrons near
the Fermi surface (s- and p-type orbitals), whereas higher harmonics rely on
bound d-electron excitations. These findings establish metals - long thought
unfavorable for HHG - as a promising platform for ultrafast strong-field
physics, extending high-harmonic spectroscopy to regimes in which lattice order
and plasma formation directly intersect. This work expands the frontier of
solid-state HHG to all-metallic attosecond pulse generation and underscores the
potential of metals as robust XUV sources for advanced attosecond metrology.; 16) Understanding Generalization in Physics Informed Models through Affine
  Variety Dimensions; In recent years, physics-informed machine learning has gained significant
attention for its ability to enhance statistical performance and sample
efficiency by integrating physical structures into machine learning models.
These structures, such as differential equations, conservation laws, and
symmetries, serve as inductive biases that can improve the generalization
capacity of the hybrid model. However, the mechanisms by which these physical
structures enhance generalization capacity are not fully understood, limiting
the ability to guarantee the performance of the models. In this study, we show
that the generalization performance of linear regressors incorporating
differential equation structures is determined by the dimension of the
associated affine variety, rather than the number of parameters. This finding
enables a unified analysis of various equations, including nonlinear ones. We
introduce a method to approximate the dimension of the affine variety and
provide experimental evidence to validate our theoretical insights.; 17) Testing Equality of Medians for Multiple Samples; In this paper, we construct a consistent non-parametric test for testing the
equality of population medians for different samples when the observations in
each sample are independent and identically distributed. This test can be
further used to test the equality of unknown location parameters for different
samples. The method discussed in this paper can be extended to any quantile
level instead of the median. We present the theoretical results and also
demonstrate the performance of this test through simulation studies.; 18) Flexible generation of optomagnonic quantum entanglement and quantum
  coherence difference in double-cavity-optomagnomechanical system; Quantum entanglement and quantum coherence generated from the
optomagnomechanical system are important resources in quantum information and
quantum computation. In this paper, a scheme for flexibly generating
optomagnonic quantum entanglement and quantum coherence difference is proposed,
based on a double-cavity-optomagnomechanical system. The parameter dependencies
of the bipartite optomagnonic entanglement, the genuine tripartite optomagnonic
entanglement, the quantum coherence difference, and the stability of the
system, are investigated intensively. The results show that this scheme endows
the magnon more flexibility to choose different mechanisms, under the condition
of maintaining the system stable. This work is valuable for connecting
different nodes in quantum networks and manipulating the magnon states with
light in the future.; 19) Comparative analysis and practical applications of cubic transmutations
  for the Pareto distribution; Transmutation is a technique for extending classical probability
distributions in order to give them more flexibility. In this paper, we are
interested in cubic transmutations of the Pareto distribution. We establish a
general formula that unifies existing cubic transmutations of the Pareto
distribution and facilitates the derivation of new cubic transmutations that
have not yet been explored in the literature. We also derive general formulas
for the related mathematical properties. Finally, we perform a comparative
analysis of the six transmutations existing in the literature using real-world
data. The results obtained confirm the flexibility and effectiveness of cubic
transmutations in modeling various types of data.; 20) Constructing optimal dynamic monitoring and treatment regimes: An
  application to hypertension care; Hypertension is a leading cause of cardiovascular diseases and morbidity,
with antihypertensive drugs and blood pressure management strategies having
heterogeneous effects on patients. Previous authors exploited this
heterogeneity to construct optimal dynamic treatment regimes for hypertension
that input patient characteristics and output the best drug or blood pressure
management strategy to prescribe. There is, however, a lack of research on
optimizing monitoring schedules for these patients. It is unclear whether
different monitoring patterns and drug add-on strategies could lower blood
pressure differently across patients. We propose a new consistent methodology
to develop optimal dynamic monitoring and add-on regimes that is doubly-robust
and relies on the theory of Robins' g-methods and dynamic weighted ordinary
least squares. We discuss the treatment of longitudinal missing data for that
inference. The approach is evaluated in large simulation studies and applied to
data from the SPRINT trial in the United States to derive a new optimal rule.
This type of rule could be used by patients or physicians to personalize the
timing of visit and by physicians to decide whether prescribing an
antihypertensive drug is beneficial.; 21) Holomorphic mappings and their fixed points on Spin Factors; In this paper we study holomorphic properties of infinite dimensional spin
factors.
  Among the infinite dimensional Banach spaces with homogeneous open unit
balls, we show that the spin factors are natural outlier spaces in which to ask
the question (as was proved in the early 1970s for Hilbert spaces): Do
biholomorphic automorphisms $g$ of the open unit ball $B$ have fixed points in
$\overline B$? In this paper, for infinite dimensional spin factors, we provide
reasonable conditions on $g$ that allow us to explicitly construct fixed points
of $g$ lying on $\partial B$. En route, we also prove that every spin factor
has the density property.
  In another direction, we focus on (compact) holomorphic maps $f:B\rightarrow
B$, having no fixed point in $B$ and examine the sequence of iterates $(f^n)$.
As $(f^n)$ does not generally converge, we instead trace the target set $T(f)$
of $f$, that is, the images of all accumulation points of $(f^n)_n$, for any
topology finer than the topology of pointwise convergence on B. We prove for a
spin factor that $T(f)$ lies on the boundary of a single bidisc unique to $f$.; 22) Remarks on ghost resonances; In this paper we study various aspects of ghost resonances: the resummation
that leads to the dressed propagator, the poles locations, the analytic
continuation into the second Riemann sheet and the spectral representations in
both first and second sheets. In particular, we show that for real masses above
the multiparticle threshold the ghost propagator has a pair of complex
conjugate poles in the first sheet, unlike the case of an ordinary unstable
resonance which has no pole in the first sheet but a complex conjugate pair in
the second sheet. Mathematical and physical implications of this feature are
discussed. We also clarify an important point regarding the two absorptive
contributions of a ghost propagator in the narrow-width approximation.
Furthermore, we argue that finite-time quantum field theories are needed to
consistently derive the dressed ghost propagator and capture the true physical
properties of ghost resonances. Throughout the work, different prescriptions to
define the ghost propagator on the real axis are considered: Feynman,
anti-Feynman and fakeon prescriptions.; 23) Score-Based Causal Discovery with Temporal Background Information; Temporal background information can improve causal discovery algorithms by
orienting edges and identifying relevant adjustment sets. We develop the
Temporal Greedy Equivalence Search (TGES) algorithm and terminology essential
for score-based causal discovery with tiered background knowledge. TGES learns
a restricted Markov equivalence class of directed acyclic graphs (DAGs) using
observational data and tiered background knowledge. To construct TGES we
formulate a scoring criterion that accounts for tiered background knowledge. We
establish theoretical results for TGES, stating that the algorithm always
returns a tiered maximally oriented partially directed acyclic graph (tiered
MPDAG) and that this tiered MPDAG contains the true DAG in the large sample
limit. We present a simulation study indicating a gain from using tiered
background knowledge and an improved precision-recall trade-off compared to the
temporal PC algorithm. We provide a real-world example on life-course health
data.; 24) A totally non-compensatory multi-criteria method for evaluating and
  improving level of satisfaction (LoS): proposal and application on Airport
  Terminal of Passengers; To evaluate and assign a service according customer's level of satisfaction
(LoS) is a relevant issue in operations management. This is a typical situation
in which the evaluators, have passed by heterogeneous experiences along their
life which implies they could consider different variables when evaluating a
product. Despite it, the models for measuring Los usually consider a
homogeneous set of criteria when facing LoS evaluation. This study applies a
totally non-compensatory modeling that allows each customer to select the
criteria, from a whole set of aspects, the customer wants to use for evaluating
LoS. The proposal was tested in evaluating LoS regarding the services provided
by Airport Terminal of Passengers (ATPs) in Brazil, with data collected in a
survey involving 19,240 passengers, interviewed at 15 Brazilian international
airports. The data collected was imputed into ELECTRE TRI ME algorithm to
obtain the a credibility degree of sorting the instances. The values of
credibility degree were them used to obtain groups of ATPs. Finally, the
statistical modes of the evaluations in each group were analyzed and compared.
The proposal allowed a full non-compensatory approach to obtain the credibility
degree even when considering perceptions from several evaluators that could use
different criteria. As a result, it was identified, for each cluster of ATP,
the criteria sets to be improved and even those to be prioritized. The pioneer
modeling proposed in this article for evaluating LoS plus its instancing in
ATPs terminals represents an original advance in the establishment of a
multi-criteria decision aid (MCDA) model to assess the quality of services and
fills a relevant gap for a full non-compensatory approach able to classify the
LoS in the airport context, considering perceptions of multiple evaluators even
if they use different criteria in their evaluations.; 25) Integrative Analysis of High-dimensional RCT and RWD Subject to
  Censoring and Hidden Confounding; In this study, we focus on estimating the heterogeneous treatment effect
(HTE) for survival outcome. The outcome is subject to censoring and the number
of covariates is high-dimensional. We utilize data from both the randomized
controlled trial (RCT), considered as the gold standard, and real-world data
(RWD), possibly affected by hidden confounding factors. To achieve a more
efficient HTE estimate, such integrative analysis requires great insight into
the data generation mechanism, particularly the accurate characterization of
unmeasured confounding effects/bias. With this aim, we propose a
penalized-regression-based integrative approach that allows for the
simultaneous estimation of parameters, selection of variables, and
identification of the existence of unmeasured confounding effects. The
consistency, asymptotic normality, and efficiency gains are rigorously
established for the proposed estimate.
  Finally, we apply the proposed method to estimate the HTE of lobar/sublobar
resection on the survival of lung cancer patients. The RCT is a multicenter
non-inferiority randomized phase 3 trial, and the RWD comes from a clinical
oncology cancer registry in the United States. The analysis reveals that the
unmeasured confounding exists and the integrative approach does enhance the
efficiency for the HTE estimation.; 26) The Llama 3 Herd of Models; Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.; 27) A Web-Based Application Leveraging Geospatial Information to Automate
  On-Farm Trial Design; On-farm sensor data have allowed farmers to implement field management
techniques and intensively track the corresponding responses. These data
combined with historical records open the door for real-time field management
improvements with the help of current advancements in computing power. However,
despite these advances, the statistical design of experiments is rarely used to
evaluate the performance of field management techniques accurately.
Traditionally, randomized block design is prevalent in statistical designs of
field trials, but in practice it is limited in dealing with large variations in
soil classes, management practices, and crop varieties. More specifically,
although this experimental design is suited for most trial types, it is not the
optimal choice when multiple factors are tested over multifarious natural
variations in farms, due to the economic constraints caused by the sheer number
of variables involved. Experimental refinement is required to better estimate
the effects of the primary factor in the presence of auxiliary factors. In this
way, farmers can better understand the characteristics and limitations of the
primary factor. This work presents a framework for automating the analysis of
local field variations by fusing soil classification data and lidar topography
data with historical yield. This framework will be leveraged to automate the
designing of field experiments based on multiple topographic features; 28) Joint Communications and Sensing for 6G Satellite Networks: Use Cases
  and Challenges; Satellite Networks (SN) have traditionally been instrumental in providing two
key services: communications and sensing. Communications satellites enable
global connectivity, while sensing satellites facilitate applications such as
Earth observation, navigation, and disaster management. However, the emergence
of novel use cases and the exponential growth in service demands make the
independent evolution of communication and sensing payloads increasingly
impractical. Addressing this challenge requires innovative approaches to
optimize satellite resources. Joint Communications and Sensing (JCAS)
technology represents a transformative paradigm for SN. By integrating
communication and sensing functionalities into unified hardware platforms, JCAS
enhances spectral efficiency, reduces operational costs, and minimizes hardware
redundancies. This paper explores the potential of JCAS in advancing the
next-generation space era, highlighting its role in emerging applications.
Furthermore, it identifies critical challenges, such as waveform design,
Doppler effect mitigation, and multi-target detection, that remain open for
future research. Through these discussions, we aim to stimulate further
research into the transformative potential of JCAS in addressing the demands of
6G and beyond SN.; 29) Inference for Heterogeneous Treatment Effects with Efficient Instruments
  and Machine Learning; We introduce a new instrumental variable (IV) estimator for heterogeneous
treatment effects in the presence of endogeneity. Our estimator is based on
double/debiased machine learning (DML) and uses efficient machine learning
instruments (MLIV) and kernel smoothing. We prove consistency and asymptotic
normality of our estimator and also construct confidence sets that are more
robust towards weak IV. Along the way, we also provide an accessible discussion
of the corresponding estimator for the homogeneous treatment effect with
efficient machine learning instruments. The methods are evaluated on synthetic
and real datasets and an implementation is made available in the R package
IVDML.; 30) Eigenvalues of nonlinear $(p,q)$-fractional Laplace operators under
  nonlocal Neumann conditions; In this paper, we investigate on a bounded open set of $\mathbb{R}^N$ with
smooth boundary, an eigenvalue problem involving the sum of nonlocal operators
$(-\Delta)_p^{s_1}+ (-\Delta)_q^{s_2}$ with $s_1,s_2\in (0,1)$, $p,q\in
(1,\infty)$ and subject to the corresponding homogeneous nonlocal
$(p,q)$-Neumann boundary condition. A careful analysis of the considered
problem leads us to a complete description of the set of eigenvalues as being
the precise interval $\{0\}\cup(\lambda_{1}(s_2,q),\infty)$, where
$\lambda_{1}(s_2,q)$ is the first nonzero eigenvalue of the homogeneous
fractional $q$-Laplacian under nonlocal $q$-Neumann boundary condition.
Furthermore, we establish that every eigenfunctions is globally bounded.; 31) Partial Information Rate Decomposition; Partial Information Decomposition (PID) is a principled and flexible method
to unveil complex high-order interactions in multi-unit network systems. Though
being defined exclusively for random variables, PID is ubiquitously applied to
multivariate time series taken as realizations of random processes with
temporal statistical structure. Here, to overcome the incorrect depiction of
high-order effects by PID schemes applied to dynamic networks, we introduce the
framework of Partial Information Rate Decomposition (PIRD). PIRD is formalized
applying lattice theory to decompose the information shared dynamically between
a target random process and a set of source processes, implemented for Gaussian
processes through a spectral expansion of information rates, and demonstrated
in practice analyzing time series from large-scale climate oscillations.; 32) Subbagging Variable Selection for Big Data; This article introduces a subbagging (subsample aggregating) approach for
variable selection in regression within the context of big data. The proposed
subbagging approach not only ensures that variable selection is scalable given
the constraints of available computational resources, but also preserves the
statistical efficiency of the resulting estimator. In particular, we propose a
subbagging loss function that aggregates the least-squares approximations of
the loss function for each subsample. Subsequently, we penalize the subbagging
loss function via an adaptive LASSO-type regularizer, and obtain a regularized
estimator to achieve variable selection. We then demonstrate that the
regularized estimator exhibits $\sqrt{N}$-consistency and possesses the oracle
properties, where $N$ represents the size of the full sample in the big data.
In addition, we propose a subbagging Bayesian information criterion to select
the regularization parameter, ensuring that the regularized estimator achieves
selection consistency. Simulation experiments are conducted to demonstrate the
numerical performance. A U.S. census dataset is analyzed to illustrate the
usefulness and computational scalability of the subbagging variable selection
method.; 33) A Population Sampling Framework for Claim Reserving in General Insurance; Claim reserving in insurance has been studied through two primary frameworks:
the macro-level approach, which estimates reserves at an aggregate level (e.g.,
Chain-Ladder), and the micro-level approach, which estimates reserves at the
individual claim level Antonio and Plat (2014). These frameworks are based on
fundamentally different theoretical foundations, creating a degree of
incompatibility that limits the adoption of more flexible models. This paper
introduces a unified statistical framework for claim reserving, grounded in
population sampling theory. We show that macro- and micro-level models
represent extreme yet natural cases of an augmented inverse probability
weighting (AIPW) estimator. This formulation allows for a seamless integration
of principles from both aggregate and individual models, enabling more accurate
and flexible estimations. Moreover, this paper also addresses critical issues
of sampling bias arising from partially observed claims data-an often
overlooked challenge in insurance. By adapting advanced statistical methods
from the sampling literature, such as double-robust estimators, weighted
estimating equations, and synthetic data generation, we improve predictive
accuracy and expand the tools available for actuaries. The framework is
illustrated using Canadian auto insurance data, highlighting the practical
benefits of the sampling-based methods.; 34) Graph-based Change Point Detection for Functional Data; Modeling functions that are sequentially observed as functional time series
is becoming increasingly common. In such models, it is often crucial to ensure
data homogeneity. We investigate the sensitivity of graph-based change point
detection for changes in the distribution of functional data that demarcate
homogeneous regions. Related test statistics and thresholds for detection are
given. A key factor in the efficacy of such tests is the graph construction.
Practical considerations for constructing a graph on arbitrary data are
explored. Simulation experiments investigate tuning parameters for graph
construction and evaluate the graph-based methods in comparison to existing
functional methods. In addition to sensitivity of lower and higher order
changes, robustness to the tuning parameter choices, and practical
recommendations, are shown. Applications to multi-year pedestrian counts,
high-frequency asset returns, and continuous electricity prices corroborate the
simulation results.; 35) Outcome-Informed Weighting for Robust ATE Estimation; Reliable causal effect estimation from observational data requires adjustment
for confounding and sufficient overlap in covariate distributions between
treatment groups. However, in high-dimensional settings, lack of overlap often
inflates the variance and weakens the robustness of inverse propensity score
weighting (IPW) based estimators. Although many approaches that rely on
covariate adjustment have been proposed to mitigate these issues, we instead
shift the focus to the outcome space. In this paper, we introduce the Augmented
Marginal outcome density Ratio (AMR) estimator, an outcome-informed weighting
method that naturally filters out irrelevant information, alleviates practical
positivity violations and outperforms standard augmented IPW and covariate
adjustment-based methods in terms of both efficiency and robustness.
Additionally, by eliminating the need for strong a priori assumptions, our
post-hoc calibration framework is also effective in settings with
high-dimensional covariates. We present experimental results on synthetic data,
the NHANES dataset and text applications, demonstrating the robustness of AMR
and its superior performance under weak overlap and high-dimensional
covariates.; 36) Continuously updated estimation of conditional hazard functions; Motivated by the need to analyze continuously updated data sets in the
context of time-to-event modeling, we propose a novel nonparametric approach to
estimate the conditional hazard function given a set of continuous and discrete
predictors. The method is based on a representation of the conditional hazard
as a ratio between a joint density and a conditional expectation determined by
the distribution of the observed variables. It is shown that such ratio
representations are available for uni- and bivariate time-to-events, in the
presence of common types of random censoring, truncation, and with possibly
cured individuals, as well as for competing risks. This opens the door to
nonparametric approaches in many time-to-event predictive models. To estimate
joint densities and conditional expectations we propose the recursive kernel
smoothing, which is well suited for online estimation. Asymptotic results for
such estimators are derived and it is shown that they achieve optimal
convergence rates. Simulation experiments show the good finite sample
performance of our recursive estimator with right censoring. The method is
applied to a real dataset of primary breast cancer.; 37) Characterization of CMOS sensor using X-ray irradiation; Recent advancements in particle physics demand pixel detectors that can
withstand increased luminosity in the future collider experiments. In response,
MALTA, a novel monolithic active pixel detector, has been developed with a
cutting-edge readout architecture. This new class of monolithic pixel detectors
is found to have exceptional radiation tolerance, superior hit rates, higher
resolution and precise timing resolution, making them ideally suited for
experiments at the LHC. To optimize the performance of these sensors before
their deployment in actual detectors, comprehensive electrical characterization
has been conducted. This study also includes comparative DAC analyses among
sensors of varying thicknesses, providing crucial insights for performance
enhancement. For the further understanding of the effect of radiation, the
sensors are being exposed to different fluence using high intensity X-ray
source.; 38) Martingale Posteriors from Score Functions; Uncertainty associated with statistical problems arises due to what has not
been seen as opposed to what has been seen. Using probability to quantify the
uncertainty the task is to construct a probability model for what has not been
seen conditional on what has been seen. The traditional Bayesian approach is to
use prior distributions for constructing the predictive distributions, though
recently a novel approach has used density estimators and the use of
martingales to establish convergence of parameter values. In this paper we
reply on martingales constructed using score functions. Hence, the method only
requires the computing of gradients arising from parametric families of density
functions. A key point is that we do not rely on Markov Chain Monte Carlo
(MCMC) algorithms, and that the method can be implemented in parallel. We
present the theoretical properties of the score driven martingale posterior.
Further, we present illustrations under different models and settings.; 39) Reconciling Binary Replicates: Beyond the Average; Binary observations are often repeated to improve data quality, creating
technical replicates. Several scoring methods are commonly used to infer the
actual individual state and obtain a probability for each state. The common
practice of averaging replicates has limitations, and alternative methods for
scoring and classifying individuals are proposed. Additionally, an indecisive
response might be wiser than classifying all individuals based on their
replicates in the medical context, where 1 indicates a particular health
condition. Building on the inherent limitations of the averaging approach,
three alternative methods are examined: the median, maximum penalized
likelihood estimation, and a Bayesian algorithm. The theoretical analysis
suggests that the proposed alternatives outperform the averaging approach,
especially the Bayesian method, which incorporates uncertainty and provides
credible intervals. Simulations and real-world medical datasets are used to
demonstrate the practical implications of these methods for improving
diagnostic accuracy and disease prevalence estimation.; 40) Scalable Private Partition Selection via Adaptive Weighting; In the differentially private partition selection problem (a.k.a. private set
union, private key discovery), users hold subsets of items from an unbounded
universe. The goal is to output as many items as possible from the union of the
users' sets while maintaining user-level differential privacy. Solutions to
this problem are a core building block for many privacy-preserving ML
applications including vocabulary extraction in a private corpus, computing
statistics over categorical data, and learning embeddings over user-provided
items.
  We propose an algorithm for this problem, MaximumAdaptiveDegree (MAD), which
adaptively reroutes weight from items with weight far above the threshold
needed for privacy to items with smaller weight, thereby increasing the
probability that less frequent items are output. Our algorithm can be
efficiently implemented in massively parallel computation systems allowing
scalability to very large datasets. We prove that our algorithm stochastically
dominates the standard parallel algorithm for this problem. We also develop a
two-round version of our algorithm where results of the computation in the
first round are used to bias the weighting in the second round to maximize the
number of items output. In experiments, our algorithms provide the best results
across the board among parallel algorithms and scale to datasets with hundreds
of billions of items, up to three orders of magnitude larger than those
analyzed by prior sequential algorithms.; 41) Leveraging Two-Phase Data for Improved Prediction of Survival Outcomes
  with Application to Nasopharyngeal Cancer; Accurate survival predicting models are essential for improving targeted
cancer therapies and clinical care among cancer patients. In this article, we
investigate and develop a method to improve predictions of survival in cancer
by leveraging two-phase data with expert knowledge and prognostic index. Our
work is motivated by two-phase data in nasopharyngeal cancer (NPC), where
traditional covariates are readily available for all subjects, but the primary
viral factor, Human Papillomavirus (HPV), is substantially missing. To address
this challenge, we propose an expert guided method that incorporates prognostic
index based on the observed covariates and clinical importance of key factors.
The proposed method makes efficient use of available data, not simply
discarding patients with unknown HPV status. We apply the proposed method and
evaluate it against other existing approaches through a series of simulation
studies and real data example of NPC patients. Under various settings, the
proposed method consistently outperforms competing methods in terms of c-index,
calibration slope, and integrated Brier score. By efficiently leveraging
two-phase data, the model provides a more accurate and reliable predictive
ability of survival models.; 42) Regularized Sparse Optimal Discriminant Clustering; We propose a new method based on sparse optimal discriminant clustering
(SODC), by a penalty term to scoring matrix based on convex clustering. With
the addition of this penalty term, it is expected to improve the accuracy of
cluster identification by attaching points from the same cluster closer
together and points from different clusters further apart. Moreover, we develop
a novel algorithm to derive the updated formula of this scoring matrix using
majorizing function. It solves the difficulty to satisfy both constraint and
containing the clustering structure to the scoring matrix. We have demonstrated
the numerical simulations and its an application to real data to assess the
performance of the proposed method.; 43) On the Performance of Uplink Pinching Antenna Systems (PASS); Pinching antenna (PA) is a flexible antenna composed of a waveguide and
multiple dielectric particles, which is capable of reconfiguring wireless
channels intelligently in line-of-sight links. By leveraging the unique
features of PAs, we exploit the uplink (UL) transmission in pinching antenna
systems (PASS). To comprehensively evaluate the performance gains of PASS in UL
transmissions, three scenarios, multiple PAs for a single user (MPSU), a single
PA for a single user (SPSU), and a single PA for multiple users (SPMU) are
considered. The positions of PAs are optimized to obtain the maximal channel
gains in the considered scenarios. For the MPSU and SPSU scenarios, by applying
the optimized position of PAs, closed-form expressions for analytical,
asymptotic and approximated ergodic rate are derived. As the further advance,
closed-form expressions of approximated ergodic rate is derived when a single
PA is fixed in the SPMU scenario. Our results demonstrate the following key
insights: i) The proposed PASS significantly outperforms conventional
Multiple-input Single-output networks by exploiting the flexibility of PAs; ii)
The PA distribution follows an asymmetric non-uniform distribution in the MPSU
scenario; iii) Optimizing PA positions significantly enhances the ergodic sum
rate performance.; 44) Evolution of Adaptive Force Chains in Reconfigurable Granular
  Metamaterials; Under an externally applied load, granular packings form force chains that
depend on the contact network and moduli of the grains. In this work, we
investigate packings of variable modulus (VM) particles, where we can direct
force chains by changing the Young's modulus of individual particles within the
packing on demand. Each VM particle is made of a silicone shell that
encapsulates a core made of a low-melting-point metallic alloy (Field's metal).
By sending an electric current through a co-located copper heater, the Field's
metal internal to each particle can be melted via Joule heating, which softens
the particle. As the particle cools to room temperature, the alloy solidifies
and the particle recovers its original modulus. To optimize the mechanical
response of granular packings containing both soft and stiff particles, we
employ an evolutionary algorithm coupled with discrete element method
simulations to predict the patterns of particle moduli that will yield specific
force outputs on the assembly boundaries. The predicted patterns of particle
moduli from the simulations were realized in experiments using 2D assemblies of
VM particles and the force outputs on the assembly boundaries were measured
using photoelastic techniques. These studies represent a step towards making
robotic granular metamaterials that can dynamically adapt their mechanical
properties in response to different environmental conditions or perform
specific tasks on demand.; 45) MouseGPT: A Large-scale Vision-Language Model for Mouse Behavior
  Analysis; Analyzing animal behavior is crucial in advancing neuroscience, yet
quantifying and deciphering its intricate dynamics remains a significant
challenge. Traditional machine vision approaches, despite their ability to
detect spontaneous behaviors, fall short due to limited interpretability and
reliance on manual labeling, which restricts the exploration of the full
behavioral spectrum. Here, we introduce MouseGPT, a Vision-Language Model (VLM)
that integrates visual cues with natural language to revolutionize mouse
behavior analysis. Built upon our first-of-its-kind dataset - incorporating
pose dynamics and open-vocabulary behavioral annotations across over 42 million
frames of diverse psychiatric conditions - MouseGPT provides a novel,
context-rich method for comprehensive behavior interpretation. Our holistic
analysis framework enables detailed behavior profiling, clustering, and novel
behavior discovery, offering deep insights without the need for labor -
intensive manual annotation. Evaluations reveal that MouseGPT surpasses
existing models in precision, adaptability, and descriptive richness,
positioning it as a transformative tool for ethology and for unraveling complex
behavioral dynamics in animal models.; 46) How do recollimation-induced instabilities shape the propagation of
  hydrodynamic relativistic jets?; Recollimation is a phenomenon of particular importance in the dynamic
evolution of jets and in the emission of high-energy radiation. Additionally,
the full comprehension of this phenomenon provides insights into fundamental
properties of jets in the vicinity of the Active Galactic Nucleus (AGN).
Three-dimensional (magneto-)hydrodynamic simulations revealed that the jet
conditions at recollimation favor the growth of strong instabilities,
challenging the traditional view-supported from two-dimensional simulations-of
confined jets undergoing a series of recollimation and reflection shocks. To
investigate the stability of relativistic jets in AGNs at recollimation sites,
we perform a set of long duration three-dimensional relativistic hydrodynamic
simulations with the state-of-the-art PLUTO code, to focus on the development
of hydrodynamical instabilities. We explore the non-linear growth of the
instabilities and their effects on the physical jet properties as a function of
the initial jet parameters: jet Lorentz factor, temperature, opening angle and
jet-environment density-contrast. The parameter space is designed to describe
low-power, weakly magnetized jets at small distances from the core (around the
parsec scale). All collimating jets we simulated develop instabilities.
Recollimation instabilities decelerate the jet, heat it, entrain external
material, and move the recollimation point to shorter distances from the core.
This is true for both conical and cylindrical jets. The instabilities, that are
first triggered by the centrifugal instability, appear to be less disruptive in
the case of narrower, denser, more relativistic, and warmer jets. These results
provide valuable insights into the complex processes governing AGN jets and
could be used to model the properties of low-power, weakly magnetized jetted
AGNs.; 47) Approaching the quantum-limited precision in frequency-comb-based
  spectral interferometry for length measurements; Over the last two decades, frequency combs have brought breakthroughs in
length metrology with traceability to length standards. In particular,
frequency-comb-based spectral interferometry is regarded as a promising
technology for next-generation length standards. However, to achieve this, the
nanometer-level precision inherent in laser interferometer is required. Here,
we report distance measurements by a frequency-comb-based spectral
interferometry with sub-nm precision close to a standard quantum limit. The
measurement precision was confirmed as 0.67 nm at an averaging time of 25 us.
The measurement sensitivity was found to be 4.5 10-12m/Hz1/2, close to the
quantum-limit. As a practical example of observing precise physical phenomena,
we demonstrated measurements of acoustic-wave-induced vibration and laser
eavesdropping. Our study will be an important step toward the practical
realization of upcoming length standards.; 48) Testing eigenstate thermalization hypothesis on small scale quantum
  processors; The Eigenstate Thermalization Hypothesis (ETH) is a framework for discussing
thermal behavior originating from chaotic dynamics in isolated many-body
quantum systems. The PXP model, where certain states do not thermalize, has
been compared with the Sachdev-Ye Kitaev (SYK) model, which is believed to be
fully thermalizing. A gate-based quantum circuit approach is utilized to
simulate time evolution and compute the Out-of-Time-Ordered Correlator (OTOC),
a measure of the extent of chaos. Considering restrictions on implementing SYK
on gate-based hardware, a simplified model called Spin-XY4 (SXY4), which has a
thermal behavior similar to SYK, is tested. An alternate method, which
optimizes control on an analog quantum device with the GRAPE (GRadient Ascent
Pulse Engineering) algorithm, is also utilized to simulate the SYK model.; 49) Doubly partially conservative sentences; The purpose of the present paper is to analyze several variants of Solovay's
theorem on the existence of doubly partially conservative sentences. First, we
investigate $\Theta$ sentences that are doubly $(\Gamma, \Lambda)$-conservative
over $T$ for several triples $(\Theta, \Gamma, \Lambda)$. Among other things,
we prove that the existence of a $\Delta_{n+1}(\mathsf{PA})$ sentence that is
doubly $(\Sigma_n, \Sigma_n)$-conservative over $T$ is equivalent to the
$\Sigma_{n+1}$-inconsistency of $T$ over $\mathsf{PA}$. Secondly, we study
$\Theta$ sentences that are hereditarily doubly $(\Gamma,
\Lambda)$-conservative over $T$ for several triples $(\Theta, \Gamma,
\Lambda)$.; 50) Multi-modal Contrastive Learning for Tumor-specific Missing Modality
  Synthesis; Multi-modal magnetic resonance imaging (MRI) is essential for providing
complementary information about brain anatomy and pathology, leading to more
accurate diagnoses. However, obtaining high-quality multi-modal MRI in a
clinical setting is difficult due to factors such as time constraints, high
costs, and patient movement artifacts. To overcome this difficulty, there is
increasing interest in developing generative models that can synthesize missing
target modality images from the available source ones. Therefore, we design a
generative model for missing MRI that integrates multi-modal contrastive
learning with a focus on critical tumor regions. Specifically, we integrate
multi-modal contrastive learning, tailored for multiple source modalities, and
enhance its effectiveness by selecting features based on entropy during the
contrastive learning process. Additionally, our network not only generates the
missing target modality images but also predicts segmentation outputs,
simultaneously. This approach improves the generator's capability to precisely
generate tumor regions, ultimately improving performance in downstream
segmentation tasks. By leveraging a combination of contrastive, segmentation,
and additional self-representation losses, our model effectively reflects
target-specific information and generate high-quality target images.
Consequently, our results in the Brain MR Image Synthesis challenge demonstrate
that the proposed model excelled in generating the missing modality.; 51) Planning and Control for Deformable Linear Object Manipulation; Manipulating a deformable linear object (DLO) such as wire, cable, and rope
is a common yet challenging task due to their high degrees of freedom and
complex deformation behaviors, especially in an environment with obstacles.
Existing local control methods are efficient but prone to failure in complex
scenarios, while precise global planners are computationally intensive and
difficult to deploy. This paper presents an efficient, easy-to-deploy framework
for collision-free DLO manipulation using mobile manipulators. We demonstrate
the effectiveness of leveraging standard planning tools for high-dimensional
DLO manipulation without requiring custom planners or extensive data-driven
models. Our approach combines an off-the-shelf global planner with a real-time
local controller. The global planner approximates the DLO as a series of rigid
links connected by spherical joints, enabling rapid path planning without the
need for problem-specific planners or large datasets. The local controller
employs control barrier functions (CBFs) to enforce safety constraints,
maintain the DLO integrity, prevent overstress, and handle obstacle avoidance.
It compensates for modeling inaccuracies by using a state-of-the-art
position-based dynamics technique that approximates physical properties like
Young's and shear moduli. We validate our framework through extensive
simulations and real-world demonstrations. In complex obstacle
scenarios-including tent pole transport, corridor navigation, and tasks
requiring varied stiffness-our method achieves a 100% success rate over
thousands of trials, with significantly reduced planning times compared to
state-of-the-art techniques. Real-world experiments include transportation of a
tent pole and a rope using mobile manipulators. We share our ROS-based
implementation to facilitate adoption in various applications.; 52) Recent advances in doubly-robust weighted ordinary least squares
  techniques for dynamic treatment regime estimation; A dynamic treatment regime (DTR) is an approach to delivering precision
medicine that uses patient characteristics to guide treatment decisions for
optimal health outcomes. Numerous methods have been proposed for DTR
estimation, including dynamic weighted ordinary least squares (dWOLS), a
regression-based approach that affords double robustness to model
misspecification within an easy to implement analytical framework. Initially,
the dWOLS approach was developed under the assumptions of continuous outcomes
and binary treatment decisions. Motivated by clinical research, subsequent
theoretical advancements have extended the dWOLS framework to address binary,
continuous and multicategory treatments across various outcome types, including
binary, continuous, and survival-type. However, certain scenarios remain
unexplored. This paper summarizes the last ten years of extension and
application of the dWOLS method, providing a comprehensive and detailed review
of the original dWOLS method and its extensions, as well as highlighting its
diverse practical applications. We also explore studies that have addressed
challenges associated with dWOLS implementation, such as model validation,
variable selection, and handling measurement errors. Using simulated data, we
present numerical illustrations along with step-by-step implementations in the
\texttt{R} environment to facilitate a deeper understanding of dWOLS-based DTR
estimation methodologies.; 53) Large Sample Inference with Dynamic Information Borrowing; Large sample behavior of dynamic information borrowing (DIB) estimators is
investigated. Asymptotic properties of several DIB approaches (adaptive risk
minimization, adaptive LASSO, Bayesian procedures with empirical power prior,
fully Bayesian procedures, and a Bayes-frequentist compromise) are explored
against shrinking to zero alternatives. As shown theoretically and with
simulations, local asymptotic distributions of DIB estimators are often
non-normal. A simple Gaussian setting with external information borrowing
illustrates that none of the considered DIB methods outperforms others in terms
of mean squared error (MSE): at different conflict values, the MSEs of DIBs are
changing between the MSEs of the maximum likelihood estimators based on the
current and pooled data. To uniquely determine an optimality criterion for DIB,
a prior distribution on the conflict needs be either implicitly or explicitly
determined using data independent considerations. Data independent assumptions
on the conflict are also needed for DIB-based hypothesis testing. New families
of DIB estimators parameterized by a sensitivity-to-conflict parameter S are
suggested and their use is illustrated in an infant mortality example. The
choice of S is determined in a data-independent manner by a cost-benefit
compromise associated with the use of external data.; 54) Semiparametric Inference for Partially Identifiable Data Fusion
  Estimands via Double Machine Learning; Many statistical estimands of interest (e.g., in regression or causality) are
functions of the joint distribution of multiple random variables. But in some
applications, data is not available that measures all random variables on each
subject, and instead the only possible approach is one of data fusion, where
multiple independent data sets, each measuring a subset of the random variables
of interest, are combined for inference. In general, since all random variables
are never observed jointly, their joint distribution, and hence also the
estimand which is a function of it, is only partially identifiable.
Unfortunately, the endpoints of the partially identifiable region depend in
general on entire conditional distributions, rendering them hard both
operationally and statistically to estimate. To address this, we present a
novel outer-bound on the region of partial identifiability (and establish
conditions under which it is tight) that depends only on certain conditional
first and second moments. This allows us to derive semiparametrically efficient
estimators of our endpoint outer-bounds that only require the standard machine
learning toolbox which learns conditional means. We prove asymptotic normality
and semiparametric efficiency of our estimators and provide consistent
estimators of their variances, enabling asymptotically valid confidence
interval construction for our original partially identifiable estimand. We
demonstrate the utility of our method in simulations and a data fusion problem
from economics.; 55) A generalized distance covariance framework for genome-wide association
  studies; When testing for the association of a single SNP with a phenotypic response,
one usually considers an additive genetic model, assuming that the mean of of
the response for the heterozygous state is the average of the means for the two
homozygous states. However, this simplification often does not hold. In this
paper, we present a novel framework for testing the association of a single SNP
and a phenotype. Different from the predominant standard approach, our
methodology is guaranteed to detect all dependencies expressed by classical
genetic association models. The asymptotic distribution under mild regularity
assumptions is derived. Moreover, the finite sample distribution under
Gaussianity is provided in which the exact p-value can be efficiently evaluated
via the classical Appell hypergeometric series. Both results are extended to a
regression-type setting with nuisance covariates, enabling hypotheses testing
in a wide range of scenarios. A connection of our approach to score tests is
explored, leading to intuitive interpretations as locally most powerful tests.
A simulation study demonstrates the computational efficiency and excellent
statistical performance of the proposed methodology. A real data example is
provided.; 56) Thresholding Nonprobability Units in Combined Data for Efficient Domain
  Estimation; Quasi-randomization approaches estimate latent participation probabilities
for units from a nonprobability / convenience sample. Estimation of
participation probabilities for convenience units allows their combination with
units from the randomized survey sample to form a survey weighted domain
estimate. One leverages convenience units for domain estimation under the
expectation that estimation precision and bias will improve relative to solely
using the survey sample; however, convenience sample units that are very
different in their covariate support from the survey sample units may inflate
estimation bias or variance. This paper develops a method to threshold or
exclude convenience units to minimize the variance of the resulting survey
weighted domain estimator. We compare our thresholding method with other
thresholding constructions in a simulation study for two classes of datasets
based on degree of overlap between survey and convenience samples on covariate
support. We reveal that excluding convenience units that each express a low
probability of appearing in \emph{both} reference and convenience samples
reduces estimation error.; 57) A step-by-step guide to generalized estimating equations using SPSS in
  the field of dentistry; The Generalized Estimating Equations (GEE) approach is a widely used
statistical method for analyzing longitudinal data and clustered data in clinic
studies. In dentistry, due to multiple outcomes obtained from one patient, the
outcomes produced from individual patients are correlated with one another.
This study focuses on the basic ideas of GEE and introduces the types of
covariance matrix and working correlation matrix. The quasi-likelihood
information criterion(QIC) and quasi-likelihood information criterion
approximation(QICu) were used to select the best working matrix and the best
fitting model for the correlated outcomes.; 58) A lattice QCD calculation of the Compton amplitude subtraction function; The Compton amplitude subtraction function is an essential component in work
concerning both the proton radius puzzle and the proton-neutron mass
difference. However, owing to the difficulty in determining the subtraction
function, it remains a key source of uncertainty in these two contexts. Here,
we use the Feynman-Hellmann method to determine this subtraction function
directly from lattice QCD. Furthermore, we demonstrate how to control dominant
discretisation artefacts for this calculation, eliminating a major source of
systematic error. This calculation is performed for a range of hard momentum
scales, and three different sets of gauge configurations for pion masses about
400 MeV. Our results show good agreement with continuum OPE expectations. As
such, this work paves the way for model-independent and precise determinations
of the subtraction function over a wide range of kinematics.; 59) Evaluating and Testing for Actionable Treatment Effect Heterogeneity; Developing tools for estimating heterogeneous treatment effects (HTE) and
individualized treatment effects has been an area of active research in recent
years. While these tools have proven to be useful in many contexts, a concern
when deploying such methods is the degree to which incorporating HTE into a
prediction model provides an advantage over predictive methods which do not
allow for variation in treatment effect across individuals. To address this
concern, we propose a procedure which evaluates the extent to which an HTE
model provides a predictive advantage. Specifically, our procedure targets the
gain in predictive performance from using a flexible predictive model
incorporating HTE versus an alternative model which is similar to the
HTE-utilizing model except that it is constrained to not allow variation in
treatment effect. By drawing upon recent work in using nested cross-validation
techniques for prediction error inference, we generate confidence intervals for
this measure of gain in predictive performance which allows one to directly
calculate the level at which one is confident of a substantial HTE-modeling
gain in prediction -- a quantity which we refer to as the h-value. Our
procedure is generic and can be directly used to assess the benefit of modeling
HTE for any method that incorporates treatment effect variation.; 60) A Partial Linear Estimator for Small Study Regression Discontinuity
  Designs; Regression discontinuity (RD) designs are a popular approach to estimating a
treatment effect of cutoff-based interventions. Two current estimation
approaches dominate the literature. One fits separate regressions on either
side of the cutoff, and the other performs finite sample inference based on a
local randomization assumption. Recent developments of these approaches have
often focused on asymptotic properties and large sample sizes. Educational
applications often contain relatively small samples or sparsity near the
cutoff, making estimation more difficult. As an alternative to the
aforementioned approaches, we develop a partial linear estimator for RD
designs. We show in simulations that our estimator outperforms certain leading
estimators in several realistic, small-sample scenarios. We apply our estimator
to school accountability scores in Indiana.; 61) Exact statistical tests using integer programming: Leveraging an
  overlooked approach for maximizing power for differences between binomial
  proportions; Traditional hypothesis testing methods for differences in binomial
proportions can either be too liberal (Wald test) or overly conservative
(Fisher's exact test), especially in small samples. Regulators favour
conservative approaches for robust type I error control, though excessive
conservatism may significantly reduce statistical power. We offer fundamental
theoretical contributions that extend an approach proposed in 1969, resulting
in the derivation of a family of exact tests designed to maximize a specific
type of power. We establish theoretical guarantees for controlling type I error
despite the discretization of the null parameter space. This theoretical
advancement is supported by a comprehensive series of experiments to
empirically quantify the power advantages compared to traditional hypothesis
tests. The approach determines the rejection region through a binary decision
for each outcome dataset and uses integer programming to find an optimal
decision boundary that maximizes power subject to type I error constraints. Our
analysis provides new theoretical properties and insights into this approach's
comparative advantages. When optimized for average power over all possible
parameter configurations under the alternative, the method exhibits remarkable
robustness, performing optimally or near-optimally across specific alternatives
while maintaining exact type I error control. The method can be further
customized for particular prior beliefs by using a weighted average. The
findings highlight both the method's practical utility and how techniques from
combinatorial optimization can enhance statistical methodology.; 62) RELICT: A Replica Detection Framework for Medical Image Generation; Despite the potential of synthetic medical data for augmenting and improving
the generalizability of deep learning models, memorization in generative models
can lead to unintended leakage of sensitive patient information and limit model
utility. Thus, the use of memorizing generative models in the medical domain
can jeopardize patient privacy. We propose a framework for identifying
replicas, i.e. nearly identical copies of the training data, in synthetic
medical image datasets. Our REpLIca deteCTion (RELICT) framework for medical
image generative models evaluates image similarity using three complementary
approaches: (1) voxel-level analysis, (2) feature-level analysis by a
pretrained medical foundation model, and (3) segmentation-level analysis. Two
clinically relevant 3D generative modelling use cases were investigated:
non-contrast head CT with intracerebral hemorrhage (N=774) and time-of-flight
MR angiography of the Circle of Willis (N=1,782). Expert visual scoring was
used as the reference standard to assess the presence of replicas. We report
the balanced accuracy at the optimal threshold to assess replica classification
performance. The reference visual rating identified 45 of 50 and 5 of 50
generated images as replicas for the NCCT and TOF-MRA use cases, respectively.
Image-level and feature-level measures perfectly classified replicas with a
balanced accuracy of 1 when an optimal threshold was selected for the NCCT use
case. A perfect classification of replicas for the TOF-MRA case was not
possible at any threshold, with the segmentation-level analysis achieving a
balanced accuracy of 0.79. Replica detection is a crucial but neglected
validation step for the development of generative models in medical imaging.
The proposed RELICT framework provides a standardized, easy-to-use tool for
replica detection and aims to facilitate responsible and ethical medical image
synthesis.; 63) EarthScape: A Multimodal Dataset for Surficial Geologic Mapping and
  Earth Surface Analysis; Surficial geologic mapping is essential for understanding Earth surface
processes, addressing modern challenges such as climate change and national
security, and supporting common applications in engineering and resource
management. However, traditional mapping methods are labor-intensive, limiting
spatial coverage and introducing potential biases. To address these
limitations, we introduce EarthScape, a novel, AI-ready multimodal dataset
specifically designed for surficial geologic mapping and Earth surface
analysis. EarthScape integrates high-resolution aerial RGB and near-infrared
(NIR) imagery, digital elevation models (DEM), multi-scale DEM-derived terrain
features, and hydrologic and infrastructure vector data. The dataset provides
detailed annotations for seven distinct surficial geologic classes encompassing
various geological processes. We present a comprehensive data processing
pipeline using open-sourced raw data and establish baseline benchmarks using
different spatial modalities to demonstrate the utility of EarthScape. As a
living dataset with a vision for expansion, EarthScape bridges the gap between
computer vision and Earth sciences, offering a valuable resource for advancing
research in multimodal learning, geospatial analysis, and geological mapping.
Our code is available at https://github.com/masseygeo/earthscape.; 64) On finite groups with bounded conjugacy classes of commutators; In 1954 B. H. Neumann discovered that if $G$ is a group in which all
conjugacy classes have finite cardinality at most $m$, then the derived group
$G'$ is finite of $m$-bounded order. In 2018 G. Dierings and P. Shumyatsky
showed that if $|x^G| \le m$ for any commutator $x\in G$, then the second
derived group $G''$ is finite and has $m$-bounded order. This paper deals with
finite groups in which $|x^G|\le m$ whenever $x\in G$ is a commutator of prime
power order. The main result is that $G''$ has $m$-bounded order.; 65) Defending Compute Thresholds Against Legal Loopholes; Existing legal frameworks on AI rely on training compute thresholds as a
proxy to identify potentially-dangerous AI models and trigger increased
regulatory attention. In the United States, Section 4.2(a) of Executive Order
14110 instructs the Secretary of Commerce to require extensive reporting from
developers of AI models above a certain training compute threshold. In the
European Union, Article 51 of the AI Act establishes a presumption that AI
models above a certain compute threshold have high impact capabilities and
hence pose systemic risk, thus subjecting their developers to several
obligations including capability evaluations, reporting, and incident
monitoring. In this paper, we examine some enhancement techniques that are
capable of decreasing training compute usage while preserving, or even
increasing, model capabilities. Since training compute thresholds rely on
training compute as a metric and trigger for increased regulatory attention,
these capability-enhancing and compute-saving techniques could constitute a
legal loophole to existing training compute thresholds. In particular, we
concentrate on four illustrative techniques (fine-tuning, model reuse, model
expansion, and above compute-optimal inference compute) with the goal of
furthering the conversation about their implications on training compute
thresholds as a legal mechanism and advancing policy recommendations that could
address the relevant legal loopholes.; 66) Generative AI and Empirical Software Engineering: A Paradigm Shift; The widespread adoption of generative AI in software engineering marks a
paradigm shift, offering new opportunities to design and utilize software
engineering tools while influencing both developers and the artifacts they
create. Traditional empirical methods in software engineering, including
quantitative, qualitative, and mixed-method approaches, are well established.
However, this paradigm shift introduces novel data types and redefines many
concepts in the software engineering process. The roles of developers, users,
agents, and researchers increasingly overlap, blurring the distinctions between
these social and technical actors within the field.
  This paper examines how integrating AI into software engineering challenges
traditional research paradigms. It focuses on the research phenomena that we
investigate, the methods and theories that we employ, the data we analyze, and
the threats to validity that emerge in this new context. Through this
exploration, our goal is to understand how AI adoption disrupts established
software development practices that creates new opportunities for empirical
software engineering research.; 67) Design of Bayesian Clinical Trials with Clustered Data and Multiple
  Endpoints; In the design of clinical trials, it is essential to assess the design
operating characteristics (i.e., the probabilities of making correct
decisions). Common practice for the evaluation of operating characteristics in
Bayesian clinical trials relies on estimating the sampling distribution of
posterior summaries via Monte Carlo simulation. It is computationally intensive
to repeat this estimation process for each design configuration considered,
particularly for clustered data that are analyzed using complex,
high-dimensional models. In this paper, we propose an efficient method to
assess operating characteristics and determine sample sizes for Bayesian trials
with clustered data and multiple endpoints. We prove theoretical results that
enable posterior probabilities to be modelled as a function of the sample size.
Using these functions, we assess operating characteristics at a range of sample
sizes given simulations conducted at only two sample sizes. These theoretical
results are also leveraged to quantify the impact of simulation variability on
our sample size recommendations. The applicability of our methodology is
illustrated using a current clinical trial with clustered data.; 68) Layered Dirichlet Modeling to Assess the Changing Contributions of MLB
  Players as they Age; The productive career of a professional athlete is limited compared to the
normal human lifespan. Most professional athletes have retired by age 40. The
early retirement age is due to a combination of age-related performance and
life considerations. While younger players typically are stronger and faster
than their older teammates, older teammates add value to a team due to their
experience and perspective. Indeed, the highest--paid major league baseball
players are those over the age of 35. These players contribute intangibly to a
team through mentorship of younger players; however, their peak athletic
performance has likely passed. Given this, it is of interest to learn how more
mature players contribute to a team in measurable ways. We examine the
distribution of plate appearance outcomes from three different age groups as
compositional data, using Layered Dirichlet Modeling (LDM). We develop a
hypothesis testing framework to compare the average proportions of outcomes for
each component among 3 of more groups. LDM can not only determine evidence for
differences among populations, but also pinpoint within which component the
largest changes are likely to occur. This framework can determine where players
can be of most use as they age.; 69) Misspecification-Robust Shrinkage and Selection for VAR Forecasts and
  IRFs; VARs are often estimated with Bayesian techniques to cope with model
dimensionality. The posterior means define a class of shrinkage estimators,
indexed by hyperparameters that determine the relative weight on maximum
likelihood estimates and prior means. In a Bayesian setting, it is natural to
choose these hyperparameters by maximizing the marginal data density. However,
this is undesirable if the VAR is misspecified. In this paper, we derive
asymptotically unbiased estimates of the multi-step forecasting risk and the
impulse response estimation risk to determine hyperparameters in settings where
the VAR is (potentially) misspecified. The proposed criteria can be used to
jointly select the optimal shrinkage hyperparameter, VAR lag length, and to
choose among different types of multi-step-ahead predictors; or among IRF
estimates based on VARs and local projections. The selection approach is
illustrated in a Monte Carlo study and an empirical application.; 70) Balancing the effective sample size in prior across different doses in
  the curve-free Bayesian decision-theoretic design for dose-finding trials; The primary goal of dose allocation in phase I trials is to minimize patient
exposure to subtherapeutic or excessively toxic doses, while accurately
recommending a phase II dose that is as close as possible to the maximum
tolerated dose (MTD). Fan et al. (2012) introduced a curve-free Bayesian
decision-theoretic design (CFBD), which leverages the assumption of a monotonic
dose-toxicity relationship without directly modeling dose-toxicity curves. This
approach has also been extended to drug combinations for determining the MTD
(Lee et al., 2017). Although CFBD has demonstrated improved trial efficiency by
using fewer patients while maintaining high accuracy in identifying the MTD, it
may artificially inflate the effective sample sizes for the updated prior
distributions, particularly at the lowest and highest dose levels. This can
lead to either overshooting or undershooting the target dose. In this paper, we
propose a modification to CFBD's prior distribution updates that balances
effective sample sizes across different doses. Simulation results show that
with the modified prior specification, CFBD achieves a more focused dose
allocation at the MTD and offers more precise dose recommendations with fewer
patients on average. It also demonstrates robustness to other well-known dose
finding designs in literature.; 71) In-sample calibration yields conformal calibration guarantees; Conformal predictive systems allow forecasters to issue predictive
distributions for real-valued future outcomes that have out-of-sample
calibration guarantees. On a more abstract level, conformal prediction makes
use of in-sample calibration guarantees to construct bands of predictions with
out-of-sample guarantees under exchangeability. The calibration guarantees are
typically that prediction intervals derived from the predictive distributions
have the correct marginal coverage. We extend this line of reasoning to
stronger notions of calibration that are common in statistical forecasting
theory. We take two prediction methods that are calibrated in-sample, and
conformalize them to obtain conformal predictive systems with stronger
out-of-sample calibration guarantees than existing approaches. The first method
corresponds to a binning of the data, while the second leverages isotonic
distributional regression (IDR), a non-parametric distributional regression
method under order constraints. We study the theoretical properties of these
new conformal predictive systems, and compare their performance in a simulation
experiment. They are then applied to two case studies on European temperature
forecasts and on predictions for the length of patient stay in Swiss intensive
care units. Both approaches are found to outperform existing conformal
predictive systems, while conformal IDR additionally provides a natural method
for quantifying epistemic uncertainty of the predictions.; 72) Dynamics and clustering of sedimenting disc lattices; Uniform arrays of particles tend to cluster as they sediment in viscous
fluids. Shape anisotropy of the particles enriches these dynamics by modifying
the mode-structure and the resulting instabilities of the array. A
one-dimensional lattice of sedimenting spheroids in the Stokesian regime
displays either an exponential or an algebraic rate of clustering depending on
the initial lattice spacing (Chajwa et al. 2020). This is caused by an
interplay between the Crowley mechanism which promotes clumping, and a
shape-induced drift mechanism which subdues it. We theoretically and
experimentally investigate the sedimentation dynamics of one-dimensional
lattices of oblate spheroids or discs and show a stark difference in clustering
behaviour: the Crowley mechanism results in clumps comprised of several
spheroids, whereas the drift mechanism results in pairs of spheroids whose
asymptotic behavior is determined by pair-hydrodynamic interactions. We find
that a Stokeslet, or point-particle, approximation is insufficient to
accurately describe the instability and that the corrections provided by the
first-reflection are necessary for obtaining some crucial dynamical features.
As opposed to a sharp boundary between exponential growth and neutral
eigenvalues under the Stokeslet approximation, the first-reflection correction
leads to exponential growth for all initial perturbations, but far more rapid
algebraic growth than exponential growth at large lattice spacing $d$. For
discs with aspect ratio 0.125, corresponding to the experimental value, the
instability growth rate is found to decrease with increasing lattice spacing
$d$, approximately as $d^{-4.5}$, which is faster than the $d^{-2}$ for spheres
(Crowley, 1971). Sedimenting pairs predominantly come together to form
'$\perp$', which our theory accounts for through an analysis that builds on
Koch & Shaqfeh (1989).; 73) Utility-Based Dose Optimization Approaches for Multiple-Dose Randomized
  Trial Designs Accounting for Multiple Endpoints; The initiation of dose optimization has driven a paradigm shift in oncology
clinical trials to determine the optimal biological dose (OBD). Early-phase
trials with randomized doses can facilitate additional investigation of the
identified OBD in targeted populations by incorporating safety, efficacy, and
biomarker data. To support dose comparison in such settings, we propose to
extend the utility score-based approach (U-MET) and introduce the clinical
utility index-based approach (CUI-MET) to account for multiple endpoints and
doses. The utility-based dose optimization approach for multiple-dose
randomized trial designs accounting for multiple endpoints and doses (U-MET-m)
extends the U-MET, using a utility score to account for multiple endpoints
jointly (e.g., toxicity-efficacy trade-off), while the CUI-MET uses a utility
index to do this marginally. U-MET-m and CUI-MET use Bayesian inference within
a hypothesis framework to compare utility metrics across doses to identify the
OBD. Here we describe simulation studies and present an example to compare the
U-MET-m design, CUI-MET, and empirical design. The U-MET-m design and CUI-MET
were shown to have satisfactory operating characteristics for selecting the
OBD. Based on these findings, we recommend using the U-MET-m and CUI-MET
designs as the primary dose comparison approach or as supportive evidence to
select the OBD.; 74) Identification and Scaling of Latent Variables in Ordinal Factor
  Analysis; Social science researchers are generally accustomed to treating ordinal
variables as though they are continuous. In this paper, we consider how
identification constraints in ordinal factor analysis can mimic the treatment
of ordinal variables as continuous. We describe model constraints that lead to
latent variable predictions equaling the average of ordinal variables. This
result leads us to propose minimal identification constraints, which we call
""integer constraints,"" that center the latent variables around the scale of the
observed, integer-coded ordinal variables. The integer constraints lead to
intuitive model parameterizations because researchers are already accustomed to
thinking about ordinal variables as though they are continuous. We provide a
proof that our proposed integer constraints are indeed minimal identification
constraints, as well as an illustration of how integer constraints work with
real data. We also provide simulation results indicating that integer
constraints are similar to other identification constraints in terms of
estimation convergence and admissibility.; 75) Max-Linear Tail Regression; The relationship between a response variable and its covariates can vary
significantly, especially in scenarios where covariates take on extremely high
or low values. This paper introduces a max-linear tail regression model
specifically designed to capture such extreme relationships. To estimate the
regression coefficients within this framework, we propose a novel M-estimator
based on extreme value theory. The consistency and asymptotic normality of our
proposed estimator are rigorously established under mild conditions. Simulation
results demonstrate that our estimation method outperforms the conditional
least squares approach. We validate the practical applicability of our model
through two case studies: one using financial data and the other using rainfall
data.; 76) Measuring Inaccuracies in the Proportional Hazard Rate Model based on
  Extropy using a Length-Biased Weighted Residual approach; In this paper, we consider the concept of the residual inaccuracy measure and
extend it to its weighted version based on extropy. Properties of this measure
are studied and the discrimination principle is applied in the class of
proportional hazard rate (PHR) models. A characterization problem for the
proposed weighted extropy-inaccuracy measure is studied. We propose some
alternative expressions of weighted residual measure of inaccuracy.
Additionally, we establish upper and lower limits and various inequalities
related to the weighted residual inaccuracy measure using extropy.
Non-parametric estimators based on the kernel density estimation method and
empirical distribution function for the proposed measure are obtained and the
performance of the estimators are also discussed using some simulation studies.
Finally, a real dataset is applied for illustrating our new proposed measure.
In general, our study highlights the potential of the weighted residual
inaccuracy measure based on extropy as a powerful tool for improving the
quality and reliability of data analysis and modelling across various
disciplines. Researchers and practitioners can benefit from incorporating this
measure into their analytical toolkit to enhance the accuracy and effectiveness
of their work.; 77) Quantum Contextual Hypergraphs, Operators, Inequalities, and
  Applications in Higher Dimensions; Quantum contextuality plays a significant role in supporting quantum
computation and quantum information theory. The key tools for this are the
Kochen--Specker and non-Kochen--Specker contextual sets. Traditionally, their
representation has been predominantly operator-based, mainly focusing on
specific constructs in dimensions ranging from three to eight. However, nearly
all of these constructs can be represented as low-dimensional hypergraphs. This
study demonstrates how to generate contextual hypergraphs in any dimension
using various methods, particularly those that do not scale in complexity with
increasing dimensions. Furthermore, we introduce innovative examples of
hypergraphs extending to dimension 32. Our methodology reveals the intricate
structural properties of hypergraphs, enabling precise quantifications of
contextuality of implemented sets. Additionally, we investigate several
promising applications of hypergraphs in quantum communication and quantum
computation, paving the way for future breakthroughs in the field.; 78) SAMGPT: Text-free Graph Foundation Model for Multi-domain Pre-training
  and Cross-domain Adaptation; Graphs are able to model interconnected entities in many online services,
supporting a wide range of applications on the Web. This raises an important
question: How can we train a graph foundational model on multiple source
domains and adapt to an unseen target domain? A major obstacle is that graphs
from different domains often exhibit divergent characteristics. Some studies
leverage large language models to align multiple domains based on textual
descriptions associated with the graphs, limiting their applicability to
text-attributed graphs. For text-free graphs, a few recent works attempt to
align different feature distributions across domains, while generally
neglecting structural differences. In this work, we propose a novel Structure
Alignment framework for text-free Multi-domain Graph Pre-Training and
cross-domain adaptation (SAMGPT). It is designed to learn multi-domain
knowledge from graphs originating in multiple source domains, which can then be
adapted to address applications in an unseen target domain. Specifically, we
introduce a set of structure tokens to harmonize structure-based aggregation
across source domains during the pre-training phase. Next, for cross-domain
adaptation, we design dual prompts, namely, holistic prompts and specific
prompts, which adapt unified multi-domain structural knowledge and
fine-grained, domain-specific information, respectively, to a target domain.
Finally, we conduct comprehensive experiments on seven public datasets to
evaluate and analyze the effectiveness of SAMGPT.; 79) NER4all or Context is All You Need: Using LLMs for low-effort,
  high-performance NER on historical texts. A humanities informed approach; Named entity recognition (NER) is a core task for historical research in
automatically establishing all references to people, places, events and the
like. Yet, do to the high linguistic and genre diversity of sources, only
limited canonisation of spellings, the level of required historical domain
knowledge, and the scarcity of annotated training data, established approaches
to natural language processing (NLP) have been both extremely expensive and
yielded only unsatisfactory results in terms of recall and precision. Our paper
introduces a new approach. We demonstrate how readily-available,
state-of-the-art LLMs significantly outperform two leading NLP frameworks,
spaCy and flair, for NER in historical documents by seven to twentytwo percent
higher F1-Scores. Our ablation study shows how providing historical context to
the task and a bit of persona modelling that turns focus away from a purely
linguistic approach are core to a successful prompting strategy. We also
demonstrate that, contrary to our expectations, providing increasing numbers of
examples in few-shot approaches does not improve recall or precision below a
threshold of 16-shot. In consequence, our approach democratises access to NER
for all historians by removing the barrier of scripting languages and
computational skills required for established NLP tools and instead leveraging
natural language prompts and consumer-grade tools and frontends.; 80) Integrating Misclassified EHR Outcomes with Validated Outcomes from a
  Non-probability Sample; Although increasingly used for research, electronic health records (EHR)
often lack gold-standard assessment of key data elements. Linking EHRs to other
data sources with higher-quality measurements can improve statistical
inference, but such analyses must account for selection bias if the linked data
source arises from a non-probability sample. We propose a set of novel
estimators targeting the average treatment effect (ATE) that combine
information from binary outcomes measured with error in a large,
population-representative EHR database with gold-standard outcomes obtained
from a smaller validation sample subject to selection bias. We evaluate our
approach in extensive simulations and an analysis of data from the Adult
Changes in Thought (ACT) study, a longitudinal study of incident dementia in a
cohort of Kaiser Permanente Washington members with linked EHR data. For a
subset of deceased ACT participants who consented to brain autopsy prior to
death, gold-standard measures of Alzheimer's disease neuropathology are
available. Our proposed estimators reduced bias and improved efficiency for the
ATE, facilitating valid inference with EHR data when key data elements are
ascertained with error.; 81) Stepwise regression revisited; This paper shows that the degree of approximate multicollinearity in a linear
regression model increases simply by including independent variables, even if
these are not highly linearly related. In the current situation where it is
relatively easy to find linear models with a large number of independent
variables, it is shown that this issue can lead to the erroneous conclusion
that there is a worrying problem of approximate multicollinearity. To avoid
this situation, an adjusted variance inflation factor is proposed to compensate
the presence of a large number of independent variables in the multiple linear
regression model. It is shown that this proposal has a direct impact on
variable selection models based on influence relationships, which translates
into a new decision criterion in the individual significance contrast to be
considered in stepwise regression models or even directly in a multiple linear
regression model.; 82) Quantum Phase Transition of Non-Hermitian Systems using Variational
  Quantum Techniques; The motivation for studying non-Hermitian systems and the role of
$\mathcal{PT}$-symmetry is discussed. We investigate the use of a quantum
algorithm to find the eigenvalues and eigenvectors of non-Hermitian
Hamiltonians, with applications to quantum phase transitions. We use a recently
proposed variational algorithm. The systems studied are the transverse Ising
model with both a purely real and a purely complex transverse field.; 83) CME Observations -- from Sun to Impact on Geospace; Our Sun is an active star expelling dynamic phenomena known as coronal mass
ejections (CMEs). The magnetic field configuration on the Sun and related solar
wind structures affect the propagation behavior of CMEs, dominate its transit
time and embedded magnetic field properties when impacting Earth. Since the
conditions on the Sun constantly change, the impact of CMEs on the different
regimes of geospace is quite variable and may differ significantly from event
to event. This short review summarizes the different manifestations of CMEs on
the Sun, their appearance in interplanetary space, and how CMEs trigger a
cascade of reactions as they interact with Earth.; 84) Empowering Multi-class Classification for Complex Functional Data with
  Simultaneous Feature Selection; The opportunity to utilize complex functional data types for conducting
classification tasks is emerging with the growing availability of imaging data.
However, the tools capable of effectively managing imaging data are limited,
let alone those that can further leverage other one-dimensional functional
data. Inspired by the extensive data provided by the Alzheimer's Disease
Neuroimaging Initiative (ADNI), we introduce a novel classifier tailored for
complex functional data. Each observation in this framework may be associated
with numerous functional processes, varying in dimensions, such as curves and
images. Each predictor is a random element in an infinite dimensional function
space, and the number of functional predictors p can potentially be much
greater than the sample size n. In this paper, we introduce a novel and
scalable classifier termed functional BIC deep neural network. By adopting a
sparse deep Rectified Linear Unit network architecture and incorporating the
LassoNet algorithm, the proposed unified model performs feature selection and
classification simultaneously, which is contrast to the existing functional
data classifiers. The challenge arises from the complex inter-correlation
structures among multiple functional processes, and at meanwhile without any
assumptions on the distribution of these processes. Simulation study and real
data application are carried out to demonstrate its favorable performance.; 85) Data-Driven Adjustment for Multiple Treatments; Covariate adjustment is one method of causal effect identification in
non-experimental settings. Prior research provides routes for finding
appropriate adjustments sets, but much of this research assumes knowledge of
the underlying causal graph. In this paper, we present two routes for finding
adjustment sets that do not require knowledge of a graph -- and instead rely on
dependencies and independencies in the data directly. We consider a setting
where the adjustment set is unaffected by treatment or outcome. Our first route
shows how to extend prior research in this area using a concept known as
c-equivalence. Our second route provides sufficient criteria for finding
adjustment sets in the setting of multiple treatments.; 86) Divide and Conquer: Heterogeneous Noise Integration for Diffusion-based
  Adversarial Purification; Existing diffusion-based purification methods aim to disrupt adversarial
perturbations by introducing a certain amount of noise through a forward
diffusion process, followed by a reverse process to recover clean examples.
However, this approach is fundamentally flawed: the uniform operation of the
forward process across all pixels compromises normal pixels while attempting to
combat adversarial perturbations, resulting in the target model producing
incorrect predictions. Simply relying on low-intensity noise is insufficient
for effective defense. To address this critical issue, we implement a
heterogeneous purification strategy grounded in the interpretability of neural
networks. Our method decisively applies higher-intensity noise to specific
pixels that the target model focuses on while the remaining pixels are
subjected to only low-intensity noise. This requirement motivates us to
redesign the sampling process of the diffusion model, allowing for the
effective removal of varying noise levels. Furthermore, to evaluate our method
against strong adaptative attack, our proposed method sharply reduces time cost
and memory usage through a single-step resampling. The empirical evidence from
extensive experiments across three datasets demonstrates that our method
outperforms most current adversarial training and purification techniques by a
substantial margin.; 87) Pivoting Factorization: A Compact Meta Low-Rank Representation of
  Sparsity for Efficient Inference in Large Language Models; The rapid growth of Large Language Models has driven demand for effective
model compression techniques to reduce memory and computation costs. Low-rank
pruning has gained attention for its tensor coherence and GPU compatibility
across all densities. However, low-rank pruning has struggled to match the
performance of semi-structured pruning, often doubling perplexity (PPL) at
similar densities. In this paper, we propose Pivoting Factorization (PIFA), a
novel lossless meta low-rank representation that unsupervisedly learns a
compact form of any low-rank representation, effectively eliminating redundant
information. PIFA identifies pivot rows (linearly independent rows) and
expresses non-pivot rows as linear combinations, achieving an additional 24.2\%
memory savings and 24.6\% faster inference over low-rank layers at r/d = 0.5,
thereby significantly enhancing performance at the same density. To mitigate
the performance degradation caused by low-rank pruning, we introduce a novel,
retraining-free low-rank reconstruction method that minimizes error
accumulation (M). MPIFA, combining M and PIFA into an end-to-end framework,
significantly outperforms existing low-rank pruning methods and, for the first
time, achieves performance comparable to semi-structured pruning, while
surpassing it in GPU efficiency and compatibility.; 88) Comment on arXiv:2307.08384 ""Efficient Quantum State Preparation with
  Walsh Series""; In this paper, we discuss the Walsh Series Loader (WSL) algorithm, proposed
by. In particular, we observe that the paper does not describe how to implement
the term of order zero of the operator WSL is based on. While this does not
affect the theoretical validity of WSL, it poses obstacles for practitioners
aiming to use WSL because, as we show in our experiments, an incorrect
implementation leads to states with very poor fidelity. In this paper, we
describe how to implement the full quantum circuit required by WSL, including
the term of order zero, releasing our source code online, and show that the
algorithm works correctly. Finally, we empirically show how, as was
theoretically demonstrated in the original article, truncated WSL does not
prepare well highly entangled states.; 89) Large Language Models for Healthcare Text Classification: A Systematic
  Review; Large Language Models (LLMs) have fundamentally transformed approaches to
Natural Language Processing (NLP) tasks across diverse domains. In healthcare,
accurate and cost-efficient text classification is crucial, whether for
clinical notes analysis, diagnosis coding, or any other task, and LLMs present
promising potential. Text classification has always faced multiple challenges,
including manual annotation for training, handling imbalanced data, and
developing scalable approaches. With healthcare, additional challenges are
added, particularly the critical need to preserve patients' data privacy and
the complexity of the medical terminology. Numerous studies have been conducted
to leverage LLMs for automated healthcare text classification and contrast the
results with existing machine learning-based methods where embedding,
annotation, and training are traditionally required. Existing systematic
reviews about LLMs either do not specialize in text classification or do not
focus on the healthcare domain. This research synthesizes and critically
evaluates the current evidence found in the literature regarding the use of
LLMs for text classification in a healthcare setting. Major databases (e.g.,
Google Scholar, Scopus, PubMed, Science Direct) and other resources were
queried, which focused on the papers published between 2018 and 2024 within the
framework of PRISMA guidelines, which resulted in 65 eligible research
articles. These were categorized by text classification type (e.g., binary
classification, multi-label classification), application (e.g., clinical
decision support, public health and opinion analysis), methodology, type of
healthcare text, and metrics used for evaluation and validation. This review
reveals the existing gaps in the literature and suggests future research lines
that can be investigated and explored.; 90) Variational inference for approximate reference priors using neural
  networks; In Bayesian statistics, the choice of the prior can have an important
influence on the posterior and the parameter estimation, especially when few
data samples are available. To limit the added subjectivity from a priori
information, one can use the framework of reference priors. However, computing
such priors is a difficult task in general. We develop in this paper a flexible
algorithm based on variational inference which computes approximations of
reference priors from a set of parametric distributions using neural networks.
We also show that our algorithm can retrieve reference priors when constraints
are specified in the optimization problem to ensure the solution is proper. We
propose a simple method to recover a relevant approximation of the parametric
posterior distribution using Markov Chain Monte Carlo (MCMC) methods even if
the density function of the parametric prior is not known in general. Numerical
experiments on several statistical models of increasing complexity are
presented. We show the usefulness of this approach by recovering the target
distribution. The performance of the algorithm is evaluated on the prior
distributions as well as the posterior distributions, jointly using variational
inference and MCMC sampling.; 91) A convoy of magnetic millirobots transports endoscopic instruments for
  minimally-invasive surgery; Small-scale robots offer significant potential in minimally-invasive medical
procedures. Due to the nature of soft biological tissues, however, robots are
exposed to complex environments with various challenges in locomotion, which is
essential to overcome for useful medical tasks. A single mini-robot often
provides insufficient force on slippery biological surfaces to carry medical
instruments, such as a fluid catheter or an electrical wire. Here, for the
first time, we report a team of millirobots (TrainBot) that can generate around
two times higher actuating force than a TrainBot unit by forming a convoy to
collaboratively carry long and heavy cargos. The feet of each unit are
optimized to increase the propulsive force around three times so that it can
effectively crawl on slippery biological surfaces. A human-scale permanent
magnetic set-up is developed to wirelessly actuate and control the TrainBot to
transport heavy and lengthy loads through narrow biological lumens, such as the
intestine and the bile duct. We demonstrate the first electrocauterization
performed by the TrainBot to relieve a biliary obstruction and open a tunnel
for fluid drainage and drug delivery. The developed technology sheds light on
the collaborative strategy of small-scale robots for future minimally-invasive
surgical procedures.; 92) Sparsity learning via structured functional factor augmentation; As one of the most powerful tools for examining the association between
functional covariates and a response, the functional regression model has been
widely adopted in various interdisciplinary studies. Usually, a limited number
of functional covariates are assumed in a functional linear regression model.
Nevertheless, correlations may exist between functional covariates in
high-dimensional functional linear regression models, which brings significant
statistical challenges to statistical inference and functional variable
selection. In this article, a novel functional factor augmentation structure
(fFAS) is proposed for multivariate functional series, and a multivariate
functional factor augmentation selection model (fFASM) is further proposed to
deal with issues arising from variable selection of correlated functional
covariates. Theoretical justifications for the proposed fFAS are provided, and
statistical inference results of the proposed fFASM are established. Numerical
investigations support the superb performance of the novel fFASM model in terms
of estimation accuracy and selection consistency.; 93) Several Representations of $\alpha$-Mutual Information and
  Interpretations as Privacy Leakage Measures; In this paper, we present several novel representations of $\alpha$-mutual
information ($\alpha$-MI) in terms of R{\' e}nyi divergence and conditional
R{\' e}nyi entropy. The representations are based on the variational
characterizations of $\alpha$-MI using a reverse channel. Based on these
representations, we provide several interpretations of the $\alpha$-MI as
privacy leakage measures using generalized mean and gain functions. Further, as
byproducts of the representations, we propose novel conditional R{\' e}nyi
entropies that satisfy the property that conditioning reduces entropy and
data-processing inequality.; 94) A note on local parameter orthogonality for multivariate data and the
  Whittle algorithm for multivariate autoregressive models; This article extends the Cox--Reid local parameter orthogonality to a
multivariate setting, gives an affirmative reply to one of Cox and Reid's
questions, and shows that the extension can lead to efficient computational
algorithms with the celebrated Whittle algorithm for multivariate
autoregressive modeling as a showcase.; 95) Precision of Treatment Hierarchy: A Metric for Quantifying Certainty in
  Treatment Hierarchies from Network Meta-Analysis; Network meta-analysis (NMA) is an extension of pairwise meta-analysis which
facilitates the estimation of relative effects for multiple competing
treatments. A hierarchy of treatments is a useful output of an NMA. Treatment
hierarchies are produced using ranking metrics. Common ranking metrics include
the Surface Under the Cumulative RAnking curve (SUCRA) and P-scores, which are
the frequentist analogue to SUCRAs. Both metrics consider the size and
uncertainty of the estimated treatment effects, with larger values indicating a
more preferred treatment. Although SUCRAs and P-scores themselves consider
uncertainty, treatment hierarchies produced by these ranking metrics are
typically reported without a measure of certainty, which might be misleading to
practitioners. We propose a new metric, Precision of Treatment Hierarchy
(POTH), which quantifies the certainty in producing a treatment hierarchy from
SUCRAs or P-scores. The metric connects three statistical quantities: The
variance of the SUCRA values, the variance of the mean rank of each treatment,
and the average variance of the distribution of individual ranks for each
treatment. POTH provides a single, interpretable value which quantifies the
degree of certainty in producing a treatment hierarchy. We show how the metric
can be adapted to apply to subsets of treatments in a network, for example, to
quantify the certainty in the hierarchy of the top three treatments. We
calculate POTH for a database of NMAs to investigate its empirical properties,
and we demonstrate its use on three published networks.; 96) Flying on Point Clouds with Reinforcement Learning; A long-cherished vision of drones is to autonomously traverse through clutter
to reach every corner of the world using onboard sensing and computation. In
this paper, we combine onboard 3D lidar sensing and sim-to-real reinforcement
learning (RL) to enable autonomous flight in cluttered environments. Compared
to vision sensors, lidars appear to be more straightforward and accurate for
geometric modeling of surroundings, which is one of the most important cues for
successful obstacle avoidance. On the other hand, sim-to-real RL approach
facilitates the realization of low-latency control, without the hierarchy of
trajectory generation and tracking. We demonstrate that, with design choices of
practical significance, we can effectively combine the advantages of 3D lidar
sensing and RL to control a quadrotor through a low-level control interface at
50Hz. The key to successfully learn the policy in a lightweight way lies in a
specialized surrogate of the lidar's raw point clouds, which simplifies
learning while retaining a fine-grained perception to detect narrow free space
and thin obstacles. Simulation statistics demonstrate the advantages of the
proposed system over alternatives, such as performing easier maneuvers and
higher success rates at different speed constraints. With lightweight
simulation techniques, the policy trained in the simulator can control a
physical quadrotor, where the system can dodge thin obstacles and safely
traverse randomly distributed obstacles.; 97) A Novel Convolutional-Free Method for 3D Medical Imaging Segmentation; Segmentation of 3D medical images is a critical task for accurate diagnosis
and treatment planning. Convolutional neural networks (CNNs) have dominated the
field, achieving significant success in 3D medical image segmentation. However,
CNNs struggle with capturing long-range dependencies and global context,
limiting their performance, particularly for fine and complex structures.
Recent transformer-based models, such as TransUNet and nnFormer, have
demonstrated promise in addressing these limitations, though they still rely on
hybrid CNN-transformer architectures. This paper introduces a novel, fully
convolutional-free model based on transformer architecture and self-attention
mechanisms for 3D medical image segmentation. Our approach focuses on improving
multi-semantic segmentation accuracy and addressing domain adaptation
challenges between thick and thin slice CT images. We propose a joint loss
function that facilitates effective segmentation of thin slices based on thick
slice annotations, overcoming limitations in dataset availability. Furthermore,
we present a benchmark dataset for multi-semantic segmentation on thin slices,
addressing a gap in current medical imaging research. Our experiments
demonstrate the superiority of the proposed model over traditional and hybrid
architectures, offering new insights into the future of convolution-free
medical image segmentation.; 98) Step-by-Step Guide to Conducting Meta-Analysis of Dichotomous Outcomes
  Using RevMan in Dental Research Step-by-Step Guide to Conducting
  Meta-Analysis of Dichotomous Outcomes Using RevMan in Dental Research; Meta-analysis is a statistical method that combines the results of individual
studies on the same topic. This method is becoming popular, due to providing
the combined result that individual studies cannot provide and giving a more
precise result. Despite meta-analysis having such significance, there are few
Korean guides for the use of the Review Manager (RevMan) software. This study
will provide a step-by-step guide, using orthodontic mini-screw as a dental
example, to help researcher carry out meta-analysis more easily and accurately.; 99) Comparative Analysis of FPGA and GPU Performance for Machine
  Learning-Based Track Reconstruction at LHCb; In high-energy physics, the increasing luminosity and detector granularity at
the Large Hadron Collider are driving the need for more efficient data
processing solutions. Machine Learning has emerged as a promising tool for
reconstructing charged particle tracks, due to its potentially linear
computational scaling with detector hits. The recent implementation of a graph
neural network-based track reconstruction pipeline in the first level trigger
of the LHCb experiment on GPUs serves as a platform for comparative studies
between computational architectures in the context of high-energy physics. This
paper presents a novel comparison of the throughput of ML model inference
between FPGAs and GPUs, focusing on the first step of the track reconstruction
pipeline$\unicode{x2013}$an implementation of a multilayer perceptron. Using
HLS4ML for FPGA deployment, we benchmark its performance against the GPU
implementation and demonstrate the potential of FPGAs for high-throughput,
low-latency inference without the need for an expertise in FPGA development and
while consuming significantly less power.; 100) Observation of a zero-energy excitation mode in the open Dicke model; Approaching phase boundaries in many-body systems can give rise to intriguing
signatures in their excitation spectra. Here, we explore the excitation
spectrum of a Bose-Einstein condensate strongly coupled to an optical cavity
and pumped by an optical standing wave, which simulates the famous
Dicke-Hepp-Lieb phase transition of the open Dicke model with dissipation
arising due to photon leakage from the cavity. For weak dissipation, the
excitation spectrum displays two strongly polaritonic modes. Close to the phase
boundary, we observe an intriguing regime where the lower-energetic of these
modes, instead of showing the expected roton-type mode softening, is found to
approach and persist at zero energy, well before the critical pump strength for
the Dicke-Hepp-Lieb transition boundary is reached. Hence, a peculiar situation
arises, where an excitation is possible at zero energy cost, but nevertheless
no instability of the system is created.",0.0,0.8065735963827293
2411.00609,applied,2411.00609-pos1-4,"Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype
  Identification Using a Novel AUROC Loss Function for Convolutional Neural
  Networks; Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016).",2411.00609-pos2-4,"Pediatric low-grade glioma: State-of-the-art and ongoing challenges; Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%–40% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence.",82,"['82', '19', '33', '5', '14', '18', '38', '25', '26', '11']","The best candidate paper that complements the main paper on improving pediatric low-grade neuroepithelial tumors molecular subtype identification using convolutional neural networks is paper 82, which discusses the state-of-the-art in pediatric low-grade glioma and ongoing challenges. This paper directly addresses the same subject matter, which is crucial for the treatment and understanding of the tumors that the main paper focuses on. Candidate paper 19 follows closely as it reviews the use of artificial neural networks for magnetoencephalography, which can provide insights into brain dynamics that may be relevant for understanding pediatric brain tumors. Candidate paper 33 discusses brain network dynamics related to emotion regulation, which is indirectly relevant as it touches upon the brain's functionalities impacted in pediatric patients. The subsequent papers explore varied aspects of neuroscience, machine learning applications in medicine, and neurological studies, but they don’t align as directly with the specific focus of the main paper. Thus, the ordering reflects the relevance and potential for a multidisciplinary approach combining the insights from the candidates with the advancements proposed in the main paper.","1) ACF-Monotonicity Formula on RCD(0,N) Metric Measure Cones; The ACF-monotonicity formula is a powerful tool in the study of two-phase
free boundary problems, which was introduced by Alt, Caffarelli, and
Friedman[1]. In this paper, we extend it to RCD(0,N) metric measure cones. As
an application, we give a rigidity result for RCD(0,N) metric measure cones.; 2) A 7T fMRI dataset of synthetic images for out-of-distribution modeling
  of vision; Large-scale visual neural datasets such as the Natural Scenes Dataset (NSD)
are boosting NeuroAI research by enabling computational models of the brain
with performances beyond what was possible just a decade ago. However, these
datasets lack out-of-distribution (OOD) components, which are crucial for the
development of more robust models. Here, we address this limitation by
releasing NSD-synthetic, a dataset consisting of 7T fMRI responses from the
eight NSD subjects for 284 carefully controlled synthetic images. We show that
NSD-synthetic's fMRI responses reliably encode stimulus-related information and
are OOD with respect to NSD. Furthermore, OOD generalization tests on
NSD-synthetic reveal differences between models of the brain that are not
detected with NSD - specifically, self-supervised deep neural networks better
explain neural responses than their task-supervised counterparts. These results
showcase how NSD-synthetic enables OOD generalization tests that facilitate the
development of more robust models of visual processing, and the formulation of
more accurate theories of human vision.; 3) Stiff-sloppy analysis of brain networks to reveal individual differences
  in task performance; Understanding how brain networks recruit resources during cognitive tasks is
key to explaining individual differences in task performance. Brain network
parameters-including activity levels of regions and their connectivity-reflect
the integration and segregation of functional subnetworks underlying task
processing. However, the complexity and high dimensionality of these parameters
pose a significant barrier to identifying functionally relevant individual
differences. Here, we introduce stiff-sloppy analysis as a framework for
uncovering the stiff parameter combinations that critically influence
task-state brain dynamics, exemplified by working memory. Using the pairwise
maximum entropy model (PMEM) calibrated to fMRI data and Fisher Information
Matrix (FIM) analysis, we reveal that the stiff dimensions of the model
parameters capture the most relevant integration and segregation processes of
the default mode network and the working memory network. Individual differences
along these stiff neural dimensions consistently correlate with working memory
performance. Notably, stiff parameters robustly predicted working memory
performance, even when the less sensitive (""sloppy"") parameters were excluded.
This study establishes stiff-sloppy analysis as a powerful approach to identify
cognition-related brain networks, bridging neural dynamics and behavior and
offering new avenues for personalized neuroscience including therapeutic
innovation.; 4) Towards an integrative approach to the study of brain-environment
  interactions in human and non-human primate; By retracing my scientific journey that began 20 years ago, I highlight in
this thesis the need to consider the organization of the brain, which is
certainly globally hierarchical, but also highly distributed and mixed in the
neuronal response of its different areas (by focusing on the sensorimotor
cortex-basal ganglia network). I also emphasize the importance of adopting
behavioral paradigms that reflect as much as possible the characteristics of
the scenarios encountered in the real life of animals. And finally, I mention
the importance of ""disintegrating"" the way data analysis is traditionally
carried out, and of taking into account the dynamic nature of behavior, for
example by favoring the study of behavioral variables and so-called ""latent""
neuronal activities. I conclude this thesis by presenting the vision of my
ideal laboratory in a perspective of 5 to 10 years from today. This laboratory
would have two experimental devices, a first, classic, for carrying out tests
in a constrained and therefore unnatural environment, the data of which would
be compared to those from a second device called ""naturalistic"" in which the
animals could potentially express their entire behavioral repertoire. The tasks
tested would be characterized by strong ecological principles, such as in those
simulating the properties of foraging, and the data would make it possible to
test hypotheses based on the progressive addition of ecological components in
these tasks, all this in order to maintain control over the interpretability of
these data which are complex by nature.; 5) Epilepsy and its driving forces: understanding the forces behind
  epileptical pathogenisis; Epilepsy is a neurological disorder characterized by seizures and epileptic
events intertwined with religious and personal beliefs since prehistoric times.
This review paper explores the historical context and challenges in defining
epilepsy. The formal definition was established twenty years ago, and the
multifaceted causes of this neurological disorder. It aims to pave the way for
personalised therapeutic strategies, research advancements, and informed public
health planning to enhance the lives of those affected by this complex
neurological condition. In addition, this review paper focuses on the
mechanisms and etiologies of epileptogenesis, categorizing them by mechanisms
and the underlying causes of the disorder. The review paper provides a brief
overview of the current state of the art in the diagnosis, diagnosis,
treatment, and treatment of epileptiform seizures.; 6) Effect of thermal conduction on accretion shocks in relativistic
  magnetized flows around rotating black holes; We examine the effects of thermal conduction on relativistic, magnetized,
viscous, advective accretion flows around rotating black holes considering
bremsstrahlung and synchrotron cooling processes. Assuming the toroidal
component of magnetic fields as the dominant one, we self-consistently solve
the steady-state fluid equations to derive the global transonic accretion
solutions for a black hole of spin $a_{\rm k}$. Depending on the model
parameters, the magnetized accretion flow undergoes shock transitions and
shock-induced global accretion solutions persist over a wide range of model
parameters including the conduction parameter ($\Upsilon_{\rm s}$),
plasma-$\beta$, and viscosity parameter ($\alpha_{\rm B}$). We find that the
shock properties -- such as shock radius ($r_{\rm s}$), compression ratio
($R$), and shock strength ($S$) -- are regulated by $\Upsilon_{\rm s}$, plasma
$\beta$, and $\alpha_{\rm B}$. Furthermore, we compute the critical conduction
parameter ($\Upsilon_{\rm s}^{\rm cri}$), a threshold beyond which shock
formation ceases to exist, and investigate its dependence on plasma-$\beta$ and
$\alpha_{\rm B}$ for both weakly rotating ($a_{\rm k} \rightarrow 0$) and
rapidly rotating ($a_{\rm k} \rightarrow 1$) black holes. Finally, we examine
the spectral energy distribution (SED) of the accretion disc and observe that
increased thermal conduction and magnetic field strength lead to more luminous
emission spectra from black hole sources.; 7) On reflected L\'evy processes with collapse; We consider a L\'evy process reflected at the origin with additional i.i.d.
collapses that occur at Poisson epochs, where a collapse is a jump downward to
a state which is a random fraction of the state just before the jump. We first
study the general case, then specialize to the case where the L\'evy process is
spectrally positive and finally we specialize further to the two cases where
the L\'evy process is a Brownian motion and a compound Poisson process with
exponential jumps minus a linear slope.; 8) The elastic ray transform; We introduce and study a new family of tensor tomography problems. At rank 2
it corresponds to linearization of travel time of elastic waves, measured for
all polarizations. We provide a kernel characterization for ranks up to 2. The
kernels consist of potential tensors, but in an unusual sense: the associated
differential operators have degree 2 instead of the familiar 1. The proofs are
based on Fourier analysis, Helmholtz decompositions, and cohomology.; 9) Phase-Locking Parametric Instability Coupling Longitudinal and
  Transverse Waves on Rivulets in a Hele-Shaw Cell; We report an instability exhibited by a fluid system when coupling two
distinct types of waves, both linearly damped. While none of them is unstable
on its own, they amplify one another, resulting in a previously unreported
convective instability. An external excitation is used to induce a parametric
cross-coupling between longitudinal and transverse deformations of a liquid
bridge between two vertical glass plates. Coherent amplification results in
waves satisfying a synchronization condition, which selects a precise
wavelength. We derive a model for this instability using depth-averaged
Navier-Stokes equations, showing the physical origin of the coamplification,
and confirm its relevance experimentally. Our findings open new perspectives in
the study of parametrically controlled pattern formation, and invites the
search for analogous parametric cross-coupling instabilities in other systems
exhibiting distinct wave types, from plasma to elastic media.; 10) An Accessible Formulation for Defining the SI Second Based on Multiple
  Atomic Transitions; This work presents a novel formulation for a redefinition of the second based
on the weighted arithmetic mean of multiple normalized frequencies. We
demonstrate that it is mathematically equivalent to the previously discussed
implementation employing a geometric mean. In our reformulation, the
normalization of frequencies provides the defining constants with immediate
physical meaning, while maintaining the decoupling of assigned weights from the
frequencies of the reference transitions. We believe that a definition based on
this formulation would be significantly more accessible to both experts and
non-specialists, enhancing understanding and facilitating broader acceptance.
We hope that this approach will help overcome barriers to the adoption of a
redefinition that effectively values all state-of-the-art atomic clocks.; 11) ELIZA Reanimated: The world's first chatbot restored on the world's
  first time sharing system; ELIZA, created by Joseph Weizenbaum at MIT in the early 1960s, is usually
considered the world's first chatbot. It was developed in MAD-SLIP on MIT's
CTSS, the world's first time-sharing system, on an IBM 7094. We discovered an
original ELIZA printout in Prof. Weizenbaum's archives at MIT, including an
early version of the famous DOCTOR script, a nearly complete version of the
MAD-SLIP code, and various support functions in MAD and FAP. Here we describe
the reanimation of this original ELIZA on a restored CTSS, itself running on an
emulated IBM 7094. The entire stack is open source, so that any user of a
unix-like OS can run the world's first chatbot on the world's first
time-sharing system.; 12) Non-Markovian dynamics of collectively-encoded qubits; Collectively-encoded qubits, involving ensembles of atomic or solid-state
emitters, present many practical advantages for quantum technologies. However,
they suffer from uncontrolled inhomogeneous dephasing which couples them to a
quasi-continuum of dark states. In most cases, this process cannot be
encompassed in a standard master equation with time-independent coefficients,
making its description either tedious or inaccurate. We show that it can be
understood as a displacement in time-frequency phase space and accurately
included in resource-efficient numerical simulations of the qubit's dynamics.
This description unveils a regime where the qubit becomes protected from
dephasing through a combination of strong driving and non-Markovianity. We
experimentally investigate this regime using a Rydberg superatom and extend its
coherent dynamics beyond the inhomogeneous-dephasing characteristic time by an
order of magnitude.; 13) Hallucinations Can Improve Large Language Models in Drug Discovery; Concerns about hallucinations in Large Language Models (LLMs) have been
raised by researchers, yet their potential in areas where creativity is vital,
such as drug discovery, merits exploration. In this paper, we come up with the
hypothesis that hallucinations can improve LLMs in drug discovery. To verify
this hypothesis, we use LLMs to describe the SMILES string of molecules in
natural language and then incorporate these descriptions as part of the prompt
to address specific tasks in drug discovery. Evaluated on seven LLMs and five
classification tasks, our findings confirm the hypothesis: LLMs can achieve
better performance with text containing hallucinations. Notably, Llama-3.1-8B
achieves an 18.35% gain in ROC-AUC compared to the baseline without
hallucination. Furthermore, hallucinations generated by GPT-4o provide the most
consistent improvements across models. Additionally, we conduct empirical
analyses and a case study to investigate key factors affecting performance and
the underlying reasons. Our research sheds light on the potential use of
hallucinations for LLMs and offers new perspectives for future research
leveraging LLMs in drug discovery.; 14) Multi-Site rs-fMRI Domain Alignment for Autism Spectrum Disorder
  Auxiliary Diagnosis Based on Hyperbolic Space; In the medical field, most resting-state fMRI (rs-fMRI) data are collected
from multiple hospital sites. Multi-site rs-fMRI data can increase the volume
of training data, enabling auxiliary diagnostic algorithms for brain diseases
to learn more accurate and stable models. However, due to the significant
heterogeneity and domain shift in rs-fMRI data across different sites, the
accuracy of auxiliary diagnosis remains unsatisfactory. Moreover, there has
been limited exploration of multi-source domain adaptation algorithms, and the
interpretability of models is often poor. To address these challenges, we
proposed a domain-adaptive algorithm based on hyperbolic space embedding.
Hyperbolic space is naturally suited for representing the topology of complex
networks such as brain functional networks. Therefore, we embedded the brain
functional network into hyperbolic space and constructed the corresponding
hyperbolic space community network to effectively extract brain network
representations. To address the heterogeneity of data across different sites
and the issue of domain shift, we introduce a constraint loss function, HMMD
(Hyperbolic Maximum Mean Discrepancy), to align the marginal distributions in
the hyperbolic space. Additionally, we employ class prototype alignment to
align the conditional distributions. This significantly improves the quality of
brain representations and enhances diagnostic classification accuracy for
Autism Spectrum Disorder (ASD). Experimental results demonstrated that the
proposed algorithm is robust to multi-site heterogeneity and shows promising
potential for brain network mechanism analysis.; 15) Autonomous Legacy Web Application Upgrades Using a Multi-Agent System; The use of Large Language Models (LLMs) for autonomous code generation is
gaining attention in emerging technologies. As LLM capabilities expand, they
offer new possibilities such as code refactoring, security enhancements, and
legacy application upgrades. Many outdated web applications pose security and
reliability challenges, yet companies continue using them due to the complexity
and cost of upgrades. To address this, we propose an LLM-based multi-agent
system that autonomously upgrades legacy web applications to the latest
versions. The system distributes tasks across multiple phases, updating all
relevant files. To evaluate its effectiveness, we employed Zero-Shot Learning
(ZSL) and One-Shot Learning (OSL) prompts, applying identical instructions in
both cases. The evaluation involved updating view files and measuring the
number and types of errors in the output. For complex tasks, we counted the
successfully met requirements. The experiments compared the proposed system
with standalone LLM execution, repeated multiple times to account for
stochastic behavior. Results indicate that our system maintains context across
tasks and agents, improving solution quality over the base model in some cases.
This study provides a foundation for future model implementations in legacy
code updates. Additionally, findings highlight LLMs' ability to update small
outdated files with high precision, even with basic prompts. The source code is
publicly available on GitHub: https://github.com/alasalm1/Multi-agent-pipeline.; 16) Volume Rigidity of Simplicial Manifolds; Classical results of Cauchy and Dehn imply that the 1-skeleton of a convex
polyhedron $P$ is rigid i.e. every continuous motion of the vertices of $P$ in
$\mathbb R^3$ which preserves its edge lengths results in a polyhedron which is
congruent to $P$. This result was extended to convex poytopes in $\mathbb R^d$
for all $d\geq 3$ by Whiteley, and to generic realisations of 1-skeletons of
simplicial $(d-1)$-manifolds in $\mathbb R^{d}$ by Kalai for $d\geq 4$ and
Fogelsanger for $d\geq 3$. We will generalise Kalai's result by showing that,
for all $d\geq 4$ and any fixed $1\leq k\leq d-3$, every generic realisation of
the $k$-skeleton of a simplicial $(d-1)$-manifold in $\mathbb R^{d}$ is volume
rigid, i.e. every continuous motion of its vertices in $\mathbb R^d$ which
preserves the volumes of its $k$-faces results in a congruent realisation. In
addition, we conjecture that our result remains true for $k=d-2$ and verify
this conjecture when $d=4,5,6$.; 17) Enhancing Olfactory Perception Through Large Language Models:
  Integrating Sensory Data for Advanced Odor Recognition; The integration of biological principles into artificial olfactory systems
has led to significant advancements in odor detection and classification.
Inspired by the intricate mechanisms of natural olfaction, researchers are
developing sophisticated systems that mimic the functionality of biological
olfactory pathways. These systems utilize high-density chemoresistive sensor
arrays (HCSA) combined with advanced computational techniques, such as
FPGA-accelerated glomerular convergence circuits (FGCC) and hierarchical graph
neural networks (HGNN). This bioinspired approach enables real-time adaptive
responses to volatile organic compounds (VOCs), enhancing the accuracy and
efficiency of odor identification. At the core of these innovations is the
multiparametric sigmoidal sensor activation (MPSA), which quantifies VOCs by
leveraging the diverse responses of sensor arrays. The implementation of
lateral inhibition via programmable synaptic crossbars (LIPSC) further refines
odor processing by mimicking neural interactions found in biological systems.
Additionally, temporal self-organizing maps (TSOM) facilitate dynamic
clustering of odor patterns, allowing for a nuanced understanding of complex
odor environments. A novel aspect of this research lies in the Grassmannian
manifold embedding (GME) of odor profiles, which provides a mathematical
framework for representing and analyzing the multidimensional nature of odors.
Coupled with Hamiltonian Monte Carlo-optimized feedback (HMC-FB), this system
effectively compensates for drift in sensor readings, ensuring consistent
performance over time. By bridging the gap between biological inspiration and
technological innovation, these artificial olfactory systems are poised to
revolutionize applications ranging from environmental monitoring to food safety
and healthcare diagnostics.; 18) Personal Danger Signals Reprocessing: New Online Group Intervention for
  Chronic Pain; Chronic pain is a significant global health issue, with many patients
experiencing persistent pain despite no identifiable organic cause, classified
as nociplastic pain. Increasing evidence highlights the role of danger signal
processing in the maintenance of chronic pain. In response, we developed
Personal Danger Signals Reprocessing (PDSR), an online, group-based
intervention designed to modify these mechanisms using coaching techniques to
enhance accessibility and affordability.
  This study evaluated the efficacy of PDSR in reducing pain and mental health
comorbidities. A cohort of women (N=19, mean age 43) participated in an 8-week
online program, receiving weekly sessions on chronic pain mechanisms within a
systemic framework. Outcomes were assessed at three time points:
pre-intervention, mid-intervention, and post-intervention. A waiting list group
(N=20, mean age 43.5) completed assessments at the same intervals.
  Participants in the PDSR group showed significant pain reduction (p < .001),
with moderate to large effects observed at mid-intervention (Cohen's D = 0.7)
and post-intervention (Cohen's D = 1.5) compared to controls. Pain interference
significantly decreased (p < .01), with large reductions in the PDSR group
(Cohen's D = -1.7, p < .0001). Well-being also improved substantially (p <
.001, Cohen's D = 1.7-1.8). Secondary outcomes, including pain catastrophizing,
sleep interference, anxiety, and depressive symptoms, consistently improved
(all p-values < .01).
  Findings suggest PDSR is an effective, scalable intervention for reducing
pain, improving function, and enhancing well-being in individuals with chronic
pain.; 19) Artificial Neural Networks for Magnetoencephalography: A review of an
  emerging field; Magnetoencephalography (MEG) is a cutting-edge neuroimaging technique that
measures the intricate brain dynamics underlying cognitive processes with an
unparalleled combination of high temporal and spatial precision. MEG data
analytics has always relied on advanced signal processing and mathematical and
statistical tools for various tasks ranging from data cleaning to probing the
signals' rich dynamics and estimating the neural sources underlying the
surface-level recordings. Like in most domains, the surge in Artificial
Intelligence (AI) has led to the increased use of Machine Learning (ML) methods
for MEG data classification. More recently, an emerging trend in this field is
using Artificial Neural Networks (ANNs) to address many MEG-related tasks. This
review provides a comprehensive overview of how ANNs are being used with MEG
data from three vantage points: First, we review work that employs ANNs for MEG
signal classification, i.e., for brain decoding. Second, we report on work that
has used ANNs as putative models of information processing in the human brain.
Finally, we examine studies that use ANNs as techniques to tackle
methodological questions in MEG, including artifact correction and source
estimation. Furthermore, we assess the current strengths and limitations of
using ANNs with MEG and discuss future challenges and opportunities in this
field. Finally, by establishing a detailed portrait of the field and providing
practical recommendations for the future, this review seeks to provide a
helpful reference for both seasoned MEG researchers and newcomers to the field
who are interested in using ANNs to enhance the exploration of the complex
dynamics of the human brain with MEG.; 20) Energy and Age-Aware MAC for Low-Power Massive IoT; Efficient multiple access remains a key challenge for emerging Internet of
Things (IoT) networks comprising a large set of devices with sporadic
activation, thus motivating significant research in the last few years. In this
paper, we consider a network wherein IoT sensors capable of energy harvesting
(EH) send updates to a central server to monitor the status of the environment
or machinery in which they are located. We develop energy-aware ALOHA-like
multiple access schemes for such a scenario using the Age of Information (AoI)
metric to quantify the freshness of an information packet. The goal is to
minimize the average AoI across the entire system while adhering to energy
constraints imposed by the EH process. Simulation results show that applying
the designed multiple access scheme improves performance from 24% up to 90%
compared to previously proposed age-dependent protocols by ensuring low average
AoI and achieving scalability while simultaneously complying with the energy
constraints considered.; 21) A Relativistic Theory of Consciousness (shortened version); This paper is a shortened version of the full paper that was published in the
journal Frontiers of Psychology in May 2022. In recent decades, the scientific
study of consciousness has significantly increased our understanding of this
elusive phenomenon. Yet, despite critical development in our understanding of
the functional side of consciousness, we still lack a fundamental theory
regarding its phenomenal aspect. The phenomenal aspect of consciousness is the
first-person answer to what it is like question, and it has thus far proved
recalcitrant to direct scientific investigation. The question of how the brain,
or any cognitive system, can create conscious experience out of neural
representations poses a great conundrum to science. Naturalistic dualists argue
that it is composed of a primitive, private, nonreductive element of reality.
Illusionists, on the other hand, argue that it is merely a cognitive illusion.
We contend that both the dualist and illusionist positions are flawed because
they tacitly assume consciousness to be an absolute property that does not
depend on the observer. We developed a conceptual and a mathematical argument
for a relativistic theory of consciousness in which a system either has or does
not have phenomenal consciousness with respect to some observer. According to
the theory, Phenomenal consciousness is neither private nor delusional, just
relativistic. In the frame of reference of the cognitive system, it will be
observable (first-person perspective) and in other frame of reference it will
not (third-person perspective). These two cognitive frames of reference are
both correct, just as in the case of an observer that claims to be at rest
while another will claim that the observer has constant velocity. Neither
observer position can be privileged, as they both describe the same underlying
reality.; 22) BOLDreams: Dreaming with pruned in-silico fMRI Encoding Models of the
  Visual Cortex; In this article we use the Natural Scenes Dataset (NSD) to train a family of
feature-weighted receptive field neural encoding models. These models use a
pre-trained vision or text backbone and map extracted features to the voxel
space via receptive field readouts. We comprehensively assess such models,
quantifying performance changes based on using different modalities like text
or images, toggling finetuning, using different pre-trained backbones, and
changing the width of the readout. We also dissect each model using explainable
AI (XAI) techniques, such as feature visualization via input optimization, also
referred to as ``dreaming'' in the AI literature, and the integrated gradients
approach to calculate implicit attention maps to illustrate which features
drive the predicted signal in different brain areas. These XAI tools illustrate
biologically plausible features that drive the predicted signal. Traversing the
model hyperparameter space reveals the existence of a maximally minimal model,
balancing simplicity while maintaining performance.; 23) Dynamic Markov Blanket Detection for Macroscopic Physics Discovery; The free energy principle (FEP), along with the associated constructs of
Markov blankets and ontological potentials, have recently been presented as the
core components of a generalized modeling method capable of mathematically
describing arbitrary objects that persist in random dynamical systems; that is,
a mathematical theory of ``every'' ``thing''. Here, we leverage the FEP to
develop a mathematical physics approach to the identification of objects,
object types, and the macroscopic, object-type-specific rules that govern their
behavior. We take a generative modeling approach and use variational Bayesian
expectation maximization to develop a dynamic Markov blanket detection
algorithm that is capable of identifying and classifying macroscopic objects,
given partial observation of microscopic dynamics. This unsupervised algorithm
uses Bayesian attention to explicitly label observable microscopic elements
according to their current role in a given system, as either the internal or
boundary elements of a given macroscopic object; and it identifies macroscopic
physical laws that govern how the object interacts with its environment.
Because these labels are dynamic or evolve over time, the algorithm is capable
of identifying complex objects that travel through fixed media or exchange
matter with their environment. This approach leads directly to a flexible class
of structured, unsupervised algorithms that sensibly partition complex
many-particle or many-component systems into collections of interacting
macroscopic subsystems, namely, ``objects'' or ``things''. We derive a few
examples of this kind of macroscopic physics discovery algorithm and
demonstrate its utility with simple numerical experiments, in which the
algorithm correctly labels the components of Newton's cradle, a burning fuse,
the Lorenz attractor, and a simulated cell.; 24) Beyond Automation: How UI/UX Designers Perceive AI as a Creative Partner
  in the Divergent Thinking Stages; Divergent thinking activities, like research and ideation, are key drivers of
innovation in UI/UX design. Existing research has explored AI's role in
automating design tasks, but leaves a critical gap in understanding how AI
specifically influences divergent thinking. To address this, we conducted
interviews with 19 professional UI/UX designers, examining their use and
perception of AI in these creative activities. We found that in this context,
participants valued AI tools that offer greater control over ideation,
facilitate collaboration, enhance efficiency to liberate creativity, and align
with their visual habits. Our results indicated four key roles AI plays in
supporting divergent thinking: aiding research, kick-starting creativity,
generating design alternatives, and facilitating prototype exploration. Through
this study, we provide insights into the evolving role of AI in the
less-investigated area of divergent thinking in UI/UX design, offering
recommendations for future AI tools that better support design innovation.; 25) Neural Constraints on Cognitive Experience and Mental Health; Understanding how neural dynamics shape cognitive experiences remains a
central challenge in neuroscience and psychiatry. Here, we present a novel
framework leveraging state-to-output controllability from dynamical systems
theory to model the interplay between cognitive perturbations, neural activity,
and subjective experience. We demonstrate that large-scale fMRI signals are
constrained to low-dimensional manifolds, where affective and cognitive states
are naturally organized. Furthermore, we provide a theoretically robust method
to estimate the controllability Gramian from steady-state neural responses,
offering a direct measure of the energy required to steer cognitive outcomes.
In five healthy participants viewing 2,185 emotionally evocative short videos,
our analyses reveal a strong alignment between neural activations and affective
ratings, with an average correlation of $r \approx 0.7$. In a clinical cohort
of 255 patients with major depressive disorder, biweekly Hamilton Rating Scale
trajectories over 11 weeks significantly mapped onto these manifolds,
explaining approximately 20% more variance than chance ($p < 10^{-10}$,
numerically better than chance in 93% reaching statistical significance in
one-third of subjects). Our work bridges dynamical systems theory and clinical
neuroscience, providing a principled approach to optimize mental health
treatments by targeting the most efficient neural pathways for cognitive
change.; 26) Morphological Neuron Classification Using Machine Learning; Classification and quantitative characterization of neuronal morphologies
from histological neuronal reconstruction is challenging since it is still
unclear how to delineate a neuronal cell class and which are the best features
to define them by. The morphological neuron characterization represents a
primary source to address anatomical comparisons, morphometric analysis of
cells, or brain modeling. The objectives of this paper are (i) to develop and
integrate a pipeline that goes from morphological feature extraction to
classification and (ii) to assess and compare the accuracy of machine learning
algorithms to classify neuron morphologies. The algorithms were trained on 430
digitally reconstructed neurons subjectively classified into layers and/or
m-types using young and/or adult development state population of the
somatosensory cortex in rats. For supervised algorithms, linear discriminant
analysis provided better classification results in comparison with others. For
unsupervised algorithms, the affinity propagation and the Ward algorithms
provided slightly better results.; 27) Multiphysics simulations of microstructure influence on hysteresis and
  eddy current losses of electrical steel; Improving efficiency of electrical machines requires fundamental knowledge on
the mechanisms behind magnetic and eddy current losses of the magnetic core
materials, with Fe-Si alloy as a prototype. These losses are intrinsically
influenced by the microstructure of the materials. This necessitates
physics-based, microstructure-informed multiscale simulations. In the present
paper, we utilised micromagnetic simulations and computational homogenization
methods to calculate the effective hysteresis and effective conductivities of
Fe-Si electrical steels. To demonstrate the methodology, binder-jet printed
electrical steel material samples with different microstructure were
investigated. The microstructure samples were digitized based on both the
descriptor-based synthetic reconstruction and SEM-image-based digitization.
More samples were generated with varying microstructure features such as grain
size and grain boundary phases. The micromagnetic simulations were then
performed to investigate the magnetic hysteresis and hysteresis loss. The eddy
current loss was also evaluated by using the effective conductivity through
computational homogenization. By performing parameter research on a series of
synthetic microstructures, effects of average grain size and grain boundary
(GB) phase thickness on the hysteresis loss and eddy current loss were
unveiled. An average grain size around 120 \si{\micro m} has the lowest
hysteresis loss, although the eddy current loss increases with the grain size.
Increasing GB-phase thickness helps reduce both losses. Results indicate the
potential to decrease loss of magnetic core materials by microstructure
optimization.; 28) A Turing Test for Artificial Nets devoted to model Human Vision; In this 2022 work we argued that, despite claims about successful modeling of
the visual brain using artificial nets, the problem is far from being solved
(even for low-level vision). Examples of open issues include: where should we
read from ANNs in order to reproduce human behavior?, this ad-hoc read-out is
considered part of the brain model or not?, should we use artificial
psychophysics or artificial physiology?, in the case of ANNs, artificial
experiments should literally match the experiments done with humans?. There is
a clear need of rigorous procedures for experimental tests for ANNs devoted to
model the visual brain, and more generally, to understand ANNs devoted to
generic vision tasks. Following our experience in using low-level facts from
Quantitative Visual Neuroscience in computer vision, in this work we presented
the idea of developing a low-level dataset compiling the basic spatio-temporal
and chromatic facts that are known to happen in the retina-V1 pathway, and they
are not currently available in existing databases such as BrainScore. In our
results we checked the behavior of three recently proposed models with similar
architecture: (1) A parametric model tuned via Maximum Differentiation [Malo &
Simoncelli SPIE 15, Martinez et al. PLOS 18, Martinez et al. Front. Neurosci.
19], (2) A non-parametric model called PerceptNet tuned to maximize the
correlation with human opinion on subjective distortions [Hepburn et al. IEEE
ICIP 19], and (3) A model with the same encoder as PerceptNet, but tuned for
image segmentation (published as Hernandez-Camara et al. Patt.Recogn.Lett. 23).
Results on 10 compelling psycho/physio visual facts show that the first model
is the one with closer behavior to the humans in terms of receptive fields, but
more interestingly, on the nonlinear behavior when facing complex
spatio-chromatic patterns of a range of luminances and contrasts.; 29) Freezing of Gait as a Complication of Pallidal Deep Brain Stimulation in
  DYT- KMT2B Patients with Evidence of Striatonigral Degeneration; Background: Mutations in KMT2B are a recognized cause of early-onset complex
dystonia, with deep brain stimulation (DBS) of the internal globus pallidus
(GPi-DBS) being an effective treatment. However, gait impairment, particularly
freezing of gait (FOG), remains a significant challenge in DYT-KMT2B patients
post-DBS. Objectives: To characterize the emergence of FOG in DYT-KMT2B
patients treated with GPi-DBS and explore potential underlying mechanisms,
including striatonigral degeneration. Methods: Five patients (four females)
with KMT2B-related dystonia and protein-truncating variants (PTVs) were
retrospectively analyzed. Clinical progression, response to GPi-DBS, and the
presence of FOG were documented. Dopaminergic function was assessed using
DaTscan (SPECT for ^123I-ioflupane) in four patients. Results: FOG developed in
all patients, with onset ranging from 1 to 15.5 years post-DBS. DaTscan
abnormalities, indicative of bilateral striatal dopaminergic denervation, were
observed in four cases. Prior to DBS, all patients exhibited dystonia
unresponsive to L-dopa, and post-DBS, FOG remained refractory to dopaminergic
treatment in most cases. Despite initial improvements in gait post-DBS, only
one patient maintained independent ambulation at the last follow-up.
Conclusions: FOG is an emerging complication in DYT-KMT2B patients with PTVs
undergoing GPi-DBS, potentially linked to underlying striatonigral
degeneration. The findings suggest a need for long-term motor surveillance and
consideration of alternative therapeutic strategies, including dopaminergic
trials, in this patient population. Further studies are required to elucidate
the precise mechanisms driving DBS-related hypokinetic gait disturbances in
DYT-KMT2B dystonia.; 30) Intelligent Spectrum Sharing in Integrated TN-NTNs: A Hierarchical Deep
  Reinforcement Learning Approach; Integrating non-terrestrial networks (NTNs) with terrestrial networks (TNs)
is key to enhancing coverage, capacity, and reliability in future wireless
communications. However, the multi-tier, heterogeneous architecture of these
integrated TN-NTNs introduces complex challenges in spectrum sharing and
interference management. Conventional optimization approaches struggle to
handle the high-dimensional decision space and dynamic nature of these
networks. This paper proposes a novel hierarchical deep reinforcement learning
(HDRL) framework to address these challenges and enable intelligent spectrum
sharing. The proposed framework leverages the inherent hierarchy of the
network, with separate policies for each tier, to learn and optimize spectrum
allocation decisions at different timescales and levels of abstraction. By
decomposing the complex spectrum sharing problem into manageable sub-tasks and
allowing for efficient coordination among the tiers, the HDRL approach offers a
scalable and adaptive solution for spectrum management in future TN-NTNs.
Simulation results demonstrate the superior performance of the proposed
framework compared to traditional approaches, highlighting its potential to
enhance spectral efficiency and network capacity in dynamic, multi-tier
environments.; 31) Deviance Detection and Regularity Sensitivity in Dissociated Neuronal
  Cultures; Understanding how neural networks process complex patterns of information is
crucial for advancing both neuroscience and artificial intelligence. To
investigate fundamental principles of neural computation, we studied
dissociated neuronal cultures, one of the most primitive living neural
networks, on high-resolution CMOS microelectrode arrays and tested whether the
dissociated culture exhibits regularity sensitivity beyond mere
stimulus-specific adaptation and deviance detection. In oddball electrical
stimulation paradigms, we confirmed that the neuronal culture produced mismatch
responses (MMRs) with true deviance detection beyond mere adaptation. These
MMRs were dependent on the N-methyl-D-aspartate (NMDA) receptors, similar to
mismatch negativity (MMN) in humans, which is known to have true deviance
detection properties. Crucially, we also showed sensitivity to the statistical
regularity of stimuli, a phenomenon previously observed only in intact brains:
the MMRs in a predictable, periodic sequence were smaller than those in a
commonly used sequence in which the appearance of the deviant stimulus was
random and unpredictable. These results challenge the traditional view that a
hierarchically structured neural network is required to process complex
temporal patterns, suggesting instead that deviant detection and regularity
sensitivity are inherent properties arising from the primitive neural network.
They also suggest new directions for the development of neuro-inspired
artificial intelligence systems, emphasizing the importance of incorporating
adaptive mechanisms and temporal dynamics in the design of neural networks.; 32) Active filtering: a predictive function of recurrent circuits of sensory
  cortex; Our brains encode many features of the sensory world into memories: we can
sing along with songs we have heard before, interpret spoken and written
language composed of words we have learned, and recognize faces and objects.
Where are these memories stored? Each area of the cerebral cortex has a huge
number of local, recurrent, excitatory-excitatory synapses, as many as 500
million per cubic millimeter. Here I review evidence that cortical recurrent
connectivity in sensory cortex is a substrate for sensory memories. Evidence
suggests that the local recurrent network encodes the structure of natural
sensory input, and that it does so via active filtering, transforming network
inputs to boost or select those associated with natural sensation. This is a
form of predictive processing, in which the cortical recurrent network
selectively amplifies some input patterns and attenuates others, and a form of
memory.; 33) Increased GM-WM in a prefrontal network and decreased GM in the insula
  and the precuneus are associated with reappraisal usage: A data fusion
  approach; Emotion regulation plays a crucial role in mental health, and difficulties in
regulating emotions can contribute to psychological disorders. While
reappraisal and suppression are well-studied strategies, the combined
contributions of gray matter (GM) and white matter (WM) to these strategies
remain unclear due to methodological limitations in previous studies. To
address this, we applied a data fusion approach using Parallel Independent
Component Analysis (Parallel ICA) to GM and WM MRI images from 165 individuals.
Parallel ICA identified two networks associated with reappraisal usage. Network
1 included a large lateral and medial prefrontal cortical network, overlapping
with the default mode network (DMN) and adjacent WM regions. Higher reappraisal
frequency was associated with greater GM-WM density within this network, and
this network was negatively correlated with perceived stress. Network 2
included the insula, precuneus, sub-gyral, and lingual gyri in its GM portion,
showing a negative association with reappraisal usage. The WM portion, adjacent
to regions of the central executive network (CEN), was positively associated
with reappraisal usage. Regarding suppression, no significant network was
associated with this strategy. This study provides new insights into individual
differences in reappraisal use, showing a positive association between
reappraisal frequency and increased gray and white matter concentration in a
large frontal network, including regions of the frontal DMN and the CEN.
Conversely, subcortical areas exhibited reduced gray and white concentration.; 34) Electrophysiological Investigation of Insect Pain Threshold; The question of whether insects experience pain has long been debated in
neuroscience and animal behavior research. Increasing evidence suggests that
insects possess the ability to detect and respond to noxious stimuli,
exhibiting behaviors indicative of pain perception. This study investigates the
relationship between pain stimuli and physiological responses in crickets
(Gryllidae), focusing on heart rate (ECG) and brain wave (EEG) patterns. We
applied a range of mechanical, chemical, thermal, and electrical stimuli to
crickets, recording ECG and EEG data while employing a deep learning-based
model to classify pain levels. Our findings revealed significant heart rate
changes and EEG fluctuations in response to various stimuli, with the highest
intensity stimuli inducing marked physiological stress. The AI-based analysis,
utilizing AlexNet for EEG signal classification, achieved 90% accuracy in
distinguishing between resting, low-pain, and high-pain states. While no social
sharing of pain was observed through ECG measurements, these results contribute
to the growing body of evidence supporting insect nociception and offer new
insights into their physiological responses to external stressors. This
research advances the understanding of insect pain mechanisms and demonstrates
the potential for AI-driven analysis in entomological studies.; 35) Dressed Subsystems in Classical Gravity; This paper considers the problem of consistently defining subsystems in
gravitational theories. It is argued that a subsystem is a spacetime subregion
in which the observables form a closed Poisson algebra. In a generally
covariant theory, the location of the subregion must be determined in relation
to other degrees of freedom. It is proposed that these degrees of freedom
should live within the region, so that an observer can determine its edge by
only measuring fields inside of it. This turns out to be equivalent to the
property that observables in the subregion generate field-dependent gauge
transformations on the causal complement. Furthermore, it is demonstrated that
this is \textit{exactly} what is necessary for the observables to form a
Poisson algebra and thus to constitute a consistent subsystem. Observables in
spacelike separated ""dressed subsystems"" are shown to commute. Several examples
are given in the context of General Relativity. Along the way, new perspectives
on the covariant phase space formalism are introduced that clarify well-known
issues, such as the factorization of subregions in gauge theories and the
unambiguous definition of Noether charges associated with one-sided boosts.
Finally, prospects for extending these results to a perturbative quantum
setting are discussed.; 36) Higher Representations and Quark Confinement; The concept of a (de)confined phase in QFT is well-defined in the presence of
$1$-form symmetries and their spontaneous symmetry breaking. However, in
scenarios where such symmetries are absent, confinement is not a well-defined
phase property. In this work, we propose that, when restricting to a specific
submanifold of the parameter space -- namely at zero temperature and fixed
quark mass -- the confined and adjoint Higgs phases of scalar QCD can be
distinguished through the different organization of their spectra, as seen from
the perspective of the baryon symmetry. The analysis is performed in terms of
an appropriate higher-categorical representation theory, recently developed for
generalized symmetries. Consistent with expectations, we find that the confined
phase permits only particles with integer baryon charges, while the Higgs phase
is characterized by the coexistence of bare quarks and center vortices,
exhibiting a non-trivial Aharonov-Bohm effect between these excitations.; 37) Power-law banded random matrix ensemble as a model for quantum many-body
  Hamiltonians; Hamiltonians of one-dimensional, disordered single-particle systems with
long-range hopping terms can naturally be modeled by power-law banded random
matrices. In this picture, the phase diagram of a power-law banded random
matrix ensemble show ergodic, weakly ergodic, multifractal, and localized
phases. Motivated by recent developments on ergodicity breaking and
localization in interacting quantum many-body systems, we explore many-body
interpretations of the power-law banded random matrix ensemble. We discuss a
number of ways to label the basis vectors with many-body configurations, and
compare the physical properties of the resulting Hamiltonians. We characterize
the scaling of the many-body eigenstate entanglement entropy with system size
for the different labeling schemes and in each of the phases. Using a scaling
analysis on the full sets of eigenstates, we subsequently provide a
quantitative picture of the boundary between the different types of scaling
behavior that we observe for the spectral-bulk and spectral-edge eigenstates.; 38) The Role of Affective States in Computational Psychiatry; Studying psychiatric illness has often been limited by difficulties in
connecting symptoms and behavior to neurobiology. Computational psychiatry
approaches promise to bridge this gap by providing formal accounts of the
latent information processing changes that underlie the development and
maintenance of psychiatric phenomena. Models based on these theories generate
individual-level parameter estimates which can then be tested for relationships
to neurobiology. In this review, we explore computational modelling approaches
to one key aspect of health and illness: affect. We discuss strengths and
limitations of key approaches to modelling affect, with a focus on
reinforcement learning, active inference, the hierarchical gaussian filter, and
drift-diffusion models. We find that, in this literature, affect is an
important source of modulation in decision making, and has a bidirectional
influence on how individuals infer both internal and external states.
Highlighting the potential role of affect in information processing changes
underlying symptom development, we extend an existing model of psychosis, where
affective changes are influenced by increasing cortical noise and consequent
increases in either perceived environmental instability or expected noise in
sensory input, becoming part of a self-reinforcing process generating
negatively valenced, over-weighted priors underlying positive symptom
development. We then provide testable predictions from this model at
computational, neurobiological, and phenomenological levels of description.; 39) Handling Incomplete Heterogeneous Data using a Data-Dependent Kernel; Handling incomplete data in real-world applications is a critical challenge
due to two key limitations of existing methods: (i) they are primarily designed
for numeric data and struggle with categorical or heterogeneous/mixed datasets;
(ii) they assume that data is missing completely at random, which is often not
the case in practice -- in reality, data is missing in patterns, leading to
biased results if these patterns are not accounted for. To address these two
limitations, this paper presents a novel approach to handling missing values
using the Probability Mass Similarity Kernel (PMK), a data-dependent kernel,
which does not make any assumptions about data types and missing mechanisms. It
eliminates the need for prior knowledge or extensive pre-processing steps and
instead leverages the distribution of observed data. Our method unifies the
representation of diverse data types by capturing more meaningful pairwise
similarities and enhancing downstream performance. We evaluated our approach
across over 10 datasets with numerical-only, categorical-only, and mixed
features under different missing mechanisms and rates. Across both
classification and clustering tasks, our approach consistently outperformed
existing techniques, demonstrating its robustness and effectiveness in managing
incomplete heterogeneous data.; 40) Dynamical phases of short-term memory mechanisms in RNNs; Short-term memory is essential for cognitive processing, yet our
understanding of its neural mechanisms remains unclear. A key focus in
neuroscience has been the study of sequential activity patterns, where neurons
fire one after another within large networks, to explain how information is
maintained. While recurrent connections were shown to drive sequential
dynamics, a mechanistic understanding of this process still remains unknown. In
this work, we first introduce two unique mechanisms that can subserve
short-term memory: slow-point manifolds generating direct sequences or limit
cycles providing temporally localized approximations. Then, through analytical
models, we identify fundamental properties that govern the selection of these
mechanisms, \textit{i.e.}, we derive theoretical scaling laws for critical
learning rates as a function of the delay period length, beyond which no
learning is possible. We empirically verify these observations by training and
evaluating more than 35,000 recurrent neural networks (RNNs) that we will
publicly release upon publication. Overall, our work provides new insights into
short-term memory mechanisms and proposes experimentally testable predictions
for systems neuroscience.; 41) Normative Cerebral Perfusion Across the Lifespan; Cerebral perfusion plays a crucial role in maintaining brain function and is
tightly coupled with neuronal activity. While previous studies have examined
cerebral perfusion trajectories across development and aging, precise
characterization of its lifespan dynamics has been limited by small sample
sizes and methodological inconsistencies. In this study, we construct the first
comprehensive normative model of cerebral perfusion across the human lifespan
(birth to 85 years) using a large multi-site dataset of over 12,000
high-quality arterial spin labeling (ASL) MRI scans. Leveraging generalized
additive models for location, scale, and shape (GAMLSS), we mapped nonlinear
growth trajectories of cerebral perfusion at global, network, and regional
levels. We observed a rapid postnatal increase in cerebral perfusion, peaking
at approximately 7.1 years, followed by a gradual decline into adulthood. Sex
differences were evident, with distinct regional maturation patterns rather
than uniform differences across all brain regions. Beyond normative modeling,
we quantified individual deviations from expected CBF patterns in
neurodegenerative and psychiatric conditions, identifying disease-specific
perfusion abnormalities across four brain disorders. Using longitudinal data,
we established typical and atypical cerebral perfusion trajectories,
highlighting the prognostic value of perfusion-based biomarkers for detecting
disease progression. Our findings provide a robust normative framework for
cerebral perfusion, facilitating precise characterization of brain health
across the lifespan and enhancing the early identification of neurovascular
dysfunction in clinical populations.; 42) A Program Logic for Under-approximating Worst-case Resource Usage; Understanding and predicting the worst-case resource usage is crucial for
software quality; however, existing methods either over-approximate with
potentially loose bounds or under-approximate without asymptotic guarantees.
This paper presents a program logic to under-approximate worst-case resource
usage, adapting incorrectness logic (IL) to reason quantitatively about
resource consumption. We propose quantitative forward and backward
under-approximate (QFUA and QBUA) triples, which generalize IL to identify
execution paths leading to high resource usage. We also introduce a variant of
QBUA that supports reasoning about high-water marks. Our logic is proven sound
and complete with respect to a simple IMP-like language, and we demonstrate its
utility through case studies involving arrays, pointers, and procedure calls.; 43) Mechanoreceptive A$\beta$ primary afferents discriminate naturalistic
  social touch inputs at a functionally relevant time scale; Interpersonal touch is an important channel of social emotional interaction.
How these physical skin-to-skin touch expressions are processed in the
peripheral nervous system is not well understood. From microneurography
recordings in humans, we evaluated the capacity of six subtypes of cutaneous
mechanoreceptive afferents to differentiate human-delivered social touch
expressions. Leveraging statistical and classification analyses, we found that
single units of multiple mechanoreceptive A$\beta$ subtypes, especially slowly
adapting type II (SA-II) and fast adapting hair follicle afferents (HFA), can
reliably differentiate social touch expressions at accuracies similar to human
recognition. We then identified the most informative firing patterns of SA-II
and HFA afferents, which indicate that average durations of 3-4 s of firing
provide sufficient discriminative information. Those two subtypes also exhibit
robust tolerance to spike-timing shifts of up to 10-20 ms, varying with touch
expressions due to their specific firing properties. Greater shifts in
spike-timing, however, can change a firing pattern's envelope to resemble that
of another expression and drastically compromise an afferent's discrimination
capacity. Altogether, the findings indicate that SA-II and HFA afferents
differentiate the skin contact of social touch at time scales relevant for such
interactions, which are 1-2 orders of magnitude longer than those for
non-social touch.; 44) Routing Optimization Based on Distributed Intelligent Network
  Softwarization for the Internet of Things; The Internet of Things (IoT) establishes connectivity between billions of
heterogeneous devices that provide a variety of essential everyday services.
The IoT faces several challenges, including energy efficiency and scalability,
that require consideration of enabling technologies such as network
softwarization. This technology is an appropriate solution for IoT, leveraging
Software Defined Networking (SDN) and Network Function Virtualization (NFV) as
two main techniques, especially when combined with Machine Learning (ML).
Although many efforts have been made to optimize routing in softwarized IoT,
the existing solutions do not take advantage of distributed intelligence. In
this paper, we propose to optimize routing in softwarized IoT networks using
Federated Deep Reinforcement Learning (FDRL), where distributed network
softwarization and intelligence (i.e., FDRL) join forces to improve routing in
constrained IoT networks. Our proposal introduces the combination of two
novelties (i.e., distributed controller design and intelligent routing) to meet
the IoT requirements (mainly performance and energy efficiency). The simulation
results confirm the effectiveness of our proposal compared to the conventional
counterparts.; 45) Reasoning within and between collective action problems; Understanding cooperation in social systems is challenging because the
ever-changing rules that govern societies interact with individual actions,
resulting in intricate collective outcomes. In virtual-world experiments, we
allowed people to make changes in the systems that they are making decisions
within and investigated how they weigh the influence of different rules in
decision-making. When choosing between worlds differing in more than one rule,
a naive heuristics model predicted participants decisions as well, and in some
cases better, than game earnings (utility) or by the subjective quality of
single rules. In contrast, when a subset of engaged participants made
instantaneous (within-world) decisions, their behavior aligned very closely
with objective utility and not with the heuristics model. Findings suggest
that, whereas choices between rules may deviate from rational benchmarks, the
frequency of real time cooperation decisions to provide feedback can be a
reliable indicator of the objective utility of these rules.; 46) Physics-Aware Inverse Design for Nanowire Single-Photon Avalanche
  Detectors via Deep Learning; Single-photon avalanche detectors (SPADs) have enabled various applications
in emerging photonic quantum information technologies in recent years. However,
despite many efforts to improve SPAD's performance, the design of SPADs
remained largely an iterative and time-consuming process where a designer makes
educated guesses of a device structure based on empirical reasoning and solves
the semiconductor drift-diffusion model for it. In contrast, the inverse
problem, i.e., directly inferring a structure needed to achieve desired
performance, which is of ultimate interest to designers, remains an unsolved
problem. We propose a novel physics-aware inverse design workflow for SPADs
using a deep learning model and demonstrate it with an example of finding the
key parameters of semiconductor nanowires constituting the unit cell of an
SPAD, given target photon detection efficiency. Our inverse design workflow is
not restricted to the case demonstrated and can be applied to design
conventional planar structure-based SPADs, photodetectors, and solar cells.; 47) I Think, Therefore I Hallucinate: Minds, Machines, and the Art of Being
  Wrong; This theoretical work examines 'hallucinations' in both human cognition and
large language models, comparing how each system can produce perceptions or
outputs that deviate from reality. Drawing on neuroscience and machine learning
research, we highlight the predictive processes that underlie human and
artificial thought. In humans, complex neural mechanisms interpret sensory
information under uncertainty, sometimes filling in gaps and creating false
perceptions. This inference occurs hierarchically: higher cortical levels send
top-down predictions to lower-level regions, while mismatches (prediction
errors) propagate upward to refine the model. LLMs, in contrast, rely on
auto-regressive modeling of text and can generate erroneous statements in the
absence of robust grounding. Despite these different foundations - biological
versus computational - the similarities in their predictive architectures help
explain why hallucinations occur. We propose that the propensity to generate
incorrect or confabulated responses may be an inherent feature of advanced
intelligence. In both humans and AI, adaptive predictive processes aim to make
sense of incomplete information and anticipate future states, fostering
creativity and flexibility, but also introducing the risk of errors. Our
analysis illuminates how factors such as feedback, grounding, and error
correction affect the likelihood of 'being wrong' in each system. We suggest
that mitigating AI hallucinations (e.g., through improved training,
post-processing, or knowledge-grounding methods) may also shed light on human
cognitive processes, revealing how error-prone predictions can be harnessed for
innovation without compromising reliability. By exploring these converging and
divergent mechanisms, the paper underscores the broader implications for
advancing both AI reliability and scientific understanding of human thought.; 48) Using economic value signals from primate prefrontal cortex in
  neuro-engineering applications; Neural signals related to movement can be measured from intracranial
recordings and used in brain-machine interface devices (BMI) to restore
physical function in impaired patients. In this study, we explore the use of
more abstract neural signals related to economic value in a BMI context. Using
data collected from the orbitofrontal cortex in non-human primates, we develop
deep learning-based neural decoders that can predict the monkey's choice in a
value-based decision-making task. Out-of-sample performance was improved by
augmenting the training set with synthesized data, showing the feasibility of
using limited training data. We further demonstrate that we can predict the
monkey's choice sooner using a neural forecasting module that is equipped with
task-related information. These findings support the feasibility of user
preference-informed neuroengineering devices that leverage abstract cognitive
signals.; 49) $^{18}$F-FDG brain PET hypometabolism in post-SARS-CoV-2 infection:
  substrate for persistent/delayed disorders?; Purpose: Several brain complications of SARS-CoV-2 infection have been
reported. It has been moreover speculated that this neurotropism could
potentially cause a delayed outbreak of neuropsychiatric and neurodegenerative
diseases of neuroinflammatory origin. A propagation mechanism has been proposed
across the cribriform plate of the ethmoid bone, from the nose to the olfactory
epithelium, and possibly afterward to other limbic structures, and deeper parts
of the brain including the brainstem. Methods: Review of clinical examination,
and whole-brain voxel-based analysis of $^{18}$F-FDG PET metabolism in
comparison with healthy subjects (p voxel<0.001, p-cluster<0.05, uncorrected),
of two patients with confirmed diagnosis of SARS-CoV-2 explored at the
post-viral stage of the disease. Results: Hypometabolism of the
olfactory/rectus gyrus was found on the two patients, especially one with
4-week prolonged anosmia. Additional hypometabolisms were found within
amygdala, hippocampus, parahippocampus, cingulate cortex, pre-/post-central
gyrus, thalamus/hypothalamus, cerebellum, pons, and medulla in the other
patient who complained of delayed onset of a painful syndrome. Conclusion:
These preliminary findings reinforce the hypotheses of SARS-CoV-2 neurotropism
through the olfactory bulb and the possible extension of this impairment to
other brain structures. $^{18}$F-FDG PET hypometabolism could constitute a
cerebral quantitative biomarker of this involvement. Post-viral cohort studies
are required to specify the exact relationship between such hypometabolisms and
the possible persistent disorders, especially involving cognitive or emotion
disturbances, residual respiratory symptoms, or painful complaints.; 50) Interplay between Static and Dynamic Disorder: Contrasting Effects on
  Dark State Population inside a Cavity; Strong light-matter interactions between molecules and quantized
electromagnetic fields inside an optical cavity open up novel possibilities,
though inevitably influenced by disorder, an inherent attribute of realistic
molecular systems. Here, we explore the steady-state optical response of
molecular emitters within a lossy cavity, with a focus on the combined effects
of static and dynamic disorder, as frequently observed in solution-phase
experiments. By analyzing the transmission spectra, molecular energy change,
and dark state population, we uncover the contrasting effects of static and
dynamic disorder on the dark state population and its interplay with polariton
states. We find that the Rabi splitting exhibits an inversion with increasing
disorder strength where the maximum splitting is determined by the interplay of
static and dynamic disorder. Furthermore, we identify a dark state-induced
polariton linewidth narrowing, revealing a mechanism distinct from motional
narrowing induced by frequency fluctuations. These mechanistic insights
highlight the critical role of dark states, establishing a foundation for
future developments in the fields of polariton chemistry and strong coupling
spectroscopy.; 51) Automatic target validation based on neuroscientific literature mining
  for tractography; Target identification for tractography studies requires solid anatomical
knowledge validated by an extensive literature review across species for each
seed structure to be studied. Manual literature review to identify targets for
a given seed region is tedious and potentially subjective. Therefore,
complementary approaches would be useful. We propose to use text-mining models
to automatically suggest potential targets from the neuroscientific literature,
full-text articles and abstracts, so that they can be used for anatomical
connection studies and more specifically for tractography. We applied
text-mining models to three structures: two well-studied structures, since
validated deep brain stimulation targets, the internal globus pallidus and the
subthalamic nucleus and, the nucleus accumbens, an exploratory target for
treating psychiatric disorders. We performed a systematic review of the
literature to document the projections of the three selected structures and
compared it with the targets proposed by text-mining models, both in rat and
primate (including human). We ran probabilistic tractography on the nucleus
accumbens and compared the output with the results of the text-mining models
and literature review. Overall, text-mining the literature could find three
times as many targets as two man-weeks of curation could. The overall
efficiency of the text-mining against literature review in our study was 98%
recall (at 36% precision), meaning that over all the targets for the three
selected seeds, only one target has been missed by text-mining. We demonstrate
that connectivity for a structure of interest can be extracted from a very
large amount of publications and abstracts. We believe this tool will be useful
in helping the neuroscience community to facilitate connectivity studies of
particular brain regions. The text mining tools used for the study are part of
the HBP Neuroinformatics Platform, publicly available at
http://connectivity-brainer.rhcloud.com; 52) Asynchronous Hebbian/anti-Hebbian networks; Lateral inhibition models coupled with Hebbian plasticity have been shown to
learn factorised causal representations of input stimuli, for instance,
oriented edges are learned from natural images. Currently, these models require
the recurrent dynamics to settle into a stable state before weight changes can
be applied, which is not only biologically implausible, but also impractical
for real-time learning systems. Here, we propose a new Hebbian learning rule
which is implemented using plausible biological mechanisms that have been
observed experimentally. We find that this rule allows for efficient,
time-continuous learning of factorised representations, very similar to the
classic noncontinuous Hebbian/anti-Hebbian learning. Furthermore, we show that
this rule naturally prevents catastrophic forgetting when stimuli from
different distributions are shown sequentially.; 53) Language modulates vision: Evidence from neural networks and human
  brain-lesion models; Comparing information structures in between deep neural networks (DNNs) and
the human brain has become a key method for exploring their similarities and
differences. Recent research has shown better alignment of vision-language DNN
models, such as CLIP, with the activity of the human ventral occipitotemporal
cortex (VOTC) than earlier vision models, supporting the idea that language
modulates human visual perception. However, interpreting the results from such
comparisons is inherently limited due to the ""black box"" nature of DNNs. To
address this, we combined model-brain fitness analyses with human brain lesion
data to examine how disrupting the communication pathway between the visual and
language systems causally affects the ability of vision-language DNNs to
explain the activity of the VOTC. Across four diverse datasets, CLIP
consistently outperformed both label-supervised (ResNet) and unsupervised
(MoCo) models in predicting VOTC activity. This advantage was left-lateralized,
aligning with the human language network. Analyses of the data of 33 stroke
patients revealed that reduced white matter integrity between the VOTC and the
language region in the left angular gyrus was correlated with decreased CLIP
performance and increased MoCo performance, indicating a dynamic influence of
language processing on the activity of the VOTC. These findings support the
integration of language modulation in neurocognitive models of human vision,
reinforcing concepts from vision-language DNN models. The sensitivity of
model-brain similarity to specific brain lesions demonstrates that leveraging
manipulation of the human brain is a promising framework for evaluating and
developing brain-like computer models.; 54) Integration of monomials over the unit spere and unit ball in $R^n$; We compute the integral of monomials of the form $x^{2\beta}$ over the unit
sphere and the unit ball in $R^n$ where $\beta = (\beta_1,...,\beta_n)$ is a
multi-index with real components $\beta_k > -1/2$, $1 \le k \le n$, and discuss
their asymptotic behavior as some, or all, $\beta_k \to\infty$. This allows for
the evaluation of integrals involving circular and hyperbolic trigonometric
functions over the unit sphere and the unit ball in $ R^n$. We also consider
the Fourier transform of monomials $x^\alpha$ restricted to the unit sphere in
$R^n$, where the multi-indices $\alpha$ have integer components, and discuss
their behaviour at the origin.; 55) Online Authentication Habits of Indian Users; Passwords have been long used as the primary authentication method for web
services. Weak passwords used by the users have prompted the use of password
management tools and two-factor authentication to ensure better account
security. While prior studies have studied their adoption individually, none of
these studies focuses particularly on the Indian setting, which is culturally
and economically different from the countries in which these studies have been
done in the past. To this end, we conducted a survey with 90 participants
residing in India to better understand the mindset of people on using password
managers and two-factor authentication (2FA).
  Our findings suggest that a majority of the participants have used 2FA and
password managers in some form, although they are sometimes unaware of their
formal names. While many participants used some form of 2FA across all their
accounts, browser-integrated and device-default password managers are
predominantly utilized for less sensitive platforms such as e-commerce and
social media rather than for more critical accounts like banking. The primary
motivation for using password managers is the convenience of auto-filling.
However, some participants avoid using password managers due to a lack of trust
in these tools. Notably, dedicated third-party applications show low adoption
for both password manager and 2FA.
  Despite acknowledging the importance of secure password practices, many
participants still reuse passwords across multiple accounts, prefer shorter
passwords, and use commonly predictable password patterns. Overall, the study
suggests that Indians are more inclined to choose default settings,
underscoring the need for tailored strategies to improve user awareness and
strengthen password security practices.; 56) Hybrid Brain-Machine Interface: Integrating EEG and EMG for Reduced
  Physical Demand; We present a hybrid brain-machine interface (BMI) that integrates
steady-state visually evoked potential (SSVEP)-based EEG and facial EMG to
improve multimodal control and mitigate fatigue in assistive applications.
Traditional BMIs relying solely on EEG or EMG suffer from inherent limitations;
EEG-based control requires sustained visual focus, leading to cognitive
fatigue, while EMG-based control induces muscular fatigue over time. Our system
dynamically alternates between EEG and EMG inputs, using EEG to detect SSVEP
signals at 9.75 Hz and 14.25 Hz and EMG from cheek and neck muscles to optimize
control based on task demands. In a virtual turtle navigation task, the hybrid
system achieved task completion times comparable to an EMG-only approach, while
90% of users reported reduced or equal physical demand. These findings
demonstrate that multimodal BMI systems can enhance usability, reduce strain,
and improve long-term adherence in assistive technologies.; 57) RacerF: Lightweight Static Data Race Detection for C Code; We present a novel static analysis for thread-modular data race detection.
Our approach exploits static analysis of sequential program behaviour whose
results are generalised for multi-threaded programs using a combination of
lightweight under- and over-approximating methods. We have implemented this
approach in a new tool called RacerF as a plugin of the Frama-C platform.
RacerF can leverage several analysis backends, most notably the Frama-C's
abstract interpreter EVA. Although our methods are mostly heuristic without
providing formal guarantees, our experimental evaluation shows that even for
intricate programs, RacerF can provide very precise results competitive with
more heavy-weight approaches while being faster than them.; 58) Di-decay signature of new physics particles at intensity frontier
  experiments; We explore the potential of intensity frontier experiments to search for
unstable feebly interacting particles (FIPs) via signatures that offer deeper
insights than the standard signature of a single FIP decay. Specifically, we
study models where FIPs are produced in pairs in a single event, so both can
simultaneously decay in the detector. These ``di-decay'' events allow us to
identify the mother particle decaying into the FIP pair and reconstruct its
properties, provide further insights into the dark sector couplings to the SM,
and are much cleaner against backgrounds. As a concrete example, we focus on
the class of models where the FIPs couple quadratically to the Higgs boson and
derive the sensitivities of SHiP and Belle II to di-decays of Higgs-like
scalars. In particular, at Belle II, finite backgrounds for the single decay
signature make di-decays competitive, providing a distinctive handle on FIP
production and lifetime.; 59) Targeting C99 Mediated Metabolic Disruptions with Ketone Therapy in
  Alzheimer's Disease; The role of ketone bodies in Alzheimers disease (AD) remains incompletely
understood, particularly regarding their influence on amyloid pathology. While
beta}hydroxybutyrate (BHB) has been implicated in neuroprotection, direct
evidence for its effects on amyloid beta(Abeta) deposition, aggregation, or
clearance is lacking. Furthermore, whether BHB acts as a disease modifying
factor or merely confers transient metabolic benefits remains unclear.
Addressing this gap is crucial for evaluating the therapeutic potential of
ketone metabolism in AD. Here, we investigated the impact of ketone bodies on
amyloidogenic toxicity using a Drosophila melanogaster model with targeted
expression of human amyloid precursor protein (APP), beta secretase 1 (BACE1),
Abeta, and the C99 fragment, an essential intermediate in Abeta generation.
Surprisingly, we found that Abeta alone elicited minimal neurotoxicity, whereas
C99 expression induced pronounced pathological effects, suggesting a critical,
underappreciated role of C99 in AD progression. Further analysis revealed that
C99 driven toxicity was associated with autophagic and lysosomal dysfunction,
leading to impaired protein clearance, oxidative stress, and mitochondrial
abnormalities. Using confocal microscopy and lysosomal pH sensitive markers, we
demonstrated that BHB treatment restored lysosomal function and alleviated
these pathological changes. Protein protein interaction network analysis in C99
expressing Drosophila brains identified protein phosphatase methylesterase 1
(PPME1) activation as a key driver of autophagic impairment, further supported
by machine learning predictions. Finally, mathematical similarity analysis of
PPI networks suggested that BHB may exert its neuroprotective effects through
mTOR inhibition, positioning it as a potential endogenous modulator of AD
related pathology.; 60) Dissipative anomalies of stresses in soft amorphous solids: footprints
  of density singularities; In soft amorphous solids, localized irreversible (plastic) stress dissipation
occurs as a response to external forcings. A crucial question is whether we can
identify structural properties linked to a region's propensity to undergo a
plastic stress drop when thermal effects are negligible. To address this
question, I follow a theoretical framework provided by Onsager's ideal
turbulence theory, representing a non-perturbative application of the
renormalization group scale-invariance principle. First, I analyze the zero
temperature limit for the fine-grained balance equation for the stress tensor
corresponding to instanton realizations. I show that irreversible stress drops
can occur if the density gradients diverge. I then derive a balance relation
for the coarse-grained instantaneous stress tensor with arbitrary
regularization scale $\ell$. From the latter, I obtain an expression for the
local inter-scale stress flux in terms of moments of the density increments. By
assuming that the density field is Besov regular, I determine the scaling of
the stress flux with $\ell$. From this scaling, I show that distributional
solutions of the noiseless Dean (NDE) equation can sustain stress dissipation
due to a non-equilibrium inter-scale stress flux if the scaling exponents of
the density structure functions are below a critical threshold. The athermal
limit of fine-grained and coarse-grained descriptions must describe the same
phenomenology, \textit{i.e.} the existence of stress dissipation must be
independent of any regularization of the dynamics. Using this principle, I
analyze the limit $\ell\rightarrow 0$ and argue that flow realizations of
athermal disordered systems correspond to ultraviolet fixed-point solutions of
the coarse-grained NDE equations with sufficiently low Besov regularity.; 61) Coevolving aerodynamic and impact ripples on Earth; Windblown sand creates multiscale bedforms on Earth, Mars, and other
planetary bodies. According to conventional wisdom, decameter-scale dunes and
decimeter-scale ripples emerge via distinct mechanisms on Earth: a hydrodynamic
instability related to a phase shift between the turbulent flow and the
topography, and a granular instability related to a synchronization of hopping
grains with the topography. Here, we report the reproducible creation of
coevolving centimeter and decimeter-scale ripples on fine-grained monodisperse
sand beds in ambient-air and low-pressure wind-tunnels, revealing two adjacent
mesoscale growth instabilities. Their morphological traits and our quantitative
grain-scale numerical simulations authenticate the smaller structures as impact
ripples but point at a hydrodynamic origin for the larger ones. This suggests
that the aeolian transport layer would have to partially respond to the
topography on a scale comparable to the average hop length, hence faster than
previously thought, but consistent with the phase lag of the inferred aeolian
sand flux relative to the wind. Hydrodynamic modelling supports the existence
of hydrodynamic aerodynamic ripples on Earth, connecting them mechanistically
to megaripples and to the debated Martian ripples. We thereby propose a unified
framework for mesoscale granular bedforms found across the Solar System.; 62) Causal Spike Timing Dependent Plasticity Prevents Assembly Fusion in
  Recurrent Networks; The organization of neurons into functionally related assemblies is a
fundamental feature of cortical networks, yet our understanding of how these
assemblies maintain distinct identities while sharing members remains limited.
Here we analyze how spike-timing-dependent plasticity (STDP) shapes the
formation and stability of overlapping neuronal assemblies in recurrently
coupled networks of spiking neuron models. Using numerical simulations and an
associated mean-field theory, we demonstrate that the temporal structure of the
STDP rule, specifically its degree of causality, critically determines whether
assemblies that share neurons maintain segregation or merge together after
training is completed. We find that causal STDP rules, where
potentiation/depression occurs strictly when presynaptic spikes precede/proceed
postsynaptic spikes, allow assemblies to remain distinct even with substantial
overlap in membership. This stability arises because causal STDP effectively
cancels the symmetric correlations introduced by common inputs from shared
neurons. In contrast, acausal STDP rules lead to assembly fusion when overlap
exceeds a critical threshold, due to unchecked growth of common input
correlations. Our results provide theoretical insight into how
spike-timing-dependent learning rules can support distributed representation
where individual neurons participate in multiple assemblies while maintaining
functional specificity.; 63) Intrinsic motivation as constrained entropy maximization; ""Intrinsic motivation"" refers to the capacity for intelligent systems to be
motivated endogenously, i.e. by features of agential architecture itself rather
than by learned associations between action and reward. This paper views active
inference, empowerment, and other formal accounts of intrinsic motivation as
variations on the theme of constrained maximum entropy inference, providing a
general perspective on intrinsic motivation complementary to existing
frameworks. The connection between free energy and empowerment noted in
previous literature is further explored, and it is argued that the
maximum-occupancy approach in practice incorporates an implicit model-evidence
constraint.; 64) Calabi-Yau Period Geometry and Restricted Moduli in Type II
  Compactifications; The period geometry of Calabi-Yau $n$-folds, characterised by their
variations of Hodge structure governed by Griffiths transversality, a graded
Frobenius algebra, an integral monodromy and an intriguing arithmetic
structure, is analysed for applications in string compactifications and to
Feynman integrals. In particular, we consider type IIB flux compactifications
on Calabi-Yau three-folds and elliptically fibred four-folds. After
constructing suitable three-parameter three-folds, we examine the relation
between symmetries of their moduli spaces and flux configurations. Although the
fixed point loci of these symmetries are projective special K\""ahler, we show
that a simultaneous stabilisation of multiple moduli on the intersection of
these loci need not be guaranteed without the existence of symmetries between
them. We furthermore consider F-theory vacua along conifolds and use mirror
symmetry to perform a complete analysis of the two-parameter moduli space of an
elliptic Calabi-Yau four-fold fibred over $\mathbb{P}^3$. We use the relation
between Calabi-Yau period geometries in various dimensions and, in particular,
the fact that the antisymmetric products of one-parameter Calabi-Yau three-fold
operators yield four-fold operators to establish pairs of flux vacua on the
moduli spaces of the three- and four-fold compactifications. We give a
splitting of the period matrix into a semisimple and nilpotent part by
utilising the Frobenius structure. This helps bringing $\epsilon$-dimensional
regulated integration by parts relations between Feynman integrals into
$\epsilon$-factorised form and solve them by iterated integrals of the periods.; 65) Auto-Associative Memories for Direct Signalling of Visual Angle During
  Object Approaches; Being hit by a ball is usually not a pleasant experience. While a ball may
not be fatal, other objects can be. To protect themselves, many organisms, from
humans to insects, have developed neuronal mechanisms to signal approaching
objects such as predators and obstacles. The study of these neuronal circuits
is still ongoing, both experimentally and theoretically. Many computational
proposals rely on temporal contrast integration, as it encodes how the visual
angle of an approaching object changes with time. However, mechanisms based on
contrast integration are severely limited when the observer is also moving, as
it is difficult to distinguish the background-induced temporal contrast from
that of an approaching object. Here, I present results of a new mechanism for
signaling object approaches, based on modern content-addressable
(auto-associative) memories. Auto-associative memories were first proposed by
Hopfield in 1982, and are a class of simple neuronal networks which transform
incomplete or noisy input patterns to complete and noise-free output patterns.
The memory holds different sizes of a generic pattern template that is
efficient for segregating an approaching object from irrelevant background
motion. Therefore, the model's output correlates directly with angular size.
Generally, the new mechanism performs on a par with previously published
models. The overall performance was systematically evaluated based on the
network's responses to artificial and real-world video footage.; 66) Role of connectivity anisotropies in the dynamics of cultured neuronal
  networks; Laboratory-grown, engineered living neuronal networks in vitro have emerged
in the last years as an experimental technique to understand the collective
behavior of neuronal assemblies in relation to their underlying connectivity.
An inherent obstacle in the design of such engineered systems is the difficulty
to predict the dynamic repertoire of the emerging network and its dependence on
experimental variables. To fill this gap, and inspired on recent experimental
studies, here we present a numerical model that aims at, first, replicating the
anisotropies in connectivity imprinted through engineering, to next realize the
collective behavior of the neuronal network and make predictions. We use
experimentally measured, biologically-realistic data combined with the
Izhikevich model to quantify the dynamics of the neuronal network in relation
to tunable structural and dynamical parameters. These parameters include the
synaptic noise, strength of the imprinted anisotropies, and average axon
lengths. The latter are involved in the simulation of the development of
neurons in vitro. We show that the model captures the behavior of engineered
neuronal cultures, in which a rich repertoire of activity patterns emerge but
whose details are strongly dependent on connectivity details and noise. Results
also show that the presence of connectivity anisotropies substantially improves
the capacity of reconstructing structural connectivity from activity data, an
aspect that is important in the quest for understanding the
structure-to-function relationship in neuronal networks. Our work provides the
in silico basis to assist experimentalists in the design of laboratory in vitro
networks and anticipate their outcome, an aspect that is particularly important
in the effort to conceive reliable brain-on-a-chip circuits and explore key
aspects such as input-output relationships or information coding.; 67) Meissner-Like Currents of Photons in Anomalous Superradiant Phases; The Meissner effect, a signature feature of superconductors, involves
circular surface currents that cancel an external field. In this study, we
present our findings on Meissner-like currents of photons in highly tunable
light-matter interaction systems. In a quantum Rabi zigzag chain exposed to a
staggered magnetic field, we identify a Meissner superradiant phase,
manifesting persistent chiral edge currents in the ground state.
Counter-flowing edge currents arise in each species of cavities,leading to
complete cancellation of net currents throughout the entire chain. This
phenomenon is analogous to surface currents in the Meissner effect. The
Meissner phase is signaled by the unusual scaling exponents of the lowest
excitation energy, which exhibit anomalous criticality with and without
geometric frustration in each species. Intriguingly, adjusting the staggered
flux induces transitions from the Meissner phase to either the even-chiral or
odd-chiral superradiant phases, where the chiral edge currents flow exclusively
in even or odd cavities, respectively. Additionally, by enhancing interspecies
interactions, chiral currents vanish in a ferromagnetic superradiant phase. Our
realization of Meissner-like currents of photons opens avenues for exploring
edge state interferometry and quantum Hall effects within light-matter coupling
platforms.; 68) Spaces and sequences in the hippocampus: a homological perspective; Topological techniques have become a popular tool for studying information
flows in neural networks. In particular, simplicial homology theory is used to
analyze how cognitive representations of space emerge from large conglomerates
of independent neuronal contributions. Meanwhile, a growing number of studies
suggest that many cognitive functions are sustained by serial patterns of
activity. Here, we investigate stashes of such patterns using path homology
theory -- an impartial, universal approach that does not require a priori
assumptions about the sequences' nature, functionality, underlying mechanisms,
or other contexts. We focus on the hippocampus -- a key enabler of learning and
memory in mammalian brains -- and quantify the ordinal arrangement of its
activity similarly to how its topology has previously been studied in terms of
simplicial homologies. The results reveal that the vast majority of sequences
produced during spatial navigation are structurally equivalent to one another.
Only a few classes of distinct sequences form an ordinal schema of serial
activity that remains stable as the pool of sequences consolidates.
Importantly, the structure of both maps is upheld by combinations of short
sequences, suggesting that brief activity motifs dominate physiological
computations. This ordinal organization emerges and stabilizes on timescales
characteristic of spatial learning, displaying similar dynamics. Yet, the
ordinal maps generally do not reflect topological affinities -- spatial and
sequential analyses address qualitatively different aspects of spike flows,
representing two complementary formats of information processing.; 69) Phase-Sensitive Enhanced Absorption, Transmission and Slow Light in a
  Cross-cavity Magnomechanical System; We theoretically propose a scheme to explore the magnetically and
magnomechanically induced transparency phenomena in a cross-cavity
magnomechanical system, focusing on the role of relative phase and the
intensity of the two probing fields in enhancing the absorption and
transmission spectra and manipulating the group delay of the transmitted light.
Interestingly, the relative phase of the two probe fields could have
overwhelming effects on both the absorption spectrum and the group delay of the
output field. Tuning the relative phase and amplitude of the probe fields can
suppress or enhance the absorption and transmission spectra. The combined
effect of the magnon-photon and magnon-phonon couplings, along with relative
phase modulations, helps to switch the probe field's behavior from subluminal
to superluminal in the current system. The current study offers a
straightforward and practical approach, demonstrating the capability to employ
the relative phase for the modulation of microwave signals within the cavity
magnomechanical system, providing insights for the design of information
transduction and quantum sensing.; 70) Converting Transformers into DGNNs Form; Recent advances in deep learning have established Transformer architectures
as the predominant modeling paradigm. Central to the success of Transformers is
the self-attention mechanism, which scores the similarity between query and key
matrices to modulate a value matrix. This operation bears striking similarities
to digraph convolution, prompting an investigation into whether digraph
convolution could serve as an alternative to self-attention. In this study, we
formalize this concept by introducing a synthetic unitary digraph convolution
based on the digraph Fourier transform. The resulting model, which we term
Converter, effectively converts a Transformer into a Directed Graph Neural
Network (DGNN) form. We have tested Converter on Long-Range Arena benchmark,
long document classification, and DNA sequence-based taxonomy classification.
Our experimental results demonstrate that Converter achieves superior
performance while maintaining computational efficiency and architectural
simplicity, which establishes it as a lightweight yet powerful Transformer
variant.; 71) Do You ""Trust"" This Visualization? An Inventory to Measure Trust in
  Visualizations; Trust plays a critical role in visual data communication and decision-making,
yet existing visualization research employs varied trust measures, making it
challenging to compare and synthesize findings across studies. In this work, we
first took a bottom-up, data-driven approach to understand what visualization
readers mean when they say they ""trust"" a visualization. We compiled and
adapted a broad set of trust-related statements from existing inventories and
collected responses on visualizations with varying degrees of trustworthiness.
Through exploratory factor analysis, we derived an operational definition of
trust in visualizations. Our findings indicate that people perceive a
trustworthy visualization as one that presents credible information and is
comprehensible and usable. Additionally, we found that general trust
disposition influences how individuals assess visualization trustworthiness.
Building on these insights, we developed a compact inventory consisting of
statements that not only effectively represent each trust factor but also
exhibit high item discrimination. We further validated our inventory through
two trust games with real-world stakes, demonstrating that our measures
reliably predict behavioral trust. Finally, we illustrate how this standardized
inventory can be applied across diverse visualization research contexts.
Utilizing our inventory, future research can examine how design choices, tasks,
and domains influence trust, and how to foster appropriate trusting behavior in
human-data interactions.; 72) Subthreshold moment analysis of neuronal populations driven by
  synchronous synaptic inputs; Even when driven by the same stimulus, neuronal responses are well-known to
exhibit a striking level of spiking variability. In-vivo electrophysiological
recordings also reveal a surprisingly large degree of variability at the
subthreshold level. In prior work, we considered biophysically relevant
neuronal models to account for the observed magnitude of membrane voltage
fluctuations. We found that accounting for these fluctuations requires weak but
nonzero synchrony in the spiking activity, in amount that are consistent with
experimentally measured spiking correlations. Here we investigate whether such
synchrony can explain additional statistical features of the measured neural
activity, including neuronal voltage covariability and voltage skewness.
Addressing this question involves conducting a generalized moment analysis of
conductance-based neurons in response to input drives modeled as correlated
jump processes. Technically, we perform such an analysis using fixed-point
techniques from queuing theory that are applicable in the stationary regime of
activity. We found that weak but nonzero synchrony can consistently explain the
experimentally reported voltage covariance and skewness. This confirms the role
of synchrony as a primary driver of cortical variability and supports that
physiological neural activity emerges as a population-level phenomenon,
especially in the spontaneous regime.; 73) Charting and Navigating Hugging Face's Model Atlas; As there are now millions of publicly available neural networks, searching
and analyzing large model repositories becomes increasingly important.
Navigating so many models requires an atlas, but as most models are poorly
documented charting such an atlas is challenging. To explore the hidden
potential of model repositories, we chart a preliminary atlas representing the
documented fraction of Hugging Face. It provides stunning visualizations of the
model landscape and evolution. We demonstrate several applications of this
atlas including predicting model attributes (e.g., accuracy), and analyzing
trends in computer vision models. However, as the current atlas remains
incomplete, we propose a method for charting undocumented regions.
Specifically, we identify high-confidence structural priors based on dominant
real-world model training practices. Leveraging these priors, our approach
enables accurate mapping of previously undocumented areas of the atlas. We
publicly release our datasets, code, and interactive atlas.; 74) Flow-based Domain Randomization for Learning and Sequencing Robotic
  Skills; Domain randomization in reinforcement learning is an established technique
for increasing the robustness of control policies trained in simulation. By
randomizing environment properties during training, the learned policy can
become robust to uncertainties along the randomized dimensions. While the
environment distribution is typically specified by hand, in this paper we
investigate automatically discovering a sampling distribution via
entropy-regularized reward maximization of a normalizing-flow-based neural
sampling distribution. We show that this architecture is more flexible and
provides greater robustness than existing approaches that learn simpler,
parameterized sampling distributions, as demonstrated in six simulated and one
real-world robotics domain. Lastly, we explore how these learned sampling
distributions, combined with a privileged value function, can be used for
out-of-distribution detection in an uncertainty-aware multi-step manipulation
planner.; 75) Is Crime Displacement Inevitable? Evidence from Police Crackdowns in
  Fortaleza, Brazil; We evaluated one of the most common policing strategies in Brazil: the
allocation of police blitzes. This place-based focused deterrence intervention
has well-defined assignments, and 3,423 interventions were precisely recorded
in Fortaleza-CE, Brazil, between 2012 and 2013. Our analysis takes advantage of
the high spatiotemporal daily data resolution coming from an unprecedented
longitudinal micro-Big Data (GPS and PING records) to make comparisons of small
intervention areas, while controlling for common daily trends, deterrence
(spatial and temporal), and diffusion; to show that an average police crackdown
causes a 35% decrease in violent crime occurrences. There are diminishing
returns of public safety to hours spent by police in a single area,
corroborating what police officers know well from their own experience and
discretionary behavior. Although crime increases by 6% immediately after the
end of a blitz, we observe lasting deterrent effects (diffusion) after 2-3
days. The residual deterrence cancels the relocation of the crime, and the
intervention does not generate significant temporal displacement. In addition,
we do not find spatial displacement from crime in blocks up to 1.5 km from a
blitz. This type of micropolicing tactics generates deterrence by being highly
visible in a street segment for a short period and intermittently quasirandom
in space-time, which produces uncertainty that might be crucial in minimizing
the temporal and spatial displacement of crime. Of public policy interest, we
show that the allocation of blitzes passes in an initial cost-benefit analysis.; 76) PyClustrPath: An efficient Python package for generating clustering
  paths with GPU acceleration; Convex clustering is a popular clustering model without requiring the number
of clusters as prior knowledge. It can generate a clustering path by
continuously solving the model with a sequence of regularization parameter
values. This paper introduces {\it PyClustrPath}, a highly efficient Python
package for solving the convex clustering model with GPU acceleration. {\it
PyClustrPath} implements popular first-order and second-order algorithms with a
clean modular design. Such a design makes {\it PyClustrPath} more scalable to
incorporate new algorithms for solving the convex clustering model in the
future. We extensively test the numerical performance of {\it PyClustrPath} on
popular clustering datasets, demonstrating its superior performance compared to
the existing solvers for generating the clustering path based on the convex
clustering model. The implementation of {\it PyClustrPath} can be found at:
https://github.com/D3IntOpt/PyClustrPath.; 77) The tardigrade as an emerging model organism for systems neuroscience; We present the case for developing the tardigrade (Hypsibius exemplaris) into
a model organism for systems neuroscience. These microscopic, transparent
animals (~300-500 microns) are among the smallest known to possess both limbs
(eight) and eyes (two), with a nervous system of only a few hundred neurons
organized into a multi-lobed brain, ventral nerve cord, and a series of ganglia
along the body. Despite their neuroanatomical simplicity, tardigrades exhibit
complex behaviors, including multi-limbed walking gaits, individual limb
grasping, phototaxis, and transitions between active and dormant states. These
behaviors position tardigrades as a uniquely powerful system for addressing
certain fundamental questions in systems neuroscience, such as: How do nervous
systems coordinate multi-limbed behaviors? How are top-down and bottom-up motor
control systems integrated? How is stereovision-guided navigation implemented?
What mechanisms underlie neural resilience and recovery during environmental
stress? We review current knowledge of tardigrade neuroanatomy, behavior, and
genomics, and we identify opportunities and challenges for leveraging their
unique biology. We propose developing essential neuroscientific tools for
tardigrades, including genetic engineering and live neuroimaging, alongside
behavioral assays linking neural activity to outputs. Leveraging their
evolutionary ties to Caenorhabditis elegans and Drosophila melanogaster, we can
adapt existing toolkits to accelerate tardigrade research - providing a bridge
between simpler invertebrate systems and more complex neural architectures.; 78) Dissociated Neuronal Cultures as Model Systems for Self-Organized
  Prediction; Dissociated neuronal cultures provide a simplified yet effective model system
for investigating self-organized prediction and information processing in
neural networks. This review consolidates current research demonstrating that
these in vitro networks display fundamental computational capabilities,
including predictive coding, adaptive learning, goal-directed behavior, and
deviance detection. We examine how these cultures develop critical dynamics
optimized for information processing, detail the mechanisms underlying learning
and memory formation, and explore the relevance of the free energy principle
within these systems. Building on these insights, we discuss how findings from
dissociated neuronal cultures inform the design of neuromorphic and reservoir
computing architectures, with the potential to enhance energy efficiency and
adaptive functionality in artificial intelligence. The reduced complexity of
neuronal cultures allows for precise manipulation and systematic investigation,
bridging theoretical frameworks with practical implementations in bio-inspired
computing. Finally, we highlight promising future directions, emphasizing
advancements in three-dimensional culture techniques, multi-compartment models,
and brain organoids that deepen our understanding of hierarchical and
predictive processes in both biological and artificial systems. This review
aims to provide a comprehensive overview of how dissociated neuronal cultures
contribute to neuroscience and artificial intelligence, ultimately paving the
way for biologically inspired computing solutions.; 79) Structuring the Environment Nudges Participants Toward Hierarchical Over
  Shortest Path Planning; Effective planning is crucial for navigating complex environments and
achieving goals efficiently. In this study, we investigated how environmental
structure influences the selection of planning strategies. Participants
navigated a space station to collect colored spheres, with environments either
structured (spheres grouped by color) or unstructured (spheres scattered
randomly). We tested three types of plans: hierarchical (grouping spheres by
color), shortest path (minimizing travel distance), and neutral (none of the
above). By manipulating environmental structure, we were able to nudge
participants toward a preference for hierarchical planning in structured
environments, while shortest path plans were favored in unstructured
environments. A mismatch between self-reported preferences and actual choices
indicated that participants often adopted implicit strategies, unaware of their
decision-making processes. These findings highlight the powerful effect of
environmental cues on planning and suggest that even subtle changes in
structure can guide the selection of planning strategies.; 80) Euclid preparation LX. The use of HST images as input for weak-lensing
  image simulations; Data from the Euclid space telescope will enable cosmic shear measurements
with very small statistical errors, requiring corresponding systematic error
control level. A common approach to correct for shear biases involves
calibrating shape measurement methods using image simulations with known input
shear. Given their high resolution, Hubble Space Telescope (HST) galaxies can,
in principle, be utilised to emulate Euclid observations. In this work, we
employ a GalSim-based testing environment to investigate whether uncertainties
in the HST point spread function (PSF) model or in data processing techniques
introduce significant biases in weak-lensing (WL) shear calibration. We used
single S\'ersic galaxy models to simulate both HST and Euclid observations. We
then `Euclidised' our HST simulations and compared the results with the
directly simulated Euclid-like images. For this comparison, we utilised a
moment-based shape measurement algorithm and galaxy model fits. Through the
Euclidisation procedure, we effectively reduced the residual multiplicative
biases in shear measurements to sub-percent levels. This achievement was made
possible by employing either the native pixel scales of the instruments,
utilising the Lanczos15 interpolation kernel, correcting for noise
correlations, and ensuring consistent galaxy signal-to-noise ratios between
simulation branches. However, the Euclidisation procedure requires further
analysis on the impact of the correlated noise, to estimate calibration bias.
Additionally, we conducted an in-depth analysis of the accuracy of TinyTim HST
PSF models using star fields observed in the F606W and F814W filters. We
observe that F606W images exhibit a broader scatter in the recovered best-fit
focus, compared to those in the F814W filter.; 81) DIDiffGes: Decoupled Semi-Implicit Diffusion Models for Real-time
  Gesture Generation from Speech; Diffusion models have demonstrated remarkable synthesis quality and diversity
in generating co-speech gestures. However, the computationally intensive
sampling steps associated with diffusion models hinder their practicality in
real-world applications. Hence, we present DIDiffGes, for a Decoupled
Semi-Implicit Diffusion model-based framework, that can synthesize
high-quality, expressive gestures from speech using only a few sampling steps.
Our approach leverages Generative Adversarial Networks (GANs) to enable
large-step sampling for diffusion model. We decouple gesture data into body and
hands distributions and further decompose them into marginal and conditional
distributions. GANs model the marginal distribution implicitly, while L2
reconstruction loss learns the conditional distributions exciplictly. This
strategy enhances GAN training stability and ensures expressiveness of
generated full-body gestures. Our framework also learns to denoise root noise
conditioned on local body representation, guaranteeing stability and realism.
DIDiffGes can generate gestures from speech with just 10 sampling steps,
without compromising quality and expressiveness, reducing the number of
sampling steps by a factor of 100 compared to existing methods. Our user study
reveals that our method outperforms state-of-the-art approaches in human
likeness, appropriateness, and style correctness. Project is
https://cyk990422.github.io/DIDiffGes.; 82) Pediatric low-grade glioma: State-of-the-art and ongoing challenges; Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%–40% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence.; 83) Long-term follow-up of DYT1 dystonia patients treated by deep brain
  stimulation: an open-label study; Long-term efficacy of internal globus pallidus (GPi) deep-brain stimulation
(DBS) in DYT1 dystonia and disease progression under DBS was studied.
Twenty-six patients of this open-label study were divided into two groups: (A)
with single bilateral GPi lead, (B) with a second bilateral GPi lead implanted
owning to subsequent worsening of symptomatology. Dystonia was assessed with
the Burke Scale. Appearance of new symptoms and distribution according to body
region were recorded. In the whole cohort, significant decreases in motor and
disability subscores (P < 0.0001) were observed at 1 year and maintained up to
10 years. Group B showed worsening of the symptoms. At 1 year, there were no
significant differences between Groups A (without subsequent worsening) and B;
at 5 years, a significant difference was found for motor and disability scores.
Within Group B, four patients exhibited additional improvement after the second
DBS surgery. In the 26 patients, significant difference (P = 0.001) was found
between the number of body regions affected by dystonia preoperatively and over
the whole follow-up. DBS efficacy in DYT1 dystonia can be maintained up to 10
years (two patients). New symptoms appear with long-term follow-up and may
improve with additional leads in a subgroup of patients.; 84) Inverse receptive field attention for naturalistic image reconstruction
  from the brain; Visual perception in the brain largely depends on the organization of
neuronal receptive fields. Although extensive research has delineated the
coding principles of receptive fields, most studies have been constrained by
their foundational assumptions. Moreover, while machine learning has
successfully been used to reconstruct images from brain data, this approach
faces significant challenges, including inherent feature biases in the model
and the complexities of brain structure and function. In this study, we
introduce an inverse receptive field attention (IRFA) model, designed to
reconstruct naturalistic images from neurophysiological data in an end-to-end
fashion. This approach aims to elucidate the tuning properties and
representational transformations within the visual cortex. The IRFA model
incorporates an attention mechanism that determines the inverse receptive field
for each pixel, weighting neuronal responses across the visual field and
feature spaces. This method allows for an examination of the dynamics of
neuronal representations across stimuli in both spatial and feature dimensions.
Our results show highly accurate reconstructions of naturalistic data,
independent of pre-trained models. Notably, IRF models trained on macaque V1,
V4, and IT regions yield remarkably consistent spatial receptive fields across
different stimuli, while the features to which neuronal representations are
selective exhibit significant variation. Additionally, we propose a data-driven
method to explore representational clustering within various visual areas,
further providing testable hypotheses.; 85) Local minimizers in $3$d of vector Allen-Cahn with a quadruple junction; For $\Omega$ a perturbation of the unit ball in $\mathbb{R}^3$, we establish
the existence of a sequence of local minimizers for the vector Allen-Cahn
energy. The sequence converges in $L^1$ to a partition of $\Omega$ whose
skeleton is given by a tetrahedral cone and thus contains a quadruple point.
This is accomplished by proving that the partition is an isolated local
minimizer of a weighted perimeter problem arising as the associated
$\Gamma$-limit of the sequence of Allen-Cahn functionals.; 86) Development and Comparison of Model-Based and Data-Driven Approaches for
  the Prediction of the Mechanical Properties of Lattice Structures; Lattice structures have great potential for several application fields
ranging from medical and tissue engineering to aeronautical one. Their
development is further speeded up by the continuing advances in additive
manufacturing technologies that allow to overcome issues typical of standard
processes and to propose tailored designs. However, the design of lattice
structures is still challenging since their properties are considerably
affected by numerous factors. The present paper aims to propose, discuss, and
compare various modeling approaches to describe, understand, and predict the
correlations between the mechanical properties and the void volume fraction of
different types of lattice structures fabricated by fused deposition modeling
3D printing. Particularly, four approaches are proposed: (i) a simplified
analytical model; (ii) a semi-empirical model combining analytical equations
with experimental correction factors; (iii) an artificial neural network
trained on experimental data; (iv) numerical simulations by finite element
analyses. The comparison among the various approaches, and with experimental
data, allows to identify the performances, advantages, and disadvantages of
each approach, thus giving important guidelines for choosing the right design
methodology based on the needs and available data.; 87) Chiral currents at zero magnetic field in some two-dimensional
  superconductors; Non-reciprocal critical currents without applying an external magnetic field
have been observed recently in several superconductors, in various forms of
Graphene, a Kagome compound and in an under-doped cuprate. A necessary
requirement for this is that the usual supercurrent be accompanied by a chiral
super-current, i.e. with the symmetry of a hall current; equivalently that the
superfluid density tensor have a chiral component. It also requires inversion
breaking. The conditions for this phenomena are derived to find that their
normal states must break time-reversal and chirality and that the
superconducting states must in addition be non-unitary. Each of the
superconductors where spontaneous non-reciprocal critical currents are observed
have shown some evidence for such broken symmetries in the normal state. The
superconducting state of such materials have topological edge currents, but
their projected electro-magnetic part is in general not an integer. The edge
states are protected in the superconductor due to a gap. Under ideal
conditions, the normal state should show an anomalous Hall effect.; 88) Generalized Oxtoby subshifts and hyperfiniteness; We show that there exists a class of symbolic subshifts which realizes all
Choquet simplices as simplices of invariant measures and the conjugacy relation
on that class is hyperfinite.; 89) Temporal Relation Extraction in Clinical Texts: A Span-based Graph
  Transformer Approach; Temporal information extraction from unstructured text is essential for
contextualizing events and deriving actionable insights, particularly in the
medical domain. We address the task of extracting clinical events and their
temporal relations using the well-studied I2B2 2012 Temporal Relations
Challenge corpus. This task is inherently challenging due to complex clinical
language, long documents, and sparse annotations. We introduce GRAPHTREX, a
novel method integrating span-based entity-relation extraction, clinical large
pre-trained language models (LPLMs), and Heterogeneous Graph Transformers (HGT)
to capture local and global dependencies. Our HGT component facilitates
information propagation across the document through innovative global landmarks
that bridge distant entities. Our method improves the state-of-the-art with
5.5% improvement in the tempeval $F_1$ score over the previous best and up to
8.9% improvement on long-range relations, which presents a formidable
challenge. This work not only advances temporal information extraction but also
lays the groundwork for improved diagnostic and prognostic models through
enhanced temporal reasoning.; 90) Arcs, Caps and Generalisations in a Finite Projective Space; Arcs and caps are fundamental structures in finite projective spaces. They
can be generalised. Here, a survey is given of some important results on these
objects, in particular on generalised ovals and generalised ovoids. The paper
also contains recent results and several open problems.; 91) Evolution of diverse (and advanced) cognitive abilities through adaptive
  fine-tuning of learning and chunking mechanisms; The evolution of cognition is frequently discussed as the evolution of
cognitive abilities or the evolution of some neuronal structures in the brain.
However, since such traits or abilities are often highly complex, understanding
their evolution requires explaining how they could have gradually evolved
through selection acting on heritable variations in simpler cognitive
mechanisms. With this in mind, making use of a previously proposed theory, here
we show how the evolution of cognitive abilities can be captured by the
fine-tuning of basic learning mechanisms and, in particular, chunking
mechanisms. We use the term chunking broadly for all types of non-elemental
learning, claiming that the process by which elements are combined into chunks
and associated with other chunks, or elements, is critical for what the brain
can do, and that it must be fine-tuned to ecological conditions. We discuss the
relevance of this approach to studies in animal cognition, using examples from
animal foraging and decision-making, problem solving, and cognitive
flexibility. Finally, we explain how even the apparent human-animal gap in
sequence learning ability can be explained in terms of different fine-tunings
of a similar chunking process.; 92) Adversarial Reasoning at Jailbreaking Time; As large language models (LLMs) are becoming more capable and widespread, the
study of their failure cases is becoming increasingly important. Recent
advances in standardizing, measuring, and scaling test-time compute suggest new
methodologies for optimizing models to achieve high performance on hard tasks.
In this paper, we apply these advances to the task of model jailbreaking:
eliciting harmful responses from aligned LLMs. We develop an adversarial
reasoning approach to automatic jailbreaking via test-time computation that
achieves SOTA attack success rates (ASR) against many aligned LLMs, even the
ones that aim to trade inference-time compute for adversarial robustness. Our
approach introduces a new paradigm in understanding LLM vulnerabilities, laying
the foundation for the development of more robust and trustworthy AI systems.; 93) On the Emergence of the Quanta Prime Sequence; This paper presents the Quanta Prime Sequence (QPS) and its foundational
theorem, showcasing a unique class of polynomials with substantial
implications. The study uncovers profound connections between Quanta Prime
numbers and essential sequences in number theory and cryptography. The
investigation highlights the sequence's contribution to the emergence of new
primes and its embodiment of core mathematical constructs, including Mersenne
numbers, Fermat numbers, Lucas numbers, Fibonacci numbers, the Chebyshev
sequence, and the Dickson sequence. The comprehensive analysis emphasizes the
sequence's intrinsic relevance to the Lucas-Lehmer primality test. This
research positions the Quanta Prime sequence as a pivotal tool in cryptographic
applications, offering novel representations of critical mathematical
structures. Additionally, a new result linking the Quanta Prime sequence to the
Harmonic series is introduced, hinting at potential progress in understanding
the Riemann Hypothesis.; 94) Predictability of temporal network dynamics in normal ageing and brain
  pathology; Spontaneous brain activity generically displays transient spatiotemporal
coherent structures, which can selectively be affected in various neurological
and psychiatric pathologies. Here we model the full brain's
electroencephalographic activity as a high-dimensional functional network
performing a trajectory in a latent graph phase space. This approach allows us
to investigate the orbital stability of brain's activity and in particular its
short-term predictability. We do this by constructing a non-parametric
statistic quantifying the expansion of initially close functional network
trajectories. We apply the method to cohorts of healthy ageing individuals, and
patients previously diagnosed with Parkinson's or Alzheimer's disease. Results
not only characterise brain dynamics from a new angle, but further show that
functional network predictability varies in a marked scale-dependent way across
healthy controls and patient groups. The path towards both pathologies is
markedly different. Furthermore, healthy ageing's predictability appears to
strongly differ from that of Parkinson's disease, but much less from that of
patients with Alzheimer's disease.; 95) Training Domain Draft Models for Speculative Decoding: Best Practices
  and Insights; Speculative decoding is an effective method for accelerating inference of
large language models (LLMs) by employing a small draft model to predict the
output of a target model. However, when adapting speculative decoding to
domain-specific target models, the acceptance rate of the generic draft model
drops significantly due to domain shift. In this work, we systematically
investigate knowledge distillation techniques for training domain draft models
to improve their speculation accuracy. We compare white-box and black-box
distillation approaches and explore their effectiveness in various data
accessibility scenarios, including historical user queries, curated domain
data, and synthetically generated alignment data. Our experiments across
Function Calling, Biology, and Chinese domains show that offline distillation
consistently outperforms online distillation by 11% to 25%, white-box
distillation surpasses black-box distillation by 2% to 10%, and data scaling
trends hold across domains. Additionally, we find that synthetic data can
effectively align draft models and achieve 80% to 93% of the performance of
training on historical user queries. These findings provide practical
guidelines for training domain-specific draft models to improve speculative
decoding efficiency.; 96) On Questions of Predictability and Control of an Intelligent System
  Using Probabilistic State-Transitions; One of the central aims of neuroscience is to reliably predict the behavioral
response of an organism using its neural activity. If possible, this implies we
can causally manipulate the neural response and design brain-computer-interface
systems to alter behavior, and vice-versa. Hence, predictions play an important
role in both fundamental neuroscience and its applications. Can we predict the
neural and behavioral states of an organism at any given time? Can we predict
behavioral states using neural states, and vice-versa, and is there a
memory-component required to reliably predict such states? Are the predictions
computable within a given timescale to meaningfully stimulate and make the
system reach the desired states? Through a series of mathematical treatments,
such conjectures and questions are discussed. Answering them might be key for
future developments in understanding intelligence and designing
brain-computer-interfaces.; 97) Combined impact of grey and superficial white matter abnormalities:
  implications for epilepsy surgery; Drug-resistant focal epilepsy is associated with abnormalities in the brain
in both grey matter (GM) and superficial white matter (SWM). However, it is
unknown if both types of abnormalities are important in supporting seizures.
Here, we test if surgical removal of GM and/or SWM abnormalities relates to
post-surgical seizure outcome in people with temporal lobe epilepsy (TLE).
  We analyzed structural imaging data from 143 TLE patients (pre-op dMRI and
pre-op T1-weighted MRI) and 97 healthy controls. We calculated GM volume
abnormalities and SWM mean diffusivity abnormalities and evaluated if their
surgical removal distinguished seizure outcome groups post-surgically.
  At a group level, GM and SWM abnormalities were most common in the
ipsilateral temporal lobe and hippocampus in people with TLE. Analyzing both
modalities together, compared to in isolation, improved surgical outcome
discrimination (GM AUC = 0.68, p < 0.01, WM AUC = 0.65, p < 0.01; Union AUC =
0.72, p < 0.01, Concordance AUC = 0.64, p = 0.04). Additionally, 100% of people
who had all concordant abnormal regions resected had ILAE$_{1,2}$ outcomes.
  These findings suggest that regions identified as abnormal from both
diffusion-weighted and T1-weighted MRIs are involved in the epileptogenic
network and that resection of both types of abnormalities may enhance the
chances of living without disabling seizures.; 98) KMT2B-related disorders: expansion of the phenotypic spectrum and
  long-term efficacy of deep brain stimulation; Heterozygous mutations in KMT2B are associated with an early-onset,
progressive, and often complex dystonia (DYT28). Key characteristics of typical
disease include focal motor features at disease presentation, evolving through
a caudocranial pattern into generalized dystonia, with prominent oromandibular,
laryngeal, and cervical involvement. Although KMT2B-related disease is emerging
as one of the most common causes of early-onset genetic dystonia, much remains
to be understood about the full spectrum of the disease. We describe a cohort
of 53 patients with KMT2B mutations, with detailed delineation of their
clinical phenotype and molecular genetic features. We report new disease
presentations, including atypical patterns of dystonia evolution and a subgroup
of patients with a non-dystonic neurodevelopmental phenotype. In addition to
the previously reported systemic features, our study has identified
co-morbidities, including the risk of status dystonicus, intrauterine growth
retardation, and endocrinopathies. Analysis of this study cohort (n = 53) in
tandem with published cases (n = 80) revealed that patients with chromosomal
deletions and protein-truncating variants had a significantly higher burden of
systemic disease (with earlier onset of dystonia) than those with missense
variants. Eighteen individuals had detailed longitudinal data available after
insertion of deep brain stimulation for medically refractory dystonia. Median
age at deep brain stimulation was 11.5 years (range: 4.5 to 37.0 years).
Follow-up after deep brain stimulation ranged from 0.25 to 22 years.
Significant improvement of motor function and disability (as assessed by the
Burke-Fahn-Marsden Dystonia Rating Scales, BFMDRS-M and BFMDRS-D) was evident
at 6 months, 1 year, and last follow-up (motor, P = 0.001, P = 0.004, and P =
0.012; disability, P = 0.009, P = 0.002, and P = 0.012).; 99) Integrated 6G TN and NTN Localization: Challenges, Opportunities, and
  Advancements; The rapid evolution of cellular networks has introduced groundbreaking
technologies, including large and distributed antenna arrays and reconfigurable
intelligent surfaces in terrestrial networks (TNs), as well as aerial and
space-based nodes in non-terrestrial networks (NTNs). These advancements enable
applications beyond traditional communication, such as high-precision
localization and sensing. While integrating TN and NTN enablers will lead to
unparalleled opportunities for seamless global localization, such integration
attempts are expected to face several challenges. To understand these
opportunities and challenges, we first examine the distinctive characteristics
of the key 6G enablers, evaluating their roles in localization from both
technical and practical perspectives. Next, to identify developments driving
TN-NTN localization, we review the latest standardization and industrial
innovation progress. Finally, we discuss the opportunities and challenges of
TN-NTN integration, illustrating its potential through two numerical case
studies.; 100) From Thought to Action: How a Hierarchy of Neural Dynamics Supports
  Language Production; Humans effortlessly communicate their thoughts through intricate sequences of
motor actions. Yet, the neural processes that coordinate language production
remain largely unknown, in part because speech artifacts limit the use of
neuroimaging. To elucidate the unfolding of language production in the brain,
we investigate with magnetoencephalography (MEG) and electroencephalography
(EEG) the neurophysiological activity of 35 skilled typists, while they typed
sentences on a keyboard. This approach confirms the hierarchical predictions of
linguistic theories: the neural activity preceding the production of each word
is marked by the sequential rise and fall of context-, word-, syllable-, and
letter-level representations. Remarkably, each of these neural representations
is maintained over long time periods within each level of the language
hierarchy. This phenomenon results in a superposition of successive
representations that is supported by a hierarchy of dynamic neural codes.
Overall, these findings provide a precise computational breakdown of the neural
dynamics that coordinate the production of language in the human brain.",1.0,0.0
2411.00609,applied,2411.00609-pos2-4,"Pediatric low-grade glioma: State-of-the-art and ongoing challenges; Abstract The most common childhood central nervous system (CNS) tumor is pediatric low-grade glioma (pLGG), representing 30%–40% of all CNS tumors in children. Although there high associated morbidity, tumor-related mortality relatively rare. pLGG now conceptualized as a chronic disease, underscoring the importance functional outcomes and quality-of-life measures. A wealth data has emerged about these tumors, including better understanding their natural history molecular drivers, paving way for use targeted inhibitors. While treatments have heralded tremendous promise, challenges remain how to best optimize use, long-term toxicities with inhibitors unknown. International Pediatric Low-Grade Glioma Coalition (iPLGGc) global group physicians scientists expertise focused on addressing key issues. Here, iPLGGc provides an overview current state-of-the-art pLGG, epidemiology, histology, landscape, treatment paradigms, survival outcomes, imaging response, ongoing challenges. This paper also serves introduction 3 other manuscripts (1) preclinical models, (2) consensus framework conducting early-phase clinical trials (3) resistance, rebound, recurrence.",2411.00609-pos1-4,"Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype
  Identification Using a Novel AUROC Loss Function for Convolutional Neural
  Networks; Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016).",66,"['1', '5', '66', '27', '36', '85', '32', '94', '66', '77']","The first candidate paper, 'Nexus Machine: An Active Message Inspired Reconfigurable Architecture for Irregular Workloads', aligns best with the main paper on pediatric low-grade glioma (pLGG) as it introduces innovative computational architecture that could enhance the analysis of medical imaging data and improve diagnostic tools for pLGG. The focus on energy efficiency and performance optimization in handling irregular workloads can directly translate to better processing of complex data such as MRI scans used in pLGG diagnosis and treatment planning. This multidisciplinary approach offers a novel solution to existing challenges in pLGG management, making it both useful and imaginative. The subsequent papers also provide various interdisciplinary insights but are less directly applicable to the core issues addressed in the main paper.","1) Nexus Machine: An Active Message Inspired Reconfigurable Architecture
  for Irregular Workloads; Modern reconfigurable architectures are increasingly favored for
resource-constrained edge devices as they balance high performance, energy
efficiency, and programmability well. However, their proficiency in handling
regular compute patterns constrains their effectiveness in executing irregular
workloads, such as sparse linear algebra and graph analytics with unpredictable
access patterns and control flow. To address this limitation, we introduce the
Nexus Machine, a novel reconfigurable architecture consisting of a PE array
designed to efficiently handle irregularity by distributing sparse tensors
across the fabric and employing active messages that morph instructions based
on dynamic control flow. As the inherent irregularity in workloads can lead to
high load imbalance among different Processing Elements (PEs), Nexus Machine
deploys and executes instructions en-route on idle PEs at run-time. Thus,
unlike traditional reconfigurable architectures with only static instructions
within each PE, Nexus Machine brings dynamic control to the idle compute units,
mitigating load imbalance and enhancing overall performance. Our experiments
demonstrate that Nexus Machine achieves 1.5x performance gain compared to
state-of-the-art (SOTA) reconfigurable architectures, within the same power
budget and area. Nexus Machine also achieves 1.6x higher fabric utilization, in
contrast to SOTA architectures.; 2) Targeting C99 Mediated Metabolic Disruptions with Ketone Therapy in
  Alzheimer's Disease; The role of ketone bodies in Alzheimers disease (AD) remains incompletely
understood, particularly regarding their influence on amyloid pathology. While
beta}hydroxybutyrate (BHB) has been implicated in neuroprotection, direct
evidence for its effects on amyloid beta(Abeta) deposition, aggregation, or
clearance is lacking. Furthermore, whether BHB acts as a disease modifying
factor or merely confers transient metabolic benefits remains unclear.
Addressing this gap is crucial for evaluating the therapeutic potential of
ketone metabolism in AD. Here, we investigated the impact of ketone bodies on
amyloidogenic toxicity using a Drosophila melanogaster model with targeted
expression of human amyloid precursor protein (APP), beta secretase 1 (BACE1),
Abeta, and the C99 fragment, an essential intermediate in Abeta generation.
Surprisingly, we found that Abeta alone elicited minimal neurotoxicity, whereas
C99 expression induced pronounced pathological effects, suggesting a critical,
underappreciated role of C99 in AD progression. Further analysis revealed that
C99 driven toxicity was associated with autophagic and lysosomal dysfunction,
leading to impaired protein clearance, oxidative stress, and mitochondrial
abnormalities. Using confocal microscopy and lysosomal pH sensitive markers, we
demonstrated that BHB treatment restored lysosomal function and alleviated
these pathological changes. Protein protein interaction network analysis in C99
expressing Drosophila brains identified protein phosphatase methylesterase 1
(PPME1) activation as a key driver of autophagic impairment, further supported
by machine learning predictions. Finally, mathematical similarity analysis of
PPI networks suggested that BHB may exert its neuroprotective effects through
mTOR inhibition, positioning it as a potential endogenous modulator of AD
related pathology.; 3) Gleaning gravitational amplitudes -- a double copy for canceling
  dilatons; Scattering amplitudes in general relativity can be conveniently computed
using the double copy, which relates them to Yang-Mills amplitudes. However,
unwanted dilatons are sourced by massive scalar matter, which must be removed
from the double copy in order to match the long range gravitational
interactions. In this paper, we study how to automatically cancel out the
dilatons by finding a suitable double-copy prescription in terms of
gauge-theory fields, effectively treating the new contributions as ghosts that
subtract out the unwanted states. At tree level, we find that an asymmetric
double copy can reproduce the dilaton graphs in general dimension, which we
explicitly verify up to six external massive scalars. Considering a one-loop
four-point example, the same asymmetric double copy needs to be supplemented by
the subtraction of bubble graphs that originate both from the axion and a
residual dilaton term.; 4) Spaces and sequences in the hippocampus: a homological perspective; Topological techniques have become a popular tool for studying information
flows in neural networks. In particular, simplicial homology theory is used to
analyze how cognitive representations of space emerge from large conglomerates
of independent neuronal contributions. Meanwhile, a growing number of studies
suggest that many cognitive functions are sustained by serial patterns of
activity. Here, we investigate stashes of such patterns using path homology
theory -- an impartial, universal approach that does not require a priori
assumptions about the sequences' nature, functionality, underlying mechanisms,
or other contexts. We focus on the hippocampus -- a key enabler of learning and
memory in mammalian brains -- and quantify the ordinal arrangement of its
activity similarly to how its topology has previously been studied in terms of
simplicial homologies. The results reveal that the vast majority of sequences
produced during spatial navigation are structurally equivalent to one another.
Only a few classes of distinct sequences form an ordinal schema of serial
activity that remains stable as the pool of sequences consolidates.
Importantly, the structure of both maps is upheld by combinations of short
sequences, suggesting that brief activity motifs dominate physiological
computations. This ordinal organization emerges and stabilizes on timescales
characteristic of spatial learning, displaying similar dynamics. Yet, the
ordinal maps generally do not reflect topological affinities -- spatial and
sequential analyses address qualitatively different aspects of spike flows,
representing two complementary formats of information processing.; 5) Multi-stage intermediate fusion for multimodal learning to classify
  non-small cell lung cancer subtypes from CT and PET; Accurate classification of histological subtypes of non-small cell lung
cancer (NSCLC) is essential in the era of precision medicine, yet current
invasive techniques are not always feasible and may lead to clinical
complications. This study presents a multi-stage intermediate fusion approach
to classify NSCLC subtypes from CT and PET images. Our method integrates the
two modalities at different stages of feature extraction, using voxel-wise
fusion to exploit complementary information across varying abstraction levels
while preserving spatial correlations. We compare our method against unimodal
approaches using only CT or PET images to demonstrate the benefits of modality
fusion, and further benchmark it against early and late fusion techniques to
highlight the advantages of intermediate fusion during feature extraction.
Additionally, we compare our model with the only existing intermediate fusion
method for histological subtype classification using PET/CT images. Our results
demonstrate that the proposed method outperforms all alternatives across key
metrics, with an accuracy and AUC equal to 0.724 and 0.681, respectively. This
non-invasive approach has the potential to significantly improve diagnostic
accuracy, facilitate more informed treatment decisions, and advance
personalized care in lung cancer management.; 6) Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs; Large language models (LLMs) exhibit hallucinations (i.e., unfaithful or
nonsensical information) when serving as AI assistants in various domains.
Since hallucinations always come with truthful content in the LLM responses,
previous factuality alignment methods that conduct response-level preference
learning inevitably introduced noises during training. Therefore, this paper
proposes a fine-grained factuality alignment method based on Direct Preference
Optimization (DPO), called Mask-DPO. Incorporating sentence-level factuality as
mask signals, Mask-DPO only learns from factually correct sentences in the
preferred samples and prevents the penalty on factual contents in the not
preferred samples, which resolves the ambiguity in the preference learning.
Extensive experimental results demonstrate that Mask-DPO can significantly
improve the factuality of LLMs responses to questions from both in-domain and
out-of-domain datasets, although these questions and their corresponding topics
are unseen during training. Only trained on the ANAH train set, the score of
Llama3.1-8B-Instruct on the ANAH test set is improved from 49.19% to 77.53%,
even surpassing the score of Llama3.1-70B-Instruct (53.44%), while its
FactScore on the out-of-domain Biography dataset is also improved from 30.29%
to 39.39%. We further study the generalization property of Mask-DPO using
different training sample scaling strategies and find that scaling the number
of topics in the dataset is more effective than the number of questions. We
provide a hypothesis of what factual alignment is doing with LLMs, on the
implication of this phenomenon, and conduct proof-of-concept experiments to
verify it. We hope the method and the findings pave the way for future research
on scaling factuality alignment.; 7) Investigating the effects of QCD matter's electrical conductivity on
  charge dependent directed flow; Charge dependent directed flow is an important observable of electromagnetic
fields in relativistic heavy-ion collisions. We demonstrate how the difference
in charge dependent directed flows between protons and antiprotons is sensitive
to the resistivity, inverse of quark-gluon plasma's electric conductivity, over
different collision centralities. Our model numerically solves the 3+1D
relativistic resistive magneto-hydrodynamic (RRMHD) equations, assuming the
electric conductivity to be a scalar. For this work, we focus on symmetric Au +
Au collisions at the top RHIC energy of $\sqrt{s}=200$ GeV. We illustrate the
time evolution of the electromagnetic fields in our model and connect that to
the charge dependent directed flow results. Our results highlight the
importance of modeling quark-gluon plasma's electric conductivity for charge
dependent observables in relativistic heavy-ion collisions.; 8) Role of connectivity anisotropies in the dynamics of cultured neuronal
  networks; Laboratory-grown, engineered living neuronal networks in vitro have emerged
in the last years as an experimental technique to understand the collective
behavior of neuronal assemblies in relation to their underlying connectivity.
An inherent obstacle in the design of such engineered systems is the difficulty
to predict the dynamic repertoire of the emerging network and its dependence on
experimental variables. To fill this gap, and inspired on recent experimental
studies, here we present a numerical model that aims at, first, replicating the
anisotropies in connectivity imprinted through engineering, to next realize the
collective behavior of the neuronal network and make predictions. We use
experimentally measured, biologically-realistic data combined with the
Izhikevich model to quantify the dynamics of the neuronal network in relation
to tunable structural and dynamical parameters. These parameters include the
synaptic noise, strength of the imprinted anisotropies, and average axon
lengths. The latter are involved in the simulation of the development of
neurons in vitro. We show that the model captures the behavior of engineered
neuronal cultures, in which a rich repertoire of activity patterns emerge but
whose details are strongly dependent on connectivity details and noise. Results
also show that the presence of connectivity anisotropies substantially improves
the capacity of reconstructing structural connectivity from activity data, an
aspect that is important in the quest for understanding the
structure-to-function relationship in neuronal networks. Our work provides the
in silico basis to assist experimentalists in the design of laboratory in vitro
networks and anticipate their outcome, an aspect that is particularly important
in the effort to conceive reliable brain-on-a-chip circuits and explore key
aspects such as input-output relationships or information coding.; 9) The Role of Affective States in Computational Psychiatry; Studying psychiatric illness has often been limited by difficulties in
connecting symptoms and behavior to neurobiology. Computational psychiatry
approaches promise to bridge this gap by providing formal accounts of the
latent information processing changes that underlie the development and
maintenance of psychiatric phenomena. Models based on these theories generate
individual-level parameter estimates which can then be tested for relationships
to neurobiology. In this review, we explore computational modelling approaches
to one key aspect of health and illness: affect. We discuss strengths and
limitations of key approaches to modelling affect, with a focus on
reinforcement learning, active inference, the hierarchical gaussian filter, and
drift-diffusion models. We find that, in this literature, affect is an
important source of modulation in decision making, and has a bidirectional
influence on how individuals infer both internal and external states.
Highlighting the potential role of affect in information processing changes
underlying symptom development, we extend an existing model of psychosis, where
affective changes are influenced by increasing cortical noise and consequent
increases in either perceived environmental instability or expected noise in
sensory input, becoming part of a self-reinforcing process generating
negatively valenced, over-weighted priors underlying positive symptom
development. We then provide testable predictions from this model at
computational, neurobiological, and phenomenological levels of description.; 10) YINYANG-ALIGN: Benchmarking Contradictory Objectives and Proposing
  Multi-Objective Optimization based DPO for Text-to-Image Alignment; Precise alignment in Text-to-Image (T2I) systems is crucial to ensure that
generated visuals not only accurately encapsulate user intents but also conform
to stringent ethical and aesthetic benchmarks. Incidents like the Google Gemini
fiasco, where misaligned outputs triggered significant public backlash,
underscore the critical need for robust alignment mechanisms. In contrast,
Large Language Models (LLMs) have achieved notable success in alignment.
Building on these advancements, researchers are eager to apply similar
alignment techniques, such as Direct Preference Optimization (DPO), to T2I
systems to enhance image generation fidelity and reliability.
  We present YinYangAlign, an advanced benchmarking framework that
systematically quantifies the alignment fidelity of T2I systems, addressing six
fundamental and inherently contradictory design objectives. Each pair
represents fundamental tensions in image generation, such as balancing
adherence to user prompts with creative modifications or maintaining diversity
alongside visual coherence. YinYangAlign includes detailed axiom datasets
featuring human prompts, aligned (chosen) responses, misaligned (rejected)
AI-generated outputs, and explanations of the underlying contradictions.; 11) Momentum coupling of classical atoms; A new method, dual-space cluster expansion, is proposed to study classical
phases transitions in the continuum. It relies on replacing the particle
positions as integration variables by the momenta of the relative displacements
of particle pairs. Due to the requirement that the particles must be static,
coupling via the momenta partitions the set of particles into a set of
clusters, and transforms the partition function into a sum over the different
cluster decompositions. This allows us to derive a formula for the density that
finite clusters can carry in the infinite system. In a simplified example, we
then demonstrate that in two and higher dimensions this density has a
threshold, beyond which the particles form infinite clusters. The transition is
accompanied by a singularity in the free energy. We also show that infinite
clusters are always present in condensed phases, most likely submacroscopic in
liquids and macroscopic in crystals.; 12) Efficient Bearing Sensor Data Compression via an Asymmetrical
  Autoencoder with a Lifting Wavelet Transform Layer; Bearing data compression is vital to manage the large volumes of data
generated during condition monitoring. In this paper, a novel asymmetrical
autoencoder with a lifting wavelet transform (LWT) layer is developed to
compress bearing sensor data. The encoder part of the network consists of a
convolutional layer followed by a wavelet filterbank layer. Specifically, a
dual-channel convolutional block with diverse convolutional kernel sizes and
varying processing depths is integrated into the wavelet filterbank layer to
enable comprehensive feature extraction from the wavelet domain. Additionally,
the adaptive hard-thresholding nonlinearity is applied to remove redundant
components while denoising the primary wavelet coefficients. On the decoder
side, inverse LWT, along with multiple linear layers and activation functions,
is employed to reconstruct the original signals. Furthermore, to enhance
compression efficiency, a sparsity constraint is introduced during training to
impose sparsity on the latent representations. The experimental results
demonstrate that the proposed approach achieves superior data compression
performance compared to state-of-the-art methods.; 13) Robustness of infinite frames and Besselian structures; This paper extends the concepts of Minimal Redundancy Condition (MRC) and
robustness of erasures for infinite frames in Hilbert spaces. We begin by
establishing a comprehensive framework for the MRC, emphasizing its importance
in ensuring the stability and resilience of frames under finite erasures.
Furthermore, we discussed the robustness of erasures, which generalizes the
ability of a frame to withstand information loss. The relationship between
robustness, MRC, and excess of a frame is carefully examined, providing new
insights into the interplay between these properties. The robustness of
Besselian frames, highlighting their potential in applications where erasure
resilience is critical. Our results contribute to a deeper understanding of
frame theory and its role in addressing challenges posed by erasure recovery.; 14) Geometric Flavours of Quantum Field Theory on a Cauchy Hypersurface:
  Gaussian Analysis for the Hamiltonian Formalism and Applications to Cosmology; This thesis explores Quantum Field Theory (QFT) on curved spacetimes using a
geometric Hamiltonian approach to the Schr\""odinger-like representation. In
particular it studies the theory of the scalar field described through its
configurations over a Cauchy hypersurface. It is focused on mathematical
consistency based on analytic and geometric tools.
  The mathematical aspects of Gaussian integration theory in
infinite-dimensional Topological Vector Spaces (TVS) are thoroughly reviewed.
It also reviews the complex and holomorphic versions of important results and
concepts of Gaussian integration. For example, the Wiener-It\^o decomposition
theorem or the definition of Hida test functions.
  The physical framework builds upon three interconnected levels: classical
General Relativity (GR), Classical Statistical Field Theory (CSFT), and QFT.
The work begins by extending the Koopman-van Hove (KvH) formalism of classical
statistical mechanics to CSFT. This description is based upon prequantization
theory. It reveals features inherent to both CSFT and QFT, that help delineate
the genuine quantum features of a theory.
  Upon the prequantum program, the QFT of the scalar field is built mixing
Geometric Quantization with the choice of Wick and Weyl orderings. Various
quantum representations are introduced: the holomorphic, Schr\""odinger,
field-momentum, and antiholomorphic. The relation among them is studied using
integral transforms, including novel infinite-dimensional Fourier transforms.
From a geometrical analysis, it is argued that a covariant time derivative that
modifies the evolution equations should be added to the Schr\""odinger equation.
This connection is unique and required by the geometrodynamical description
ofthe coupling of QFT and GR.
  Finally, studying the free model on cosmological spacetimes it obtains
particle creation effects on a dynamical equation.; 15) SkyStore: Cost-Optimized Object Storage Across Regions and Clouds; Modern applications span multiple clouds to reduce costs, avoid vendor
lock-in, and leverage low-availability resources in another cloud. However,
standard object stores operate within a single cloud, forcing users to manually
manage data placement across clouds, i.e., navigate their diverse APIs and
handle heterogeneous costs for network and storage. This is often a complex
choice: users must either pay to store objects in a remote cloud, or pay to
transfer them over the network based on application access patterns and cloud
provider cost offerings. To address this, we present SkyStore, a unified object
store that addresses cost-optimal data management across regions and clouds.
SkyStore introduces a virtual object and bucket API to hide the complexity of
interacting with multiple clouds. At its core, SkyStore has a novel TTL-based
data placement policy that dynamically replicates and evicts objects according
to application access patterns while optimizing for lower cost. Our evaluation
shows that across various workloads, SkyStore reduces the overall cost by up to
6x over academic baselines and commercial alternatives like AWS multi-region
buckets. SkyStore also has comparable latency, and its availability and fault
tolerance are on par with standard cloud offerings. We release the data and
code of SkyStore at https://github.com/skyplane-project/skystore.; 16) Modular Forms and Certain ${}_2F_1(1)$ Hypergeometric Series; Using the framework relating hypergeometric motives to modular forms, we
define an explicit family of weight 2 Hecke eigenforms with complex
multiplication. We use the theory of ${}_2F_1(1)$ hypergeometric series and
Ramanujan's theory of alternative bases to compute the exact central $L$-value
of these Hecke eigenforms in terms of special beta values. We also show the
integral Fourier coefficients can be written in terms of Jacobi sums,
reflecting a motivic relation between the hypergeometric series and the modular
forms.; 17) Eye-in-Finger: Smart Fingers for Delicate Assembly and Disassembly of
  LEGO; Manipulation and insertion of small and tight-toleranced objects in robotic
assembly remain a critical challenge for vision-based robotics systems due to
the required precision and cluttered environment. Conventional global or
wrist-mounted cameras often suffer from occlusions when either assembling or
disassembling from an existing structure. To address the challenge, this paper
introduces ""Eye-in-Finger"", a novel tool design approach that enhances robotic
manipulation by embedding low-cost, high-resolution perception directly at the
tool tip. We validate our approach using LEGO assembly and disassembly tasks,
which require the robot to manipulate in a cluttered environment and achieve
sub-millimeter accuracy and robust error correction due to the tight
tolerances. Experimental results demonstrate that our proposed system enables
real-time, fine corrections to alignment error, increasing the tolerance of
calibration error from 0.4mm to up to 2.0mm for the LEGO manipulation robot.; 18) I Think, Therefore I Hallucinate: Minds, Machines, and the Art of Being
  Wrong; This theoretical work examines 'hallucinations' in both human cognition and
large language models, comparing how each system can produce perceptions or
outputs that deviate from reality. Drawing on neuroscience and machine learning
research, we highlight the predictive processes that underlie human and
artificial thought. In humans, complex neural mechanisms interpret sensory
information under uncertainty, sometimes filling in gaps and creating false
perceptions. This inference occurs hierarchically: higher cortical levels send
top-down predictions to lower-level regions, while mismatches (prediction
errors) propagate upward to refine the model. LLMs, in contrast, rely on
auto-regressive modeling of text and can generate erroneous statements in the
absence of robust grounding. Despite these different foundations - biological
versus computational - the similarities in their predictive architectures help
explain why hallucinations occur. We propose that the propensity to generate
incorrect or confabulated responses may be an inherent feature of advanced
intelligence. In both humans and AI, adaptive predictive processes aim to make
sense of incomplete information and anticipate future states, fostering
creativity and flexibility, but also introducing the risk of errors. Our
analysis illuminates how factors such as feedback, grounding, and error
correction affect the likelihood of 'being wrong' in each system. We suggest
that mitigating AI hallucinations (e.g., through improved training,
post-processing, or knowledge-grounding methods) may also shed light on human
cognitive processes, revealing how error-prone predictions can be harnessed for
innovation without compromising reliability. By exploring these converging and
divergent mechanisms, the paper underscores the broader implications for
advancing both AI reliability and scientific understanding of human thought.; 19) Transfer Learning in Physics-Informed Neural Networks: Full Fine-Tuning,
  Lightweight Fine-Tuning, and Low-Rank Adaptation; AI for PDEs has garnered significant attention, particularly Physics-Informed
Neural Networks (PINNs). However, PINNs are typically limited to solving
specific problems, and any changes in problem conditions necessitate
retraining. Therefore, we explore the generalization capability of transfer
learning in the strong and energy form of PINNs across different boundary
conditions, materials, and geometries. The transfer learning methods we employ
include full finetuning, lightweight finetuning, and Low-Rank Adaptation
(LoRA). The results demonstrate that full finetuning and LoRA can significantly
improve convergence speed while providing a slight enhancement in accuracy.; 20) Machine Learners Should Acknowledge the Legal Implications of Large
  Language Models as Personal Data; Does GPT know you? The answer depends on your level of public recognition;
however, if your information was available on a website, the answer is probably
yes. All Large Language Models (LLMs) memorize training data to some extent. If
an LLM training corpus includes personal data, it also memorizes personal data.
Developing an LLM typically involves processing personal data, which falls
directly within the scope of data protection laws. If a person is identified or
identifiable, the implications are far-reaching: the AI system is subject to EU
General Data Protection Regulation requirements even after the training phase
is concluded. To back our arguments: (1.) We reiterate that LLMs output
training data at inference time, be it verbatim or in generalized form. (2.) We
show that some LLMs can thus be considered personal data on their own. This
triggers a cascade of data protection implications such as data subject rights,
including rights to access, rectification, or erasure. These rights extend to
the information embedded with-in the AI model. (3.) This paper argues that
machine learning researchers must acknowledge the legal implications of LLMs as
personal data throughout the full ML development lifecycle, from data
collection and curation to model provision on, e.g., GitHub or Hugging Face.
(4.) We propose different ways for the ML research community to deal with these
legal implications. Our paper serves as a starting point for improving the
alignment between data protection law and the technical capabilities of LLMs.
Our findings underscore the need for more interaction between the legal domain
and the ML community.; 21) Causality of brane universe with the general bulk-based formalism with
  the non-zero Schwarzschild mass; In brane world scenarios, it is generally accepted that electromagnetic waves
(EMWs) are confined to propagate along the brane, while gravitational waves
(GWs) are free to travel through the bulk. This distinction has been utilized
in various studies addressing the cosmological horizon problem in the
literature. In this paper, we employ general bulk-based formalisms to
re-examine this problem, specifically focusing on the influence of non-zero
Schwarzschild mass on the geodesics. We begin with the Lagrangian of test
particles and employ the method of variation of action to derive the equations
of motion. Our findings reveal that the Schwarzschild mass significantly
affects the ratio of the gravitational horizon to the photon horizon. In the
low-energy regime, we can obtain a constraint on the constant curvature radius
of anti-de Sitter, i.e. $l H_0 \lesssim 10^{-29}$. This finding is consistent
with existing literature. However, in the high-energy regime the ratio
$r_g/r_\gamma$ increases by thirty orders of magnitude, reaching up to
$10^{33}$. Furthermore, when we take into account the nucleosynthesis
constraint $\sigma^{1/4} < 1 MeV$, the ratio is further enhanced to $10^{40}$.
Our results indicate that a non-zero Schwarzschild mass significantly
influences the gravity of the brane world. Some of these effects may be
leveraged to address or mitigate the standard problems that encountered in the
conventional universe.; 22) New Characterization of regional controllability and controllability of
  deterministic cellular automata via topological and symbolic dynamics notions; This article presents a new characterization of controllability and regional
controllability of Deterministic Cellular Automata (CA for short). It focuses
on analyzing these problems within the framework of control theory, which have
been extensively studied for continuous systems modeled by partial differential
equations (PDEs). In the analysis of linear systems, the Kalman rank condition
is ubiquitous and has been used to obtain the main results characterizing
controllability. The aim of this paper is to highlight new ways to prove the
regional controllability and controllability of CA using concepts from symbolic
dynamics instead of using the Kalman condition. Necessary and sufficient
conditions are given using the notions of chain transitive, chain mixing, trace
approximation, transitive SFT and mixing SFT. Finally, we demonstrate that the
presence of visibly blocking words implies that the cellular automaton is not
controllable.; 23) Transition energy fields in the method of correlation equations; In this paper, the well-known method of correlation equations for
constructing Gibbs measures is generalized based on the concept of the
transition energy field. Using the properties of transition energies, we obtain
the system of correlation equations for lattice systems with finite spin space.
It is shown that for a sufficiently small value of the one-point transition
energies, the corresponding system of correlation functions, considered in
infinite space, has a solution which is unique. Finally, the convergence of
finite-volume correlation functions to the limiting correlation function is
shown.; 24) Comparative study of the ans\""atze in quantum language models; Quantum language models are the alternative to classical language models,
which borrow concepts and methods from quantum machine learning and
computational linguistics. While several quantum natural language processing
(QNLP) methods and frameworks exist for text classification and generation,
there is a lack of systematic study to compare the performance across various
ans\""atze, in terms of their hyperparameters and classical and quantum methods
to implement them. Here, we evaluate the performance of quantum natural
language processing models based on these ans\""atze at different levels in text
classification tasks. We perform a comparative study and optimize the QNLP
models by fine-tuning several critical hyperparameters. Our results demonstrate
how the balance between simplification and expressivity affects model
performance. This study provides extensive data to improve our understanding of
QNLP models and opens the possibility of developing better QNLP algorithms.; 25) BOLDreams: Dreaming with pruned in-silico fMRI Encoding Models of the
  Visual Cortex; In this article we use the Natural Scenes Dataset (NSD) to train a family of
feature-weighted receptive field neural encoding models. These models use a
pre-trained vision or text backbone and map extracted features to the voxel
space via receptive field readouts. We comprehensively assess such models,
quantifying performance changes based on using different modalities like text
or images, toggling finetuning, using different pre-trained backbones, and
changing the width of the readout. We also dissect each model using explainable
AI (XAI) techniques, such as feature visualization via input optimization, also
referred to as ``dreaming'' in the AI literature, and the integrated gradients
approach to calculate implicit attention maps to illustrate which features
drive the predicted signal in different brain areas. These XAI tools illustrate
biologically plausible features that drive the predicted signal. Traversing the
model hyperparameter space reveals the existence of a maximally minimal model,
balancing simplicity while maintaining performance.; 26) Thermodynamic properties of fcc lead: A scalar and fully relativistic
  first principle study; This study investigates the thermodynamic properties of face-centered cubic
lead (fcc-Pb) using ab-initio methods within the quasi-harmonic approximation
(QHA), examining the influence of spin-orbit coupling (SOC) and the
exchange-correlation functionals. Two types of ultrasoft pseudopotential
(US-PP) are considered: one that excludes (scalar relativistic PP) and one that
includes the SOC effects (fully relativistic PP). Further, for each PP, we test
the performance of three popular exchange-correlation functionals:
Perdew-Burke-Ernzerhof generalized gradient approximation (PBE) (Perdew et al.
Phys. Rev. Lett. 77, 3865 (1996)), PBE modified for dense solids (PBEsol)
(Perdew et al. Phys. Rev. Lett. 100, 136406 (2008)), and local density
approximation (LDA) (Perdew et al. Phys. Rev. B 23, 5048 (1981)). We calculate
the Helmholtz free energy, incorporating lattice vibrations (phonons) and
electronic excitation contributions. The estimated equation of state (at 4 K
and 301 K), phonon dispersions (at 100 K and 300 K), mode-Gr\""uneisen
parameters ({\gamma}q{\eta}) (at 100 K), volume thermal expansion coefficient
(\b{eta}), isobaric heat capacity (CP), bulk modulus (BS), and thermodynamic
average Gr\""uneisen parameter ({\gamma}) are compared with the available
experimental and theoretical studies. Moreover, the 0 K pressure-dependent
elastic constant-coefficient (Cij) of fcc lead and Pugh ratio, Debye
temperature, and longitudinal and transverse sound velocities for
polycrystalline lead are presented. The contributions of electronic excitations
in all the thermodynamic properties are found to be negligible. With increasing
pressure, the role of spin-orbit effects decreases but does not vanish. Our
findings demonstrate that SOC leads to results distinct from the SR approach,
but agreement with the experiment is not consistently improved by including
SOC.; 27) Normative Cerebral Perfusion Across the Lifespan; Cerebral perfusion plays a crucial role in maintaining brain function and is
tightly coupled with neuronal activity. While previous studies have examined
cerebral perfusion trajectories across development and aging, precise
characterization of its lifespan dynamics has been limited by small sample
sizes and methodological inconsistencies. In this study, we construct the first
comprehensive normative model of cerebral perfusion across the human lifespan
(birth to 85 years) using a large multi-site dataset of over 12,000
high-quality arterial spin labeling (ASL) MRI scans. Leveraging generalized
additive models for location, scale, and shape (GAMLSS), we mapped nonlinear
growth trajectories of cerebral perfusion at global, network, and regional
levels. We observed a rapid postnatal increase in cerebral perfusion, peaking
at approximately 7.1 years, followed by a gradual decline into adulthood. Sex
differences were evident, with distinct regional maturation patterns rather
than uniform differences across all brain regions. Beyond normative modeling,
we quantified individual deviations from expected CBF patterns in
neurodegenerative and psychiatric conditions, identifying disease-specific
perfusion abnormalities across four brain disorders. Using longitudinal data,
we established typical and atypical cerebral perfusion trajectories,
highlighting the prognostic value of perfusion-based biomarkers for detecting
disease progression. Our findings provide a robust normative framework for
cerebral perfusion, facilitating precise characterization of brain health
across the lifespan and enhancing the early identification of neurovascular
dysfunction in clinical populations.; 28) Predictability of temporal network dynamics in normal ageing and brain
  pathology; Spontaneous brain activity generically displays transient spatiotemporal
coherent structures, which can selectively be affected in various neurological
and psychiatric pathologies. Here we model the full brain's
electroencephalographic activity as a high-dimensional functional network
performing a trajectory in a latent graph phase space. This approach allows us
to investigate the orbital stability of brain's activity and in particular its
short-term predictability. We do this by constructing a non-parametric
statistic quantifying the expansion of initially close functional network
trajectories. We apply the method to cohorts of healthy ageing individuals, and
patients previously diagnosed with Parkinson's or Alzheimer's disease. Results
not only characterise brain dynamics from a new angle, but further show that
functional network predictability varies in a marked scale-dependent way across
healthy controls and patient groups. The path towards both pathologies is
markedly different. Furthermore, healthy ageing's predictability appears to
strongly differ from that of Parkinson's disease, but much less from that of
patients with Alzheimer's disease.; 29) Deviance Detection and Regularity Sensitivity in Dissociated Neuronal
  Cultures; Understanding how neural networks process complex patterns of information is
crucial for advancing both neuroscience and artificial intelligence. To
investigate fundamental principles of neural computation, we studied
dissociated neuronal cultures, one of the most primitive living neural
networks, on high-resolution CMOS microelectrode arrays and tested whether the
dissociated culture exhibits regularity sensitivity beyond mere
stimulus-specific adaptation and deviance detection. In oddball electrical
stimulation paradigms, we confirmed that the neuronal culture produced mismatch
responses (MMRs) with true deviance detection beyond mere adaptation. These
MMRs were dependent on the N-methyl-D-aspartate (NMDA) receptors, similar to
mismatch negativity (MMN) in humans, which is known to have true deviance
detection properties. Crucially, we also showed sensitivity to the statistical
regularity of stimuli, a phenomenon previously observed only in intact brains:
the MMRs in a predictable, periodic sequence were smaller than those in a
commonly used sequence in which the appearance of the deviant stimulus was
random and unpredictable. These results challenge the traditional view that a
hierarchically structured neural network is required to process complex
temporal patterns, suggesting instead that deviant detection and regularity
sensitivity are inherent properties arising from the primitive neural network.
They also suggest new directions for the development of neuro-inspired
artificial intelligence systems, emphasizing the importance of incorporating
adaptive mechanisms and temporal dynamics in the design of neural networks.; 30) Dynamic Markov Blanket Detection for Macroscopic Physics Discovery; The free energy principle (FEP), along with the associated constructs of
Markov blankets and ontological potentials, have recently been presented as the
core components of a generalized modeling method capable of mathematically
describing arbitrary objects that persist in random dynamical systems; that is,
a mathematical theory of ``every'' ``thing''. Here, we leverage the FEP to
develop a mathematical physics approach to the identification of objects,
object types, and the macroscopic, object-type-specific rules that govern their
behavior. We take a generative modeling approach and use variational Bayesian
expectation maximization to develop a dynamic Markov blanket detection
algorithm that is capable of identifying and classifying macroscopic objects,
given partial observation of microscopic dynamics. This unsupervised algorithm
uses Bayesian attention to explicitly label observable microscopic elements
according to their current role in a given system, as either the internal or
boundary elements of a given macroscopic object; and it identifies macroscopic
physical laws that govern how the object interacts with its environment.
Because these labels are dynamic or evolve over time, the algorithm is capable
of identifying complex objects that travel through fixed media or exchange
matter with their environment. This approach leads directly to a flexible class
of structured, unsupervised algorithms that sensibly partition complex
many-particle or many-component systems into collections of interacting
macroscopic subsystems, namely, ``objects'' or ``things''. We derive a few
examples of this kind of macroscopic physics discovery algorithm and
demonstrate its utility with simple numerical experiments, in which the
algorithm correctly labels the components of Newton's cradle, a burning fuse,
the Lorenz attractor, and a simulated cell.; 31) Intrinsic motivation as constrained entropy maximization; ""Intrinsic motivation"" refers to the capacity for intelligent systems to be
motivated endogenously, i.e. by features of agential architecture itself rather
than by learned associations between action and reward. This paper views active
inference, empowerment, and other formal accounts of intrinsic motivation as
variations on the theme of constrained maximum entropy inference, providing a
general perspective on intrinsic motivation complementary to existing
frameworks. The connection between free energy and empowerment noted in
previous literature is further explored, and it is argued that the
maximum-occupancy approach in practice incorporates an implicit model-evidence
constraint.; 32) Freezing of Gait as a Complication of Pallidal Deep Brain Stimulation in
  DYT- KMT2B Patients with Evidence of Striatonigral Degeneration; Background: Mutations in KMT2B are a recognized cause of early-onset complex
dystonia, with deep brain stimulation (DBS) of the internal globus pallidus
(GPi-DBS) being an effective treatment. However, gait impairment, particularly
freezing of gait (FOG), remains a significant challenge in DYT-KMT2B patients
post-DBS. Objectives: To characterize the emergence of FOG in DYT-KMT2B
patients treated with GPi-DBS and explore potential underlying mechanisms,
including striatonigral degeneration. Methods: Five patients (four females)
with KMT2B-related dystonia and protein-truncating variants (PTVs) were
retrospectively analyzed. Clinical progression, response to GPi-DBS, and the
presence of FOG were documented. Dopaminergic function was assessed using
DaTscan (SPECT for ^123I-ioflupane) in four patients. Results: FOG developed in
all patients, with onset ranging from 1 to 15.5 years post-DBS. DaTscan
abnormalities, indicative of bilateral striatal dopaminergic denervation, were
observed in four cases. Prior to DBS, all patients exhibited dystonia
unresponsive to L-dopa, and post-DBS, FOG remained refractory to dopaminergic
treatment in most cases. Despite initial improvements in gait post-DBS, only
one patient maintained independent ambulation at the last follow-up.
Conclusions: FOG is an emerging complication in DYT-KMT2B patients with PTVs
undergoing GPi-DBS, potentially linked to underlying striatonigral
degeneration. The findings suggest a need for long-term motor surveillance and
consideration of alternative therapeutic strategies, including dopaminergic
trials, in this patient population. Further studies are required to elucidate
the precise mechanisms driving DBS-related hypokinetic gait disturbances in
DYT-KMT2B dystonia.; 33) A multi-class non-local macroscopic model with time delay for mixed
  autonomous / human-driven traffic; In this paper, we present a class of systems of non-local conservation laws
in one space-dimension incorporating time delay, which can be used to
investigate the interaction between autonomous and human-driven vehicles, each
characterized by a different reaction time and interaction range. We construct
approximate solutions using a Hilliges-Weidlich scheme and we provide uniform L
$\infty$ and BV estimates which ensure the convergence of the scheme, thus
obtaining existence of entropy weak solutions of bounded variation. Uniqueness
follows from an L 1 stability result derived from the entropy condition.
Additionally, we provide numerical simulations to illustrate applications to
mixed autonomous / human-driven traffic flow modeling. In particular, we show
that the presence of autonomous vehicles improves overall traffic flow and
stability.; 34) From Thought to Action: How a Hierarchy of Neural Dynamics Supports
  Language Production; Humans effortlessly communicate their thoughts through intricate sequences of
motor actions. Yet, the neural processes that coordinate language production
remain largely unknown, in part because speech artifacts limit the use of
neuroimaging. To elucidate the unfolding of language production in the brain,
we investigate with magnetoencephalography (MEG) and electroencephalography
(EEG) the neurophysiological activity of 35 skilled typists, while they typed
sentences on a keyboard. This approach confirms the hierarchical predictions of
linguistic theories: the neural activity preceding the production of each word
is marked by the sequential rise and fall of context-, word-, syllable-, and
letter-level representations. Remarkably, each of these neural representations
is maintained over long time periods within each level of the language
hierarchy. This phenomenon results in a superposition of successive
representations that is supported by a hierarchy of dynamic neural codes.
Overall, these findings provide a precise computational breakdown of the neural
dynamics that coordinate the production of language in the human brain.; 35) A Homogeneous Catalog of Oscillating Solar-Type Stars Observed by the
  Kepler Mission and a New Amplitude Scaling Relation Including Chromospheric
  Activity; We present a homogeneous catalog of global asteroseismic parameters and
derived stellar parameters for 765 Kepler main-sequence and subgiant stars. The
catalog was produced by re-analyzing all available Kepler DR25 short-cadence
data using pySYD, an automated pipeline to extract global asteroseismic
parameters. We find 50 new detections, seven of which are also planet candidate
host stars. We find excellent agreement between our $\nu_{\text{max}}$ and
$\Delta \nu$ measurements and literature values, with an average offset of $0.2
\pm 0.4\%$ ($\sigma=5\%$) and $0.2 \pm 0.7\%$ ($\sigma=2\%$), respectively. In
addition, we derive stellar radii and masses with an average precision of
$2.7\%$ and $10.4\%$, respectively, and find a median offset of $0.4 \pm 0.4\%$
($\sigma=10\%$) between our radii derived with asteroseismology and those from
Gaia parallaxes. Using spectroscopic $\log{R'_{\text{HK}}}$ activity
measurements from Keck/HIRES, we derive a new amplitude scaling relation with
an activity term for main-sequence and subgiant stars, which reduces the offset
between expected and observed oscillation amplitudes from $9.3 \pm 1.6\%$ to
$1.7 \pm 0.9\%$. Our work is the largest and most homogeneous asteroseismic
catalog of Kepler main-sequence and subgiant stars to date, including a total
of 101 stars hosting planet candidates and 451 stars with measured rotation
periods.; 36) Increased GM-WM in a prefrontal network and decreased GM in the insula
  and the precuneus are associated with reappraisal usage: A data fusion
  approach; Emotion regulation plays a crucial role in mental health, and difficulties in
regulating emotions can contribute to psychological disorders. While
reappraisal and suppression are well-studied strategies, the combined
contributions of gray matter (GM) and white matter (WM) to these strategies
remain unclear due to methodological limitations in previous studies. To
address this, we applied a data fusion approach using Parallel Independent
Component Analysis (Parallel ICA) to GM and WM MRI images from 165 individuals.
Parallel ICA identified two networks associated with reappraisal usage. Network
1 included a large lateral and medial prefrontal cortical network, overlapping
with the default mode network (DMN) and adjacent WM regions. Higher reappraisal
frequency was associated with greater GM-WM density within this network, and
this network was negatively correlated with perceived stress. Network 2
included the insula, precuneus, sub-gyral, and lingual gyri in its GM portion,
showing a negative association with reappraisal usage. The WM portion, adjacent
to regions of the central executive network (CEN), was positively associated
with reappraisal usage. Regarding suppression, no significant network was
associated with this strategy. This study provides new insights into individual
differences in reappraisal use, showing a positive association between
reappraisal frequency and increased gray and white matter concentration in a
large frontal network, including regions of the frontal DMN and the CEN.
Conversely, subcortical areas exhibited reduced gray and white concentration.; 37) AlphaPIG: The Nicest Way to Prolong Interactive Gestures in Extended
  Reality; Mid-air gestures serve as a common interaction modality across Extended
Reality (XR) applications, enhancing engagement and ownership through intuitive
body movements. However, prolonged arm movements induce shoulder fatigue, known
as ""Gorilla Arm Syndrome"", degrading user experience and reducing interaction
duration. Although existing ergonomic techniques derived from Fitts' law (such
as reducing target distance, increasing target width, and modifying
control-display gain) provide some fatigue mitigation, their implementation in
XR applications remains challenging due to the complex balance between user
engagement and physical exertion. We present AlphaPIG, a meta-technique
designed to Prolong Interactive Gestures by leveraging real-time fatigue
predictions. AlphaPIG assists designers in extending and improving XR
interactions by enabling automated fatigue-based interventions. Through
adjustment of intervention timing and intensity decay rate, designers can
explore and control the trade-off between fatigue reduction and potential
effects such as decreased body ownership. We validated AlphaPIG's effectiveness
through a study (N=22) implementing the widely-used Go-Go technique. Results
demonstrated that AlphaPIG significantly reduces shoulder fatigue compared to
non-adaptive Go-Go, while maintaining comparable perceived body ownership and
agency. Based on these findings, we discuss positive and negative perceptions
of the intervention. By integrating real-time fatigue prediction with adaptive
intervention mechanisms, AlphaPIG constitutes a critical first step towards
creating fatigue-aware applications in XR.; 38) Irreversible multi-band effects and Lifshitz transitions at the
  LaAlO3/SrTiO3 interface under field effect; In this work, we investigate the irreversible effects of an applied electric
field on the magnetotransport properties of LaAlO3/SrTiO3 conducting
interfaces, with focus on their multiband character. We study samples of
different types, namely with either crystalline or amorphous LaAlO3 overlayer.
Our two-band analysis highlights the similarity of the electronic properties of
crystalline and amorphous interfaces, regardless much different carrier
densities and mobilities. Furthermore, filling and depletion of the two bands
follow very similar patterns, at least in qualitative terms, in the two types
of samples. In agreement with previous works on crystalline interfaces, we
observe that an irreversible charge depletion takes place after application of
a first positive back gate voltage step. Such charge depletion affects much
more, in relative terms, the higher and three-dimensional dyz, dzx bands than
the lower and bidimensional dxy, driving the system through the Lifshitz
transition from two-band to single band behavior. The quantitative analysis of
experimental data evidences the roles of disorder, apparent in the depletion
regime, and temperature. Noteworthy, filling and depletion of the two bands
follow very similar patterns in crystalline and amorphous samples, at least in
qualitative terms, regardless much different carrier densities and mobilities.; 39) Intent-Interest Disentanglement and Item-Aware Intent Contrastive
  Learning for Sequential Recommendation; Recommender systems aim to provide personalized item recommendations by
capturing user behaviors derived from their interaction history. Considering
that user interactions naturally occur sequentially based on users' intents in
mind, user behaviors can be interpreted as user intents. Therefore,
intent-based sequential recommendations are actively studied recently to model
user intents from historical interactions for a more precise user understanding
beyond traditional studies that often overlook the underlying semantics behind
user interactions. However, existing studies face three challenges: 1) the
limited understanding of user behaviors by focusing solely on intents, 2) the
lack of robustness in categorizing intents due to arbitrary fixed numbers of
intent categories, and 3) the neglect of interacted items in modeling of user
intents. To address these challenges, we propose Intent-Interest
Disentanglement and Item-Aware Intent Contrastive Learning for Sequential
Recommendation (IDCLRec). IDCLRec disentangles user behaviors into intents
which are dynamic motivations and interests which are stable tastes of users
for a comprehensive understanding of user behaviors. A causal cross-attention
mechanism is used to identify consistent interests across interactions, while
residual behaviors are modeled as intents by modeling their temporal dynamics
through a similarity adjustment loss. In addition, without predefining the
number of intent categories, an importance-weighted attention mechanism
captures user-specific categorical intent considering the importance of intent
for each interaction. Furthermore, we introduce item-aware contrastive learning
which aligns intents that occurred the same interaction and aligns intent with
item combinations occurred by the corresponding intent. Extensive experiments
conducted on real-world datasets demonstrate the effectiveness of IDCLRec.; 40) Structuring the Environment Nudges Participants Toward Hierarchical Over
  Shortest Path Planning; Effective planning is crucial for navigating complex environments and
achieving goals efficiently. In this study, we investigated how environmental
structure influences the selection of planning strategies. Participants
navigated a space station to collect colored spheres, with environments either
structured (spheres grouped by color) or unstructured (spheres scattered
randomly). We tested three types of plans: hierarchical (grouping spheres by
color), shortest path (minimizing travel distance), and neutral (none of the
above). By manipulating environmental structure, we were able to nudge
participants toward a preference for hierarchical planning in structured
environments, while shortest path plans were favored in unstructured
environments. A mismatch between self-reported preferences and actual choices
indicated that participants often adopted implicit strategies, unaware of their
decision-making processes. These findings highlight the powerful effect of
environmental cues on planning and suggest that even subtle changes in
structure can guide the selection of planning strategies.; 41) Language modulates vision: Evidence from neural networks and human
  brain-lesion models; Comparing information structures in between deep neural networks (DNNs) and
the human brain has become a key method for exploring their similarities and
differences. Recent research has shown better alignment of vision-language DNN
models, such as CLIP, with the activity of the human ventral occipitotemporal
cortex (VOTC) than earlier vision models, supporting the idea that language
modulates human visual perception. However, interpreting the results from such
comparisons is inherently limited due to the ""black box"" nature of DNNs. To
address this, we combined model-brain fitness analyses with human brain lesion
data to examine how disrupting the communication pathway between the visual and
language systems causally affects the ability of vision-language DNNs to
explain the activity of the VOTC. Across four diverse datasets, CLIP
consistently outperformed both label-supervised (ResNet) and unsupervised
(MoCo) models in predicting VOTC activity. This advantage was left-lateralized,
aligning with the human language network. Analyses of the data of 33 stroke
patients revealed that reduced white matter integrity between the VOTC and the
language region in the left angular gyrus was correlated with decreased CLIP
performance and increased MoCo performance, indicating a dynamic influence of
language processing on the activity of the VOTC. These findings support the
integration of language modulation in neurocognitive models of human vision,
reinforcing concepts from vision-language DNN models. The sensitivity of
model-brain similarity to specific brain lesions demonstrates that leveraging
manipulation of the human brain is a promising framework for evaluating and
developing brain-like computer models.; 42) Construction of $p$-energy measures associated with strongly local
  $p$-energy forms; We construct canonical p-energy measures associated with $p$-energy forms
without any self-similarity, where $p$-energy forms are $L^p$ variations of
Dirichlet forms, which have recently been studied mainly on fractals.
Furthermore, we prove that the measures satisfy suitable chain rules and that
the uniqueness of such a ""good"" energy measures. As an application, we also
prove the p-energy version of Mosco's domination principle.; 43) Stiff-sloppy analysis of brain networks to reveal individual differences
  in task performance; Understanding how brain networks recruit resources during cognitive tasks is
key to explaining individual differences in task performance. Brain network
parameters-including activity levels of regions and their connectivity-reflect
the integration and segregation of functional subnetworks underlying task
processing. However, the complexity and high dimensionality of these parameters
pose a significant barrier to identifying functionally relevant individual
differences. Here, we introduce stiff-sloppy analysis as a framework for
uncovering the stiff parameter combinations that critically influence
task-state brain dynamics, exemplified by working memory. Using the pairwise
maximum entropy model (PMEM) calibrated to fMRI data and Fisher Information
Matrix (FIM) analysis, we reveal that the stiff dimensions of the model
parameters capture the most relevant integration and segregation processes of
the default mode network and the working memory network. Individual differences
along these stiff neural dimensions consistently correlate with working memory
performance. Notably, stiff parameters robustly predicted working memory
performance, even when the less sensitive (""sloppy"") parameters were excluded.
This study establishes stiff-sloppy analysis as a powerful approach to identify
cognition-related brain networks, bridging neural dynamics and behavior and
offering new avenues for personalized neuroscience including therapeutic
innovation.; 44) Hybrid Brain-Machine Interface: Integrating EEG and EMG for Reduced
  Physical Demand; We present a hybrid brain-machine interface (BMI) that integrates
steady-state visually evoked potential (SSVEP)-based EEG and facial EMG to
improve multimodal control and mitigate fatigue in assistive applications.
Traditional BMIs relying solely on EEG or EMG suffer from inherent limitations;
EEG-based control requires sustained visual focus, leading to cognitive
fatigue, while EMG-based control induces muscular fatigue over time. Our system
dynamically alternates between EEG and EMG inputs, using EEG to detect SSVEP
signals at 9.75 Hz and 14.25 Hz and EMG from cheek and neck muscles to optimize
control based on task demands. In a virtual turtle navigation task, the hybrid
system achieved task completion times comparable to an EMG-only approach, while
90% of users reported reduced or equal physical demand. These findings
demonstrate that multimodal BMI systems can enhance usability, reduce strain,
and improve long-term adherence in assistive technologies.; 45) GenTL: A General Transfer Learning Model for Building Thermal Dynamics; Transfer Learning (TL) is an emerging field in modeling building thermal
dynamics. This method reduces the data required for a data-driven model of a
target building by leveraging knowledge from a source building. Consequently,
it enables the creation of data-efficient models that can be used for advanced
control and fault detection & diagnosis. A major limitation of the TL approach
is its inconsistent performance across different sources. Although accurate
source-building selection for a target is crucial, it remains a persistent
challenge.
  We present GenTL, a general transfer learning model for single-family houses
in Central Europe. GenTL can be efficiently fine-tuned to a large variety of
target buildings. It is pretrained on a Long Short-Term Memory (LSTM) network
with data from 450 different buildings. The general transfer learning model
eliminates the need for source-building selection by serving as a universal
source for fine-tuning. Comparative analysis with conventional single-source to
single-target TL demonstrates the efficacy and reliability of the general
pretraining approach. Testing GenTL on 144 target buildings for fine-tuning
reveals an average prediction error (RMSE) reduction of 42.1 % compared to
fine-tuning single-source models.; 46) 1-Adamantanamine implementation in surface engineering of biomimetic
  PVDF-based membranes for enhanced membrane distillation; Membrane distillation (MD) stands at the forefront of desalination
technology, harnessing the power of phase change to separate water vapor from
saline using minimal energy resources efficiently. In response to this
challenge, membranes with tuned pores morphology and surface chemistry with
biomimetic 3D pine-like structures with improved affinity to water
(desalination) and/or hazardous VOC (VOC removal) were developed and studied
systematically. By implementing VIPS-PVDF membranes and a green modifier of
1-adamantanamine for the first time, membranes with a revolutionary network
architecture were generated. The modifier was introduced either physically to
the polymeric matrix or chemically through covalent attachment onto the surface
and inside the porous structure. As a result, membranes that defy wetting under
extreme hydrostatic pressures (>11.5 bar) were produced while preserving
unparalleled vapor transport efficiency. The 1-adamantanamine promotes
transport and enhances the affinity to the VOC, ensuring excellent membrane
performance at different applications of the MD process. Transport was enhanced
more than 3.6 times and separation factor beta changed from 3.48 to 15.22 for
MTBE removal and from 2.0 to 3.46 for EtOH removal when comparing pristine PVDF
with membrane chemically modified with 1-adamantanamine (PVDF_Ch02). The
process separation index during the MTBE removal changed from 20 kg m-2 h-1
(PVDF) to 297 kg m-2 h-1 (PVDF_Ch02). All materials were highly stable and
durable during the MD applications. This innovative approach not only
revolutionizes desalination but also holds immense promise for diverse
applications beyond, particularly in the realm of wastewater treatment. A study
of the icing process on a cold plate with new membranes provided deeper insight
into the icing mechanism and the role of membrane LEP in it.; 47) Dissociated Neuronal Cultures as Model Systems for Self-Organized
  Prediction; Dissociated neuronal cultures provide a simplified yet effective model system
for investigating self-organized prediction and information processing in
neural networks. This review consolidates current research demonstrating that
these in vitro networks display fundamental computational capabilities,
including predictive coding, adaptive learning, goal-directed behavior, and
deviance detection. We examine how these cultures develop critical dynamics
optimized for information processing, detail the mechanisms underlying learning
and memory formation, and explore the relevance of the free energy principle
within these systems. Building on these insights, we discuss how findings from
dissociated neuronal cultures inform the design of neuromorphic and reservoir
computing architectures, with the potential to enhance energy efficiency and
adaptive functionality in artificial intelligence. The reduced complexity of
neuronal cultures allows for precise manipulation and systematic investigation,
bridging theoretical frameworks with practical implementations in bio-inspired
computing. Finally, we highlight promising future directions, emphasizing
advancements in three-dimensional culture techniques, multi-compartment models,
and brain organoids that deepen our understanding of hierarchical and
predictive processes in both biological and artificial systems. This review
aims to provide a comprehensive overview of how dissociated neuronal cultures
contribute to neuroscience and artificial intelligence, ultimately paving the
way for biologically inspired computing solutions.; 48) Epilepsy and its driving forces: understanding the forces behind
  epileptical pathogenisis; Epilepsy is a neurological disorder characterized by seizures and epileptic
events intertwined with religious and personal beliefs since prehistoric times.
This review paper explores the historical context and challenges in defining
epilepsy. The formal definition was established twenty years ago, and the
multifaceted causes of this neurological disorder. It aims to pave the way for
personalised therapeutic strategies, research advancements, and informed public
health planning to enhance the lives of those affected by this complex
neurological condition. In addition, this review paper focuses on the
mechanisms and etiologies of epileptogenesis, categorizing them by mechanisms
and the underlying causes of the disorder. The review paper provides a brief
overview of the current state of the art in the diagnosis, diagnosis,
treatment, and treatment of epileptiform seizures.; 49) Subthreshold moment analysis of neuronal populations driven by
  synchronous synaptic inputs; Even when driven by the same stimulus, neuronal responses are well-known to
exhibit a striking level of spiking variability. In-vivo electrophysiological
recordings also reveal a surprisingly large degree of variability at the
subthreshold level. In prior work, we considered biophysically relevant
neuronal models to account for the observed magnitude of membrane voltage
fluctuations. We found that accounting for these fluctuations requires weak but
nonzero synchrony in the spiking activity, in amount that are consistent with
experimentally measured spiking correlations. Here we investigate whether such
synchrony can explain additional statistical features of the measured neural
activity, including neuronal voltage covariability and voltage skewness.
Addressing this question involves conducting a generalized moment analysis of
conductance-based neurons in response to input drives modeled as correlated
jump processes. Technically, we perform such an analysis using fixed-point
techniques from queuing theory that are applicable in the stationary regime of
activity. We found that weak but nonzero synchrony can consistently explain the
experimentally reported voltage covariance and skewness. This confirms the role
of synchrony as a primary driver of cortical variability and supports that
physiological neural activity emerges as a population-level phenomenon,
especially in the spontaneous regime.; 50) Boundary regularity for nonlocal elliptic equations over Reifenberg flat
  domains; We prove sharp boundary regularity of solutions to nonlocal elliptic
equations arising from operators comparable to the fractional Laplacian over
Reifenberg flat sets and with null exterior condition. More precisely, if the
operator has order $2s$ then the solution is $C^{s-\varepsilon}$ regular for
all $\varepsilon>0$ provided the flatness parameter is small enough. The proof
relies on an induction argument and its main ingredients are the construction
of a suitable barrier and the comparison principle.; 51) Active filtering: a predictive function of recurrent circuits of sensory
  cortex; Our brains encode many features of the sensory world into memories: we can
sing along with songs we have heard before, interpret spoken and written
language composed of words we have learned, and recognize faces and objects.
Where are these memories stored? Each area of the cerebral cortex has a huge
number of local, recurrent, excitatory-excitatory synapses, as many as 500
million per cubic millimeter. Here I review evidence that cortical recurrent
connectivity in sensory cortex is a substrate for sensory memories. Evidence
suggests that the local recurrent network encodes the structure of natural
sensory input, and that it does so via active filtering, transforming network
inputs to boost or select those associated with natural sensation. This is a
form of predictive processing, in which the cortical recurrent network
selectively amplifies some input patterns and attenuates others, and a form of
memory.; 52) Long-time asymptotics of the KdV equation with delta function initial
  profile; This work investigates the long-time asymptotic behaviors of the solution to
the KdV equation with delta function initial profile in different regions,
employing the Riemann-Hilbert formulation and Deift-Zhou nonlinear steepest
descent method. Despite the initial condition being discontinuous and having
singularity at the origin, the long-time asymptotics of the solution to this
problem exhibits remarkable richness and regularity. Specifically, when the
initial value is a delta potential well, the asymptotic solution is
predominantly dominated by a single soliton in certain region for $x>0$, while
in other regions, the dispersive tails including self-similar region,
collisionless shock region and dispersive wave region, play a more significant
role. Conversely, when the initial value is a delta potential barrier, the
soliton region is absent, although the dispersive tails still persist. The
leading-order terms of the solution in each region are derived, highlighting
the efficacy of the Riemann-Hilbert formulation in elucidating the long-time
behaviors of integrable systems.; 53) Reasoning within and between collective action problems; Understanding cooperation in social systems is challenging because the
ever-changing rules that govern societies interact with individual actions,
resulting in intricate collective outcomes. In virtual-world experiments, we
allowed people to make changes in the systems that they are making decisions
within and investigated how they weigh the influence of different rules in
decision-making. When choosing between worlds differing in more than one rule,
a naive heuristics model predicted participants decisions as well, and in some
cases better, than game earnings (utility) or by the subjective quality of
single rules. In contrast, when a subset of engaged participants made
instantaneous (within-world) decisions, their behavior aligned very closely
with objective utility and not with the heuristics model. Findings suggest
that, whereas choices between rules may deviate from rational benchmarks, the
frequency of real time cooperation decisions to provide feedback can be a
reliable indicator of the objective utility of these rules.; 54) Co-Learning Bayesian Optimization; Bayesian optimization (BO) is well known to be sample-efficient for solving
black-box problems. However, the BO algorithms can sometimes get stuck in
suboptimal solutions even with plenty of samples. Intrinsically, such
suboptimal problem of BO can attribute to the poor surrogate accuracy of the
trained Gaussian process (GP), particularly that in the regions where the
optimal solutions locate. Hence, we propose to build multiple GP models instead
of a single GP surrogate to complement each other and thus resolving the
suboptimal problem of BO. Nevertheless, according to the bias-variance tradeoff
equation, the individual prediction errors can increase when increasing the
diversity of models, which may lead to even worse overall surrogate accuracy.
On the other hand, based on the theory of Rademacher complexity, it has been
proved that exploiting the agreement of models on unlabeled information can
help to reduce the complexity of the hypothesis space, and therefore achieving
the required surrogate accuracy with fewer samples. Such value of model
agreement has been extensively demonstrated for co-training style algorithms to
boost model accuracy with a small portion of samples. Inspired by the above, we
propose a novel BO algorithm labeled as co-learning BO (CLBO), which exploits
both model diversity and agreement on unlabeled information to improve the
overall surrogate accuracy with limited samples, and therefore achieving more
efficient global optimization. Through tests on five numerical toy problems and
three engineering benchmarks, the effectiveness of proposed CLBO has been well
demonstrated.; 55) A Relativistic Theory of Consciousness (shortened version); This paper is a shortened version of the full paper that was published in the
journal Frontiers of Psychology in May 2022. In recent decades, the scientific
study of consciousness has significantly increased our understanding of this
elusive phenomenon. Yet, despite critical development in our understanding of
the functional side of consciousness, we still lack a fundamental theory
regarding its phenomenal aspect. The phenomenal aspect of consciousness is the
first-person answer to what it is like question, and it has thus far proved
recalcitrant to direct scientific investigation. The question of how the brain,
or any cognitive system, can create conscious experience out of neural
representations poses a great conundrum to science. Naturalistic dualists argue
that it is composed of a primitive, private, nonreductive element of reality.
Illusionists, on the other hand, argue that it is merely a cognitive illusion.
We contend that both the dualist and illusionist positions are flawed because
they tacitly assume consciousness to be an absolute property that does not
depend on the observer. We developed a conceptual and a mathematical argument
for a relativistic theory of consciousness in which a system either has or does
not have phenomenal consciousness with respect to some observer. According to
the theory, Phenomenal consciousness is neither private nor delusional, just
relativistic. In the frame of reference of the cognitive system, it will be
observable (first-person perspective) and in other frame of reference it will
not (third-person perspective). These two cognitive frames of reference are
both correct, just as in the case of an observer that claims to be at rest
while another will claim that the observer has constant velocity. Neither
observer position can be privileged, as they both describe the same underlying
reality.; 56) Operational Feasibility Analysis of a Cryogenic Active Intake Device for
  Atmosphere-Breathing Electric Propulsion; Atmosphere-breathing electric propulsion (ABEP) systems are emerging for
orbit maintenance in very-low-Earth orbit (VLEO) by capturing atmospheric
propellant \textit{in situ} using an intake device. A previous study proposed
the cryocondensation-regeneration active intake device (CRAID) to significantly
enhance intake performance. This study investigates the operational feasibility
of CRAID. A conceptual prototype model (CPM) is presented to verify its
feasibility, and numerical analyses demonstrate the practical operational
sequences, required cryocooler capacity, intake performance, and flight
envelope. The numerical analyses employ the direct simulation Monte Carlo
(DSMC) method with a phase change model and a 0D analytical model for RF ion
thrusters. A significant improvement in intake performance is estimated based
on the practical sequences, with compression performance at least 1000 times
higher than that of prevalent intake devices. The capability for consistent
propellant supply is observed regardless of atmospheric conditions. A model
satellite incorporating CPM confirms that CRAID enables complete drag
compensation at altitudes above 190 km without limiting the upper boundary of
the flight envelope.; 57) Inverse receptive field attention for naturalistic image reconstruction
  from the brain; Visual perception in the brain largely depends on the organization of
neuronal receptive fields. Although extensive research has delineated the
coding principles of receptive fields, most studies have been constrained by
their foundational assumptions. Moreover, while machine learning has
successfully been used to reconstruct images from brain data, this approach
faces significant challenges, including inherent feature biases in the model
and the complexities of brain structure and function. In this study, we
introduce an inverse receptive field attention (IRFA) model, designed to
reconstruct naturalistic images from neurophysiological data in an end-to-end
fashion. This approach aims to elucidate the tuning properties and
representational transformations within the visual cortex. The IRFA model
incorporates an attention mechanism that determines the inverse receptive field
for each pixel, weighting neuronal responses across the visual field and
feature spaces. This method allows for an examination of the dynamics of
neuronal representations across stimuli in both spatial and feature dimensions.
Our results show highly accurate reconstructions of naturalistic data,
independent of pre-trained models. Notably, IRF models trained on macaque V1,
V4, and IT regions yield remarkably consistent spatial receptive fields across
different stimuli, while the features to which neuronal representations are
selective exhibit significant variation. Additionally, we propose a data-driven
method to explore representational clustering within various visual areas,
further providing testable hypotheses.; 58) Agent-centric Information Access; As large language models (LLMs) become more specialized, we envision a future
where millions of expert LLMs exist, each trained on proprietary data and
excelling in specific domains. In such a system, answering a query requires
selecting a small subset of relevant models, querying them efficiently, and
synthesizing their responses. This paper introduces a framework for
agent-centric information access, where LLMs function as knowledge agents that
are dynamically ranked and queried based on their demonstrated expertise.
Unlike traditional document retrieval, this approach requires inferring
expertise on the fly, rather than relying on static metadata or predefined
model descriptions. This shift introduces several challenges, including
efficient expert selection, cost-effective querying, response aggregation
across multiple models, and robustness against adversarial manipulation. To
address these issues, we propose a scalable evaluation framework that leverages
retrieval-augmented generation and clustering techniques to construct and
assess thousands of specialized models, with the potential to scale toward
millions.; 59) A 7T fMRI dataset of synthetic images for out-of-distribution modeling
  of vision; Large-scale visual neural datasets such as the Natural Scenes Dataset (NSD)
are boosting NeuroAI research by enabling computational models of the brain
with performances beyond what was possible just a decade ago. However, these
datasets lack out-of-distribution (OOD) components, which are crucial for the
development of more robust models. Here, we address this limitation by
releasing NSD-synthetic, a dataset consisting of 7T fMRI responses from the
eight NSD subjects for 284 carefully controlled synthetic images. We show that
NSD-synthetic's fMRI responses reliably encode stimulus-related information and
are OOD with respect to NSD. Furthermore, OOD generalization tests on
NSD-synthetic reveal differences between models of the brain that are not
detected with NSD - specifically, self-supervised deep neural networks better
explain neural responses than their task-supervised counterparts. These results
showcase how NSD-synthetic enables OOD generalization tests that facilitate the
development of more robust models of visual processing, and the formulation of
more accurate theories of human vision.; 60) SelectiveFinetuning: Enhancing Transfer Learning in Sleep Staging
  through Selective Domain Alignment; In practical sleep stage classification, a key challenge is the variability
of EEG data across different subjects and environments. Differences in
physiology, age, health status, and recording conditions can lead to domain
shifts between data. These domain shifts often result in decreased model
accuracy and reliability, particularly when the model is applied to new data
with characteristics different from those it was originally trained on, which
is a typical manifestation of negative transfer. To address this, we propose
SelectiveFinetuning in this paper. Our method utilizes a pretrained Multi
Resolution Convolutional Neural Network (MRCNN) to extract EEG features,
capturing the distinctive characteristics of different sleep stages. To
mitigate the effect of domain shifts, we introduce a domain aligning mechanism
that employs Earth Mover Distance (EMD) to evaluate and select source domain
data closely matching the target domain. By finetuning the model with selective
source data, our SelectiveFinetuning enhances the model's performance on target
domain that exhibits domain shifts compared to the data used for training.
Experimental results show that our method outperforms existing baselines,
offering greater robustness and adaptability in practical scenarios where data
distributions are often unpredictable.; 61) Minimal Nilpotent Orbits of type D and E; We first show the closure of the minimal nilpotent adjoint orbit Omin^{D_n}
in so_{2n} is isomorphic to the affinization of T^*(SL_{n-1}/[P,P]) where P is
the parabolic subgroup P_{(1,1,n-3)} of SL_{n-1}(C). Then we prove that the
closure of the minimal nilpotent adjoint orbit Omin^{E_6} of the complex simple
Lie algebra E_6 is isomorphic to the affinization of T^*(SL_4/P^u) where P^u is
the unipotent radical of the parabolic subgroup P_{(2,2)} of SL_4(\C). In the
end we will formulate a similar result for type E_7.; 62) Towards an integrative approach to the study of brain-environment
  interactions in human and non-human primate; By retracing my scientific journey that began 20 years ago, I highlight in
this thesis the need to consider the organization of the brain, which is
certainly globally hierarchical, but also highly distributed and mixed in the
neuronal response of its different areas (by focusing on the sensorimotor
cortex-basal ganglia network). I also emphasize the importance of adopting
behavioral paradigms that reflect as much as possible the characteristics of
the scenarios encountered in the real life of animals. And finally, I mention
the importance of ""disintegrating"" the way data analysis is traditionally
carried out, and of taking into account the dynamic nature of behavior, for
example by favoring the study of behavioral variables and so-called ""latent""
neuronal activities. I conclude this thesis by presenting the vision of my
ideal laboratory in a perspective of 5 to 10 years from today. This laboratory
would have two experimental devices, a first, classic, for carrying out tests
in a constrained and therefore unnatural environment, the data of which would
be compared to those from a second device called ""naturalistic"" in which the
animals could potentially express their entire behavioral repertoire. The tasks
tested would be characterized by strong ecological principles, such as in those
simulating the properties of foraging, and the data would make it possible to
test hypotheses based on the progressive addition of ecological components in
these tasks, all this in order to maintain control over the interpretability of
these data which are complex by nature.; 63) The tardigrade as an emerging model organism for systems neuroscience; We present the case for developing the tardigrade (Hypsibius exemplaris) into
a model organism for systems neuroscience. These microscopic, transparent
animals (~300-500 microns) are among the smallest known to possess both limbs
(eight) and eyes (two), with a nervous system of only a few hundred neurons
organized into a multi-lobed brain, ventral nerve cord, and a series of ganglia
along the body. Despite their neuroanatomical simplicity, tardigrades exhibit
complex behaviors, including multi-limbed walking gaits, individual limb
grasping, phototaxis, and transitions between active and dormant states. These
behaviors position tardigrades as a uniquely powerful system for addressing
certain fundamental questions in systems neuroscience, such as: How do nervous
systems coordinate multi-limbed behaviors? How are top-down and bottom-up motor
control systems integrated? How is stereovision-guided navigation implemented?
What mechanisms underlie neural resilience and recovery during environmental
stress? We review current knowledge of tardigrade neuroanatomy, behavior, and
genomics, and we identify opportunities and challenges for leveraging their
unique biology. We propose developing essential neuroscientific tools for
tardigrades, including genetic engineering and live neuroimaging, alongside
behavioral assays linking neural activity to outputs. Leveraging their
evolutionary ties to Caenorhabditis elegans and Drosophila melanogaster, we can
adapt existing toolkits to accelerate tardigrade research - providing a bridge
between simpler invertebrate systems and more complex neural architectures.; 64) Enhancing Olfactory Perception Through Large Language Models:
  Integrating Sensory Data for Advanced Odor Recognition; The integration of biological principles into artificial olfactory systems
has led to significant advancements in odor detection and classification.
Inspired by the intricate mechanisms of natural olfaction, researchers are
developing sophisticated systems that mimic the functionality of biological
olfactory pathways. These systems utilize high-density chemoresistive sensor
arrays (HCSA) combined with advanced computational techniques, such as
FPGA-accelerated glomerular convergence circuits (FGCC) and hierarchical graph
neural networks (HGNN). This bioinspired approach enables real-time adaptive
responses to volatile organic compounds (VOCs), enhancing the accuracy and
efficiency of odor identification. At the core of these innovations is the
multiparametric sigmoidal sensor activation (MPSA), which quantifies VOCs by
leveraging the diverse responses of sensor arrays. The implementation of
lateral inhibition via programmable synaptic crossbars (LIPSC) further refines
odor processing by mimicking neural interactions found in biological systems.
Additionally, temporal self-organizing maps (TSOM) facilitate dynamic
clustering of odor patterns, allowing for a nuanced understanding of complex
odor environments. A novel aspect of this research lies in the Grassmannian
manifold embedding (GME) of odor profiles, which provides a mathematical
framework for representing and analyzing the multidimensional nature of odors.
Coupled with Hamiltonian Monte Carlo-optimized feedback (HMC-FB), this system
effectively compensates for drift in sensor readings, ensuring consistent
performance over time. By bridging the gap between biological inspiration and
technological innovation, these artificial olfactory systems are poised to
revolutionize applications ranging from environmental monitoring to food safety
and healthcare diagnostics.; 65) Concurrent Multifractality and Anomalous Hall Response in the Nodal Line
  Semimetal Fe$_3$GeTe$_2$ Near Localization; Topological states of matter exhibit unique protection against scattering by
disorder. Different topological classes exhibit distinct forms and degrees of
protection. Here, we investigate the response of the ferromagnetic nodal line
semimetal Fe$_3$GeTe$_2$ to disorder and electronic interactions. By combining
global magneto-transport with atomic-scale scanning tunneling spectroscopy we
find a simultaneous onset of diverse phenomena below a common temperature scale
of about 15 K: A crossover from metallic to insulating temperature dependence
of the longitudinal resistivity, saturation of the anomalous Hall conductivity
to its maximal value, formation of a sharp zero-bias dip in the tunneling
density of state, and emergence of multi-fractal structure of the electronic
wavefunction peaking at the Fermi energy. These concurrent observations reflect
the emergence of a novel energy scale possibly related to the opening of a gap
in the nodal line band of Fe$_3$GeTe$_2$. Our study provides overarching
insight into the role of disorder, electronic interactions and Berry curvature
in setting the micro- and macro-scale responses of topological semimetals.; 66) Improving Pediatric Low-Grade Neuroepithelial Tumors Molecular Subtype
  Identification Using a Novel AUROC Loss Function for Convolutional Neural
  Networks; Pediatric Low-Grade Neuroepithelial Tumors (PLGNT) are the most common pediatric cancer type, accounting for 40% of brain tumors in children, and identifying PLGNT molecular subtype is crucial treatment planning. However, gold standard to determine biopsy, which can be impractical or dangerous patients. This research improves performance Convolutional Neural Networks (CNNs) classifying subtypes through MRI scans by introducing a loss function that specifically model's Area Under Receiver Operating Characteristic (ROC) Curve (AUROC), offering non-invasive diagnostic alternative. In this study, retrospective dataset 339 children with (143 BRAF fusion, 71 V600E mutation, 125 non-BRAF) was curated. We employed CNN model Monte Carlo random data splitting. The baseline trained using binary cross entropy (BCE), achieved an AUROC 86.11% differentiating fusion mutations, improved 87.71% our proposed (p-value 0.045). With multiclass classification, from 74.42% 76. 59% 0.0016).; 67) Morphological Neuron Classification Using Machine Learning; Classification and quantitative characterization of neuronal morphologies
from histological neuronal reconstruction is challenging since it is still
unclear how to delineate a neuronal cell class and which are the best features
to define them by. The morphological neuron characterization represents a
primary source to address anatomical comparisons, morphometric analysis of
cells, or brain modeling. The objectives of this paper are (i) to develop and
integrate a pipeline that goes from morphological feature extraction to
classification and (ii) to assess and compare the accuracy of machine learning
algorithms to classify neuron morphologies. The algorithms were trained on 430
digitally reconstructed neurons subjectively classified into layers and/or
m-types using young and/or adult development state population of the
somatosensory cortex in rats. For supervised algorithms, linear discriminant
analysis provided better classification results in comparison with others. For
unsupervised algorithms, the affinity propagation and the Ward algorithms
provided slightly better results.; 68) $G$-systems and 4E Cognitive Science; We introduce a class dynamical systems called $G$-systems equipped with a
coupling operation. We use $G$-systems to define the notions of dependence
(borrowed from dependence logic) and causality (borrowed from Pearl) for
dynamical systems. As a converse to coupling we define decomposition or
``reducibility''. We give a characterization of reducibility in terms of the
dependence ""atom"". We do all this with the motivation of developing
mathematical foundations for 4E cognitive science, see introductory sections.; 69) An investigation of the relationship between morphology and chemistry of
  the D-type spherules from the recovery expedition of the CNEOS 2014-01-08
  bolide: Implications for origins; Cosmic spherules have largely been classified into S-, I-, and G-types
according to their compositions, and are identified to have chondritic or
achondritic materials as precursors. A recent recovery expedition attempted to
sample fragments of the CNEOS 2014-01-08 bolide retrieved roughly 850 magnetic
particles, some of which have unknown origins. Among those identified were a
new group of highly differentiated materials consisting of close to 160
specimens categorized as ""D-type"" particles. We studied the D-type particles
with the goal of comparing their various morphological features to their
chemical compositional groupings. Four morphological classifications are
considered: ""scoriaceous,"" ""stubby,"" ""blocky,"" and ""vesicular."" The specimens
from the ""scoriaceous"" and ""stubby"" groups exhibit a spinel/magnetite rim in at
least one instance, characteristic of atmospheric entry, and textures
indicative of quenching such as dendritic microcrystalline structures,
suggesting that a subset of specimens from these groups are candidates for
materials of extraterrestrial origin. The particles exhibiting ""blocky"" and
""vesicular"" textures are likely to be terrestrial in origin, with no obvious
quench features or signs of ablation. The D-type particles identified and
characterized in this study have a spectrum of terrestrial and probable
extraterrestrial origins.; 70) Neural Constraints on Cognitive Experience and Mental Health; Understanding how neural dynamics shape cognitive experiences remains a
central challenge in neuroscience and psychiatry. Here, we present a novel
framework leveraging state-to-output controllability from dynamical systems
theory to model the interplay between cognitive perturbations, neural activity,
and subjective experience. We demonstrate that large-scale fMRI signals are
constrained to low-dimensional manifolds, where affective and cognitive states
are naturally organized. Furthermore, we provide a theoretically robust method
to estimate the controllability Gramian from steady-state neural responses,
offering a direct measure of the energy required to steer cognitive outcomes.
In five healthy participants viewing 2,185 emotionally evocative short videos,
our analyses reveal a strong alignment between neural activations and affective
ratings, with an average correlation of $r \approx 0.7$. In a clinical cohort
of 255 patients with major depressive disorder, biweekly Hamilton Rating Scale
trajectories over 11 weeks significantly mapped onto these manifolds,
explaining approximately 20% more variance than chance ($p < 10^{-10}$,
numerically better than chance in 93% reaching statistical significance in
one-third of subjects). Our work bridges dynamical systems theory and clinical
neuroscience, providing a principled approach to optimize mental health
treatments by targeting the most efficient neural pathways for cognitive
change.; 71) On Questions of Predictability and Control of an Intelligent System
  Using Probabilistic State-Transitions; One of the central aims of neuroscience is to reliably predict the behavioral
response of an organism using its neural activity. If possible, this implies we
can causally manipulate the neural response and design brain-computer-interface
systems to alter behavior, and vice-versa. Hence, predictions play an important
role in both fundamental neuroscience and its applications. Can we predict the
neural and behavioral states of an organism at any given time? Can we predict
behavioral states using neural states, and vice-versa, and is there a
memory-component required to reliably predict such states? Are the predictions
computable within a given timescale to meaningfully stimulate and make the
system reach the desired states? Through a series of mathematical treatments,
such conjectures and questions are discussed. Answering them might be key for
future developments in understanding intelligence and designing
brain-computer-interfaces.; 72) Asynchronous Hebbian/anti-Hebbian networks; Lateral inhibition models coupled with Hebbian plasticity have been shown to
learn factorised causal representations of input stimuli, for instance,
oriented edges are learned from natural images. Currently, these models require
the recurrent dynamics to settle into a stable state before weight changes can
be applied, which is not only biologically implausible, but also impractical
for real-time learning systems. Here, we propose a new Hebbian learning rule
which is implemented using plausible biological mechanisms that have been
observed experimentally. We find that this rule allows for efficient,
time-continuous learning of factorised representations, very similar to the
classic noncontinuous Hebbian/anti-Hebbian learning. Furthermore, we show that
this rule naturally prevents catastrophic forgetting when stimuli from
different distributions are shown sequentially.; 73) Reasoning-Grounded Natural Language Explanations for Language Models; We propose a large language model explainability technique for obtaining
faithful natural language explanations by grounding the explanations in a
reasoning process. When converted to a sequence of tokens, the outputs of the
reasoning process can become part of the model context and later be decoded to
natural language as the model produces either the final answer or the
explanation. To improve the faithfulness of the explanations, we propose to use
a joint predict-explain approach, in which the answers and explanations are
inferred directly from the reasoning sequence, without the explanations being
dependent on the answers and vice versa. We demonstrate the plausibility of the
proposed technique by achieving a high alignment between answers and
explanations in several problem domains, observing that language models often
simply copy the partial decisions from the reasoning sequence into the final
answers or explanations. Furthermore, we show that the proposed use of
reasoning can also improve the quality of the answers.; 74) SOE's ESG Performance on Financial Flexibility: The Evidence from the
  Hong Kong Stock Market; As the global economic environment becomes increasingly unstable, enhancing
financial flexibility to cope with risks has become the consensus of many
companies. At the same time, environmental, social, and governance (ESG)
performance may be one of the effective ways. We studied the impact of a firm's
ESG performance on its financial flexibility with a sample of companies listed
on the Hong Kong stock market from 2018 to 2022. The empirical results show
that good environmental, social and governance performance can significantly
improve a firm's financial flexibility. In addition, this paper also finds that
the influence of ESG performance on financial flexibility is weak for
state-owned enterprises due to the influence of governance structure and market
characteristics. Finally, the further analysis shows that there is a mediating
role played by financing constraints in this process. This study can provide
background information for state-owned enterprises' governance, information
disclosure, and corporate operations. It also has guiding significance for
relevant investors, management and officials.; 75) $^{18}$F-FDG brain PET hypometabolism in post-SARS-CoV-2 infection:
  substrate for persistent/delayed disorders?; Purpose: Several brain complications of SARS-CoV-2 infection have been
reported. It has been moreover speculated that this neurotropism could
potentially cause a delayed outbreak of neuropsychiatric and neurodegenerative
diseases of neuroinflammatory origin. A propagation mechanism has been proposed
across the cribriform plate of the ethmoid bone, from the nose to the olfactory
epithelium, and possibly afterward to other limbic structures, and deeper parts
of the brain including the brainstem. Methods: Review of clinical examination,
and whole-brain voxel-based analysis of $^{18}$F-FDG PET metabolism in
comparison with healthy subjects (p voxel<0.001, p-cluster<0.05, uncorrected),
of two patients with confirmed diagnosis of SARS-CoV-2 explored at the
post-viral stage of the disease. Results: Hypometabolism of the
olfactory/rectus gyrus was found on the two patients, especially one with
4-week prolonged anosmia. Additional hypometabolisms were found within
amygdala, hippocampus, parahippocampus, cingulate cortex, pre-/post-central
gyrus, thalamus/hypothalamus, cerebellum, pons, and medulla in the other
patient who complained of delayed onset of a painful syndrome. Conclusion:
These preliminary findings reinforce the hypotheses of SARS-CoV-2 neurotropism
through the olfactory bulb and the possible extension of this impairment to
other brain structures. $^{18}$F-FDG PET hypometabolism could constitute a
cerebral quantitative biomarker of this involvement. Post-viral cohort studies
are required to specify the exact relationship between such hypometabolisms and
the possible persistent disorders, especially involving cognitive or emotion
disturbances, residual respiratory symptoms, or painful complaints.; 76) Electrophysiological Investigation of Insect Pain Threshold; The question of whether insects experience pain has long been debated in
neuroscience and animal behavior research. Increasing evidence suggests that
insects possess the ability to detect and respond to noxious stimuli,
exhibiting behaviors indicative of pain perception. This study investigates the
relationship between pain stimuli and physiological responses in crickets
(Gryllidae), focusing on heart rate (ECG) and brain wave (EEG) patterns. We
applied a range of mechanical, chemical, thermal, and electrical stimuli to
crickets, recording ECG and EEG data while employing a deep learning-based
model to classify pain levels. Our findings revealed significant heart rate
changes and EEG fluctuations in response to various stimuli, with the highest
intensity stimuli inducing marked physiological stress. The AI-based analysis,
utilizing AlexNet for EEG signal classification, achieved 90% accuracy in
distinguishing between resting, low-pain, and high-pain states. While no social
sharing of pain was observed through ECG measurements, these results contribute
to the growing body of evidence supporting insect nociception and offer new
insights into their physiological responses to external stressors. This
research advances the understanding of insect pain mechanisms and demonstrates
the potential for AI-driven analysis in entomological studies.; 77) Causal Spike Timing Dependent Plasticity Prevents Assembly Fusion in
  Recurrent Networks; The organization of neurons into functionally related assemblies is a
fundamental feature of cortical networks, yet our understanding of how these
assemblies maintain distinct identities while sharing members remains limited.
Here we analyze how spike-timing-dependent plasticity (STDP) shapes the
formation and stability of overlapping neuronal assemblies in recurrently
coupled networks of spiking neuron models. Using numerical simulations and an
associated mean-field theory, we demonstrate that the temporal structure of the
STDP rule, specifically its degree of causality, critically determines whether
assemblies that share neurons maintain segregation or merge together after
training is completed. We find that causal STDP rules, where
potentiation/depression occurs strictly when presynaptic spikes precede/proceed
postsynaptic spikes, allow assemblies to remain distinct even with substantial
overlap in membership. This stability arises because causal STDP effectively
cancels the symmetric correlations introduced by common inputs from shared
neurons. In contrast, acausal STDP rules lead to assembly fusion when overlap
exceeds a critical threshold, due to unchecked growth of common input
correlations. Our results provide theoretical insight into how
spike-timing-dependent learning rules can support distributed representation
where individual neurons participate in multiple assemblies while maintaining
functional specificity.; 78) Using economic value signals from primate prefrontal cortex in
  neuro-engineering applications; Neural signals related to movement can be measured from intracranial
recordings and used in brain-machine interface devices (BMI) to restore
physical function in impaired patients. In this study, we explore the use of
more abstract neural signals related to economic value in a BMI context. Using
data collected from the orbitofrontal cortex in non-human primates, we develop
deep learning-based neural decoders that can predict the monkey's choice in a
value-based decision-making task. Out-of-sample performance was improved by
augmenting the training set with synthesized data, showing the feasibility of
using limited training data. We further demonstrate that we can predict the
monkey's choice sooner using a neural forecasting module that is equipped with
task-related information. These findings support the feasibility of user
preference-informed neuroengineering devices that leverage abstract cognitive
signals.; 79) Contextual effects of sentiment deployment in human and machine
  translation; This paper illustrates how the overall sentiment of a text may be shifted in
translation and the implications for automated sentiment analyses, particularly
those that utilize machine translation and assess findings via semantic
similarity metrics. While human and machine translation will produce more
lemmas that fit the expected frequency of sentiment in the target language,
only machine translation will also reduce the overall semantic field of the
text, particularly in regard to words with epistemic content.; 80) Dynamical phases of short-term memory mechanisms in RNNs; Short-term memory is essential for cognitive processing, yet our
understanding of its neural mechanisms remains unclear. A key focus in
neuroscience has been the study of sequential activity patterns, where neurons
fire one after another within large networks, to explain how information is
maintained. While recurrent connections were shown to drive sequential
dynamics, a mechanistic understanding of this process still remains unknown. In
this work, we first introduce two unique mechanisms that can subserve
short-term memory: slow-point manifolds generating direct sequences or limit
cycles providing temporally localized approximations. Then, through analytical
models, we identify fundamental properties that govern the selection of these
mechanisms, \textit{i.e.}, we derive theoretical scaling laws for critical
learning rates as a function of the delay period length, beyond which no
learning is possible. We empirically verify these observations by training and
evaluating more than 35,000 recurrent neural networks (RNNs) that we will
publicly release upon publication. Overall, our work provides new insights into
short-term memory mechanisms and proposes experimentally testable predictions
for systems neuroscience.; 81) Exploring the Impact of Temperature Scaling in Softmax for
  Classification and Adversarial Robustness; The softmax function is a fundamental component in deep learning. This study
delves into the often-overlooked parameter within the softmax function, known
as ""temperature,"" providing novel insights into the practical and theoretical
aspects of temperature scaling for image classification. Our empirical studies,
adopting convolutional neural networks and transformers on multiple benchmark
datasets, reveal that moderate temperatures generally introduce better overall
performance. Through extensive experiments and rigorous theoretical analysis,
we explore the role of temperature scaling in model training and unveil that
temperature not only influences learning step size but also shapes the model's
optimization direction. Moreover, for the first time, we discover a surprising
benefit of elevated temperatures: enhanced model robustness against common
corruption, natural perturbation, and non-targeted adversarial attacks like
Projected Gradient Descent. We extend our discoveries to adversarial training,
demonstrating that, compared to the standard softmax function with the default
temperature value, higher temperatures have the potential to enhance
adversarial training. The insights of this work open new avenues for improving
model performance and security in deep learning applications.; 82) Automatic target validation based on neuroscientific literature mining
  for tractography; Target identification for tractography studies requires solid anatomical
knowledge validated by an extensive literature review across species for each
seed structure to be studied. Manual literature review to identify targets for
a given seed region is tedious and potentially subjective. Therefore,
complementary approaches would be useful. We propose to use text-mining models
to automatically suggest potential targets from the neuroscientific literature,
full-text articles and abstracts, so that they can be used for anatomical
connection studies and more specifically for tractography. We applied
text-mining models to three structures: two well-studied structures, since
validated deep brain stimulation targets, the internal globus pallidus and the
subthalamic nucleus and, the nucleus accumbens, an exploratory target for
treating psychiatric disorders. We performed a systematic review of the
literature to document the projections of the three selected structures and
compared it with the targets proposed by text-mining models, both in rat and
primate (including human). We ran probabilistic tractography on the nucleus
accumbens and compared the output with the results of the text-mining models
and literature review. Overall, text-mining the literature could find three
times as many targets as two man-weeks of curation could. The overall
efficiency of the text-mining against literature review in our study was 98%
recall (at 36% precision), meaning that over all the targets for the three
selected seeds, only one target has been missed by text-mining. We demonstrate
that connectivity for a structure of interest can be extracted from a very
large amount of publications and abstracts. We believe this tool will be useful
in helping the neuroscience community to facilitate connectivity studies of
particular brain regions. The text mining tools used for the study are part of
the HBP Neuroinformatics Platform, publicly available at
http://connectivity-brainer.rhcloud.com; 83) Rigidity Results Involving Stabilized Scalar Curvature; We establish a rigidity theorem for Brendle and Hung's recent systolic
inequality, which involves Gromov's notion of \(T^{\rtimes}\)-stabilized scalar
curvature. Our primary technique is the construction of foliations by free
boundary weighted constant mean curvature hypersurfaces, enabling us to
generalize several classical scalar curvature rigidity results to the
\(T^{\rtimes}\)-stabilized setting. Additionally, we develop a monotone
quantity using Ricci flow coupled with a heat equation, which is essential for
rigidity analysis.; 84) Evolution of diverse (and advanced) cognitive abilities through adaptive
  fine-tuning of learning and chunking mechanisms; The evolution of cognition is frequently discussed as the evolution of
cognitive abilities or the evolution of some neuronal structures in the brain.
However, since such traits or abilities are often highly complex, understanding
their evolution requires explaining how they could have gradually evolved
through selection acting on heritable variations in simpler cognitive
mechanisms. With this in mind, making use of a previously proposed theory, here
we show how the evolution of cognitive abilities can be captured by the
fine-tuning of basic learning mechanisms and, in particular, chunking
mechanisms. We use the term chunking broadly for all types of non-elemental
learning, claiming that the process by which elements are combined into chunks
and associated with other chunks, or elements, is critical for what the brain
can do, and that it must be fine-tuned to ecological conditions. We discuss the
relevance of this approach to studies in animal cognition, using examples from
animal foraging and decision-making, problem solving, and cognitive
flexibility. Finally, we explain how even the apparent human-animal gap in
sequence learning ability can be explained in terms of different fine-tunings
of a similar chunking process.; 85) Combined impact of grey and superficial white matter abnormalities:
  implications for epilepsy surgery; Drug-resistant focal epilepsy is associated with abnormalities in the brain
in both grey matter (GM) and superficial white matter (SWM). However, it is
unknown if both types of abnormalities are important in supporting seizures.
Here, we test if surgical removal of GM and/or SWM abnormalities relates to
post-surgical seizure outcome in people with temporal lobe epilepsy (TLE).
  We analyzed structural imaging data from 143 TLE patients (pre-op dMRI and
pre-op T1-weighted MRI) and 97 healthy controls. We calculated GM volume
abnormalities and SWM mean diffusivity abnormalities and evaluated if their
surgical removal distinguished seizure outcome groups post-surgically.
  At a group level, GM and SWM abnormalities were most common in the
ipsilateral temporal lobe and hippocampus in people with TLE. Analyzing both
modalities together, compared to in isolation, improved surgical outcome
discrimination (GM AUC = 0.68, p < 0.01, WM AUC = 0.65, p < 0.01; Union AUC =
0.72, p < 0.01, Concordance AUC = 0.64, p = 0.04). Additionally, 100% of people
who had all concordant abnormal regions resected had ILAE$_{1,2}$ outcomes.
  These findings suggest that regions identified as abnormal from both
diffusion-weighted and T1-weighted MRIs are involved in the epileptogenic
network and that resection of both types of abnormalities may enhance the
chances of living without disabling seizures.; 86) Multi-Site rs-fMRI Domain Alignment for Autism Spectrum Disorder
  Auxiliary Diagnosis Based on Hyperbolic Space; In the medical field, most resting-state fMRI (rs-fMRI) data are collected
from multiple hospital sites. Multi-site rs-fMRI data can increase the volume
of training data, enabling auxiliary diagnostic algorithms for brain diseases
to learn more accurate and stable models. However, due to the significant
heterogeneity and domain shift in rs-fMRI data across different sites, the
accuracy of auxiliary diagnosis remains unsatisfactory. Moreover, there has
been limited exploration of multi-source domain adaptation algorithms, and the
interpretability of models is often poor. To address these challenges, we
proposed a domain-adaptive algorithm based on hyperbolic space embedding.
Hyperbolic space is naturally suited for representing the topology of complex
networks such as brain functional networks. Therefore, we embedded the brain
functional network into hyperbolic space and constructed the corresponding
hyperbolic space community network to effectively extract brain network
representations. To address the heterogeneity of data across different sites
and the issue of domain shift, we introduce a constraint loss function, HMMD
(Hyperbolic Maximum Mean Discrepancy), to align the marginal distributions in
the hyperbolic space. Additionally, we employ class prototype alignment to
align the conditional distributions. This significantly improves the quality of
brain representations and enhances diagnostic classification accuracy for
Autism Spectrum Disorder (ASD). Experimental results demonstrated that the
proposed algorithm is robust to multi-site heterogeneity and shows promising
potential for brain network mechanism analysis.; 87) Artificial Neural Networks for Magnetoencephalography: A review of an
  emerging field; Magnetoencephalography (MEG) is a cutting-edge neuroimaging technique that
measures the intricate brain dynamics underlying cognitive processes with an
unparalleled combination of high temporal and spatial precision. MEG data
analytics has always relied on advanced signal processing and mathematical and
statistical tools for various tasks ranging from data cleaning to probing the
signals' rich dynamics and estimating the neural sources underlying the
surface-level recordings. Like in most domains, the surge in Artificial
Intelligence (AI) has led to the increased use of Machine Learning (ML) methods
for MEG data classification. More recently, an emerging trend in this field is
using Artificial Neural Networks (ANNs) to address many MEG-related tasks. This
review provides a comprehensive overview of how ANNs are being used with MEG
data from three vantage points: First, we review work that employs ANNs for MEG
signal classification, i.e., for brain decoding. Second, we report on work that
has used ANNs as putative models of information processing in the human brain.
Finally, we examine studies that use ANNs as techniques to tackle
methodological questions in MEG, including artifact correction and source
estimation. Furthermore, we assess the current strengths and limitations of
using ANNs with MEG and discuss future challenges and opportunities in this
field. Finally, by establishing a detailed portrait of the field and providing
practical recommendations for the future, this review seeks to provide a
helpful reference for both seasoned MEG researchers and newcomers to the field
who are interested in using ANNs to enhance the exploration of the complex
dynamics of the human brain with MEG.; 88) New Method for Robust Critical Analysis of Magnetic Systems; Here, we present new methods for critical analysis to determine the range of
exchange interaction(s) and appropriate values of critical exponents for
different magnetic systems. From computational and experimental investigations
of magnetic behavior of Ni and Gd, we show that the critical behavior remains
the same on either side of transition temperature. Using our proposed method of
analysis for Gd, we find a critical role of competing interactions where the
local electron moments follow 3D Ising type short-range interactions whereas
the itinerant electron moments constitute mean-field type long-range RKKY
interaction.; 89) Auto-Associative Memories for Direct Signalling of Visual Angle During
  Object Approaches; Being hit by a ball is usually not a pleasant experience. While a ball may
not be fatal, other objects can be. To protect themselves, many organisms, from
humans to insects, have developed neuronal mechanisms to signal approaching
objects such as predators and obstacles. The study of these neuronal circuits
is still ongoing, both experimentally and theoretically. Many computational
proposals rely on temporal contrast integration, as it encodes how the visual
angle of an approaching object changes with time. However, mechanisms based on
contrast integration are severely limited when the observer is also moving, as
it is difficult to distinguish the background-induced temporal contrast from
that of an approaching object. Here, I present results of a new mechanism for
signaling object approaches, based on modern content-addressable
(auto-associative) memories. Auto-associative memories were first proposed by
Hopfield in 1982, and are a class of simple neuronal networks which transform
incomplete or noisy input patterns to complete and noise-free output patterns.
The memory holds different sizes of a generic pattern template that is
efficient for segregating an approaching object from irrelevant background
motion. Therefore, the model's output correlates directly with angular size.
Generally, the new mechanism performs on a par with previously published
models. The overall performance was systematically evaluated based on the
network's responses to artificial and real-world video footage.; 90) Design, Dynamic Modeling and Control of a 2-DOF Robotic Wrist Actuated
  by Twisted and Coiled Actuators; Robotic wrists play a pivotal role in the functionality of industrial
manipulators and humanoid robots, facilitating manipulation and grasping tasks.
In recent years, there has been a growing interest in integrating artificial
muscle-driven actuators for robotic wrists, driven by advancements in
technology offering high energy density, lightweight construction, and compact
designs. However, in the study of robotic wrists driven by artificial muscles,
dynamic model-based controllers are often overlooked, despite their critical
importance for motion analysis and dynamic control of robots. This paper
presents a novel design of a two-degree-of-freedom (2-DOF) robotic wrist driven
by twisted and coiled actuators (TCA) utilizing a parallel mechanism with a
3RRRR configuration. The proposed robotic wrist is expected to feature
lightweight structures and superior motion performance while mitigating
friction issues. The Lagrangian dynamic model of the wrist is established,
along with a nonlinear model predictive controller (NMPC) designed for
trajectory tracking tasks. A prototype of the robotic wrist is developed, and
extensive experiments are conducted to validate its superior motion performance
and the proposed dynamic model. Subsequently, extensive comparative experiments
between NMPC and PID controller were conducted under various operating
conditions. The experimental results demonstrate the effectiveness and
robustness of the dynamic model-based controller in the motion control of
TCA-driven robotic wrists.; 91) Token-by-Token Regeneration and Domain Biases: A Benchmark of LLMs on
  Advanced Mathematical Problem-Solving; Large language models (LLMs) excel in many natural language tasks, yet they
struggle with complex mathemat-ical problem-solving, particularly in symbolic
reasoning and maintaining consistent output. This study evalu-ates 10 LLMs with
7 to 8 billion parameters using 945 competition-level problems from the MATH
dataset. The focus is on their ability to generate executable Python code as a
step in their reasoning process, involving over 9,450 code executions. The
research introduces an evaluation framework using mistral-large-2411 to rate
answers on a 5-point scale, which helps address inconsistencies in mathematical
notation. It also examines the impact of regenerating output token-by-token on
refining results. The findings reveal a significant 34.5% per-formance gap
between the top commercial model (gpt-4o-mini, scoring 83.7%) and the least
effective open-source model (open-codestral-mamba:v0.1, scoring 49.2%). This
disparity is especially noticeable in complex areas like Number Theory. While
token-by-token regeneration slightly improved accuracy (+0.8%) for the model
llama3.1:8b, it also reduced code execution time by 36.7%, highlighting a
trade-off between efficiency and precision. The study also noted a consistent
trend where harder problems correlated with lower accuracy across all models.
Despite using controlled execution environments, less than 1% of the generated
code was unsafe, and 3.17% of problems remained unsolved after 10 attempts,
suggesting that hybrid reasoning methods may be beneficial.; 92) Mechanoreceptive A$\beta$ primary afferents discriminate naturalistic
  social touch inputs at a functionally relevant time scale; Interpersonal touch is an important channel of social emotional interaction.
How these physical skin-to-skin touch expressions are processed in the
peripheral nervous system is not well understood. From microneurography
recordings in humans, we evaluated the capacity of six subtypes of cutaneous
mechanoreceptive afferents to differentiate human-delivered social touch
expressions. Leveraging statistical and classification analyses, we found that
single units of multiple mechanoreceptive A$\beta$ subtypes, especially slowly
adapting type II (SA-II) and fast adapting hair follicle afferents (HFA), can
reliably differentiate social touch expressions at accuracies similar to human
recognition. We then identified the most informative firing patterns of SA-II
and HFA afferents, which indicate that average durations of 3-4 s of firing
provide sufficient discriminative information. Those two subtypes also exhibit
robust tolerance to spike-timing shifts of up to 10-20 ms, varying with touch
expressions due to their specific firing properties. Greater shifts in
spike-timing, however, can change a firing pattern's envelope to resemble that
of another expression and drastically compromise an afferent's discrimination
capacity. Altogether, the findings indicate that SA-II and HFA afferents
differentiate the skin contact of social touch at time scales relevant for such
interactions, which are 1-2 orders of magnitude longer than those for
non-social touch.; 93) AnDB: Breaking Boundaries with an AI-Native Database for Universal
  Semantic Analysis; In this demonstration, we present AnDB, an AI-native database that supports
traditional OLTP workloads and innovative AI-driven tasks, enabling unified
semantic analysis across structured and unstructured data. While structured
data analytics is mature, challenges remain in bridging the semantic gap
between user queries and unstructured data. AnDB addresses these issues by
leveraging cutting-edge AI-native technologies, allowing users to perform
semantic queries using intuitive SQL-like statements without requiring AI
expertise. This approach eliminates the ambiguity of traditional text-to-SQL
systems and provides a seamless end-to-end optimization for analyzing all data
types. AnDB automates query processing by generating multiple execution plans
and selecting the optimal one through its optimizer, which balances accuracy,
execution time, and financial cost based on user policies and internal
optimizing mechanisms. AnDB future-proofs data management infrastructure,
empowering users to effectively and efficiently harness the full potential of
all kinds of data without starting from scratch.; 94) Long-term follow-up of DYT1 dystonia patients treated by deep brain
  stimulation: an open-label study; Long-term efficacy of internal globus pallidus (GPi) deep-brain stimulation
(DBS) in DYT1 dystonia and disease progression under DBS was studied.
Twenty-six patients of this open-label study were divided into two groups: (A)
with single bilateral GPi lead, (B) with a second bilateral GPi lead implanted
owning to subsequent worsening of symptomatology. Dystonia was assessed with
the Burke Scale. Appearance of new symptoms and distribution according to body
region were recorded. In the whole cohort, significant decreases in motor and
disability subscores (P < 0.0001) were observed at 1 year and maintained up to
10 years. Group B showed worsening of the symptoms. At 1 year, there were no
significant differences between Groups A (without subsequent worsening) and B;
at 5 years, a significant difference was found for motor and disability scores.
Within Group B, four patients exhibited additional improvement after the second
DBS surgery. In the 26 patients, significant difference (P = 0.001) was found
between the number of body regions affected by dystonia preoperatively and over
the whole follow-up. DBS efficacy in DYT1 dystonia can be maintained up to 10
years (two patients). New symptoms appear with long-term follow-up and may
improve with additional leads in a subgroup of patients.; 95) Perception of an AI Teammate in an Embodied Control Task Affects Team
  Performance, Reflected in Human Teammates' Behaviors and Physiological
  Responses; The integration of artificial intelligence (AI) into human teams is widely
expected to enhance performance and collaboration. However, our study reveals a
striking and counterintuitive result: human-AI teams performed worse than
human-only teams, especially when task difficulty increased. Using a virtual
reality-based sensorimotor task, we observed that the inclusion of an active
human-like AI teammate disrupted team dynamics, leading to elevated arousal,
reduced engagement, and diminished communication intensity among human
participants. These effects persisted even as the human teammates' perception
of the AI teammate improved over time. These findings challenge prevailing
assumptions about the benefits of AI in team settings and highlight the
critical need for human-centered AI design to mitigate adverse physiological
and behavioral impacts, ensuring more effective human-AI collaboration.; 96) KMT2B-related disorders: expansion of the phenotypic spectrum and
  long-term efficacy of deep brain stimulation; Heterozygous mutations in KMT2B are associated with an early-onset,
progressive, and often complex dystonia (DYT28). Key characteristics of typical
disease include focal motor features at disease presentation, evolving through
a caudocranial pattern into generalized dystonia, with prominent oromandibular,
laryngeal, and cervical involvement. Although KMT2B-related disease is emerging
as one of the most common causes of early-onset genetic dystonia, much remains
to be understood about the full spectrum of the disease. We describe a cohort
of 53 patients with KMT2B mutations, with detailed delineation of their
clinical phenotype and molecular genetic features. We report new disease
presentations, including atypical patterns of dystonia evolution and a subgroup
of patients with a non-dystonic neurodevelopmental phenotype. In addition to
the previously reported systemic features, our study has identified
co-morbidities, including the risk of status dystonicus, intrauterine growth
retardation, and endocrinopathies. Analysis of this study cohort (n = 53) in
tandem with published cases (n = 80) revealed that patients with chromosomal
deletions and protein-truncating variants had a significantly higher burden of
systemic disease (with earlier onset of dystonia) than those with missense
variants. Eighteen individuals had detailed longitudinal data available after
insertion of deep brain stimulation for medically refractory dystonia. Median
age at deep brain stimulation was 11.5 years (range: 4.5 to 37.0 years).
Follow-up after deep brain stimulation ranged from 0.25 to 22 years.
Significant improvement of motor function and disability (as assessed by the
Burke-Fahn-Marsden Dystonia Rating Scales, BFMDRS-M and BFMDRS-D) was evident
at 6 months, 1 year, and last follow-up (motor, P = 0.001, P = 0.004, and P =
0.012; disability, P = 0.009, P = 0.002, and P = 0.012).; 97) A Turing Test for Artificial Nets devoted to model Human Vision; In this 2022 work we argued that, despite claims about successful modeling of
the visual brain using artificial nets, the problem is far from being solved
(even for low-level vision). Examples of open issues include: where should we
read from ANNs in order to reproduce human behavior?, this ad-hoc read-out is
considered part of the brain model or not?, should we use artificial
psychophysics or artificial physiology?, in the case of ANNs, artificial
experiments should literally match the experiments done with humans?. There is
a clear need of rigorous procedures for experimental tests for ANNs devoted to
model the visual brain, and more generally, to understand ANNs devoted to
generic vision tasks. Following our experience in using low-level facts from
Quantitative Visual Neuroscience in computer vision, in this work we presented
the idea of developing a low-level dataset compiling the basic spatio-temporal
and chromatic facts that are known to happen in the retina-V1 pathway, and they
are not currently available in existing databases such as BrainScore. In our
results we checked the behavior of three recently proposed models with similar
architecture: (1) A parametric model tuned via Maximum Differentiation [Malo &
Simoncelli SPIE 15, Martinez et al. PLOS 18, Martinez et al. Front. Neurosci.
19], (2) A non-parametric model called PerceptNet tuned to maximize the
correlation with human opinion on subjective distortions [Hepburn et al. IEEE
ICIP 19], and (3) A model with the same encoder as PerceptNet, but tuned for
image segmentation (published as Hernandez-Camara et al. Patt.Recogn.Lett. 23).
Results on 10 compelling psycho/physio visual facts show that the first model
is the one with closer behavior to the humans in terms of receptive fields, but
more interestingly, on the nonlinear behavior when facing complex
spatio-chromatic patterns of a range of luminances and contrasts.; 98) Fractional-flux oscillations of Josephson critical currents in multi-gap
  superconductors: a test for unconventional superconductivity; Josephson-junction interferometry has played a pivotal role in uncovering
unconventional superconductivity in the cuprates. Using a Ginzburg-Landau-like
approach, we generalize previous results to the genuine multi-gap case. Thus,
we show that fractional flux oscillations of the Josephson critical current can
arise as a direct consequence of multi-gap superconductivity. These
oscillations reveal key information about the underlying superconducting
states, including the unconventional $s_\pm$-wave state. Thus, our findings
suggest new phase-sensitive experiments to characterize the Cooper pairing of
new emerging superconductors such as the nickelates.; 99) Personal Danger Signals Reprocessing: New Online Group Intervention for
  Chronic Pain; Chronic pain is a significant global health issue, with many patients
experiencing persistent pain despite no identifiable organic cause, classified
as nociplastic pain. Increasing evidence highlights the role of danger signal
processing in the maintenance of chronic pain. In response, we developed
Personal Danger Signals Reprocessing (PDSR), an online, group-based
intervention designed to modify these mechanisms using coaching techniques to
enhance accessibility and affordability.
  This study evaluated the efficacy of PDSR in reducing pain and mental health
comorbidities. A cohort of women (N=19, mean age 43) participated in an 8-week
online program, receiving weekly sessions on chronic pain mechanisms within a
systemic framework. Outcomes were assessed at three time points:
pre-intervention, mid-intervention, and post-intervention. A waiting list group
(N=20, mean age 43.5) completed assessments at the same intervals.
  Participants in the PDSR group showed significant pain reduction (p < .001),
with moderate to large effects observed at mid-intervention (Cohen's D = 0.7)
and post-intervention (Cohen's D = 1.5) compared to controls. Pain interference
significantly decreased (p < .01), with large reductions in the PDSR group
(Cohen's D = -1.7, p < .0001). Well-being also improved substantially (p <
.001, Cohen's D = 1.7-1.8). Secondary outcomes, including pain catastrophizing,
sleep interference, anxiety, and depressive symptoms, consistently improved
(all p-values < .01).
  Findings suggest PDSR is an effective, scalable intervention for reducing
pain, improving function, and enhancing well-being in individuals with chronic
pain.; 100) $B_s^0 \rightarrow \mu^+ \mu^-$ in a flavor violating extension of MSSM; $B$ meson rare decays play a crucial role in exploring new physics beyond the
standard model. In this study, we explore the rare decay process $B_s^0
\rightarrow \mu^+ \mu^-$ in a flavor violating extension of the Minimal
Supersymmetric Standard Model (MSSM), namely the $\mu$-from-$\nu$ SSM
($\mu\nu$SSM). Combined with the decay $\bar{B}\rightarrow X_s\gamma$, the
numerical results indicate that the $\mu\nu$SSM can successfully accommodate
the experimental data for $B_s^0 \rightarrow \mu^+ \mu^-$ and additionally
narrow down the parameter space.",0.3333333333333333,0.0
2411.00726,applied,2411.00726-pos1-5,"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",2411.00726-pos2-5,"Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema; This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection",97,"['1', '2', '4', '15', '7', '9', '10', '12', '13', '8']","The first candidate paper explores the effects of a new healthy food supplement on osteoporosis and rheumatoid arthritis, which could provide insights into leveraging the transformer architecture for recognizing patterns in images related to health. This collaboration can lead to innovative approaches in medical imaging analysis. The second candidate discusses calibration and errors in astrophysical observations, emphasizing the need for accurate data interpretation, which aligns with the data handling aspects of transformers in image recognition. The remaining candidates, while interesting, do not contribute as directly to a multidisciplinary research idea that combines health, imaging, and data processing. The implications for medical imaging using transformer-based methods could be groundbreaking, providing a novel angle on disease assessment through image analysis.","1) Effect of a new type of healthy and live food supplement on osteoporosis
  blood parameters and induced rheumatoid arthritis in Wistar rats; Summary Osteoporosis is a skeletal disorder, characterized by a decrease in
bone strength and puts the individual at risk for fracture. On the other hand,
rheumatoid arthritis is a systemic disease of unknown etiology that causes
inflammation of the joints of the organs. Purpose Due to the destructive
effects of these diseases and its increasing prevalence and lack of appropriate
medication for treatment, the present study aimed to evaluate the therapeutic
effect of a new type of healthy and live food supplement on rheumatoid
arthritis and induced osteoporosis in rats. Methods In this research, healthy
and live food powder were synthesized by a new and green route. This organic
biomaterial was named NBS. The NBS food supplement had various vitamins, macro
and micro molecules, and ingredients. The new healthy and nutritious diet
showed that the use of this supplement led to the return of the parameters to
normal levels. Results The concentration of 12.5 mg/ kg showed the least
therapeutic effect and 50 mg/ kg had the highest therapeutic effect for
osteoporosis. The results of blood parameters involved in inflammation in both
healthy and patient groups showed that the use of complete adjuvant induction
causes joint inflammation. In the study of the interaction of the
concentrations, it was observed that the concentration of 50 mg/ kg had the
highest therapeutic effect against the disease in the studied mice. Conclusion
The results showed that the new healthy and viable supplement restores the
blood osteoporotic and rheumatoid factors of the mice to normal.; 2) Impact of Calibration and Position Errors on Astrophysical Parameters of
  the {\hi} 21cm Signal; The Epoch of Reionization (EoR) and Cosmic Dawn (CD) are pivotal stages
during the first billion years of the universe, exerting a significant
influence on the development of cosmic structure. The detection of the
redshifted 21-cm signal from these epochs is challenging due to the dominance
of significantly stronger astrophysical foregrounds and the presence
systematics. This work used the 21cm E2E pipeline, followed by simulation
methodology described \cite{2022Mazumder} to conduct synthetic observations of
a simulated sky model that includes both the redshifted 21-cm signal and
foregrounds. A framework was constructed using Artificial Neural Networks (ANN)
and Bayesian techniques to directly deduce astrophysical parameters from the
measured power spectrum. This approach eliminates the need for explicit
telescope effects correction in interferometric arrays such as SKA-Low and
HERA. The present work investigates the impact of calibration and position
errors on retrieving the redshifted 21-cm power spectrum for the above arrays.
We assessed the effects of these inaccuracies on the deduced astrophysical
parameters and established acceptable tolerance levels. Based on our results,
the calibration error tolerance for ideal signal detection is 0.001 %. However,
if the position errors exceed 0.048 arcseconds, the remaining foregrounds would
obscure the target signal.; 3) The Open Relativistic Two-body Problem; The open relativistic two-body problem, when two interacting particles also
are in external potentials, is considered in terms of the principle of the
least action. Based on the consistent modification of the relativistic version
Newton's third law in external fields, the exact covariant operator equations,
which govern dynamics of either a scalar particle or each of the components of
the 16 component spinor of spin-$1/2$ fermions, depending on the particle type,
are derived in the center-mass and relative motion variables, beyond the
consideration in the Breit frame only. The class of external fields and
interaction potentials, when the two-body problem can be covariantly
reformulated in (3+1) phase space of relative motion variables, uncoupled from
the center-mass motion of such a system, is outlined. In the case of fermions
the new $\gamma$-matrix basis generating the Dirac-like equation for the 16
component spinors in the (3+1) phase space is found. The chosen basis allows us
to decouple the derived Dirac-like equation into four independent equations
governing the dynamics of the four-component spinors. The developed approach is
examined in studying the low-energy positronium states in magnetic fields.; 4) Bridging high resolution sub-cellular imaging with physiologically
  relevant engineered tissues; While high-resolution microscopic techniques are crucial for studying
cellular structures in cell biology, obtaining such images from thick 3D
engineered tissues remains challenging. In this review, we explore advancements
in fluorescence microscopy, alongside the use of various fluorescent probes and
material processing techniques to address these challenges. We navigate through
the diverse array of imaging options available in tissue engineering field,
from wide field to super-resolution microscopy, so researchers can make more
informed decisions based on the specific tissue and cellular structures of
interest. Finally, we provide some recent examples of how traditional
limitations on obtaining high-resolution images on sub-cellular architecture
within 3D tissues have been overcome by combining imaging advancements with
innovative tissue engineering approaches.; 5) Non-equilibrium Dynamics of Fermi Polarons Driven by Time-dependent
  Interaction; We systematically studied the dynamics of Fermi polarons driven by
time-dependent scattering length using a time-dependent variational method.
Starting from the non-interacting initial state, we calculated the evolution
behavior of the contact, energy, and quasiparticle residue of this system as
the scattering length $a_s(t)$ increases from zero. In the short-time
evolution, we obtained analytical results, verifying that when $a_s(t)$ grows
as $\sqrt{t}$, the contact $C(t)$ and energy $E(t)$ exhibit the maximum growth
rate. Furthermore, we numerically solved the long-time evolution, when $a_s(t)$
is diven with a strength $\beta$ and a power $\alpha$ as $a_s(t) =
{\sqrt{2}\beta}/{k_F} {(\epsilon_Ft)}^{\alpha}$. For large driving strength,
due to the interference between the polaron states and the non-interacting
states, $C(t)$ and $Z(t)$ exhibit oscillatory behavior; For small $\beta$, the
oscillatory behavior disappears due to the decay of the repulsive polaron into
the continuum, and $C(t)$ and $Z(t)$ gradually relax to zero. The energy always
saturates over long times for different driving strength. Additionally, our
method is applicable to any time-dependent form of the scattering length.; 6) Random Bridges in Spaces of Growing Dimension; We investigate the limiting behaviour of the path of random bridges treated
as random sets in $\mathbb{R}^{d}$ with the Euclidean metric and the dimension
$d$ increasing to infinity. The main result states that. in the square
integrable case, the limit (in the Gromov-Hausdorff sense) is deterministic,
namely, it is $[0,1]$ equipped with the pseudo-metric $\sqrt{|t-s|(1-|t-s|)}$.
We also show that, in the heavy-tailed case with summands regularly varying of
order $\alpha \in (0,1)$, the limiting metric space has a random metric derived
from the bridge variant of a subordinator.; 7) From No to Know: Taxonomy, Challenges, and Opportunities for Negation
  Understanding in Multimodal Foundation Models; Negation, a linguistic construct conveying absence, denial, or contradiction,
poses significant challenges for multilingual multimodal foundation models.
These models excel in tasks like machine translation, text-guided generation,
image captioning, audio interactions, and video processing but often struggle
to accurately interpret negation across diverse languages and cultural
contexts. In this perspective paper, we propose a comprehensive taxonomy of
negation constructs, illustrating how structural, semantic, and cultural
factors influence multimodal foundation models. We present open research
questions and highlight key challenges, emphasizing the importance of
addressing these issues to achieve robust negation handling. Finally, we
advocate for specialized benchmarks, language-specific tokenization,
fine-grained attention mechanisms, and advanced multimodal architectures. These
strategies can foster more adaptable and semantically precise multimodal
foundation models, better equipped to navigate and accurately interpret the
complexities of negation in multilingual, multimodal environments.; 8) Quasiparticle Fermi surfaces of niobium and niobium-titanium alloys at
  high pressure; The electronic structure of pure niobium and the niobium-titanium alloy
Nb$_{0.44}$Ti$_{0.56}$ in the bcc-phase at pressures up to $250$ GPa is
investigated, to reveal possible factors conducing toward the robust
superconductivity reported for Ti-doped niobium upon a considerable volume
reduction. We model the structural disorder using the coherent potential
approximation, and the electronic correlations are taken into account using
dynamical mean-field theory. At high pressure, a significant change in the
topology of the Fermi surface is observed, while electronic correlations weaken
with increasing pressure. Thus, the normal state of Nb$_{0.44}$Ti$_{0.56}$ is
found to be a Fermi liquid with a well-defined Fermi surface, and well-defined
quasiparticles near it. The systematic study of the impact of disorder upon the
Fermi surface at such ultra high pressures allows notable insights into the
nature of the electronic states near the Fermi level, i.e., within the energy
scale relevant for superconducting pairing. Furthermore, our results clearly
indicate the necessity of further experimental Fermi surface explorations.; 9) Unsupervised Anomaly Detection through Mass Repulsing Optimal Transport; Detecting anomalies in datasets is a longstanding problem in machine
learning. In this context, anomalies are defined as a sample that significantly
deviates from the remaining data. Meanwhile, optimal transport (OT) is a field
of mathematics concerned with the transportation, between two probability
measures, at least effort. In classical OT, the optimal transportation strategy
of a measure to itself is the identity. In this paper, we tackle anomaly
detection by forcing samples to displace its mass, while keeping the least
effort objective. We call this new transportation problem Mass Repulsing
Optimal Transport (MROT). Naturally, samples lying in low density regions of
space will be forced to displace mass very far, incurring a higher
transportation cost. We use these concepts to design a new anomaly score.
Through a series of experiments in existing benchmarks, and fault detection
problems, we show that our algorithm improves over existing methods.; 10) Counterfactual experience augmented off-policy reinforcement learning; Reinforcement learning control algorithms face significant challenges due to
out-of-distribution and inefficient exploration problems. While model-based
reinforcement learning enhances the agent's reasoning and planning capabilities
by constructing virtual environments, training such virtual environments can be
very complex. In order to build an efficient inference model and enhance the
representativeness of learning data, we propose the Counterfactual Experience
Augmentation (CEA) algorithm. CEA leverages variational autoencoders to model
the dynamic patterns of state transitions and introduces randomness to model
non-stationarity. This approach focuses on expanding the learning data in the
experience pool through counterfactual inference and performs exceptionally
well in environments that follow the bisimulation assumption. Environments with
bisimulation properties are usually represented by discrete observation and
action spaces, we propose a sampling method based on maximum kernel density
estimation entropy to extend CEA to various environments. By providing reward
signals for counterfactual state transitions based on real information, CEA
constructs a complete counterfactual experience to alleviate the
out-of-distribution problem of the learning data, and outperforms general SOTA
algorithms in environments with difference properties. Finally, we discuss the
similarities, differences and properties of generated counterfactual
experiences and real experiences. The code is available at
https://github.com/Aegis1863/CEA.; 11) Stochastic Geometry for Modeling and Analysis of Sensing and
  Communications: A Survey; One of the most promising technologies for next-generation wireless networks
is integrated communication and sensing (ISAC). It is considered a key enabler
for applications that require both enhanced communication and accurate sensing
capabilities. Examples of such applications include smart environments,
augmented and virtual reality, or the internet of things, where the
capabilities of intelligent sensing and broadband communications are vital.
Therefore, ISAC has attracted the research interest of both academia and
industry, and many investigations have been carried out over the past decade.
The articles in the literature include system models, performance evaluation,
and optimization studies of several ISAC alternative designs. Stochastic
geometry is the study and analysis of random spatial patterns, and as such,
stochastic geometry tools have been considered for the performance evaluation
of wireless networks with different types of nodes. In this paper, we aim to
provide a comprehensive survey of current research progress in performance
evaluation of ISAC systems using stochastic geometry tools. The survey covers
terrestrial, aerial, and vehicular networks, where the random spatial location
of the corresponding network elements and propagation scatterers and/or
blockages is treated with various point processes. The paper starts with a
short tutorial on ISAC technology, stochastic geometry tools, and metrics used
in performance evaluation of communication and sensing. Then, the technical
components of the system models utilized in the surveyed papers are discussed.
Subsequently, we present the key results of the literature in all types of
networks using three levels of integration: sensing-assisted communication,
communication-assisted sensing, and joint sensing and communication. Finally,
future research challenges and promising directions are discussed.; 12) On the Picard numbers of moduli spaces of one-dimensional sheaves on
  surfaces; Motivated by asymptotic phenomena of moduli spaces of higher rank stable
sheaves on algebraic surfaces, we study the Picard number of the moduli space
of one-dimensional stable sheaves supported in a sufficiently positive divisor
class on a surface. We give an asymptotic lower bound of the Picard number in
general. In some special cases, we show that this lower bound is attained based
on the geometry of moduli spaces of stable pairs and relative Hilbert schemes
of points. Additionally, we discuss several related questions and provide
examples where the asymptotic irreducibility of the moduli space fails,
highlighting a notable distinction from the higher rank case.; 13) Low-Complexity Cram\'er-Rao Lower Bound and Sum Rate Optimization in
  ISAC Systems; While Cram\'er-Rao lower bound is an important metric in sensing functions in
integrated sensing and communications (ISAC) designs, its optimization usually
involves a computationally expensive solution such as semidefinite relaxation.
In this paper, we aim to develop a low-complexity yet efficient algorithm for
CRLB optimization. We focus on a beamforming design that maximizes the weighted
sum between the communications sum rate and the sensing CRLB, subject to a
transmit power constraint. Given the non-convexity of this problem, we propose
a novel method that combines successive convex approximation (SCA) with a
shifted generalized power iteration (SGPI) approach, termed SCA-SGPI. The SCA
technique is utilized to approximate the non-convex objective function with
convex surrogates, while the SGPI efficiently solves the resulting quadratic
subproblems. Simulation results demonstrate that the proposed SCA-SGPI
algorithm not only achieves superior tradeoff performance compared to existing
method but also significantly reduces computational time, making it a promising
solution for practical ISAC applications.; 14) LimeSoDa: A Dataset Collection for Benchmarking of Machine Learning
  Regressors in Digital Soil Mapping; Digital soil mapping (DSM) relies on a broad pool of statistical methods, yet
determining the optimal method for a given context remains challenging and
contentious. Benchmarking studies on multiple datasets are needed to reveal
strengths and limitations of commonly used methods. Existing DSM studies
usually rely on a single dataset with restricted access, leading to incomplete
and potentially misleading conclusions. To address these issues, we introduce
an open-access dataset collection called Precision Liming Soil Datasets
(LimeSoDa). LimeSoDa consists of 31 field- and farm-scale datasets from various
countries. Each dataset has three target soil properties: (1) soil organic
matter or soil organic carbon, (2) clay content and (3) pH, alongside a set of
features. Features are dataset-specific and were obtained by optical
spectroscopy, proximal- and remote soil sensing. All datasets were aligned to a
tabular format and are ready-to-use for modeling. We demonstrated the use of
LimeSoDa for benchmarking by comparing the predictive performance of four
learning algorithms across all datasets. This comparison included multiple
linear regression (MLR), support vector regression (SVR), categorical boosting
(CatBoost) and random forest (RF). The results showed that although no single
algorithm was universally superior, certain algorithms performed better in
specific contexts. MLR and SVR performed better on high-dimensional spectral
datasets, likely due to better compatibility with principal components. In
contrast, CatBoost and RF exhibited considerably better performances when
applied to datasets with a moderate number (< 20) of features. These
benchmarking results illustrate that the performance of a method is highly
context-dependent. LimeSoDa therefore provides an important resource for
improving the development and evaluation of statistical methods in DSM.; 15) PSO-Net: Development of an automated psoriasis assessment system using
  attention-based interpretable deep neural networks; Psoriasis is a chronic skin condition that requires long-term treatment and
monitoring. Although, the Psoriasis Area and Severity Index (PASI) is utilized
as a standard measurement to assess psoriasis severity in clinical trials, it
has many drawbacks such as (1) patient burden for in-person clinic visits for
assessment of psoriasis, (2) time required for investigator scoring and (3)
variability of inter- and intra-rater scoring. To address these drawbacks, we
propose a novel and interpretable deep learning architecture called PSO-Net,
which maps digital images from different anatomical regions to derive
attention-based scores. Regional scores are further combined to estimate an
absolute PASI score. Moreover, we devise a novel regression activation map for
interpretability through ranking attention scores. Using this approach, we
achieved inter-class correlation scores of 82.2% [95% CI: 77- 87%] and 87.8%
[95% CI: 84-91%] with two different clinician raters, respectively.; 16) Fluctuation-dissipation theorems for multi-phase flow with memory in
  porous media; Recent works have reported on the collective behavior of multiphase systems
under fractional flow. Such behavior has been linked to pressure and/or flux
fluctuations under stationary flow conditions that occur over a broad range of
resonance frequencies and associated relaxation times. However, there currently
exists no theoretical development to deal with such phenomena. The aim of this
paper is to develop a fundamental theory that can describe such behavior.
Fluctuation-dissipation theorems for the case with memory are formulated,
providing a new route to obtain frequency-dependent porous media permeability.
  We propose that multiphase flow systems can be explained by a multipeak
Lorentzian memory function and provide supporting experimental data from the
flow of decane and water in a porous medium made of glass beads. Our
fluctuation dissipation theorems provide information on different types of
relaxation phenomena and resonance frequencies that occur during fractional
flow. We show, using experimental data, that Green-Kubo-like expressions can be
formulated for two-phase fluid flow driven by a constant pressure drop. The
resulting autocorrelation functions, or rather their Fourier transforms,
exhibit multiple Lorentzian peak shapes. Resonances are similar to those of
electric conductance. The analysis offers a new route to steady-state relative
permeability measurements, including information on the relaxation times and
resonance regimes that exist during fractional flow. Overall, the theory
presented and supported by fractional flow experiments provides a rich set of
possible directions for future developments that could fundamentally change the
way multiphase flow systems are understood and studied.; 17) The ENUBET monitored neutrino beam and its implementation at CERN; The ENUBET project recently concluded the R&D for a site independent design
of a monitored neutrino beam for high precision cross section measurements, in
which the neutrino flux is inferred from the measurement of charged leptons in
an instrumented decay tunnel. In this phase three fundamental results were
obtained and will be discussed here: 1) a beamline not requiring a horn and
relying on static focusing elements allows to perform a $\nu_e$ cross section
measurement in the DUNE energy range with 1% statistical uncertainty employing
$10^{20}$ 400 GeV protons on target (pot) and a neutrino detector of the size
of ProtoDUNE; 2) the instrumentation of the decay tunnel, based on a cost
effective sampling calorimeter solution, has been tested with a large scale
prototype achieving the performance required to identify positrons and muons
from kaon decays with high signal-to-noise ratio; 3) the systematics budget on
the neutrino flux is constrained at the 1% level by fitting the charged leptons
observables measured in the decay tunnel. Based on these successful results
ENUBET is now pursuing a study for a site dependent implementation at CERN in
the framework of Physics Beyond Colliders. In this context a new beamline, able
to enrich the neutrino flux at the energy of HK and to reduce by more than a
factor 3 the needed pot, has been designed and is being optimized. The civil
engineering and radioprotection studies for the siting of ENUBET in the North
Area towards the two ProtoDUNEs are also in the scope of this work, with the
goal of proposing a neutrino cross section experiment in 2026. The combined use
of both the neutrino detectors and of the improved beamline would allow to
perform cross section measurements with unprecedented precision in about 5
years with a proton request compatible with the needs of other users after CERN
Long Shutdown 3.; 18) Hallucination Mitigation using Agentic AI Natural Language-Based
  Frameworks; Hallucinations remain a significant challenge in current Generative AI
models, undermining trust in AI systems and their reliability. This study
investigates how orchestrating multiple specialized Artificial Intelligent
Agents can help mitigate such hallucinations, with a focus on systems
leveraging Natural Language Processing (NLP) to facilitate seamless agent
interactions. To achieve this, we design a pipeline that introduces over three
hundred prompts, purposefully crafted to induce hallucinations, into a
front-end agent. The outputs are then systematically reviewed and refined by
second- and third-level agents, each employing distinct large language models
and tailored strategies to detect unverified claims, incorporate explicit
disclaimers, and clarify speculative content. Additionally, we introduce a set
of novel Key Performance Indicators (KPIs) specifically designed to evaluate
hallucination score levels. A dedicated fourth-level AI agent is employed to
evaluate these KPIs, providing detailed assessments and ensuring accurate
quantification of shifts in hallucination-related behaviors. A core component
of this investigation is the use of the OVON (Open Voice Network) framework,
which relies on universal NLP-based interfaces to transfer contextual
information among agents. Through structured JSON messages, each agent
communicates its assessment of the hallucination likelihood and the reasons
underlying questionable content, thereby enabling the subsequent stage to
refine the text without losing context. The results demonstrate that employing
multiple specialized agents capable of interoperating with each other through
NLP-based agentic frameworks can yield promising outcomes in hallucination
mitigation, ultimately bolstering trust within the AI community.; 19) Barbara Rokowska's combinatorial research with her extensive biography
  (1926--2012); We discuss the significance of some interesting results by Barbara Rokowska
about combinatorial constructions. Her interest in finite mathematics and
number theory began with an embellishment and detailing of some work by Erdos.
Rokowska and Schinzel then solved the problem posed by Paul Erdos concerning
the existence of prime numbers of a certain kind. Her subsequent work
highlighted the difficulty in constructing Steiner systems with certain
properties and showed the importance of rigorous proof techniques in this area
of mathematics. This is the first such summary of the main results obtained by
Rokowska, her collaborators and PhD students.
  A biography of Barbara Rokowska has been added as an appendix.; 20) GLAM: Global-Local Variation Awareness in Mamba-based World Model; Mimicking the real interaction trajectory in the inference of the world model
has been shown to improve the sample efficiency of model-based reinforcement
learning (MBRL) algorithms. Many methods directly use known state sequences for
reasoning. However, this approach fails to enhance the quality of reasoning by
capturing the subtle variation between states. Much like how humans infer
trends in event development from this variation, in this work, we introduce
Global-Local variation Awareness Mamba-based world model (GLAM) that improves
reasoning quality by perceiving and predicting variation between states. GLAM
comprises two Mambabased parallel reasoning modules, GMamba and LMamba, which
focus on perceiving variation from global and local perspectives, respectively,
during the reasoning process. GMamba focuses on identifying patterns of
variation between states in the input sequence and leverages these patterns to
enhance the prediction of future state variation. LMamba emphasizes reasoning
about unknown information, such as rewards, termination signals, and visual
representations, by perceiving variation in adjacent states. By integrating the
strengths of the two modules, GLAM accounts for highervalue variation in
environmental changes, providing the agent with more efficient
imagination-based training. We demonstrate that our method outperforms existing
methods in normalized human scores on the Atari 100k benchmark.; 21) Collider Prospects for the Neutrino Magnetic Moment Portal; The transition magnetic moment between active and sterile neutrinos is
theoretically well-motivated scenario beyond the Standard Model, which can be
probed in cosmology, astrophysics, and at terrestrial experiments. In this
work, we focus on the latter by examining such an interaction at proposed
lepton colliders. Specifically, in addition to revisiting LEP, we consider
CEPC, FCC-ee, CLIC, and the muon collider, motivated by the potential
realization of any of them. Within the effective field theory framework, we
present parameter regions that can be probed, highlighting the dependence on
the lepton flavor interacting with the sterile neutrino. By including several
new processes with large sterile neutrino production cross sections at
high-energy lepton colliders, we find that the expected sensitivity for the
active-to-sterile neutrino transition magnetic moment can reach $d_\gamma
\simeq \mathcal{O}(10^{-7})$ GeV$^{-1}$.; 22) Solutions of systems of certain Fermat-type PDDEs; The objective of this paper is to investigate the existence and the forms of
the pair of finite order entire and meromorphic solutions of some certain
systems of Fermat-type partial differential-difference equations of several
complex variables. These results represent some refinements and generalizations
of the earlier findings, especially the results due to Xu {\it et al.} (J.
Math. Anal. Appl. 483(2) (2020)). We provide some examples to support the
results.; 23) Learning with Exact Invariances in Polynomial Time; We study the statistical-computational trade-offs for learning with exact
invariances (or symmetries) using kernel regression. Traditional methods, such
as data augmentation, group averaging, canonicalization, and frame-averaging,
either fail to provide a polynomial-time solution or are not applicable in the
kernel setting. However, with oracle access to the geometric properties of the
input space, we propose a polynomial-time algorithm that learns a classifier
with \emph{exact} invariances. Moreover, our approach achieves the same excess
population risk (or generalization error) as the original kernel regression
problem. To the best of our knowledge, this is the first polynomial-time
algorithm to achieve exact (not approximate) invariances in this context. Our
proof leverages tools from differential geometry, spectral theory, and
optimization. A key result in our development is a new reformulation of the
problem of learning under invariances as optimizing an infinite number of
linearly constrained convex quadratic programs, which may be of independent
interest.; 24) Boundary Modes in String Field Theory; We discuss the construction of boundary contributions to free string field
theory actions in the context of the bosonic string. We show that it is
generally possible to obtain a well-defined variational principle by adding a
simple boundary term (which only depends on the value of the bulk fields at the
boundary) to the original bulk action. However, it is in general not possible
to do this in a gauge-invariant way unless suitable boundary degrees of freedom
are added. We explicitly construct such boundary contributions for the massless
level of both the open and the closed SFT, as well as for the tensionless limit
of the full string field theory. Our results reproduce linearized general
relativity with the Gibbons-Hawking-York term and provide similar
gauge-invariant actions for the infinite tower of massless higher-spin gauge
theories for all Regge trajectories. By writing down a gauge-invariant action
for the first massive level of the open string, we provide evidence that an
analogous construction should be possible for the full tensile string field
theory.; 25) Leveraging Dual Process Theory in Language Agent Framework for Real-time
  Simultaneous Human-AI Collaboration; Agents built on large language models (LLMs) have excelled in turn-by-turn
human-AI collaboration but struggle with simultaneous tasks requiring real-time
interaction. Latency issues and the challenge of inferring variable human
strategies hinder their ability to make autonomous decisions without explicit
instructions. Through experiments with current independent System 1 and System
2 methods, we validate the necessity of using Dual Process Theory (DPT) in
real-time tasks. We propose DPT-Agent, a novel language agent framework that
integrates System 1 and System 2 for efficient real-time simultaneous human-AI
collaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and
code-as-policy for fast, intuitive, and controllable decision-making.
DPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous
reflection to infer human intentions and perform reasoning-based autonomous
decisions. We demonstrate the effectiveness of DPT-Agent through further
experiments with rule-based agents and human collaborators, showing significant
improvements over mainstream LLM-based frameworks. DPT-Agent can effectively
help LLMs convert correct slow thinking and reasoning into executable actions,
thereby improving performance. To the best of our knowledge, DPT-Agent is the
first language agent framework that achieves successful real-time simultaneous
human-AI collaboration autonomously. Code of DPT-Agent can be found in
https://github.com/sjtu-marl/DPT-Agent.; 26) On the quantum cohomology of blow-ups of four-dimensional quadrics; We propose a conjecture relevant to Galkin's lower bound conjecture, and
verify it for the blow-ups of a four-dimensional quadric at a point or along a
projective plane. We also show that Conjecture $\mathcal{O}$ holds in these two
cases.; 27) Fourth-Moment Theorems for Sums of Multiple Integrals; Nualart & Pecatti ([Nualart and Peccati, 2005, Thm 1]) established the first
fourth-moment theorem for random variables in a fixed Wiener chaos, i.e. they
showed that convergence of the sequence of fourth moments to the fourth moment
of the standard Gaussian distribution is sufficient for weak convergence to the
standard Gaussian. In this paper, we provide what we believe to be the first
generalization to chaos expansions with more than a single term. Specifically,
we show that a fourth-moment theorem holds for random variables consisting of
sums of two multiple integrals of orders $p, q \in N$, where $p, q$ have
different parities. Furthermore, we show that such random variables cannot
themselves be Gaussian, again generalizing what is known for the fixed Wiener
chaos setting. Finally, we show a fourth-moment theorem for variables with
infinite Wiener chaos expansions when the terms in the expansions are
independent and satisfy an additional regularity condition in terms of the
Ornstein-Uhlenbeck operator.; 28) Mechanistic Interpretability of Emotion Inference in Large Language
  Models; Large language models (LLMs) show promising capabilities in predicting human
emotions from text. However, the mechanisms through which these models process
emotional stimuli remain largely unexplored. Our study addresses this gap by
investigating how autoregressive LLMs infer emotions, showing that emotion
representations are functionally localized to specific regions in the model.
Our evaluation includes diverse model families and sizes and is supported by
robustness checks. We then show that the identified representations are
psychologically plausible by drawing on cognitive appraisal theory, a
well-established psychological framework positing that emotions emerge from
evaluations (appraisals) of environmental stimuli. By causally intervening on
construed appraisal concepts, we steer the generation and show that the outputs
align with theoretical and intuitive expectations. This work highlights a novel
way to causally intervene and precisely shape emotional text generation,
potentially benefiting safety and alignment in sensitive affective domains.; 29) FeynGame 3.0; A major update of the program FeynGame is introduced. One of its main new
functionalities is to visualize Feynman graphs generated by QGRAF. The QGRAF
output can be either pasted into the FeynGame canvas for individual graphs, or
the whole QGRAF output file can be processed. In addition, a number of new
features and improvements have been implemented into FeynGame-3.0 in order to
further facilitate the efficient drawing of Feynman diagrams in publication
quality. FeynGame is freely available as jar or MacOS app file from
https://web.physik.rwth-aachen.de/user/harlander/software/feyngame, and as
source code from https://gitlab.com/feyngame/FeynGame.; 30) Precision measurements of muonium and muonic helium hyperfine structure
  at J-PARC; At the J-PARC Muon Science Facility (MUSE), the MuSEUM collaboration is now
performing new precision measurements of the ground state hyperfine structure
(HFS) of both muonium and muonic helium atoms. High-precision measurements of
the muonium ground-state HFS are recognized as one of the most sensitive tools
for testing bound-state quantum electrodynamics theory to precisely probe the
standard model and determine fundamental constants of the positive muon
magnetic moment and mass. The same technique can also be employed to measure
muonic helium HFS, obtain the negative muon magnetic moment and mass, and test
and improve the theory of the three-body atomic system. Measurements at zero
magnetic field have already yielded more accurate results than previous
experiments for both muonium and muonic helium atoms. High-field measurements
are now ready to start collecting data using the world's most intense pulsed
muon beam at the MUSE H-line. We aim to improve the precision of previous
measurements ten times for muonium and a hundred times or more for muonic
helium. We review all the key developments for these new measurements, focusing
on the high-field experiment, and report the latest results and prospects.; 31) Mass conservation, positivity and energy identical-relation preserving
  scheme for the Navier-Stokes equations with variable density; In this paper, we consider a mass conservation, positivity and energy
identical-relation preserving scheme for the Navier-Stokes equations with
variable density. Utilizing the square transformation, we first ensure the
positivity of the numerical fluid density, which is form-invariant and
regardless of the discrete scheme. Then, by proposing a new recovery technique
to eliminate the numerical dissipation of the energy and to balance the loss of
the mass when approximating the reformation form, we preserve the original
energy identical-relation and mass conservation of the proposed scheme. To the
best of our knowledge, this is the first work that can preserve the original
energy identical-relation for the Navier-Stokes equations with variable
density. Moreover, the error estimates of the considered scheme are derived.
Finally, we show some numerical examples to verify the correctness and
efficiency.; 32) Coalition Formation for Heterogeneous Federated Learning Enabled Channel
  Estimation in RIS-assisted Cell-free MIMO; Downlink channel estimation remains a significant bottleneck in
reconfigurable intelligent surface-assisted cell-free multiple-input
multiple-output communication systems. Conventional approaches primarily rely
on centralized deep learning methods to estimate the high-dimensional and
complex cascaded channels. These methods require data aggregation from all
users for centralized model training, leading to excessive communication
overhead and significant data privacy concerns. Additionally, the large size of
local learning models imposes heavy computational demands on end users,
necessitating strong computational capabilities that most commercial devices
lack. To address the aforementioned challenges, a coalition-formation-guided
heterogeneous federated learning (FL) framework is proposed. This framework
leverages coalition formation to guide the formation of heterogeneous FL user
groups for efficient channel estimation. Specifically, by utilizing a
distributed deep reinforcement learning (DRL) approach, each FL user
intelligently and independently decides whether to join or leave a coalition,
aiming at improving channel estimation accuracy, while reducing local model
size and computational costs for end users. Moreover, to accelerate the DRL-FL
convergence process and reduce computational burdens on end users, a transfer
learning method is introduced. This method incorporates both received reference
signal power and distance similarity metrics, by considering that nodes with
similar distances to the base station and comparable received signal power have
a strong likelihood of experiencing similar channel fading. Massive experiments
performed that reveal that, compared with the benchmarks, the proposed
framework significantly reduces the computational overhead of end users by 16%,
improves data privacy, and improves channel estimation accuracy by 20%.; 33) Threshold for the existence of scattering states for nonlinear
  Schr\""odinger equations without gauge invariance; This paper is concerned with a threshold phenomenon for the existence of
scattering states for nonlinear Schr\""odinger equations. The nonlinearity
includes a non-oscillatory term of the order lower than the Strauss exponent.
We show that no scattering states exist for the equation in a weighted Sobolev
space. It is emphasized that our method admits initial data with good
properties, such as compactly supported smooth functions. The result indicates
that the Strauss exponent acts as a threshold for the power of the nonlinearity
that determines whether solutions scatter or not in the weighted space.; 34) Low-Rank Adapting Models for Sparse Autoencoders; Sparse autoencoders (SAEs) decompose language model representations into a
sparse set of linear latent vectors. Recent works have improved SAEs using
language model gradients, but these techniques require many expensive backward
passes during training and still cause a significant increase in cross entropy
loss when SAE reconstructions are inserted into the model. In this work, we
improve on these limitations by taking a fundamentally different approach: we
use low-rank adaptation (LoRA) to finetune the language model itself around a
previously trained SAE. We analyze our method across SAE sparsity, SAE width,
language model size, LoRA rank, and model layer on the Gemma Scope family of
SAEs. In these settings, our method reduces the cross entropy loss gap by 30%
to 55% when SAEs are inserted during the forward pass. We also find that
compared to end-to-end (e2e) SAEs, our approach achieves the same downstream
cross entropy loss 3$\times$ to 20$\times$ faster on Gemma-2-2B and 2$\times$
to 10$\times$ faster on Llama-3.2-1B. We further show that our technique
improves downstream metrics and can adapt multiple SAEs at once. Our results
demonstrate that improving model interpretability is not limited to post-hoc
SAE training; Pareto improvements can also be achieved by directly optimizing
the model itself.; 35) Pitch Plane Trajectory Tracking Control for Sounding Rockets via
  Adaptive Feedback Linearization; This paper proposes a pitch plane trajectory tacking control solution for
suborbital launch vehicles relying on adaptive feedback linearization.
Initially, the 2D dynamics and kinematics for a single-engine,
thrust-vector-controlled sounding rocket are obtained for control design
purposes. Then, an inner-outer control strategy, which simultaneously tackles
attitude and position control, is adopted, with the inner-loop comprising the
altitude and pitch control and the outer-loop addressing the horizontal
(downrange) position control. Feedback linearization is used to cancel out the
non-linearities in both the inner and outer dynamics. Making use of Lyapunov
stability theory, an adaptation law, which provides online estimates on the
inner-loop aerodynamic uncertainty, is jointly designed with the output
tracking controller via adaptive backstepping, ensuring global reference
tracking in the region where the feedback linearization is well-defined. The
zero dynamics of the inner-stabilized system are then exploited to obtain the
outerloop dynamics and derive a Linear Quadratic Regulator (LQR) with integral
action, which can stabilize them as well as reject external disturbances. In
the outermost loop, the estimate on the correspondent aerodynamic uncertainty
is indirectly obtained by using the inner loop estimates together with known
aerodynamics relations. The resulting inner-outer position control solution is
proven to be asymptotically stable in the region of interest. Using a
single-stage sounding rocket, propelled by a liquid engine, as reference
vehicle, different mission scenarios are tested in a simulation environment to
verify the adaptability of the proposed control strategy. The system is able to
track the requested trajectories while rejecting external wind disturbances.
Furthermore, the need to re-tune the control gains in between different mission
scenarios is minimal to none.; 36) Exponentiation of Graphs; Motivated by very large-scale communication networks, we newly introduce
exponentiation of graphs. Using the exponential operation on graphs, we can
construct various graphs of multi-exponential order with logarithmic diameter.
We show that every connected exponential graph is maximally connected. For
exponential graphs, we also present a necessary and sufficient condition to be
super edge-connected and sufficient conditions to be Hamiltonian and to have
edge-disjoint Hamiltonian cycles and completely independent spanning trees.
Applying our results to previously known networks, we have maximally connected
and super edge-connected Hamiltonian graphs of doubly exponential order with
logarithmic diameter. We furthermore define iterated exponential graphs which
may be of not only practical but also theoretical interest.; 37) Rumor Detection by Multi-task Suffix Learning based on Time-series Dual
  Sentiments; The widespread dissemination of rumors on social media has a significant
impact on people's lives, potentially leading to public panic and fear. Rumors
often evoke specific sentiments, resonating with readers and prompting sharing.
To effectively detect and track rumors, it is essential to observe the
fine-grained sentiments of both source and response message pairs as the rumor
evolves over time. However, current rumor detection methods fail to account for
this aspect. In this paper, we propose MSuf, the first multi-task suffix
learning framework for rumor detection and tracking using time series dual
(coupled) sentiments. MSuf includes three modules: (1) an LLM to extract
sentiment intensity features and sort them chronologically; (2) a module that
fuses the sorted sentiment features with their source text word embeddings to
obtain an aligned embedding; (3) two hard prompts are combined with the aligned
vector to perform rumor detection and sentiment analysis using one frozen LLM.
MSuf effectively enhances the performance of LLMs for rumor detection with only
minimal parameter fine-tuning. Evaluating MSuf on four rumor detection
benchmarks, we find significant improvements compared to other emotion-based
methods.; 38) Modern Bayesian Sampling Methods for Cosmological Inference: A
  Comparative Study; We present a comprehensive comparison of different Markov Chain Monte Carlo
(MCMC) sampling methods, evaluating their performance on both standard test
problems and cosmological parameter estimation. Our analysis includes
traditional Metropolis-Hastings MCMC, Hamiltonian Monte Carlo (HMC), slice
sampling, nested sampling as implemented in dynesty, and PolyChord. We examine
samplers through multiple metrics including runtime, memory usage, effective
sample size, and parameter accuracy, testing their scaling with dimension and
response to different probability distributions. While all samplers perform
well with simple Gaussian distributions, we find that HMC and nested sampling
show advantages for more complex distributions typical of cosmological
problems. Traditional MCMC and slice sampling become less efficient in higher
dimensions, while nested methods maintain accuracy but at higher computational
cost. In cosmological applications using BAO data, we observe similar patterns,
with particular challenges arising from parameter degeneracies and poorly
constrained parameters.; 39) Transportation Network Analysis, Volume I: Static and Dynamic Traffic
  Assignment; This book covers static and dynamic traffic assignment models used in
transportation planning and network analysis. Traffic assignment is the final
step in the traditional planning process, and recent decades have seen many
advances in formulating and solving such models. The book discusses classical
solution methods alongside recent ones used in contemporary planning software.
  The primary audience for the book is graduate students new to transportation
network analysis, and to this end there are appendices providing general
mathematical background, and more specific background in formulating
optimization problems. We have also included appendices discussing more general
optimization applications outside of traffic assignment. We believe the book is
also of interest to practitioners seeking to understand recent advances in
network analysis, and to researchers wanting a unified reference for traffic
assignment content.
  A second volume is currently under preparation, and will cover transit,
freight, and logistics models in transportation networks. A free PDF version of
the text will always be available online at
https://sboyles.github.io/blubook.html. We will periodically post updated
versions of the text at this link, along with slides and other instructor
resources.; 40) Three-dimensional imaging of biological cells using surface plasmon
  coupled emission; Biological cell imaging has become one of the most crucial research interests
due to its wide-ranging applications in biomedical and microbiology studies.
However, three-dimensional (3D) imaging of biological cells remains critically
challenging and often requires prohibitively expensive and complex equipment.
Therefore, a low-cost imaging technique with a simpler optical arrangement is
highly desirable. We propose an approach to obtain accurate 3D cell images
using surface plasmon coupled emission (SPCE) patterns from a fluorescently
labeled biological cell, eliminating the need for conventional microscopes or
extensive data processing. An imaging methodology has been developed and
theoretically demonstrated to reconstruct 3D cell structures from detected SPCE
patterns. The reconstructed 3D images closely match the actual cell geometries.
The technique has been applied to both regular and irregular cell shapes. In
each case, the root-mean-square error (RMSE) between the reconstructed images
and the actual structures remains within a few percent. For a circular-shaped
cell base, the RMSE is $\lesssim 1.4\%$, while for irregular cell bases, the
RMSE is $\lesssim 2.8\%$. Finally, a 3D image of a random cellular structure is
obtained with an RMSE of $\lesssim 6.5\%$. Despite being in its initial stages
of development, the proposed technique demonstrates promising results
considering its simplicity and low cost.; 41) No gauge cancellation at high energy in the five-vector $R_\xi$ gauge; We propose a novel $R_\xi$ gauge in the five-vector (5V) framework within the
Abelian Higgs model. In the Cartesian basis of the complex Higgs field, the 5V
$R_\xi$ gauge ensures non-divergent tree-level amplitudes for each Feynman
diagram in the high-energy limit. This framework pinpoints the origin of
high-energy divergences in tree-level amplitudes for each diagram, providing a
criterion for quantifying the degree of divergences from other gauges. The 5V
description necessitates treating the Goldstone field as the fifth gauge-field
component, offering deeper insight into the dynamics of massive gauge bosons,
particularly its longitudinal mode. The impact of this framework is
demonstrated by rigorously comparing tree-level amplitudes from the 5V $R_\xi$
gauge with those from the conventional 4V $R_\xi$ gauge and the Feynman diagram
gauge, the latter of which exhibits no gauge cancellation, similar to the 5V
$R_\xi$ gauge.; 42) Practical parameter identifiability of respiratory mechanics in the
  extremely preterm infant; The complexity of mathematical models describing respiratory mechanics has
grown in recent years, however, parameter identifiability of such models has
only been studied in the last decade in the context of observable data. This
study investigates parameter identifiability of a nonlinear respiratory
mechanics model tuned to the physiology of an extremely preterm infant, using
global Morris screening, local deterministic sensitivity analysis, and singular
value decomposition-based subset selection. The model predicts airflow and
dynamic pulmonary volumes and pressures under varying levels of continuous
positive airway pressure, and a range of parameters characterizing both
surfactant-treated and surfactant-deficient lung. Sensitivity analyses
indicated eleven parameters influence model outputs over the range of
continuous positive airway pressure and lung health scenarios. The model was
adapted to data from a spontaneously breathing 1 kg infant using gradient-based
optimization to estimate the parameter subset characterizing the patient's
state of health.; 43) Evaluating Amazon Effects and the Limited Impact of COVID-19 With
  Purchases Crowdsourced from US Consumers; We leverage a recently published dataset of Amazon purchase histories,
crowdsourced from thousands of US consumers, to study how online purchasing
behaviors have changed over time, how changes vary across demographic groups,
the impact of the COVID-19 pandemic, and relationships between online and
offline retail. This work provides a case study in how consumer-level purchases
data can reveal purchasing behaviors and trends beyond those available from
aggregate metrics. For example, in addition to analyzing spending behavior, we
develop new metrics to quantify changes in consumers' online purchase frequency
and the diversity of products purchased, to better reflect the growing ubiquity
and dominance of online retail. Between 2018 and 2022 these consumer-level
metrics grew on average by more than 85%, peaking in 2021. We find a steady
upward trend in individuals' online purchasing prior to COVID-19, with a
significant increase in the first year of COVID, but without a lasting effect.
Purchasing behaviors in 2022 were no greater than the result of the
pre-pandemic trend. We also find changes in purchasing significantly differ by
demographics, with different responses to the pandemic. We further use the
consumer-level data to show substitution effects between online and offline
retail in sectors where Amazon heavily invested: books, shoes, and grocery.
Prior to COVID we find year-to-year changes in the number of consumers making
online purchases for books and shoes negatively correlated with changes in
employment at local bookstores and shoe stores. During COVID we find online
grocery purchasing negatively correlated with in-store grocery visits. This
work demonstrates how crowdsourced, open purchases data can enable economic
insights that may otherwise only be available to private firms.; 44) From ChatGPT to DeepSeek: Can LLMs Simulate Humanity?; Simulation powered by Large Language Models (LLMs) has become a promising
method for exploring complex human social behaviors. However, the application
of LLMs in simulations presents significant challenges, particularly regarding
their capacity to accurately replicate the complexities of human behaviors and
societal dynamics, as evidenced by recent studies highlighting discrepancies
between simulated and real-world interactions. We rethink LLM-based simulations
by emphasizing both their limitations and the necessities for advancing LLM
simulations. By critically examining these challenges, we aim to offer
actionable insights and strategies for enhancing the applicability of LLM
simulations in human society in the future.; 45) ASVspoof 5: Design, Collection and Validation of Resources for Spoofing,
  Deepfake, and Adversarial Attack Detection Using Crowdsourced Speech; ASVspoof 5 is the fifth edition in a series of challenges which promote the
study of speech spoofing and deepfake attacks as well as the design of
detection solutions. We introduce the ASVspoof 5 database which is generated in
crowdsourced fashion from data collected in diverse acoustic conditions (cf.
studio-quality data for earlier ASVspoof databases) and from ~2,000 speakers
(cf. ~100 earlier). The database contains attacks generated with 32 different
algorithms, also crowdsourced, and optimised to varying degrees using new
surrogate detection models. Among them are attacks generated with a mix of
legacy and contemporary text-to-speech synthesis and voice conversion models,
in addition to adversarial attacks which are incorporated for the first time.
ASVspoof 5 protocols comprise seven speaker-disjoint partitions. They include
two distinct partitions for the training of different sets of attack models,
two more for the development and evaluation of surrogate detection models, and
then three additional partitions which comprise the ASVspoof 5 training,
development and evaluation sets. An auxiliary set of data collected from an
additional 30k speakers can also be used to train speaker encoders for the
implementation of attack algorithms. Also described herein is an experimental
validation of the new ASVspoof 5 database using a set of automatic speaker
verification and spoof/deepfake baseline detectors. With the exception of
protocols and tools for the generation of spoofed/deepfake speech, the
resources described in this paper, already used by participants of the ASVspoof
5 challenge in 2024, are now all freely available to the community.; 46) I Stan Alien Idols and Also the People Behind Them: Understanding How
  Seams Between Virtual and Real Identities Engage VTuber Fans -- A Case Study
  of PLAVE; Virtual YouTubers (VTubers) have recently gained popularity as streamers
using computer-generated avatars and real-time motion capture to create
distinct virtual identities. While prior research has explored how VTubers
construct virtual personas and engage audiences, little attention has been
given to viewers' reactions when virtual and real identities blur-what we refer
to as ""seams."" To address this gap, we conducted a case study on PLAVE, a
popular Korean VTuber Kpop idol group, interviewing 24 of their fans. Our
findings identified two main sources of seams: technical glitches and identity
collapses, where VTubers act inconsistently with their virtual personas,
revealing aspects of their real selves. These seams played a pivotal role in
shaping diverse fan engagements, with some valuing authenticity linked to real
identities, while others prioritized the coherence of virtual personas.
Overall, our findings underscore the importance of seams in shaping viewer
experiences.; 47) EMERALD: Evidence Management for Continuous Certification as a Service
  in the Cloud; The conspicuous lack of cloud-specific security certifications, in addition
to the existing market fragmentation, hinder transparency and accountability in
the provision and usage of European cloud services. Both issues ultimately
reflect on the level of customers' trustworthiness and adoption of cloud
services. The upcoming demand for continuous certification has not yet been
definitively addressed and it remains unclear how the level 'high' of the
European Cybersecurity Certification Scheme for Cloud Services (EUCS) shall be
technologically achieved. The introduction of AI in cloud services is raising
the complexity of certification even further. This paper presents the EMERALD
Certification-as-a-Service (CaaS) concept for continuous certification of
harmonized cybersecurity schemes, like the EUCS. EMERALD CaaS aims to provide
agile and lean re-certification to consumers that adhere to a defined level of
security and trust in a uniform way across heterogeneous environments
consisting of combinations of different resources (Cloud, Edge, IoT). Initial
findings suggest that EMERALD will significantly contribute to continuous
certification, boosting providers and users of cloud services to maintain
regulatory compliance towards the latest and upcoming security schemes.; 48) Advancing Problem-Based Learning in Biomedical Engineering in the Era of
  Generative AI; Problem-Based Learning (PBL) has significantly impacted biomedical
engineering (BME) education since its introduction in the early 2000s,
effectively enhancing critical thinking and real-world knowledge application
among students. With biomedical engineering rapidly converging with artificial
intelligence (AI), integrating effective AI education into established
curricula has become challenging yet increasingly necessary. Recent
advancements, including AI's recognition by the 2024 Nobel Prize, have
highlighted the importance of training students comprehensively in biomedical
AI. However, effective biomedical AI education faces substantial obstacles,
such as diverse student backgrounds, limited personalized mentoring,
constrained computational resources, and difficulties in safely scaling
hands-on practical experiments due to privacy and ethical concerns associated
with biomedical data. To overcome these issues, we conducted a three-year
(2021-2023) case study implementing an advanced PBL framework tailored
specifically for biomedical AI education, involving 92 undergraduate and 156
graduate students from the joint Biomedical Engineering program of Georgia
Institute of Technology and Emory University. Our approach emphasizes
collaborative, interdisciplinary problem-solving through authentic biomedical
AI challenges. The implementation led to measurable improvements in learning
outcomes, evidenced by high research productivity (16 student-authored
publications), consistently positive peer evaluations, and successful
development of innovative computational methods addressing real biomedical
challenges. Additionally, we examined the role of generative AI both as a
teaching subject and an educational support tool within the PBL framework. Our
study presents a practical and scalable roadmap for biomedical engineering
departments aiming to integrate robust AI education into their curricula.; 49) SIR on locally converging dynamic random graphs; In this paper, we study the trajectory of a classic SIR epidemic on a family
of dynamic random graphs of fixed size, whose set of edges continuously evolves
over time. We set general infection and recovery times, and start the epidemic
from a positive, yet small, proportion of vertices. We show that in such a
case, the spread of an infectious disease around a typical individual can be
approximated by the spread of the disease in a local neighbourhood of a
uniformly chosen vertex. We formalize this by studying general dynamic random
graphs that converge dynamically locally in probability and demonstrate that
the epidemic on these graphs converges to the epidemic on their dynamic local
limit graphs. We provide a detailed treatment of the theory of dynamic local
convergence, which remains a relatively new topic in the study of random
graphs. One main conclusion of our paper is that a specific form of dynamic
local convergence is required for our results to hold.; 50) Search for the FCNC charmonium decay $J/\psi \to D^0 \mu^+ \mu^- +
  \text{c.c.}$; Based on a data sample of $(10087 \pm 44) \times 10^6$ $J/\psi$ events taken
with the BESIII detector, we search for the flavor-changing neutral current
charmonium decay $J/\psi \to D^{0} \mu^{+} \mu^{-} + \text{c.c.}$. No
significant signal above the background is observed, and the upper limit on its
branching fraction is set to be $\mathcal{B}(J/\psi \to D^{0}\mu^{+}\mu^{-} +
\text{c.c.} ) < 1.1 \times 10^{-7}$ at the 90% confidence level. This marks the
first search for a flavor-changing neutral current charmonium decay involving
muons in the final state.; 51) Contextual Gradient Flow Modeling for Large Language Model
  Generalization in Multi-Scale Feature Spaces; Optimization methodologies for training large-scale neural architectures
often rely on uniform gradient propagation mechanisms that fail to align with
hierarchical linguistic structures, limiting their capacity to generalize
across diverse language distributions. A structured gradient refinement
framework was introduced to incorporate multi-scale contextual adjustments,
improving parameter adaptation through dynamic weighting strategies that
enhanced representation coherence. Empirical evaluations demonstrated that
structured propagation mechanisms contributed to reductions in gradient
oscillations, resulting in more stable training dynamics and improved
optimization efficiency. The comparative performance assessment indicated that
models incorporating hierarchical propagation strategies exhibited greater
robustness in long-range dependency retention and cross-domain adaptation. The
hierarchical adjustment of weight updates provided an alternative to
conventional backpropagation, reducing sensitivity to initialization conditions
while improving overall convergence efficiency. The experimental results
confirmed that structured gradient propagation influenced representation
learning trajectories, aligning parameter updates with broader linguistic
dependencies rather than isolated token-level relationships. Statistical
evaluations indicated that structured optimization strategies mitigated
overfitting while preserving adaptability across heterogeneous text
distributions. The findings established that structured gradient propagation
provided an empirically validated framework for refining hierarchical
representation learning, supporting more effective integration of linguistic
dependencies into optimization dynamics.; 52) Rational Gaussian wavelets and corresponding model driven neural
  networks; In this paper we consider the continuous wavelet transform using Gaussian
wavelets multiplied by an appropriate rational term. The zeros and poles of
this rational modifier act as free parameters and their choice highly
influences the shape of the mother wavelet. This allows the proposed
construction to approximate signals with complex morphology using only a few
wavelet coefficients. We show that the proposed rational Gaussian wavelets are
admissible and provide numerical approximations of the wavelet coefficients
using variable projection operators. In addition, we show how the proposed
variable projection based rational Gaussian wavelet transform can be used in
neural networks to obtain a highly interpretable feature learning layer. We
demonstrate the effectiveness of the proposed scheme through a biomedical
application, namely, the detection of ventricular ectopic beats (VEBs) in real
ECG measurements.; 53) Hydrodynamics in a villi-patterned channel due to pendular-wave activity; Inspired by the motility of small intestine, we investigate the flow
generated by a propagating pendular-wave along the walls of a symmetric 2D
channel lined with elongated villi-like microstructures. The rigid villi follow
simple harmonic axial motion, but with a phase lag to their neighbours,
resulting in travelling intervillous contractions. We use lattice Boltzmann
simulations to resolve the flow around the villi and the luminal channel space,
sampling small to moderate Womersley numbers. We analyse an emergent `mixing'
boundary layer in the lumen, separating two distinct flow regions. A mixing
region is formed above the villi, characterised by semi-vortical flow patterns,
which travel along with the wave. In the channel centre, unidirectional axial
flow emerges, flowing opposite to the imposed wave direction, even in the
Stokes flow regime. This behaviour contrasts with canonical wave driven
peristaltic flow (Jaffrin & Shapiro 1971). The fluid trapped in the
intervillous gaps is forced to take a non-reciprocal path due to the travelling
pendular-wave, which breaks time-symmetry in the Stokes regime. We propose an
effective velocity boundary condition, incorporating perturbations due to the
presence of the villi. In the inertial regime, the boundary layer shrinks due
to dynamic flow confinement arising from increased oscillatory flow inertia. We
identify the Stokes to inertial transition, and find phenomenological scaling
laws for the boundary layer decay and axial and radial fluid fluxes. Our
results not only advance the understanding of transport within the gut but also
suggest a novel way for microfluidic flow control in confined channels.; 54) Bohmian Mechanics fails to compute multi-time correlations; The violation of Bell type inequalities in quantum systems manifests that
quantum states cannot be described by classical probability distributions. Yet,
Bohmian mechanics is a realistic, non-local theory of classical particle
trajectories that is claimed to be indistinguishable by observations from more
traditional approaches to quantum mechanics. We set up a spatial version of the
GHZ system with qubits realised as positional observables that demonstrates
that the Bohmian theory fails to match predictions of textbook quantum
mechanics (and most likely experients) unless enlarged by a microscopic theory
of collapse of the wave function after observation. For this discrepancy to
occur it is essential that positions at different times do not commute.; 55) Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental
  differences between male and female heart shapes; Sex-based differences in cardiovascular disease are well documented, yet the
precise nature and extent of these discrepancies in cardiac anatomy remain
incompletely understood. Traditional scaling models often fail to capture the
interplay of age, blood pressure, and body size, prompting a more nuanced
investigation. Here, we employ statistical shape modeling in a healthy subset
(n=456) of the UK Biobank to explore sex-specific variations in biventricular
anatomy. We reconstruct 3D meshes and perform multivariate analyses of shape
coefficients, controlling for age, blood pressure, and various body size
metrics. Our findings reveal that sex alone explains at least 25 percent of
morphological variability, with strong discrimination between men and women
(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the
most discriminative modes highlight pronounced differences in cardiac chamber
volumes, the anterior-posterior width of the right ventricle, and the relative
positioning of the cardiac chambers. These results underscore that sex has a
fundamental influence on cardiac morphology, which may have important clinical
implications for differing cardiac structural assessments in men and women.
Future work should investigate how these anatomical differences manifest in
various cardiovascular conditions, ultimately paving the way for more precise
risk stratification and personalized therapeutic strategies for both men and
women.; 56) A numerical method for low Mach number compressible flows by
  simultaneous relaxation of dependent variables; Density varies spatiotemporally in low Mach number flows. Hence,
incompressibility cannot be assumed, and the density must be accurately solved.
Various methods have been proposed to analyze low Mach number flows, but their
energy conservation properties have not been investigated in detail. This study
proposes a new method for simultaneously relaxing velocity, pressure, density,
and internal energy using a conservative finite difference scheme with
excellent energy conservation properties to analyze low Mach number flows. In
the analysis for sound wave propagation in an inviscid compressible flow, the
amplitude amplification ratio and frequency of sound wave obtained by this
numerical method agree well with the theoretical values. In the analysis for a
three-dimensional periodic inviscid compressible flow, each total amount for
the momentum, total energy, and entropy are discretely conserved. When no
approximation, such as low Mach number approximations, is applied to the
fundamental equations, the excellent conservation properties of momentum, total
energy, and entropy are achieved. In decaying compressible isotropic
turbulence, this computational method can capture turbulence fluctuations.
Analyzing the Taylor-Green decaying vortex, we confirmed the validity of this
computational scheme for compressible viscous flows. In the calculation for the
natural convection in a cavity, the validity of this numerical method was
presented even in incompressible flows considering density variation. In a
three-dimensional Taylor decaying vortex problem, it was shown that this
numerical method can accurately calculate incompressible flows. We clarified
the accuracy and validity of the present numerical method by analyzing various
flow models and demonstrated the possibility of applying this method to complex
flow fields.; 57) Towards Human Cognition: Visual Context Guides Syntactic Priming in
  Fusion-Encoded Models; We introduced PRISMATIC, the first multimodal structural priming dataset, and
proposed a reference-free evaluation metric that assesses priming effects
without predefined target sentences. Using this metric, we constructed and
tested models with different multimodal encoding architectures (dual encoder
and fusion encoder) to investigate their structural preservation capabilities.
Our findings show that models with both encoding methods demonstrate comparable
syntactic priming effects. However, only fusion-encoded models exhibit robust
positive correlations between priming effects and visual similarity, suggesting
a cognitive process more aligned with human psycholinguistic patterns. This
work provides new insights into evaluating and understanding how syntactic
information is processed in multimodal language models.; 58) Collective Reasoning Among LLMs A Framework for Answer Validation
  Without Ground Truth; We present a collaborative framework where multiple large language models,
namely GPT-4-0125-preview, Meta-LLaMA-3-70B-Instruct, Claude-3-Opus, and
Gemini-1.5-Flash, work together to generate and respond to complex PhD-level
probability questions in the absence of definitive ground truth. This study
explores how inter-model consensus enhances response reliability and serves as
a proxy for assessing the quality of generated questions. To quantify agreement
and consistency, we employ statistical methods including chi-square tests,
Fleiss' Kappa, and confidence interval analysis, measuring both response
precision and question clarity. Our findings highlight that Claude and Gemini
generate well-structured and less ambiguous questions, leading to higher
inter-model agreement. This is reflected in their narrower confidence intervals
and stronger alignment with answering models. Conversely, LLaMA demonstrates
increased variability and lower reliability in question formulation, as
indicated by broader confidence intervals and reduced consensus rates. These
results suggest that multi-model collaboration not only enhances the
reliability of responses but also provides a valuable framework for assessing
and improving question quality in the absence of explicit ground truth. This
research offers meaningful insights into optimizing AI-driven reasoning through
collaborative large-language model interactions.; 59) A posteriori error estimates for the Lindblad master equation; We are interested in the simulation of open quantum systems governed by the
Lindblad master equation in an infinite-dimensional Hilbert space. To simulate
the solution of this equation, the standard approach involves two sequential
approximations: first, we truncate the Hilbert space to derive a differential
equation in a finite-dimensional subspace. Then, we use discrete time-step to
obtain a numerical solution to the finite-dimensional evolution.
  In this paper, we establish bounds for these two approximations that can be
explicitely computed to guarantee the accuracy of the numerical results.
Through numerical examples, we demonstrate the efficiency of our method,
empirically highlighting the tightness of the upper bound. While adaptive
time-stepping is already a common practice in the time discretization of the
Lindblad equation, we extend this approach by showing how to dynamically adjust
the truncation of the Hilbert space. This enables fully adaptive simulations of
the density matrix. For large-scale simulations, this approach significantly
reduces computational time and relieves users of the challenge of selecting an
appropriate truncation.; 60) Towards a Reward-Free Reinforcement Learning Framework for Vehicle
  Control; Reinforcement learning plays a crucial role in vehicle control by guiding
agents to learn optimal control strategies through designing or learning
appropriate reward signals. However, in vehicle control applications, rewards
typically need to be manually designed while considering multiple implicit
factors, which easily introduces human biases. Although imitation learning
methods does not rely on explicit reward signals, they necessitate high-quality
expert actions, which are often challenging to acquire. To address these
issues, we propose a reward-free reinforcement learning framework (RFRLF). This
framework directly learns the target states to optimize agent behavior through
a target state prediction network (TSPN) and a reward-free state-guided policy
network (RFSGPN), avoiding the dependence on manually designed reward signals.
Specifically, the policy network is learned via minimizing the differences
between the predicted state and the expert state. Experimental results
demonstrate the effectiveness of the proposed RFRLF in controlling vehicle
driving, showing its advantages in improving learning efficiency and adapting
to reward-free environments.; 61) Simplified model of immunotherapy for glioblastoma multiforme: cancer
  stem cells hypothesis perspective; Despite ongoing efforts in cancer research, a fully effective treatment for
glioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer
immunotherapy is one of the potential cure candidates, efforts have been made
to assess its effectiveness using mathematical modeling. In this paper, we
consider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),
which also takes into account the dynamics of cancer stem cells, i.e., the type
of cancer cells that are hypothesized to be largely responsible for cancer
recurrence. We modify the initial ODE system by applying simplifying
assumptions and analyze the existence and stability of steady states of the
obtained simplified model depending on the treatment levels.; 62) Characterizing User Behavior: The Interplay Between Mobility Patterns
  and Mobile Traffic; Mobile devices have become essential for capturing human activity, and
eXtended Data Records (XDRs) offer rich opportunities for detailed user
behavior modeling, which is useful for designing personalized digital services.
Previous studies have primarily focused on aggregated mobile traffic and
mobility analyses, often neglecting individual-level insights. This paper
introduces a novel approach that explores the dependency between traffic and
mobility behaviors at the user level. By analyzing 13 individual features that
encompass traffic patterns and various mobility aspects, we enhance the
understanding of how these behaviors interact. Our advanced user modeling
framework integrates traffic and mobility behaviors over time, allowing for
fine-grained dependencies while maintaining population heterogeneity through
user-specific signatures. Furthermore, we develop a Markov model that infers
traffic behavior from mobility and vice versa, prioritizing significant
dependencies while addressing privacy concerns. Using a week-long XDR dataset
from 1,337,719 users across several provinces in Chile, we validate our
approach, demonstrating its robustness and applicability in accurately
inferring user behavior and matching mobility and traffic profiles across
diverse urban contexts.; 63) Scalable skewed Bayesian inference for latent Gaussian models; Approximate Bayesian inference for the class of latent Gaussian models can be
achieved efficiently with integrated nested Laplace approximations (INLA).
Based on recent reformulations in the INLA methodology, we propose a further
extension that is necessary in some cases like heavy-tailed likelihoods or
binary regression with imbalanced data. This extension formulates a skewed
version of the Laplace method such that some marginals are skewed and some are
kept Gaussian while the dependence is maintained with the Gaussian copula from
the Laplace method. Our approach is formulated to be scalable in model and data
size, using a variational inferential framework enveloped in INLA. We
illustrate the necessity and performance using simulated cases, as well as a
case study of a rare disease where class imbalance is naturally present.; 64) Multi-Agent Risks from Advanced AI; The rapid development of advanced AI agents and the imminent deployment of
many instances of these agents will give rise to multi-agent systems of
unprecedented complexity. These systems pose novel and under-explored risks. In
this report, we provide a structured taxonomy of these risks by identifying
three key failure modes (miscoordination, conflict, and collusion) based on
agents' incentives, as well as seven key risk factors (information asymmetries,
network effects, selection pressures, destabilising dynamics, commitment
problems, emergent agency, and multi-agent security) that can underpin them. We
highlight several important instances of each risk, as well as promising
directions to help mitigate them. By anchoring our analysis in a range of
real-world examples and experimental evidence, we illustrate the distinct
challenges posed by multi-agent systems and their implications for the safety,
governance, and ethics of advanced AI.; 65) Polarizations on a triangulated category; In a recent collaboration, Hiroki Matsui and the author introduced a new
proof of the reconstruction theorem of Bondal-Orlov and Ballard, using Matsui's
construction of a ringed space associated to a triangulated category. This
paper first shows that these ideas can be applied to reconstructions of more
general varieties from their perfect derived categories. For further
applications of these ideas, we introduce the framework of a polarized
triangulated category, a pair $(\mathcal T,\tau)$ consisting of a triangulated
category $\mathcal T$ and an autoequivalence $\tau$ (called a polarization), to
which we can associate a ringed space called the pt-spectrum. As concrete
applications, we observe that several reconstruction results of Favero
naturally fit within this framework, leading to both generalizations and new
proofs of these results. Furthermore, we explore broader implications of
polarizations and pt-spectra in tensor triangular geometry, noncommutative
projective geometry, birational geometry and homological mirror symmetry.; 66) Instability of Baryonic Black Branes; Baryonic black branes describe the quantum critical phase of the conformal
conifold gauge theory at strong coupling. This phase extends to zero
temperature at a finite baryonic chemical potential, represented by extremal
black branes with $AdS_2\times R^3\times T^{1,1}$ throat in asymptotic
$AdS_5\times T^{1,1}$ geometry. We demonstrate here that this phase is
dynamically unstable below some critical value of $T_c/\mu$: the instability is
represented by a diffusive mode in the hydrodynamic sound channel with a
negative diffusion coefficient. We also identify a new (exotic) ordered phase
of the conifold gauge theory: this phase originates at the same critical value
of $T_c/\mu$, but extends to arbitrary high temperatures, and is characterized
by an expectation value of a dimension-2 operator, ${\cal O}_2\propto T^2$, in
the limit $\frac \mu T\to 0$.; 67) Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative
  with broad processability and staining-free antibacterial capability; Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free
strategy for broad-spectrum infection control in chronic wounds, minimising
bacterial resistance risks. However, rapid photosensitiser diffusion, tissue
staining, side toxicity, and short-lived antimicrobial effects present
significant clinical limitations for integrating APDT into wound dressings. To
address these challenges, we present the design of a bespoke polyvinyl alcohol
(PVA) derivative conjugated with both phenothiazine and methacrylate
functionalities, enabling staining-free antibacterial photodynamic effects,
cellular tolerability and processability into various wound dressing formats,
including films, textile fibres and nanoscale coatings. Tosylation of PVA is
leveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis
spectroscopy and the minimal release of TB in vitro. UV-induced network
formation is exploited to accomplish cast films and nanoscale integrated wound
dressing coatings. UV curing is also successfully coupled with an in-house wet
spinning process to realise individual, water-insoluble fibres as the building
blocks of fibrous wound dressings. A fluorometric assay supports the generation
of reactive oxygen species when the UV-cured samples are exposed to work, but
not UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and
the complete eradication of P. aeruginosa. Direct and extract cytotoxicity
tests with UV-cured films and fibres demonstrate the viability of L929
fibroblasts following 60-min light irradiation and 72-hour cell culture. The
bespoke molecular architecture, broad processability and cellular tolerability
of this PVA derivative are highly attractive aiming to integrate durable
staining-free photodynamic capability in a wide range of healthcare
technologies, from chronic wound dressings up to minimally invasive localised
therapy.; 68) First-order phase transition in dynamical 3-flavor QCD at imaginary
  isospin; We revisit QCD with three mass-degenerate quark flavors at an imaginary
isospin chemical potential set to 4 pi T/3. This choice corresponds to a
special point in the parameter space, where the theory possesses an exact Z(3)
center symmetry. Through a finite-size scaling analysis, we demonstrate that in
this case the finite temperature QCD transition is of first order and entails
singular behavior both in the Polyakov loop and in the quark condensate. Our
results are based on simulations with stout-smeared staggered quarks and a
dedicated multi-histogram analysis.; 69) Single-pass Detection of Jailbreaking Input in Large Language Models; Defending aligned Large Language Models (LLMs) against jailbreaking attacks
is a challenging problem, with existing approaches requiring multiple requests
or even queries to auxiliary LLMs, making them computationally heavy. Instead,
we focus on detecting jailbreaking input in a single forward pass. Our method,
called Single Pass Detection SPD, leverages the information carried by the
logits to predict whether the output sentence will be harmful. This allows us
to defend in just one forward pass. SPD can not only detect attacks effectively
on open-source models, but also minimizes the misclassification of harmless
inputs. Furthermore, we show that SPD remains effective even without complete
logit access in GPT-3.5 and GPT-4. We believe that our proposed method offers a
promising approach to efficiently safeguard LLMs against adversarial attacks.; 70) Further results for a family of continuous piecewise linear planar maps; We consider the family of piecewise linear maps $F(x,y)=\left(|x| - y + a, x
- |y| + b\right),$ where $(a,b)\in \mathbb{R}^2$. In our previous work [10], we
presented a comprehensive study of this family. In this paper, we give three
new results that complement the ones presented in that reference. All them
refer to the most interesting and complicated case, $a<0$. For this case, the
dynamics of each map is concentrated in a one-dimensional invariant graph that
depend on $b$. In [10], we studied the dynamics of the family on these graphs.
In particular, we described whether the topological entropy associated with the
map on the graph is positive or zero in terms of the parameter $c=-b/a$. Among
the results obtained, we found that there are points of discontinuity of the
entropy in the transitions from positive to zero entropy. In this paper, as a
first result, we present a detailed explicit analysis of the entropy behavior
for the case $4<c<8$, which shows the continuity of this transition from
positive to zero entropy. As a second result, we prove that for certain values
of the parameter $c$, each invariant graph contains a subset of full Lebesgue
measure where there are at most three distinct $\omega$-limit sets, which are
periodic orbits when $c \in \mathbb{Q}$. Within the framework of the third
result, we provide an explicit methodology to obtain accurate rational lower
and upper bounds for the values of the parameter $c$ at which the transition
from zero to positive entropy occurs.; 71) Direct numerical simulation benchmarks for the prediction of boundary
  layer bypass transition in the narrow sense; We report a comprehensive set of direct numerical simulation benchmarks of
bypass transition in the narrow sense with inlet freestream turbulent intensity
levels of 0.75%, 1.5%, 2.25%, 3.0%, and 6.0%, respectively. Detailed
descriptions of length scales and the rate of viscous dissipation are provided.
We ask two key physical questions. First, how do the decay rates and length
scales of freestream turbulence over a transitional and turbulent boundary
layer compare to those in spatially developing isotropic turbulence without the
wall? Second, what bypass mechanisms drive turbulent spot inception at the
intermediate rage of freestream turbulence intensity level? We find that the
boundary-layer freestream turbulence decay and length scales evolve similarly
to their spatially developing isotropic turbulence flow without the wall
counterparts. We also present evidence of the coexistence of two turbulent spot
inception mechanisms at the inlet FST level of 2.25%: the long low-speed streak
primary and secondary instabilities (only in lower inlet FST levels) and the
self-amplifying process of oblique vortex filaments interacting with a
Delta-shaped low-speed patch underneath (prevailing only in higher inlet FST
levels).; 72) Investing in nature: Stakeholder's willingness to pay for Tunisian
  forest services; This study explores the economic value of Aleppo pine forests, a unique and
threatened ecosystem in the border region of central Tunisia. These forests
play a vital role in supporting small rural communities, but face increasing
pressures and restrictions on their use. This research aims to assign a
monetary value to forest conservation, considering the region's specific
socio-economic context. Strategies for empowering local residents as key actors
in developing sustainable cross-border initiatives are further investigated.
Employing the contingent valuation method, a survey of 350 local residents and
international users was conducted to assess their willigness to pay fo forest
conservation efforts. Logistic regression analysis revealed that
sociodemographic factors, such as monthly income and preferred payment method,
significantly influence both and the likehood of participation. These findingd
highlight the feasibility and importance of reconciling economic development
with ecological sustainability in this critical region.; 73) A tumor-immune model of chronic myeloid leukemia with optimal
  immunotherapeutic protocols; The interactions between tumor cells and the immune system play a crucial
role in cancer evolution. In this study, we explore how these interactions
influence cancer progression by modeling the relationships among naive T cells,
effector T cells, and chronic myeloid leukemia cells. We examine the existence
of equilibria, the asymptotic stability of the positive steady state, and the
global stability of the tumor-free equilibrium. Additionally, we develop a
partial differential equation to describe the conditions under which the
concentration of cancer cells reaches a level that allows for effective control
of cancer evolution. Finally, we apply our proposed model to investigate
optimal treatment strategies that aim to minimize both the concentration of
cancer cells at the end of treatment and the accumulation of tumor burden, as
well as the cost associated with treatment during the intervention period. Our
study reveals an optimal therapeutic protocol using optimal control theory. We
perform numerical simulations to illustrate our theoretical results and to
explore the dynamic behavior of the system and optimal therapeutic protocols.
The simulations indicate that the optimal treatment strategy can be more
effective than a constant treatment approach, even when applying the same
treatment interval and total drug input.; 74) Tumor microenvironment (Part I): Tissue integrity in a rat model of
  peripheral neural cancer; ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are
thought to be a factor in the integrity of nerve tissues. In this report, we
attempted to trace the expression of ICAM-1, responsible for cell-to-cell
adhesion, and of MPZ, the main constituent of myelin sheath, in malignant
tissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells
(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of
the SNs were collected after 7, 14 and 21 days and compared to a sham-operated
group of rats (n = 6 each). Tissues were sectioned and histologically examined,
under light microscope, and stained for measuring the immunoreactivity of
ICAM-1 and MPZ under laser scanning microscope. The cancer model was
established, and the tumor growth was confirmed. ICAM-1 showed severe
decreases, proportional to the growing anaplastic cells, as compared to the
sham group. MPZ revealed, however, a distinct defensive pattern before
substantially decreasing in a comparison with sham. These results support the
notion that malignancies damage peripheral nerves and cause severe axonal
injury and loss of neuronal integrity, and clearly define the role of ICAM-1
and MPZ in safeguarding the nerve tissues.; 75) Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for
  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering; In osteochondral tissue engineering (OCTE), simultaneously regenerating
subchondral bone and cartilage tissue presents a significant challenge.
Multiphasic scaffolds were created and manufactured using 3D printing to
address this issue. Excellent interfacial mechanical properties and
biocompatibility enhance the growth and chondrogenic differentiation of bone
marrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is
mimicked by incorporating varying concentrations of graphene oxide (GO) (0%,
1%, and 2% w/v) into a bioink composed of alginate (Alg) and gelatin (Gel).
Based on evaluations of mechanical and biocompatibility properties, 1% GO is
selected for further studies. Subsequently, the GO concentration is kept
constant while varying the platelet-rich plasma (PRP) dosage in the multiphasic
scaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w/v) are integrated into
the Alg-Gel bioink to simulate cartilage tissues. Results indicate that
3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical
properties, with no significant differences observed. However, BM-MSCs exposed
to 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,
real-time PCR and Alcian blue staining confirm increased chondrogenic
expression and glycosaminoglycans (GAGs) synthesis. This work highlights the
promising potential of 3D-printed multiphasic frameworks in the development of
OCTE.; 76) Comparative Study of the Median Based Unit Rayleigh and its Generalized
  Form the Generalized Odd Median Based Unit Rayleigh; In the present paper, the author discusses the Generalized Odd Median Base
Unit Rayleigh (GOMBUR) in relation to the Median Based Unit Rayleigh (MBUR) to
evaluate the additive value of the new shape parameter on the estimation
process as regards validity indices, goodness of fit statistics, estimated
variances of the estimated parameters and their standard errors. This
evaluation is conducted on real datasets. Each dataset is analyzed by fitting
different competitor distributions in addition to MBUR and GOMBUR
distributions. The parameter estimation is achieved by applying Maximum
likelihood estimator (MLE) using Nelder Mead optimizer.; 77) Integrating anatomy and electrophysiology in the healthy human heart:
  Insights from biventricular statistical shape analysis using universal
  coordinates; A cardiac digital twin is a virtual replica of a patient-specific heart,
mimicking its anatomy and physiology. A crucial step of building a cardiac
digital twin is anatomical twinning, where the computational mesh of the
digital twin is tailored to the patient-specific cardiac anatomy. In a number
of studies, the effect of anatomical variation on clinically relevant
functional measurements like electrocardiograms (ECGs) is investigated, using
computational simulations. While such a simulation environment provides
researchers with a carefully controlled ground truth, the impact of anatomical
differences on functional measurements in real-world patients remains
understudied. In this study, we develop a biventricular statistical shape model
and use it to quantify the effect of biventricular anatomy on ECG-derived and
demographic features, providing novel insights for the development of digital
twins of cardiac electrophysiology. To this end, a dataset comprising
high-resolution cardiac CT scans from 271 healthy individuals, including
athletes, is utilized. Furthermore, a novel, universal, ventricular
coordinate-based method is developed to establish lightweight shape
correspondence. The performance of the shape model is rigorously established,
focusing on its dimensionality reduction capabilities and the training data
requirements. Additionally, a comprehensive synthetic cohort is made available,
featuring ready-to-use biventricular meshes with fiber structures and
anatomical region annotations. These meshes are well-suited for
electrophysiological simulations.; 78) On the regularity of Fourier interpolation formulas; By applying new functional analysis tools in the framework of Fourier
interpolation formulas, such as sc-Fredholm operators and Schauder frames, we
are able to improve and refine several properties of these aforementioned
formulas on the real line.
  As two examples of our main contributions, we highlight: (i) that we may
upgrade perturbed interpolation bases all the way to the Schwartz space, which
shows that even the perturbed interpolation formulas are as regular as the
Radchenko-Viazovska case; (ii) that a certain subset of the interpolation
formulae considered by Kulikov-Nazarov-Sodin may actually be upgraded to be
convergent in the Schwartz class, giving a first partial answer to a question
posed by those authors.
  As a final contribution of this work, we also show that, if the perturbations
are sufficiently small, then even analyticity properties of the basis functions
are preserved. This shows, in particular, that any function that vanishes on
all but finitely many of the (perturbed) nodes is automatically analytic, a
feature previously only known to hold in supercritical contexts besides the
Radchenko-Viazovska case.; 79) Assessing the Impact of the Quality of Textual Data on Feature
  Representation and Machine Learning Models; Background: Data collected in controlled settings typically results in
high-quality datasets. However, in real-world applications, the quality of data
collection is often compromised. It is well established that the quality of a
dataset significantly impacts the performance of machine learning models.
  Methods: A rudimentary error rate metric was developed to evaluate textual
dataset quality at the token level. Mixtral Large Language Model (LLM) was used
to quantify and correct errors in low quality datasets. The study analyzed two
healthcare datasets: the high-quality MIMIC-III public hospital dataset and a
lower-quality private dataset from Australian aged care homes. Errors were
systematically introduced into MIMIC at varying rates, while the ACH dataset
quality was improved using the LLM.
  Results: For the sampled 35,774 and 6,336 patients from the MIMIC and ACH
datasets respectively, we used Mixtral to introduce errors in MIMIC and correct
errors in ACH. Mixtral correctly detected errors in 63% of progress notes, with
17% containing a single token misclassified due to medical terminology. LLMs
demonstrated potential for improving progress note quality by addressing
various errors. Under varying error rates, feature representation performance
was tolerant to lower error rates (<10%) but declined significantly at higher
rates.
  Conclusions: The study revealed that models performed relatively well on
datasets with lower error rates (<10%), but their performance declined
significantly as error rates increased (>=10%). Therefore, it is crucial to
evaluate the quality of a dataset before utilizing it for machine learning
tasks. For datasets with higher error rates, implementing corrective measures
is essential to ensure the reliability and effectiveness of machine learning
models.; 80) Neural Networks: According to the Principles of Grassmann Algebra; In this paper, we explore the algebra of quantum idempotents and the
quantization of fermions which gives rise to a Hilbert space equal to the
Grassmann algebra associated with the Lie algebra. Since idempotents carry
representations of the algebra under consideration, they form algebraic
varieties and smooth manifolds in the natural topology. In addition to the
motivation of linking up mathematical physics with machine learning, it is also
shown that by using idempotents and invariant subspace of the corresponding
algebras, these representations encode and perhaps provide a probabilistic
interpretation of reasoning and relational paths in geometrical terms.; 81) Quantum Parameter Estimation for Detectors in Constantly Accelerated
  Motion; We investigate quantum parameter estimation by analyzing the dynamics of
quantum Fisher information (QFI) for the state parameters of accelerated
detectors undergoing four different acceleration scenarios: linear, cusped,
catenary, and circular motions. Our results show that QFI for the acceleration
parameter converges to a nonnegative asymptotic value over long evolution
times, with this value strongly dependent on the specific acceleration
scenario. In contrast, QFI for the weight parameter degrades to zero over time.
Notably, for sufficiently large accelerations, optimal precision in estimating
the acceleration parameter can be achieved within a finite evolution time
rather than requiring an infinitely long measurement duration. Comparing
different scenarios, we find that for small accelerations relative to the
detector's energy gap, linear motion provides the most accurate estimation of
the weight parameter, introducing the least noise among all scenarios. However,
for large accelerations, circular motion becomes the optimal scenario for
estimating the weight parameter. This behavior stands in sharp contrast to the
estimation of the acceleration parameter, where circular motion is optimal both
for small accelerations and for large accelerations over extremely long
evolution times. These distinctions in QFI may provide a new tool for
identifying the specific acceleration scenario of an accelerated detector.; 82) Linearization method and sharp thresholds for spherically symmetric
  multidimensional pressureless Euler-Poisson equations; We show that the question about the criterion of a singularity formation for
radially symmetric solutions to the Cauchy problem for a fairly wide class of
equations related to the pressureless Euler-Poisson equations can be reduced to
the study of solutions to a linear homogeneous ordinary differential equation.
In some cases, such a criterion can be obtained in terms of the initial data.
In the remaining cases, it is possible to construct a simple numerical
procedure, on the basis of which the question about preserving smoothness for
any set of initial data can be solved.; 83) Searching Axion-like Dark Matter by Amplifying Weak Magnetic Field with
  Quantum Zeno effect; The enhancement of weak signals and the detection of hypothetical particles,
facilitated by quantum amplification, are crucial for advancing fundamental
physics and its practical applications. Recently, it was experimentally
observed that magnetic field can be amplified by using nuclear spins under
Markovian noise, [H. Su, et al., Phys. Rev. Lett. 133, 191801 (2024)]. Here, we
theoretically propose amplifying the magnetic-field signal by using nuclear
spins by the quantum Zeno effect (QZE). Under identical conditions, we
demonstrate that compared to the Markovian case the amplification of the weak
magnetic field can be enhanced by a factor about $e^{1/2}$ under a Gaussian
noise. Moreover, through numerical simulations we determine the optimal
experimental parameters for amplification conditions. This work shows that the
combination of the QZE and spin amplification effectively enhances the
amplification of the weak magnetic field. Our findings may provide valuable
guidance for the design of experiments on establishing new constraints of dark
matter and exotic interactions in the near future.; 84) Periodic phase slips and frequency comb generation at tunable microwave
  frequencies in superconducting diabolo structures; Superconductors are characterized by macroscopic phase coherence and have
enabled cryogenic electronics and quantum technologies. Recent advances in 3D
nanofabrication now offer possibilities for tuning functional properties
relevant for on-chip 3D integration of superconductors. However,
non-equilibrium phenomena in 3D nanostructures exposed to transport currents
remain largeley unexplored. Here, we employ numerical simulations to
investigate phase slips -- discrete $2\pi$ jumps in the phase of the
superconducting order parameter -- in a tubular Nb superconductor with a
central constriction, which is subjected to both direct current (DC) and
alternating current (AC) transport currents. We find that under DC drive, the
system stabilizes periodic phase slips, resulting in GHz voltage oscillations.
Introducing an additional AC frequency modulation generates microwave frequency
combs which depend characteristically on the interaction between moving
vortices and phase slips. Our findings open avenues for developing on-chip
frequency comb generators in 3D cryoelectronics.; 85) A Belyi-type criterion for vector bundles on curves defined over a
  number field; Let $X_0$ be an irreducible smooth projective curve defined over
$\overline{\mathbb Q}$ and $f_0 : X_0 \rightarrow
\mathbb{P}^1_{\overline{\mathbb Q}}$ a nonconstant morphism whose branch locus
is contained in the subset $\{0,1, \infty\} \subset
\mathbb{P}^1_{\overline{\mathbb Q}}$. For any vector bundle $E$ on $X =
X_0\times_{{\rm Spec}\,\overline{\mathbb Q}} {\rm Spec} \mathbb{C}$, consider
the direct image $f_*E$ on $\mathbb{P}^1_{\mathbb C}$, where $f= (f_0)_{\mathbb
C}$. It decomposes into a direct sum of line bundles and also it has a natural
parabolic structure. We prove that $E$ is the base change, to $\mathbb C$, of a
vector bundle on $X_0$ if and only if there is an isomorphism $f_*E
\stackrel{\sim}{\rightarrow} \bigoplus_{i=1}^r {\mathcal O}_{{\mathbb
P}^1_{\mathbb C}}(m_i)$, where $r = {\rm rank}(f_*E)$, that takes the parabolic
structure on $f_*E$ to a parabolic structure on $\bigoplus_{i=1}^r {\mathcal
O}_{{\mathbb P}^1_{\mathbb C}}(m_i)$ defined over $\overline{\mathbb Q}$.; 86) Non-linear Neumann weighted eigenvalues in outward cuspidal domains; In this article we study the non-linear Neumann spectral problem in outward
cuspidal domains. By using composition operators we prove embedding of Sobolev
spaces into weighted Lebesgue spaces. As a consequence, we obtain solvability
of the Neumann spectral problem and estimates of Neumann weighted eigenvalues
in outward cuspidal domains.; 87) Flat Band and Many-body Gap in Chirally Twisted Triple Bilayer Graphene; We experimentally investigate the band structures of chirally twisted triple
bilayer graphene. The new kind of moir\'e structure, formed by three pieces of
helically stacked Bernal bilayer graphene, has flat bands at charge neutral
point based on the continuum approximation. We experimentally confirm the
existence of flat bands and directly acquire the gap in-between flat bands as
well as between the flat bands and dispersive bands from the capacitance
measurements. We discover a finite gap even at zero perpendicular electric
field, possibly induced by the Coulomb interaction and ferromagnetism. Our
quantitative study not only provides solid evidence for the flat-band and
interesting physics, but also introduces a quantitative approach to explore
phenomena of similar moir\'e systems.; 88) Can linear algebra create perfect knockoffs?; As new Model-X knockoff construction techniques are developed, primarily
concerned with determining the correct conditional distribution from which to
sample, we focus less on deriving the correct multivariate distribution and
instead ask if ``perfect'' knockoffs can be constructed using linear algebra.
Using mean absolute correlation between knockoffs and features as a measure of
quality, we do produce knockoffs that are pseudo-perfect, however, the
optimization algorithm is computationally very expensive. We outline a series
of methods to significantly reduce the computation time of the algorithm.; 89) Metaprism Design for Wireless Communications: Angle-Frequency Analysis,
  Physical Realizability Constraints, and Performance Optimization; Recent advancements in smart radio environment technologies aim to enhance
wireless network performance through the use of low-cost electromagnetic (EM)
devices. Among these, reconfigurable intelligent surfaces (RIS) have garnered
attention for their ability to modify incident waves via programmable
scattering elements. An RIS is a nearly passive device, in which the tradeoff
between performance, power consumption, and optimization overhead depend on how
often the RIS needs to be reconfigured. This paper focuses on the metaprism
(MTP), a static frequency-selective metasurface which relaxes the
reconfiguration requirements of RISs and allows for the creation of different
beams at various frequencies. In particular, we address the design of an ideal
MTP based on its frequency-dependent reflection coefficients, defining the
general properties necessary to achieve the desired beam steering function in
the angle-frequency domain. We also discuss the limitations of previous studies
that employed oversimplified models, which may compromise performance. Key
contributions include a detailed exploration of the equivalence of the MTP to
an ideal S-parameter multiport model and an analysis of its implementation
using Foster's circuits. Additionally, we introduce a realistic multiport
network model that incorporates aspects overlooked by ideal scattering models,
along with an ad hoc optimization strategy for this model. The performance of
the proposed optimization approach and circuits implementation are validated
through simulations using a commercial full-wave EM simulator, confirming the
effectiveness of the proposed method.; 90) StarCast: A Secure and Spectrum-Efficient Group Communication Scheme for
  LEO Satellite Networks; Low Earth Orbit (LEO) satellite networks serve as a cornerstone
infrastructure for providing ubiquitous connectivity in areas where terrestrial
infrastructure is unavailable. With the emergence of Direct-to-Cell (DTC)
satellites, these networks can provide direct access to mobile phones and IoT
devices without relying on terrestrial base stations, leading to a surge in
massive connectivity demands for the serving satellite. To address this issue,
group communication is an effective paradigm that enables simultaneous content
delivery to multiple users and thus optimizes bandwidth reuse. Although
extensive research has been conducted to improve group communication
performance, securing this communication without compromising its inherent
spectrum efficiency remains a critical challenge. To address this, we introduce
StarCast, a secure group encryption scheme for LEO satellite networks. Our
solution leverages ciphertext-policy attribute-based encryption (CP-ABE) to
implement fine-grained access control by embedding access policies directly
within the ciphertext. Unlike standard secure communication approaches that
require dedicated per-user channels and significantly deplete limited satellite
spectrum resources, StarCast maintains efficient spectrum reuse within user
groups while ensuring that only authorized users can access transmitted data.
Additionally, it significantly reduces the costly key management overhead
associated with conventional encryption schemes.; 91) Scalable calibration of individual-based epidemic models through
  categorical approximations; Traditional compartmental models capture population-level dynamics but fail
to characterize individual-level risk. The computational cost of exact
likelihood evaluation for partially observed individual-based models, however,
grows exponentially with the population size, necessitating approximate
inference. Existing sampling-based methods usually require multiple simulations
of the individuals in the population and rely on bespoke proposal distributions
or summary statistics. We propose a deterministic approach to approximating the
likelihood using categorical distributions. The approximate likelihood is
amenable to automatic differentiation so that parameters can be estimated by
maximization or posterior sampling using standard software libraries such as
Stan or TensorFlow with little user effort. We prove the consistency of the
maximum approximate likelihood estimator. We empirically test our approach on
several classes of individual-based models for epidemiology: different sets of
disease states, individual-specific transition rates, spatial interactions,
under-reporting and misreporting. We demonstrate ground truth recovery and
comparable marginal log-likelihood values at substantially reduced cost
compared to competitor methods. Finally, we show the scalability and
effectiveness of our approach with a real-world application on the 2001 UK
Foot-and-Mouth outbreak, where the simplicity of the CAL allows us to include
162775 farms.; 92) A Survey of Challenges and Sensing Technologies in Autonomous Retail
  Systems; Autonomous stores leverage advanced sensing technologies to enable
cashier-less shopping, real-time inventory tracking, and seamless customer
interactions. However, these systems face significant challenges, including
occlusion in vision-based tracking, scalability of sensor deployment, theft
prevention, and real-time data processing. To address these issues, researchers
have explored multi-modal sensing approaches, integrating computer vision,
RFID, weight sensing, vibration-based detection, and LiDAR to enhance accuracy
and efficiency. This survey provides a comprehensive review of sensing
technologies used in autonomous retail environments, highlighting their
strengths, limitations, and integration strategies. We categorize existing
solutions across inventory tracking, environmental monitoring, people-tracking,
and theft detection, discussing key challenges and emerging trends. Finally, we
outline future directions for scalable, cost-efficient, and privacy-conscious
autonomous store systems.; 93) Convective stability analysis of massive neutron stars formed in binary
  mergers; We perform fully general-relativistic hydrodynamics simulations of binary
neutron star mergers over $100\,\rm ms$ post-merger to investigate the dynamics
of remnant massive neutron stars (NSs). Our focus is mainly on the analysis of
convective stability and mode characteristics of the massive NSs. We derive
stability criteria for hot, differentially rotating relativistic stars that
account for both buoyant and rotational restoring forces, and apply them for
the first time to the post-merger massive NSs. Our results show no evidence of
large-scale convective instability, as both angle-averaged specific entropy and
specific angular momentum increase outward within the massive NSs. Rotational
effects significantly enhance stability for local regions that would be
otherwise unstable by the Schwarzschild criterion. Additionally, our mode
analysis of matter fields and gravitational waves reveals no excitation of
inertial modes after the damping of quadrupolar $f$-modes in the massive NSs,
contrasting with previous studies. As in many previous works, we observe the
excitation of an $m=1$ one-armed mode. However, we also find that the growth of
the $m=1$ mode amplitude after the merger may correlate strongly with the
violation of linear momentum conservation, indicating that we cannot reject the
possibility that the excitation of the one-armed mode has a numerical origin.; 94) On a class of triangular cross-diffusion systems and its fast reaction
  approximation; The purpose of this paper is to investigate the emergence of cross-diffusion
terms in the time evolution of two slow-fast species in competition for
resources. A class of triangular cross-diffusion systems is obtained as the
singular limit of a fast reaction-diffusion system, as the time scale
$\epsilon$ of the fast reaction goes to 0. We prove that the classical solution
of the fast reaction-diffusion system converges towards a weak solution of the
cross-diffusion system and we show a convergence rate for bounded solutions.
The main tool used is a family of energy functionals, giving suitable a priori
estimates, uniformly on $\epsilon$.; 95) Reconfiguration of square-tiled surfaces; We consider a combinatorial reconfiguration problem on a subclass of
quadrangulations of surfaces called square-tiled surfaces. Our elementary move
is a shear in a cylinder that corresponds to a well-chosen sequence of diagonal
flips that preserves the square-tiled properties. We conjecture that the
connected components of this reconfiguration problem are in bijection with the
connected components of the moduli space of quadratic differentials. We prove
that the conjecture holds in the so-called hyperelliptic components of Abelian
square-tiled surfaces. More precisely, we show that any two such square-tiled
surfaces of genus $g$ can be connected by $O(g)$ powers of cylinder shears.; 96) Observability-Blocking Controls for Double-Integrator and Higher Order
  Integrator Networks; The design of state-feedback controls to block observability at remote nodes
is studied for double integrator network (DIN) and higher order integrator
network models. A preliminary design algorithm is presented first for DIN that
requires $m+2$ actuation nodes to block observability for the measurement
obtained from a set of $m$ nodes. The algorithm is based on eigenstructure
assignment technique and leverages the properties of the eigenvectors in DIN.
Next, the topological structure of the network is exploited to reduce the
number of controllers required for blocking observability. The number of
actuation nodes in sparser design depends on the cardinality of a cutset
separating the actuation and measurement locations. Later, the design
principles are generalized for blocking observability in $N$-th order
integrator network models.; 97) Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema; This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection; 98) Predicting the N\'eel temperatures in general helimagnetic materials: a
  comparison between mean field theory, random phase approximation,
  renormalized spin wave theory and classical Monte Carlo simulations; The critical temperature for magnetic order comprises a crucial property of
any magnetic material and ranges from a few Kelvin in certain antiferromagnets
to 1400 K in ferromagnetic Co. However, the prediction of critical temperatures
based on, for example, a spin wave dispersion is in general non-trivial. For
ferromagnets and simple collinear antiferromagnets, estimates may be obtained
from the Heisenberg model using either renormalized spin wave theory or the
Green's function random phase approximation (RPA), but a systematic assessment
of the accuracy of such approaches seems to be lacking in the literature. In
this work, we propose generalizations of both renormalized spin wave theory and
RPA to calculate the critical temperatures of single-$Q$ helimagnetic ground
states, which include ferromagnets and antiferromagnets as special cases. We
compare the methods to classical Monte Carlo simulations and Mean field theory,
using experimental exchange parameters for a wide range of materials; MnO and
NiO (single site N\'eel ground states), MnF$_2$ (altermagnet), Cr$_2$O$_3$ and
Fe$_2$O$_3$ (two site N\'eel states) and Ba$_3$NbFe$_3$Si$_2$O$_{14}$
(incommensurate helimagnet). In all cases, we observe that predictions from RPA
are in excellent agreement with experimental values and RPA thus constitutes a
rather reliable all-purpose method for calculating critical temperatures.; 99) Geometric immunosuppression in CAR-T cell treatment: Insights from
  mathematical modeling; Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising
treatment for hematological malignancies, offering a targeted approach to
cancer treatment. Understanding the complexities of CAR-T cell therapy within
solid tumors poses challenges due to the intricate interactions within the
tumor microenvironment. Mathematical modeling may serve as a valuable tool to
unravel the dynamics of CAR-T cell therapy and improve its effectiveness in
solid tumors. This study aimed to investigate the impact of spatial aspects in
CAR-T therapy of solid tumors, utilizing cellular automata for modeling
purposes. Our main objective was to deepen our understanding of treatment
effects by analyzing scenarios with different spatial distributions and varying
the initial quantities of tumor and CAR-T cells. Tumor geometry significantly
influenced treatment efficacy in-silico, with notable differences observed
between tumors with block-like arrangements and those with sparse cell
distributions, leading to the concept of immune suppression due to geometrical
effects. This research delves into the intricate relationship between spatial
dynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting
the relevance of tumor geometry in the outcome of cellular immunotherapy
treatments. Our results provide a basis for improving the efficacy of CAR-T
cell treatments by combining them with other ones reducing the density of
compact tumor areas and thus opening access ways for tumor killing T-cells.; 100) Low mode approximation in the axion magnetohydrodynamics; We study the evolution of interacting large scale magnetic and axionic
fields. Based on the new induction equation accounting for the contribution of
spatially inhomogeneous axions, we consider the evolution of a magnetized
spherical axion structure. Using the thin layer approximation, we derive the
system of the nonlinear ordinary differential equations for harmonics of
poloidal and toroidal magnetic fields, as well as for the axion field. In this
system, we account for up to four modes. Considering this small and dense axion
clump to be in a solar plasma, we numerically simulate the evolution of
magnetic fields. We obtain that the behavior of magnetic fields depends on the
initial fields configuration. Moreover, we find an indication on a magnetic
field instability in the magnetohydrodynamics with inhomogeneous axions.",0.0,0.45560514958746035
2411.00726,applied,2411.00726-pos2-5,"Relation Between Retinal Vasculature and Retinal Thickness in Macular Edema; This study has investigated the relationship of retinal vasculature and thickness for Macular Edema (ME) subjects. Ninety sets Fluorescein Angiograph (FA) Optical Coherence Tomography (OCT) 54 participants were analyzed. Multivariate analysis using binary logistic regression model was used to association between vessel parameters thickness. The results reveal feature i.e. fractal dimension (FD) as most sensitive parameter changes in associated with ME. Thus, indicating a direct which is caused due neovascular causing exudates, leakages hemorrhages, applications alternate modality detection",2411.00726-pos1-5,"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",54,"['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']","Paper 1 presents a sophisticated model of perception and learning through chunking and abstraction that can be integrated with the findings of the main paper on retinal vasculature and thickness in macular edema. This integration could lead to novel approaches in the analysis and modeling of retinal structures and their relationship with fluid dynamics, enhancing predictive modeling in ocular conditions. The following papers, while relevant, do not contribute as effectively to the multidisciplinary idea, with paper 2 focusing on network forensics instead of medical imaging, and subsequent papers falling further from the core concept of combining computational models with retinal analysis.","1) From Dionysius Emerges Apollo -- Learning Patterns and Abstractions from
  Perceptual Sequences; Cognition swiftly breaks high-dimensional sensory streams into familiar parts
and uncovers their relations. Why do structures emerge, and how do they enable
learning, generalization, and prediction? What computational principles
underlie this core aspect of perception and intelligence? A sensory stream,
simplified, is a one-dimensional sequence. In learning such sequences, we
naturally segment them into parts -- a process known as chunking. In the first
project, I investigated factors influencing chunking in a serial reaction time
task and showed that humans adapt to underlying chunks while balancing speed
and accuracy. Building on this, I developed models that learn chunks and parse
sequences chunk by chunk. Normatively, I proposed chunking as a rational
strategy for discovering recurring patterns and nested hierarchies, enabling
efficient sequence factorization. Learned chunks serve as reusable primitives
for transfer, composition, and mental simulation -- letting the model compose
the new from the known. I demonstrated this model's ability to learn
hierarchies in single and multi-dimensional sequences and highlighted its
utility for unsupervised pattern discovery. The second part moves from concrete
to abstract sequences. I taxonomized abstract motifs and examined their role in
sequence memory. Behavioral evidence suggests that humans exploit pattern
redundancies for compression and transfer. I proposed a non-parametric
hierarchical variable model that learns both chunks and abstract variables,
uncovering invariant symbolic patterns. I showed its similarity to human
learning and compared it to large language models. Taken together, this thesis
suggests that chunking and abstraction as simple computational principles
enable structured knowledge acquisition in hierarchically organized sequences,
from simple to complex, concrete to abstract.; 2) An Identity and Interaction Based Network Forensic Analysis; In todays landscape of increasing electronic crime, network forensics plays a
pivotal role in digital investigations. It aids in understanding which systems
to analyse and as a supplement to support evidence found through more
traditional computer based investigations. However, the nature and
functionality of the existing Network Forensic Analysis Tools (NFATs) fall
short compared to File System Forensic Analysis Tools (FS FATs) in providing
usable data. The analysis tends to focus upon IP addresses, which are not
synonymous with user identities, a point of significant interest to
investigators. This paper presents several experiments designed to create a
novel NFAT approach that can identify users and understand how they are using
network based applications whilst the traffic remains encrypted. The
experiments build upon the prior art and investigate how effective this
approach is in classifying users and their actions. Utilising an in-house
dataset composed of 50 million packers, the experiments are formed of three
incremental developments that assist in improving performance. Building upon
the successful experiments, a proposed NFAT interface is presented to
illustrate the ease at which investigators would be able to ask relevant
questions of user interactions. The experiments profiled across 27 users, has
yielded an average 93.3% True Positive Identification Rate (TPIR), with 41% of
users experiencing 100% TPIR. Skype, Wikipedia and Hotmail services achieved a
notably high level of recognition performance. The study has developed and
evaluated an approach to analyse encrypted network traffic more effectively
through the modelling of network traffic and to subsequently visualise these
interactions through a novel network forensic analysis tool.; 3) Beyond checkmate: exploring the creative chokepoints in AI text; Large Language Models (LLMs) have revolutionized Natural Language Processing
(NLP) and Artificial Intelligence (AI), unlocking unprecedented capabilities.
This rapid advancement has spurred research into various aspects of LLMs, their
text generation & reasoning capability, and potential misuse, fueling the
necessity for robust detection methods. While numerous prior research has
focused on detecting LLM-generated text (AI text) and thus checkmating them,
our study investigates a relatively unexplored territory: portraying the
nuanced distinctions between human and AI texts across text segments. Whether
LLMs struggle with or excel at incorporating linguistic ingenuity across
different text segments carries substantial implications for determining their
potential as effective creative assistants to humans. Through an analogy with
the structure of chess games-comprising opening, middle, and end games-we
analyze text segments (introduction, body, and conclusion) to determine where
the most significant distinctions between human and AI texts exist. While AI
texts can approximate the body segment better due to its increased length, a
closer examination reveals a pronounced disparity, highlighting the importance
of this segment in AI text detection. Additionally, human texts exhibit higher
cross-segment differences compared to AI texts. Overall, our research can shed
light on the intricacies of human-AI text distinctions, offering novel insights
for text detection and understanding.; 4) Nonuniqueness analysis on the Navier-Stokes equation in $C_{t}L^{q}$
  space; In the presence of any prescribed kinetic energy, we implement the
intermittent convex integration scheme with $L^{q}$-normalized intermittent
jets to give a direct proof for the existence of solution to the Navier-Stokes
equation in $C_{t}L^{q}$ for some uniform $2<q\ll3$ without the help of
interpolation inequality. The result shows the sharp nonuniqueness that there
evolve infinite nontrivial weak solutions of the Navier-Stokes equation
starting from zero initial data. Furthermore, we improve the regularity of
solution to be of $C_{t}W^{\alpha,q}$ in virtue of the fractional
Gagliardo-Nirenberg inequalities with some $0<\alpha\ll1$. More importantly,
the proof framework provides a stepping stone for future progress on the method
of intermittent convex integration due to the fact that $L^{q}$-normalized
building blocks carry the threshold effect of the exponent $q$ arbitrarily
close to the critical value $3$.; 5) Exact Approximation In The Field Of Formal Series; In this article, we prove a lower bound for the Hausdorff dimension of the
set of exactly $\psi$-approximable vectors with values in a local field of
positive characteristic. This is the analogue of the corresponding theorem of
Bandi and de Saxc'e on reals \cite{bandi2023hausdorff} and is a
higher-dimensional version of a theorem of Zhang \cite{MR2834892}.; 6) ALMA 360 pc high-frequency observations reveal warm dust in the center
  of a $z=6.9$ quasar; The temperature of the cold dust in z>6 galaxies is a potential tracer of
Active Galactic Nucleus (AGN) and stellar feedback, and is the dominant source
of uncertainty in inferring properties from the far-infrared (FIR) emission of
these galaxies. We present the first resolved dust temperature map in a $z>6$
quasar host galaxy. We combine new 360 pc resolution ALMA Band 9 continuum
observations with literature 190 pc Band 6 observations to derive the dust
temperature and opacity at 0.1<r<0.5 kpc scales in a $z=6.9$ luminous quasar
host galaxy (J2348-3054). We find that the dust temperature (and opacity)
increases at the center (r<216 pc) of the galaxy up to $T_d=73-88$ K, and
potentially up to $T_d<149$ K at r<110 pc. The combination of the resolved and
integrated FIR Spectral Energy Distribution (SED) further reveal a dust
temperature gradient and a significant contribution of the AGN hot dust torus
at $\nu_{\rm{obs}}\gtrsim 700$ GHz. By taking into account the torus
contribution and resolved optically-thick emission, we derive a total infrared
luminosity ($L_{TIR}=8.78\pm0.10)\times 10^{12}L_\odot$) and corresponding
star-formation rate (SFR$=1307\pm15\ M_\odot\ \rm{yr}^{-1}$), that are at least
a factor $\sim 3.6$ ($\sim0.56$ dex) lower than previous measurements assuming
optically-thin emission. We compare the resolved dust temperature, mass and IR
luminosity profiles to simulations where they are only reproduced by models in
which the AGN radiation heats the dust in the center of the galaxy. Our
observations provide evidence that dust in J2348--3054 cannot be assumed to be
uniformly cold and optically thin. Whether J2348-3054 is representative of the
larger population of high-redshift quasars and galaxies remains to be
determined with dedicated high-resolution and high-frequency ALMA observations.; 7) Journey from the Wilson exact RG towards the Wegner-Morris Fokker-Planck
  RG and the Carosso field-coarsening via Langevin stochastic processes; Within the Wilson RG of 'incomplete integration' as a function of the RG-time
$t$, the non-linear differential RG flow for the energy $E_t[\phi(.)]$
translates for the probability distribution $P_t[\phi(.)] \sim e^{-
E_t[\phi(.)]} $ into the linear Fokker-Planck RG flow associated to independent
non-identical Ornstein-Uhlenbeck processes for the Fourier modes. The
corresponding Langevin stochastic differential equation for the real-space
field $\phi_t(\vec x)$ can be then interpreted within the Carosso perspective
as genuine infinitesimal coarsening-transformations that are the analog of
spin-blocking, and whose irreversible character is essential to overcome the
paradox of the naive description of the Wegner-Morris RG flow as a mere
infinitesimal change of variables in the partition function integral. This
interpretation suggests to consider new RG-schemes, in particular the Carosso
RG where the Langevin SDE corresponds to the well known stochastic heat
equation or the Edwards-Wilkinson dynamics. We stress the advantages of this
stochastic formulation of exact RG flows. While statistical field theory is
usually written in infinite space, we focus here on the formulation on a large
volume $L^d$ with periodic boundary conditions, in order to distinguish between
extensive and intensives observables while keeping the translation-invariance.
Since the empirical magnetization $m_e \equiv \frac{1}{L^d} \int_{L^d} d^d \vec
x \ \phi(\vec x) $ is an intensive variable corresponding to the zero-momentum
Fourier coefficient of the field, its probability distribution $p_L(m_e)$ can
be obtained from the gradual integration over all the other Fourier
coefficients associated to non-vanishing-momenta via exact differential RG, in
order to obtain the large deviation properties with respect to the volume
$L^d$.; 8) Estimating Optimal Dynamic Treatment Regimes Using Irregularly Observed
  Data: A Target Trial Emulation and Bayesian Joint Modeling Approach; An optimal dynamic treatment regime (DTR) is a sequence of decision rules
aimed at providing the best course of treatments individualized to patients.
While conventional DTR estimation uses longitudinal data, such data can also be
irregular, where patient-level variables can affect visit times, treatment
assignments and outcomes. In this work, we first extend the target trial
framework - a paradigm to estimate statistical estimands specified under
hypothetical randomized trials using observational data - to the DTR context;
this extension allows treatment regimes to be defined with intervenable visit
times. We propose an adapted version of G-computation marginalizing over random
effects for rewards that encapsulate a treatment strategy's value. To estimate
components of the G-computation formula, we then articulate a Bayesian joint
model to handle correlated random effects between the outcome, visit and
treatment processes. We show via simulation studies that, in the estimation of
regime rewards, failure to account for the observational treatment and visit
processes produces bias which can be removed through joint modeling. We also
apply our proposed method on data from INSPIRE 2 and 3 studies to estimate
optimal injection cycles of Interleukin 7 to treat HIV-infected individuals.; 9) Efficient Monte Carlo Event Generation for Neutrino-Nucleus Exclusive
  Cross Sections; Modern neutrino-nucleus cross section predictions need to incorporate
sophisticated nuclear models to achieve greater predictive precision. However,
the computational complexity of these advanced models often limits their
practicality for experimental analyses. To address this challenge, we introduce
a new Monte Carlo method utilizing Normalizing Flows to generate surrogate
cross sections that closely approximate those of the original model while
significantly reducing computational overhead. As a case study, we built a
Monte Carlo event generator for the neutrino-nucleus cross section model
developed by the Ghent group. This model employs a Hartree-Fock procedure to
establish a quantum mechanical framework in which both the bound and scattering
nucleon states are solutions to the mean-field nuclear potential. The surrogate
cross sections generated by our method demonstrate excellent accuracy with a
relative effective sample size of more than $98.4 \%$, providing a
computationally efficient alternative to traditional Monte Carlo sampling
methods for differential cross sections.; 10) Differential Machine Learning for Time Series Prediction; Accurate time series prediction is challenging due to the inherent
nonlinearity and sensitivity to initial conditions. We propose a novel approach
that enhances neural network predictions through differential learning, which
involves training models on both the original time series and its differential
series. Specifically, we develop a differential long short-term memory
(Diff-LSTM) network that uses a shared LSTM cell to simultaneously process both
data streams, effectively capturing intrinsic patterns and temporal dynamics.
Evaluated on the Mackey-Glass, Lorenz, and R\""ossler chaotic time series, as
well as a real-world financial dataset from ACI Worldwide Inc., our results
demonstrate that the Diff- LSTM network outperforms prevalent models such as
recurrent neural networks, convolutional neural networks, and bidirectional and
encoder-decoder LSTM networks in both short-term and long-term predictions.
This framework offers a promising solution for enhancing time series
prediction, even when comprehensive knowledge of the underlying dynamics of the
time series is not fully available.; 11) Modeling of stochastic processes in $L_p(T)$ using orthogonal
  polynomials; In this paper, models that approximate stochastic processes from the space
$Sub_\varphi(\Omega)$ with given reliability and accuracy in $L_p(T)$ are
considered for some specific functions $\varphi(t)$. For processes that are
decomposited in series using orthonormal bases, such models are constructed in
the case where elements of such decomposition cannot be found explicitly.; 12) ExoSim 2: the new Exoplanet Observation Simulator applied to the Ariel
  space mission; ExoSim 2 is the next generation of the Exoplanet Observation Simulator
(ExoSim) tailored for spectro-photometric observations of transiting exoplanets
from space, ground, and sub-orbital platforms. This software is a complete
rewrite implemented in Python 3, embracing object-oriented design principles,
which allow users to replace each component with their functions when required.
ExoSim 2 is publicly available on GitHub, serving as a valuable resource for
the scientific community. ExoSim 2 employs a modular architecture using Task
classes, encapsulating simulation algorithms and functions. This flexible
design facilitates the extensibility and adaptability of ExoSim 2 to diverse
instrument configurations to address the evolving needs of the scientific
community. Data management within ExoSim 2 is handled by the Signal class,
which represents a structured data cube incorporating time, space, and spectral
dimensions. The code execution in ExoSim 2 follows a three-step workflow: the
creation of focal planes, the production of Sub-Exposure blocks, and the
generation of non-destructive reads (NDRs). Each step can be executed
independently, optimizing time and computational resources. ExoSim 2 has been
extensively validated against other tools like ArielRad and has demonstrated
consistency in estimating photon conversion efficiency, saturation time, and
signal generation. The simulator has also been validated independently for
instantaneous read-out and jitter simulation, and for astronomical signal
representation. In conclusion, ExoSim 2 offers a robust and flexible tool for
exoplanet observation simulation, capable of adapting to diverse instrument
configurations and evolving scientific needs. Its design principles and
validation results underscore its potential as a valuable resource in the field
of exoplanet research.; 13) Algebraization of rigid analytic varieties and formal schemes via
  perfect complexes; In this paper, we extend a theorem of To\""en and Vaqui\'e to the
non-Archimedean and formal settings. More precisely, we prove that a smooth and
proper rigid analytic variety is algebraizable if and only if its category of
perfect complexes is smooth and proper. As a corollary, we deduce an analogous
statement for formal schemes and demonstrate that, in general, the bounded
derived category of coherent sheaves on a formal scheme is not smooth.; 14) Fast Debiasing of the LASSO Estimator; In high-dimensional sparse regression, the \textsc{Lasso} estimator offers
excellent theoretical guarantees but is well-known to produce biased estimates.
To address this, \cite{Javanmard2014} introduced a method to ``debias"" the
\textsc{Lasso} estimates for a random sub-Gaussian sensing matrix
$\boldsymbol{A}$. Their approach relies on computing an ``approximate inverse""
$\boldsymbol{M}$ of the matrix $\boldsymbol{A}^\top \boldsymbol{A}/n$ by
solving a convex optimization problem. This matrix $\boldsymbol{M}$ plays a
critical role in mitigating bias and allowing for construction of confidence
intervals using the debiased \textsc{Lasso} estimates. However the computation
of $\boldsymbol{M}$ is expensive in practice as it requires iterative
optimization. In the presented work, we re-parameterize the optimization
problem to compute a ``debiasing matrix"" $\boldsymbol{W} :=
\boldsymbol{AM}^{\top}$ directly, rather than the approximate inverse
$\boldsymbol{M}$. This reformulation retains the theoretical guarantees of the
debiased \textsc{Lasso} estimates, as they depend on the \emph{product}
$\boldsymbol{AM}^{\top}$ rather than on $\boldsymbol{M}$ alone. Notably, we
provide a simple, computationally efficient, closed-form solution for
$\boldsymbol{W}$ under similar conditions for the sensing matrix
$\boldsymbol{A}$ used in the original debiasing formulation, with an additional
condition that the elements of every row of $\boldsymbol{A}$ have uncorrelated
entries. Also, the optimization problem based on $\boldsymbol{W}$ guarantees a
unique optimal solution, unlike the original formulation based on
$\boldsymbol{M}$. We verify our main result with numerical simulations.; 15) On the spectral gap of negatively curved covers; Given a negatively curved compact Riemannian surface $X$, we give an explicit
estimate, valid with high probability as the degree goes to infinity, of the
first non-trivial eigenvalue of the Laplacian on random Riemannian covers of
$X$. The explicit gap is given in terms of the bottom of the spectrum of the
universal cover of $X$ and the topological entropy of the geodesic flow on X.
This result generalizes in variable curvature a result of Magee-Naud-Puder for
hyperbolic surfaces. We then formulate a conjecture on the optimal spectral gap
and show that there exists covers with near optimal spectral gaps using a
result of Louder-Magee and techniques of strong convergence from random matrix
theory.; 16) Recommender Systems for Social Good: The Role of Accountability and
  Sustainability; This work examines the role of recommender systems in promoting
sustainability, social responsibility, and accountability, with a focus on
alignment with the United Nations Sustainable Development Goals (SDGs). As
recommender systems become increasingly integrated into daily interactions,
they must go beyond personalization to support responsible consumption, reduce
environmental impact, and foster social good. We explore strategies to mitigate
the carbon footprint of recommendation models, ensure fairness, and implement
accountability mechanisms. By adopting these approaches, recommender systems
can contribute to sustainable and socially beneficial outcomes, aligning
technological advancements with the SDGs focused on environmental
sustainability and social well-being.; 17) Approximate Bayesian Kernel Machine Regression via Random Fourier
  Features for Estimating Joint Health Effects of Multiple Exposures; Environmental epidemiology has traditionally focused on examining health
effects of single exposures, more recently with adjustment for co-occurring
exposures. Advancements in exposure assessments and statistical tools have
enabled a shift towards studying multiple exposures and their combined health
impacts. Bayesian Kernel Machine Regression (BKMR) is a popular approach to
flexibly estimate the joint and nonlinear effects of multiple exposures.
However, BKMR faces computation challenges for large datasets, as inverting the
kernel repeatedly in Markov chain Monte Carlo (MCMC) algorithms can be
time-consuming and often infeasible in practice. To address this issue, we
propose a faster version of BKMR using supervised random Fourier features to
approximate the Gaussian process. We use periodic functions as basis functions
and this approximation re-frames the kernel machine regression into a linear
mixed-effect model that facilitates computationally efficient estimation and
prediction. Bayesian inference was conducted using MCMC with Hamiltonian Monte
Carlo algorithms. Analytic code for implementing Fast BKMR was developed for R.
Simulation studies demonstrated that this approximation method yields results
comparable to the original Gaussian process while reducing the computation time
by 29 to 99%, depending on the number of basis functions and sample sizes. Our
approach is also more robust to kernel misspecification in some scenarios.
Finally, we applied this approach to analyze over 270,000 birth records,
examining associations between multiple ambient air pollutants and birthweight
in Georgia.; 18) Theoretical Analysis of KL-regularized RLHF with Multiple Reference
  Models; Recent methods for aligning large language models (LLMs) with human feedback
predominantly rely on a single reference model, which limits diversity, model
overfitting, and underutilizes the wide range of available pre-trained models.
Incorporating multiple reference models has the potential to address these
limitations by broadening perspectives, reducing bias, and leveraging the
strengths of diverse open-source LLMs. However, integrating multiple reference
models into reinforcement learning with human feedback (RLHF) frameworks poses
significant theoretical challenges, particularly in reverse KL-regularization,
where achieving exact solutions has remained an open problem. This paper
presents the first \emph{exact solution} to the multiple reference model
problem in reverse KL-regularized RLHF. We introduce a comprehensive
theoretical framework that includes rigorous statistical analysis and provides
sample complexity guarantees. Additionally, we extend our analysis to forward
KL-regularized RLHF, offering new insights into sample complexity requirements
in multiple reference scenarios. Our contributions lay the foundation for more
advanced and adaptable LLM alignment techniques, enabling the effective use of
multiple reference models. This work paves the way for developing alignment
frameworks that are both theoretically sound and better suited to the
challenges of modern AI ecosystems.; 19) SNAKE: A Sustainable and Multi-functional Traffic Analysis System
  utilizing Specialized Large-Scale Models with a Mixture of Experts
  Architecture; The rapid advancement of internet technology has led to a surge in data
transmission, making network traffic classification crucial for security and
management. However, there are significant deficiencies in its efficiency for
handling multiattribute analysis and its ability to expand model knowledge,
making it difficult to adapt to the ever-changing network environment and
complex identification requirements. To address this issue, we proposed the
SNAKE (Sustainable Network Analysis with Knowledge Exploration) system, which
adopts a multi-gated mixture of experts architecture to construct a
multi-functional traffic classification model. The system analyzes traffic
attributes at different levels through multiple expert sub-models, providing
predictions for these attributes via gating and a final Tower network.
Additionally, through an intelligent gating configuration, the system enables
extremely fast model integration and evolution across various knowledge
expansion scenarios. Its excellent compatibility allows it to continuously
evolve into a multi-functional largescale model in the field of traffic
analysis. Our experimental results demonstrate that the SNAKE system exhibits
remarkable scalability when faced with incremental challenges in diverse
traffic classification tasks. Currently, we have integrated multiple models
into the system, enabling it to classify a wide range of attributes, such as
encapsulation usage, application types and numerous malicious behaviors. We
believe that SNAKE can pioneeringly create a sustainable and multifunctional
large-scale model in the field of network traffic analysis after continuous
expansion.; 20) The occurrence of powerful flares stronger than X10 class in Solar
  Cycles; Solar flares stronger than X10 (S-flares, >X10) are the highest class flares
which significantly impact on the Sun's evolution and space weather. Based on
observations of Geostationary Orbiting Environmental Satellites (GOES) at soft
X-ray (SXR) wavelength and the daily sunspot numbers (DSNs) since 1975, we
obtained some interesting and heuristic conclusions: (1) Both S-flares and the
more powerful extremely strong flares (ES-flares, >X14.3) mostly occur in the
late phases of solar cycles and low-latitude regions on the solar disk; (2)
Similar to X-class flares, the occurrence of S-flares in each solar cycle is
somewhat random, but the occurrence of ES-flares seems to be dominated by the
mean DSN (Vm) and its root-mean-square deviation during the valley phase (Vd)
before the cycle: the ES-flare number is strongly correlated with Vd, and the
occurrence time of the first ES-flare is anti-correlated with Vd and Vm. These
facts indicate that the higher the Vm and Vd, the stronger the solar cycle, the
more the ES-flares and the earlier they occurred. We proposed that the Sun may
have a low-latitude active zone (LAZ), and most ES-flares are generated from
the interaction between LAZ and the newly emerging active regions. The
correlations and the linear regression functions may provide an useful method
to predict the occurrence of ES-flares in an upcoming solar cycle, which
derives that solar cycle 25 will have about 2 ES-flares after the spring of
2027.; 21) Multispectral radiation temperature inversion based on
  Transformer-LSTM-SVM; The key challenge in multispectral radiation thermometry is accurately
measuring emissivity. Traditional constrained optimization methods often fail
to meet practical requirements in terms of precision, efficiency, and noise
resistance. However, the continuous advancement of neural networks in data
processing offers a potential solution to this issue. This paper presents a
multispectral radiation thermometry algorithm that combines Transformer, LSTM
(Long Short-Term Memory), and SVM (Support Vector Machine) to mitigate the
impact of emissivity, thereby enhancing accuracy and noise resistance. In
simulations, compared to the BP neural network algorithm, GIM-LSTM, and
Transformer-LSTM algorithms, the Transformer-LSTM-SVM algorithm demonstrates an
improvement in accuracy of 1.23%, 0.46% and 0.13%, respectively, without noise.
When 5% random noise is added, the accuracy increases by 1.39%, 0.51%, and
0.38%, respectively. Finally, experiments confirmed that the maximum
temperature error using this method is less than 1%, indicating that the
algorithm offers high accuracy, fast processing speed, and robust noise
resistance. These characteristics make it well-suited for real-time
high-temperature measurements with multi-wavelength thermometry equipment.; 22) Beyond Human Intervention: Algorithmic Collusion through Multi-Agent
  Learning Strategies; Collusion in market pricing is a concept associated with human actions to
raise market prices through artificially limited supply. Recently, the idea of
algorithmic collusion was put forward, where the human action in the pricing
process is replaced by automated agents. Although experiments have shown that
collusive market equilibria can be reached through such techniques, without the
need for human intervention, many of the techniques developed remain
susceptible to exploitation by other players, making them difficult to
implement in practice. In this article, we explore a situation where an agent
has a multi-objective strategy, and not only learns to unilaterally exploit
market dynamics originating from other algorithmic agents, but also learns to
model the behaviour of other agents directly. Our results show how common
critiques about the viability of algorithmic collusion in real-life settings
can be overcome through the usage of slightly more complex algorithms.; 23) Low 4.5 {\mu}m Dayside Emission Disfavors a Dark Bare-Rock scenario for
  the Hot Super-Earth TOI-431 b; The full range of conditions under which rocky planets can host atmospheres
remains poorly understood, especially in the regime of close-in orbits around
late-type stars. One way to assess the presence of atmospheres on rocky
exoplanets is to measure their dayside emission as they are eclipsed by their
host stars. Here, we present Spitzer observations of the 4.5 $\mu$m secondary
eclipses of the rocky super-Earth TOI-431 b, whose mass and radius indicate an
Earth-like bulk composition (3.07 $\pm$ 0.35 M$_{\oplus}$, 1.28 $\pm$ 0.04
R$_{\oplus}$). Exposed to more than 2000 times the irradiation of Earth,
dayside temperatures of up to 2400K are expected if the planet is a dark
bare-rock without a significant atmosphere. Intriguingly, despite the strong
stellar insolation, we measure a secondary eclipse depth of only 33 $\pm$ 22
ppm, which corresponds to a dayside brightness temperature of
$1520_{-390}^{+360}$K. This notably low eclipse depth disagrees with the dark
bare-rock scenario at the 2.5$\sigma$ level, and suggests either that the
planet is surrounded by an atmosphere, or that it is a bare-rock with a highly
reflective surface. In the atmosphere scenario, the low dayside emission
implies the efficient redistribution of heat to the nightside, or by molecular
absorption in the 4-5 $\mu$m bandpass. In the bare-rock scenario, a surface
composition made of a high-albedo mineral species such as ultramafic rock can
lead to reduced thermal emission consistent with low eclipse depth measurement.
Follow-up spectroscopic observations with the James Webb Space Telescope hold
the key to constraining the nature of the planet.; 24) A Circular Construction Product Ontology for End-of-Life Decision-Making; Efficient management of end-of-life (EoL) products is critical for advancing
circularity in supply chains, particularly within the construction industry
where EoL strategies are hindered by heterogenous lifecycle data and data
silos. Current tools like Environmental Product Declarations (EPDs) and Digital
Product Passports (DPPs) are limited by their dependency on seamless data
integration and interoperability which remain significant challenges. To
address these, we present the Circular Construction Product Ontology (CCPO), an
applied framework designed to overcome semantic and data heterogeneity
challenges in EoL decision-making for construction products. CCPO standardises
vocabulary and facilitates data integration across supply chain stakeholders
enabling lifecycle assessments (LCA) and robust decision-making. By aggregating
disparate data into a unified product provenance, CCPO enables automated EoL
recommendations through customisable SWRL rules aligned with European standards
and stakeholder-specific circularity SLAs, demonstrating its scalability and
integration capabilities. The adopted circular product scenario depicts CCPO's
application while competency question evaluations show its superior performance
in generating accurate EoL suggestions highlighting its potential to greatly
improve decision-making in circular supply chains and its applicability in
real-world construction environments.; 25) PredicateFix: Repairing Static Analysis Alerts with Bridging Predicates; Using Large Language Models (LLMs) to fix static analysis alerts in program
code is becoming increasingly popular and helpful. However, these models often
have the problem of hallucination and perform poorly for complex and less
common alerts, limiting their performance. Retrieval-augmented generation (RAG)
aims to solve this problem by providing the model with a relevant example, but
the unsatisfactory quality of such examples challenges the effectiveness of
existing approaches.
  To address this challenge, this paper utilizes the predicates in the analysis
rule, which can serve as a bridge between the alert and relevant code snippets
within a clean code corpus, called key examples. Based on the above insight, we
propose an algorithm to retrieve key examples for an alert automatically. Then,
we build PredicateFix as a RAG pipeline to fix alerts flagged by the CodeQL
code checker and another imperative static analyzer for Golang. Evaluation with
multiple LLMs shows that PredicateFix increases the number of correct repairs
by 27.1% ~ 72.5%, significantly outperforming other baseline RAG approaches.; 26) Acoustic Neural 3D Reconstruction Under Pose Drift; We consider the problem of optimizing neural implicit surfaces for 3D
reconstruction using acoustic images collected with drifting sensor poses. The
accuracy of current state-of-the-art 3D acoustic modeling algorithms is highly
dependent on accurate pose estimation; small errors in sensor pose can lead to
severe reconstruction artifacts. In this paper, we propose an algorithm that
jointly optimizes the neural scene representation and sonar poses. Our
algorithm does so by parameterizing the 6DoF poses as learnable parameters and
backpropagating gradients through the neural renderer and implicit
representation. We validated our algorithm on both real and simulated datasets.
It produces high-fidelity 3D reconstructions even under significant pose drift.; 27) Speculative Decoding for Multi-Sample Inference; We propose a novel speculative decoding method tailored for multi-sample
reasoning scenarios, such as self-consistency and Best-of-N sampling. Our
method exploits the intrinsic consensus of parallel generation paths to
synthesize high-quality draft tokens without requiring auxiliary models or
external databases. By dynamically analyzing structural patterns across
parallel reasoning paths through a probabilistic aggregation mechanism, it
identifies consensus token sequences that align with the decoding distribution.
Evaluations on mathematical reasoning benchmarks demonstrate a substantial
improvement in draft acceptance rates over baselines, while reducing the
latency in draft token construction. This work establishes a paradigm shift for
efficient multi-sample inference, enabling seamless integration of speculative
decoding with sampling-based reasoning techniques.; 28) Coherent spin dynamics of electrons and holes photogenerated with large
  kinetic energy in lead halide perovskite crystals; The coherent spin dynamics of electrons and holes are studied in a
FA0.9Cs0.1PbI2.8Br0.2 perovskite bulk crystal, using time-resolved Kerr
ellipticity in a two-color pump-probe scheme. The probe photon energy is tuned
to the exciton resonance, while the pump photon energy is detuned from it up to
0.75 eV to higher energies. The spin-oriented electrons and holes
photogenerated with significant excess kinetic energy relax into states in
vicinity of the band gap, where they undergo Larmor precession in an external
magnetic field. At cryogenic temperatures down to 1.6 K, the spin dephasing
time reaches the nanosecond range. During energy relaxation, the carrier spin
relaxation is inefficient and only happens when the carriers become localized.
In experiments with two pump pulses, all-optical control of the amplitudes and
phases of the electron and hole spin signals is achieved in the additive regime
by varying the intensities of the pump pulses and the time delay between them.; 29) Curated loci prime editing (cliPE) for accessible multiplexed assays of
  variant effect (MAVEs); Multiplexed assays of variant effect (MAVEs) perform simultaneous
characterization of many variants. Prime editing has been recently adopted for
introducing many variants in their native genomic contexts. However, robust
protocols and standards are limited, preventing widespread uptake. Herein, we
describe curated loci prime editing (cliPE) which is an accessible, low-cost
experimental pipeline to perform MAVEs using prime editing of a target gene, as
well as a companion Shiny app (pegRNA Designer) to rapidly and easily design
user-specific MAVE libraries.; 30) Practical parameter identifiability of respiratory mechanics in the
  extremely preterm infant; The complexity of mathematical models describing respiratory mechanics has
grown in recent years, however, parameter identifiability of such models has
only been studied in the last decade in the context of observable data. This
study investigates parameter identifiability of a nonlinear respiratory
mechanics model tuned to the physiology of an extremely preterm infant, using
global Morris screening, local deterministic sensitivity analysis, and singular
value decomposition-based subset selection. The model predicts airflow and
dynamic pulmonary volumes and pressures under varying levels of continuous
positive airway pressure, and a range of parameters characterizing both
surfactant-treated and surfactant-deficient lung. Sensitivity analyses
indicated eleven parameters influence model outputs over the range of
continuous positive airway pressure and lung health scenarios. The model was
adapted to data from a spontaneously breathing 1 kg infant using gradient-based
optimization to estimate the parameter subset characterizing the patient's
state of health.; 31) Adaptive Target Localization under Uncertainty using Multi-Agent Deep
  Reinforcement Learning with Knowledge Transfer; Target localization is a critical task in sensitive applications, where
multiple sensing agents communicate and collaborate to identify the target
location based on sensor readings. Existing approaches investigated the use of
Multi-Agent Deep Reinforcement Learning (MADRL) to tackle target localization.
Nevertheless, these methods do not consider practical uncertainties, like false
alarms when the target does not exist or when it is unreachable due to
environmental complexities. To address these drawbacks, this work proposes a
novel MADRL-based method for target localization in uncertain environments. The
proposed MADRL method employs Proximal Policy Optimization to optimize the
decision-making of sensing agents, which is represented in the form of an
actor-critic structure using Convolutional Neural Networks. The observations of
the agents are designed in an optimized manner to capture essential information
in the environment, and a team-based reward functions is proposed to produce
cooperative agents. The MADRL method covers three action dimensionalities that
control the agents' mobility to search the area for the target, detect its
existence, and determine its reachability. Using the concept of Transfer
Learning, a Deep Learning model builds on the knowledge from the MADRL model to
accurately estimating the target location if it is unreachable, resulting in
shared representations between the models for faster learning and lower
computational complexity. Collectively, the final combined model is capable of
searching for the target, determining its existence and reachability, and
estimating its location accurately. The proposed method is tested using a
radioactive target localization environment and benchmarked against existing
methods, showing its efficacy.; 32) The Layer Hall Effect without External Electric Field; The layer Hall effect is an intriguing phenomenon observed in magnetic
topological layered materials, where the Hall response arises from the opposite
deflection of electrons on top and bottom layers. To realize layer Hall effect,
space-time $\mathcal{PT}$ symmetry is typically broken by applying an external
electric field. In this work, we propose a new mechanism to realize the layer
Hall effect by introducing inequivalent exchange fields on both surfaces of a
topological insulator thin film, in the absence of an electric field. This
approach yields a distinct Hall response compared to the conventional
electric-field-induced layer Hall effect, particularly with respect to the
Fermi level. Taking the topological insulator Sb$_2$Te$_3$ as a concrete
example, we demonstrate the feasibility of inducing the layer Hall effect only
by coupling the top and bottom surfaces of Sb$_2$Te$_3$ with different magnetic
insulators. Notably, we show that both built-in electric-field-induced and
inequivalent exchange-fields-induced layer Hall effects can be achieved by
tuning the stacking order between Sb$_2$Te$_3$ and the magnetic layers. Given
the well-established experimental techniques for fabricating topological
insulator thin films, our work offers a viable pathway for realizing layer Hall
effect without external electric field.; 33) Generalized Factor Neural Network Model for High-dimensional Regression; We tackle the challenges of modeling high-dimensional data sets, particularly
those with latent low-dimensional structures hidden within complex, non-linear,
and noisy relationships. Our approach enables a seamless integration of
concepts from non-parametric regression, factor models, and neural networks for
high-dimensional regression. Our approach introduces PCA and Soft PCA layers,
which can be embedded at any stage of a neural network architecture, allowing
the model to alternate between factor modeling and non-linear transformations.
This flexibility makes our method especially effective for processing
hierarchical compositional data. We explore ours and other techniques for
imposing low-rank structures on neural networks and examine how architectural
design impacts model performance. The effectiveness of our method is
demonstrated through simulation studies, as well as applications to forecasting
future price movements of equity ETF indices and nowcasting with macroeconomic
data.; 34) Introspective Loop Closure for SLAM with 4D Imaging Radar; Simultaneous Localization and Mapping (SLAM) allows mobile robots to navigate
without external positioning systems or pre-existing maps. Radar is emerging as
a valuable sensing tool, especially in vision-obstructed environments, as it is
less affected by particles than lidars or cameras. Modern 4D imaging radars
provide three-dimensional geometric information and relative velocity
measurements, but they bring challenges, such as a small field of view and
sparse, noisy point clouds. Detecting loop closures in SLAM is critical for
reducing trajectory drift and maintaining map accuracy. However, the
directional nature of 4D radar data makes identifying loop closures, especially
from reverse viewpoints, difficult due to limited scan overlap. This article
explores using 4D radar for loop closure in SLAM, focusing on similar and
opposing viewpoints. We generate submaps for a denser environment
representation and use introspective measures to reject false detections in
feature-degenerate environments. Our experiments show accurate loop closure
detection in geometrically diverse settings for both similar and opposing
viewpoints, improving trajectory estimation with up to 82 % improvement in ATE
and rejecting false positives in self-similar environments.; 35) Anomaly Matching in 6d $\mathcal{N}=(2,0)$ SCFTs from M5 Cobordism; We investigate the anomalies of 6d $\mathcal{N} = (2, 0)$ superconformal
field theories for any ADE gauge group using the modern characterization of
anomalies by cobordism. We propose that, in order to account for all features
of the anomaly, a bordism theory with exotic tangential structure is needed. We
then attempt to match the anomaly in the IR effective theory on the tensor
branch with a suitable WZW term. Once again, we show that the exotic bordism
structure is necessary to achieve this. Our results suggest a natural
explanation for the origin of the Hopf-Wess-Zumino term.; 36) Searching for long-lived particles from stopped pions and muons at the
  CiADS-BDE; The CiADS-BDE is a beam-dump experiment recently proposed for searching for
light, long-lived particles (LLPs) at China initiative Accelerator Driven
System. Primarily thanks to the large numbers of protons on target at the
experiment, it has been shown to be sensitive to large, unique regions of the
parameter space of dark photon, with a small detector volume of
$\mathcal{O}(0.01\text{--}1)$ m$^3$. Here, we explore the search prospect of
the CiADS-BDE for a series of new-physic models predicting LLPs that could
emanate from decays at rest of charged pions and muons at the facility, namely,
heavy neutral leptons, axionlike particles, and light binos in the
R-parity-violating supersymmetry. For these benchmark models, we find that the
CiADS-BDE can also probe vast parameter regions beyond the existing bounds.; 37) SafeAuto: Knowledge-Enhanced Safe Autonomous Driving with Multimodal
  Foundation Models; Traditional autonomous driving systems often struggle to integrate high-level
reasoning with low-level control, resulting in suboptimal and sometimes unsafe
driving behaviors. The emergence of Multimodal Large Language Models (MLLMs),
which can process both visual and textual data, presents an opportunity to
unify perception and reasoning tasks within a single framework. However,
effectively embedding precise safety knowledge into MLLMs for autonomous
driving remains a significant challenge. To address this, we propose SafeAuto,
a novel framework that enhances MLLM-based autonomous driving systems by
incorporating both unstructured and structured knowledge. Specifically, we
first introduce the Position-Dependent Cross-Entropy (PDCE) loss function,
designed to improve the accuracy of low-level control signal predictions when
numerical values are represented as text. Second, to ensure safe autonomous
driving by explicitly integrating precise safety knowledge into the MLLM, we
develop a reasoning component for SafeAuto. This component translates driving
safety regulations into first-order logic rules (e.g., ""red light => stop"") and
incorporates these rules into a probabilistic graphical model, such as a Markov
Logic Network (MLN). The MLN is trained to verify the predicted next actions
using environmental attributes identified by attribute recognition models
(e.g., detecting a red light) to form the predicates. Additionally, we
construct a Multimodal RAG model that leverages video data, control signals,
and environmental attributes to learn more effectively from past similar
driving experiences. By integrating PDCE, MLN, and Multimodal RAG, SafeAuto
significantly outperforms existing baselines across multiple datasets. This
advancement enables more accurate, reliable, and safer autonomous driving
systems that learn from experience, obey traffic laws, and perform precise
control actions.; 38) GeoSim.AI: AI assistants for numerical simulations in geomechanics; The ability to accomplish tasks via natural language instructions is one of
the most efficient forms of interaction between humans and technology. This
efficiency has been translated into practical applications with generative AI
tools now allowing users to get things done through natural language queries.
The emergence of advanced Large Language Models (LLMs) marks a pivotal shift in
this direction. With ongoing advancements in the field of generative AI,
integrating natural language commands into sophisticated technical fields in
science and engineering is becoming increasingly feasible. This paper
introduces GeoSim.AI - a suite of AI assistants for numerical simulations in
geomechanics - thereby demonstrating the transformative potential of generative
AI in geotechnical engineering. We investigate how AI assistants powered by
LLMs can streamline the process of creating complex simulation inputs and
interpreting results by translating natural language instructions or image
inputs into precise technical commands and scripts. This approach aims to
bridge the gap between human intent and the intricate requirements of numerical
modeling tools, potentially revolutionizing how researchers and engineers
interact with simulation software. We present demonstrations involving AI
assistants for performing slope stability analyses in various software
packages. The demonstrations highlight the potential of this technology to
significantly enhance productivity and accessibility in computational
geomechanics. GeoSim.AI is under active development, continuously expanding the
suite of AI assistants for various numerical simulation problems in
geotechnical engineering.; 39) Can a Single Model Master Both Multi-turn Conversations and Tool Use?
  CoALM: A Unified Conversational Agentic Language Model; Large Language Models (LLMs) with API-calling capabilities enabled building
effective Language Agents (LA), while also revolutionizing the conventional
task-oriented dialogue (TOD) paradigm. However, current approaches face a
critical dilemma: TOD systems are often trained on a limited set of target
APIs, requiring new data to maintain their quality when interfacing with new
services, while LAs are not trained to maintain user intent over multi-turn
conversations. Because both robust multi-turn management and advanced function
calling are crucial for effective conversational agents, we evaluate these
skills on three popular benchmarks: MultiWOZ 2.4 (TOD), BFCL V3 (LA), and
API-Bank (LA), and our analyses reveal that specialized approaches excel in one
domain but underperform in the other. To bridge this chasm, we introduce CoALM
(Conversational Agentic Language Model), a unified approach that integrates
both conversational and agentic capabilities. We created CoALM-IT, a carefully
constructed multi-task dataset that interleave multi-turn ReAct reasoning with
complex API usage. Using CoALM-IT, we train three models CoALM 8B, CoALM 70B,
and CoALM 405B, which outperform top domain-specific models, including GPT-4o,
across all three benchmarks. This demonstrates the feasibility of a single
model approach for both TOD and LA, setting a new standard for conversational
agents.; 40) Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for
  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering; In osteochondral tissue engineering (OCTE), simultaneously regenerating
subchondral bone and cartilage tissue presents a significant challenge.
Multiphasic scaffolds were created and manufactured using 3D printing to
address this issue. Excellent interfacial mechanical properties and
biocompatibility enhance the growth and chondrogenic differentiation of bone
marrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is
mimicked by incorporating varying concentrations of graphene oxide (GO) (0%,
1%, and 2% w/v) into a bioink composed of alginate (Alg) and gelatin (Gel).
Based on evaluations of mechanical and biocompatibility properties, 1% GO is
selected for further studies. Subsequently, the GO concentration is kept
constant while varying the platelet-rich plasma (PRP) dosage in the multiphasic
scaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w/v) are integrated into
the Alg-Gel bioink to simulate cartilage tissues. Results indicate that
3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical
properties, with no significant differences observed. However, BM-MSCs exposed
to 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,
real-time PCR and Alcian blue staining confirm increased chondrogenic
expression and glycosaminoglycans (GAGs) synthesis. This work highlights the
promising potential of 3D-printed multiphasic frameworks in the development of
OCTE.; 41) RAGO: Systematic Performance Optimization for Retrieval-Augmented
  Generation Serving; Retrieval-augmented generation (RAG), which combines large language models
(LLMs) with retrievals from external knowledge databases, is emerging as a
popular approach for reliable LLM serving. However, efficient RAG serving
remains an open challenge due to the rapid emergence of many RAG variants and
the substantial differences in workload characteristics across them. In this
paper, we make three fundamental contributions to advancing RAG serving. First,
we introduce RAGSchema, a structured abstraction that captures the wide range
of RAG algorithms, serving as a foundation for performance optimization.
Second, we analyze several representative RAG workloads with distinct
RAGSchema, revealing significant performance variability across these
workloads. Third, to address this variability and meet diverse performance
requirements, we propose RAGO (Retrieval-Augmented Generation Optimizer), a
system optimization framework for efficient RAG serving. Our evaluation shows
that RAGO achieves up to a 2x increase in QPS per chip and a 55% reduction in
time-to-first-token latency compared to RAG systems built on LLM-system
extensions.; 42) Safe LLM-Controlled Robots with Formal Guarantees via Reachability
  Analysis; The deployment of Large Language Models (LLMs) in robotic systems presents
unique safety challenges, particularly in unpredictable environments. Although
LLMs, leveraging zero-shot learning, enhance human-robot interaction and
decision-making capabilities, their inherent probabilistic nature and lack of
formal guarantees raise significant concerns for safety-critical applications.
Traditional model-based verification approaches often rely on precise system
models, which are difficult to obtain for real-world robotic systems and may
not be fully trusted due to modeling inaccuracies, unmodeled dynamics, or
environmental uncertainties. To address these challenges, this paper introduces
a safety assurance framework for LLM-controlled robots based on data-driven
reachability analysis, a formal verification technique that ensures all
possible system trajectories remain within safe operational limits. Our
framework specifically investigates the problem of instructing an LLM to
navigate the robot to a specified goal and assesses its ability to generate
low-level control actions that successfully guide the robot safely toward that
goal. By leveraging historical data to construct reachable sets of states for
the robot-LLM system, our approach provides rigorous safety guarantees against
unsafe behaviors without relying on explicit analytical models. We validate the
framework through experimental case studies in autonomous navigation and task
planning, demonstrating its effectiveness in mitigating risks associated with
LLM-generated commands. This work advances the integration of formal methods
into LLM-based robotics, offering a principled and practical approach to
ensuring safety in next-generation autonomous systems.; 43) SafeSpeech: A Comprehensive and Interactive Tool for Analysing Sexist
  and Abusive Language in Conversations; Detecting toxic language including sexism, harassment and abusive behaviour,
remains a critical challenge, particularly in its subtle and context-dependent
forms. Existing approaches largely focus on isolated message-level
classification, overlooking toxicity that emerges across conversational
contexts. To promote and enable future research in this direction, we introduce
SafeSpeech, a comprehensive platform for toxic content detection and analysis
that bridges message-level and conversation-level insights. The platform
integrates fine-tuned classifiers and large language models (LLMs) to enable
multi-granularity detection, toxic-aware conversation summarization, and
persona profiling. SafeSpeech also incorporates explainability mechanisms, such
as perplexity gain analysis, to highlight the linguistic elements driving
predictions. Evaluations on benchmark datasets, including EDOS, OffensEval, and
HatEval, demonstrate the reproduction of state-of-the-art performance across
multiple tasks, including fine-grained sexism detection.; 44) Gotham Dataset 2025: A Reproducible Large-Scale IoT Network Dataset for
  Intrusion Detection and Security Research; In this paper, a dataset of IoT network traffic is presented. Our dataset was
generated by utilising the Gotham testbed, an emulated large-scale Internet of
Things (IoT) network designed to provide a realistic and heterogeneous
environment for network security research. The testbed includes 78 emulated IoT
devices operating on various protocols, including MQTT, CoAP, and RTSP. Network
traffic was captured in Packet Capture (PCAP) format using tcpdump, and both
benign and malicious traffic were recorded. Malicious traffic was generated
through scripted attacks, covering a variety of attack types, such as Denial of
Service (DoS), Telnet Brute Force, Network Scanning, CoAP Amplification, and
various stages of Command and Control (C&C) communication. The data were
subsequently processed in Python for feature extraction using the Tshark tool,
and the resulting data was converted to Comma Separated Values (CSV) format and
labelled. The data repository includes the raw network traffic in PCAP format
and the processed labelled data in CSV format. Our dataset was collected in a
distributed manner, where network traffic was captured separately for each IoT
device at the interface between the IoT gateway and the device. Our dataset was
collected in a distributed manner, where network traffic was separately
captured for each IoT device at the interface between the IoT gateway and the
device. With its diverse traffic patterns and attack scenarios, this dataset
provides a valuable resource for developing Intrusion Detection Systems and
security mechanisms tailored to complex, large-scale IoT environments. The
dataset is publicly available at Zenodo.; 45) Fisher-Guided Selective Forgetting: Mitigating The Primacy Bias in Deep
  Reinforcement Learning; Deep Reinforcement Learning (DRL) systems often tend to overfit to early
experiences, a phenomenon known as the primacy bias (PB). This bias can
severely hinder learning efficiency and final performance, particularly in
complex environments. This paper presents a comprehensive investigation of PB
through the lens of the Fisher Information Matrix (FIM). We develop a framework
characterizing PB through distinct patterns in the FIM trace, identifying
critical memorization and reorganization phases during learning. Building on
this understanding, we propose Fisher-Guided Selective Forgetting (FGSF), a
novel method that leverages the geometric structure of the parameter space to
selectively modify network weights, preventing early experiences from
dominating the learning process. Empirical results across DeepMind Control
Suite (DMC) environments show that FGSF consistently outperforms baselines,
particularly in complex tasks. We analyze the different impacts of PB on actor
and critic networks, the role of replay ratios in exacerbating the effect, and
the effectiveness of even simple noise injection methods. Our findings provide
a deeper understanding of PB and practical mitigation strategies, offering a
FIM-based geometric perspective for advancing DRL.; 46) Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative
  with broad processability and staining-free antibacterial capability; Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free
strategy for broad-spectrum infection control in chronic wounds, minimising
bacterial resistance risks. However, rapid photosensitiser diffusion, tissue
staining, side toxicity, and short-lived antimicrobial effects present
significant clinical limitations for integrating APDT into wound dressings. To
address these challenges, we present the design of a bespoke polyvinyl alcohol
(PVA) derivative conjugated with both phenothiazine and methacrylate
functionalities, enabling staining-free antibacterial photodynamic effects,
cellular tolerability and processability into various wound dressing formats,
including films, textile fibres and nanoscale coatings. Tosylation of PVA is
leveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis
spectroscopy and the minimal release of TB in vitro. UV-induced network
formation is exploited to accomplish cast films and nanoscale integrated wound
dressing coatings. UV curing is also successfully coupled with an in-house wet
spinning process to realise individual, water-insoluble fibres as the building
blocks of fibrous wound dressings. A fluorometric assay supports the generation
of reactive oxygen species when the UV-cured samples are exposed to work, but
not UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and
the complete eradication of P. aeruginosa. Direct and extract cytotoxicity
tests with UV-cured films and fibres demonstrate the viability of L929
fibroblasts following 60-min light irradiation and 72-hour cell culture. The
bespoke molecular architecture, broad processability and cellular tolerability
of this PVA derivative are highly attractive aiming to integrate durable
staining-free photodynamic capability in a wide range of healthcare
technologies, from chronic wound dressings up to minimally invasive localised
therapy.; 47) Quantum simulation of a class of highly-oscillatory transport equations
  via Schr\""odingerisation; In this paper, we present quantum algorithms for a class of
highly-oscillatory transport equations, which arise in semiclassical
computation of surface hopping problems and other related non-adiabatic quantum
dynamics, based on the Born-Oppenheimer approximation. Our method relies on the
classical nonlinear geometric optics method, and the recently developed
Schr\""odingerisation approach for quantum simulation of partial differential
equations. The Schr\""odingerisation technique can transform any linear ordinary
and partial differential equations into Hamiltonian systems evolving under
unitary dynamics, via a warped phase transformation that maps these equations
to one higher dimension. We study possible paths for better recoveries of the
solution to the original problem by shifting the bad eigenvalues in the
Schr\""odingerized system. Our method ensures the uniform error estimates
independent of the wave length, thus allowing numerical accuracy, in maximum
norm, even without numerically resolving the physical oscillations. Various
numerical experiments are performed to demonstrate the validity of this
approach.; 48) DataSciBench: An LLM Agent Benchmark for Data Science; This paper presents DataSciBench, a comprehensive benchmark for evaluating
Large Language Model (LLM) capabilities in data science. Recent related
benchmarks have primarily focused on single tasks, easily obtainable ground
truth, and straightforward evaluation metrics, which limits the scope of tasks
that can be evaluated. In contrast, DataSciBench is constructed based on a more
comprehensive and curated collection of natural and challenging prompts for
uncertain ground truth and evaluation metrics. We develop a semi-automated
pipeline for generating ground truth (GT) and validating evaluation metrics.
This pipeline utilizes and implements an LLM-based self-consistency and human
verification strategy to produce accurate GT by leveraging collected prompts,
predefined task types, and aggregate functions (metrics). Furthermore, we
propose an innovative Task - Function - Code (TFC) framework to assess each
code execution outcome based on precisely defined metrics and programmatic
rules. Our experimental framework involves testing 6 API-based models, 8
open-source general models, and 9 open-source code generation models using the
diverse set of prompts we have gathered. This approach aims to provide a more
comprehensive and rigorous evaluation of LLMs in data science, revealing their
strengths and weaknesses. Experimental results demonstrate that API-based
models outperform open-sourced models on all metrics and
Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced
models. We release all code and data at https://github.com/THUDM/DataSciBench.; 49) Large Language Models Are Innate Crystal Structure Generators; Crystal structure generation is fundamental to materials discovery, enabling
the prediction of novel materials with desired properties. While existing
approaches leverage Large Language Models (LLMs) through extensive fine-tuning
on materials databases, we show that pre-trained LLMs can inherently generate
stable crystal structures without additional training. Our novel framework
MatLLMSearch integrates pre-trained LLMs with evolutionary search algorithms,
achieving a 78.38% metastable rate validated by machine learning interatomic
potentials and 31.7% DFT-verified stability via quantum mechanical
calculations, outperforming specialized models such as CrystalTextLLM. Beyond
crystal structure generation, we further demonstrate that our framework can be
readily adapted to diverse materials design tasks, including crystal structure
prediction and multi-objective optimization of properties such as deformation
energy and bulk modulus, all without fine-tuning. These results establish
pre-trained LLMs as versatile and effective tools for materials discovery,
opening up new venues for crystal structure generation with reduced
computational overhead and broader accessibility.; 50) Asymptotics of the occupation measure defined on a nonnegative matrix of
  two-dimensional quasi-birth-and-death type; We consider a nonnegative matrix having the same block structure as that of
the transition probability matrix of a two-dimensional quasi-birth-and-death
process (2d-QBD process for short) and define two kinds of measure for the
nonnegative matrix. One corresponds to the mean number of visits to each state
before the 2d-QBD process starting from the level zero returns to the level
zero for the first time. The other corresponds to the probabilities that the
2d-QBD process starting from each state visits the level zero. We call the
former the occupation measure and the latter the hitting measure. We obtain
asymptotic properties of the occupation measure such as the asymptotic decay
rate in an arbitrary direction. Those of the hitting measure can be obtained
from the results for the occupation measure by using a kind of duality between
the two measures.; 51) SeqSeg: Learning Local Segments for Automatic Vascular Model
  Construction; Computational modeling of cardiovascular function has become a critical part
of diagnosing, treating and understanding cardiovascular disease. Most
strategies involve constructing anatomically accurate computer models of
cardiovascular structures, which is a multistep, time-consuming process. To
improve the model generation process, we herein present SeqSeg (sequential
segmentation): a novel deep learning based automatic tracing and segmentation
algorithm for constructing image-based vascular models. SeqSeg leverages local
U-Net-based inference to sequentially segment vascular structures from medical
image volumes. We tested SeqSeg on CT and MR images of aortic and aortofemoral
models and compared the predictions to those of benchmark 2D and 3D global
nnU-Net models, which have previously shown excellent accuracy for medical
image segmentation. We demonstrate that SeqSeg is able to segment more complete
vasculature and is able to generalize to vascular structures not annotated in
the training data.; 52) DEALing with Image Reconstruction: Deep Attentive Least Squares; State-of-the-art image reconstruction often relies on complex, highly
parameterized deep architectures. We propose an alternative: a data-driven
reconstruction method inspired by the classic Tikhonov regularization. Our
approach iteratively refines intermediate reconstructions by solving a sequence
of quadratic problems. These updates have two key components: (i) learned
filters to extract salient image features, and (ii) an attention mechanism that
locally adjusts the penalty of filter responses. Our method achieves
performance on par with leading plug-and-play and learned regularizer
approaches while offering interpretability, robustness, and convergent
behavior. In effect, we bridge traditional regularization and deep learning
with a principled reconstruction approach.; 53) New centrality measure: ksi-centrality; We introduce new centrality measures called ksi-centrality and normalized
ksi-centrality which defined the importance of vertex up to importance of its
neighbors. First, we show that normalized ksi-centrality can be rewritten in
terms of Laplacian matrix such that its expression will be similar to local
clustering coefficient. After that we introduce average normalized
ksi-coefficient and show that for random Erdos-Renyi graph it is almost the
same as average clustering coefficient. Also, it shows similar behavior to
clustering coefficient for Windmill and Wheel graphs. In the end, we show that
distributions of ksi-centrality and normalized ksi-centrality differentiate
networks based on real data from the artificial networks including small-world
networks Watts-Strogatz and Barabasi-Albert. In addition we show the connection
between normalized ksi-centrality and average normalized ksi-coefficient and
algebraic connectivity of a graph.; 54) An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.; 55) CP-Guard+: A New Paradigm for Malicious Agent Detection and Defense in
  Collaborative Perception; Collaborative perception (CP) is a promising method for safe connected and
autonomous driving, which enables multiple vehicles to share sensing
information to enhance perception performance. However, compared with
single-vehicle perception, the openness of a CP system makes it more vulnerable
to malicious attacks that can inject malicious information to mislead the
perception of an ego vehicle, resulting in severe risks for safe driving. To
mitigate such vulnerability, we first propose a new paradigm for malicious
agent detection that effectively identifies malicious agents at the feature
level without requiring verification of final perception results, significantly
reducing computational overhead. Building on this paradigm, we introduce
CP-GuardBench, the first comprehensive dataset provided to train and evaluate
various malicious agent detection methods for CP systems. Furthermore, we
develop a robust defense method called CP-Guard+, which enhances the margin
between the representations of benign and malicious features through a
carefully designed Dual-Centered Contrastive Loss (DCCLoss). Finally, we
conduct extensive experiments on both CP-GuardBench and V2X-Sim, and
demonstrate the superiority of CP-Guard+.; 56) ArtMentor: AI-Assisted Evaluation of Artworks to Explore Multimodal
  Large Language Models Capabilities; Can Multimodal Large Language Models (MLLMs), with capabilities in
perception, recognition, understanding, and reasoning, function as independent
assistants in art evaluation dialogues? Current MLLM evaluation methods, which
rely on subjective human scoring or costly interviews, lack comprehensive
coverage of various scenarios. This paper proposes a process-oriented
Human-Computer Interaction (HCI) space design to facilitate more accurate MLLM
assessment and development. This approach aids teachers in efficient art
evaluation while also recording interactions for MLLM capability assessment. We
introduce ArtMentor, a comprehensive space that integrates a dataset and three
systems to optimize MLLM evaluation. The dataset consists of 380 sessions
conducted by five art teachers across nine critical dimensions. The modular
system includes agents for entity recognition, review generation, and
suggestion generation, enabling iterative upgrades. Machine learning and
natural language processing techniques ensure the reliability of evaluations.
The results confirm GPT-4o's effectiveness in assisting teachers in art
evaluation dialogues. Our contributions are available at
https://artmentor.github.io/.; 57) Application of the Pathline Method to the Aircraft Reactor Experiment; In this work, a new numerical method for the transport of Delayed Neutron
Precursors (DNPs) is applied to the Aircraft Reactor Experiment (ARE). The
pathline method is based on the Method of Characteristics (MOC) and leverages
the pathlines of the liquid nuclear fuel to derive an integral form of the DNPs
balance equation. The method has previously been tested on the CNRS benchmark
and in a simplified 2D geometry where turbulent diffusivity was significant
compared to advection. Here, the pathline method is applied to a real-world
Molten Salt Reactor (MSR), the ARE. DNPs transport is implemented in the
framework of the coupling between neutron transport solver
APOLLO3\textregistered{} and computational fluid dynamics code TrioCFD, both
developed at the French Atomic and Energy Commission (CEA). The DNPs
concentration obtained with the pathline method were compared with those
previously computed by TrioCFD, highlighting the importance of recirculation of
fission products. The L-7 experiment was also replicated to demonstrate the
method's capability.; 58) One Extension to Explain Them All, One Parameter to Minimize $\chi^2$,
  One Framework to Bring Them All, and in One Model Bind Them; The increasing precision of Cosmic Microwave Background (CMB) observations
has unveiled significant tensions between different datasets, notably between
Planck and the Atacama Cosmology Telescope (ACT), as well as with the
late-Universe measurements of the Hubble constant. In this work, we explore a
variety of $\Lambda$CDM extensions to assess their ability to reconcile these
discrepancies. The statistical preference for these extensions remains
moderate, and imposing $n_s=1$ often worsens model performance. Our findings
highlight the limitations of incremental modifications to $\Lambda$CDM and
suggest that either more complex new physics or, more likely, improved
systematic understanding in the CMB sector may be required to fully address the
observed tensions. While CMB experiments are often considered the gold standard
of precision cosmology, our results reinforce that these measurements are not
immune to systematic uncertainties, which may be underestimated in current
analyses.; 59) The Effect of Shear-Thinning Rheology on the Dynamics and Pressure
  Distribution of a Single Rigid Ellipsoidal Particle in Viscous Fluid Flow; This paper evaluates the behavior of a single rigid ellipsoidal particle
suspended in homogenous viscous flow with a power-law Generalized Newtonian
Fluid (GNF) rheology using a custom-built finite element analysis (FEA)
simulation. The combined effects of the shear-thinning fluid rheology, the
particle aspect ratio, the initial particle orientation and the
shear-extensional rate factor in various homogenous flow regimes on the
particle's dynamics and surface pressure evolution are investigated. The
shear-thinning fluid behavior was found to modify the particle's trajectory and
alter the particle's kinematic response. Moreover, the pressure distribution
over the particle's surface is significantly reduced by the shear-thinning
fluid rheology. The FEA model is validated by comparing results of the
Newtonian case with results obtained from the well-known Jefferys analytical
model. Furthermore, Jefferys model is extended to define the particle's
trajectory in a special class of homogenous Newtonian flows with combined
extension and shear rate components typically found in axisymmetric nozzle flow
contractions. The findings provide an improved understanding of key transport
phenomenon related to physical processes involving fluid-structure interaction
(FSI) such as that which occurs within the flow-field developed during material
extrusion-deposition additive manufacturing of fiber reinforced polymeric
composites. These results provide insight into important microstructural
formations within the print beads.; 60) Random Bridges in Spaces of Growing Dimension; We investigate the limiting behaviour of the path of random bridges treated
as random sets in $\mathbb{R}^{d}$ with the Euclidean metric and the dimension
$d$ increasing to infinity. The main result states that. in the square
integrable case, the limit (in the Gromov-Hausdorff sense) is deterministic,
namely, it is $[0,1]$ equipped with the pseudo-metric $\sqrt{|t-s|(1-|t-s|)}$.
We also show that, in the heavy-tailed case with summands regularly varying of
order $\alpha \in (0,1)$, the limiting metric space has a random metric derived
from the bridge variant of a subordinator.; 61) Force-Based Robotic Imitation Learning: A Two-Phase Approach for
  Construction Assembly Tasks; The drive for efficiency and safety in construction has boosted the role of
robotics and automation. However, complex tasks like welding and pipe insertion
pose challenges due to their need for precise adaptive force control, which
complicates robotic training. This paper proposes a two-phase system to improve
robot learning, integrating human-derived force feedback. The first phase
captures real-time data from operators using a robot arm linked with a virtual
simulator via ROS-Sharp. In the second phase, this feedback is converted into
robotic motion instructions, using a generative approach to incorporate force
feedback into the learning process. This method's effectiveness is demonstrated
through improved task completion times and success rates. The framework
simulates realistic force-based interactions, enhancing the training data's
quality for precise robotic manipulation in construction tasks.; 62) Theme-Explanation Structure for Table Summarization using Large Language
  Models: A Case Study on Korean Tabular Data; This paper proposes the Theme-Explanation Structure-based Table Summarization
(Tabular-TX) pipeline designed to process tabular data efficiently. Tabular-TX
preprocesses tabular data by focusing on highlighted cells. It then generates
summary sentences following a structured format, where the Theme Part appears
as an adverbial phrase, and the Explanation Part follows as a predictive
clause. This approach enables tailored analysis by considering the structural
characteristics of tables and their comparability. Unlike conventional
fine-tuning approaches that require extensive labeled data and computational
resources, our method leverages In-Context Learning to dynamically adapt to
different table structures without additional training, ensuring efficient and
scalable table interpretation. Experimental results demonstrate that Tabular-TX
significantly outperforms conventional fine-tuning-based methods, particularly
in low-resource scenarios, by leveraging table structures and metadata more
effectively through structured prompts. The results confirm that Tabular-TX
enables more effective processing of complex tabular data. Furthermore, it
serves as a viable alternative for table-based question answering and
summarization tasks in resource-constrained environments.; 63) Integrating anatomy and electrophysiology in the healthy human heart:
  Insights from biventricular statistical shape analysis using universal
  coordinates; A cardiac digital twin is a virtual replica of a patient-specific heart,
mimicking its anatomy and physiology. A crucial step of building a cardiac
digital twin is anatomical twinning, where the computational mesh of the
digital twin is tailored to the patient-specific cardiac anatomy. In a number
of studies, the effect of anatomical variation on clinically relevant
functional measurements like electrocardiograms (ECGs) is investigated, using
computational simulations. While such a simulation environment provides
researchers with a carefully controlled ground truth, the impact of anatomical
differences on functional measurements in real-world patients remains
understudied. In this study, we develop a biventricular statistical shape model
and use it to quantify the effect of biventricular anatomy on ECG-derived and
demographic features, providing novel insights for the development of digital
twins of cardiac electrophysiology. To this end, a dataset comprising
high-resolution cardiac CT scans from 271 healthy individuals, including
athletes, is utilized. Furthermore, a novel, universal, ventricular
coordinate-based method is developed to establish lightweight shape
correspondence. The performance of the shape model is rigorously established,
focusing on its dimensionality reduction capabilities and the training data
requirements. Additionally, a comprehensive synthetic cohort is made available,
featuring ready-to-use biventricular meshes with fiber structures and
anatomical region annotations. These meshes are well-suited for
electrophysiological simulations.; 64) Controlling discrete semilinear wave equations toward flocks; In this work, we initiate the research on controlling nonlinear waves
propagating on lattices from a completely new perspective. We consider
nonlinear waves on a lattice as a system of interacting particles and study
their collective flocking behavior. By designing suitable feedback controls, we
show that any admissible flock can be reached within a finite amount of time.
Finally, we highlight the connection between our flocking problem and a
minimal-time problem in the framework of nonlinear Hamilton-Jacobi equations
and optimal control theory.; 65) Physically-Based Mesh Generation for Confined 3D Point Clouds Using
  Flexible Foil Models; We propose a method for constructing high-quality, closed-surface meshes from
confined 3D point clouds via a physically-based simulation of flexible foils
under spatial constraints. The approach integrates dynamic elasticity,
pressure-driven deformation, and adaptive snapping to fixed vertices, providing
a robust framework for realistic and physically accurate mesh creation.
Applications in computer graphics and computational geometry are discussed.; 66) Predicting Cascading Failures in Power Systems using Machine Learning; Cascading failure studies help assess and enhance the robustness of power
systems against severe power outages. Onset time is a critical parameter in the
analysis and management of power system stability and reliability, representing
the timeframe within which initial disturbances may lead to subsequent
cascading failures. In this paper, different traditional machine learning
algorithms are used to predict the onset time of cascading failures. The
prediction task is articulated as a multi-class classification problem,
employing machine learning algorithms. The results on the UIUC 150-Bus power
system data available publicly show high classification accuracy with Random
Forest. The hyperparameters of the Random Forest classifier are tuned using
Bayesian Optimization. This study highlights the potential of machine learning
models in predicting cascading failures, providing a foundation for the
development of more resilient power systems.; 67) Collaborative Evaluation of Deepfake Text with Deliberation-Enhancing
  Dialogue Systems; The proliferation of generative models has presented significant challenges
in distinguishing authentic human-authored content from deepfake content.
Collaborative human efforts, augmented by AI tools, present a promising
solution. In this study, we explore the potential of DeepFakeDeLiBot, a
deliberation-enhancing chatbot, to support groups in detecting deepfake text.
Our findings reveal that group-based problem-solving significantly improves the
accuracy of identifying machine-generated paragraphs compared to individual
efforts. While engagement with DeepFakeDeLiBot does not yield substantial
performance gains overall, it enhances group dynamics by fostering greater
participant engagement, consensus building, and the frequency and diversity of
reasoning-based utterances. Additionally, participants with higher perceived
effectiveness of group collaboration exhibited performance benefits from
DeepFakeDeLiBot. These findings underscore the potential of deliberative
chatbots in fostering interactive and productive group dynamics while ensuring
accuracy in collaborative deepfake text detection. \textit{Dataset and source
code used in this study will be made publicly available upon acceptance of the
manuscript.; 68) Asymptotic lengths of permutahedra and associahedra; We define asymptotic lengths for families of oriented polytopes. We show that
permutahedra with weak order orientations have asymptotic total length 1 and
associahedra with Tamari order orientations have asymptotic total length 1/2.; 69) Scalar Field Kantowski--Sachs Solutions in Teleparallel $F(T)$ Gravity; In this paper, we investigate time-dependent Kantowski--Sachs spherically
symmetric teleparallel $F(T)$ gravity with a scalar field source. We begin by
setting the exact field equations to be solved and solve conservation laws for
possible scalar field potential, $V\left(\phi\right)$, solutions. Then, we find
new non-trivial teleparallel $F(T)$ solutions by using power-law and
exponential ansatz for each potential case arising from conservation laws, such
as linear, quadratic, or logarithmic, to name a few. We find a general formula
allowing us to compute all possible new teleparallel $F(T)$ solutions
applicable for any scalar field potential and ansatz. Then, we apply this
formula and find a large number of exact and approximate new teleparallel
$F(T)$ solutions for several types of cases. Some new $F(T)$ solution classes
may be relevant for future cosmological applications, especially concerning
dark matter, dark energy quintessence, phantom energy leading to the Big Rip
event, and quintom models of physical processes.; 70) Tumor microenvironment (Part I): Tissue integrity in a rat model of
  peripheral neural cancer; ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are
thought to be a factor in the integrity of nerve tissues. In this report, we
attempted to trace the expression of ICAM-1, responsible for cell-to-cell
adhesion, and of MPZ, the main constituent of myelin sheath, in malignant
tissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells
(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of
the SNs were collected after 7, 14 and 21 days and compared to a sham-operated
group of rats (n = 6 each). Tissues were sectioned and histologically examined,
under light microscope, and stained for measuring the immunoreactivity of
ICAM-1 and MPZ under laser scanning microscope. The cancer model was
established, and the tumor growth was confirmed. ICAM-1 showed severe
decreases, proportional to the growing anaplastic cells, as compared to the
sham group. MPZ revealed, however, a distinct defensive pattern before
substantially decreasing in a comparison with sham. These results support the
notion that malignancies damage peripheral nerves and cause severe axonal
injury and loss of neuronal integrity, and clearly define the role of ICAM-1
and MPZ in safeguarding the nerve tissues.; 71) Squeeze Out Tokens from Sample for Finer-Grained Data Governance; Widely observed data scaling laws, in which error falls off as a power of the
training size, demonstrate the diminishing returns of unselective data
expansion. Hence, data governance is proposed to downsize datasets through
pruning non-informative samples. Yet, isolating the impact of a specific sample
on overall model performance is challenging, due to the vast computation
required for tryout all sample combinations. Current data governors circumvent
this complexity by estimating sample contributions through heuristic-derived
scalar scores, thereby discarding low-value ones. Despite thorough sample
sieving, retained samples contain substantial undesired tokens intrinsically,
underscoring the potential for further compression and purification. In this
work, we upgrade data governance from a 'sieving' approach to a 'juicing' one.
Instead of scanning for least-flawed samples, our dual-branch DataJuicer
applies finer-grained intra-sample governance. It squeezes out informative
tokens and boosts image-text alignments. Specifically, the vision branch
retains salient image patches and extracts relevant object classes, while the
text branch incorporates these classes to enhance captions. Consequently,
DataJuicer yields more refined datasets through finer-grained governance.
Extensive experiments across datasets demonstrate that DataJuicer significantly
outperforms existing DataSieve in image-text retrieval, classification, and
dense visual reasoning.; 72) Electrically driven resonant magnetization, spin-pumping and
  charge-to-spin conversion from chiral-spin modes at THz frequencies; Chiral-spin modes are collective excitations of a spin-orbit (SO) coupled
system that lead to resonances in many observables. Here we identify resonances
in ""cross-response"", i.e., electric-field induced magnetization and
magnetic-field induced electric currents, known also as the Edelstein effect
and its inverse, respectively. We show that the chiral-spin modes resonantly
enhance the electrically induced magnetization. As specific examples, we
consider a single-valley two-dimensional electron gas with Rashba or
Dresselhaus SO coupling and a two-valley Dirac system with proximity-induced
Rashba and valley-Zeeman SO couplings. We suggest an architecture for a
spin-pumping experiment based on THz excitation of chiral-spin modes, which
would demonstrate a resonant enhancement of charge-to-spin conversion.; 73) Distributed Nonparametric Estimation: from Sparse to Dense Samples per
  Terminal; Consider the communication-constrained problem of nonparametric function
estimation, in which each distributed terminal holds multiple i.i.d. samples.
Under certain regularity assumptions, we characterize the minimax optimal rates
for all regimes, and identify phase transitions of the optimal rates as the
samples per terminal vary from sparse to dense. This fully solves the problem
left open by previous works, whose scopes are limited to regimes with either
dense samples or a single sample per terminal. To achieve the optimal rates, we
design a layered estimation protocol by exploiting protocols for the parametric
density estimation problem. We show the optimality of the protocol using
information-theoretic methods and strong data processing inequalities, and
incorporating the classic balls and bins model. The optimal rates are immediate
for various special cases such as density estimation, Gaussian, binary, Poisson
and heteroskedastic regression models.; 74) Zeros of symmetric power period polynomials; Suppose that $k$ and $N$ are positive integers. Let $f$ be a newform on
$\Gamma_0(N)$ of weight $k$ with $L$-function $L_f(s)$.
  Previous works have studied the zeros of the period polynomial $r_f(z)$,
which is a generating function for the critical values of $L_f(s)$ and has a
functional equation relating $z$ and $-1/Nz$.
  In particular, $r_f(z)$ satisfies a version of the Riemann hypothesis: all of
its zeros are on the circle of symmetry
  $\{z \in \C \ : \ |z|=1/\sqrt{N}\}$.
  In this paper, for a positive integer $m$, we define a natural analogue of
$r_f(z)$ for the $m^{\operatorname{th}}$ symmetric power $L$-function of $f$
when $N$ is squarefree. Our analogue also has a functional equation relating
$z$ and $-1/Nz$. We prove the corresponding version of the Riemann hypothesis
when $k$ is large enough. Moreover, when
$k>2(\operatorname{log}_2(13e^{2\pi}/9)+m)+1$, we prove our result when $N$ is
large enough.; 75) Fluid Antenna Enabled Over-the-Air Federated Learning: Joint
  Optimization of Positioning, Beamforming, and User Selection; Over-the-air (OTA) federated learning (FL) effectively utilizes communication
bandwidth, yet it is vulnerable to errors during analog aggregation. While
removing users with unfavorable channel conditions can mitigate these errors,
it also reduces the available local training data for FL, which in turn hinders
the convergence rate of the training process. To tackle this issue, we propose
using fluid antenna (FA) techniques to enhance the degrees of freedom within
the channel space, ultimately boosting the convergence speed of FL training.
Moreover, we develop a novel approach that effectively coordinates uplink
receiver beamforming, user selection, and FA positioning to optimize the
convergence rate of OTA FL training in dynamic wireless environments. We
address this challenging stochastic optimization by reformulating it as a
mixed-integer programming problem by utilizing the training loss upper bound.
We then introduce a penalty dual decomposition (PDD) method to solve the
mixed-integer mixed programming problem. Experimental results indicate that
incorporating FA techniques significantly accelerates the training convergence
of FL and greatly surpasses conventional methods.; 76) Scalable Differentially Private Bayesian Optimization; In recent years, there has been much work on scaling Bayesian Optimization to
high-dimensional problems, for example hyperparameter tuning in large neural
network models. These scalable methods have been successful, finding high
objective values much more quickly than traditional global Bayesian
Optimization or random search-based methods. At the same time, these large
neural network models often use sensitive data, but preservation of
Differential Privacy has not scaled alongside these modern Bayesian
Optimization procedures. Here we develop a method to privately estimate
potentially high-dimensional parameter spaces using Gradient Informative
Bayesian Optimization. Our theoretical results prove that under suitable
conditions, our method converges exponentially fast to a ball around the
optimal parameter configuration. Moreover, regardless of whether the
assumptions are satisfied, we show that our algorithm maintains privacy and
empirically demonstrates superior performance to existing methods in the
high-dimensional hyperparameter setting.; 77) A Universal Transformer-Based Coarse-Grained Molecular Dynamics
  Framework for Protein Dynamics; We present a novel, universal, Transformer-based coarse-grained molecular
dynamics (CG-MD) framework for simulating protein dynamics. Our trained model
generalizes to all protein systems, regardless of sequence length or number of
chains. First, we extend a tree-structured protein representation to
accommodate multi-chain proteins, demonstrating sub-angstrom-level accuracy in
reconstructing a 169-amino-acid protein structure. Then, representing
collective variables as language-like sequences, we use a Transformer network
as a propagator for stochastic differential equations, generating MD
trajectories over 10,000 times faster than all-atom MD simulations. This single
trained model accurately simulates both single-chain and two-chain proteins,
and the generated trajectories closely resemble all-atom MD trajectories in
their RMSD profiles. With sufficient training data, we anticipate that our
model can achieve universality across all proteins, offering a ~10,000x
acceleration of MD simulations with high accuracy.; 78) Effect of a new type of healthy and live food supplement on osteoporosis
  blood parameters and induced rheumatoid arthritis in Wistar rats; Summary Osteoporosis is a skeletal disorder, characterized by a decrease in
bone strength and puts the individual at risk for fracture. On the other hand,
rheumatoid arthritis is a systemic disease of unknown etiology that causes
inflammation of the joints of the organs. Purpose Due to the destructive
effects of these diseases and its increasing prevalence and lack of appropriate
medication for treatment, the present study aimed to evaluate the therapeutic
effect of a new type of healthy and live food supplement on rheumatoid
arthritis and induced osteoporosis in rats. Methods In this research, healthy
and live food powder were synthesized by a new and green route. This organic
biomaterial was named NBS. The NBS food supplement had various vitamins, macro
and micro molecules, and ingredients. The new healthy and nutritious diet
showed that the use of this supplement led to the return of the parameters to
normal levels. Results The concentration of 12.5 mg/ kg showed the least
therapeutic effect and 50 mg/ kg had the highest therapeutic effect for
osteoporosis. The results of blood parameters involved in inflammation in both
healthy and patient groups showed that the use of complete adjuvant induction
causes joint inflammation. In the study of the interaction of the
concentrations, it was observed that the concentration of 50 mg/ kg had the
highest therapeutic effect against the disease in the studied mice. Conclusion
The results showed that the new healthy and viable supplement restores the
blood osteoporotic and rheumatoid factors of the mice to normal.; 79) Effects of Ru-doping on the magnetism of Ag3LiIr2O6, a candidate Kitaev
  quantum spin liquid; We report our investigations on Ag3LiIr1.4Ru0.6O6, which results from the Ru
substitution in the Kitaev quantum spin liquid candidate Ag3LiIr2O6. It
crystallizes in the monoclinic C2/m space group like its parent compound,
Ag3LiIr2O6. Our susceptibility measurements reveal an effective moment = 2.6
muB, which is higher than the moments of the parent compound and less than that
of the Ru-analog (Ag3LiRu2O6), suggesting the presence of magnetic Ir4+ (Jeff=
1/2) and Ru4+ (S=1). Bulk magnetic susceptibility suggests long-range order
(LRO)at T~20 K, whereas no clear signature is present in the heat capacity.
Likewise, there is a loss of the 7Li NMR spectral intensity around T~20 K as
expected at the onset of LRO, but a complete wipe-out is not seen in contrast
to the result in Ag3LiIr2O6. There is also a T~20 K anomaly in the 7Li NMR
relaxation rate and also a fall in the 7Li NMR shift with decreasing
temperature. These results suggest LRO at T~20 K in Ag3LiIr1.4Ru0.6O6. However,
at low-T below 10 K, we observe a power law variation in magnetic heat capacity
and spin lattice relaxation rate, temperature-independent-7K, and no further
loss of the 7Li NMR spectral intensity. These results might suggest the
persistence or stabilisation of a quantum spin liquid-like phase, perhaps from
a fraction of the sample in Ag3LiIr1.4Ru0.6O6 below 10 K. Our muon spin
relaxation measurements suggest ordering around 20 K, consistent with our other
probes. It appears that the main effect of Ru-substitution is to shift the LRO
to a higher temperature in comparison with Ag3LiIr2O6, though there are
signatures of a novel phase below about 10 K.; 80) A Survey on Lawvere's Fixed-Point Theorem; This paper provides an overview of Lawvere's Fixed-Point Theorem in category
theory and aims to detail the universal framework underlying self-reference and
recursive structures. First, we rigorously define fundamental concepts - such
as terminal objects, products, Cartesian Closed Categories, exponential
objects, evaluation maps, currying, and point-surjective morphisms - and
explain their intuitive meanings through concrete examples and commutative
diagrams. Based on these foundational notions, we derive key lemmas (the
universality of currying, the diagonal lemma, and the fixed-point construction
lemma) and integrate them to develop a proof of Lawvere's Fixed-Point Theorem.
Furthermore, we discuss the impact of this theorem on fixed-point combinators
in programming languages, type theory, and homotopy type theory, as well as
current research trends and open problems. In doing so, we clarify how the
abstract principle of self-reference contributes to a wide range of
applications in both mathematics and computational theory.; 81) Predictive Learning in Energy-based Models with Attractor Structures; Predictive models are highly advanced in understanding the mechanisms of
brain function. Recent advances in machine learning further underscore the
power of prediction for optimal representation in learning. However, there
remains a gap in creating a biologically plausible model that explains how the
neural system achieves prediction. In this paper, we introduce a framework that
employs an energy-based model (EBM) to capture the nuanced processes of
predicting observation after action within the neural system, encompassing
prediction, learning, and inference. We implement the EBM with a hierarchical
structure and integrate a continuous attractor neural network for memory,
constructing a biologically plausible model. In experimental evaluations, our
model demonstrates efficacy across diverse scenarios. The range of actions
includes eye movement, motion in environments, head turning, and static
observation while the environment changes. Our model not only makes accurate
predictions for environments it was trained on, but also provides reasonable
predictions for unseen environments, matching the performances of machine
learning methods in multiple tasks. We hope that this study contributes to a
deep understanding of how the neural system performs prediction.; 82) A new transcendence measure for the values of the exponential function
  at algebraic arguments; Let $P\in \mathbb Z[X]\setminus\{0\}$ be of degree $\delta\ge 1$ and usual
height $H\ge 1$, and let $\alpha\in \overline{\mathbb Q}^*$ be of degree $d\ge
2$. Mahler proved in 1931 the following transcendence measure for $e^\alpha$:
for any $\varepsilon\&gt;0$, there exists $c\&gt;0$ such that $\vert
P(e^\alpha)\vert\&gt;c/H^{\mu(d,\delta)+\varepsilon}$ where the exponent
$\mu(d,\delta)=(4d^2-2d)\delta+2d-1$. Zheng obtained a better result in 1991
with $\mu(d,\delta)=(4d^2-2d)\delta-1$. In this paper, we provide a new
explicit exponent $\mu(d,\delta)$ which improves on Zheng's transcendence
measure for all $\delta\ge 2$ and all $d\ge 2$. When $\delta=1$, we recover his
bound for all $d\ge 2$, which had in fact already been obtained by Kappe in
1966. Our improvement rests upon the optimization of an accessory parameter in
Siegel's classical determinant method applied to Hermite-Pad{\'e} approximants
to powers of the exponential function.; 83) Counting Imaginary Quadratic Fields with an Ideal Class Group of 5-rank
  at least 2; We prove that there are $\gg\frac{X^{\frac{1}{3}}}{(\log X)^2}$ imaginary
quadratic fields $k$ with discriminant $|d_k|\leq X$ and an ideal class group
of $5$-rank at least $2$. This improves a result of Byeon, who proved the lower
bound $\gg X^{\frac{1}{4}}$ in the same setting. We use a method of Howe,
Lepr\'{e}vost, and Poonen to construct a genus $2$ curve $C$ over $\mathbb{Q}$
such that $C$ has a rational Weierstrass point and the Jacobian of $C$ has a
rational torsion subgroup of $5$-rank $2$. We deduce the main result from the
existence of the curve $C$ and a quantitative result of Kulkarni and the second
author.; 84) Bridging high resolution sub-cellular imaging with physiologically
  relevant engineered tissues; While high-resolution microscopic techniques are crucial for studying
cellular structures in cell biology, obtaining such images from thick 3D
engineered tissues remains challenging. In this review, we explore advancements
in fluorescence microscopy, alongside the use of various fluorescent probes and
material processing techniques to address these challenges. We navigate through
the diverse array of imaging options available in tissue engineering field,
from wide field to super-resolution microscopy, so researchers can make more
informed decisions based on the specific tissue and cellular structures of
interest. Finally, we provide some recent examples of how traditional
limitations on obtaining high-resolution images on sub-cellular architecture
within 3D tissues have been overcome by combining imaging advancements with
innovative tissue engineering approaches.; 85) Geometric immunosuppression in CAR-T cell treatment: Insights from
  mathematical modeling; Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising
treatment for hematological malignancies, offering a targeted approach to
cancer treatment. Understanding the complexities of CAR-T cell therapy within
solid tumors poses challenges due to the intricate interactions within the
tumor microenvironment. Mathematical modeling may serve as a valuable tool to
unravel the dynamics of CAR-T cell therapy and improve its effectiveness in
solid tumors. This study aimed to investigate the impact of spatial aspects in
CAR-T therapy of solid tumors, utilizing cellular automata for modeling
purposes. Our main objective was to deepen our understanding of treatment
effects by analyzing scenarios with different spatial distributions and varying
the initial quantities of tumor and CAR-T cells. Tumor geometry significantly
influenced treatment efficacy in-silico, with notable differences observed
between tumors with block-like arrangements and those with sparse cell
distributions, leading to the concept of immune suppression due to geometrical
effects. This research delves into the intricate relationship between spatial
dynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting
the relevance of tumor geometry in the outcome of cellular immunotherapy
treatments. Our results provide a basis for improving the efficacy of CAR-T
cell treatments by combining them with other ones reducing the density of
compact tumor areas and thus opening access ways for tumor killing T-cells.; 86) Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental
  differences between male and female heart shapes; Sex-based differences in cardiovascular disease are well documented, yet the
precise nature and extent of these discrepancies in cardiac anatomy remain
incompletely understood. Traditional scaling models often fail to capture the
interplay of age, blood pressure, and body size, prompting a more nuanced
investigation. Here, we employ statistical shape modeling in a healthy subset
(n=456) of the UK Biobank to explore sex-specific variations in biventricular
anatomy. We reconstruct 3D meshes and perform multivariate analyses of shape
coefficients, controlling for age, blood pressure, and various body size
metrics. Our findings reveal that sex alone explains at least 25 percent of
morphological variability, with strong discrimination between men and women
(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the
most discriminative modes highlight pronounced differences in cardiac chamber
volumes, the anterior-posterior width of the right ventricle, and the relative
positioning of the cardiac chambers. These results underscore that sex has a
fundamental influence on cardiac morphology, which may have important clinical
implications for differing cardiac structural assessments in men and women.
Future work should investigate how these anatomical differences manifest in
various cardiovascular conditions, ultimately paving the way for more precise
risk stratification and personalized therapeutic strategies for both men and
women.; 87) Crystal skeletons: Combinatorics and axioms; Crystal skeletons were introduced by Maas-Gari\'epy in 2023 by contracting
quasi-crystal components in a crystal graph. On the representation theoretic
level, crystal skeletons model the expansion of Schur functions into Gessel's
quasisymmetric functions. Motivated by questions of Schur positivity, we
provide a combinatorial description of crystal skeletons, and prove many new
properties, including a conjecture by Maas-Gari\'epy that crystal skeletons
generalize dual equivalence graphs. We then present a new axiomatic approach to
crystal skeletons. We give three versions of the axioms based on
$GL_n$-branching, $S_n$-branching, and local axioms in analogy to the local
Stembridge axioms for crystals based on novel commutation relations.; 88) A stochastic maximum principle of mean-field type with monotonicity
  conditions; The objective of this paper is to weaken the Lipschitz condition to a
monotonicity condition and to study the corresponding Pontryagin stochastic
maximum principle (SMP) for a mean-field optimal control problem under
monotonicity conditions.The dynamics of the controlled state process is
governed by a mean-field stochastic differential equation (SDE) whose
coefficients depend not only on the control, the controlled state process
itself but also on its law, and in particular, these coefficients satisfy the
monotonicity condition with respect to both the controlled state process and
its distribution. The associated cost functional is also of mean-field type.
Under the assumption of a convex control domain we derive the SMP, which
provides a necessary optimality condition for control processes. Under
additional convexity assumptions on the Hamiltonian, we further prove that this
necessary condition is also a sufficient one. To achieve this, we first address
the challenges related to the existence and the uniqueness of solutions for
mean-field backward stochastic differential equations and mean-field SDEs whose
coefficients satisfy monotonicity conditions with respect to both the solution
as well as its distribution. On the other hand we also construct several
illustrative examples demonstrating the generality of our results compared to
existing literature.; 89) Fighter Jet Navigation and Combat using Deep Reinforcement Learning with
  Explainable AI; This paper presents the development of an Artificial Intelligence (AI) based
fighter jet agent within a customized Pygame simulation environment, designed
to solve multi-objective tasks via deep reinforcement learning (DRL). The jet's
primary objectives include efficiently navigating the environment, reaching a
target, and selectively engaging or evading an enemy. A reward function
balances these goals while optimized hyperparameters enhance learning
efficiency. Results show more than 80\% task completion rate, demonstrating
effective decision-making. To enhance transparency, the jet's action choices
are analyzed by comparing the rewards of the actual chosen action (factual
action) with those of alternate actions (counterfactual actions), providing
insights into the decision-making rationale. This study illustrates DRL's
potential for multi-objective problem-solving with explainable AI. Project page
is available at:
\href{https://github.com/swatikar95/Autonomous-Fighter-Jet-Navigation-and-Combat}{Project
GitHub Link}.; 90) Learning Control of Neural Sound Effects Synthesis from Physically
  Inspired Models; Sound effects model design commonly uses digital signal processing techniques
with full control ability, but it is difficult to achieve realism within a
limited number of parameters. Recently, neural sound effects synthesis methods
have emerged as a promising approach for generating high-quality and realistic
sounds, but the process of synthesizing the desired sound poses difficulties in
terms of control. This paper presents a real-time neural synthesis model guided
by a physically inspired model, enabling the generation of high-quality sounds
while inheriting the control interface of the physically inspired model. We
showcase the superior performance of our model in terms of sound quality and
control.; 91) GPU-accelerated LISA parameter estimation with full time domain response; We conduct the first full Bayesian inference analysis for LISA parameter
estimation incorporating the effects of subdominant harmonics and
spin-precession through a full time domain response. The substantial
computational demands of using time domain waveforms for LISA are significantly
mitigated by implementing a novel Python version of the IMRPhenomT family of
waveform models and the LISA response with GPU acceleration. This time domain
response alleviates the theoretical necessity of developing specific transfer
functions to approximate the LISA response in the Fourier domain for each
specific type of system and allows for the use of unequal arms configurations
and realistic LISA orbits. Our analysis includes a series of zero-noise
injections for a Massive Black Hole Binary with aligned and precessing spins.
We investigate the impact of including subdominant harmonics, compare equal and
unequal arm configurations, and analyze different Time-Delay-Interferometry
(TDI) configurations. We utilize full and uniform priors, with a lower
frequency cutoff of 0.1mHz, and a signal duration of approximately two months,
sampled every 5 seconds. The sampler is initialized based on Fisher estimates.
Our results demonstrate LISA capability to measure the two spin magnitudes and
the primary spin tilt angle, alongside sky localization, with percent-level
precision, while component masses are determined with sub-percent accuracy.; 92) Investigating Compositional Reasoning in Time Series Foundation Models; Large pre-trained time series foundation models (TSFMs) have demonstrated
promising zero-shot performance across a wide range of domains. However, a
question remains: Do TSFMs succeed solely by memorizing training patterns, or
do they possess the ability to reason? While reasoning is a topic of great
interest in the study of Large Language Models (LLMs), it is undefined and
largely unexplored in the context of TSFMs. In this work, inspired by language
modeling literature, we formally define compositional reasoning in forecasting
and distinguish it from in-distribution generalization. We evaluate the
reasoning and generalization capabilities of 23 popular deep learning
forecasting models on multiple synthetic and real-world datasets. Additionally,
through controlled studies, we systematically examine which design choices in
TSFMs contribute to improved reasoning abilities. Our study yields key insights
into the impact of TSFM architecture design on compositional reasoning and
generalization. We find that patch-based Transformers have the best reasoning
performance, closely followed by residualized MLP-based architectures, which
are 97\% less computationally complex in terms of FLOPs and 86\% smaller in
terms of the number of trainable parameters. Interestingly, in some zero-shot
out-of-distribution scenarios, these models can outperform moving average and
exponential smoothing statistical baselines trained on in-distribution data.
Only a few design choices, such as the tokenization method, had a significant
(negative) impact on Transformer model performance.; 93) Simplified model of immunotherapy for glioblastoma multiforme: cancer
  stem cells hypothesis perspective; Despite ongoing efforts in cancer research, a fully effective treatment for
glioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer
immunotherapy is one of the potential cure candidates, efforts have been made
to assess its effectiveness using mathematical modeling. In this paper, we
consider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),
which also takes into account the dynamics of cancer stem cells, i.e., the type
of cancer cells that are hypothesized to be largely responsible for cancer
recurrence. We modify the initial ODE system by applying simplifying
assumptions and analyze the existence and stability of steady states of the
obtained simplified model depending on the treatment levels.; 94) Frozonium: Freezing Anharmonicity in Floquet Superconducting Circuits; Floquet engineering is a powerful method that can be used to modify the
properties of interacting many-body Hamiltonians via the application of
periodic time-dependent drives. Here we consider the physics of an inductively
shunted superconducting Josephson junction in the presence of Floquet drives in
the fluxonium regime and beyond, which we dub the frozonium artificial atom. We
find that in the vicinity of special ratios of the drive amplitude and
frequency, the many-body dynamics can be tuned to that of an effectively linear
bosonic oscillator, with additional nonlinear corrections that are suppressed
in higher powers of the drive frequency. By analyzing the inverse participation
ratios between the time-evolved frozonium wavefunctions and the eigenbasis of a
linear oscillator, we demonstrate the ability to achieve a novel dynamical
control using a combination of numerical exact diagonalization and
Floquet-Magnus expansion. We discuss the physics of resonances between
quasi-energy states induced by the drive, and ways to mitigate their effects.
We also highlight the enhanced protection of frozonium against external sources
of noise present in experimental setups. This work lays the foundation for
future applications in quantum memory and bosonic quantum control using
superconducting circuits.; 95) EIT in V+ inverted $\Xi$ system using Rydberg state in thermal Rb atoms; Rydberg excitation using blue and IR transition is an advantageous path for
quantum computation in alkali elements. Aiming to stabilize the IR laser for
quantum computation, we study electromagnetically induced transparency (EIT)
spectrum using Rydberg state in V+inverted $\Xi$ system (${5S_{1/2}}$
$\rightarrow$ ${5P_{3/2}}$ and ${5S_{1/2}}$ $\rightarrow$ ${6P_{1/2}}$
$\rightarrow$ ${r=69D_{3/2}}$) in Rb vapour cell at room temperature. The probe
laser absorption at 780 nm is monitored in the presence of the two control
lasers at 421 nm and 1003 nm. In comparison to the previously studied inverted
$\Xi$ system, this system has a good signal-to-noise ratio even at room
temperature with similar linewidth (around $10$~MHz). We also observe
Autler-Towns splitting of the EIT due to the high power of probe and blue
control lasers. For completeness and comparison, we also study the EIT in an
inverted $\Xi$ system using $5S_{1/2}\rightarrow6P_{1/2}\rightarrow 69D_{3/2}$
transitions.; 96) Few-shot LLM Synthetic Data with Distribution Matching; As large language models (LLMs) advance, their ability to perform in-context
learning and few-shot language generation has improved significantly. This has
spurred using LLMs to produce high-quality synthetic data to enhance the
performance of smaller models like online retrievers or weak LLMs. However,
LLM-generated synthetic data often differs from the real data in key language
attributes (e.g., styles, tones, content proportions, etc.). As a result,
mixing these synthetic data directly with real data may distort the original
data distribution, potentially hindering performance improvements. To solve
this, we introduce SynAlign: a synthetic data generation and filtering
framework based on key attribute distribution matching. Before generation,
SynAlign employs an uncertainty tracker surrogated by the Gaussian Process
model to iteratively select data clusters distinct from selected ones as
demonstrations for new data synthesis, facilitating the efficient exploration
diversity of the real data. Then, a latent attribute reasoning method is
employed: the LLM summarizes linguistic attributes of demonstrations and then
synthesizes new data based on them. This approach facilitates synthesizing
diverse data with linguistic attributes that appear in real data.After
generation, the Maximum Mean Discrepancy is used as the objective function to
learn the sampling weight of each synthetic data, ensuring distribution
matching with the real data. Our experiments on multiple text prediction tasks
show significant performance improvements. We also conducted an online A/B test
on an online retriever to demonstrate SynAlign's effectiveness.; 97) Preconditioned Inexact Stochastic ADMM for Deep Model; The recent advancement of foundation models (FMs) has brought about a
paradigm shift, revolutionizing various sectors worldwide. The popular
optimizers used to train these models are stochastic gradient descent-based
algorithms, which face inherent limitations, such as slow convergence and
stringent assumptions for convergence. In particular, data heterogeneity
arising from distributed settings poses significant challenges to their
theoretical and numerical performance. This paper develops an algorithm, PISA
({P}reconditioned {I}nexact {S}tochastic {A}lternating Direction Method of
Multipliers), which enables scalable parallel computing and supports various
second-moment schemes. Grounded in rigorous theoretical guarantees, the
algorithm converges under the sole assumption of Lipschitz continuity of the
gradient, thereby removing the need for other conditions commonly imposed by
stochastic methods. This capability enables PISA to tackle the challenge of
data heterogeneity effectively. Comprehensive experimental evaluations for
training or fine-tuning diverse FMs, including vision models, large language
models, reinforcement learning models, generative adversarial networks, and
recurrent neural networks, demonstrate its superior numerical performance
compared to various state-of-the-art optimizers.; 98) I Predict Therefore I Am: Is Next Token Prediction Enough to Learn
  Human-Interpretable Concepts from Data?; The remarkable achievements of large language models (LLMs) have led many to
conclude that they exhibit a form of intelligence. This is as opposed to
explanations of their capabilities based on their ability to perform relatively
simple manipulations of vast volumes of data. To illuminate the distinction
between these explanations, we introduce a novel generative model that
generates tokens on the basis of human interpretable concepts represented as
latent discrete variables. Under mild conditions, even when the mapping from
the latent space to the observed space is non-invertible, we establish an
identifiability result: the representations learned by LLMs through next-token
prediction can be approximately modeled as the logarithm of the posterior
probabilities of these latent discrete concepts, up to an invertible linear
transformation. This theoretical finding not only provides evidence that LLMs
capture underlying generative factors, but also strongly reinforces the linear
representation hypothesis, which posits that LLMs learn linear representations
of human-interpretable concepts. Empirically, we validate our theoretical
results through evaluations on both simulation data and the Pythia, Llama, and
DeepSeek model families.; 99) A tumor-immune model of chronic myeloid leukemia with optimal
  immunotherapeutic protocols; The interactions between tumor cells and the immune system play a crucial
role in cancer evolution. In this study, we explore how these interactions
influence cancer progression by modeling the relationships among naive T cells,
effector T cells, and chronic myeloid leukemia cells. We examine the existence
of equilibria, the asymptotic stability of the positive steady state, and the
global stability of the tumor-free equilibrium. Additionally, we develop a
partial differential equation to describe the conditions under which the
concentration of cancer cells reaches a level that allows for effective control
of cancer evolution. Finally, we apply our proposed model to investigate
optimal treatment strategies that aim to minimize both the concentration of
cancer cells at the end of treatment and the accumulation of tumor burden, as
well as the cost associated with treatment during the intervention period. Our
study reveals an optimal therapeutic protocol using optimal control theory. We
perform numerical simulations to illustrate our theoretical results and to
explore the dynamic behavior of the system and optimal therapeutic protocols.
The simulations indicate that the optimal treatment strategy can be more
effective than a constant treatment approach, even when applying the same
treatment interval and total drug input.; 100) Tweezer-assisted subwavelength positioning of atomic arrays in an
  optical cavity; We present an experimental technique that enables the preparation of
defect-free arrays of 87Rb atoms within a microscopic high-finesse optical
standing-wave cavity. By employing optical tweezers, we demonstrate atom
positioning with a precision well below the cavity wavelength, a crucial
requirement for cavity-QED experiments in which maximum atom-cavity coupling
strength is required. We leverage our control capabilities to assemble an array
of up to seven atoms with an efficiency that exceeds previous probabilistic
methods by 4 orders of magnitude. The atoms are subsequently transferred from
the tweezer array to a two-dimensional intracavity optical lattice that offers
enhanced coherence for spin qubits while maintaining strong atom confinement.
Our system overcomes the efficiency limitations of previous probabilistic
loading techniques of cavity-coupled atom arrays and opens the path to
multiqubit quantum networks with atoms strongly coupled to optical cavities.",0.0,0.5012658353418871
2411.05236,applied,2411.05236-pos1-6,"DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT; Shannon capacity of signal transduction for multiple independent receptors; Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver.",2411.05236-pos2-6,"Channelrhodopsin-2, a directly light-gated cation-selective membrane channel; Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane α helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination.",6,"['1', '9', '3', '2', '4', '6', '10', '8', '5', '7']","The best candidate paper that matches the main paper on Visible Light Communication (VLC) is number 1. It discusses the complexity and performance of programming languages, which can contribute to the efficient design and implementation of VLC systems and improve communication protocols. Candidate 2, while interesting, focuses on laser-induced nuclear excitation, which is less relevant to the communication technology context. Other candidates mainly focus on biological, computational, or statistical topics that do not bridge the gap between VLC and practical implementation or communication protocols. Therefore, the top candidates reflect synergy in hardware and software optimization for VLC systems.","1) Closing a Source Complexity Gap between Chapel and HPX; A previous case study measured performance vs source-code complexity across
multiple languages. The case study identified Chapel and HPX provide similar
performance and code complexity. This paper is the result of initial steps
toward closing the source-code complexity gap between Chapel and HPX by using a
source-to-source compiler. The investigation assesses the single-machine
performance of both Chapel and Chplx applications across Arm and x86.; 2) Nuclear Excitation and Control Induced by Intense Vortex Laser; The existing intense laser-based approaches for nuclear excitation offer
ultrafast temporal resolution and high efficiency compared to traditional
accelerator probes. However, controlling nuclear properties such as spin and
magnetic moment remains an unprecedented challenge. Here, we put forward a
novel method for nuclear excitation and control induced by intense vortex
lasers. We develop a theory incorporating the orbital angular momentum (OAM) of
vortex laser within the nuclear hyperfine mixing framework. We find that
intense vortex laser can effectively excite hydrogen-like thorium-229 nucleus
and induce three-dimensional rotation of the nuclear magnetic moment. This
rotation arises from the localized electromagnetic field and new transition
channels excited by the vortex laser, and can be reconstructed through
radiation spectrum analysis. Moreover, the OAM of vortex laser enables the
chaotic system to exhibit topologically protected periodic patterns in nuclear
excitation and radiation, facilitating precise experimental measurements. Our
findings underscore the potential of vortex laser for high-precision nuclear
control and imaging, deepening our understanding of nuclear properties and
hyperfine structure, and advancing quantum information and nuclear
technologies.; 3) Fluorescence Phasor Analysis: Basic Principles and Biophysical
  Applications; Fluorescence is one of the most widely used techniques in biological
sciences. Its exceptional sensitivity and versatility make it a tool of first
choice for quantitative studies in biophysics. The concept of phasors,
originally introduced by Charles Steinmetz in the late 19th century for
analyzing alternating current circuits, has since found applications across
diverse disciplines, including fluorescence spectroscopy. The main idea behind
fluorescence phasors was posited by Gregorio Weber in 1981. By analyzing the
complementary nature of pulse and phase fluorometry data, he shows that two
magnitudes -- denoted as $G$ and $S$ -- derived from the frequency-domain
fluorescence measurements correspond to the real and imaginary part of the
Fourier transform of the fluorescence intensity in the time domain. This review
provides a historical perspective on how the concept of phasors originates and
how it integrates into fluorescence spectroscopy. We discuss their fundamental
algebraic properties, which enable intuitive model-free analysis of
fluorescence data despite the complexity of the underlying phenomena. Some
applications in biophysics illustrate the power of this approach in studying
diverse phenomena, including protein folding, protein interactions, phase
transitions in lipid mixtures and formation of high-order structures in nucleic
acids.; 4) Policy Regularization on Globally Accessible States in Cross-Dynamics
  Reinforcement Learning; To learn from data collected in diverse dynamics, Imitation from Observation
(IfO) methods leverage expert state trajectories based on the premise that
recovering expert state distributions in other dynamics facilitates policy
learning in the current one. However, Imitation Learning inherently imposes a
performance upper bound of learned policies. Additionally, as the environment
dynamics change, certain expert states may become inaccessible, rendering their
distributions less valuable for imitation. To address this, we propose a novel
framework that integrates reward maximization with IfO, employing F-distance
regularized policy optimization. This framework enforces constraints on
globally accessible states--those with nonzero visitation frequency across all
considered dynamics--mitigating the challenge posed by inaccessible states. By
instantiating F-distance in different ways, we derive two theoretical analysis
and develop a practical algorithm called Accessible State Oriented Policy
Regularization (ASOR). ASOR serves as a general add-on module that can be
incorporated into various RL approaches, including offline RL and off-policy
RL. Extensive experiments across multiple benchmarks demonstrate ASOR's
effectiveness in enhancing state-of-the-art cross-domain policy transfer
algorithms, significantly improving their performance.; 5) A density theorem for prime squares; Let $s\geq 8$ be an integer and $P$ be a set of primes with relative lower
density greater than $\sqrt{1-\min\{s,16\}/32}$. We prove that every
sufficiently large integer $n\equiv s({\rm mod}24)$ can be represented by a sum
of $s$ squares of primes in $P$.; 6) Channelrhodopsin-2, a directly light-gated cation-selective membrane channel; Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane α helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination.; 7) Fixed-Population Causal Inference for Models of Equilibrium; In contrast to problems of interference in (exogenous) treatments, models of
interference in unit-specific (endogenous) outcomes do not usually produce a
reduced-form representation where outcomes depend on other units' treatment
status only at a short network distance, or only through a known exposure
mapping. This remains true if the structural mechanism depends on outcomes of
peers only at a short network distance, or through a known exposure mapping. In
this paper, we first define causal estimands that are identified and estimable
from a single experiment on the network under minimal assumptions on the
structure of interference, and which represent average partial causal responses
which generally vary with other global features of the realized assignment.
Under a fixed-population, design-based approach, we show unbiasedness and
consistency for inverse-probability weighting (IPW) estimators for those causal
parameters from a randomized experiment on a single network. We also analyze
more closely the case of marginal interventions in a model of equilibrium with
smooth response functions where we can recover LATE-type weighted averages of
derivatives of those response functions. Under additional structural
assumptions, these ""agnostic"" causal estimands can be combined to recover model
parameters, but also retain their less restrictive causal interpretation.; 8) Exact Algorithms for Minimum Dilation Triangulation; We provide a spectrum of new theoretical insights and practical results for
finding a Minimum Dilation Triangulation (MDT), a natural geometric
optimization problem of considerable previous attention: Given a set $P$ of $n$
points in the plane, find a triangulation $T$, such that a shortest Euclidean
path in $T$ between any pair of points increases by the smallest possible
factor compared to their straight-line distance. No polynomial-time algorithm
is known for the problem; moreover, evaluating the objective function involves
computing the sum of (possibly many) square roots. On the other hand, the
problem is not known to be NP-hard.
  (1) We provide practically robust methods and implementations for computing
an MDT for benchmark sets with up to 30,000 points in reasonable time on
commodity hardware, based on new geometric insights into the structure of
optimal edge sets. Previous methods only achieved results for up to $200$
points, so we extend the range of optimally solvable instances by a factor of
$150$.
  (2) We develop scalable techniques for accurately evaluating many
shortest-path queries that arise as large-scale sums of square roots, allowing
us to certify exact optimal solutions, with previous work relying on (possibly
inaccurate) floating-point computations.
  (3) We resolve an open problem by establishing a lower bound of $1.44116$ on
the dilation of the regular $84$-gon (and thus for arbitrary point sets),
improving the previous worst-case lower bound of $1.4308$ and greatly reducing
the remaining gap to the upper bound of $1.4482$ from the literature. In the
process, we provide optimal solutions for regular $n$-gons up to $n = 100$.; 9) Adaptive Hybrid FFT: A Novel Pipeline and Memory-Based Architecture for
  Radix-$2^k$ FFT in Large Size Processing; In the field of digital signal processing, the fast Fourier transform (FFT)
is a fundamental algorithm, with its processors being implemented using either
the pipelined architecture, well-known for high-throughput applications but
weak in hardware utilization, or the memory-based architecture, designed for
area-constrained scenarios but failing to meet stringent throughput
requirements. Therefore, we propose an adaptive hybrid FFT, which leverages the
strengths of both pipelined and memory-based architectures. In this paper, we
propose an adaptive hybrid FFT processor that combines the advantages of both
architectures, and it has the following features. First, a set of radix-$2^k$
multi-path delay commutators (MDC) units are developed to support
high-performance large-size processing. Second, a conflict-free memory access
scheme is formulated to ensure a continuous data flow without data contention.
Third, We demonstrate the existence of a series of bit-dimension permutations
for reordering input data, satisfying the generalized constraints of
variable-length, high-radix, and any level of parallelism for wide adaptivity.
Furthermore, the proposed FFT processor has been implemented on a
field-programmable gate array (FPGA). As a result, the proposed work
outperforms conventional memory-based FFT processors by requiring fewer
computation cycles. It achieves higher hardware utilization than pipelined FFT
architectures, making it suitable for highly demanding applications.; 10) DNRSelect: Active Best View Selection for Deferred Neural Rendering; Deferred neural rendering (DNR) is an emerging computer graphics pipeline
designed for high-fidelity rendering and robotic perception. However, DNR
heavily relies on datasets composed of numerous ray-traced images and demands
substantial computational resources. It remains under-explored how to reduce
the reliance on high-quality ray-traced images while maintaining the rendering
fidelity. In this paper, we propose DNRSelect, which integrates a reinforcement
learning-based view selector and a 3D texture aggregator for deferred neural
rendering. We first propose a novel view selector for deferred neural rendering
based on reinforcement learning, which is trained on easily obtained rasterized
images to identify the optimal views. By acquiring only a few ray-traced images
for these selected views, the selector enables DNR to achieve high-quality
rendering. To further enhance spatial awareness and geometric consistency in
DNR, we introduce a 3D texture aggregator that fuses pyramid features from
depth maps and normal maps with UV maps. Given that acquiring ray-traced images
is more time-consuming than generating rasterized images, DNRSelect minimizes
the need for ray-traced data by using only a few selected views while still
achieving high-fidelity rendering results. We conduct detailed experiments and
ablation studies on the NeRF-Synthetic dataset to demonstrate the effectiveness
of DNRSelect. The code will be released.; 11) Segment Any-Quality Images with Generative Latent Space Enhancement; Despite their success, Segment Anything Models (SAMs) experience significant
performance drops on severely degraded, low-quality images, limiting their
effectiveness in real-world scenarios. To address this, we propose GleSAM,
which utilizes Generative Latent space Enhancement to boost robustness on
low-quality images, thus enabling generalization across various image
qualities. Specifically, we adapt the concept of latent diffusion to SAM-based
segmentation frameworks and perform the generative diffusion process in the
latent space of SAM to reconstruct high-quality representation, thereby
improving segmentation. Additionally, we introduce two techniques to improve
compatibility between the pre-trained diffusion model and the segmentation
framework. Our method can be applied to pre-trained SAM and SAM2 with only
minimal additional learnable parameters, allowing for efficient optimization.
We also construct the LQSeg dataset with a greater diversity of degradation
types and levels for training and evaluating the model. Extensive experiments
demonstrate that GleSAM significantly improves segmentation robustness on
complex degradations while maintaining generalization to clear images.
Furthermore, GleSAM also performs well on unseen degradations, underscoring the
versatility of our approach and dataset.; 12) Bounds on a Wavefunction Overlap with Hamiltonian Eigen-states:
  Performance Guarantees for the Quantum Phase Estimation Algorithm; Estimating the overlap between an approximate wavefunction and a target
eigenstate of the system Hamiltonian is essential for the efficiency of quantum
phase estimation. In this work, we derive upper and lower bounds on this
overlap using expectation values of Hamiltonian powers and bounds on target
eigenenergies. The accuracy of these bounds can be systematically improved by
computing higher-order Hamiltonian moments and refining eigenenergy estimates.
Our method offers a practical approach to assessing initial state quality and
can be implemented on both classical and quantum computers.; 13) Constraining the Nuclear Equation of State of neutron star via
  high-frequency quasi-periodic oscillation in short gamma-ray bursts; The determination of the equation of state (EOS) of a neutron star (NS) and
its maximum mass is very important for understanding the formation and
properties of NSs under extreme conditions, but they remain open questions.
Short-duration gamma-ray bursts (GRBs) are believed to originate from the
merger of binary NSs or giant flares (GFs) of soft gamma repeaters (SGRs).
Recently, the high-frequency quasi-periodic oscillations (QPOs) have been
claimed to be identified from two short GRBs (GRB 931101B and GRB 910711). In
this paper, we propose that the observed high-frequency QPOs in these two short
GRBs result from torsional oscillations in the GFs of SGRs associated with cold
NSs, or from radial oscillations of hypermassive NSs as the hot remnants of
binary NS mergers, and then to constrain the EOS of NSs. For torsional
oscillations, the six selected EOSs (TM1, NL3, APR, SLy4, DDME2, and GM1) of
NSs suitable for the zero-temperature condition exhibit significant overlap in
mass ranges, suggesting that we cannot constrain the EOS of NSs. For radial
oscillations, the six selected EOSs (IUF, TM1, TMA, FSG, BHBLp, and NL3) of NSs
suitable for the high-temperature condition cannot be ruled out when redshift
is considered. However, it is found that the EOS can only be constrained if the
redshift and temperature of the remnant can be measured.; 14) REACT: Real-time Efficient Attribute Clustering and Transfer for
  Updatable 3D Scene Graph; Modern-day autonomous robots need high-level map representations to perform
sophisticated tasks. Recently, 3D scene graphs (3DSGs) have emerged as a
promising alternative to traditional grid maps, blending efficient memory use
and rich feature representation. However, most efforts to apply them have been
limited to static worlds. This work introduces REACT, a framework that
efficiently performs real-time attribute clustering and transfer to relocalize
object nodes in a 3DSG. REACT employs a novel method for comparing object
instances using an embedding model trained on triplet loss, facilitating
instance clustering and matching. Experimental results demonstrate that REACT
is able to relocalize objects while maintaining computational efficiency. The
REACT framework's source code will be available as an open-source project,
promoting further advancements in reusable and updatable 3DSGs.; 15) A 3-Step Optimization Framework with Hybrid Models for a Humanoid
  Robot's Jump Motion; High dynamic jump motions are challenging tasks for humanoid robots to
achieve environment adaptation and obstacle crossing. The trajectory
optimization is a practical method to achieve high-dynamic and explosive
jumping. This paper proposes a 3-step trajectory optimization framework for
generating a jump motion for a humanoid robot. To improve iteration speed and
achieve ideal performance, the framework comprises three sub-optimizations. The
first optimization incorporates momentum, inertia, and center of pressure
(CoP), treating the robot as a static reaction momentum pendulum (SRMP) model
to generate corresponding trajectories. The second optimization maps these
trajectories to joint space using effective Quadratic Programming (QP) solvers.
Finally, the third optimization generates whole-body joint trajectories
utilizing trajectories generated by previous parts. With the combined
consideration of momentum and inertia, the robot achieves agile forward jump
motions. A simulation and experiments (Fig. \ref{Fig First page fig}) of
forward jump with a distance of 1.0 m and 0.5 m height are presented in this
paper, validating the applicability of the proposed framework.; 16) DeepSilencer: A Novel Deep Learning Model for Predicting siRNA Knockdown
  Efficiency; Background: Small interfering RNA (siRNA) is a promising therapeutic agent
due to its ability to silence disease-related genes via RNA interference. While
traditional machine learning and early deep learning methods have made progress
in predicting siRNA efficacy, there remains significant room for improvement.
Advanced deep learning techniques can enhance prediction accuracy, reducing the
reliance on extensive wet-lab experiments and accelerating the identification
of effective siRNA sequences. This approach also provides deeper insights into
the mechanisms of siRNA efficacy, facilitating more targeted and efficient
therapeutic strategies.
  Methods: We introduce DeepSilencer, an innovative deep learning model
designed to predict siRNA knockdown efficiency. DeepSilencer utilizes advanced
neural network architectures to capture the complex features of siRNA
sequences. Our key contributions include a specially designed deep learning
model, an innovative online data sampling method, and an improved loss function
tailored for siRNA prediction. These enhancements collectively boost the
model's prediction accuracy and robustness.
  Results: Extensive evaluations on multiple test sets demonstrate that
DeepSilencer achieves state-of-the-art performance using only siRNA sequences
and basic physicochemical properties. Our model surpasses several other methods
and shows superior predictive performance, particularly when incorporating
thermodynamic parameters.
  Conclusion: The advancements in data sampling, model design, and loss
function significantly enhance the predictive capabilities of DeepSilencer.
These improvements underscore its potential to advance RNAi therapeutic design
and development, offering a powerful tool for researchers and clinicians.; 17) An Evaluation on the Role of Non-Coding RNA in HIV Transcription and
  Latency: A Review; The existence of latent cellular reservoirs is recognized as the major
barrier to an HIV cure. Reactivating and eliminating ""shock and kill"" or
permanently silencing ""block and lock"" the latent HIV reservoir, as well as
gene editing, remain promising approaches, but so far have proven to be only
partially successful. Moreover, using latency reversing agents or ""block and
lock"" drugs pose additional considerations, including the ability to cause
cellular toxicity, a potential lack of specificity for HIV, or low potency when
each agent is used alone. RNA molecules, such as microRNAs (miRNAs) and long
non-coding RNAs (lncRNAs) are becoming increasingly recognized as important
regulators of gene expression. RNA-based approaches for combatting HIV latency
represent a promising strategy since both miRNAs and lncRNAs are more cell-type
and tissue specific than protein coding genes. Thus, a higher specificity of
targeting the latent HIV reservoir with less overall cellular toxicity can
likely be achieved. In this review, we summarize current knowledge about HIV
gene expression regulation by miRNAs and lncRNAs encoded in the human genome,
as well as regulatory molecules encoded in the HIV genome. We discuss both the
transcriptional and post-transcriptional regulation of HIV gene expression to
align with the current definition of latency, and describe RNA molecules that
either promote HIV latency or have anti-latency properties. Finally, we provide
perspectives on using each class of RNAs as potential targets for combatting
HIV latency, and describe the complexity of the interactions between different
RNA molecules, their protein targets, and HIV.; 18) Think or Step-by-Step? UnZIPping the Black Box in Zero-Shot Prompts; Zero-shot prompting techniques have significantly improved the performance of
Large Language Models (LLMs). However, we lack a clear understanding of why
zero-shot prompts are so effective. For example, in the prompt ""Let's think
step-by-step,"" is ""think"" or ""step-by-step"" more crucial to its success?
Existing interpretability methods, such as gradient-based and attention-based
approaches, are computationally intensive and restricted to open-source models.
We introduce the ZIP score (Zero-shot Importance of Perturbation score), a
versatile metric applicable to both open and closed-source models, based on
systematic input word perturbations. Our experiments across four recent LLMs,
seven widely-used prompts, and several tasks, reveal interesting patterns in
word importance. For instance, while both 'step-by-step' and 'think' show high
ZIP scores, which one is more influential depends on the model and task. We
validate our method using controlled experiments and compare our results with
human judgments, finding that proprietary models align more closely with human
intuition regarding word significance. These findings enhance our understanding
of LLM behavior and contribute to developing more effective zero-shot prompts
and improved model analysis.; 19) Monolithic 4H-SiC nanomechanical resonators with high intrinsic quality
  factors; We present an extensive study of 4H-SiC nanomechanical resonators
electrochemically etched out of a monocrystalline wafer. Combining piezo-driven
interferometric determination of the mechanical spectra with
scanning-laser-Doppler vibrometry, an unambiguous assignment of resonance peaks
to flexural and torsional modes is achieved. The investigation of multiple
harmonic eigenmodes of singly and doubly clamped resonators with varying
geometry allows for a comprehensive characterization. Excellent intrinsic
mechanical quality factors up to $2\times10^5$ are found at room temperature,
approaching the thermoelastic limit at eigenfrquencies exceeding 10 MHz.
Mechanical stress is essentially absent. Young's modulus in agreement with
literature. These findings are robust under post-processing treatments, in
particular atomic layer etching and high-temperature thermal annealing. The
resulting on-chip high-quality mechanical resonators represent a valuable
technological element for a broad range of applications. In particular, the
monolithic architecture meets the requirements of spin-based photonic quantum
technologies on the upcoming SiC platform.; 20) Investigating the Scalability of Approximate Sparse Retrieval Algorithms
  to Massive Datasets; Learned sparse text embeddings have gained popularity due to their
effectiveness in top-k retrieval and inherent interpretability. Their
distributional idiosyncrasies, however, have long hindered their use in
real-world retrieval systems. That changed with the recent development of
approximate algorithms that leverage the distributional properties of sparse
embeddings to speed up retrieval. Nonetheless, in much of the existing
literature, evaluation has been limited to datasets with only a few million
documents such as MSMARCO. It remains unclear how these systems behave on much
larger datasets and what challenges lurk in larger scales. To bridge that gap,
we investigate the behavior of state-of-the-art retrieval algorithms on massive
datasets. We compare and contrast the recently-proposed Seismic and graph-based
solutions adapted from dense retrieval. We extensively evaluate Splade
embeddings of 138M passages from MsMarco-v2 and report indexing time and other
efficiency and effectiveness metrics.; 21) Temporal Connectivity Augmentation; Connectivity in temporal graphs relies on the notion of temporal paths, in
which edges follow a chronological order (either strict or non-strict). In this
work, we investigate the question of how to make a temporal graph connected.
More precisely, we tackle the problem of finding, among a set of proposed
temporal edges, the smallest subset such that its addition makes the graph
temporally connected (TCA). We study the complexity of this problem and
variants, under restricted lifespan of the graph, i.e. the maximum time step in
the graph. Our main result on TCA is that for any fixed lifespan at least 2, it
is NP-complete in both the strict and non-strict setting. We additionally
provide a set of restrictions in the non-strict setting which makes the problem
solvable in polynomial time and design an algorithm achieving this complexity.
Interestingly, we prove that the source variant (making a given vertex a source
in the augmented graph) is as difficult as TCA. On the opposite, we prove that
the version where a list of connectivity demands has to be satisfied is
solvable in polynomial time, when the size of the list is fixed. Finally, we
highlight a variant of the previous case for which even with two pairs the
problem is already NP-hard.; 22) Non-linear Quantum Monte Carlo; The mean of a random variable can be understood as a $\textit{linear}$
functional on the space of probability distributions. Quantum computing is
known to provide a quadratic speedup over classical Monte Carlo methods for
mean estimation. In this paper, we investigate whether a similar quadratic
speedup is achievable for estimating $\textit{non-linear}$ functionals of
probability distributions. We propose a quantum-inside-quantum Monte Carlo
algorithm that achieves such a speedup for a broad class of non-linear
estimation problems, including nested conditional expectations and stochastic
optimization. Our algorithm improves upon the direct application of the quantum
multilevel Monte Carlo algorithm introduced by An et al.. The existing lower
bound indicates that our algorithm is optimal up polylogarithmic factors. A key
innovation of our approach is a new sequence of multilevel Monte Carlo
approximations specifically designed for quantum computing, which is central to
the algorithm's improved performance.; 23) Deep Learning of Proteins with Local and Global Regions of Disorder; Although machine learning has transformed protein structure prediction of
folded protein ground states with remarkable accuracy, intrinsically disordered
proteins and regions (IDPs/IDRs) are defined by diverse and dynamical
structural ensembles that are predicted with low confidence by algorithms such
as AlphaFold. We present a new machine learning method, IDPForge (Intrinsically
Disordered Protein, FOlded and disordered Region GEnerator), that exploits a
transformer protein language diffusion model to create all-atom IDP ensembles
and IDR disordered ensembles that maintains the folded domains. IDPForge does
not require sequence-specific training, back transformations from
coarse-grained representations, nor ensemble reweighting, as in general the
created IDP/IDR conformational ensembles show good agreement with solution
experimental data, and options for biasing with experimental restraints are
provided if desired. We envision that IDPForge with these diverse capabilities
will facilitate integrative and structural studies for proteins that contain
intrinsic disorder.; 24) Goldilocks Isolation: High Performance VMs with Edera; Organizations run applications on cloud infrastructure shared between
multiple users and organizations. Popular tooling for this shared
infrastructure, including Docker and Kubernetes, supports such multi-tenancy
through the use of operating system virtualization. With operating system
virtualization (known as containerization), multiple applications share the
same kernel, reducing the runtime overhead. However, this shared kernel
presents a large attack surface and has led to a proliferation of container
escape attacks in which a kernel exploit lets an attacker escape the isolation
of operating system virtualization to access other applications or the
operating system itself. To address this, some systems have proposed a return
to hypervisor virtualization for stronger isolation between applications.
However, no existing system has achieved both the isolation of hypervisor
virtualization and the performance and usability of operating system
virtualization.
  We present Edera, an optimized type 1 hypervisor that uses paravirtualization
to improve the runtime of hypervisor virtualization. We illustrate Edera's
usability and performance through two use cases. First, we create a container
runtime compatible with Kubernetes that runs on the Edera hypervisor. This
implementation can be used as a drop-in replacement for the Kubernetes runtime
and is compatible with all the tooling in the Kubernetes ecosystem. Second, we
use Edera to provide driver isolation for hardware drivers, including those for
networking, storage, and GPUs. This use of isolation protects the hypervisor
and other applications from driver vulnerabilities. We find that Edera has
runtime comparable to Docker with .9% slower cpu speeds, an average of 3%
faster system call performance, and memory performance 0-7% faster. It achieves
this with a 648 millisecond increase in startup time from Docker's 177.4
milliseconds.; 25) Improving Retrieval-Augmented Generation through Multi-Agent
  Reinforcement Learning; Retrieval-augmented generation (RAG) is extensively utilized to incorporate
external, current knowledge into large language models, thereby minimizing
hallucinations. A standard RAG pipeline may comprise several components, such
as query rewriting, document retrieval, document filtering, and answer
generation. However, these components are typically optimized separately
through supervised fine-tuning, which can lead to misalignments between the
objectives of individual modules and the overarching aim of generating accurate
answers in question-answering (QA) tasks. Although recent efforts have explored
reinforcement learning (RL) to optimize specific RAG components, these
approaches often focus on overly simplistic pipelines with only two components
or do not adequately address the complex interdependencies and collaborative
interactions among the modules. To overcome these challenges, we propose
treating the RAG pipeline as a multi-agent cooperative task, with each
component regarded as an RL agent. Specifically, we present MMOA-RAG, a
Multi-Module joint Optimization Algorithm for RAG, which employs multi-agent
reinforcement learning to harmonize all agents' goals towards a unified reward,
such as the F1 score of the final answer. Experiments conducted on various QA
datasets demonstrate that MMOA-RAG improves the overall pipeline performance
and outperforms existing baselines. Furthermore, comprehensive ablation studies
validate the contributions of individual components and the adaptability of
MMOA-RAG across different RAG components and datasets. The code of MMOA-RAG is
on https://github.com/chenyiqun/MMOA-RAG.; 26) In the graphical Sierpinski gasket, the reverse Riesz transform is
  unbounded on $L^p$, $p\in (1,2)$; In this article, we proved that the reverse Riesz transform on the graphical
Sierpinski gasket is unbounded on $L^p$ for $p\in (1,2)$. Together with
previous results, it shows that the Riesz transform on the graphical Sierpinski
gasket is bounded on $L^p$ if and only if $p\in (1,2]$ and the reverse Riesz
transform is bounded on $L^p$ if and only if $p\in [2,\infty)$.
  Moreover, our method is quite flexible - but requires explicit computations -
and hints to the fact that the reverse Riesz transforms is never bounded on
$L^p$, $p\in (1,2)$, on graphs with slow diffusions.; 27) A Comprehensive Review of Protein Language Models; At the intersection of the rapidly growing biological data landscape and
advancements in Natural Language Processing (NLP), protein language models
(PLMs) have emerged as a transformative force in modern research. These models
have achieved remarkable progress, highlighting the need for timely and
comprehensive overviews. However, much of the existing literature focuses
narrowly on specific domains, often missing a broader analysis of PLMs. This
study provides a systematic review of PLMs from a macro perspective, covering
key historical milestones and current mainstream trends. We focus on the models
themselves and their evaluation metrics, exploring aspects such as model
architectures, positional encoding, scaling laws, and datasets. In the
evaluation section, we discuss benchmarks and downstream applications. To
further support ongoing research, we introduce relevant mainstream tools.
Lastly, we critically examine the key challenges and limitations in this
rapidly evolving field.; 28) Engineered Zwitterion-Infused Clay Composites with Antibacterial and
  Antifungal Efficacy; Microbes and pathogens play a detrimental role in healing wounds, causing
infections like impetigo through bodily fluids and skin and entering the
bloodstream through the wounds, thereby hindering the healing process and
tissue regeneration. Clay, known for its long history of natural therapeutic
use, has emerged as one of the most promising candidates for biomedical
applications due to its non-toxic nature, porosity, high surface area,
ubiquity, and excellent cation exchange capacity. This study demonstrates an
innovative approach to engineering an organo-functionalized,
infection-resistant, easy-to-use bandage material from clay, an environmentally
benign and sustainable material. The hybrid membranes have been developed using
clays, zwitterions, silver ions, and terbinafine hydrochloride (TBH) to impart
antibacterial and antifungal efficacy. A critical aspect of this study is
embedding organic molecules and metal ions with the clays and releasing them to
resist the growth and kill the pathogens. The antimicrobial efficacy of the
membranes has been tested using a zone of inhibition study against the most
common microbes in skin wounds, viz. S. aureus, E. coli, and C. albicans.
Results from our studies not only demonstrate the potential of these hybrid
clay membranes as a cost-effective, scalable, and effective solution for
treating microbial infections but also instill newer avenues for point-of-care
wound-healing treatments, offering hope for improved patient outcomes.; 29) EditID: Training-Free Editable ID Customization for Text-to-Image
  Generation; We propose EditID, a training-free approach based on the DiT architecture,
which achieves highly editable customized IDs for text to image generation.
Existing text-to-image models for customized IDs typically focus more on ID
consistency while neglecting editability. It is challenging to alter facial
orientation, character attributes, and other features through prompts. EditID
addresses this by deconstructing the text-to-image model for customized IDs
into an image generation branch and a character feature branch. The character
feature branch is further decoupled into three modules: feature extraction,
feature fusion, and feature integration. By introducing a combination of
mapping features and shift features, along with controlling the intensity of ID
feature integration, EditID achieves semantic compression of local features
across network depths, forming an editable feature space. This enables the
successful generation of high-quality images with editable IDs while
maintaining ID consistency, achieving excellent results in the IBench
evaluation, which is an editability evaluation framework for the field of
customized ID text-to-image generation that quantitatively demonstrates the
superior performance of EditID. EditID is the first text-to-image solution to
propose customizable ID editability on the DiT architecture, meeting the
demands of long prompts and high quality image generation.; 30) Nonsuppressible viremia during HIV-1 therapy meets molecular virology; HIV-1 replication can be suppressed with antiretroviral therapy (ART), but
individuals who stop taking ART soon become viremic again. Some people
experience extended times of detectable viremia despite optimal adherence to
ART. In the issue of the JCI, White, Wu, and coauthors elucidate a source of
nonsuppressible viremia (NSV) in treatment-adherent patients clonally expanded
T cells harboring HIV-1 proviruses with small deletions or mutations in the
5'-leader, the UTR that includes the major splice donor site of viral RNA.
These mutations altered viral RNA-splicing efficiency and RNA dimerization and
packaging, yet still allowed production of detectable levels of noninfectious
virus particles. These particles lacked the HIV-1 Env surface protein required
for cell entry and failed to form the mature capsid cone required for
infectivity. These studies improve our understanding of NSV and the regulation
of viral functions in the 5'-leader with implications for rationalized care in
individuals with NSV.; 31) Role of seed layer in growing atomically flat TiTe2/Sb2Te3
  heterostructure thin films at the wafer scale; Chalcogenide phase-change materials (PCMs) are a leading candidate for
advanced memory and computing applications. Epitaxial-like growth of
chalcogenide thin films at the wafer scale is important to guarantee the
homogeneity of the thin film but is challenging with magnetron sputtering,
particularly for the growth of phase-change heterostructure (PCH), such as
TiTe2/Sb2Te3. In this work, we report how to obtain highly textured
TiTe2/Sb2Te3 heterostructure thin films with atomically sharp interfaces on
standard silicon substrates. By combining atomic-scale characterization and ab
initio simulations, we reveal the critical role of the Sb2Te3 seed layer in
forming a continuous Si-Sb-Te mixed transition layer, which provides a
wafer-scale flat surface for the subsequent epitaxial-like growth of
TiTe2/Sb2Te3 thin film. By gradually reducing the thickness of the seed layer,
we determine its critical limit to be ~2 nm. Non-negligible in-plane tensile
strain was observed in the TiTe2 slabs due to the lattice mismatch with the
adjacent Sb2Te3 ones, suggesting that the chemical interaction across the
structural gaps in the heterostructure is stronger than a pure van der Waals
interaction. Finally, we outline the potential choices of chalcogenides for
atomically flat seed layers on standard silicon substrates, which can be used
for wafer-scale synthesis of other high-quality PCM or PCH thin films.; 32) From #Dr00gtiktok to #harmreduction: Exploring Substance Use Hashtags on
  TikTok; The rise of TikTok as a primary source of information for youth, combined
with its unique short-form video format, creates urgent questions about how
substance use content manifests and spreads on the platform. This paper
provides the first in-depth exploration of substance use-related content on
TikTok, covering all major substance categories as classified by the Drug
Enforcement Agency. Through social network analysis and qualitative coding, we
examined more than 2,333 hashtags across 39,509 videos, identified 16 distinct
hashtag communities and analyzed their interconnections and thematic content.
Our analysis revealed a highly interconnected small-world network where
recovery-focused hashtags like #addiction, #recovery, and #sober serve as
central bridges between communities. Through manual coding of 351
representative videos, we found that Recovery Advocacy content (33.9%) and
Satirical content (28.2%) dominate, while direct substance depiction appears in
only 26% of videos, with active use shown in just 6.5% of them. This suggests
TikTok functions primarily as a recovery support platform rather than a space
promoting substance use. We found strong alignment between hashtag communities
and video content, indicating organic community formation rather than attempts
to evade content moderation. Our findings inform how platforms can balance
content moderation with preserving valuable recovery support communities, while
also providing insights for the design of social media-based recovery
interventions.; 33) Human Guided Learning of Transparent Regression Models; We present a human-in-the-loop (HIL) approach to permutation regression, the
novel task of predicting a continuous value for a given ordering of items. The
model is a gradient boosted regression model that incorporates simple
human-understandable constraints of the form x < y, i.e. item x has to be
before item y, as binary features. The approach, HuGuR (Human Guided
Regression), lets a human explore the search space of such transparent
regression models. Interacting with HuGuR, users can add, remove, and refine
order constraints interactively, while the coefficients are calculated on the
fly. We evaluate HuGuR in a user study and compare the performance of
user-built models with multiple baselines on 9 data sets. The results show that
the user-built models outperform the compared methods on small data sets and in
general perform on par with the other methods, while being in principle
understandable for humans. On larger datasets from the same domain,
machine-induced models begin to outperform the user-built models. Further work
will study the trust users have in models when constructed by themselves and
how the scheme can be transferred to other pattern domains, such as strings,
sequences, trees, or graphs.; 34) Exploring Tensor Network Algorithms as a Quantum-Inspired Method for
  Quantum Reservoir Computing; Quantum reservoir computing (QRC) has emerged as a promising hybrid quantum
machine learning (QML) method that leverages the complex dynamics of quantum
systems and classical machine learning models. Motivated by the development of
this new QML method, we explore how quantum-inspired techniques like tensor
networks (TNs), specifically the Time Dependent Variational Principle (TDVP)
with Matrix Product State (MPS), can be used for the QRC algorithm. To
demonstrate the utility of our quantum-inspired method, we performed numerical
experiments on the MNIST dataset and compared the performance of our
quantum-inspired QRC with different classical machine learning (ML) methods.
The results reveal that high-quality embeddings can be generated by performing
the time-evolution of MPS system consisting of one-dimensional chain of Rydberg
atoms. This quantum-inspired method is highly scalable, enabling the simulation
of 100 qubits with a low classical computing overhead. Finally, this study also
underscores the potential of tensor networks as quantum-inspired algorithms to
enhance the capability of quantum machine learning algorithms to study datasets
with large numbers of features.; 35) Processing and Analyzing Real-World Driving Data: Insights on Trips,
  Scenarios, and Human Driving Behaviors; Analyzing large volumes of real-world driving data is essential for providing
meaningful and reliable insights into real-world trips, scenarios, and human
driving behaviors. To this end, we developed a multi-level data processing
approach that adds new information, segments data, and extracts desired
parameters. Leveraging a confidential but extensive dataset (over 1 million
km), this approach leads to three levels of in-depth analysis: trip, scenario,
and driving. The trip-level analysis explains representative properties
observed in real-world trips, while the scenario-level analysis focuses on
scenario conditions resulting from road events that reduce vehicle speed. The
driving-level analysis identifies the cause of driving regimes for specific
situations and characterizes typical human driving behaviors. Such analyses can
support the design of both trip- and scenario-based tests, the modeling of
human drivers, and the establishment of guidelines for connected and automated
vehicles.; 36) Combining Static Analysis Techniques for Program Comprehension Using
  Slicito; While program comprehension tools often use static program analysis
techniques to obtain useful information, they usually work only with
sufficiently scalable techniques with limited precision. A possible improvement
of this approach is to let the developer interactively reduce the scope of the
code being analyzed and then apply a more precise analysis technique to the
reduced scope. This paper presents a new version of the tool SLICITO that
allows developers to perform this kind of exploration on C# code in Visual
Studio. A common usage of SLICITO is to use interprocedural data-flow analysis
to identify the parts of the code most relevant for the given task and then
apply symbolic execution to reason about the precise behavior of these parts.
Inspired by Moldable Development, SLICITO provides a set of program analysis
and visualization building blocks that can be used to create specialized
program comprehension tools directly in Visual Studio. We demonstrate the full
scope of features on a real industrial example both in the text and in the
following video: https://www.slicito.com/icpc2025video.mp4; 37) COLOR: A compositional linear operation-based representation of protein
  sequences for identification of monomer contributions to properties; The properties of biological materials like proteins and nucleic acids are
largely determined by their primary sequence. While certain segments in the
sequence strongly influence specific functions, identifying these segments, or
so-called motifs, is challenging due to the complexity of sequential data.
While deep learning (DL) models can accurately capture sequence-property
relationships, the degree of nonlinearity in these models limits the assessment
of monomer contributions to a property - a critical step in identifying key
motifs. Recent advances in explainable AI (XAI) offer attention and
gradient-based methods for estimating monomeric contributions. However, these
methods are primarily applied to classification tasks, such as binding site
identification, where they achieve limited accuracy (40-45%) and rely on
qualitative evaluations. To address these limitations, we introduce a DL model
with interpretable steps, enabling direct tracing of monomeric contributions.
We also propose a metric ($\mathcal{I}$), inspired by the masking technique in
the field of image analysis and natural language processing, for quantitative
analysis on datasets mainly containing distinct properties of anti-cancer
peptides (ACP), antimicrobial peptides (AMP), and collagen. Our model exhibits
22% higher explainability, pinpoints critical motifs (RRR, RRI, and RSS) that
significantly destabilize ACPs, and identifies motifs in AMPs that are 50% more
effective in converting non-AMPs to AMPs. These findings highlight the
potential of our model in guiding mutation strategies for designing
protein-based biomaterials.; 38) Shear-gravity transition determines the steep velocity dispersion-size
  relation in molecular clouds: confronting analytical formula with
  observations; The velocity dispersion-size relation is a crucial indicator of the dynamic
properties of interstellar gas. Recent observations reveal a steep velocity
dispersion-size relation ($\sigma_{\rm v}\sim R^{\beta}$) with the index
$\beta> 0.6$, which cannot be explained by a single mechanism with only gravity
($\beta\sim0.5$) or shear ($\beta \sim 1$). We present a two-component model
$\sigma_{\rm v_{mixture}} = \lambda_1 \sigma_{\rm v_{g}} + \lambda_2
\sigma_{\rm v_{shear}} = A[(GM/R)^{\frac{1}{2}} + f(R/t_{\rm shear})]$ to
explain the steep velocity dispersion-size relation in the observation from
e.g. Miville et al. (2017) and Zhou et al. (2022). We find that the velocity
dispersion of small clouds is mainly caused by self-gravity, while large clouds
are primarily affected by shear, and these two regimes are linked by a gradual
transition with a transition scale $\sim100$ pc. The variation of cloud
velocity dispersion with the Galactocentric distance results from the variation
of both cloud internal density structure and Galactic shear. Our two-component
model captures how the dynamics of the molecular gas can be affected by both
internal and external factors, and we expect it to be applied to data from
galaxies with different physical conditions to reveal the physics.; 39) Pushing the boundaries of Structure-Based Drug Design through
  Collaboration with Large Language Models; Structure-Based Drug Design (SBDD) has revolutionized drug discovery by
enabling the rational design of molecules for specific protein targets. Despite
significant advancements in improving docking scores, advanced 3D-SBDD
generative models still face challenges in producing drug-like candidates that
meet medicinal chemistry standards and pharmacokinetic requirements. These
limitations arise from their inherent focus on molecular interactions, often
neglecting critical aspects of drug-likeness. To address these shortcomings, we
introduce the Collaborative Intelligence Drug Design (CIDD) framework, which
combines the structural precision of 3D-SBDD models with the chemical reasoning
capabilities of large language models (LLMs). CIDD begins by generating
supporting molecules with 3D-SBDD models and then refines these molecules
through LLM-supported modules to enhance drug-likeness and structural
reasonability. When evaluated on the CrossDocked2020 dataset, CIDD achieved a
remarkable success ratio of 37.94%, significantly outperforming the previous
state-of-the-art benchmark of 15.72%. Although improving molecular interactions
and drug-likeness is often seen as a trade-off, CIDD uniquely achieves a
balanced improvement in both by leveraging the complementary strengths of
different models, offering a robust and innovative pathway for designing
therapeutically promising drug candidates.; 40) Vacuum stress between conducting plates: the curved spacetime version; Brown and Maclay \cite{Brown} found the energy-momentum tensor for the
Casimir effect of parallel plates in 1969. We find its curved spacetime version
in a static background using the point splitting regularization method.
Previous results in the literature are reinforced and some consequences
discussed.; 41) Valley Emission and Upconversion in Isotopically Engineered Monolayer
  WS$_2$ under Resonant Excitation; In the quest to optimize the optoelectronic and valleytronic properties of 2D
materials, various strategies such as strain engineering, doping, and
heterostructuring have been explored. In this direction, isotope engineering
also offers a potential avenue to alter electron-phonon interaction and impact
quasiparticle scattering processes. In this study, we investigate the
dependence of sulfur isotopes on upconversion and valley scattering phenomena
by collecting the resonance photoluminescence (PL) under an applied magnetic
field from 0 to 14 T at 4 K for the chemical vapor deposition-grown monolayer
(1L) of W$^{N}$S$_2$, W$^{32}$S$_2$, and W$^{34}$S$_2$. The upconversion of the
mixed-state sulfur 1L (W$^{N}$S$_2$) exhibits one M-phonon absorption, with an
obtained optical gain of nearly 30 meV, while the pure sulfur isotope labelled
1Ls (W$^{32}$S$_2$ and W$^{34}$S$_2$) require two phonons (M and $\Gamma$),
yielding a gain of around 80 meV. It is also found that the exciton degree of
polarization (DOP) of W$^{N}$S$_2$ changes significantly by $\sim$ -30$\%$ as
the field increases from 0 to 14 T, while for W$^{32}$S$_2$ and W$^{34}$S$_2$,
the exciton DOP increases by up to $\sim$ 8$\%$. Similarly, distinct changes in
the DOP are observed for trions and localized excitons among all the samples,
attributed to the different valley scattering phenomena. The 1L W$^{N}$S$_2$
demonstrates a combination of intraband and interband scattering, whereas in
the case of W$^{32}$S$_2$ intraband scattering is preferred; W$^{34}$S$_2$
predominantly exhibits interband scattering. Finally, a phenomenological model
is proposed to describe the upconversion and valley scattering processes.; 42) Electron-channel blockade for plasmonic wavepackets; Collective excitation of an interacting electron liquid called plasmon has
distinct properties compared to those of a bare electron. Plasmons excited by a
short voltage pulse and transmitted through quantum devices will distribute
amongst electron conduction channels via Coulomb interactions, which is known
as charge fractionalization. This process spreads plasmons into all
Coulomb-coupled electron conduction channels, including those in neighbouring
circuits, and makes it difficult to control them in quantum circuits. Here we
demonstrate the isolation and on-demand selection of electron conduction
channels contributing to the plasmon using a cavity, which enables us to
control the velocity of propagating plasmons. We demonstrate an
electron-channel blockade effect, where charge fractionalization to
cavity-confined electron conduction channels is prohibited by the narrow energy
distribution of the plasmon itself. This effect is unaffected by the energy
fluctuation of the surrounding circuits. The electron-channel blockade offers a
powerful tool for designing plasmonic circuits as it can be used to control the
plasmon velocity by local parameters, suppress unwanted plasmonic excitation in
nearby circuits, and select electron-channels of plasmon eigenstates in quantum
interferometers.; 43) Prediction of Binding Affinity for ErbB Inhibitors Using Deep Neural
  Network Model with Morgan Fingerprints as Features; The ErbB receptor family, including EGFR and HER2, plays a crucial role in
cell growth and survival and is associated with the progression of various
cancers such as breast and lung cancer. In this study, we developed a deep
learning model to predict the binding affinity of ErbB inhibitors using
molecular fingerprints derived from SMILES representations. The SMILES
representations for each ErbB inhibitor were obtained from the ChEMBL database.
We first generated Morgan fingerprints from the SMILES strings and applied
AutoDock Vina docking to calculate the binding affinity values. After filtering
the dataset based on binding affinity, we trained a deep neural network (DNN)
model to predict binding affinity values from the molecular fingerprints. The
model achieved significant performance, with a Mean Squared Error (MSE) of
0.2591, Mean Absolute Error (MAE) of 0.3658, and an R-squared value of 0.9389
on the training set. Although performance decreased slightly on the test set (R
squared = 0.7731), the model still demonstrated robust generalization
capabilities. These results indicate that the deep learning approach is highly
effective for predicting the binding affinity of ErbB inhibitors, offering a
valuable tool for virtual screening and drug discovery.; 44) Extinction Distributions in Nearby Star-resolved Galaxies. I. M31; An extinction distribution of the Andromeda Galaxy (M31) is constructed with
member stars as tracers by fitting multiband photometric data from UKIRT/WFCAM,
PS1, and Gaia DR3. The resulting extinction distribution covers approximately
10 deg$^2$ of M31 with a resolution of approximately 50 arcsec, providing the
largest coverage to date based on stellar observations. The derived average
extinction, $A_V = 1.17$ mag, agrees well with previous studies. To account for
foreground extinction, an extinction map of the Milky Way toward M31 with a
resolution of $\sim$ 1.7 arcmin is also constructed, yielding an average
extinction of $A_V \approx 0.185$ mag. The results offer a valuable tool for
extinction correction in future observations, such as those from the China
Space Station Telescope, and provide insights for improving dust models based
on the spatial distribution of dust in galaxies like M31.; 45) MaizeEar-SAM: Zero-Shot Maize Ear Phenotyping; Quantifying the variation in yield component traits of maize (Zea mays L.),
which together determine the overall productivity of this globally important
crop, plays a critical role in plant genetics research, plant breeding, and the
development of improved farming practices. Grain yield per acre is calculated
by multiplying the number of plants per acre, ears per plant, number of kernels
per ear, and the average kernel weight. The number of kernels per ear is
determined by the number of kernel rows per ear multiplied by the number of
kernels per row. Traditional manual methods for measuring these two traits are
time-consuming, limiting large-scale data collection. Recent automation efforts
using image processing and deep learning encounter challenges such as high
annotation costs and uncertain generalizability.
  We tackle these issues by exploring Large Vision Models for zero-shot,
annotation-free maize kernel segmentation. By using an open-source large vision
model, the Segment Anything Model (SAM), we segment individual kernels in RGB
images of maize ears and apply a graph-based algorithm to calculate the number
of kernels per row. Our approach successfully identifies the number of kernels
per row across a wide range of maize ears, showing the potential of zero-shot
learning with foundation vision models combined with image processing
techniques to improve automation and reduce subjectivity in agronomic data
collection. All our code is open-sourced to make these affordable phenotyping
methods accessible to everyone.; 46) Magnetic Thomson Transport in High Opacity Domains; X-ray radiation from neutron stars manifests itself in a variety of settings.
Isolated pulsars, and magnetars both exhibit quasi-thermal persistent soft
X-ray emission from their surfaces. Transient magnetospheric bursts from
magnetars and pulsed signals from accreting neutron stars mostly appear in
harder X rays. The emission zones pertinent to these signals are all highly
Thomson optically thick so that their radiation anisotropy and polarization can
be modeled using sophisticated simulations of scattering transport from
extended emission regions. Validation of such codes and their efficient
construction is enhanced by a deep understanding of scattering transport in
high opacity domains. This paper presents a new analysis of the polarized
magnetic Thomson radiative transfer in the asymptotic limit of high opacity.
The integro-differential equations for photon scattering transport that result
from a phase matrix construction are reduced to a compact pair of equations.
This pair is then solved numerically for two key parameters that describe the
photon anisotropy and polarization configuration of high Thomson opacity
environs. Empirical approximations for these parameters as functions of the
ratio of the photon and cyclotron frequencies are presented. Implementation of
these semi-analytic transport solutions as interior boundary conditions is
shown to speed up scattering simulations. The solutions also enable the
specification of the anisotropic radiation pressure. The analysis is directly
applicable to the atmospheres of magnetars and moderate-field pulsars, and to
the accretion columns of magnetized X-ray binaries, and can be adapted to
address other neutron star settings.; 47) Fractional fast diffusion with initial data a Radon measure; We establish a complete Widder Theory for the fractional fast diffusion
equation. Our work focuses on nonnegative solutions satisfying a certain
integral size condition at infinity. We prove that these solutions possess a
Radon measure as initial trace, and prove the existence and uniqueness of
solutions originating from such initial data. The uniqueness result is the main
issue. Most of its difficulty comes from the singular character of the
nonlinearity.; 48) Silicon is the next frontier in plant synthetic biology; Silicon has striking similarity with carbon and is found in plant cells.
However, there is no specific role that has been assigned to silicon in the
life cycle of plants. The amount of silicon in plant cells is species specific
and can reach levels comparable to macronutrients. Silicon is the central
element for artificial intelligence, nanotechnology and digital revolution thus
can act as an informational molecule like nucleic acids while the diverse
bonding potential of silicon with different chemical species is analogous to
carbon and thus can serve as a structural candidate such as proteins. The
discovery of large amounts of silicon on Mars and the moon along with the
recent developments of enzyme that can incorporate silicon into organic
molecules has propelled the theory of creating silicon-based life. More
recently, bacterial cytochrome has been modified through directed evolution
such that it could cleave silicon-carbon bonds in organo-silicon compounds thus
consolidating on the idea of utilizing silicon in biomolecules. In this article
the potential of silicon-based life forms has been hypothesized along with the
reasoning that autotrophic virus-like particles can be a lucrative candidate to
investigate such potential. Such investigations in the field of synthetic
biology and astrobiology will have corollary benefit on Earth in the areas of
medicine, sustainable agriculture and environmental sustainability.
Bibliometric analysis indicates an increasing interest in synthetic biology.
Germany leads in research related to plant synthetic biology, while
Biotechnology and Biological Sciences Research Council (BBSRC) at UK has
highest financial commitments and Chinese Academy of Sciences generates the
highest number of publications in the field.; 49) Towards Fine-grained Interactive Segmentation in Images and Videos; The recent Segment Anything Models (SAMs) have emerged as foundational visual
models for general interactive segmentation. Despite demonstrating robust
generalization abilities, they still suffer performance degradations in
scenarios demanding accurate masks. Existing methods for high-precision
interactive segmentation face a trade-off between the ability to perceive
intricate local details and maintaining stable prompting capability, which
hinders the applicability and effectiveness of foundational segmentation
models. To this end, we present an SAM2Refiner framework built upon the SAM2
backbone. This architecture allows SAM2 to generate fine-grained segmentation
masks for both images and videos while preserving its inherent strengths.
Specifically, we design a localization augment module, which incorporates local
contextual cues to enhance global features via a cross-attention mechanism,
thereby exploiting potential detailed patterns and maintaining semantic
information. Moreover, to strengthen the prompting ability toward the enhanced
object embedding, we introduce a prompt retargeting module to renew the
embedding with spatially aligned prompt features. In addition, to obtain
accurate high resolution segmentation masks, a mask refinement module is
devised by employing a multi-scale cascaded structure to fuse mask features
with hierarchical representations from the encoder. Extensive experiments
demonstrate the effectiveness of our approach, revealing that the proposed
method can produce highly precise masks for both images and videos, surpassing
state-of-the-art methods.; 50) Mechanism of Electricacupuncture Treating Detrusor Bladder Neck
  Dyscoordination After Suprasacral Spinal Cord Injury by Proteomics; Objectives This study aimed to elucidate the potential mechanisms of
electroacupuncture (EA) in restoring detrusor-bladder neck dyssynergesia (DBND)
following suprasacral spinal cord injury.
  Methods A total of 52 adult female Sprague-Dawley rats were randomly assigned
to either a sham group (n=12) or a spinal cord injury model group (n=40). In
the model group, DBND was induced in 40 rats through Hassan Shaker spinal cord
transection, with 24 rats surviving spinal shock and subsequently randomized
into two groups: a model-only group (DBND, n=12) and an EA intervention group
(DBND+EA, n=12). DBND+EA was administered at Ciliao (BL32), Zhongji (RN3), and
Sanyinjiao (SP6) acupoints, for 20 minutes per session, once daily for 10
consecutive days. On day 29 post-injury, all rats underwent urodynamic
assessments, followed by hematoxylin and eosin (HE) staining, tandem mass tag
(TMT) proteomics, and Western blot (WB) analysis of the detrusor and bladder
neck tissues.
  Results Urodynamic evaluation demonstrated that EA intervention enhanced
bladder function in DBND rats. HE staining indicated reduced fibroplasia in the
detrusor muscle and alleviated inflammation in the bladder neck following EA.
TMT proteomic analysis revealed 30 differentially expressed proteins (DEPs) in
the detrusor and 59 DEPs in the bladder neck post-EA treatment. WB results
corroborated these TMT findings.
  Conclusion EA effectively promotes synergy between the detrusor muscle and
bladder neck in DBND, likely by enhancing detrusor contractility and
facilitating bladder neck relaxation during urination. This study provides
mechanistic insights into the therapeutic role of EA in managing DBND.; 51) Inverse problems with experiment-guided AlphaFold; Proteins exist as a dynamic ensemble of multiple conformations, and these
motions are often crucial for their functions. However, current structure
prediction methods predominantly yield a single conformation, overlooking the
conformational heterogeneity revealed by diverse experimental modalities. Here,
we present a framework for building experiment-grounded protein structure
generative models that infer conformational ensembles consistent with measured
experimental data. The key idea is to treat state-of-the-art protein structure
predictors (e.g., AlphaFold3) as sequence-conditioned structural priors, and
cast ensemble modeling as posterior inference of protein structures given
experimental measurements. Through extensive real-data experiments, we
demonstrate the generality of our method to incorporate a variety of
experimental measurements. In particular, our framework uncovers previously
unmodeled conformational heterogeneity from crystallographic densities, and
generates high-accuracy NMR ensembles orders of magnitude faster than the
status quo. Notably, we demonstrate that our ensembles outperform AlphaFold3
and sometimes better fit experimental data than publicly deposited structures
to the Protein Data Bank (PDB). We believe that this approach will unlock
building predictive models that fully embrace experimentally observed
conformational diversity.; 52) A Graph Width Perspective on Partially Ordered Hamiltonian Paths; We consider the problem of finding a Hamiltonian path with precedence
constraints in the form of a partial order on the vertex set. This problem is
known as Partially Ordered Hamiltonian Path Problem (POHPP). Here, we study the
complexity for graph width parameters for which the ordinary Hamiltonian Path
problem is in $\mathsf{FPT}$. We show that POHPP is $\mathsf{NP}$-complete for
graphs of pathwidth 4. We complement this result by giving polynomial-time
algorithms for graphs of pathwidth 3 and treewidth 2. Furthermore, we show that
POHPP is $\mathsf{NP}$-hard for graphs of clique cover number 2 and
$\mathsf{W[1]}$-hard for some distance-to-$\mathcal{G}$ parameters, including
distance to path and distance to clique. In addition, we present $\mathsf{XP}$
and $\mathsf{FPT}$ algorithms for parameters such as distance to block and
feedback edge set number.; 53) Collinear and TMD distributions with dynamical soft-gluon resolution
  scale; Soft-gluon resolution scales characterize parton branching Monte Carlo
implementations of the evolution equations for parton distribution functions in
Quantum Chromodynamics (QCD). We examine scenarios with dynamical, i.e.,
branching-scale dependent, resolution scale, and discuss physical implications
for both collinear and transverse-momentum dependent (TMD) distributions. We
perform the first determination of parton distributions with dynamical
resolution scale, at next-to-leading order (NLO) in perturbation theory, from
fits to precision deep-inelastic scattering measurements from HERA. We present
an application of TMD distributions with dynamical resolution scale to
Drell-Yan lepton-pair transverse momentum spectra at the LHC, and comment on
the extraction of non-perturbative intrinsic-kT parameters from Drell-Yan data
at small transverse momenta.; 54) Nonlinear optimals and their role in sustaining turbulence in channel
  flow; We investigate the energy transfer from the mean profile to velocity
fluctuations in channel flow by calculating nonlinear optimal disturbances,i.e.
the initial condition of a given finite energy that achieves the highest
possible energy growth during a given fixed time horizon. It is found that for
a large range of time horizons and initial disturbance energies, the nonlinear
optimal exhibits streak spacing and amplitude consistent with DNS at least at
Re_tau = 180, which suggests that they isolate the relevant physical mechanisms
that sustain turbulence. Moreover, the time horizon necessary for a nonlinear
disturbance to outperform a linear optimal is consistent with previous
DNS-based estimates using eddy turnover time, which offers a new perspective on
how some turbulent time scales are determined.; 55) Alzheimer's Disease Classification Using Retinal OCT: TransnetOCT and
  Swin Transformer Models; Retinal optical coherence tomography (OCT) images are the biomarkers for
neurodegenerative diseases, which are rising in prevalence. Early detection of
Alzheimer's disease using retinal OCT is a primary challenging task. This work
utilizes advanced deep learning techniques to classify retinal OCT images of
subjects with Alzheimer's disease (AD) and healthy controls (CO). The goal is
to enhance diagnostic capabilities through efficient image analysis. In the
proposed model, Raw OCT images have been preprocessed with ImageJ and given to
various deep-learning models to evaluate the accuracy. The best classification
architecture is TransNetOCT, which has an average accuracy of 98.18% for input
OCT images and 98.91% for segmented OCT images for five-fold cross-validation
compared to other models, and the Swin Transformer model has achieved an
accuracy of 93.54%. The evaluation accuracy metric demonstrated TransNetOCT and
Swin transformer models capability to classify AD and CO subjects reliably,
contributing to the potential for improved diagnostic processes in clinical
settings.; 56) Tensor parametric Hamiltonian operator inference; This work presents a tensor-based approach to constructing data-driven
reduced-order models corresponding to semi-discrete partial differential
equations with canonical Hamiltonian structure. By expressing parameter-varying
operators with affine dependence as contractions of a generalized parameter
vector against a constant tensor, this method leverages the operator inference
framework to capture parametric dependence in the learned reduced-order model
via the solution to a convex, least-squares optimization problem. This leads to
a concise and straightforward implementation which compactifies previous
parametric operator inference approaches and directly extends to learning
parametric operators with symmetry constraints, a key feature required for
constructing structure-preserving surrogates of Hamiltonian systems. The
proposed approach is demonstrated on both a (non-Hamiltonian) heat equation
with variable diffusion coefficient as well as a Hamiltonian wave equation with
variable wave speed.; 57) RiboFlow: Conditional De Novo RNA Sequence-Structure Co-Design via
  Synergistic Flow Matching; Ribonucleic acid (RNA) binds to molecules to achieve specific biological
functions. While generative models are advancing biomolecule design, existing
methods for designing RNA that target specific ligands face limitations in
capturing RNA's conformational flexibility, ensuring structural validity, and
overcoming data scarcity. To address these challenges, we introduce RiboFlow, a
synergistic flow matching model to co-design RNA structures and sequences based
on target molecules. By integrating RNA backbone frames, torsion angles, and
sequence features in an unified architecture, RiboFlow explicitly models RNA's
dynamic conformations while enforcing sequence-structure consistency to improve
validity. Additionally, we curate RiboBind, a large-scale dataset of
RNA-molecule interactions, to resolve the scarcity of high-quality structural
data. Extensive experiments reveal that RiboFlow not only outperforms
state-of-the-art RNA design methods by a large margin but also showcases
controllable capabilities for achieving high binding affinity to target
ligands. Our work bridges critical gaps in controllable RNA design, offering a
framework for structure-aware, data-efficient generation.; 58) Leveraging Sequence Purification for Accurate Prediction of Multiple
  Conformational States with AlphaFold2; AlphaFold2 (AF2) has transformed protein structure prediction by harnessing
co-evolutionary constraints embedded in multiple sequence alignments (MSAs).
MSAs not only encode static structural information, but also hold critical
details about protein dynamics, which underpin biological functions. However,
these subtle co-evolutionary signatures, which dictate conformational state
preferences, are often obscured by noise within MSA data and thus remain
challenging to decipher. Here, we introduce AF-ClaSeq, a systematic framework
that isolates these co-evolutionary signals through sequence purification and
iterative enrichment. By extracting sequence subsets that preferentially encode
distinct structural states, AF-ClaSeq enables high-confidence predictions of
alternative conformations. Our findings reveal that the successful sampling of
alternative states depends not on MSA depth but on sequence purity.
Intriguingly, purified sequences encoding specific structural states are
distributed across phylogenetic clades and superfamilies, rather than confined
to specific lineages. Expanding upon AF2's transformative capabilities,
AF-ClaSeq provides a powerful approach for uncovering hidden structural
plasticity, advancing allosteric protein and drug design, and facilitating
dynamics-based protein function annotation.; 59) An Energy-Adaptive Elastic Equivariant Transformer Framework for Protein
  Structure Representation; Structure-informed protein representation learning is essential for effective
protein function annotation and \textit{de novo} design. However, the presence
of inherent noise in both crystal and AlphaFold-predicted structures poses
significant challenges for existing methods in learning robust protein
representations. To address these issues, we propose a novel equivariant
Transformer-State Space Model(SSM) hybrid framework, termed $E^3$former,
designed for efficient protein representation. Our approach uses energy
function-based receptive fields to construct proximity graphs and incorporates
an equivariant high-tensor-elastic selective SSM within the transformer
architecture. These components enable the model to adapt to complex atom
interactions and extract geometric features with higher signal-to-noise ratios.
Empirical results demonstrate that our model outperforms existing methods in
structure-intensive tasks, such as inverse folding and binding site prediction,
particularly when using predicted structures, owing to its enhanced tolerance
to data deviation and noise. Our approach offers a novel perspective for
conducting biological function research and drug discovery using noisy protein
structure data.; 60) Progress of the anti-obesity of Berberine; Obesity is defined as the excessive accumulation or abnormal distribution of
body fat. According to data from World Obesity Atlas 2024, the increase in
prevalence of obesity has become a major worldwide health problem in adults as
well as among children and adolescents. Although an increasing number of drugs
have been approved for the treatment of obesity in recent years, many of these
drugs have inevitable side effects which have increased the demand for new
safe, accessible and effective drugs for obesity and prompt interest in natural
products. Berberine (BBR) and its metabolites, known for their multiple
pharmacological effects. Recent studies have emphatically highlighted the
anti-obesity benefits of BBR and the underlying mechanisms have been gradually
elucidated. However, its clinical application is limited by poor oral
absorption and low bioavailability. Based on this, this review summarizes
current research on the anti-obesity effects of BBR and its metabolites,
including advancements in clinical trail results, understanding potential
molecular mechanisms and absorption and bioavailability. As a natural compound
derived from plants, BBR holds potential as an alternative approach for
managing obesity.; 61) DVHGNN: Multi-Scale Dilated Vision HGNN for Efficient Vision Recognition; Recently, Vision Graph Neural Network (ViG) has gained considerable attention
in computer vision. Despite its groundbreaking innovation, Vision Graph Neural
Network encounters key issues including the quadratic computational complexity
caused by its K-Nearest Neighbor (KNN) graph construction and the limitation of
pairwise relations of normal graphs. To address the aforementioned challenges,
we propose a novel vision architecture, termed Dilated Vision HyperGraph Neural
Network (DVHGNN), which is designed to leverage multi-scale hypergraph to
efficiently capture high-order correlations among objects. Specifically, the
proposed method tailors Clustering and Dilated HyperGraph Construction (DHGC)
to adaptively capture multi-scale dependencies among the data samples.
Furthermore, a dynamic hypergraph convolution mechanism is proposed to
facilitate adaptive feature exchange and fusion at the hypergraph level.
Extensive qualitative and quantitative evaluations of the benchmark image
datasets demonstrate that the proposed DVHGNN significantly outperforms the
state-of-the-art vision backbones. For instance, our DVHGNN-S achieves an
impressive top-1 accuracy of 83.1% on ImageNet-1K, surpassing ViG-S by +1.0%
and ViHGNN-S by +0.6%.; 62) The standard coil or globule phases cannot describe the denatured state
  of structured proteins and intrinsically disordered proteins; The concepts of globule and random coil were developed to describe the phases
of homopolymers and then used to characterize the denatured state of structured
cytosolic proteins and intrinsically disordered proteins. Using multi-scale
molecular dynamics simulations, we were able to explore the conformational
space of the disordered conformations of both types of protein under biological
conditions in an affordable amount of computational time. By studying the size
of the protein and the density correlations in space, we conclude that the
standard phases of homopolymers and the tools to detect them cannot be applied
straightforwardly to proteins.; 63) Steering Protein Family Design through Profile Bayesian Flow; Protein family design emerges as a promising alternative by combining the
advantages of de novo protein design and mutation-based directed evolution.In
this paper, we propose ProfileBFN, the Profile Bayesian Flow Networks, for
specifically generative modeling of protein families. ProfileBFN extends the
discrete Bayesian Flow Network from an MSA profile perspective, which can be
trained on single protein sequences by regarding it as a degenerate profile,
thereby achieving efficient protein family design by avoiding large-scale MSA
data construction and training. Empirical results show that ProfileBFN has a
profound understanding of proteins. When generating diverse and novel family
proteins, it can accurately capture the structural characteristics of the
family. The enzyme produced by this method is more likely than the previous
approach to have the corresponding function, offering better odds of generating
diverse proteins with the desired functionality.; 64) Advances in RNA secondary structure prediction and RNA modifications:
  Methods, data, and applications; Due to the hierarchical organization of RNA structures and their pivotal
roles in fulfilling RNA functions, the formation of RNA secondary structure
critically influences many biological processes and has thus been a crucial
research topic. This review sets out to explore the computational prediction of
RNA secondary structure and its connections to RNA modifications, which have
emerged as an active domain in recent years. We first examine the progression
of RNA secondary structure prediction methodology, focusing on a set of
representative works categorized into thermodynamic, comparative, machine
learning, and hybrid approaches. Next, we survey the advances in RNA
modifications and computational methods for identifying RNA modifications,
focusing on the prominent modification types. Subsequently, we highlight the
interplay between RNA modifications and secondary structures, emphasizing how
modifications such as m6A dynamically affect RNA folding and vice versa. In
addition, we also review relevant data sources and provide a discussion of
current challenges and opportunities in the field. Ultimately, we hope our
review will be able to serve as a cornerstone to aid in the development of
innovative methods for this emerging topic and foster therapeutic applications
in the future.; 65) Calorimetric Wire Detector for Measurement of Atomic Hydrogen Beams; A calorimetric detector for minimally disruptive measurements of atomic
hydrogen beams is described. The calorimeter measures heat released by the
recombination of hydrogen atoms into molecules on a thin wire. As a
demonstration, the angular distribution of a beam with a peak intensity of
$\approx 10^{16} \,{\rm{atoms}}/{(\rm{cm}^2 \rm{s})}$ is measured by
translating the wire across the beam. The data agree well with an analytic
model of the beam from the thermal hydrogen atom source. Using the beam shape
model, the relative intensity of the beam can be determined to 5% precision or
better at any angle.; 66) A versatile experimental method to measure the traction forces at
  interfaces; Measurement of surface forces, including cohesive forces and contact forces,
is critical for understanding and controlling interactions at interfaces to
optimize the interfacial performance of applications. The objective of this
paper is to introduce a general in-situ method that enables the measurement of
3D micron-scale displacements and corresponding force distribution at
interfaces in dry or wet environment. Stereo digital image correlation was used
to measure the 3D-displacement of a soft and deformable substrate. The
efficiency and accuracy of the technique were evaluated by applying compression
to the substrate using a steel ball, with the measured 3D displacements
aligning closely with finite element analysis simulations. To further assess
the method's applicability, the wet adhesion between mussel plaques and
substrate was tested under aqueous conditions. The interfacial displacements
and forces at different stages during the test were measured. The application
of the technique can be extended for varied circumstances regarding force range
and substrate materials based on Winkler Spring model.; 67) Learning Hamiltonian Dynamics with Bayesian Data Assimilation; In this paper, we develop a neural network-based approach for time-series
prediction in unknown Hamiltonian dynamical systems. Our approach leverages a
surrogate model and learns the system dynamics using generalized coordinates
(positions) and their conjugate momenta while preserving a constant
Hamiltonian. To further enhance long-term prediction accuracy, we introduce an
Autoregressive Hamiltonian Neural Network, which incorporates autoregressive
prediction errors into the training objective. Additionally, we employ Bayesian
data assimilation to refine predictions in real-time using online measurement
data. Numerical experiments on a spring-mass system and highly elliptic orbits
under gravitational perturbations demonstrate the effectiveness of the proposed
method, highlighting its potential for accurate and robust long-term
predictions.; 68) Applying computational protein design to therapeutic antibody discovery
  -- current state and perspectives; Machine learning applications in protein sciences have ushered in a new era
for designing molecules in silico. Antibodies, which currently form the largest
group of biologics in clinical use, stand to benefit greatly from this shift.
Despite the proliferation of these protein design tools, their direct
application to antibodies is often limited by the unique structural biology of
these molecules. Here, we review the current computational methods for antibody
design, highlighting their role in advancing computational drug discovery.; 69) A glance to Luttinger liquid and its platforms; The concept of a Tomonaga-Luttinger liquid (TLL) has been established as a
fundamental theory for the understanding of one-dimensional quantum systems.
Originally formulated as a replacement for Landau's Fermi-liquid theory, which
accurately predicts the behaviour of most 3D metals but fails dramatically in
1D, the TLL description applies to a even broader class of 1D systems,including
bosons and anyons. After a certain number of theoretical breakthroughs, its
descriptive power has now been confirmed experimentally in different
experimental platforms. They extend from organic conductors, carbon nanotubes,
quantum wires, topological edge states of quantum spin Hall insulators to cold
atoms, Josephson junctions, Bose liquids confined within 1D nanocapillaries and
spin chains. In the ground state of such systems, quantum fluctuations become
correlated on all length scales, but, counter-intuitively, no long-range order
exists. In this respect, this review will illustrate the validity of conformal
field theory for describing real-world systems, establishing the boundaries for
its application and, on the other side will discuss the spectacular
demonstration of how the quantum-critical TLL state governs the properties of
many-body systems in one dimension.; 70) Neural equilibria for long-term prediction of nonlinear conservation
  laws; We introduce Neural Discrete Equilibrium (NeurDE), a machine learning (ML)
approach for long-term forecasting of flow phenomena that relies on a ""lifting""
of physical conservation laws into the framework of kinetic theory. The kinetic
formulation provides an excellent structure for ML algorithms by separating
nonlinear, non-local physics into a nonlinear but local relaxation to
equilibrium and a linear non-local transport. This separation allows the ML to
focus on the local nonlinear components while addressing the simpler linear
transport with efficient classical numerical algorithms. To accomplish this, we
design an operator network that maps macroscopic observables to equilibrium
states in a manner that maximizes entropy, yielding expressive BGK-type
collisions. By incorporating our surrogate equilibrium into the lattice
Boltzmann (LB) algorithm, we achieve accurate flow forecasts for a wide range
of challenging flows. We show that NeurDE enables accurate prediction of
compressible flows, including supersonic flows, while tracking shocks over
hundreds of time steps, using a small velocity lattice-a heretofore
unattainable feat without expensive numerical root finding.; 71) Super-Hamiltonians for super-Macdonald polynomials; The Macdonald finite-difference Hamiltonian is lifted to a
super-generalization. In addition to canonical bosonic time variables $p_k$ new
Grassmann time variables $\theta_k$ are introduced, and the Hamiltonian is
represented as a differential operator acting on a space of functions of both
types of variables $p_k$ and $\theta_k$. Eigenfunctions for this Hamiltonian
are a suitable generalization of Macdonald polynomials to super-Macdonald
polynomials discussed earlier in the literature. Peculiarities of the
construction in comparison to the canonical bosonic case are discussed.; 72) Non-Markovain Quantum State Diffusion for the Tunneling in SARS-COVID-19
  virus; In the context of biology, unlike the comprehensively established Standard
Model in physics, many biological processes lack a complete theoretical
framework and are often described phenomenologically. A pertinent example is
olfaction -- the process through which humans and animals distinguish various
odors. The conventional biological explanation for olfaction relies on the lock
and key model, which, while useful, does not fully account for all observed
phenomena. As an alternative or complement to this model, vibration-assisted
electron tunneling has been proposed. Drawing inspiration from the
vibration-assisted electron tunneling model for olfaction, we have developed a
theoretical model for electron tunneling in SARS-CoV-2 virus infection within a
non-Markovian framework. We approach this by solving the non-Markovian quantum
stochastic Schrodinger equation. In our model, the spike protein and the GPCR
receptor are conceptualized as a dimer, utilizing the spin-Boson model to
facilitate the description of electron tunneling. Our analysis demonstrates
that electron tunneling in this context exhibits inherently non-Markovian
characteristics, extending into the intermediate and strong coupling regimes
between the dimer components. This behavior stands in stark contrast to
predictions from Markovian models, which fail to accurately describe electron
tunneling in the strong coupling limit. Notably, Markovian approximations often
lead to unphysical negative probabilities in this regime, underscoring their
limitations and highlighting the necessity of incorporating non-Markovian
dynamics for a more realistic description of biological quantum processes. This
approach not only broadens our understanding of viral infection mechanisms but
also enhances the biological accuracy and relevance of our theoretical
framework in describing complex biological interactions.; 73) Make Making Sustainable: Exploring Sustainability Practices, Challenges,
  and Opportunities in Making Activities; The recent democratization of personal fabrication has significantly advanced
the maker movement and reshaped applied research in HCI and beyond. However,
this growth has also raised increasing sustainability concerns, as material
waste is an inevitable byproduct of making and rapid prototyping. In this work,
we examine the sustainability landscape within the modern maker community,
focusing on grassroots makerspaces and maker-oriented research labs through
in-depth interviews with diverse stakeholders involved in making and managing
making-related activities. Our findings highlight four key themes: the various
types of ""waste"" generated through the making process, the strategies (or lack
thereof) for managing this waste, the motivations driving (un)sustainable
practices, and the challenges faced. We synthesize these insights into design
considerations and takeaways for technical HCI researchers and the broader
community, focusing on future tools, infrastructures, and educational
approaches to foster sustainable making.; 74) On a local property of infinite Galois extensions implying the Northcott
  property; In 2001, Bombieri and Zannier studied the Northcott property (N) for infinite
Galois extensions of the rationals. In particular they provided a local
property of the extensions that imply property (N). Later, Checcoli and Fehm
demonstrated the existence of infinite extensions satisfying this local
property. In this article, we establish two main results. First, we show that
this local property, unlike property (N), is not preserved under finite
extensions. Second, we show that, for an infinite Galois extension of Q, such
local property cannot be read on the Galois group. More precisely, we exhibit
several profinite groups that are realizable over Q by fields that do not
satisfy the local property.; 75) Electron capture of superheavy nuclei with realistic lepton wave
  functions; The superheavy nuclei push the periodic table of the elements and the chart
of the nuclides to their limits, providing a unique laboratory for studies of
the electron-nucleus interactions. The most important weak decay mode in known
superheavy nuclei is electron capture (EC). In the standard calculations of EC,
the lepton wave functions are usually considered in the lowest-order
approximation. In this work, we investigate the sensitivity of EC rates on the
choice of the electron wave functions by (i) assuming the single-particle
approximation for the electron wave functions, and (ii) carrying out
Dirac-Hartree-Fock (DHF) calculations. The nuclear response is generated based
on the state-of-the-art quasiparticle random phase approximation employing
relativistic nuclear energy density functional theory. We show that using the
improved lepton wave functions reduces the EC rates up to 40\% in the
superheavy nucleus oganesson ($Z=118$). Interestingly, because of screening
effects, the difference between the EC rates obtained with the DHF and
single-particle calculations is fairly small.; 76) Extended circular nim; Circular nim $CN(m, k)$ is a variant of nim, in which there are $m$ piles of
tokens arranged in a circle and each player, in their turn, chooses at most $k$
consecutive piles in the circle and removes an arbitrary number of tokens from
each pile. The player must remove at least one token in total. For some cases
of $m$ and $k$, closed formulas to determine which player has a winning
strategy have been found. Almost all cases are still open problems. In this
paper, we consider a variant of circular nim, extended circular nim. In
extended circular nim $ECN(m_S, k)$, there are $m$ piles of tokes arranged in a
circle. $S$ is a set of positive integers less than or equal to half of $m$.
Each player, in their turn, chooses at most $k$ piles selected every $s$-th
pile in a circle for an $s \in S$. We find some closed formulas to determine
which player has a winning strategy for the cases where the number of piles is
no more than eight.; 77) R1-Zero's ""Aha Moment"" in Visual Reasoning on a 2B Non-SFT Model; Recently DeepSeek R1 demonstrated how reinforcement learning with simple
rule-based incentives can enable autonomous development of complex reasoning in
large language models, characterized by the ""aha moment"", in which the model
manifest self-reflection and increased response length during training.
However, attempts to extend this success to multimodal reasoning often failed
to reproduce these key characteristics. In this report, we present the first
successful replication of these emergent characteristics for multimodal
reasoning on only a non-SFT 2B model. Starting with Qwen2-VL-2B and applying
reinforcement learning directly on the SAT dataset, our model achieves 59.47%
accuracy on CVBench, outperforming the base model by approximately ~30% and
exceeding both SFT setting by ~2%. In addition, we share our failed attempts
and insights in attempting to achieve R1-like reasoning using RL with instruct
models. aiming to shed light on the challenges involved. Our key observations
include: (1) applying RL on instruct model often results in trivial reasoning
trajectories, and (2) naive length reward are ineffective in eliciting
reasoning capabilities. The project code is available at
https://github.com/turningpoint-ai/VisualThinker-R1-Zero; 78) Non-uniqueness of mild solutions to supercritical heat equations; We consider the focusing power nonlinearity heat equation
  \begin{equation}\label{Eq:Heat_abstract}\tag{NLH}
  \partial_t u -\Delta u = |u|^{p-1}u, \quad p>1,
  \end{equation} in dimensions $d \geq 3$. It is well-known that if $p$ is
large enough then \eqref{Eq:Heat_abstract} is unconditionally locally
well-posed in $L^q(\mathbb{R}^d)$ for $q \geq d(p-1)/2$. We prove that this
result is optimal in the sense that uniqueness of local solutions fails when $q
< d(p-1)/2$ as long as $p < p_{JL}$, where $p_{JL}$ stands for the
Joseph-Lundgren exponent. Our proof is based on the method that
Jia-\v{S}ver\'ak proposed in \cite{JiaSve15} to show non-uniqueness of Leray
solutions to incompressible 3d Navier-Stokes equations. In particular, we
rigorously verify for \eqref{Eq:Heat_abstract} the (analogue of the) spectral
assumption made in \cite{JiaSve15}. To our knowledge, this is the first
rigorous implementation of the Jia-\v{S}ver\'ak method to a nonlinear parabolic
equation without forcing.; 79) Decay of mass for a semilinear heat equation with mixed local-nonlocal
  operators; In this paper, we are concerned with the Cauchy problem for the
reaction-diffusion equation $\partial_t u+t^\beta\mathcal{L} u= - h(t)u^p$
posed on $\mathbb{R}^N$, driven by the mixed local-nonlocal operator
$\mathcal{L}=-\Delta+(-\Delta)^{\alpha/2}$, $\alpha\in(0,2)$, and supplemented
with a nonnegative integrable initial data, where $p>1$, $\beta\geq 0$, and
$h:(0,\infty)\to(0,\infty)$ is a locally integrable function. We study the
large time behavior of non-negative solutions and show that the nonlinear term
determines the large time asymptotic for $p\leq 1+{\alpha}/{N(\beta+1)},$ while
the classical/anomalous diffusion effects win if $p>1+{\alpha}/{N(\beta+1)}$.; 80) Digital Twin Buildings: 3D Modeling, GIS Integration, and Visual
  Descriptions Using Gaussian Splatting, ChatGPT/Deepseek, and Google Maps
  Platform; Urban digital twins are virtual replicas of cities that use multi-source data
and data analytics to optimize urban planning, infrastructure management, and
decision-making. Towards this, we propose a framework focused on the
single-building scale. By connecting to cloud mapping platforms such as Google
Map Platforms APIs, by leveraging state-of-the-art multi-agent Large Language
Models data analysis using ChatGPT(4o) and Deepseek-V3/R1, and by using our
Gaussian Splatting-based mesh extraction pipeline, our Digital Twin Buildings
framework can retrieve a building's 3D model, visual descriptions, and achieve
cloud-based mapping integration with large language model-based data analytics
using a building's address, postal code, or geographic coordinates.; 81) LLM-USO: Large Language Model-based Universal Sizing Optimizer; The design of analog circuits is a cornerstone of integrated circuit (IC)
development, requiring the optimization of complex, interconnected
sub-structures such as amplifiers, comparators, and buffers. Traditionally,
this process relies heavily on expert human knowledge to refine design
objectives by carefully tuning sub-components while accounting for their
interdependencies. Existing methods, such as Bayesian Optimization (BO), offer
a mathematically driven approach for efficiently navigating large design
spaces. However, these methods fall short in two critical areas compared to
human expertise: (i) they lack the semantic understanding of the sizing
solution space and its direct correlation with design objectives before
optimization, and (ii) they fail to reuse knowledge gained from optimizing
similar sub-structures across different circuits. To overcome these
limitations, we propose the Large Language Model-based Universal Sizing
Optimizer (LLM-USO), which introduces a novel method for knowledge
representation to encode circuit design knowledge in a structured text format.
This representation enables the systematic reuse of optimization insights for
circuits with similar sub-structures. LLM-USO employs a hybrid framework that
integrates BO with large language models (LLMs) and a learning summary module.
This approach serves to: (i) infuse domain-specific knowledge into the BO
process and (ii) facilitate knowledge transfer across circuits, mirroring the
cognitive strategies of expert designers. Specifically, LLM-USO constructs a
knowledge summary mechanism to distill and apply design insights from one
circuit to related ones. It also incorporates a knowledge summary critiquing
mechanism to ensure the accuracy and quality of the summaries and employs
BO-guided suggestion filtering to identify optimal design points efficiently.; 82) GAIR: Improving Multimodal Geo-Foundation Model with Geo-Aligned
  Implicit Representations; Advancements in vision and language foundation models have inspired the
development of geo-foundation models (GeoFMs), enhancing performance across
diverse geospatial tasks. However, many existing GeoFMs primarily focus on
overhead remote sensing (RS) data while neglecting other data modalities such
as ground-level imagery. A key challenge in multimodal GeoFM development is to
explicitly model geospatial relationships across modalities, which enables
generalizability across tasks, spatial scales, and temporal contexts. To
address these limitations, we propose GAIR, a novel multimodal GeoFM
architecture integrating overhead RS data, street view (SV) imagery, and their
geolocation metadata. We utilize three factorized neural encoders to project an
SV image, its geolocation, and an RS image into the embedding space. The SV
image needs to be located within the RS image's spatial footprint but does not
need to be at its geographic center. In order to geographically align the SV
image and RS image, we propose a novel implicit neural representations (INR)
module that learns a continuous RS image representation and looks up the RS
embedding at the SV image's geolocation. Next, these geographically aligned SV
embedding, RS embedding, and location embedding are trained with contrastive
learning objectives from unlabeled data. We evaluate GAIR across 10 geospatial
tasks spanning RS image-based, SV image-based, and location embedding-based
benchmarks. Experimental results demonstrate that GAIR outperforms
state-of-the-art GeoFMs and other strong baselines, highlighting its
effectiveness in learning generalizable and transferable geospatial
representations.; 83) Hierarchical Bayesian estimation of population-level torque law
  parameters from anomalous pulsar braking indices; Abridged. Stochastic fluctuations in the spin frequency $\nu$ of a
rotation-powered pulsar affect how accurately one measures the power-law
braking index, $n_{\rm pl}$, defined through $\dot{\nu}=K\nu^{n_{\rm pl}}$, and
can lead to measurements of anomalous braking indices, with $\vert n \vert =
\vert \nu \ddot{\nu}/ \dot{\nu}^{2} \vert \gg1$, where the overdot symbolizes a
derivative with respect to time. Previous studies show that the variance of the
measured $n$ obeys the predictive, falsifiable formula $\langle n^{2} \rangle =
n_{\rm
pl}^{2}+\sigma^{2}_{\ddot{\nu}}\nu^{2}\gamma_{\ddot{\nu}}^{-2}\dot{\nu}^{-4}T_{\rm
obs}^{-1}$ for $\dot{K}=0$, where $\sigma_{\ddot{\nu}}$ is the timing noise
amplitude, $\gamma_{\ddot{\nu}}^{-1}$ is a stellar damping time-scale, and
$T_{\rm obs}$ is the total observing time. Here we combine this formula with a
hierarchical Bayesian scheme to infer the population-level distribution of
$n_{\rm pl}$ for a pulsar population of size $M$. The scheme is validated using
synthetic data. For a plausible test population with $M=100$ and injected
$n_{\rm pl}$ values drawn from a population-level Gaussian with mean $\mu_{\rm
pl}=4$ and standard deviation $\sigma_{\rm pl}=0.5$, intermediate between
electromagnetic braking and mass quadrupole gravitational radiation reaction,
the Bayesian scheme infers $\mu_{\rm pl}=3.89^{+0.24}_{-0.23}$ and $\sigma_{\rm
pl}=0.43^{+0.21}_{-0.14}$. The $M=100$ per-pulsar posteriors for $n_{\rm pl}$
and $\sigma^{2}_{\ddot{\nu}}\gamma_{\ddot{\nu}}^{-2}$ contain $87\%$ and
$69\%$, respectively, of the injected values within their $90\%$ credible
intervals. Comparable accuracy is achieved for (i) population sizes spanning
the range $50 \leq M \leq 300$, and (ii) wide priors satisfying $\mu_{\rm pl}
\leq 10^{3}$ and $\sigma_{\rm pl} \leq 10^{2}$, which accommodate plausible
spin-down mechanisms with $\dot{K}\neq0$.; 84) Artificial Intelligence Approaches for Anti-Addiction Drug Discovery; Drug addiction is a complex and pervasive global challenge that continues to
pose significant public health concerns. Traditional approaches to
anti-addiction drug discovery have struggled to deliver effective therapeutics,
facing high attrition rates, long development timelines, and inefficiencies in
processing large-scale data. Artificial intelligence (AI) has emerged as a
transformative solution to address these issues. Using advanced algorithms, AI
is revolutionizing drug discovery by enhancing the speed and precision of key
processes. This review explores the transformative role of AI in the pipeline
for anti-addiction drug discovery, including data collection, target
identification, and compound optimization. By highlighting the potential of AI
to overcome traditional barriers, this review systematically examines how AI
addresses critical gaps in anti-addiction research, emphasizing its potential
to revolutionize drug discovery and development, overcome challenges, and
advance more effective therapeutic strategies.; 85) Artificial Intelligence in Spectroscopy: Advancing Chemistry from
  Prediction to Generation and Beyond; The rapid advent of machine learning (ML) and artificial intelligence (AI)
has catalyzed major transformations in chemistry, yet the application of these
methods to spectroscopic and spectrometric data, referred to as Spectroscopy
Machine Learning (SpectraML), remains relatively underexplored. Modern
spectroscopic techniques (MS, NMR, IR, Raman, UV-Vis) generate an ever-growing
volume of high-dimensional data, creating a pressing need for automated and
intelligent analysis beyond traditional expert-based workflows. In this survey,
we provide a unified review of SpectraML, systematically examining
state-of-the-art approaches for both forward tasks (molecule-to-spectrum
prediction) and inverse tasks (spectrum-to-molecule inference). We trace the
historical evolution of ML in spectroscopy, from early pattern recognition to
the latest foundation models capable of advanced reasoning, and offer a
taxonomy of representative neural architectures, including graph-based and
transformer-based methods. Addressing key challenges such as data quality,
multimodal integration, and computational scalability, we highlight emerging
directions such as synthetic data generation, large-scale pretraining, and few-
or zero-shot learning. To foster reproducible research, we also release an
open-source repository containing recent papers and their corresponding curated
datasets (https://github.com/MINE-Lab-ND/SpectrumML_Survey_Papers). Our survey
serves as a roadmap for researchers, guiding progress at the intersection of
spectroscopy and AI.; 86) Generating and Detecting Various Types of Fake Image and Audio Content:
  A Review of Modern Deep Learning Technologies and Tools; This paper reviews the state-of-the-art in deepfake generation and detection,
focusing on modern deep learning technologies and tools based on the latest
scientific advancements. The rise of deepfakes, leveraging techniques like
Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs),
Diffusion models and other generative models, presents significant threats to
privacy, security, and democracy. This fake media can deceive individuals,
discredit real people and organizations, facilitate blackmail, and even
threaten the integrity of legal, political, and social systems. Therefore,
finding appropriate solutions to counter the potential threats posed by this
technology is essential. We explore various deepfake methods, including face
swapping, voice conversion, reenactment and lip synchronization, highlighting
their applications in both benign and malicious contexts. The review critically
examines the ongoing ""arms race"" between deepfake generation and detection,
analyzing the challenges in identifying manipulated contents. By examining
current methods and highlighting future research directions, this paper
contributes to a crucial understanding of this rapidly evolving field and the
urgent need for robust detection strategies to counter the misuse of this
powerful technology. While focusing primarily on audio, image, and video
domains, this study allows the reader to easily grasp the latest advancements
in deepfake generation and detection.; 87) Fused Partial Gromov-Wasserstein for Structured Objects; Structured data, such as graphs, are vital in machine learning due to their
capacity to capture complex relationships and interactions. In recent years,
the Fused Gromov-Wasserstein (FGW) distance has attracted growing interest
because it enables the comparison of structured data by jointly accounting for
feature similarity and geometric structure. However, as a variant of optimal
transport (OT), classical FGW assumes an equal mass constraint on the compared
data. In this work, we relax this mass constraint and propose the Fused Partial
Gromov-Wasserstein (FPGW) framework, which extends FGW to accommodate
unbalanced data. Theoretically, we establish the relationship between FPGW and
FGW and prove the metric properties of FPGW. Numerically, we introduce
Frank-Wolfe solvers for the proposed FPGW framework and provide a convergence
analysis. Finally, we evaluate the FPGW distance through graph classification
and clustering experiments, demonstrating its robust performance, especially
when data is corrupted by outlier noise.; 88) ProtComposer: Compositional Protein Structure Generation with 3D
  Ellipsoids; We develop ProtComposer to generate protein structures conditioned on spatial
protein layouts that are specified via a set of 3D ellipsoids capturing
substructure shapes and semantics. At inference time, we condition on
ellipsoids that are hand-constructed, extracted from existing proteins, or from
a statistical model, with each option unlocking new capabilities.
Hand-specifying ellipsoids enables users to control the location, size,
orientation, secondary structure, and approximate shape of protein
substructures. Conditioning on ellipsoids of existing proteins enables
redesigning their substructure's connectivity or editing substructure
properties. By conditioning on novel and diverse ellipsoid layouts from a
simple statistical model, we improve protein generation with expanded Pareto
frontiers between designability, novelty, and diversity. Further, this enables
sampling designable proteins with a helix-fraction that matches PDB proteins,
unlike existing generative models that commonly oversample conceptually simple
helix bundles. Code is available at https://github.com/NVlabs/protcomposer.; 89) Tight Bounds on the Number of Closest Pairs in Vertical Slabs; Let $S$ be a set of $n$ points in $\mathbb{R}^d$, where $d \geq 2$ is a
constant, and let $H_1,H_2,\ldots,H_{m+1}$ be a sequence of vertical
hyperplanes that are sorted by their first coordinates, such that exactly $n/m$
points of $S$ are between any two successive hyperplanes. Let $|A(S,m)|$ be the
number of different closest pairs in the ${{m+1} \choose 2}$ vertical slabs
that are bounded by $H_i$ and $H_j$, over all $1 \leq i < j \leq m+1$. We prove
tight bounds for the largest possible value of $|A(S,m)|$, over all point sets
of size $n$, and for all values of $1 \leq m \leq n$.
  As a result of these bounds, we obtain, for any constant $\epsilon>0$, a data
structure of size $O(n)$, such that for any vertical query slab $Q$, the
closest pair in the set $Q \cap S$ can be reported in $O(n^{1/2+\epsilon})$
time. Prior to this work, no linear space data structure with sublinear query
time was known.; 90) Multi-Lingual Cyber Threat Detection in Tweets/X Using ML, DL, and LLM:
  A Comparative Analysis; Cyber threat detection has become an important area of focus in today's
digital age due to the growing spread of fake information and harmful content
on social media platforms such as Twitter (now 'X'). These cyber threats, often
disguised within tweets, pose significant risks to individuals, communities,
and even nations, emphasizing the need for effective detection systems. While
previous research has explored tweet-based threats, much of the work is limited
to specific languages, domains, or locations, or relies on single-model
approaches, reducing their applicability to diverse real-world scenarios. To
address these gaps, our study focuses on multi-lingual tweet cyber threat
detection using a variety of advanced models. The research was conducted in
three stages: (1) We collected and labeled tweet datasets in four languages
English, Chinese, Russian, and Arabic employing both manual and polarity-based
labeling methods to ensure high-quality annotations. (2) Each dataset was
analyzed individually using machine learning (ML) and deep learning (DL) models
to assess their performance on distinct languages. (3) Finally, we combined all
four datasets into a single multi-lingual dataset and applied DL and large
language model (LLM) architectures to evaluate their efficacy in identifying
cyber threats across various languages. Our results show that among machine
learning models, Random Forest (RF) attained the highest performance; however,
the Bi-LSTM architecture consistently surpassed other DL and LLM architectures
across all datasets. These findings underline the effectiveness of Bi-LSTM in
multilingual cyber threat detection. The code for this paper can be found at
this link: https://github.com/Mmurrad/Tweet-Data-Classification.git.; 91) Applications of Random Matrix Theory in Machine Learning and Brain
  Mapping; Brain mapping analyzes the wavelengths of brain signals and outputs them in a
map, which is then analyzed by a radiologist. Introducing Machine Learning (ML)
into the brain mapping process reduces the variable of human error in reading
such maps and increases efficiency. A key area of interest is determining the
correlation between the functional areas of the brain on a voxel (3-dimensional
pixel) wise basis. This leads to determining how a brain is functioning and can
be used to detect diseases, disabilities, and sicknesses. As such, random noise
presents a challenge in consistently determining the actual signals from the
scan. This paper discusses how an algorithm created by Random Matrix Theory
(RMT) can be used as a tool for ML, as it detects the correlation of the
functional areas of the brain. Random matrices are simulated to represent the
voxel signal intensity strength for each time interval where a stimulus is
presented in an fMRI scan. Using the Marchenko-Pastur law for Wishart Matrices,
a result of RMT, it was found that no matter what type of noise was added to
the random matrices, the observed eigenvalue distribution of the Wishart
Matrices would converge to the theoretical distribution. This means that RMT is
robust and has a high test-re-test reliability. These results further indicate
that a strong correlation exists between the eigenvalues, and hence the
functional regions of the brain. Any eigenvalue that differs significantly from
those predicted from RMT may indicate the discovery of a new discrete brain
network.; 92) PyMOLfold: Interactive Protein and Ligand Structure Prediction in PyMOL; PyMOLfold is a flexible and open-source plugin designed to seamlessly
integrate AI-based protein structure prediction and visualization within the
widely used PyMOL molecular graphics system. By leveraging state-of-the-art
protein folding models such as ESM3, Boltz-1, and Chai-1, PyMOLfold allows
researchers to directly predict protein tertiary structures from amino acid
sequences without requiring external tools or complex workflows. Furthermore,
with certain models, users can provide a SMILES string of a ligand and have the
small molecule placed in the protein structure. This unique capability bridges
the gap between computational folding and structural visualization, enabling
users to input a primary sequence, perform a folding prediction, and
immediately explore the resulting 3D structure within the same intuitive
platform.; 93) Remodeling Peptide-MHC-TCR Triad Binding as Sequence Fusion for
  Immunogenicity Prediction; The complex nature of tripartite peptide-MHC-TCR interactions is a critical
yet underexplored area in immunogenicity prediction. Traditional studies on
TCR-antigen binding have not fully addressed the complex dependencies in triad
binding. In this paper, we propose new modeling approaches for these tripartite
interactions, utilizing sequence information from MHCs, peptides, and TCRs. Our
methods adhere to native sequence forms and align with biological processes to
enhance prediction accuracy. By incorporating representation learning
techniques, we introduce a fusion mechanism to integrate the three sequences
effectively. Empirical experiments show that our models outperform traditional
methods, achieving a 2.8 to 13.3 percent improvement in prediction accuracy
across existing benchmarks. We further validate our approach with extensive
ablation studies, demonstrating the effectiveness of the proposed model
components. The model implementation, code, and supplementary materials,
including a manuscript with colored hyperlinks and a technical appendix for
digital viewing, will be open-sourced upon publication.; 94) How Large is the Universe of RNA-Like Motifs? A Clustering Analysis of
  RNA Graph Motifs Using Topological Descriptors; We introduce a computational topology-based approach with unsupervised
machine-learning algorithms to estimate the database size and content of
RNA-like graph topologies. Specifically, we apply graph theory enumeration to
generate all 110,667 possible 2D dual graphs for vertex numbers ranging from 2
to 9. Among them, only 0.11% graphs correspond to approximately 200,000 known
RNA atomic fragments (collected in 2021) using the RNA-as-Graphs (RAG) mapping
method. The remaining 99.89% of the dual graphs may be RNA-like or
non-RNA-like. To determine which dual graphs in the 99.89% hypothetical set are
more likely to be associated with RNA structures, we apply computational
topology descriptors using the Persistent Spectral Graphs (PSG) method to
characterize each graph using 19 PSG-based features and use clustering
algorithms that partition all possible dual graphs into two clusters, RNA-like
cluster and non-RNA-like cluster. The distance of each dual graph to the center
of the RNA-like cluster represents the likelihood of it belonging to RNA
structures. From validation, our PSG-based RNA-like cluster includes 97.3% of
the 121 known RNA dual graphs, suggesting good performance. Furthermore,
46.017% of the hypothetical RNAs are predicted to be RNA-like. Significantly,
we observe that all the top 15 RNA-like dual graphs can be separated into
multiple subgraphs, whereas the top 15 non-RNA-like dual graphs tend not to
have any subgraphs. Moreover, a significant topological difference between top
RNA-like and non-RNA-like graphs is evident when comparing their topological
features. These findings provide valuable insights into the size of the RNA
motif universe and RNA design strategies, offering a novel framework for
predicting RNA graph topologies and guiding the discovery of novel RNA motifs,
perhaps anti-viral therapeutics by subgraph assembly.; 95) Co-Optimizing Distributed Energy Resources under Demand Charges and
  Bi-Directional Power Flow; We address the co-optimization of behind-the-meter (BTM) distributed energy
resources (DER), including flexible demands, renewable distributed generation
(DG), and battery energy storage systems (BESS) under net energy metering (NEM)
frameworks with demand charges. We formulate the problem as a stochastic
dynamic program that accounts for renewable generation uncertainty and
operational surplus maximization. Our theoretical analysis reveals that the
optimal policy follows a threshold structure. Finally, we show that even a
simple algorithm leveraging this threshold structure performs well in
simulation, emphasizing its importance in developing near-optimal algorithms.
These findings provide crucial insights for implementing prosumer energy
management systems under complex tariff structures.; 96) Gigahertz directional light modulation with electro-optic metasurfaces; Active metasurfaces promise spatiotemporal control over optical wavefronts,
but achieving high-speed modulation with pixel-level control has remained an
unmet challenge. While local phase control can be achieved with nanoscale
optical confinement, such as in plasmonic nanoparticles, the resulting
electrode spacings lead to large capacitance, limiting speed. Here, we
demonstrate the operation of a gigahertz-tunable metasurface for beam steering
through local control of metasurface elements in a plasmonic-organic hybrid
architecture. Our device comprises a corrugated metallic slot array engineered
to support plasmonic quasi-bound states in the continuum (quasi-BICs). These
plasmonic quasi-BICs provide ideal optical confinement and electrical
characteristics for integrating organic electro-optic (OEO) materials like JRD1
and have not been previously utilized in optical metasurfaces. We obtain a
quasi-static resonance tunability of 0.4 nm/V, which we leverage to steer light
between three diffraction orders and achieve an electro-optic bandwidth of ~4
GHz, with the potential for further speed improvements through scaling rules.
This work showcases on-chip spatiotemporal control of light at the
sub-micrometer and gigahertz level, opening new possibilities for applications
in 3D sensing and high-speed spatial light modulation.; 97) Academic Literature Recommendation in Large-scale Citation Networks
  Enhanced by Large Language Models; Literature recommendation is essential for researchers to find relevant
articles in an ever-growing academic field. However, traditional methods often
struggle due to data limitations and methodological challenges. In this work,
we construct a large citation network and propose a hybrid recommendation
framework for scientific article recommendation. Specifically, the citation
network contains 190,381 articles from 70 journals, covering statistics,
econometrics, and computer science, spanning from 1981 to 2022. The
recommendation mechanism integrates network-based citation patterns with
content-based semantic similarities. To enhance content-based recommendations,
we employ text-embedding-3-small model of OpenAI to generate an embedding
vector for the abstract of each article. The model has two key advantages:
computational efficiency and embedding stability during incremental updates,
which is crucial for handling dynamic academic databases. Additionally, the
recommendation mechanism is designed to allow users to adjust weights according
to their preferences, providing flexibility and personalization. Extensive
experiments have been conducted to verify the effectiveness of our approach. In
summary, our work not only provides a complete data system for building and
analyzing citation networks, but also introduces a practical recommendation
method that helps researchers navigate the growing volume of academic
literature, making it easier to find the most relevant and influential articles
in the era of information overload.; 98) TikTok's recommendations skewed towards Republican content during the
  2024 U.S. presidential race; TikTok is a major force among social media platforms with over a billion
monthly active users worldwide and 170 million in the United States. The
platform's status as a key news source, particularly among younger
demographics, raises concerns about its potential influence on politics in the
U.S. and globally. Despite these concerns, there is scant research
investigating TikTok's recommendation algorithm for political biases. We fill
this gap by conducting 323 independent algorithmic audit experiments testing
partisan content recommendations in the lead-up to the 2024 U.S. presidential
elections. Specifically, we create hundreds of ""sock puppet"" TikTok accounts in
Texas, New York, and Georgia, seeding them with varying partisan content and
collecting algorithmic content recommendations for each of them. Collectively,
these accounts viewed ~394,000 videos from April 30th to November 11th, 2024,
which we label for political and partisan content. Our analysis reveals
significant asymmetries in content distribution: Republican-seeded accounts
received ~11.8% more party-aligned recommendations compared to their
Democratic-seeded counterparts, and Democratic-seeded accounts were exposed to
~7.5% more opposite-party recommendations on average. These asymmetries exist
across all three states and persist when accounting for video- and
channel-level engagement metrics such as likes, views, shares, comments, and
followers, and are driven primarily by negative partisanship content. Our
findings provide insights into the inner workings of TikTok's recommendation
algorithm during a critical election period, raising fundamental questions
about platform neutrality.; 99) Physical Plausibility-aware Trajectory Prediction via Locomotion
  Embodiment; Humans can predict future human trajectories even from momentary observations
by using human pose-related cues. However, previous Human Trajectory Prediction
(HTP) methods leverage the pose cues implicitly, resulting in implausible
predictions. To address this, we propose Locomotion Embodiment, a framework
that explicitly evaluates the physical plausibility of the predicted trajectory
by locomotion generation under the laws of physics. While the plausibility of
locomotion is learned with an indifferentiable physics simulator, it is
replaced by our differentiable Locomotion Value function to train an HTP
network in a data-driven manner. In particular, our proposed Embodied
Locomotion loss is beneficial for efficiently training a stochastic HTP network
using multiple heads. Furthermore, the Locomotion Value filter is proposed to
filter out implausible trajectories at inference. Experiments demonstrate that
our method enhances even the state-of-the-art HTP methods across diverse
datasets and problem settings. Our code is available at:
https://github.com/ImIntheMiddle/EmLoco.; 100) Synthesis of an arbitrary elliptical polarization operator; We prove a theorem for transforming the polarization eigenstates of an
arbitrary elliptical birefringent device into the eigenstates of another
similar device by means of a birefringent device. The theorem is applied to
synthesize a specific birefringent device from a circular birefringent device,
and a practical setup is described. The resulting birefringence and
birefringent axis of the synthesized device can be independently set or
modulated. Finally, the theorem is extended to elliptical dichroic devices and
elliptical polarizers.",0.16666666666666666,0.3562071871080222
2411.05236,applied,2411.05236-pos2-6,"Channelrhodopsin-2, a directly light-gated cation-selective membrane channel; Microbial-type rhodopsins are found in archaea, prokaryotes, and eukaryotes. Some of them represent membrane ion transport proteins such as bacteriorhodopsin, a light-driven proton pump, or channelrhodopsin-1 (ChR1), recently identified light-gated channel from the green alga Chlamydomonas reinhardtii . ChR1 ChR2, related microbial-type rhodopsin C. , were shown to be involved generation photocurrents this alga. We demonstrate by functional expression, both oocytes Xenopus laevis mammalian cells, that ChR2 is directly light-switched cation-selective channel. This opens rapidly after absorption photon generate large permeability for monovalent divalent cations. desensitizes continuous light smaller steady-state conductance. Recovery desensitization accelerated extracellular H + negative potential, whereas closing decelerated intracellular expressed mainly under low-light conditions, suggesting involvement photoreception dark-adapted cells. The predicted seven-transmembrane α helices characteristic G protein-coupled receptors but reflect different motif Finally, we may used depolarize small simply illumination.",2411.05236-pos1-6,"DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT; Shannon capacity of signal transduction for multiple independent receptors; Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver.",1,"['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']","The first candidate paper discusses visible light communication systems and their technical implementation, which aligns well with the light-gated cation-selective channel function of Channelrhodopsin-2 described in the main paper. This connection can lead to novel applications in bioengineering and communication technology, leveraging the properties of ChR2 in practical scenarios. The other candidate papers, while interesting, do not directly contribute to a multidisciplinary synthesis as effectively as the first one does.","1) DESIGN AND IMPLEMENTATION OF VISIBLE LIGHT COMMUNICATION SYSTEM IN INDOOR ENVIRONMENT; Shannon capacity of signal transduction for multiple independent receptors; Visible Light communication (VLC) using White Light Emitting Diode (LED) is a promising technology for next generation communication for short range, high speed wireless data transmission. In this paper inexpensive transmitter and receiver of VLC system is designed and its performance is evaluated. The effect of natural and artificial ambient light noise sources is also considered. Experimental results show that the data transmission distance achieved upto 0.45m.Performance analysis is done with respect to optical power, photo sensitivity of photodiode at the receiver and the increase in distance between the transmitter and receiver.; 2) RiboFlow: Conditional De Novo RNA Sequence-Structure Co-Design via
  Synergistic Flow Matching; Ribonucleic acid (RNA) binds to molecules to achieve specific biological
functions. While generative models are advancing biomolecule design, existing
methods for designing RNA that target specific ligands face limitations in
capturing RNA's conformational flexibility, ensuring structural validity, and
overcoming data scarcity. To address these challenges, we introduce RiboFlow, a
synergistic flow matching model to co-design RNA structures and sequences based
on target molecules. By integrating RNA backbone frames, torsion angles, and
sequence features in an unified architecture, RiboFlow explicitly models RNA's
dynamic conformations while enforcing sequence-structure consistency to improve
validity. Additionally, we curate RiboBind, a large-scale dataset of
RNA-molecule interactions, to resolve the scarcity of high-quality structural
data. Extensive experiments reveal that RiboFlow not only outperforms
state-of-the-art RNA design methods by a large margin but also showcases
controllable capabilities for achieving high binding affinity to target
ligands. Our work bridges critical gaps in controllable RNA design, offering a
framework for structure-aware, data-efficient generation.; 3) How To Make Your Cell Tracker Say ""I dunno!""; Cell tracking is a key computational task in live-cell microscopy, but fully
automated analysis of high-throughput imaging requires reliable and, thus,
uncertainty-aware data analysis tools, as the amount of data recorded within a
single experiment exceeds what humans are able to overlook. We here propose and
benchmark various methods to reason about and quantify uncertainty in linear
assignment-based cell tracking algorithms. Our methods take inspiration from
statistics and machine learning, leveraging two perspectives on the cell
tracking problem explored throughout this work: Considering it as a Bayesian
inference problem and as a classification problem. Our methods admit a
framework-like character in that they equip any frame-to-frame tracking method
with uncertainty quantification. We demonstrate this by applying it to various
existing tracking algorithms including the recently presented Transformer-based
trackers. We demonstrate empirically that our methods yield useful and
well-calibrated tracking uncertainties.; 4) Estimating the Poynting flux of Alfv\'enic waves in polar coronal holes
  across Solar Cycle 24; Alfv\'enic waves are known to be prevalent throughout the corona and solar
wind. Determining the Poynting flux supplied by the waves is required for
constraining their role in plasma heating and acceleration, as well as
providing a constraint for Alfv\'en wave driven models that aim to predict
coronal and solar wind properties. Previous studies of the Alfv\'enic waves in
polar coronal holes have been able to provide a measure of energy flux for
arbitrary case studies. Here we build upon previous work and take a more
systematic approach, examining if there is evidence for any variation in
vertical Poynting flux over the course of the solar cycle. We use imaging data
from SDO/AIA to measure the displacements of the fine-scale structure present
in coronal holes. It is found that the measure for vertical Poynting flux is
broadly similar over the solar cycle, implying a consistent contribution from
waves to the energy budget of the solar wind. There is variation in energy flux
across the measurements (around 30\%), but this is suggested to be due to
differences in the individual coronal holes rather than a feature of the solar
cycle. Our direct estimates are in agreement with recent studies by
\cite{Huang_2023,Huang2024} who constrain the vertical Poynting flux through
comparison of predicted wind properties from Alfv\'enic wave driven turbulence
models to those observed with OMNI at 1~AU. Taken together, both sets of
results points towards a lack of correlation between the coronal Poynting flux
from waves and the solar cycle.; 5) DHP: Discrete Hierarchical Planning for Hierarchical Reinforcement
  Learning Agents; In this paper, we address the challenge of long-horizon visual planning tasks
using Hierarchical Reinforcement Learning (HRL). Our key contribution is a
Discrete Hierarchical Planning (DHP) method, an alternative to traditional
distance-based approaches. We provide theoretical foundations for the method
and demonstrate its effectiveness through extensive empirical evaluations.
  Our agent recursively predicts subgoals in the context of a long-term goal
and receives discrete rewards for constructing plans as compositions of
abstract actions. The method introduces a novel advantage estimation strategy
for tree trajectories, which inherently encourages shorter plans and enables
generalization beyond the maximum tree depth. The learned policy function
allows the agent to plan efficiently, requiring only $\log N$ computational
steps, making re-planning highly efficient. The agent, based on a soft-actor
critic (SAC) framework, is trained using on-policy imagination data.
Additionally, we propose a novel exploration strategy that enables the agent to
generate relevant training examples for the planning modules. We evaluate our
method on long-horizon visual planning tasks in a 25-room environment, where it
significantly outperforms previous benchmarks at success rate and average
episode length. Furthermore, an ablation study highlights the individual
contributions of key modules to the overall performance.; 6) Dynamics and Inequalities in Digital Social Networks: A Computational
  and Sociological Review; Digital networks have profoundly transformed the ways in which individuals
interact, exchange information, and establish connections, leading to the
emergence of phenomena such as virality, misinformation cascades, and online
polarization. This review conducts a thorough examination of the micro-macro
linkages within digital social networks, analyzing how individual actions like
liking, sharing, and commenting coalesce into broader systemic patterns and how
these interactions are influenced by algorithmic mediation. Utilizing a
multidisciplinary literature base, this study explores the interaction among
user behaviors, network structures, and platform algorithms that intensify
biases, strengthen homophily, and foster echo chambers. We delve into crucial
dynamics including the scalability's impact on weak tie propagation, the
amplification effects on influencers, and the rise of digital inequalities,
employing both theoretical and empirical approaches. By synthesizing insights
from sociology, network theory, and computational social science, this paper
underscores the necessity for novel frameworks that integrate algorithmic
processes into established micro-macro models. The conclusion presents
practical strategies aimed at promoting fairer digital networks through
decentralized architectures, algorithmic fairness, and improved digital
inclusion, tackling significant challenges such as polarization and
misinformation within networked societies.; 7) Tradutor: Building a Variety Specific Translation Model; Language models have become foundational to many widely used systems.
However, these seemingly advantageous models are double-edged swords. While
they excel in tasks related to resource-rich languages like English, they often
lose the fine nuances of language forms, dialects, and varieties that are
inherent to languages spoken in multiple regions of the world. Languages like
European Portuguese are neglected in favor of their more popular counterpart,
Brazilian Portuguese, leading to suboptimal performance in various linguistic
tasks. To address this gap, we introduce the first open-source translation
model specifically tailored for European Portuguese, along with a novel dataset
specifically designed for this task. Results from automatic evaluations on two
benchmark datasets demonstrate that our best model surpasses existing
open-source translation systems for Portuguese and approaches the performance
of industry-leading closed-source systems for European Portuguese. By making
our dataset, models, and code publicly available, we aim to support and
encourage further research, fostering advancements in the representation of
underrepresented language varieties.; 8) Unhackable Temporal Rewarding for Scalable Video MLLMs; In the pursuit of superior video-processing MLLMs, we have encountered a
perplexing paradox: the ""anti-scaling law"", where more data and larger models
lead to worse performance. This study unmasks the culprit: ""temporal hacking"",
a phenomenon where models shortcut by fixating on select frames, missing the
full video narrative. In this work, we systematically establish a comprehensive
theory of temporal hacking, defining it from a reinforcement learning
perspective, introducing the Temporal Perplexity (TPL) score to assess this
misalignment, and proposing the Unhackable Temporal Rewarding (UTR) framework
to mitigate the temporal hacking. Both theoretically and empirically, TPL
proves to be a reliable indicator of temporal modeling quality, correlating
strongly with frame activation patterns. Extensive experiments reveal that UTR
not only counters temporal hacking but significantly elevates video
comprehension capabilities. This work not only advances video-AI systems but
also illuminates the critical importance of aligning proxy rewards with true
objectives in MLLM development.; 9) How Large is the Universe of RNA-Like Motifs? A Clustering Analysis of
  RNA Graph Motifs Using Topological Descriptors; We introduce a computational topology-based approach with unsupervised
machine-learning algorithms to estimate the database size and content of
RNA-like graph topologies. Specifically, we apply graph theory enumeration to
generate all 110,667 possible 2D dual graphs for vertex numbers ranging from 2
to 9. Among them, only 0.11% graphs correspond to approximately 200,000 known
RNA atomic fragments (collected in 2021) using the RNA-as-Graphs (RAG) mapping
method. The remaining 99.89% of the dual graphs may be RNA-like or
non-RNA-like. To determine which dual graphs in the 99.89% hypothetical set are
more likely to be associated with RNA structures, we apply computational
topology descriptors using the Persistent Spectral Graphs (PSG) method to
characterize each graph using 19 PSG-based features and use clustering
algorithms that partition all possible dual graphs into two clusters, RNA-like
cluster and non-RNA-like cluster. The distance of each dual graph to the center
of the RNA-like cluster represents the likelihood of it belonging to RNA
structures. From validation, our PSG-based RNA-like cluster includes 97.3% of
the 121 known RNA dual graphs, suggesting good performance. Furthermore,
46.017% of the hypothetical RNAs are predicted to be RNA-like. Significantly,
we observe that all the top 15 RNA-like dual graphs can be separated into
multiple subgraphs, whereas the top 15 non-RNA-like dual graphs tend not to
have any subgraphs. Moreover, a significant topological difference between top
RNA-like and non-RNA-like graphs is evident when comparing their topological
features. These findings provide valuable insights into the size of the RNA
motif universe and RNA design strategies, offering a novel framework for
predicting RNA graph topologies and guiding the discovery of novel RNA motifs,
perhaps anti-viral therapeutics by subgraph assembly.; 10) An Evaluation on the Role of Non-Coding RNA in HIV Transcription and
  Latency: A Review; The existence of latent cellular reservoirs is recognized as the major
barrier to an HIV cure. Reactivating and eliminating ""shock and kill"" or
permanently silencing ""block and lock"" the latent HIV reservoir, as well as
gene editing, remain promising approaches, but so far have proven to be only
partially successful. Moreover, using latency reversing agents or ""block and
lock"" drugs pose additional considerations, including the ability to cause
cellular toxicity, a potential lack of specificity for HIV, or low potency when
each agent is used alone. RNA molecules, such as microRNAs (miRNAs) and long
non-coding RNAs (lncRNAs) are becoming increasingly recognized as important
regulators of gene expression. RNA-based approaches for combatting HIV latency
represent a promising strategy since both miRNAs and lncRNAs are more cell-type
and tissue specific than protein coding genes. Thus, a higher specificity of
targeting the latent HIV reservoir with less overall cellular toxicity can
likely be achieved. In this review, we summarize current knowledge about HIV
gene expression regulation by miRNAs and lncRNAs encoded in the human genome,
as well as regulatory molecules encoded in the HIV genome. We discuss both the
transcriptional and post-transcriptional regulation of HIV gene expression to
align with the current definition of latency, and describe RNA molecules that
either promote HIV latency or have anti-latency properties. Finally, we provide
perspectives on using each class of RNAs as potential targets for combatting
HIV latency, and describe the complexity of the interactions between different
RNA molecules, their protein targets, and HIV.; 11) Steering Protein Family Design through Profile Bayesian Flow; Protein family design emerges as a promising alternative by combining the
advantages of de novo protein design and mutation-based directed evolution.In
this paper, we propose ProfileBFN, the Profile Bayesian Flow Networks, for
specifically generative modeling of protein families. ProfileBFN extends the
discrete Bayesian Flow Network from an MSA profile perspective, which can be
trained on single protein sequences by regarding it as a degenerate profile,
thereby achieving efficient protein family design by avoiding large-scale MSA
data construction and training. Empirical results show that ProfileBFN has a
profound understanding of proteins. When generating diverse and novel family
proteins, it can accurately capture the structural characteristics of the
family. The enzyme produced by this method is more likely than the previous
approach to have the corresponding function, offering better odds of generating
diverse proteins with the desired functionality.; 12) SMTPD: A New Benchmark for Temporal Prediction of Social Media
  Popularity; Social media popularity prediction task aims to predict the popularity of
posts on social media platforms, which has a positive driving effect on
application scenarios such as content optimization, digital marketing and
online advertising. Though many studies have made significant progress, few of
them pay much attention to the integration between popularity prediction with
temporal alignment. In this paper, with exploring YouTube's multilingual and
multi-modal content, we construct a new social media temporal popularity
prediction benchmark, namely SMTPD, and suggest a baseline framework for
temporal popularity prediction. Through data analysis and experiments, we
verify that temporal alignment and early popularity play crucial roles in
social media popularity prediction for not only deepening the understanding of
temporal dynamics of popularity in social media but also offering a suggestion
about developing more effective prediction models in this field. Code is
available at https://github.com/zhuwei321/SMTPD.; 13) Strongly self-dual polytopes; This article aims to study the class of strongly self-dual polytopes
(ssd-polytopes for short), defined in a paper by Lov\'asz \cite{lovasz}. He
described a series of such polytopes (called $L$-type polytopes), which he used
to solve a combinatorial problem. From a geometrical point of view, there are
interesting questions: what additional elements of this class exist, and are
there any with a different structure from the $L$-type ones? We show that in
dimension three, one of their faces defines $L$-type polyhedra. Illustrating
the algorithm of the proof, we present an ssd-polytope of 23 vertices whose
combinatorial structure differ from those of $L$-type ones. Finally, with an
elementary discussion, we prove that for fewer than nine vertices, there are
only fifth one ssd-polyhedra, four of them can be constructed by Lov\'asz's
method, and we can find the fifth one with ""the diameter gradient flow
algorithm"" of Katz, Memoli and Wang \cite{katz-memoli-wang}.; 14) Enhancing Subject-Independent Accuracy in fNIRS-based Brain-Computer
  Interfaces with Optimized Channel Selection; Achieving high subject-independent accuracy in functional near-infrared
spectroscopy (fNIRS)-based brain-computer interfaces (BCIs) remains a
challenge, particularly when minimizing the number of channels. This study
proposes a novel feature extraction scheme and a Pearson correlation-based
channel selection algorithm to enhance classification accuracy while reducing
hardware complexity. Using an open-access fNIRS dataset, our method improved
average accuracy by 28.09% compared to existing approaches, achieving a peak
subject-independent accuracy of 95.98% with only two channels. These results
demonstrate the potential of our optimized feature extraction and channel
selection methods for developing efficient, subject-independent fNIRS-based BCI
systems.; 15) Unraveling screening mechanisms in Kondo impurities using an
  NRG-MPS-based method; The Kondo effect is a hallmark of strongly-correlated systems, where an
impurity's local degrees of freedom are screened by conduction electrons,
forming a many-body singlet. With increasing degrees of freedom in the
impurity, theoretical studies face significant challenges in accurately
identifying and characterizing the underlying mechanisms that screen the
impurity. In this work, we introduce a straightforward yet powerful methodology
for identifying the formation of Kondo singlets and their screening mechanisms,
by utilizing the numerical renormalization group (NRG) combined with the matrix
product states (MPS) technique. We demonstrate the effectiveness of our method
on the single and two-level Anderson impurity models (AIM). Furthermore, we
discuss potential generalizations of the method to multichannel and
multiorbital Kondo impurities. Harnessing advanced tensor network techniques,
our approach extends to complex impurity systems, offering a robust and
versatile framework for studying Kondo physics.; 16) Probing Large Language Models in Reasoning and Translating Complex
  Linguistic Puzzles; This paper investigates the utilization of Large Language Models (LLMs) for
solving complex linguistic puzzles, a domain requiring advanced reasoning and
adept translation capabilities akin to human cognitive processes. We explore
specific prompting techniques designed to enhance ability of LLMs to reason and
elucidate their decision-making pathways, with a focus on Input-Output
Prompting (IO), Chain-of-Thought Prompting (CoT), and Solo Performance
Prompting (SPP). Utilizing datasets from the Puzzling Machine Competition and
various Linguistics Olympiads, we employ a comprehensive set of metrics to
assess the performance of GPT-4 0603, a prominent LLM, across these prompting
methods. Our findings illuminate the potential of LLMs in linguistic reasoning
and complex translation tasks, highlighting their capabilities and identifying
limitations in the context of linguistic puzzles. This research contributes
significantly to the broader field of Natural Language Processing (NLP) by
providing insights into the optimization of LLM applications for improved
reasoning and translation accuracy, thereby enriching the ongoing dialogue in
NLP advancements.; 17) Deep Learning of Proteins with Local and Global Regions of Disorder; Although machine learning has transformed protein structure prediction of
folded protein ground states with remarkable accuracy, intrinsically disordered
proteins and regions (IDPs/IDRs) are defined by diverse and dynamical
structural ensembles that are predicted with low confidence by algorithms such
as AlphaFold. We present a new machine learning method, IDPForge (Intrinsically
Disordered Protein, FOlded and disordered Region GEnerator), that exploits a
transformer protein language diffusion model to create all-atom IDP ensembles
and IDR disordered ensembles that maintains the folded domains. IDPForge does
not require sequence-specific training, back transformations from
coarse-grained representations, nor ensemble reweighting, as in general the
created IDP/IDR conformational ensembles show good agreement with solution
experimental data, and options for biasing with experimental restraints are
provided if desired. We envision that IDPForge with these diverse capabilities
will facilitate integrative and structural studies for proteins that contain
intrinsic disorder.; 18) Hengqin-RA-v1: Advanced Large Language Model for Diagnosis and Treatment
  of Rheumatoid Arthritis with Dataset based Traditional Chinese Medicine; Large language models (LLMs) primarily trained on English texts, often face
biases and inaccuracies in Chinese contexts. Their limitations are pronounced
in fields like Traditional Chinese Medicine (TCM), where cultural and clinical
subtleties are vital, further hindered by a lack of domain-specific data, such
as rheumatoid arthritis (RA). To address these issues, this paper introduces
Hengqin-RA-v1, the first large language model specifically tailored for TCM
with a focus on diagnosing and treating RA. We also present HQ-GCM-RA-C1, a
comprehensive RA-specific dataset curated from ancient Chinese medical
literature, classical texts, and modern clinical studies. This dataset empowers
Hengqin-RA-v1 to deliver accurate and culturally informed responses,
effectively bridging the gaps left by general-purpose models. Extensive
experiments demonstrate that Hengqin-RA-v1 outperforms state-of-the-art models,
even surpassing the diagnostic accuracy of TCM practitioners in certain cases.; 19) Leveraging Sequence Purification for Accurate Prediction of Multiple
  Conformational States with AlphaFold2; AlphaFold2 (AF2) has transformed protein structure prediction by harnessing
co-evolutionary constraints embedded in multiple sequence alignments (MSAs).
MSAs not only encode static structural information, but also hold critical
details about protein dynamics, which underpin biological functions. However,
these subtle co-evolutionary signatures, which dictate conformational state
preferences, are often obscured by noise within MSA data and thus remain
challenging to decipher. Here, we introduce AF-ClaSeq, a systematic framework
that isolates these co-evolutionary signals through sequence purification and
iterative enrichment. By extracting sequence subsets that preferentially encode
distinct structural states, AF-ClaSeq enables high-confidence predictions of
alternative conformations. Our findings reveal that the successful sampling of
alternative states depends not on MSA depth but on sequence purity.
Intriguingly, purified sequences encoding specific structural states are
distributed across phylogenetic clades and superfamilies, rather than confined
to specific lineages. Expanding upon AF2's transformative capabilities,
AF-ClaSeq provides a powerful approach for uncovering hidden structural
plasticity, advancing allosteric protein and drug design, and facilitating
dynamics-based protein function annotation.; 20) Progressive Local Alignment for Medical Multimodal Pre-training; Local alignment between medical images and text is essential for accurate
diagnosis, though it remains challenging due to the absence of natural local
pairings and the limitations of rigid region recognition methods. Traditional
approaches rely on hard boundaries, which introduce uncertainty, whereas
medical imaging demands flexible soft region recognition to handle irregular
structures. To overcome these challenges, we propose the Progressive Local
Alignment Network (PLAN), which designs a novel contrastive learning-based
approach for local alignment to establish meaningful word-pixel relationships
and introduces a progressive learning strategy to iteratively refine these
relationships, enhancing alignment precision and robustness. By combining these
techniques, PLAN effectively improves soft region recognition while suppressing
noise interference. Extensive experiments on multiple medical datasets
demonstrate that PLAN surpasses state-of-the-art methods in phrase grounding,
image-text retrieval, object detection, and zero-shot classification, setting a
new benchmark for medical image-text alignment.; 21) On the $\Sigma^1$ and $\Sigma^2$-invariants of Artin groups; We prove the $\Sigma^1$-conjecture for the family of balanced Artin groups, a
family that generalizes the considered by Kochloukova in arXiv:2009.14269, the
considered by Escart\'in-Martinez in arXiv:2309.03091 and the family of
coherent Artin groups. We state a conjecture on the $\Sigma^2$-invariant for
Artin groups and prove it for the families of $2$-dimensional and coherent
Artin groups.; 22) High angular resolution evidence of dust traps from deep ALMA Band 3
  observations of LkCa15; Dust traps are the most promising mechanisms to explain the observed
substructures in protoplanetary discs. In this work, we present high-angular
resolution ($\sim$60 mas, 9.4\,au) and high-sensitivity Atacama Large
Millimetre/submillimetre Array (ALMA) observations at 3 mm of the transitional
disc around LkCa15. The new data, combined with previous high-resolution
observations at $\lambda=0.87,1.3$ mm, make LkCa15 an ideal laboratory for
testing the dust trapping mechanism. We found that the width of the three rings
decreases linearly with frequency, and the spectral indices show local minima
at the locations of the rings, consistent with dust trap models.
Multi-wavelength modelling confirms that the dust surface density and maximum
grain size peak at 69 and 101\,au, and suggestive peak at 42\,au. The estimated
total dust mass is between 13-250 M$_{\oplus}$, depending on the chosen
opacity. The inner disc shows bright and unresolved emission at 3 mm,
exhibiting a spectral index of $\alpha_{1.3-3 \rm mm} = 0.3 \pm 0.37$, and
$\alpha_{\rm 3mm-3cm}$ ranging from $-0.1$ to $0.0$. These properties are
consistent with free-free emission from an ionised jet or disc wind. Dust
evolution models and radiative transfer calculations suggest that a viscosity
coefficient of $\alpha = 10^{-3}$, a fragmentation velocity of 10 m\,s$^{-1}$,
and DSHARP opacities provide the best match to the observed properties.; 23) Advancing Dense Endoscopic Reconstruction with Gaussian Splatting-driven
  Surface Normal-aware Tracking and Mapping; Simultaneous Localization and Mapping (SLAM) is essential for precise
surgical interventions and robotic tasks in minimally invasive procedures.
While recent advancements in 3D Gaussian Splatting (3DGS) have improved SLAM
with high-quality novel view synthesis and fast rendering, these systems
struggle with accurate depth and surface reconstruction due to multi-view
inconsistencies. Simply incorporating SLAM and 3DGS leads to mismatches between
the reconstructed frames. In this work, we present Endo-2DTAM, a real-time
endoscopic SLAM system with 2D Gaussian Splatting (2DGS) to address these
challenges. Endo-2DTAM incorporates a surface normal-aware pipeline, which
consists of tracking, mapping, and bundle adjustment modules for geometrically
accurate reconstruction. Our robust tracking module combines point-to-point and
point-to-plane distance metrics, while the mapping module utilizes normal
consistency and depth distortion to enhance surface reconstruction quality. We
also introduce a pose-consistent strategy for efficient and geometrically
coherent keyframe sampling. Extensive experiments on public endoscopic datasets
demonstrate that Endo-2DTAM achieves an RMSE of $1.87\pm 0.63$ mm for depth
reconstruction of surgical scenes while maintaining computationally efficient
tracking, high-quality visual appearance, and real-time rendering. Our code
will be released at github.com/lastbasket/Endo-2DTAM.; 24) Remodeling Peptide-MHC-TCR Triad Binding as Sequence Fusion for
  Immunogenicity Prediction; The complex nature of tripartite peptide-MHC-TCR interactions is a critical
yet underexplored area in immunogenicity prediction. Traditional studies on
TCR-antigen binding have not fully addressed the complex dependencies in triad
binding. In this paper, we propose new modeling approaches for these tripartite
interactions, utilizing sequence information from MHCs, peptides, and TCRs. Our
methods adhere to native sequence forms and align with biological processes to
enhance prediction accuracy. By incorporating representation learning
techniques, we introduce a fusion mechanism to integrate the three sequences
effectively. Empirical experiments show that our models outperform traditional
methods, achieving a 2.8 to 13.3 percent improvement in prediction accuracy
across existing benchmarks. We further validate our approach with extensive
ablation studies, demonstrating the effectiveness of the proposed model
components. The model implementation, code, and supplementary materials,
including a manuscript with colored hyperlinks and a technical appendix for
digital viewing, will be open-sourced upon publication.; 25) C codegen considered unnecessary: go directly to binary, do not pass C.
  Compilation of Julia code for deployment in model-based engineering; Since time immemorial an old adage has always seemed to ring true: you cannot
use a high-level productive programming language like Python or R for real-time
control and embedded-systems programming, you must rewrite your program in C.
We present a counterexample to this mantra by demonstrating how recent compiler
developments in the Julia programming language allow users of Julia and the
equation-based modeling language ModelingToolkit to compile and deploy binaries
for real-time model-based estimation and control. Contrary to the approach
taken by a majority of modeling and simulation tools, we do not generate C
code, and instead demonstrate how we may use the native Julia code-generation
pipeline through LLVM to compile architecture-specific binaries from high-level
code. This approach avoids many of the restrictions typically placed on
high-level languages to enable C-code generation. As case studies, we include a
nonlinear state estimator derived from an equation-based model which is
compiled into a program that performs state estimation for deployment onto a
Raspberry Pi, as well as a PID controller library implemented in Julia and
compiled into a shared library callable from a C program.; 26) Applying computational protein design to therapeutic antibody discovery
  -- current state and perspectives; Machine learning applications in protein sciences have ushered in a new era
for designing molecules in silico. Antibodies, which currently form the largest
group of biologics in clinical use, stand to benefit greatly from this shift.
Despite the proliferation of these protein design tools, their direct
application to antibodies is often limited by the unique structural biology of
these molecules. Here, we review the current computational methods for antibody
design, highlighting their role in advancing computational drug discovery.; 27) PyMOLfold: Interactive Protein and Ligand Structure Prediction in PyMOL; PyMOLfold is a flexible and open-source plugin designed to seamlessly
integrate AI-based protein structure prediction and visualization within the
widely used PyMOL molecular graphics system. By leveraging state-of-the-art
protein folding models such as ESM3, Boltz-1, and Chai-1, PyMOLfold allows
researchers to directly predict protein tertiary structures from amino acid
sequences without requiring external tools or complex workflows. Furthermore,
with certain models, users can provide a SMILES string of a ligand and have the
small molecule placed in the protein structure. This unique capability bridges
the gap between computational folding and structural visualization, enabling
users to input a primary sequence, perform a folding prediction, and
immediately explore the resulting 3D structure within the same intuitive
platform.; 28) Theory of composite Ramsey sequences of radiofrequency pulses beyond the
  rotating wave approximation; We develop a theory of composite Ramsey sequences of rf pulses interacting
with the Zeeman structure at the long-lived atomic level, beyond the rotating
wave approximation. Such sequences are proposed in experiments to detect the
violation of local Lorentz invariance [R. Shaniv, et al., Phys. Rev. Lett. 120,
103202 (2018)]. Based on Fourier analysis, we have shown that taking into
account non-resonant contributions leads to a radical change in the dynamics of
the quantum system (with respect to the rotating wave approximation) in the
case when the number of Ramsey pulses exceeds several tens. As a result, the
effectiveness of using such rf pulses sequences to test local Lorentz
invariance has not yet been fully determined and requires additional research.; 29) Timing Matters: How Using LLMs at Different Timings Influences Writers'
  Perceptions and Ideation Outcomes in AI-Assisted Ideation; Large Language Models (LLMs) have been widely used to support ideation in the
writing process. However, whether generating ideas with the help of LLMs leads
to idea fixation or idea expansion is unclear. This study examines how
different timings of LLM usage - either at the beginning or after independent
ideation - affect people's perceptions and ideation outcomes in a writing task.
In a controlled experiment with 60 participants, we found that using LLMs from
the beginning reduced the number of original ideas and lowered creative
self-efficacy and self-credit, mediated by changes in autonomy and ownership.
We discuss the challenges and opportunities associated with using LLMs to
assist in idea generation. We propose delaying the use of LLMs to support
ideation while considering users' self-efficacy, autonomy, and ownership of the
ideation outcomes.; 30) Squeezing-enhanced accurate differential sensing under large phase noise; Atom interferometers are reaching sensitivities fundamentally constrained by
quantum fluctuations. A main challenge is to integrate entanglement into
quantum sensing protocols to enhance precision while ensuring robustness
against noise and systematics. Here, we theoretically investigate differential
phase measurements with two atom interferometers using spin-squeezed states,
accounting for common-mode phase noise spanning the full $2\pi$ range. We
estimate the differential signal using model-free ellipse fitting, a robust
method requiring no device calibration and resilient to additional noise
sources. Our results show that spin-squeezing enables sensitivities below the
standard quantum limit. Specifically, we identify optimal squeezed states that
minimize the differential phase variance, scaling as $N^{-2/3}$, while
eliminating bias inherent in ellipse fitting methods. We benchmark our protocol
against the Cram\'er-Rao bound and compare it with hybrid methods that
incorporate auxiliary classical sensors. Our findings provide a pathway to
robust and high-precision atom interferometry, in realistic noisy environments
and using readily available states and estimation methods.; 31) Quantum geometric tensors from sub-bundle geometry; The geometric properties of quantum states are crucial for understanding many
physical phenomena in quantum mechanics, condensed matter physics, and optics.
The central object describing these properties is the quantum geometric tensor,
which unifies the Berry curvature and the quantum metric. In this work, we use
the differential-geometric framework of vector bundles to analyze the
properties of parameter-dependent quantum states and generalize the quantum
geometric tensor to this setting. This construction is based on an arbitrary
connection on a Hermitian vector bundle, which defines a notion of quantum
state transport in parameter space, and a sub-bundle projector, which
constrains the set of accessible quantum states. We show that the sub-bundle
geometry is similar to that of submanifolds in Riemannian geometry and is
described by a generalization of the Gauss-Codazzi-Mainardi equations. This
leads to a novel definition of the quantum geometric tensor, which contains an
additional curvature contribution. To illustrate our results, we describe the
sub-bundle geometry arising in the semiclassical treatment of Dirac fields
propagating in curved spacetime and show how the quantum geometric tensor, with
its additional curvature contributions, is obtained in this case. As a concrete
example, we consider Dirac fermions confined to a hyperbolic plane and
demonstrate how spatial curvature influences the quantum geometry. This work
sets the stage for further exploration of quantum systems in curved geometries,
with applications in both high-energy physics and condensed matter systems.; 32) Artificial Intelligence Approaches for Anti-Addiction Drug Discovery; Drug addiction is a complex and pervasive global challenge that continues to
pose significant public health concerns. Traditional approaches to
anti-addiction drug discovery have struggled to deliver effective therapeutics,
facing high attrition rates, long development timelines, and inefficiencies in
processing large-scale data. Artificial intelligence (AI) has emerged as a
transformative solution to address these issues. Using advanced algorithms, AI
is revolutionizing drug discovery by enhancing the speed and precision of key
processes. This review explores the transformative role of AI in the pipeline
for anti-addiction drug discovery, including data collection, target
identification, and compound optimization. By highlighting the potential of AI
to overcome traditional barriers, this review systematically examines how AI
addresses critical gaps in anti-addiction research, emphasizing its potential
to revolutionize drug discovery and development, overcome challenges, and
advance more effective therapeutic strategies.; 33) Testing degree heterogeneity in directed networks; We are concerned with the likelihood ratio tests in the $p_0$ model for
testing degree heterogeneity in directed networks. It is an exponential family
distribution on directed graphs with the out-degree sequence and the in-degree
sequence as naturally sufficient statistics. For two growing dimensional null
hypotheses: a specified null $H_{0}: \theta_{i}=\theta_{i}^{0}$ for
$i=1,\ldots,r$ and a homogenous null $H_{0}: \theta_{1}=\cdots=\theta_{r}$, we
reveal high dimensional Wilks' phenomena that the normalized log-likelihood
ratio statistic,
$[2\{\ell(\widehat{\bs\theta})-\ell(\widehat{\bs\theta}^{0})\}-r]/(2r)^{1/2}$,
converges in distribution to a standard normal distribution as $r\rightarrow
\infty$. Here, $\ell( \bs{\theta})$ is the log-likelihood function,
$\widehat{\bs{\theta}}$ is the unrestricted maximum likelihood estimator (MLE)
of $\bs\theta$, and $\widehat{\bs{\theta}}^0$ is the restricted MLE for
$\bs\theta$ under the null $H_{0}$. For the homogenous null $H_0:
\theta_1=\cdots=\theta_r$ with a fixed $r$, we establish the Wilks-type theorem
that $2\{\ell(\widehat{\bs{\theta}}) - \ell(\widehat{\bs{\theta}}^0)\}$
converges in distribution to a chi-square distribution with $r-1$ degrees of
freedom as $n\rightarrow \infty$, not depending on the nuisance parameters.
These results extend a recent work by \cite{yan2023likelihood} to directed
graphs. Simulation studies and real data analyses illustrate the theoretical
results.; 34) PDRs4All. XII. FUV-driven formation of hydrocarbon radicals and their
  relation with PAHs; We present subarcsecond-resolution ALMA mosaics of the Orion Bar PDR in [CI]
609um, C2H (4-3), and C18O (3-2) emission lines complemented by JWST images of
H2 and aromatic infrared band (AIB) emission. The rim of the Bar shows very
corrugated structures made of small-scale H2 dissociation fronts (DFs). The
[CI] 609 um emission peaks very close (~0.002 pc) to the main H2-emitting DFs,
suggesting the presence of gas density gradients. These DFs are also bright and
remarkably similar in C2H emission, which traces ""hydrocarbon radical peaks""
characterized by very high C2H abundances, reaching up to several x10^-7. The
high abundance of C2H and of related hydrocarbon radicals, such as CH3, CH2,
and CH, can be attributed to gas-phase reactions driven by elevated
temperatures, the presence of C+ and C, and the reactivity of FUV-pumped H2.
The hydrocarbon radical peaks roughly coincide with maxima of the 3.4/3.3 um
AIB intensity ratio, a proxy for the aliphatic-to-aromatic content of PAHs.
This implies that the conditions triggering the formation of simple
hydrocarbons also favor the formation (and survival) of PAHs with aliphatic
side groups, potentially via the contribution of bottom-up processes in which
abundant hydrocarbon radicals react in situ with PAHs. Ahead of the DFs, in the
atomic PDR zone (where [H]>>[H2]), the AIB emission is the brightest, but small
PAHs and carbonaceous grains undergo photo-processing due to the stronger FUV
field. Our detection of trace amounts of C2H in this zone may result from the
photoerosion of these species. This study provides a spatially resolved view of
the chemical stratification of key carbon carriers in a PDR. Overall, both
bottom-up and top-down processes appear to link simple hydrocarbon molecules
with PAHs in molecular clouds; however, the exact chemical pathways and their
relative contributions remain to be quantified.; 35) A population synthesis study of the Gaia 100 pc unresolved white
  dwarf-main sequence binary population; Binary stars consisting of a white dwarf and a main sequence star (WDMS) are
valuable for studying key astrophysical questions. However, observational
biases strongly affect the known population, particularly unresolved systems
where the main sequence star outshines the white dwarf. This work aims to
comprehensively simulate the population of unresolved WDMS binaries within 100
pc of the Sun and to compare the outcome with the currently most complete
volume-limited sample available from Gaia data. We employ a population
synthesis code, MRBIN, extensively developed by our group and based on Monte
Carlo techniques, which uses a standard binary stellar evolutionary code
adapted to cover a wide range of stars across all ages, masses, and
metallicities. Selection criteria matching those of Gaia observations are
applied to generate synthetic populations comparable to the observed WDMS
sample. The synthetic data accurately populate the expected regions in the Gaia
color-magnitude diagram. However, simulations predict a lower number of
extremely low-mass white dwarfs, suggesting potential issues in observed mass
derivations. Additionally, our analysis constrains the common envelope
efficiency to 0.1-0.4, consistent with previous findings, and estimates a total
completeness of about 25% for the observed sample, confirming the strong
observational limitations for unresolved WDMS.; 36) No ""chaos"" in bosonic string massive scalar amplitudes; We compute directly in covariant formalism a variety of four point massive
scalar amplitudes in bosonic string with scalars up to level 10 and at least
one tachyon.
  All computed amplitudes are perfectly regular and show no sign of erratic
behavior.
  We give a naive argument based on a very simple model of why this can
reasonably be expected and we check on the explicit examples that the intuition
derived from the model is sensible.
  Furthermore we discuss the complications due to null states (BRST exact
states) in actually computing the covariant bosonic string spectrum and how
these can be overcome by using a ``$\Pi$ gauge'' which makes manifest that the
degeneration of any spin at any mass level is independent on the spacetime
dimension.; 37) Sakshm AI: Advancing AI-Assisted Coding Education for Engineering
  Students in India Through Socratic Tutoring and Comprehensive Feedback; The advent of Large Language Models (LLMs) is reshaping education,
particularly in programming, by enhancing problem-solving, enabling
personalized feedback, and supporting adaptive learning. Existing AI tools for
programming education struggle with key challenges, including the lack of
Socratic guidance, direct code generation, limited context retention, minimal
adaptive feedback, and the need for prompt engineering. To address these
challenges, we introduce Sakshm AI, an intelligent tutoring system for learners
across all education levels. It fosters Socratic learning through Disha, its
inbuilt AI chatbot, which provides context-aware hints, structured feedback,
and adaptive guidance while maintaining conversational memory and supporting
language flexibility. This study examines 1170 registered participants,
analyzing platform logs, engagement trends, and problem-solving behavior to
assess Sakshm AI's impact. Additionally, a structured survey with 45 active
users and 25 in-depth interviews was conducted, using thematic encoding to
extract qualitative insights. Our findings reveal how AI-driven Socratic
guidance influences problem-solving behaviors and engagement, offering key
recommendations for optimizing AI-based coding platforms. This research
combines quantitative and qualitative insights to inform AI-assisted education,
providing a framework for scalable, intelligent tutoring systems that improve
learning outcomes. Furthermore, Sakshm AI represents a significant step toward
Sustainable Development Goal 4 Quality Education, providing an accessible and
structured learning tool for undergraduate students, even without expert
guidance. This is one of the first large-scale studies examining AI-assisted
programming education across multiple institutions and demographics.; 38) QFAL: Quantum Federated Adversarial Learning; Quantum federated learning (QFL) merges the privacy advantages of federated
systems with the computational potential of quantum neural networks (QNNs), yet
its vulnerability to adversarial attacks remains poorly understood. This work
pioneers the integration of adversarial training into QFL, proposing a robust
framework, quantum federated adversarial learning (QFAL), where clients
collaboratively defend against perturbations by combining local adversarial
example generation with federated averaging (FedAvg). We systematically
evaluate the interplay between three critical factors: client count (5, 10,
15), adversarial training coverage (0-100%), and adversarial attack
perturbation strength (epsilon = 0.01-0.5), using the MNIST dataset. Our
experimental results show that while fewer clients often yield higher
clean-data accuracy, larger federations can more effectively balance accuracy
and robustness when partially adversarially trained. Notably, even limited
adversarial coverage (e.g., 20%-50%) can significantly improve resilience to
moderate perturbations, though at the cost of reduced baseline performance.
Conversely, full adversarial training (100%) may regain high clean accuracy but
is vulnerable under stronger attacks. These findings underscore an inherent
trade-off between robust and standard objectives, which is further complicated
by quantum-specific factors. We conclude that a carefully chosen combination of
client count and adversarial coverage is critical for mitigating adversarial
vulnerabilities in QFL. Moreover, we highlight opportunities for future
research, including adaptive adversarial training schedules, more diverse
quantum encoding schemes, and personalized defense strategies to further
enhance the robustness-accuracy trade-off in real-world quantum federated
environments.; 39) Quasi-two-dimensional Fermi surfaces of the antiferromagnet
  U$_2$RhIn$_8$ revealed by de Haas-van Alphen measurements; We report temperature-dependent Hall effect and low-temperature de Haas-van
Alphen (dHvA) effect measurements of the antiferromagnetic heavy-fermion
compound U$_2$RhIn$_8$. Temperature dependence of the Hall resistivity suggests
a considerable reduction of the carrier density in the antiferromagnetic phase.
The observed angular dependence of the dHvA frequencies suggests the existence
of three almost ideally two-dimensional Fermi surfaces one of which is quite
large. The measured effective masses range from 2$m_0$ to 14$m_0$ for the field
applied along the $c$ axis. Local density approximation band-structure
calculations performed for the paramagnetic ground state reveal more
three-dimensional Fermi surfaces than those observed in the experiment. On the
other hand, Fermi surfaces obtained for the antiferromagnetic ground state by
band folding are more two dimensional. These calculations account reasonably
well for the experimental results assuming a slight modification of the
calculated Fermi surfaces.; 40) Insights from Network Science can advance Deep Graph Learning; Deep graph learning and network science both analyze graphs but approach
similar problems from different perspectives. Whereas network science focuses
on models and measures that reveal the organizational principles of complex
systems with explicit assumptions, deep graph learning focuses on flexible and
generalizable models that learn patterns in graph data in an automated fashion.
Despite these differences, both fields share the same goal: to better model and
understand patterns in graph-structured data. Early efforts to integrate
methods, models, and measures from network science and deep graph learning
indicate significant untapped potential. In this position, we explore
opportunities at their intersection. We discuss open challenges in deep graph
learning, including data augmentation, improved evaluation practices,
higher-order models, and pooling methods. Likewise, we highlight challenges in
network science, including scaling to massive graphs, integrating continuous
gradient-based optimization, and developing standardized benchmarks.; 41) Two-dimensional topological quantum field theories of rank two over
  Dedekind domains; We give examples of Frobenius algebras of rank two over ground Dedekind rings
which are projective but not free and discuss possible applications of these
algebras to link homology.; 42) Mechanism of Electricacupuncture Treating Detrusor Bladder Neck
  Dyscoordination After Suprasacral Spinal Cord Injury by Proteomics; Objectives This study aimed to elucidate the potential mechanisms of
electroacupuncture (EA) in restoring detrusor-bladder neck dyssynergesia (DBND)
following suprasacral spinal cord injury.
  Methods A total of 52 adult female Sprague-Dawley rats were randomly assigned
to either a sham group (n=12) or a spinal cord injury model group (n=40). In
the model group, DBND was induced in 40 rats through Hassan Shaker spinal cord
transection, with 24 rats surviving spinal shock and subsequently randomized
into two groups: a model-only group (DBND, n=12) and an EA intervention group
(DBND+EA, n=12). DBND+EA was administered at Ciliao (BL32), Zhongji (RN3), and
Sanyinjiao (SP6) acupoints, for 20 minutes per session, once daily for 10
consecutive days. On day 29 post-injury, all rats underwent urodynamic
assessments, followed by hematoxylin and eosin (HE) staining, tandem mass tag
(TMT) proteomics, and Western blot (WB) analysis of the detrusor and bladder
neck tissues.
  Results Urodynamic evaluation demonstrated that EA intervention enhanced
bladder function in DBND rats. HE staining indicated reduced fibroplasia in the
detrusor muscle and alleviated inflammation in the bladder neck following EA.
TMT proteomic analysis revealed 30 differentially expressed proteins (DEPs) in
the detrusor and 59 DEPs in the bladder neck post-EA treatment. WB results
corroborated these TMT findings.
  Conclusion EA effectively promotes synergy between the detrusor muscle and
bladder neck in DBND, likely by enhancing detrusor contractility and
facilitating bladder neck relaxation during urination. This study provides
mechanistic insights into the therapeutic role of EA in managing DBND.; 43) Event-based Video Person Re-identification via Cross-Modality and
  Temporal Collaboration; Video-based person re-identification (ReID) has become increasingly important
due to its applications in video surveillance applications. By employing events
in video-based person ReID, more motion information can be provided between
continuous frames to improve recognition accuracy. Previous approaches have
assisted by introducing event data into the video person ReID task, but they
still cannot avoid the privacy leakage problem caused by RGB images. In order
to avoid privacy attacks and to take advantage of the benefits of event data,
we consider using only event data. To make full use of the information in the
event stream, we propose a Cross-Modality and Temporal Collaboration (CMTC)
network for event-based video person ReID. First, we design an event transform
network to obtain corresponding auxiliary information from the input of raw
events. Additionally, we propose a differential modality collaboration module
to balance the roles of events and auxiliaries to achieve complementary
effects. Furthermore, we introduce a temporal collaboration module to exploit
motion information and appearance cues. Experimental results demonstrate that
our method outperforms others in the task of event-based video person ReID.; 44) Prediction of Binding Affinity for ErbB Inhibitors Using Deep Neural
  Network Model with Morgan Fingerprints as Features; The ErbB receptor family, including EGFR and HER2, plays a crucial role in
cell growth and survival and is associated with the progression of various
cancers such as breast and lung cancer. In this study, we developed a deep
learning model to predict the binding affinity of ErbB inhibitors using
molecular fingerprints derived from SMILES representations. The SMILES
representations for each ErbB inhibitor were obtained from the ChEMBL database.
We first generated Morgan fingerprints from the SMILES strings and applied
AutoDock Vina docking to calculate the binding affinity values. After filtering
the dataset based on binding affinity, we trained a deep neural network (DNN)
model to predict binding affinity values from the molecular fingerprints. The
model achieved significant performance, with a Mean Squared Error (MSE) of
0.2591, Mean Absolute Error (MAE) of 0.3658, and an R-squared value of 0.9389
on the training set. Although performance decreased slightly on the test set (R
squared = 0.7731), the model still demonstrated robust generalization
capabilities. These results indicate that the deep learning approach is highly
effective for predicting the binding affinity of ErbB inhibitors, offering a
valuable tool for virtual screening and drug discovery.; 45) Non-Topological Soliton Bardeen Boson Stars and Their Frozen States; We investigate a Bardeen model coupling Einstein gravity with nonlinear
electromagnetic fields and non-topological soliton complex scalar fields,
governed by the magnetic charge $\tilde{q}$, the complex scalar field frequency
$\tilde{\omega}$, and the self-interaction parameter $\tilde{\eta}$. Our
results reveal that the magnetic charge $\tilde{q}$ exhibits
$\tilde{\eta}$-dependent critical values $\tilde{q}_c$, beyond which
($\tilde{q} > \tilde{q}_c$) Bardeen boson stars (BBSs) may transition into
frozen states ($\tilde{\omega} \to 0$). These frozen states are characterized
by a critical horizon whose radius $\tilde r^\mathrm{H}_{c}$ satisfies $\tilde
r_{\text{inner}}^{\text{H,RN}} < \tilde r_{c} < \tilde
r_{\text{outer}}^{\text{H,RN}}$, where $\tilde r_{\text{inner}}^{\text{H,RN}}$
and $\tilde r_{\text{outer}}^{\text{RN}}$ denote the inner and outer horizons
of magnetic Reissner-Nordstr\""{o}m (RN) black holes with equivalent mass and
magnetic charge. Notably, the ADM mass of frozen BBSs is independent of
$\tilde{\eta}$. Furthermore, light ring (LR) solutions exist universally across
all tested combinations of $\tilde{q}$ and $\tilde{\eta}$, with all frozen BBSs
exhibiting LRs whose outer radius $\tilde r_{\text{outer}}^{\text{LR}}$ is
independent of $\tilde{\eta}$. Compared to magnetic RN black holes, frozen BBSs
possess a smaller outer LR radius ($\tilde r_{\text{outer}}^{\text{LR}} <
\tilde r_{\text{outer}}^{\text{LR, RN}}$).; 46) De-centering the (Traditional) User: Multistakeholder Evaluation of
  Recommender Systems; Multistakeholder recommender systems are those that account for the impacts
and preferences of multiple groups of individuals, not just the end users
receiving recommendations. Due to their complexity, evaluating these systems
cannot be restricted to the overall utility of a single stakeholder, as is
often the case of more mainstream recommender system applications. In this
article, we focus our discussion on the intricacies of the evaluation of
multistakeholder recommender systems. We bring attention to the different
aspects involved in the evaluation of multistakeholder recommender systems -
from the range of stakeholders involved (including but not limited to producers
and consumers) to the values and specific goals of each relevant stakeholder.
Additionally, we discuss how to move from theoretical principles to practical
implementation, providing specific use case examples. Finally, we outline open
research directions for the RecSys community to explore. We aim to provide
guidance to researchers and practitioners about how to think about these
complex and domain-dependent issues of evaluation in the course of designing,
developing, and researching applications with multistakeholder aspects.; 47) A Multicast-Capable AXI Crossbar for Many-core Machine Learning
  Accelerators; To keep up with the growing computational requirements of machine learning
workloads, many-core accelerators integrate an ever-increasing number of
processing elements, putting the efficiency of memory and interconnect
subsystems to the test. In this work, we present the design of a
multicast-capable AXI crossbar, with the goal of enhancing data movement
efficiency in massively parallel machine learning accelerators. We propose a
lightweight, yet flexible, multicast implementation, with a modest area and
timing overhead (12% and 6% respectively) even on the largest
physically-implementable 16-to-16 AXI crossbar. To demonstrate the flexibility
and end-to-end benefits of our design, we integrate our extension into an
open-source 288-core accelerator. We report tangible performance improvements
on a key computational kernel for machine learning workloads, matrix
multiplication, measuring a 29% speedup on our reference system.; 48) FedCDC: A Collaborative Framework for Data Consumers in Federated
  Learning Market; Federated learning (FL) allows machine learning models to be trained on
distributed datasets without directly accessing local data. In FL markets,
numerous Data Consumers compete to recruit Data Owners for their respective
training tasks, but budget constraints and competition can prevent them from
securing sufficient data. While existing solutions focus on optimizing
one-to-one matching between Data Owners and Data Consumers, we propose
\methodname{}, a novel framework that facilitates collaborative recruitment and
training for Data Consumers with similar tasks. Specifically, \methodname{}
detects shared subtasks among multiple Data Consumers and coordinates the joint
training of submodels specialized for these subtasks. Then, through ensemble
distillation, these submodels are integrated into each Data Consumer global
model. Experimental evaluations on three benchmark datasets demonstrate that
restricting Data Consumers access to Data Owners significantly degrades model
performance; however, by incorporating \methodname{}, this performance loss is
effectively mitigated, resulting in substantial accuracy gains for all
participating Data Consumers.; 49) An Energy-Adaptive Elastic Equivariant Transformer Framework for Protein
  Structure Representation; Structure-informed protein representation learning is essential for effective
protein function annotation and \textit{de novo} design. However, the presence
of inherent noise in both crystal and AlphaFold-predicted structures poses
significant challenges for existing methods in learning robust protein
representations. To address these issues, we propose a novel equivariant
Transformer-State Space Model(SSM) hybrid framework, termed $E^3$former,
designed for efficient protein representation. Our approach uses energy
function-based receptive fields to construct proximity graphs and incorporates
an equivariant high-tensor-elastic selective SSM within the transformer
architecture. These components enable the model to adapt to complex atom
interactions and extract geometric features with higher signal-to-noise ratios.
Empirical results demonstrate that our model outperforms existing methods in
structure-intensive tasks, such as inverse folding and binding site prediction,
particularly when using predicted structures, owing to its enhanced tolerance
to data deviation and noise. Our approach offers a novel perspective for
conducting biological function research and drug discovery using noisy protein
structure data.; 50) The Limiting Spectral Distribution of Various Matrix Ensembles Under the
  Anticommutator Operation; Inspired by the quantization of classical quantities and Rankin Selberg
convolution, we study the anticommutator operation $\{\cdot, \cdot\}$, where
$\{A,B\} = AB + BA$, applied to real symmetric random matrix ensembles
including Gaussian orthogonal ensemble (GOE), the palindromic Toeplitz ensemble
(PTE), the $k$-checkerboard ensemble, and the block $k$-circulant ensemble
($k$-BCE). Using combinatorial and topological techniques related to
non-crossing and free matching properties of GOE and PTE, we obtain closed-form
formulae for the moments of the limiting spectral distributions of $\{$GOE,
GOE$\}$, $\{$PTE, PTE$\}$, $\{$GOE, PTE$\}$ and establish the corresponding
limiting spectral distributions with generating functions and convolution. On
the other hand, $\{$GOE, $k$-checkerboard$\}$ and $\{$$k$-checkerboard,
$j$-checkerboard$\}$ exhibit entirely different spectral behavior than the
other anticommutator ensembles: while the spectrum of $\{$GOE,
$k$-checkerboard$\}$ consists of 1 bulk regime of size $\Theta(N)$ and 1 blip
regime of size $\Theta(N^{3/2})$, the spectrum of $\{$$k$-checkerboard,
$j$-checkerboard$\}$ consists of 1 bulk regime of size $\Theta(N)$, 2
intermediary blip regimes of size $\Theta(N^{3/2})$, and 1 largest blip regime
of size $\Theta(N^2)$. In both cases, with the appropriate weight function, we
are able to isolate the largest regime for other regime(s) and analyze its
moments and convergence results via combinatorics. We end with numerical
computation of lower even moments of $\{$GOE, $k$-BCE$\}$ and $\{$$k$-BCE,
$k$-BCE$\}$ based on genus expansion and discussion on the challenge with
analyzing the intermediary blip regimes of $\{$$k$-checkerboard,
$j$-checkerboard$\}$.; 51) Enhancing Train Transportation in Sri Lanka: A Smart IOT based
  Multi-Subsystem Approach using MQTT; This research proposes a system as a solution for the challenges faced by Sri
Lanka' s historic railway system, such as scheduling delays, overcrowding,
manual ticketing, and management inefficiencies. It proposes a multi-subsystem
approach, incorporating GPS tracking, RFID-based e-ticketing, seat reservation,
and vision-based people counting. The GPS based real time train tracking system
performs accurately within 24 meters, with the MQTT protocol showing twice the
speed of the HTTP-based system. All subsystems use the MQTT protocol to enhance
efficiency, reliability, and passenger experience. The study's data and
methodology demonstrate the effectiveness of these innovations in improving
scheduling, passenger flow, and overall system performance, offering promising
solutions for modernizing Sri Lanka's railway infrastructure.; 52) High-dimensional Sobolev tests on hyperspheres; We derive the limit null distribution of the class of Sobolev tests of
uniformity on the hypersphere when the dimension and the sample size diverge to
infinity at arbitrary rates. The limiting non-null behavior of these tests is
obtained for a sequence of integrated von Mises-Fisher local alternatives. The
asymptotic results are applied to test for high-dimensional rotational symmetry
and spherical symmetry. Numerical experiments illustrate the derived behavior
of the uniformity and spherically symmetry tests under the null and under local
and fixed alternatives.; 53) Advances in RNA secondary structure prediction and RNA modifications:
  Methods, data, and applications; Due to the hierarchical organization of RNA structures and their pivotal
roles in fulfilling RNA functions, the formation of RNA secondary structure
critically influences many biological processes and has thus been a crucial
research topic. This review sets out to explore the computational prediction of
RNA secondary structure and its connections to RNA modifications, which have
emerged as an active domain in recent years. We first examine the progression
of RNA secondary structure prediction methodology, focusing on a set of
representative works categorized into thermodynamic, comparative, machine
learning, and hybrid approaches. Next, we survey the advances in RNA
modifications and computational methods for identifying RNA modifications,
focusing on the prominent modification types. Subsequently, we highlight the
interplay between RNA modifications and secondary structures, emphasizing how
modifications such as m6A dynamically affect RNA folding and vice versa. In
addition, we also review relevant data sources and provide a discussion of
current challenges and opportunities in the field. Ultimately, we hope our
review will be able to serve as a cornerstone to aid in the development of
innovative methods for this emerging topic and foster therapeutic applications
in the future.; 54) Increasing the Robustness of the Fine-tuned Multilingual
  Machine-Generated Text Detectors; Since the proliferation of LLMs, there have been concerns about their misuse
for harmful content creation and spreading. Recent studies justify such fears,
providing evidence of LLM vulnerabilities and high potential of their misuse.
Humans are no longer able to distinguish between high-quality machine-generated
and authentic human-written texts. Therefore, it is crucial to develop
automated means to accurately detect machine-generated content. It would enable
to identify such content in online information space, thus providing an
additional information about its credibility. This work addresses the problem
by proposing a robust fine-tuning process of LLMs for the detection task,
making the detectors more robust against obfuscation and more generalizable to
out-of-distribution data.; 55) A Comprehensive Review of Protein Language Models; At the intersection of the rapidly growing biological data landscape and
advancements in Natural Language Processing (NLP), protein language models
(PLMs) have emerged as a transformative force in modern research. These models
have achieved remarkable progress, highlighting the need for timely and
comprehensive overviews. However, much of the existing literature focuses
narrowly on specific domains, often missing a broader analysis of PLMs. This
study provides a systematic review of PLMs from a macro perspective, covering
key historical milestones and current mainstream trends. We focus on the models
themselves and their evaluation metrics, exploring aspects such as model
architectures, positional encoding, scaling laws, and datasets. In the
evaluation section, we discuss benchmarks and downstream applications. To
further support ongoing research, we introduce relevant mainstream tools.
Lastly, we critically examine the key challenges and limitations in this
rapidly evolving field.; 56) Fibonacci-Net: A Lightweight CNN model for Automatic Brain Tumor
  Classification; This research proposes a very lightweight model ""Fibonacci-Net"" along with a
novel pooling technique, for automatic brain tumor classification from
imbalanced Magnetic Resonance Imaging (MRI) datasets. Automatic brain tumor
detection from MRI dataset has garnered significant attention in the research
community, since the inception of Convolutional Neural Network (CNN) models.
However, the performance of conventional CNN models is hindered due to class
imbalance problems. The novelties of this work are as follows: (I) A
lightweight CNN model is proposed in which the number of filters in different
convolutional layers is chosen according to the numbers of Fibonacci series.
(II) In the last two blocks of the proposed model, depth-wise separable
convolution (DWSC) layers are employed to considerably reduce the computational
complexity of the model. (III) Two parallel concatenations (or, skip
connections) are deployed from 2nd to 4th, and 3rd to 5th convolutional block
in the proposed Fibonacci-Net. This skip connection encompasses a novel
Average-2Max pooling layer that produces two stacks of convoluted output,
having a bit different statistics. Therefore, this parallel concatenation block
works as an efficient feature augmenter inside the model, thus, automatically
alleviating the class imbalance problem to a certain extent. For validity
purpose, we have implemented the proposed framework on three MRI datasets which
are highly class-imbalanced. (a) The first dataset has four classes, i.e.,
glioma tumor, meningioma tumor, pituitary tumor, and no-tumor. (b) Second and
third MRI datasets have 15 and 44 classes respectively. Experimental results
reveal that, after employing the proposed Fibonacci-Net we have achieved 96.2%
accuracy, 97.17% precision, 95.9% recall, 96.5% F1 score, and 99.9% specificity
on the most challenging ``44-classes MRI dataset''.; 57) Value of Information in Social Learning; This study extends Blackwell's (1953) comparison of information to a
sequential social learning model, where agents make decisions sequentially
based on both private signals and the observed actions of others. In this
context, we introduce a new binary relation over information structures: An
information structure is more socially valuable than another if it yields
higher expected payoffs for all agents, regardless of their preferences. First,
we establish that this binary relation is strictly stronger than the Blackwell
order. Then, we provide a necessary and sufficient condition for our binary
relation and propose a simpler sufficient condition that is easier to verify.; 58) Facies Classification with Copula Entropy; In this paper we propose to apply copula entropy (CE) to facies
classification. In our method, the correlations between geological variables
and facies classes are measured with CE and then the variables associated with
large negative CEs are selected for classification. We verified the proposed
method on a typical facies dataset for facies classification and the
experimental results show that the proposed method can select less geological
variables for facies classification without sacrificing classification
performance. The geological variables such selected are also interpretable to
geologists with geological meanings due to the rigorous definition of CE.; 59) LLM-Agents Driven Automated Simulation Testing and Analysis of small
  Uncrewed Aerial Systems; Thorough simulation testing is crucial for validating the correct behavior of
small Uncrewed Aerial Systems (sUAS) across multiple scenarios, including
adverse weather conditions (such as wind, and fog), diverse settings (hilly
terrain, or urban areas), and varying mission profiles (surveillance,
tracking). While various sUAS simulation tools exist to support developers, the
entire process of creating, executing, and analyzing simulation tests remains a
largely manual and cumbersome task. Developers must identify test scenarios,
set up the simulation environment, integrate the System under Test (SuT) with
simulation tools, formulate mission plans, and collect and analyze results.
These labor-intensive tasks limit the ability of developers to conduct
exhaustive testing across a wide range of scenarios. To alleviate this problem,
in this paper, we propose AutoSimTest, a Large Language Model (LLM)-driven
framework, where multiple LLM agents collaborate to support the sUAS simulation
testing process. This includes: (1) creating test scenarios that subject the
SuT to unique environmental contexts; (2) preparing the simulation environment
as per the test scenario; (3) generating diverse sUAS missions for the SuT to
execute; and (4) analyzing simulation results and providing an interactive
analytics interface. Further, the design of the framework is flexible for
creating and testing scenarios for a variety of sUAS use cases, simulation
tools, and SuT input requirements. We evaluated our approach by (a) conducting
simulation testing of PX4 and ArduPilot flight-controller-based SuTs, (b)
analyzing the performance of each agent, and (c) gathering feedback from sUAS
developers. Our findings indicate that AutoSimTest significantly improves the
efficiency and scope of the sUAS testing process, allowing for more
comprehensive and varied scenario evaluations while reducing the manual effort.; 60) Phase-matching of high harmonic generation in twisted solids; High harmonic generation (HHG) in solids could enable attosecond and
ultraviolet light sources with high compactness, great controllability and rich
functions. However, the HHG process is accompanied by a quite large wavevector
mismatch that is uncompensated by any traditional phase-matching method,
directly limiting its energy conversion efficiency. Here, we propose an
effective strategy for phase-matching of HHG with arbitrary harmonic orders in
solids. Two flakes of solids with an interlayer twist induce a nonlinear
optical phase that depends on the crystal symmetry, twist angle and harmonic
order, which can be accurately designed to compensate for the phase mismatch in
HHG. Guided by the twist-phase-matching theory, we achieved a record-high
conversion efficiency of $~1.5\times10^{-5}$ for the fifth HHG in twisted
hexagonal boron nitride crystals with a total thickness of only 1 ${\mu}m$. Our
work establishes a foundation for developing ultrashort-wavelength and
ultrafast-pulse laser sources in compact solid-state tabletop systems for
fundamental and applied sciences.; 61) ProtComposer: Compositional Protein Structure Generation with 3D
  Ellipsoids; We develop ProtComposer to generate protein structures conditioned on spatial
protein layouts that are specified via a set of 3D ellipsoids capturing
substructure shapes and semantics. At inference time, we condition on
ellipsoids that are hand-constructed, extracted from existing proteins, or from
a statistical model, with each option unlocking new capabilities.
Hand-specifying ellipsoids enables users to control the location, size,
orientation, secondary structure, and approximate shape of protein
substructures. Conditioning on ellipsoids of existing proteins enables
redesigning their substructure's connectivity or editing substructure
properties. By conditioning on novel and diverse ellipsoid layouts from a
simple statistical model, we improve protein generation with expanded Pareto
frontiers between designability, novelty, and diversity. Further, this enables
sampling designable proteins with a helix-fraction that matches PDB proteins,
unlike existing generative models that commonly oversample conceptually simple
helix bundles. Code is available at https://github.com/NVlabs/protcomposer.; 62) Inverse problems with experiment-guided AlphaFold; Proteins exist as a dynamic ensemble of multiple conformations, and these
motions are often crucial for their functions. However, current structure
prediction methods predominantly yield a single conformation, overlooking the
conformational heterogeneity revealed by diverse experimental modalities. Here,
we present a framework for building experiment-grounded protein structure
generative models that infer conformational ensembles consistent with measured
experimental data. The key idea is to treat state-of-the-art protein structure
predictors (e.g., AlphaFold3) as sequence-conditioned structural priors, and
cast ensemble modeling as posterior inference of protein structures given
experimental measurements. Through extensive real-data experiments, we
demonstrate the generality of our method to incorporate a variety of
experimental measurements. In particular, our framework uncovers previously
unmodeled conformational heterogeneity from crystallographic densities, and
generates high-accuracy NMR ensembles orders of magnitude faster than the
status quo. Notably, we demonstrate that our ensembles outperform AlphaFold3
and sometimes better fit experimental data than publicly deposited structures
to the Protein Data Bank (PDB). We believe that this approach will unlock
building predictive models that fully embrace experimentally observed
conformational diversity.; 63) Progress of the anti-obesity of Berberine; Obesity is defined as the excessive accumulation or abnormal distribution of
body fat. According to data from World Obesity Atlas 2024, the increase in
prevalence of obesity has become a major worldwide health problem in adults as
well as among children and adolescents. Although an increasing number of drugs
have been approved for the treatment of obesity in recent years, many of these
drugs have inevitable side effects which have increased the demand for new
safe, accessible and effective drugs for obesity and prompt interest in natural
products. Berberine (BBR) and its metabolites, known for their multiple
pharmacological effects. Recent studies have emphatically highlighted the
anti-obesity benefits of BBR and the underlying mechanisms have been gradually
elucidated. However, its clinical application is limited by poor oral
absorption and low bioavailability. Based on this, this review summarizes
current research on the anti-obesity effects of BBR and its metabolites,
including advancements in clinical trail results, understanding potential
molecular mechanisms and absorption and bioavailability. As a natural compound
derived from plants, BBR holds potential as an alternative approach for
managing obesity.; 64) Non-Markovain Quantum State Diffusion for the Tunneling in SARS-COVID-19
  virus; In the context of biology, unlike the comprehensively established Standard
Model in physics, many biological processes lack a complete theoretical
framework and are often described phenomenologically. A pertinent example is
olfaction -- the process through which humans and animals distinguish various
odors. The conventional biological explanation for olfaction relies on the lock
and key model, which, while useful, does not fully account for all observed
phenomena. As an alternative or complement to this model, vibration-assisted
electron tunneling has been proposed. Drawing inspiration from the
vibration-assisted electron tunneling model for olfaction, we have developed a
theoretical model for electron tunneling in SARS-CoV-2 virus infection within a
non-Markovian framework. We approach this by solving the non-Markovian quantum
stochastic Schrodinger equation. In our model, the spike protein and the GPCR
receptor are conceptualized as a dimer, utilizing the spin-Boson model to
facilitate the description of electron tunneling. Our analysis demonstrates
that electron tunneling in this context exhibits inherently non-Markovian
characteristics, extending into the intermediate and strong coupling regimes
between the dimer components. This behavior stands in stark contrast to
predictions from Markovian models, which fail to accurately describe electron
tunneling in the strong coupling limit. Notably, Markovian approximations often
lead to unphysical negative probabilities in this regime, underscoring their
limitations and highlighting the necessity of incorporating non-Markovian
dynamics for a more realistic description of biological quantum processes. This
approach not only broadens our understanding of viral infection mechanisms but
also enhances the biological accuracy and relevance of our theoretical
framework in describing complex biological interactions.; 65) COLOR: A compositional linear operation-based representation of protein
  sequences for identification of monomer contributions to properties; The properties of biological materials like proteins and nucleic acids are
largely determined by their primary sequence. While certain segments in the
sequence strongly influence specific functions, identifying these segments, or
so-called motifs, is challenging due to the complexity of sequential data.
While deep learning (DL) models can accurately capture sequence-property
relationships, the degree of nonlinearity in these models limits the assessment
of monomer contributions to a property - a critical step in identifying key
motifs. Recent advances in explainable AI (XAI) offer attention and
gradient-based methods for estimating monomeric contributions. However, these
methods are primarily applied to classification tasks, such as binding site
identification, where they achieve limited accuracy (40-45%) and rely on
qualitative evaluations. To address these limitations, we introduce a DL model
with interpretable steps, enabling direct tracing of monomeric contributions.
We also propose a metric ($\mathcal{I}$), inspired by the masking technique in
the field of image analysis and natural language processing, for quantitative
analysis on datasets mainly containing distinct properties of anti-cancer
peptides (ACP), antimicrobial peptides (AMP), and collagen. Our model exhibits
22% higher explainability, pinpoints critical motifs (RRR, RRI, and RSS) that
significantly destabilize ACPs, and identifies motifs in AMPs that are 50% more
effective in converting non-AMPs to AMPs. These findings highlight the
potential of our model in guiding mutation strategies for designing
protein-based biomaterials.; 66) Self-Refinement of Auxiliary-Field Quantum Monte Carlo via
  Non-Orthogonal Configuration Interaction; For optimal accuracy, auxiliary-field quantum Monte Carlo (AFQMC) requires
trial states consisting of multiple Slater determinants. We develop an
efficient algorithm to select the determinants from an AFQMC random walk
eliminating the need for other methods. When determinants contribute
significantly to the non-orthogonal configuration interaction energy, we
include them in the trial state. These refined trial wave functions
significantly reduce the phaseless bias and sampling variance of the local
energy estimator. With 100 to 200 determinants, we lower the error of AFQMC by
up to a factor of 10 for second row elements that are not accurately described
with a Hartree-Fock trial wave function. For the HEAT set, we improve the
average error to within the chemical accuracy. For benzene, the largest studied
system, we reduce AFQMC error by 80% with 214 Slater determinants and find a
10-fold increase of the time to solution. We show that the remaining error of
the method prevails in systems with static correlation or strong spin
contamination.; 67) Deep Learning Based Segmentation of Blood Vessels from H&E Stained
  Oesophageal Adenocarcinoma Whole-Slide Images; Blood vessels (BVs) play a critical role in the Tumor Micro-Environment
(TME), potentially influencing cancer progression and treatment response.
However, manually quantifying BVs in Hematoxylin and Eosin (H&E) stained images
is challenging and labor-intensive due to their heterogeneous appearances. We
propose a novel approach of constructing guiding maps to improve the
performance of state-of-the-art segmentation models for BV segmentation, the
guiding maps encourage the models to learn representative features of BVs. This
is particularly beneficial for computational pathology, where labeled training
data is often limited and large models are prone to overfitting. We have
quantitative and qualitative results to demonstrate the efficacy of our
approach in improving segmentation accuracy. In future, we plan to validate
this method to segment BVs across various tissue types and investigate the role
of cellular structures in relation to BVs in the TME.; 68) Fluorescence Phasor Analysis: Basic Principles and Biophysical
  Applications; Fluorescence is one of the most widely used techniques in biological
sciences. Its exceptional sensitivity and versatility make it a tool of first
choice for quantitative studies in biophysics. The concept of phasors,
originally introduced by Charles Steinmetz in the late 19th century for
analyzing alternating current circuits, has since found applications across
diverse disciplines, including fluorescence spectroscopy. The main idea behind
fluorescence phasors was posited by Gregorio Weber in 1981. By analyzing the
complementary nature of pulse and phase fluorometry data, he shows that two
magnitudes -- denoted as $G$ and $S$ -- derived from the frequency-domain
fluorescence measurements correspond to the real and imaginary part of the
Fourier transform of the fluorescence intensity in the time domain. This review
provides a historical perspective on how the concept of phasors originates and
how it integrates into fluorescence spectroscopy. We discuss their fundamental
algebraic properties, which enable intuitive model-free analysis of
fluorescence data despite the complexity of the underlying phenomena. Some
applications in biophysics illustrate the power of this approach in studying
diverse phenomena, including protein folding, protein interactions, phase
transitions in lipid mixtures and formation of high-order structures in nucleic
acids.; 69) A weakly compressible SPH method for RANS simulation of wall-bounded
  turbulent flows; This paper presents a Weakly Compressible Smoothed Particle Hydrodynamics
(WCSPH) method for solving the two-equation Reynolds-Averaged Navier-Stokes
(RANS) model. The turbulent wall-bounded flow with or without mild flow
separation, a crucial flow pattern in engineering applications, yet rarely
explored in the SPH community, is simulated. The inconsistency between the
Lagrangian characteristic and RANS model, mainly due to the intense particle
shear and near-wall discontinuity, is firstly revealed and addressed by the
mainstream and nearwall improvements, respectively. The mainstream
improvements, including Adaptive Riemann-eddy Dissipation (ARD) and Limited
Transport Velocity Formulation (LTVF), address dissipation incompatibility and
turbulent kinetic energy over-prediction issues. The nearwall improvements,
such as the particle-based wall model realization, weighted near-wall
compensation scheme, and constant $y_p$ strategy, improve the accuracy and
stability of the adopted wall model, where the wall dummy particles are still
used for future coupling of solid dynamics. Besides, to perform rigorous
convergence tests, an level-set-based boundary-offset technique is developed to
ensure consistent $y^+$ across different resolutions. The benchmark
wall-bounded turbulent cases, including straight, mildly- and strongly-curved,
and Half Converging and Diverging (HCD) channels are calculated. Good
convergence is, to our best knowledge, firstly achieved for both velocity and
turbulent kinetic energy for the SPH-RANS method. All the results agree well
with the data from the experiments or simulated by the Eulerian methods at
engineering-acceptable resolutions. The proposed method bridges particle-based
and mesh-based RANS models, providing adaptability for other turbulence models
and potential for turbulent fluid-structure interaction (FSI) simulations.; 70) The standard coil or globule phases cannot describe the denatured state
  of structured proteins and intrinsically disordered proteins; The concepts of globule and random coil were developed to describe the phases
of homopolymers and then used to characterize the denatured state of structured
cytosolic proteins and intrinsically disordered proteins. Using multi-scale
molecular dynamics simulations, we were able to explore the conformational
space of the disordered conformations of both types of protein under biological
conditions in an affordable amount of computational time. By studying the size
of the protein and the density correlations in space, we conclude that the
standard phases of homopolymers and the tools to detect them cannot be applied
straightforwardly to proteins.; 71) Nonsuppressible viremia during HIV-1 therapy meets molecular virology; HIV-1 replication can be suppressed with antiretroviral therapy (ART), but
individuals who stop taking ART soon become viremic again. Some people
experience extended times of detectable viremia despite optimal adherence to
ART. In the issue of the JCI, White, Wu, and coauthors elucidate a source of
nonsuppressible viremia (NSV) in treatment-adherent patients clonally expanded
T cells harboring HIV-1 proviruses with small deletions or mutations in the
5'-leader, the UTR that includes the major splice donor site of viral RNA.
These mutations altered viral RNA-splicing efficiency and RNA dimerization and
packaging, yet still allowed production of detectable levels of noninfectious
virus particles. These particles lacked the HIV-1 Env surface protein required
for cell entry and failed to form the mature capsid cone required for
infectivity. These studies improve our understanding of NSV and the regulation
of viral functions in the 5'-leader with implications for rationalized care in
individuals with NSV.; 72) ""Kya family planning after marriage hoti hai?"": Integrating Cultural
  Sensitivity in an LLM Chatbot for Reproductive Health; Access to sexual and reproductive health information remains a challenge in
many communities globally, due to cultural taboos and limited availability of
healthcare providers. Public health organizations are increasingly turning to
Large Language Models (LLMs) to improve access to timely and personalized
information. However, recent HCI scholarship indicates that significant
challenges remain in incorporating context awareness and mitigating bias in
LLMs. In this paper, we study the development of a culturally-appropriate
LLM-based chatbot for reproductive health with underserved women in urban
India. Through user interactions, focus groups, and interviews with multiple
stakeholders, we examine the chatbot's response to sensitive and highly
contextual queries on reproductive health. Our findings reveal strengths and
limitations of the system in capturing local context, and complexities around
what constitutes ""culture"". Finally, we discuss how local context might be
better integrated, and present a framework to inform the design of
culturally-sensitive chatbots for community health.; 73) MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse; We present MetaSpatial, the first reinforcement learning (RL)-based framework
designed to enhance 3D spatial reasoning in vision-language models (VLMs),
enabling real-time 3D scene generation without the need for hard-coded
optimizations. MetaSpatial addresses two core challenges: (i) the lack of
internalized 3D spatial reasoning in VLMs, which limits their ability to
generate realistic layouts, and (ii) the inefficiency of traditional supervised
fine-tuning (SFT) for layout generation tasks, as perfect ground truth
annotations are unavailable. Our key innovation is a multi-turn RL-based
optimization mechanism that integrates physics-aware constraints and rendered
image evaluations, ensuring generated 3D layouts are coherent, physically
plausible, and aesthetically consistent. Methodologically, MetaSpatial
introduces an adaptive, iterative reasoning process, where the VLM refines
spatial arrangements over multiple turns by analyzing rendered outputs,
improving scene coherence progressively. Empirical evaluations demonstrate that
MetaSpatial significantly enhances the spatial consistency and formatting
stability of various scale models. Post-training, object placements are more
realistic, aligned, and functionally coherent, validating the effectiveness of
RL for 3D spatial reasoning in metaverse, AR/VR, digital twins, and game
development applications. Our code, data, and training pipeline are publicly
available at https://github.com/PzySeere/MetaSpatial.; 74) AIN: The Arabic INclusive Large Multimodal Model; Amid the swift progress of large language models (LLMs) and their evolution
into large multimodal models (LMMs), significant strides have been made in
high-resource languages such as English and Chinese. While Arabic LLMs have
seen notable progress, Arabic LMMs remain largely unexplored, often narrowly
focusing on a few specific aspects of the language and visual understanding. To
bridge this gap, we introduce AIN-the Arabic Inclusive Multimodal
Model-designed to excel across diverse domains. AIN is an English-Arabic
bilingual LMM designed to excel in English and Arabic, leveraging carefully
constructed 3.6 million high-quality Arabic-English multimodal data samples.
AIN demonstrates state-of-the-art Arabic performance, while also possessing
strong English-language visual capabilities. On the recent CAMEL-Bench
benchmark comprising 38 sub-domains including, multi-image understanding,
complex visual perception, handwritten document understanding, video
understanding, medical imaging, plant diseases, and remote sensing-based land
use understanding, our AIN demonstrates strong performance with the 7B model
outperforming GPT-4o by an absolute gain of 3.4% averaged over eight domains
and 38 sub-domains. AIN's superior capabilities position it as a significant
step toward empowering Arabic speakers with advanced multimodal generative AI
tools across diverse applications.; 75) Robust Multilinear Principal Component Analysis; Multilinear Principal Component Analysis (MPCA) is an important tool for
analyzing tensor data. It performs dimension reduction similar to PCA for
multivariate data. However, standard MPCA is sensitive to outliers. It is
highly influenced by observations deviating from the bulk of the data, called
casewise outliers, as well as by individual outlying cells in the tensors,
so-called cellwise outliers. This latter type of outlier is highly likely to
occur in tensor data, as tensors typically consist of many cells. This paper
introduces a novel robust MPCA method that can handle both types of outliers
simultaneously, and can cope with missing values as well. This method uses a
single loss function to reduce the influence of both casewise and cellwise
outliers. The solution that minimizes this loss function is computed using an
iteratively reweighted least squares algorithm with a robust initialization.
Graphical diagnostic tools are also proposed to identify the different types of
outliers that have been found by the new robust MPCA method. The performance of
the method and associated graphical displays is assessed through simulations
and illustrated on two real datasets.; 76) Engineered Zwitterion-Infused Clay Composites with Antibacterial and
  Antifungal Efficacy; Microbes and pathogens play a detrimental role in healing wounds, causing
infections like impetigo through bodily fluids and skin and entering the
bloodstream through the wounds, thereby hindering the healing process and
tissue regeneration. Clay, known for its long history of natural therapeutic
use, has emerged as one of the most promising candidates for biomedical
applications due to its non-toxic nature, porosity, high surface area,
ubiquity, and excellent cation exchange capacity. This study demonstrates an
innovative approach to engineering an organo-functionalized,
infection-resistant, easy-to-use bandage material from clay, an environmentally
benign and sustainable material. The hybrid membranes have been developed using
clays, zwitterions, silver ions, and terbinafine hydrochloride (TBH) to impart
antibacterial and antifungal efficacy. A critical aspect of this study is
embedding organic molecules and metal ions with the clays and releasing them to
resist the growth and kill the pathogens. The antimicrobial efficacy of the
membranes has been tested using a zone of inhibition study against the most
common microbes in skin wounds, viz. S. aureus, E. coli, and C. albicans.
Results from our studies not only demonstrate the potential of these hybrid
clay membranes as a cost-effective, scalable, and effective solution for
treating microbial infections but also instill newer avenues for point-of-care
wound-healing treatments, offering hope for improved patient outcomes.; 77) Bolt3D: Generating 3D Scenes in Seconds; We present a latent diffusion model for fast feed-forward 3D scene
generation. Given one or more images, our model Bolt3D directly samples a 3D
scene representation in less than seven seconds on a single GPU. We achieve
this by leveraging powerful and scalable existing 2D diffusion network
architectures to produce consistent high-fidelity 3D scene representations. To
train this model, we create a large-scale multiview-consistent dataset of 3D
geometry and appearance by applying state-of-the-art dense 3D reconstruction
techniques to existing multiview image datasets. Compared to prior multiview
generative models that require per-scene optimization for 3D reconstruction,
Bolt3D reduces the inference cost by a factor of up to 300 times.; 78) DeepSilencer: A Novel Deep Learning Model for Predicting siRNA Knockdown
  Efficiency; Background: Small interfering RNA (siRNA) is a promising therapeutic agent
due to its ability to silence disease-related genes via RNA interference. While
traditional machine learning and early deep learning methods have made progress
in predicting siRNA efficacy, there remains significant room for improvement.
Advanced deep learning techniques can enhance prediction accuracy, reducing the
reliance on extensive wet-lab experiments and accelerating the identification
of effective siRNA sequences. This approach also provides deeper insights into
the mechanisms of siRNA efficacy, facilitating more targeted and efficient
therapeutic strategies.
  Methods: We introduce DeepSilencer, an innovative deep learning model
designed to predict siRNA knockdown efficiency. DeepSilencer utilizes advanced
neural network architectures to capture the complex features of siRNA
sequences. Our key contributions include a specially designed deep learning
model, an innovative online data sampling method, and an improved loss function
tailored for siRNA prediction. These enhancements collectively boost the
model's prediction accuracy and robustness.
  Results: Extensive evaluations on multiple test sets demonstrate that
DeepSilencer achieves state-of-the-art performance using only siRNA sequences
and basic physicochemical properties. Our model surpasses several other methods
and shows superior predictive performance, particularly when incorporating
thermodynamic parameters.
  Conclusion: The advancements in data sampling, model design, and loss
function significantly enhance the predictive capabilities of DeepSilencer.
These improvements underscore its potential to advance RNAi therapeutic design
and development, offering a powerful tool for researchers and clinicians.; 79) Multi-Granular Multimodal Clue Fusion for Meme Understanding; With the continuous emergence of various social media platforms frequently
used in daily life, the multimodal meme understanding (MMU) task has been
garnering increasing attention. MMU aims to explore and comprehend the meanings
of memes from various perspectives by performing tasks such as metaphor
recognition, sentiment analysis, intention detection, and offensiveness
detection. Despite making progress, limitations persist due to the loss of
fine-grained metaphorical visual clue and the neglect of multimodal text-image
weak correlation. To overcome these limitations, we propose a multi-granular
multimodal clue fusion model (MGMCF) to advance MMU. Firstly, we design an
object-level semantic mining module to extract object-level image feature
clues, achieving fine-grained feature clue extraction and enhancing the model's
ability to capture metaphorical details and semantics. Secondly, we propose a
brand-new global-local cross-modal interaction model to address the weak
correlation between text and images. This model facilitates effective
interaction between global multimodal contextual clues and local unimodal
feature clues, strengthening their representations through a bidirectional
cross-modal attention mechanism. Finally, we devise a dual-semantic guided
training strategy to enhance the model's understanding and alignment of
multimodal representations in the semantic space. Experiments conducted on the
widely-used MET-MEME bilingual dataset demonstrate significant improvements
over state-of-the-art baselines. Specifically, there is an 8.14% increase in
precision for offensiveness detection task, and respective accuracy
enhancements of 3.53%, 3.89%, and 3.52% for metaphor recognition, sentiment
analysis, and intention detection tasks. These results, underpinned by in-depth
analyses, underscore the effectiveness and potential of our approach for
advancing MMU.; 80) Simultaneous optical power delivery and distributed sensing through
  cross-band wavelength multiplexing over fiber link; Optical fibers offer significant advantages in both power delivery and
distributed sensing. In remote areas where stable power supply is not easy to
access, the distributed optical fiber sensing (DOFS) which offers long distance
monitoring capability and the power-over-fiber (PoF) which can provide energy
for connected electronics or other sensors are highly desired simultaneously.
In this letter, the PoF-DOFS hybrid system is proposed and experimentally
verified for the first time. By multiplexing the power channel and sensing
channel with large wavelength separation, the cross-talk is greatly reduced.
The results show that the Brillouin frequency shift under different temperature
in the Brillouin optical time domain reflectometry remains unaffected by the
high-power transmission background and the power delivery efficiency up to ~66%
can be achieved over 1.3 km fiber link. This work paves the way for further
research on PoF-DOFS hybrid system and gives a valuable solution for creating
multi-parameter, multi-scale sensing network without the need for local power
source.; 81) Enhancing Facial Privacy Protection via Weakening Diffusion Purification; The rapid growth of social media has led to the widespread sharing of
individual portrait images, which pose serious privacy risks due to the
capabilities of automatic face recognition (AFR) systems for mass surveillance.
Hence, protecting facial privacy against unauthorized AFR systems is essential.
Inspired by the generation capability of the emerging diffusion models, recent
methods employ diffusion models to generate adversarial face images for privacy
protection. However, they suffer from the diffusion purification effect,
leading to a low protection success rate (PSR). In this paper, we first propose
learning unconditional embeddings to increase the learning capacity for
adversarial modifications and then use them to guide the modification of the
adversarial latent code to weaken the diffusion purification effect. Moreover,
we integrate an identity-preserving structure to maintain structural
consistency between the original and generated images, allowing human observers
to recognize the generated image as having the same identity as the original.
Extensive experiments conducted on two public datasets, i.e., CelebA-HQ and
LADN, demonstrate the superiority of our approach. The protected faces
generated by our method outperform those produced by existing facial privacy
protection approaches in terms of transferability and natural appearance.; 82) Flavour Non-Universality and Higgs Compositeness; We present a flavour non-universal extension of the Standard Model combined
with the idea of Higgs compositeness. At the TeV scale, the gauge groups
$SU(2)_R$ and $U(1)_{B-L}$ are assumed to act in a non-universal manner on
light- and third-generation fermions, while the Higgs emerges as a pseudo
Nambu-Goldstone boson of the spontaneous global symmetry breaking $Sp(4)\to
SU(2)_L\times SU(2)_R^{[3]}$, attributed to new strong dynamics. We discuss how
the radiatively generated Higgs potential has the ingredients to realize the
unavoidable tuning necessary to separate electroweak and composite scales. In
particular, the flavoured gauge bosons resulting from the deconstruction must
lie in the vicinity of the TeV scale, thereby providing interesting
phenomenology that can be probed at near future colliders.; 83) Pushing the boundaries of Structure-Based Drug Design through
  Collaboration with Large Language Models; Structure-Based Drug Design (SBDD) has revolutionized drug discovery by
enabling the rational design of molecules for specific protein targets. Despite
significant advancements in improving docking scores, advanced 3D-SBDD
generative models still face challenges in producing drug-like candidates that
meet medicinal chemistry standards and pharmacokinetic requirements. These
limitations arise from their inherent focus on molecular interactions, often
neglecting critical aspects of drug-likeness. To address these shortcomings, we
introduce the Collaborative Intelligence Drug Design (CIDD) framework, which
combines the structural precision of 3D-SBDD models with the chemical reasoning
capabilities of large language models (LLMs). CIDD begins by generating
supporting molecules with 3D-SBDD models and then refines these molecules
through LLM-supported modules to enhance drug-likeness and structural
reasonability. When evaluated on the CrossDocked2020 dataset, CIDD achieved a
remarkable success ratio of 37.94%, significantly outperforming the previous
state-of-the-art benchmark of 15.72%. Although improving molecular interactions
and drug-likeness is often seen as a trade-off, CIDD uniquely achieves a
balanced improvement in both by leveraging the complementary strengths of
different models, offering a robust and innovative pathway for designing
therapeutically promising drug candidates.; 84) MANDARIN: Mixture-of-Experts Framework for Dynamic Delirium and Coma
  Prediction in ICU Patients: Development and Validation of an Acute Brain
  Dysfunction Prediction Model; Acute brain dysfunction (ABD) is a common, severe ICU complication,
presenting as delirium or coma and leading to prolonged stays, increased
mortality, and cognitive decline. Traditional screening tools like the Glasgow
Coma Scale (GCS), Confusion Assessment Method (CAM), and Richmond
Agitation-Sedation Scale (RASS) rely on intermittent assessments, causing
delays and inconsistencies. In this study, we propose MANDARIN
(Mixture-of-Experts Framework for Dynamic Delirium and Coma Prediction in ICU
Patients), a 1.5M-parameter mixture-of-experts neural network to predict ABD in
real-time among ICU patients. The model integrates temporal and static data
from the ICU to predict the brain status in the next 12 to 72 hours, using a
multi-branch approach to account for current brain status. The MANDARIN model
was trained on data from 92,734 patients (132,997 ICU admissions) from 2
hospitals between 2008-2019 and validated externally on data from 11,719
patients (14,519 ICU admissions) from 15 hospitals and prospectively on data
from 304 patients (503 ICU admissions) from one hospital in 2021-2024. Three
datasets were used: the University of Florida Health (UFH) dataset, the
electronic ICU Collaborative Research Database (eICU), and the Medical
Information Mart for Intensive Care (MIMIC)-IV dataset. MANDARIN significantly
outperforms the baseline neurological assessment scores (GCS, CAM, and RASS)
for delirium prediction in both external (AUROC 75.5% CI: 74.2%-76.8% vs 68.3%
CI: 66.9%-69.5%) and prospective (AUROC 82.0% CI: 74.8%-89.2% vs 72.7% CI:
65.5%-81.0%) cohorts, as well as for coma prediction (external AUROC 87.3% CI:
85.9%-89.0% vs 72.8% CI: 70.6%-74.9%, and prospective AUROC 93.4% CI:
88.5%-97.9% vs 67.7% CI: 57.7%-76.8%) with a 12-hour lead time. This tool has
the potential to assist clinicians in decision-making by continuously
monitoring the brain status of patients in the ICU.; 85) Language modulates vision: Evidence from neural networks and human
  brain-lesion models; Comparing information structures in between deep neural networks (DNNs) and
the human brain has become a key method for exploring their similarities and
differences. Recent research has shown better alignment of vision-language DNN
models, such as CLIP, with the activity of the human ventral occipitotemporal
cortex (VOTC) than earlier vision models, supporting the idea that language
modulates human visual perception. However, interpreting the results from such
comparisons is inherently limited due to the ""black box"" nature of DNNs. To
address this, we combined model-brain fitness analyses with human brain lesion
data to examine how disrupting the communication pathway between the visual and
language systems causally affects the ability of vision-language DNNs to
explain the activity of the VOTC. Across four diverse datasets, CLIP
consistently outperformed both label-supervised (ResNet) and unsupervised
(MoCo) models in predicting VOTC activity. This advantage was left-lateralized,
aligning with the human language network. Analyses of the data of 33 stroke
patients revealed that reduced white matter integrity between the VOTC and the
language region in the left angular gyrus was correlated with decreased CLIP
performance and increased MoCo performance, indicating a dynamic influence of
language processing on the activity of the VOTC. These findings support the
integration of language modulation in neurocognitive models of human vision,
reinforcing concepts from vision-language DNN models. The sensitivity of
model-brain similarity to specific brain lesions demonstrates that leveraging
manipulation of the human brain is a promising framework for evaluating and
developing brain-like computer models.; 86) Enhancing Automated Interpretability with Output-Centric Feature
  Descriptions; Automated interpretability pipelines generate natural language descriptions
for the concepts represented by features in large language models (LLMs), such
as plants or the first word in a sentence. These descriptions are derived using
inputs that activate the feature, which may be a dimension or a direction in
the model's representation space. However, identifying activating inputs is
costly, and the mechanistic role of a feature in model behavior is determined
both by how inputs cause a feature to activate and by how feature activation
affects outputs. Using steering evaluations, we reveal that current pipelines
provide descriptions that fail to capture the causal effect of the feature on
outputs. To fix this, we propose efficient, output-centric methods for
automatically generating feature descriptions. These methods use the tokens
weighted higher after feature stimulation or the highest weight tokens after
applying the vocabulary ""unembedding"" head directly to the feature. Our
output-centric descriptions better capture the causal effect of a feature on
model outputs than input-centric descriptions, but combining the two leads to
the best performance on both input and output evaluations. Lastly, we show that
output-centric descriptions can be used to find inputs that activate features
previously thought to be ""dead"".; 87) NodeNAS: Node-Specific Graph Neural Architecture Search for
  Out-of-Distribution Generalization; Graph neural architecture search (GraphNAS) has demonstrated advantages in
mitigating performance degradation of graph neural networks (GNNs) due to
distribution shifts. Recent approaches introduce weight sharing across tailored
architectures, generating unique GNN architectures for each graph end-to-end.
However, existing GraphNAS methods do not account for distribution patterns
across different graphs and heavily rely on extensive training data. With
sparse or single training graphs, these methods struggle to discover optimal
mappings between graphs and architectures, failing to generalize to
out-of-distribution (OOD) data. In this paper, we propose node-specific graph
neural architecture search(NodeNAS), which aims to tailor distinct aggregation
methods for different nodes through disentangling node topology and graph
distribution with limited datasets. We further propose adaptive aggregation
attention based Multi-dim NodeNAS method(MNNAS), which learns an node-specific
architecture customizer with good generalizability. Specifically, we extend the
vertical depth of the search space, supporting simultaneous node-specific
architecture customization across multiple dimensions. Moreover, we model the
power-law distribution of node degrees under varying assortativity, encoding
structure invariant information to guide architecture customization across each
dimension. Extensive experiments across supervised and unsupervised tasks
demonstrate that MNNAS surpasses state-of-the-art algorithms and achieves
excellent OOD generalization.; 88) From Features to Transformers: Redefining Ranking for Scalable Impact; We present LiGR, a large-scale ranking framework developed at LinkedIn that
brings state-of-the-art transformer-based modeling architectures into
production. We introduce a modified transformer architecture that incorporates
learned normalization and simultaneous set-wise attention to user history and
ranked items. This architecture enables several breakthrough achievements,
including: (1) the deprecation of most manually designed feature engineering,
outperforming the prior state-of-the-art system using only few features
(compared to hundreds in the baseline), (2) validation of the scaling law for
ranking systems, showing improved performance with larger models, more training
data, and longer context sequences, and (3) simultaneous joint scoring of items
in a set-wise manner, leading to automated improvements in diversity. To enable
efficient serving of large ranking models, we describe techniques to scale
inference effectively using single-pass processing of user history and set-wise
attention. We also summarize key insights from various ablation studies and A/B
tests, highlighting the most impactful technical approaches.; 89) TS-SatMVSNet: Slope Aware Height Estimation for Large-Scale Earth
  Terrain Multi-view Stereo; 3D terrain reconstruction with remote sensing imagery achieves cost-effective
and large-scale earth observation and is crucial for safeguarding natural
disasters, monitoring ecological changes, and preserving the
environment.Recently, learning-based multi-view stereo~(MVS) methods have shown
promise in this task. However, these methods simply modify the general
learning-based MVS framework for height estimation, which overlooks the terrain
characteristics and results in insufficient accuracy. Considering that the
Earth's surface generally undulates with no drastic changes and can be measured
by slope, integrating slope considerations into MVS frameworks could enhance
the accuracy of terrain reconstructions. To this end, we propose an end-to-end
slope-aware height estimation network named TS-SatMVSNet for large-scale remote
sensing terrain reconstruction.To effectively obtain the slope representation,
drawing from mathematical gradient concepts, we innovatively proposed a
height-based slope calculation strategy to first calculate a slope map from a
height map to measure the terrain undulation. To fully integrate slope
information into the MVS pipeline, we separately design two slope-guided
modules to enhance reconstruction outcomes at both micro and macro levels.
Specifically, at the micro level, we designed a slope-guided interval partition
module for refined height estimation using slope values. At the macro level, a
height correction module is proposed, using a learnable Gaussian smoothing
operator to amend the inaccurate height values. Additionally, to enhance the
efficacy of height estimation, we proposed a slope direction loss for
implicitly optimizing height estimation results. Extensive experiments on the
WHU-TLC dataset and MVS3D dataset show that our proposed method achieves
state-of-the-art performance and demonstrates competitive generalization
ability.; 90) Broadcast Channel Cooperative Gain: An Operational Interpretation of
  Partial Information Decomposition; Partial information decomposition has recently found applications in
biological signal processing and machine learning. Despite its impacts, the
decomposition was introduced through an informal and heuristic route, and its
exact operational meaning is unclear. In this work, we fill this gap by
connecting partial information decomposition to the capacity of the broadcast
channel, which has been well-studied in the information theory literature. We
show that the synergistic information in the decomposition can be rigorously
interpreted as the cooperative gain, or a lower bound of this gain, on the
corresponding broadcast channel. This interpretation can help practitioners to
better explain and expand the applications of the partial information
decomposition technique.; 91) On a character correspondence associated to $\mathfrak{F}$-projectors; We study the conditions under which the head characters of a solvable group,
as defined by I. M. Isaacs, behave well with respect to restriction. We also
determine the intersection of the kernels of all head characters of the group.
Using G. Navarro's definition of $\mathfrak{F}'$-characters, we generalize
these results for any saturated formation $\mathfrak{F}$ containing the
nilpotent groups.; 92) DP color functions of hypergraphs; In this article, we introduce the DP color function of a hypergraph, based on
the DP coloring introduced by Bernshteyn and Kostochka, which is the minimum
value where the minimum is taken over all its k-fold covers. It is an extension
of its chromatic polynomial. we obtain an upper bound for the DP color
functions of hypergraphs when hypergraphs are connected r-uniform hypergraphs
for any r greater than one. The upper bound is attained if and only if the
hypergraph is a r-uniform hypertree. We also show the cases of the DP color
function equal to its chromatic polynomial. These conclusions coincide with the
known results of graphs.; 93) Efficient Bayesian Computation Using Plug-and-Play Priors for Poisson
  Inverse Problems; This paper introduces a novel plug-and-play (PnP) Langevin sampling
methodology for Bayesian inference in low-photon Poisson imaging problems, a
challenging class of problems with significant applications in astronomy,
medicine, and biology. PnP Langevin sampling algorithms offer a powerful
framework for Bayesian image restoration, enabling accurate point estimation as
well as advanced inference tasks, including uncertainty quantification and
visualization analyses, and empirical Bayesian inference for automatic model
parameter tuning. However, existing PnP Langevin algorithms are not well-suited
for low-photon Poisson imaging due to high solution uncertainty and poor
regularity properties, such as exploding gradients and non-negativity
constraints. To address these challenges, we propose two strategies for
extending Langevin PnP sampling to Poisson imaging models: (i) an accelerated
PnP Langevin method that incorporates boundary reflections and a Poisson
likelihood approximation and (ii) a mirror sampling algorithm that leverages a
Riemannian geometry to handle the constraints and the poor regularity of the
likelihood without approximations. The effectiveness of these approaches is
demonstrated through extensive numerical experiments and comparisons with
state-of-the-art methods.; 94) CerraData-4MM: A multimodal benchmark dataset on Cerrado for land use
  and land cover classification; The Cerrado faces increasing environmental pressures, necessitating accurate
land use and land cover (LULC) mapping despite challenges such as class
imbalance and visually similar categories. To address this, we present
CerraData-4MM, a multimodal dataset combining Sentinel-1 Synthetic Aperture
Radar (SAR) and Sentinel-2 MultiSpectral Imagery (MSI) with 10m spatial
resolution. The dataset includes two hierarchical classification levels with 7
and 14 classes, respectively, focusing on the diverse Bico do Papagaio
ecoregion. We highlight CerraData-4MM's capacity to benchmark advanced semantic
segmentation techniques by evaluating a standard U-Net and a more sophisticated
Vision Transformer (ViT) model. The ViT achieves superior performance in
multimodal scenarios, with the highest macro F1-score of 57.60% and a mean
Intersection over Union (mIoU) of 49.05% at the first hierarchical level. Both
models struggle with minority classes, particularly at the second hierarchical
level, where U-Net's performance drops to an F1-score of 18.16%. Class
balancing improves representation for underrepresented classes but reduces
overall accuracy, underscoring the trade-off in weighted training.
CerraData-4MM offers a challenging benchmark for advancing deep learning models
to handle class imbalance and multimodal data fusion. Code, trained models, and
data are publicly available at https://github.com/ai4luc/CerraData-4MM.; 95) MagnetDB: A Longitudinal Torrent Discovery Dataset with IMDb-Matched
  Movies and TV Shows; BitTorrent remains a prominent channel for illicit distribution of
copyrighted material, yet the supply side of such content remains understudied.
We introduce MagnetDB, a longitudinal dataset of torrents discovered through
the BitTorrent DHT between 2018 and 2024, containing more than 28.6 million
torrents and metadata of more than 950 million files. While our primary focus
is on enabling research based on the supply of pirated movies and TV shows, the
dataset also encompasses other legitimate and illegitimate torrents. By
applying IMDb-matching and annotation to movie and TV show torrents, MagnetDB
facilitates detailed analyses of pirated content evolution in the BitTorrent
network. Researchers can leverage MagnetDB to examine distribution trends,
subcultural practices, and the gift economy within piracy ecosystems. Through
its scale and temporal scope, MagnetDB presents a unique opportunity for
investigating the broader dynamics of BitTorrent and advancing empirical
knowledge on digital piracy.; 96) PICBench: Benchmarking LLMs for Photonic Integrated Circuits Design; While large language models (LLMs) have shown remarkable potential in
automating various tasks in digital chip design, the field of Photonic
Integrated Circuits (PICs)-a promising solution to advanced chip
designs-remains relatively unexplored in this context. The design of PICs is
time-consuming and prone to errors due to the extensive and repetitive nature
of code involved in photonic chip design. In this paper, we introduce PICBench,
the first benchmarking and evaluation framework specifically designed to
automate PIC design generation using LLMs, where the generated output takes the
form of a netlist. Our benchmark consists of dozens of meticulously crafted PIC
design problems, spanning from fundamental device designs to more complex
circuit-level designs. It automatically evaluates both the syntax and
functionality of generated PIC designs by comparing simulation outputs with
expert-written solutions, leveraging an open-source simulator. We evaluate a
range of existing LLMs, while also conducting comparative tests on various
prompt engineering techniques to enhance LLM performance in automated PIC
design. The results reveal the challenges and potential of LLMs in the PIC
design domain, offering insights into the key areas that require further
research and development to optimize automation in this field. Our benchmark
and evaluation code is available at https://github.com/PICDA/PICBench.; 97) Social hierarchy shapes foraging decisions; Social foraging is a widespread form of animal foraging in which groups of
individuals coordinate their decisions to exploit resources in the environment.
Animals show a variety of social structures from egalitarian to hierarchical.
In this study, we examine how different forms of social hierarchy shape
foraging decisions. We developed a mechanistic analytically tractable model to
study the underlying processes of social foraging, tying the microscopic
individual to the macroscopic group levels. Based on a stochastic evidence
accumulation framework, we developed a model of patch-leaving decisions in a
large hierarchical group with leading and following individuals. Across a
variety of information sharing mechanisms, we were able to analytically
quantify emergent collective dynamics. We found that follower-leader dynamics
through observations of leader movements or through counting the number of
individuals in a patch confers, for most conditions, a benefit for the
following individuals by increasing their accuracy in inferring patch richness.
On the other hand, misinformation, through the communication of false beliefs
about food rewards or patch quality, shows to be detrimental to following
individuals, but paradoxically may lead to increased group cohesion. In an era
where there is a huge amount of animal foraging data collected, our model
provides a systematic way to conceptualize and understand those data by
uncovering hidden mechanisms underlying social foraging decisions.; 98) Silicon is the next frontier in plant synthetic biology; Silicon has striking similarity with carbon and is found in plant cells.
However, there is no specific role that has been assigned to silicon in the
life cycle of plants. The amount of silicon in plant cells is species specific
and can reach levels comparable to macronutrients. Silicon is the central
element for artificial intelligence, nanotechnology and digital revolution thus
can act as an informational molecule like nucleic acids while the diverse
bonding potential of silicon with different chemical species is analogous to
carbon and thus can serve as a structural candidate such as proteins. The
discovery of large amounts of silicon on Mars and the moon along with the
recent developments of enzyme that can incorporate silicon into organic
molecules has propelled the theory of creating silicon-based life. More
recently, bacterial cytochrome has been modified through directed evolution
such that it could cleave silicon-carbon bonds in organo-silicon compounds thus
consolidating on the idea of utilizing silicon in biomolecules. In this article
the potential of silicon-based life forms has been hypothesized along with the
reasoning that autotrophic virus-like particles can be a lucrative candidate to
investigate such potential. Such investigations in the field of synthetic
biology and astrobiology will have corollary benefit on Earth in the areas of
medicine, sustainable agriculture and environmental sustainability.
Bibliometric analysis indicates an increasing interest in synthetic biology.
Germany leads in research related to plant synthetic biology, while
Biotechnology and Biological Sciences Research Council (BBSRC) at UK has
highest financial commitments and Chinese Academy of Sciences generates the
highest number of publications in the field.; 99) TransientVerse: A Comprehensive Real-Time Alert and Multi-Wavelength
  Analysis System for Transient Astronomical Events; Transient astrophysical events are characterized by short timescales, high
energy, and multi-wavelength radiation, often accompanied by violent energy
releases. These phenomena are a major focus of modern astronomical research. To
reveal their underlying physical mechanisms, near-real-time, multi-wavelength,
and multi-messenger follow-up observations are essential. However, current
transient alert systems face multiple challenges, including fragmented
messages, inconsistent formats, and difficulties in retrospective analysis, all
of which hinder the efficiency of triggering observations. This paper presents
\textbf{TransientVerse}, an innovative real-time database platform to integrate
and disseminate transient alerts. The platform uses an automated pipeline to
integrate real-time alerts from multiple sources (e.g., ATel, VOEvent, and
GCN). It structures unstructured text data into a dual-format database for
transient alerts by using open-source large language models. TransientVerse
offers retrospective searches, data visualization, literature reviews, and
customized subscriptions for efficient event tracking and analysis.
Additionally, for Fast Radio Bursts (FRBs), the platform provides real-time
statistics on repeat burst rates across different time intervals and alerts
astronomers about high-frequency burst sources, enabling rapid follow-up
observations and optimizing the use of limited observation windows.
TransientVerse improves the efficiency of acquiring transient events in real
time, lowers the technical barriers for simultaneous observations, and provides
robust technical support for multi-wavelength, multi-messenger time-domain
astronomy and astrophysics studies.; 100) From Sub-Ability Diagnosis to Human-Aligned Generation: Bridging the Gap
  for Text Length Control via MARKERGEN; Despite the rapid progress of large language models (LLMs), their
length-controllable text generation (LCTG) ability remains below expectations,
posing a major limitation for practical applications. Existing methods mainly
focus on end-to-end training to reinforce adherence to length constraints.
However, the lack of decomposition and targeted enhancement of LCTG
sub-abilities restricts further progress. To bridge this gap, we conduct a
bottom-up decomposition of LCTG sub-abilities with human patterns as reference
and perform a detailed error analysis. On this basis, we propose MarkerGen, a
simple-yet-effective plug-and-play approach that:(1) mitigates LLM fundamental
deficiencies via external tool integration;(2) conducts explicit length
modeling with dynamically inserted markers;(3) employs a three-stage generation
scheme to better align length constraints while maintaining content quality.
Comprehensive experiments demonstrate that MarkerGen significantly improves
LCTG across various settings, exhibiting outstanding effectiveness and
generalizability.",1.0,1.0
2411.02815,applied,2411.02815-pos1-7,"Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation; In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate.",2411.02815-pos2-7,"Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network; An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images.; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",40,"['40', '4', '28', '7', '90', '10', '8', '38', '91', '78']","The selection of papers is based on their potential to complement the main paper's focus on liver anatomy and segmentation. The top paper references advanced computational techniques that could enhance the accuracy and efficiency of liver segmentation from imaging data, directly aligning with the themes of the main paper. Subsequent papers address related fields such as dynamic modeling, advanced data analysis, and machine learning, which could further support and expand the multidisciplinary approach needed for effective liver segmentation. Each selected paper improves upon previous methodologies, suggesting innovative solutions that could be applied to the analysis of liver structures.","1) Higher-Order Quantum Operations; An operational description of quantum phenomena concerns developing models
that describe experimentally observed behaviour. $\textit{Higher-order quantum
operations}\unicode{x2014}$quantum operations that transform quantum
operations$\unicode{x2014}$are fundamental to modern quantum theory, extending
beyond basic state preparations, evolutions, and measurements described by the
Born rule. These operations naturally emerge in quantum circuit architectures,
correlated open dynamics, and investigations of quantum causality, to name but
a few fields of application. This Review Article provides both a pedagogical
introduction to the framework of higher-order quantum operations and a
comprehensive survey of current literature, illustrated through physical
examples. We conclude by identifying open problems and future research
directions in this rapidly evolving field.; 2) LLMs Can Easily Learn to Reason from Demonstrations Structure, not
  content, is what matters!; Large reasoning models (LRMs) tackle complex reasoning problems by following
long chain-of-thoughts (Long CoT) that incorporate reflection, backtracking,
and self-validation. However, the training techniques and data requirements to
elicit Long CoT remain poorly understood. In this work, we find that a Large
Language model (LLM) can effectively learn Long CoT reasoning through
data-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank
adaptation (LoRA). With just 17k long CoT training samples, the
Qwen2.5-32B-Instruct model achieves significant improvements on a wide range of
math and coding benchmarks, including 56.7% (+40.0%) on AIME 2024 and 57.0%
(+8.1%) on LiveCodeBench, competitive to the proprietary o1-preview model's
score of 44.6% and 59.1%. More importantly, we find that the structure of Long
CoT is critical to the learning process, whereas the content of individual
reasoning steps has minimal impact. Perturbations affecting content, such as
training on incorrect samples or removing reasoning keywords, have little
impact on performance. In contrast, structural modifications that disrupt
logical consistency in the Long CoT, such as shuffling or deleting reasoning
steps, significantly degrade accuracy. For example, a model trained on Long CoT
samples with incorrect answers still achieves only 3.2% lower accuracy compared
to training with fully correct samples. These insights deepen our understanding
of how to elicit reasoning capabilities in LLMs and highlight key
considerations for efficiently training the next generation of reasoning
models. This is the academic paper of our previous released Sky-T1-32B-Preview
model. Codes are available at https://github.com/NovaSky-AI/SkyThought.; 3) The role of Trees of Fragmenting Granules (TFG) in the formation of the
  solar supergranular pattern from Hinode observations; We present in this paper an exceptional scientific dataset allowing to
investigate the structure and evolution of the interior of solar
supergranulation cells. Trees of Fragmenting Granules (TFG) and associated
flows were evidenced using Local Correlation Tracking techniques (LCT) from a
24 H duration sequence of Hinode (JAXA/NASA) observations. The treatment of the
dataset exhibits the evolution of the TFG and shows that their mutual
interactions are able to build horizontal flows with longer lifetime than
granules (1 to 2 hours) over a scale of 10 arcsec (the mesogranulation). These
flows act on the diffusion of the intranetwork magnetic elements and also on
the location and shape of the network. Hence, the TFG appear as one of the
major elements involved in supergranular formation and evolution.; 4) DynSTG-Mamba: Dynamic Spatio-Temporal Graph Mamba with Cross-Graph
  Knowledge Distillation for Gait Disorders Recognition; Gait disorder recognition plays a crucial role in the early diagnosis and
monitoring of movement disorders. Existing approaches, including
spatio-temporal graph convolutional networks (ST-GCNs), often face high memory
demands and struggle to capture complex spatio-temporal dependencies, limiting
their efficiency in clinical applications. To address these challenges, we
introduce DynSTG-Mamba (Dynamic Spatio-Temporal Graph Mamba), a novel framework
that combines DF-STGNN and STG-Mamba to enhance motion sequence modeling. The
DF-STGNN incorporates a dynamic spatio-temporal filter that adaptively adjusts
spatial connections between skeletal joints and temporal interactions across
different movement phases. This approach ensures better feature propagation
through dynamic graph structures by considering the hierarchical nature and
dynamics of skeletal gait data. Meanwhile, STG-Mamba, an extension of Mamba
adapted for skeletal motion data, ensures a continuous propagation of states,
facilitating the capture of long-term dependencies while reducing computational
complexity. To reduce the number of model parameters and computational costs
while maintaining consistency, we propose Cross-Graph Relational Knowledge
Distillation, a novel knowledge transfer mechanism that aligns relational
information between teacher (large architecture) and student models (small
architecture) while using shared memory. This ensures that the interactions and
movement patterns of the joints are accurately preserved in the motion
sequences. We validate our DynSTG-Mamba on KOA-NM, PD-WALK, and ATAXIA
datasets, where it outperforms state-of-the-art approaches by achieving in
terms of Accuracy, F1-score, and Recall. Our results highlight the efficiency
and robustness of our approach, offering a lightweight yet highly accurate
solution for automated gait analysis and movement disorder assessment.; 5) SFMNet: Sparse Focal Modulation for 3D Object Detection; We propose SFMNet, a novel 3D sparse detector that combines the efficiency of
sparse convolutions with the ability to model long-range dependencies. While
traditional sparse convolution techniques efficiently capture local structures,
they struggle with modeling long-range relationships. However, capturing
long-range dependencies is fundamental for 3D object detection. In contrast,
transformers are designed to capture these long-range dependencies through
attention mechanisms. But, they come with high computational costs, due to
their quadratic query-key-value interactions. Furthermore, directly applying
attention to non-empty voxels is inefficient due to the sparse nature of 3D
scenes. Our SFMNet is built on a novel Sparse Focal Modulation (SFM) module,
which integrates short- and long-range contexts with linear complexity by
leveraging a new hierarchical sparse convolution design. This approach enables
SFMNet to achieve high detection performance with improved efficiency, making
it well-suited for large-scale LiDAR scenes. We show that our detector achieves
state-of-the-art performance on autonomous driving datasets.; 6) The Legacy of Henrietta Leavitt: A Re-analysis of the First Cepheid
  Period-Luminosity Relation; Henrietta Swan Leavitt's discovery of the relationship between the period and
luminosity (hereafter the Leavitt Law) of 25 variable stars in the Small
Magellanic Cloud, published in 1912, revolutionized cosmology. These variables,
eventually identified as Cepheids, became the first known ""standard candles""
for measuring extragalactic distances and remain the gold standard for this
task today. Leavitt measured light curves, periods, and minimum and maximum
magnitudes from painstaking visual inspection of photographic plates. Her work
paved the way for the first precise series of distance measurements that helped
set the scale of the Universe, and later the discovery of its expansion by
Edwin Hubble in 1929. Here, we re-analyze Leavitt's first Period-Luminosity
relation using observations of the same set of stars but with modern data and
methods of Cepheid analysis. Using only data from Leavitt's notebooks, we
assess the quality of her light curves, measured periods, and the slope and
scatter of her Period-Luminosity relations. We show that modern data and
methods, for the same objects, reduce the scatter of the Period-Luminosity
relation by a factor of two. We also find a bias brightward at the short period
end, due to the non-linearity of the plates and environmental crowding.
Overall, Leavitt's results are in excellent agreement with contemporary
measurements, reinforcing the value of Cepheids in cosmology today, a testament
to the enduring quality of her work.; 7) Surgical Gaussian Surfels: Highly Accurate Real-time Surgical Scene
  Rendering; Accurate geometric reconstruction of deformable tissues in monocular
endoscopic video remains a fundamental challenge in robot-assisted minimally
invasive surgery. Although recent volumetric and point primitive methods based
on neural radiance fields (NeRF) and 3D Gaussian primitives have efficiently
rendered surgical scenes, they still struggle with handling artifact-free tool
occlusions and preserving fine anatomical details. These limitations stem from
unrestricted Gaussian scaling and insufficient surface alignment constraints
during reconstruction. To address these issues, we introduce Surgical Gaussian
Surfels (SGS), which transforms anisotropic point primitives into
surface-aligned elliptical splats by constraining the scale component of the
Gaussian covariance matrix along the view-aligned axis. We predict accurate
surfel motion fields using a lightweight Multi-Layer Perceptron (MLP) coupled
with locality constraints to handle complex tissue deformations. We use
homodirectional view-space positional gradients to capture fine image details
by splitting Gaussian Surfels in over-reconstructed regions. In addition, we
define surface normals as the direction of the steepest density change within
each Gaussian surfel primitive, enabling accurate normal estimation without
requiring monocular normal priors. We evaluate our method on two in-vivo
surgical datasets, where it outperforms current state-of-the-art methods in
surface geometry, normal map quality, and rendering efficiency, while remaining
competitive in real-time rendering performance. We make our code available at
https://github.com/aloma85/SurgicalGaussianSurfels; 8) A Driver Advisory System Based on Large Language Model for High-speed
  Train; With the rapid development of China high-speed railway, drivers face
increasingly significant technical challenges during operations, such as fault
handling. Currently, drivers depend on the onboard mechanic when facing
technical issues, for instance, traction loss or sensor faults. This dependency
can hinder effective operation, even lead to accidents, while waiting for
faults to be addressed. To enhance the accuracy and explainability of actions
during fault handling, an Intelligent Driver Advisory System (IDAS) framework
based on a large language model (LLM) named IDAS-LLM, is introduced. Initially,
domain-fine-tuning of the LLM is performed using a constructed railway
knowledge question-and-answer dataset to improve answer accuracy in
railway-related questions. Subsequently, integration of the Retrieval-augmented
Generation (RAG) architecture is pursued for system design to enhance the
explainability of generated responses. Comparative experiments are conducted
using the constructed railway driving knowledge assessment dataset. Results
indicate that domain-fine-tuned LLMs show an improvement in answer accuracy by
an average of 10%, outperforming some current mainstream LLMs. Additionally,
the inclusion of the RAG framework increases the average recall rate of
question-and-answer sessions by about 4%. Finally, the fault handling
capability of IDAS-LLM is demonstrated through simulations of real operational
scenarios, proving that the proposed framework has practical application
prospects.; 9) Finding all solutions of qKZ equations in characteristic $p$; In [MV] the difference qKZ equations were considered modulo a prime number
$p$ and a family of polynomial solutions of the qKZ equations modulo $p$ was
constructed by an elementary procedure as suitable $p$-approximations of the
hypergeometric integrals. In this paper, we study in detail the first family of
nontrivial example of the qKZ equations in characteristic $p$. We describe all
solutions of these qKZ equations in characteristic $p$ by demonstrating that
they all stem from the $p$-hypergeometric solutions. We also prove a Lagrangian
property (called the orthogonality property) of the subbundle of the qKZ bundle
spanned by the $p$-hypergeometric sections. This paper extends the results of
[VV1] on the differential KZ equations to the difference qKZ equations.; 10) MAPoRL: Multi-Agent Post-Co-Training for Collaborative Large Language
  Models with Reinforcement Learning; Leveraging multiple large language models (LLMs) to build collaborative
multi-agentic workflows has demonstrated significant potential. However, most
previous studies focus on prompting the out-of-the-box LLMs, relying on their
innate capability for collaboration, which may not improve LLMs' performance as
shown recently. In this paper, we introduce a new post-training paradigm MAPoRL
(Multi-Agent Post-co-training for collaborative LLMs with Reinforcement
Learning), to explicitly elicit the collaborative behaviors and further unleash
the power of multi-agentic LLM frameworks. In MAPoRL, multiple LLMs first
generate their own responses independently and engage in a multi-turn
discussion to collaboratively improve the final answer. In the end, a MAPoRL
verifier evaluates both the answer and the discussion, by assigning a score
that verifies the correctness of the answer, while adding incentives to
encourage corrective and persuasive discussions. The score serves as the
co-training reward, and is then maximized through multi-agent RL. Unlike
existing LLM post-training paradigms, MAPoRL advocates the co-training of
multiple LLMs together using RL for better generalization. Accompanied by
analytical insights, our experiments demonstrate that training individual LLMs
alone is insufficient to induce effective collaboration. In contrast,
multi-agent co-training can boost the collaboration performance across
benchmarks, with generalization to unseen domains.; 11) Splitting algorithms for paraxial and It\^o-Schr\""odinger models of wave
  propagation in random media; This paper introduces a full discretization procedure to solve wave beam
propagation in random media modeled by a paraxial wave equation or an
It\^o-Schr\""odinger stochastic partial differential equation. This method bears
similarities with the phase screen method used routinely to solve such
problems. The main axis of propagation is discretized by a centered splitting
scheme with step $\Delta z$ while the transverse variables are treated by a
spectral method after appropriate spatial truncation. The originality of our
approach is its theoretical validity even when the typical wavelength $\theta$
of the propagating signal satisfies $\theta\ll\Delta z$. More precisely, we
obtain a convergence of order $\Delta z$ in mean-square sense while the errors
on statistical moments are of order $(\Delta z)^2$ as expected for standard
centered splitting schemes. This is a surprising result as splitting schemes
typically do not converge when $\Delta z$ is not the smallest scale of the
problem. The analysis is based on equations satisfied by statistical moments in
the It\^o-Schr\""odinger case and on integral (Duhamel) expansions for the
paraxial model. Several numerical simulations illustrate and confirm the
theoretical findings.; 12) Monge-Kantorovich quantiles and ranks for image data; This paper defines quantiles, ranks and statistical depths for image data by
leveraging ideas from measure transportation. The first step is to embed a
distribution of images in a tangent space, with the framework of linear optimal
transport. Therein, Monge-Kantorovich quantiles are shown to provide a
meaningful ordering of image data, with outward images having unusual shapes.
Numerical experiments showcase the relevance of the proposed procedure, for
descriptive analysis, outlier detection or statistical testing.; 13) DenseSplat: Densifying Gaussian Splatting SLAM with Neural Radiance
  Prior; Gaussian SLAM systems excel in real-time rendering and fine-grained
reconstruction compared to NeRF-based systems. However, their reliance on
extensive keyframes is impractical for deployment in real-world robotic
systems, which typically operate under sparse-view conditions that can result
in substantial holes in the map. To address these challenges, we introduce
DenseSplat, the first SLAM system that effectively combines the advantages of
NeRF and 3DGS. DenseSplat utilizes sparse keyframes and NeRF priors for
initializing primitives that densely populate maps and seamlessly fill gaps. It
also implements geometry-aware primitive sampling and pruning strategies to
manage granularity and enhance rendering efficiency. Moreover, DenseSplat
integrates loop closure and bundle adjustment, significantly enhancing
frame-to-frame tracking accuracy. Extensive experiments on multiple large-scale
datasets demonstrate that DenseSplat achieves superior performance in tracking
and mapping compared to current state-of-the-art methods.; 14) A Multi-Objective Evaluation Framework for Analyzing Utility-Fairness
  Trade-Offs in Machine Learning Systems; The evaluation of fairness models in Machine Learning involves complex
challenges, such as defining appropriate metrics, balancing trade-offs between
utility and fairness, and there are still gaps in this stage. This work
presents a novel multi-objective evaluation framework that enables the analysis
of utility-fairness trade-offs in Machine Learning systems. The framework was
developed using criteria from Multi-Objective Optimization that collect
comprehensive information regarding this complex evaluation task. The
assessment of multiple Machine Learning systems is summarized, both
quantitatively and qualitatively, in a straightforward manner through a radar
chart and a measurement table encompassing various aspects such as convergence,
system capacity, and diversity. The framework's compact representation of
performance facilitates the comparative analysis of different Machine Learning
strategies for decision-makers, in real-world applications, with single or
multiple fairness requirements. The framework is model-agnostic and flexible to
be adapted to any kind of Machine Learning systems, that is, black- or
white-box, any kind and quantity of evaluation metrics, including
multidimensional fairness criteria. The functionality and effectiveness of the
proposed framework is shown with different simulations, and an empirical study
conducted on a real-world dataset with various Machine Learning systems.; 15) Measuring AI agent autonomy: Towards a scalable approach with code
  inspection; AI agents are AI systems that can achieve complex goals autonomously.
Assessing the level of agent autonomy is crucial for understanding both their
potential benefits and risks. Current assessments of autonomy often focus on
specific risks and rely on run-time evaluations -- observations of agent
actions during operation. We introduce a code-based assessment of autonomy that
eliminates the need to run an AI agent to perform specific tasks, thereby
reducing the costs and risks associated with run-time evaluations. Using this
code-based framework, the orchestration code used to run an AI agent can be
scored according to a taxonomy that assesses attributes of autonomy: impact and
oversight. We demonstrate this approach with the AutoGen framework and select
applications.; 16) ART: Anonymous Region Transformer for Variable Multi-Layer Transparent
  Image Generation; Multi-layer image generation is a fundamental task that enables users to
isolate, select, and edit specific image layers, thereby revolutionizing
interactions with generative models. In this paper, we introduce the Anonymous
Region Transformer (ART), which facilitates the direct generation of variable
multi-layer transparent images based on a global text prompt and an anonymous
region layout. Inspired by Schema theory suggests that knowledge is organized
in frameworks (schemas) that enable people to interpret and learn from new
information by linking it to prior knowledge.}, this anonymous region layout
allows the generative model to autonomously determine which set of visual
tokens should align with which text tokens, which is in contrast to the
previously dominant semantic layout for the image generation task. In addition,
the layer-wise region crop mechanism, which only selects the visual tokens
belonging to each anonymous region, significantly reduces attention computation
costs and enables the efficient generation of images with numerous distinct
layers (e.g., 50+). When compared to the full attention approach, our method is
over 12 times faster and exhibits fewer layer conflicts. Furthermore, we
propose a high-quality multi-layer transparent image autoencoder that supports
the direct encoding and decoding of the transparency of variable multi-layer
images in a joint manner. By enabling precise control and scalable layer
generation, ART establishes a new paradigm for interactive content creation.; 17) NLP-Based .NET CLR Event Logs Analyzer; In this paper, we present a tool for analyzing .NET CLR event logs based on a
novel method inspired by Natural Language Processing (NLP) approach. Our
research addresses the growing need for effective monitoring and optimization
of software systems through detailed event log analysis. We utilize a
BERT-based architecture with an enhanced tokenization process customized to
event logs. The tool, developed using Python, its libraries, and an SQLite
database, allows both conducting experiments for academic purposes and
efficiently solving industry-emerging tasks. Our experiments demonstrate the
efficacy of our approach in compressing event sequences, detecting recurring
patterns, and identifying anomalies. The trained model shows promising results,
with a high accuracy rate in anomaly detection, which demonstrates the
potential of NLP methods to improve the reliability and stability of software
systems.; 18) Monitoring Reasoning Models for Misbehavior and the Risks of Promoting
  Obfuscation; Mitigating reward hacking--where AI systems misbehave due to flaws or
misspecifications in their learning objectives--remains a key challenge in
constructing capable and aligned models. We show that we can monitor a frontier
reasoning model, such as OpenAI o3-mini, for reward hacking in agentic coding
environments by using another LLM that observes the model's chain-of-thought
(CoT) reasoning. CoT monitoring can be far more effective than monitoring agent
actions and outputs alone, and we further found that a LLM weaker than o3-mini,
namely GPT-4o, can effectively monitor a stronger model. Because CoT monitors
can be effective at detecting exploits, it is natural to ask whether those
exploits can be suppressed by incorporating a CoT monitor directly into the
agent's training objective. While we show that integrating CoT monitors into
the reinforcement learning reward can indeed produce more capable and more
aligned agents in the low optimization regime, we find that with too much
optimization, agents learn obfuscated reward hacking, hiding their intent
within the CoT while still exhibiting a significant rate of reward hacking.
Because it is difficult to tell when CoTs have become obfuscated, it may be
necessary to pay a monitorability tax by not applying strong optimization
pressures directly to the chain-of-thought, ensuring that CoTs remain
monitorable and useful for detecting misaligned behavior.; 19) Demons in the Detail: On Implementing Load Balancing Loss for Training
  Specialized Mixture-of-Expert Models; This paper revisits the implementation of
$\textbf{L}$oad-$\textbf{b}$alancing $\textbf{L}$oss (LBL) when training
Mixture-of-Experts (MoEs) models. Specifically, LBL for MoEs is defined as $N_E
\sum_{i=1}^{N_E} f_i p_i$, where $N_E$ is the total number of experts, $f_i$
represents the frequency of expert $i$ being selected, and $p_i$ denotes the
average gating score of the expert $i$. Existing MoE training frameworks
usually employ the parallel training strategy so that $f_i$ and the LBL are
calculated within a $\textbf{micro-batch}$ and then averaged across parallel
groups. In essence, a micro-batch for training billion-scale LLMs normally
contains very few sequences. So, the micro-batch LBL is almost at the sequence
level, and the router is pushed to distribute the token evenly within each
sequence. Under this strict constraint, even tokens from a domain-specific
sequence ($\textit{e.g.}$, code) are uniformly routed to all experts, thereby
inhibiting expert specialization. In this work, we propose calculating LBL
using a $\textbf{global-batch}$ to loose this constraint. Because a
global-batch contains much more diverse sequences than a micro-batch, which
will encourage load balance at the corpus level. Specifically, we introduce an
extra communication step to synchronize $f_i$ across micro-batches and then use
it to calculate the LBL. Through experiments on training MoEs-based LLMs (up to
$\textbf{42.8B}$ total parameters and $\textbf{400B}$ tokens), we surprisingly
find that the global-batch LBL strategy yields excellent performance gains in
both pre-training perplexity and downstream tasks. Our analysis reveals that
the global-batch LBL also greatly improves the domain specialization of MoE
experts.; 20) R2LDM: An Efficient 4D Radar Super-Resolution Framework Leveraging
  Diffusion Model; We introduce R2LDM, an innovative approach for generating dense and accurate
4D radar point clouds, guided by corresponding LiDAR point clouds. Instead of
utilizing range images or bird's eye view (BEV) images, we represent both LiDAR
and 4D radar point clouds using voxel features, which more effectively capture
3D shape information. Subsequently, we propose the Latent Voxel Diffusion Model
(LVDM), which performs the diffusion process in the latent space. Additionally,
a novel Latent Point Cloud Reconstruction (LPCR) module is utilized to
reconstruct point clouds from high-dimensional latent voxel features. As a
result, R2LDM effectively generates LiDAR-like point clouds from paired raw
radar data. We evaluate our approach on two different datasets, and the
experimental results demonstrate that our model achieves 6- to 10-fold
densification of radar point clouds, outperforming state-of-the-art baselines
in 4D radar point cloud super-resolution. Furthermore, the enhanced radar point
clouds generated by our method significantly improve downstream tasks,
achieving up to 31.7% improvement in point cloud registration recall rate and
24.9% improvement in object detection accuracy.; 21) Emission-Line Diagnostics at z>4: [OIII]{\lambda}4363/H\gamma; We use JWST Near-Infrared Spectrograph (NIRSpec) observations from the the
Cosmic Evolution Early Release survey (CEERS), GLASS-JWST ERS (GLASS), and JWST
Advanced Deep Extragalactic Survey (JADES) to measure rest-frame optical
emission-line ratios of 90 galaxies at z>4. The stacked spectra of galaxies
with and without a broad-line feature reveal a difference in the
[OIII]$\lambda$ 4363 and H$\gamma$ ratios. This motivated our investigation of
the [OIII]/H$\gamma$ vs [NeIII]/[OII] diagram. We define two AGN/SF
classification lines based on 1869 SDSS galaxies at z$\sim$0. After applying a
redshift correction to the AGN/SF lines we find 76.8% of BLAGN continue to land
in the AGN region of the diagnostic largely due to the [NeIII]/[OII] ratio.
However, 40.2% of non-BLAGN land in the AGN region as well, this could be due
to star forming galaxies having harder ionization of there are narrow line AGN
which are not accounted for. This indicates the potential of the [NeIII]/[OII]
ratio to continue classifying galaxies to z$\sim$6. We further inspect galaxies
without broad emission lines in each region of [OIII]/H\gamma vs [NeIII]/[OII]
diagram and found that they have slightly stronger CIII]$\lambda$1908 fluxes
and equivalent width when landing in the BLAGN region. However, the cause of
this higher ionization is unclear. Additionally, we find that BLAGN are
characterized by a higher ionization (at constant electron temperature)
compared to non-broad line galaxies.; 22) Automatic Identification of Samples in Hip-Hop Music via Multi-Loss
  Training and an Artificial Dataset; Sampling, the practice of reusing recorded music or sounds from another
source in a new work, is common in popular music genres like hip-hop and rap.
Numerous services have emerged that allow users to identify connections between
samples and the songs that incorporate them, with the goal of enhancing music
discovery. Designing a system that can perform the same task automatically is
challenging, as samples are commonly altered with audio effects like pitch- and
time-stretching and may only be seconds long. Progress on this task has been
minimal and is further blocked by the limited availability of training data.
Here, we show that a convolutional neural network trained on an artificial
dataset can identify real-world samples in commercial hip-hop music. We extract
vocal, harmonic, and percussive elements from several databases of
non-commercial music recordings using audio source separation, and train the
model to fingerprint a subset of these elements in transformed versions of the
original audio. We optimize the model using a joint classification and metric
learning loss and show that it achieves 13% greater precision on real-world
instances of sampling than a fingerprinting system using acoustic landmarks,
and that it can recognize samples that have been both pitch shifted and time
stretched. We also show that, for half of the commercial music recordings we
tested, our model is capable of locating the position of a sample to within
five seconds.; 23) Accelerating true orbit pseudorandom number generation using Newton's
  method; The binary expansions of irrational algebraic numbers can serve as
high-quality pseudorandom binary sequences. This study presents an efficient
method for computing the exact binary expansions of real quadratic algebraic
integers using Newton's method. To this end, we clarify conditions under which
the first $N$ bits of the binary expansion of an irrational number match those
of its upper rational approximation. Furthermore, we establish that the
worst-case time complexity of generating a sequence of length $N$ with the
proposed method is equivalent to the complexity of multiplying two $N$-bit
integers, showing its efficiency compared to a previously proposed true orbit
generator. We report the results of numerical experiments on computation time
and memory usage, highlighting in particular that the proposed method
successfully accelerates true orbit pseudorandom number generation. We also
confirm that a generated pseudorandom sequence successfully passes all the
statistical tests included in RabbitFile of TestU01.; 24) A comprehensive numerical investigation of a coupled mathematical model
  of neuronal excitability; Being an example for a relaxation oscillator, the FitzHugh-Nagumo model has
been widely employed for describing the generation of action potentials. In
this paper, we begin with a biological interpretation of what the subsequent
mathematical and numerical analyses of the model entail. The interaction
between action potential variable and recovery variable is then revisited
through linear stability analysis around the equilibrium and local stability
conditions are determined. Analytical results are compared with numerical
simulations. The study aims to show an alternative approach regarding Taylor
polynomials and constructed difference scheme which play a key role in the
numerical approach for the problem. The robustness of the schemes is
investigated in terms of convergency and stability of the techniques. This
systematic approach by the combination of numerical techniques provides
beneficial results which are uniquely designed for the FitzHugh-Nagumo model.
We describe the matrix representations with the collocation points. Then the
method is applied in order to acquire a system of nonlinear algebraic
equations. On the other hand, we apply finite difference scheme and its
stability is also performed. Moreover, the numerical simulations are shown.
Consequently, a comprehensive investigation of the related model is examined.; 25) ForestSplats: Deformable transient field for Gaussian Splatting in the
  Wild; Recently, 3D Gaussian Splatting (3D-GS) has emerged, showing real-time
rendering speeds and high-quality results in static scenes. Although 3D-GS
shows effectiveness in static scenes, their performance significantly degrades
in real-world environments due to transient objects, lighting variations, and
diverse levels of occlusion. To tackle this, existing methods estimate
occluders or transient elements by leveraging pre-trained models or integrating
additional transient field pipelines. However, these methods still suffer from
two defects: 1) Using semantic features from the Vision Foundation model (VFM)
causes additional computational costs. 2) The transient field requires
significant memory to handle transient elements with per-view Gaussians and
struggles to define clear boundaries for occluders, solely relying on
photometric errors. To address these problems, we propose ForestSplats, a novel
approach that leverages the deformable transient field and a superpixel-aware
mask to efficiently represent transient elements in the 2D scene across
unconstrained image collections and effectively decompose static scenes from
transient distractors without VFM. We designed the transient field to be
deformable, capturing per-view transient elements. Furthermore, we introduce a
superpixel-aware mask that clearly defines the boundaries of occluders by
considering photometric errors and superpixels. Additionally, we propose
uncertainty-aware densification to avoid generating Gaussians within the
boundaries of occluders during densification. Through extensive experiments
across several benchmark datasets, we demonstrate that ForestSplats outperforms
existing methods without VFM and shows significant memory efficiency in
representing transient elements.; 26) CollabLLM: From Passive Responders to Active Collaborators; Large Language Models are typically trained with next-turn rewards, limiting
their ability to optimize for long-term interaction. As a result, they often
respond passively to ambiguous or open-ended user requests, failing to help
users reach their ultimate intents and leading to inefficient conversations. To
address these limitations, we introduce CollabLLM, a novel and general training
framework that enhances multiturn human-LLM collaboration. Its key innovation
is a collaborative simulation that estimates the long-term contribution of
responses using Multiturn-aware Rewards. By reinforcement fine-tuning these
rewards, CollabLLM goes beyond responding to user requests, and actively
uncovers user intent and offers insightful suggestions-a key step towards more
human-centered AI. We also devise a multiturn interaction benchmark with three
challenging tasks such as document creation. CollabLLM significantly
outperforms our baselines with averages of 18.5% higher task performance and
46.3% improved interactivity by LLM judges. Finally, we conduct a large user
study with 201 judges, where CollabLLM increases user satisfaction by 17.6% and
reduces user spent time by 10.4%.; 27) Characterizing Transfer Graphs of Suspicious ERC-20 Tokens; Ethereum is currently the second largest blockchain by market capitalization
and a popular platform for cryptocurrencies. As it has grown, the high value
present and the anonymity afforded by the technology have led Ethereum to
become a hotbed for various cybercrimes. This paper seeks to understand how
these fraudulent schemes may be characterized and develop methods for detecting
them. One key feature introduced by Ethereum is the ability to use programmable
smart contracts to execute code on the blockchain. A common use of smart
contracts is implementing fungible tokens with the ERC-20 interface. Such
tokens can be used to impersonate legitimate tokens and defraud users. By
parsing the event logs emitted by these ERC-20 contracts over 20 different
periods of 100K blocks, we construct token transfer graphs for each of the
available ERC-20 tokens on the blockchain. By analyzing these graphs, we find a
set of characteristics by which suspicious contracts are distinguished from
legitimate ones. These observations result in a simple model that can identify
scam contracts with an average of 88.7% accuracy. This suggests that the
mechanism by which fraudulent schemes function strongly correlates with their
transfer graphs and that these graphs may be used to improve scam-detection
mechanisms, contributing to making Ethereum safer.; 28) A Robust Remote Photoplethysmography Method; Remote photoplethysmography (rPPG) is a method for measuring a subjects heart
rate remotely using a camera. Factors such as subject movement, ambient light
level, makeup etc. complicate such measurements by distorting the observed
pulse. Recent works on this topic have proposed a variety of approaches for
accurately measuring heart rate in humans, however these methods were tested in
ideal conditions, where the subject does not make significant movements and all
measurements are taken at the same level of illumination. In more realistic
conditions these methods suffer from decreased accuracy. The study proposes a
more robust method that is less susceptible to distortions and has minimal
hardware requirements. The proposed method uses a combination of mathematical
transforms to calculate the subjects heart rate. It performs best when used
with a camera that has been modified by removing its infrared filter, although
using an unmodified camera is also possible. The method was tested on 26 videos
taken from 19 volunteers of varying gender and age. The obtained results were
compared to reference data and the average mean absolute error was found to be
at 1.95 beats per minute, which is noticeably better than the results from
previous works. The remote photoplethysmography method proposed in the present
article is more resistant to distortions than methods from previous
publications and thus allows one to remotely and accurately measure the
subjects heart rate without imposing any significant limitations on the
subjects behavior.; 29) Prompt to Restore, Restore to Prompt: Cyclic Prompting for Universal
  Adverse Weather Removal; Universal adverse weather removal (UAWR) seeks to address various weather
degradations within a unified framework. Recent methods are inspired by prompt
learning using pre-trained vision-language models (e.g., CLIP), leveraging
degradation-aware prompts to facilitate weather-free image restoration,
yielding significant improvements. In this work, we propose CyclicPrompt, an
innovative cyclic prompt approach designed to enhance the effectiveness,
adaptability, and generalizability of UAWR. CyclicPrompt Comprises two key
components: 1) a composite context prompt that integrates weather-related
information and context-aware representations into the network to guide
restoration. This prompt differs from previous methods by marrying learnable
input-conditional vectors with weather-specific knowledge, thereby improving
adaptability across various degradations. 2) The erase-and-paste mechanism,
after the initial guided restoration, substitutes weather-specific knowledge
with constrained restoration priors, inducing high-quality weather-free
concepts into the composite prompt to further fine-tune the restoration
process. Therefore, we can form a cyclic ""Prompt-Restore-Prompt"" pipeline that
adeptly harnesses weather-specific knowledge, textual contexts, and reliable
textures. Extensive experiments on synthetic and real-world datasets validate
the superior performance of CyclicPrompt. The code is available at:
https://github.com/RongxinL/CyclicPrompt.; 30) MutualForce: Mutual-Aware Enhancement for 4D Radar-LiDAR 3D Object
  Detection; Radar and LiDAR have been widely used in autonomous driving as LiDAR provides
rich structure information, and radar demonstrates high robustness under
adverse weather. Recent studies highlight the effectiveness of fusing radar and
LiDAR point clouds. However, challenges remain due to the modality misalignment
and information loss during feature extractions. To address these issues, we
propose a 4D radar-LiDAR framework to mutually enhance their representations.
Initially, the indicative features from radar are utilized to guide both radar
and LiDAR geometric feature learning. Subsequently, to mitigate their sparsity
gap, the shape information from LiDAR is used to enrich radar BEV features.
Extensive experiments on the View-of-Delft (VoD) dataset demonstrate our
approach's superiority over existing methods, achieving the highest mAP of
71.76% across the entire area and 86.36\% within the driving corridor.
Especially for cars, we improve the AP by 4.17% and 4.20% due to the strong
indicative features and symmetric shapes.; 31) Retrievals Can Be Detrimental: A Contrastive Backdoor Attack Paradigm on
  Retrieval-Augmented Diffusion Models; Diffusion models (DMs) have recently demonstrated remarkable generation
capability. However, their training generally requires huge computational
resources and large-scale datasets. To solve these, recent studies empower DMs
with the advanced Retrieval-Augmented Generation (RAG) technique and propose
retrieval-augmented diffusion models (RDMs). By incorporating rich knowledge
from an auxiliary database, RAG enhances diffusion models' generation and
generalization ability while significantly reducing model parameters. Despite
the great success, RAG may introduce novel security issues that warrant further
investigation. In this paper, we reveal that the RDM is susceptible to backdoor
attacks by proposing a multimodal contrastive attack approach named BadRDM. Our
framework fully considers RAG's characteristics and is devised to manipulate
the retrieved items for given text triggers, thereby further controlling the
generated contents. Specifically, we first insert a tiny portion of images into
the retrieval database as target toxicity surrogates. Subsequently, a malicious
variant of contrastive learning is adopted to inject backdoors into the
retriever, which builds shortcuts from triggers to the toxicity surrogates.
Furthermore, we enhance the attacks through novel entropy-based selection and
generative augmentation strategies that can derive better toxicity surrogates.
Extensive experiments on two mainstream tasks demonstrate the proposed BadRDM
achieves outstanding attack effects while preserving the model's benign
utility.; 32) UniCoRN: Unified Commented Retrieval Network with LMMs; Multimodal retrieval methods have limitations in handling complex,
compositional queries that require reasoning about the visual content of both
the query and the retrieved entities. On the other hand, Large Multimodal
Models (LMMs) can answer with language to more complex visual questions, but
without the inherent ability to retrieve relevant entities to support their
answers. We aim to address these limitations with UniCoRN, a Unified Commented
Retrieval Network that combines the strengths of composed multimodal retrieval
methods and generative language approaches, going beyond Retrieval-Augmented
Generation (RAG). We introduce an entity adapter module to inject the retrieved
multimodal entities back into the LMM, so it can attend to them while
generating answers and comments. By keeping the base LMM frozen, UniCoRN
preserves its original capabilities while being able to perform both retrieval
and text generation tasks under a single integrated framework. To assess these
new abilities, we introduce the Commented Retrieval task (CoR) and a
corresponding dataset, with the goal of retrieving an image that accurately
answers a given question and generate an additional textual response that
provides further clarification and details about the visual information. We
demonstrate the effectiveness of UniCoRN on several datasets showing
improvements of +4.5% recall over the state of the art for composed multimodal
retrieval and of +14.9% METEOR / +18.4% BEM over RAG for commenting in CoR.; 33) A Quantum Analog of Delsarte's Linear Programming Bounds; This thesis presents results in quantum error correction within the context
of finite dimensional quantum metric spaces. In classical error correction, a
focal problem is the study of large codes of metric spaces. For a class of
finite metric spaces that are also metric association schemes, Delsarte
introduced a method of using linear programming to compute upper bounds on the
size of codes. Within quantum error correction, there is an analogous study of
large quantum codes of quantum metric spaces and, in the setting of quantum
Hamming space, a quantum analog of Delsarte's method was discovered by Shor and
Laflamme and independently by Rains. Later, Bumgardner introduced an analogous
method for single-spin codes, or quantum codes related to the Lie algebra
$\mathfrak{su}(2)$. The main contribution of this thesis is a generalization of
the results of Shor, Laflamme, Rains, and Bumgardner to a class of finite
dimensional quantum metric spaces analogous to metric association schemes of
the classical case.; 34) Edit as You See: Image-guided Video Editing via Masked Motion Modeling; Recent advancements in diffusion models have significantly facilitated
text-guided video editing. However, there is a relative scarcity of research on
image-guided video editing, a method that empowers users to edit videos by
merely indicating a target object in the initial frame and providing an RGB
image as reference, without relying on the text prompts. In this paper, we
propose a novel Image-guided Video Editing Diffusion model, termed IVEDiff for
the image-guided video editing. IVEDiff is built on top of image editing
models, and is equipped with learnable motion modules to maintain the temporal
consistency of edited video. Inspired by self-supervised learning concepts, we
introduce a masked motion modeling fine-tuning strategy that empowers the
motion module's capabilities for capturing inter-frame motion dynamics, while
preserving the capabilities for intra-frame semantic correlations modeling of
the base image editing model. Moreover, an optical-flow-guided motion reference
network is proposed to ensure the accurate propagation of information between
edited video frames, alleviating the misleading effects of invalid information.
We also construct a benchmark to facilitate further research. The comprehensive
experiments demonstrate that our method is able to generate temporally smooth
edited videos while robustly dealing with various editing objects with high
quality.; 35) Learn Your Scales: Towards Scale-Consistent Generative Novel View
  Synthesis; Conventional depth-free multi-view datasets are captured using a moving
monocular camera without metric calibration. The scales of camera positions in
this monocular setting are ambiguous. Previous methods have acknowledged scale
ambiguity in multi-view data via various ad-hoc normalization pre-processing
steps, but have not directly analyzed the effect of incorrect scene scales on
their application. In this paper, we seek to understand and address the effect
of scale ambiguity when used to train generative novel view synthesis methods
(GNVS). In GNVS, new views of a scene or object can be minimally synthesized
given a single image and are, thus, unconstrained, necessitating the use of
generative methods. The generative nature of these models captures all aspects
of uncertainty, including any uncertainty of scene scales, which act as
nuisance variables for the task. We study the effect of scene scale ambiguity
in GNVS when sampled from a single image by isolating its effect on the
resulting models and, based on these intuitions, define new metrics that
measure the scale inconsistency of generated views. We then propose a framework
to estimate scene scales jointly with the GNVS model in an end-to-end fashion.
Empirically, we show that our method reduces the scale inconsistency of
generated views without the complexity or downsides of previous scale
normalization methods. Further, we show that removing this ambiguity improves
generated image quality of the resulting GNVS model.; 36) Towards Robust and Realistic Human Pose Estimation via WiFi Signals; Robust WiFi-based human pose estimation is a challenging task that bridges
discrete and subtle WiFi signals to human skeletons. This paper revisits this
problem and reveals two critical yet overlooked issues: 1) cross-domain gap,
i.e., due to significant variations between source-target domain pose
distributions; and 2) structural fidelity gap, i.e., predicted skeletal poses
manifest distorted topology, usually with misplaced joints and disproportionate
bone lengths. This paper fills these gaps by reformulating the task into a
novel two-phase framework dubbed DT-Pose: Domain-consistent representation
learning and Topology-constrained Pose decoding. Concretely, we first propose a
temporal-consistent contrastive learning strategy with uniformity
regularization, coupled with self-supervised masking-reconstruction operations,
to enable robust learning of domain-consistent and motion-discriminative
WiFi-specific representations. Beyond this, we introduce a simple yet effective
pose decoder with task prompts, which integrates Graph Convolution Network
(GCN) and Transformer layers to constrain the topology structure of the
generated skeleton by exploring the adjacent-overarching relationships among
human joints. Extensive experiments conducted on various benchmark datasets
highlight the superior performance of our method in tackling these fundamental
challenges in both 2D/3D human pose estimation tasks.; 37) Broad Spectrum Coherent Frequency Conversion with Kinetic Inductance
  Superconducting Metastructures; Parametric frequency converters (PFCs) play a critical role in bridging the
frequency gap between quantum information carriers. PFCs in the microwave band
are particularly important for superconducting quantum processors, but their
operating bandwidth is often strongly limited. Here, we present a multimode
kinetic metastructure for parametric frequency conversion between broadly
spanning frequency modes. This device comprises a chain of asymmetric kinetic
inductance grids designed to deliver efficient three-wave mixing nonlinearity.
We demonstrate high efficient coherent conversion among broadly distributed
modes, and the mode frequency is continuously tunable by controlling the
external magnetic field strength, making it ideally suited for quantum
computing and communication applications requiring flexible and efficient
frequency conversion.; 38) Large deformation and collapse analysis of re-entrant auxetic and
  hexagonal honeycomb lattice structures subjected to tension and compression; Additively manufactured auxetic structures offer desirable qualities like
lightweight, good energy absorption, excellent indentation resistance, high
shear stiffness and fracture toughness among others. A wide range of materials
from polymers to metals can be used to fabricate these structures. In contrast
to conventional materials, auxetic structures exhibit negative Poisson's
ratios. Hence, unique mechanical properties can be achieved by specific design.
In this work, two types of structures, namely re-entrant auxetic and
non-auxetic hexagonal honeycomb, are investigated. Large deformation analyses
in both 2D plane strain and 3D are conducted using linear triangular and
tetrahedral multi-field displacement-pressure elements. Hyperelastic with
rate-independent plasticity constitutive models are utilized and calibrated
with experimental uni-axial tensile test results. The structures are subjected
to compression and tension at both transversal and longitudinal directions. The
contact domain method is employed to capture both self-contact and the
interaction between the structure and loading plates. The obtained results show
consistency with the experimental data. The outcomes of the analyses regarding
the re-entrant auxetic structure agree with the expected behavior, showing a
negative value of Poisson's ratio and greater efficiency of energy absorption
than the hexagonal honeycomb. By understanding the influence of the loading
direction on the structural behavior, equivalent Poisson's ratio and energy
absorption a reliable theoretical framework for prospective designs of the
lattice materials can be established.; 39) Landmarks Are Alike Yet Distinct: Harnessing Similarity and
  Individuality for One-Shot Medical Landmark Detection; Landmark detection plays a crucial role in medical imaging applications such
as disease diagnosis, bone age estimation, and therapy planning. However,
training models for detecting multiple landmarks simultaneously often
encounters the ""seesaw phenomenon"", where improvements in detecting certain
landmarks lead to declines in detecting others. Yet, training a separate model
for each landmark increases memory usage and computational overhead. To address
these challenges, we propose a novel approach based on the belief that
""landmarks are distinct"" by training models with pseudo-labels and template
data updated continuously during the training process, where each model is
dedicated to detecting a single landmark to achieve high accuracy. Furthermore,
grounded on the belief that ""landmarks are also alike"", we introduce an
adapter-based fusion model, combining shared weights with landmark-specific
weights, to efficiently share model parameters while allowing flexible
adaptation to individual landmarks. This approach not only significantly
reduces memory and computational resource requirements but also effectively
mitigates the seesaw phenomenon in multi-landmark training. Experimental
results on publicly available medical image datasets demonstrate that the
single-landmark models significantly outperform traditional multi-point joint
training models in detecting individual landmarks. Although our adapter-based
fusion model shows slightly lower performance compared to the combined results
of all single-landmark models, it still surpasses the current state-of-the-art
methods while achieving a notable improvement in resource efficiency.; 40) Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network; An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images.; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.; 41) X-Field: A Physically Grounded Representation for 3D X-ray
  Reconstruction; X-ray imaging is indispensable in medical diagnostics, yet its use is tightly
regulated due to potential health risks. To mitigate radiation exposure, recent
research focuses on generating novel views from sparse inputs and
reconstructing Computed Tomography (CT) volumes, borrowing representations from
the 3D reconstruction area. However, these representations originally target
visible light imaging that emphasizes reflection and scattering effects, while
neglecting penetration and attenuation properties of X-ray imaging. In this
paper, we introduce X-Field, the first 3D representation specifically designed
for X-ray imaging, rooted in the energy absorption rates across different
materials. To accurately model diverse materials within internal structures, we
employ 3D ellipsoids with distinct attenuation coefficients. To estimate each
material's energy absorption of X-rays, we devise an efficient path
partitioning algorithm accounting for complex ellipsoid intersections. We
further propose hybrid progressive initialization to refine the geometric
accuracy of X-Filed and incorporate material-based optimization to enhance
model fitting along material boundaries. Experiments show that X-Field achieves
superior visual fidelity on both real-world human organ and synthetic object
datasets, outperforming state-of-the-art methods in X-ray Novel View Synthesis
and CT Reconstruction.; 42) Consistent Warped $\mathbb R\times T^{1,1}$ Reduction of Heterotic
  Supergravity; We find a curious duality between the bosonic sector of heterotic
supergravity and the noncritical bosonic string with conformal anomaly. This
allows us to map the Mink$_4\times S^3\times S^3$ vacuum of the bosonic string
to a new warped $\mathbb R\times (T^{1,1}\times$Mink$_4)$ vacuum of the
heterotic theory, which turns out to be supersymmetric, preserving $1/4$ of the
supersymmetry. We perform consistent dimensional reduction of the heterotic
theory on the warped $\mathbb R\times T^{1,1}$ internal space and obtain ${\cal
N}=1$, $D=4$ supergravity, with one complex scalar multiplet and $SU(2)\times
SU(2)$ Yang-Mills multiplets, where the gauge symmetry is originated from the
isometry group of $T^{1,1}$.; 43) Gaussian Graph Network: Learning Efficient and Generalizable Gaussian
  Representations from Multi-view Images; 3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis
performance. While conventional methods require per-scene optimization, more
recently several feed-forward methods have been proposed to generate
pixel-aligned Gaussian representations with a learnable network, which are
generalizable to different scenes. However, these methods simply combine
pixel-aligned Gaussians from multiple views as scene representations, thereby
leading to artifacts and extra memory cost without fully capturing the
relations of Gaussians from different images. In this paper, we propose
Gaussian Graph Network (GGN) to generate efficient and generalizable Gaussian
representations. Specifically, we construct Gaussian Graphs to model the
relations of Gaussian groups from different views. To support message passing
at Gaussian level, we reformulate the basic graph operations over Gaussian
representations, enabling each Gaussian to benefit from its connected Gaussian
groups with Gaussian feature fusion. Furthermore, we design a Gaussian pooling
layer to aggregate various Gaussian groups for efficient representations. We
conduct experiments on the large-scale RealEstate10K and ACID datasets to
demonstrate the efficiency and generalization of our method. Compared to the
state-of-the-art methods, our model uses fewer Gaussians and achieves better
image quality with higher rendering speed.; 44) LookCloser: Frequency-aware Radiance Field for Tiny-Detail Scene; Humans perceive and comprehend their surroundings through information
spanning multiple frequencies. In immersive scenes, people naturally scan their
environment to grasp its overall structure while examining fine details of
objects that capture their attention. However, current NeRF frameworks
primarily focus on modeling either high-frequency local views or the broad
structure of scenes with low-frequency information, which is limited to
balancing both. We introduce FA-NeRF, a novel frequency-aware framework for
view synthesis that simultaneously captures the overall scene structure and
high-definition details within a single NeRF model. To achieve this, we propose
a 3D frequency quantification method that analyzes the scene's frequency
distribution, enabling frequency-aware rendering. Our framework incorporates a
frequency grid for fast convergence and querying, a frequency-aware feature
re-weighting strategy to balance features across different frequency contents.
Extensive experiments show that our method significantly outperforms existing
approaches in modeling entire scenes while preserving fine details.; 45) Revisiting MnSe : a Magnetic Semiconductor with Spin-Phonon coupling; Spin-phonon interactions in 2D magnetic materials are crucial in advancing
next-generation spintronic devices. Therefore, identifying new materials with
significant spin-phonon interactions is of great importance. In this context,
MnSe, previously recognized as an exemplary non-layered p-type semiconductor
emerges in this study as an intriguing material with notable spin-phonon
characteristics. The complex magnetism in pristine MnSe, primarily dominated by
antiferromagnetism with a weak ferromagnetic component, gives rise to both
spontaneous and conventional exchange bias effects at low temperatures. In an
effort to understand this intriguing magnetism, we conducted a detailed Raman
spectroscopy study, which reveals unconventional deviations from the usual
phonon anharmonicity around Neel temperature (170 K), in the self-energies of
the P1, P2, and P3 modes. Notably, the P1 mode is most sensitive to spin-phonon
coupling, while the P2 mode is particularly responsive to the structural phase
transition at 250 K. Therefore, these findings provide comprehensive insights
into the phase transitions of pristine MnSe, particularly highlighting the
previously unobserved interplay between its magnetic behavior and phonon
dynamics.; 46) VideoMAP: Toward Scalable Mamba-based Video Autoregressive Pretraining; Recent Mamba-based architectures for video understanding demonstrate
promising computational efficiency and competitive performance, yet struggle
with overfitting issues that hinder their scalability. To overcome this
challenge, we introduce VideoMAP, a Hybrid Mamba-Transformer framework
featuring a novel pre-training approach. VideoMAP uses a 4:1
Mamba-to-Transformer ratio, effectively balancing computational cost and model
capacity. This architecture, combined with our proposed frame-wise masked
autoregressive pre-training strategy, delivers significant performance gains
when scaling to larger models. Additionally, VideoMAP exhibits impressive
sample efficiency, significantly outperforming existing methods with less
training data. Experiments show that VideoMAP outperforms existing models
across various datasets, including Kinetics-400, Something-Something V2,
Breakfast, and COIN. Furthermore, we demonstrate the potential of VideoMAP as a
visual encoder for multimodal large language models, highlighting its ability
to reduce memory usage and enable the processing of longer video sequences. The
code is open-source at https://github.com/yunzeliu/MAP; 47) R-ParVI: Particle-based variational inference through lens of rewards; A reward-guided, gradient-free ParVI method, \textit{R-ParVI}, is proposed
for sampling partially known densities (e.g. up to a constant). R-ParVI
formulates the sampling problem as particle flow driven by rewards: particles
are drawn from a prior distribution, navigate through parameter space with
movements determined by a reward mechanism blending assessments from the target
density, with the steady state particle configuration approximating the target
geometry. Particle-environment interactions are simulated by stochastic
perturbations and the reward mechanism, which drive particles towards high
density regions while maintaining diversity (e.g. preventing from collapsing
into clusters). R-ParVI offers fast, flexible, scalable and stochastic sampling
and inference for a class of probabilistic models such as those encountered in
Bayesian inference and generative modelling.; 48) EVE: Towards End-to-End Video Subtitle Extraction with Vision-Language
  Models; The advent of Large Vision-Language Models (LVLMs) has advanced the
video-based tasks, such as video captioning and video understanding. Some
previous research indicates that taking texts in videos as input can further
improve the performance of video understanding. As a type of indispensable
information in short videos or movies, subtitles can assist LVLMs to better
understand videos. Most existing methods for video subtitle extraction are
based on a multi-stage framework, handling each frame independently. They can
hardly exploit the temporal information of videos. Although some LVLMs exhibit
the robust OCR capability, predicting accurate timestamps for subtitle texts is
still challenging. In this paper, we propose an End-to-end Video Subtitle
Extraction method, called EVE, which consists of three modules: a vision
encoder, an adapter module, and a large language model. To effectively compress
the visual tokens from the vision encoder, we propose a novel adapter
InterleavedVT to interleave two modalities. It contains a visual compressor and
a textual region compressor. The proposed InterleavedVT exploits both the
merits of average pooling and Q-Former in token compression. Taking the
temporal information of videos into account, we introduce a sliding-window
mechanism in the textual region compressor. To benchmark the video subtitle
extraction task, we propose a large dataset ViSa including 2.5M videos.
Extensive experiments on ViSa demonstrate that the proposed EVE can outperform
existing open-sourced tools and LVLMs.; 49) Fast, accurate, and predictive method for atom detection in
  site-resolved images of microtrap arrays; We introduce a new method, rooted in estimation theory, to detect the
individual atoms in site-resolved images of microtrap arrays, such as optical
lattices or optical tweezers arrays. Using simulated images, we demonstrate a
ten-fold reduction of the detection error rate compared to the popular method
based on Wiener deconvolution, under a wide range of experimental conditions.
The runtime is fully compatible with real-time applications, even for a very
large arrays. Finally, we propose a rigorous definition for the signal-to-noise
ratio of an image, and show that it can be used as a predictor for the
detection error rate, which opens new prospect for the design of future
experiments.; 50) Test-Time Modality Generalization for Medical Image Segmentation; Generalizable medical image segmentation is essential for ensuring consistent
performance across diverse unseen clinical settings. However, existing methods
often overlook the capability to generalize effectively across arbitrary unseen
modalities. In this paper, we introduce a novel Test-Time Modality
Generalization (TTMG) framework, which comprises two core components:
Modality-Aware Style Projection (MASP) and Modality-Sensitive Instance
Whitening (MSIW), designed to enhance generalization in arbitrary unseen
modality datasets. The MASP estimates the likelihood of a test instance
belonging to each seen modality and maps it onto a distribution using
modality-specific style bases, guiding its projection effectively. Furthermore,
as high feature covariance hinders generalization to unseen modalities, the
MSIW is applied during training to selectively suppress modality-sensitive
information while retaining modality-invariant features. By integrating MASP
and MSIW, the TTMG framework demonstrates robust generalization capabilities
for medical image segmentation in unseen modalities a challenge that current
methods have largely neglected. We evaluated TTMG alongside other domain
generalization techniques across eleven datasets spanning four modalities
(colonoscopy, ultrasound, dermoscopy, and radiology), consistently achieving
superior segmentation performance across various modality combinations.; 51) Many-Task Federated Fine-Tuning via Unified Task Vectors; Federated Learning (FL) traditionally assumes homogeneous client tasks;
however, in real-world scenarios, clients often specialize in diverse tasks,
introducing task heterogeneity. To address this challenge, Many-Task FL
(MaT-FL) has emerged, enabling clients to collaborate effectively despite task
diversity. Existing MaT-FL approaches rely on client grouping or personalized
layers, requiring the server to manage individual models and failing to account
for clients handling multiple tasks. We propose MaTU, a MaT-FL approach that
enables joint learning of task vectors across clients, eliminating the need for
clustering or client-specific weight storage at the server. Our method
introduces a novel aggregation mechanism that determines task similarity based
on the direction of clients task vectors and constructs a unified task vector
encapsulating all tasks. To address task-specific requirements, we augment the
unified task vector with lightweight modulators that facilitate knowledge
transfer among related tasks while disentangling dissimilar ones. Evaluated
across 30 datasets, MaTU achieves superior performance over state-of-the-art
MaT-FL approaches, with results comparable to per-task fine-tuning, while
delivering significant communication savings.; 52) Lacunary elliptic maximal operator on the Heisenberg group; In this paper, we prove \( L^p \) boundedness results for lacunary elliptic
maximal operators on the Heisenberg group. Furthermore, we extend these \( L^p
\) estimates from skew-symmetric matrices, which naturally arise in Heisenberg
group operations, to arbitrary matrices \( A \), investigating how the
curvature induced by \( A \) governs the \( L^p \) boundedness of lacunary
circular and elliptic maximal operators. Specifically, we provide necessary and
sufficient conditions on \( A \) that determine whether these operators are
bounded or unbounded on \( L^p \).; 53) Cognitive-Aligned Document Selection for Retrieval-augmented Generation; Large language models (LLMs) inherently display hallucinations since the
precision of generated texts cannot be guaranteed purely by the parametric
knowledge they include. Although retrieval-augmented generation (RAG) systems
enhance the accuracy and reliability of generative models by incorporating
external documents, these retrieved documents often fail to adequately support
the model's responses in practical applications. To address this issue, we
propose GGatrieval (Fine-\textbf{G}rained \textbf{G}rounded \textbf{A}lignment
Re\textbf{trieval} for verifiable generation), which leverages an LLM to
dynamically update queries and filter high-quality, reliable retrieval
documents. Specifically, we parse the user query into its syntactic components
and perform fine-grained grounded alignment with the retrieved documents. For
query components that cannot be individually aligned, we propose a dynamic
semantic compensation mechanism that iteratively refines and rewrites the query
while continuously updating the retrieval results. This iterative process
continues until the retrieved documents sufficiently support the query's
response. Our approach introduces a novel criterion for filtering retrieved
documents, closely emulating human strategies for acquiring targeted
information. This ensures that the retrieved content effectively supports and
verifies the generated outputs. On the ALCE benchmark, our method significantly
surpasses a wide range of baselines, achieving state-of-the-art performance.; 54) Energy Landscape Shaping for Robust Control of Atoms in Optical Lattices; Robust quantum control is crucial for realizing practical quantum
technologies. Energy landscape shaping offers an alternative to conventional
dynamic control, providing theoretically enhanced robustness and simplifying
implementation for certain applications. This work demonstrates the feasibility
of robust energy landscape control in a practical implementation with ultracold
atoms. We leverage a digital mirror device (DMD) to shape optical potentials,
creating complex energy landscapes. To achieve a desired objective, such as
efficient quantum state transfer, we formulate a novel hybrid optimization
approach that effectively handles both continuous (laser power) and discrete
(DMD pixel activation) control parameters. This approach combines constrained
quasi-Newton methods with surrogate models for efficient exploration of the
vast parameter space. Furthermore, we introduce a framework for analyzing the
robustness of the resulting control schemes against experimental uncertainties.
By modeling uncertainties as structured perturbations, we systematically assess
controller performance and identify robust solutions. We apply these techniques
to maximize spin transfer in a chain of trapped atoms, achieving high-fidelity
control while maintaining robustness. Our findings provide insights into the
experimental viability of controlled spin transfer in cold atom systems. More
broadly, the presented optimization and robustness analysis methods apply to a
wide range of quantum control problems, offering a toolkit for designing and
evaluating robust controllers in complex experimental settings.; 55) Para-Lane: Multi-Lane Dataset Registering Parallel Scans for
  Benchmarking Novel View Synthesis; To evaluate end-to-end autonomous driving systems, a simulation environment
based on Novel View Synthesis (NVS) techniques is essential, which synthesizes
photo-realistic images and point clouds from previously recorded sequences
under new vehicle poses, particularly in cross-lane scenarios. Therefore, the
development of a multi-lane dataset and benchmark is necessary. While recent
synthetic scene-based NVS datasets have been prepared for cross-lane
benchmarking, they still lack the realism of captured images and point clouds.
To further assess the performance of existing methods based on NeRF and 3DGS,
we present the first multi-lane dataset registering parallel scans specifically
for novel driving view synthesis dataset derived from real-world scans,
comprising 25 groups of associated sequences, including 16,000 front-view
images, 64,000 surround-view images, and 16,000 LiDAR frames. All frames are
labeled to differentiate moving objects from static elements. Using this
dataset, we evaluate the performance of existing approaches in various testing
scenarios at different lanes and distances. Additionally, our method provides
the solution for solving and assessing the quality of multi-sensor poses for
multi-modal data alignment for curating such a dataset in real-world. We plan
to continually add new sequences to test the generalization of existing methods
across different scenarios. The dataset is released publicly at the project
page: https://nizqleo.github.io/paralane-dataset/.; 56) VFX Creator: Animated Visual Effect Generation with Controllable
  Diffusion Transformer; Crafting magic and illusions is one of the most thrilling aspects of
filmmaking, with visual effects (VFX) serving as the powerhouse behind
unforgettable cinematic experiences. While recent advances in generative
artificial intelligence have driven progress in generic image and video
synthesis, the domain of controllable VFX generation remains relatively
underexplored. In this work, we propose a novel paradigm for animated VFX
generation as image animation, where dynamic effects are generated from
user-friendly textual descriptions and static reference images. Our work makes
two primary contributions: (i) Open-VFX, the first high-quality VFX video
dataset spanning 15 diverse effect categories, annotated with textual
descriptions, instance segmentation masks for spatial conditioning, and
start-end timestamps for temporal control. (ii) VFX Creator, a simple yet
effective controllable VFX generation framework based on a Video Diffusion
Transformer. The model incorporates a spatial and temporal controllable LoRA
adapter, requiring minimal training videos. Specifically, a plug-and-play mask
control module enables instance-level spatial manipulation, while tokenized
start-end motion timestamps embedded in the diffusion process, alongside the
text encoder, allow precise temporal control over effect timing and pace.
Extensive experiments on the Open-VFX test set demonstrate the superiority of
the proposed system in generating realistic and dynamic effects, achieving
state-of-the-art performance and generalization ability in both spatial and
temporal controllability. Furthermore, we introduce a specialized metric to
evaluate the precision of temporal control. By bridging traditional VFX
techniques with generative approaches, VFX Creator unlocks new possibilities
for efficient and high-quality video effect generation, making advanced VFX
accessible to a broader audience.; 57) A Data-Driven Paradigm-Based Image Denoising and Mosaicking Approach for
  High-Resolution Acoustic Camera; In this work, an approach based on a data-driven paradigm to denoise and
mosaic acoustic camera images is proposed. Acoustic cameras, also known as 2D
forward-looking sonar, could collect high-resolution acoustic images in dark
and turbid water. However, due to the unique sensor imaging mechanism, main
vision-based processing methods, like image denoising and mosaicking are still
in the early stages. Due to the complex noise interference in acoustic images
and the narrow field of view of acoustic cameras, it is difficult to restore
the entire detection scene even if enough acoustic images are collected.
Relevant research work addressing these issues focuses on the design of
handcrafted operators for acoustic image processing based on prior knowledge
and sensor models. However, such methods lack robustness due to noise
interference and insufficient feature details on acoustic images. This study
proposes an acoustic image denoising and mosaicking method based on a
data-driven paradigm and conducts experimental testing using collected acoustic
camera images. The results demonstrate the effectiveness of the proposal.; 58) On $G^p$-unimodality of radius functions in graphs: structure and
  algorithms; For every weight assignment $\pi$ to the vertices in a graph $G$, the radius
function $r_\pi$ maps every vertex of $G$ to its largest weighted distance to
the other vertices. The center problem asks to find a center, i.e., a vertex of
$G$ that minimizes $r_\pi$. We here study some local properties of radius
functions in graphs, and their algorithmic implications; our work is inspired
by the nice property that in Euclidean spaces every local minimum of every
radius function $r_\pi$ is a center. We study a discrete analogue of this
property for graphs, which we name $G^p$-unimodality: specifically, every
vertex that minimizes the radius function in its ball of radius $p$ must be a
central vertex. While it has long been known since Dragan (1989) that graphs
with $G$-unimodal radius functions $r_\pi$ are exactly the Helly graphs, the
class of graphs with $G^2$-unimodal radius functions has not been studied
insofar. We prove the latter class to be much larger than the Helly graphs,
since it also comprises (weakly) bridged graphs, graphs with convex balls, and
bipartite Helly graphs.
  Recently, using the $G$-unimodality of radius functions $r_\pi$, a randomized
$\widetilde{\mathcal{O}}(\sqrt{n}m)$-time local search algorithm for the center
problem on Helly graphs was proposed by Ducoffe (2023). Assuming the Hitting
Set Conjecture (Abboud et al., 2016), we prove that a similar result for the
class of graphs with $G^2$-unimodal radius functions is unlikely. However, we
design local search algorithms (randomized or deterministic) for the center
problem on many of its important subclasses.; 59) MaTVLM: Hybrid Mamba-Transformer for Efficient Vision-Language Modeling; With the advancement of RNN models with linear complexity, the quadratic
complexity challenge of transformers has the potential to be overcome. Notably,
the emerging Mamba-2 has demonstrated competitive performance, bridging the gap
between RNN models and transformers. However, due to sequential processing and
vanishing gradients, RNN models struggle to capture long-range dependencies,
limiting contextual understanding. This results in slow convergence, high
resource demands, and poor performance on downstream understanding and complex
reasoning tasks. In this work, we present a hybrid model MaTVLM by substituting
a portion of the transformer decoder layers in a pre-trained VLM with Mamba-2
layers. Leveraging the inherent relationship between attention and Mamba-2, we
initialize Mamba-2 with corresponding attention weights to accelerate
convergence. Subsequently, we employ a single-stage distillation process, using
the pre-trained VLM as the teacher model to transfer knowledge to the MaTVLM,
further enhancing convergence speed and performance. Furthermore, we
investigate the impact of differential distillation loss within our training
framework. We evaluate the MaTVLM on multiple benchmarks, demonstrating
competitive performance against the teacher model and existing VLMs while
surpassing both Mamba-based VLMs and models of comparable parameter scales.
Remarkably, the MaTVLM achieves up to 3.6x faster inference than the teacher
model while reducing GPU memory consumption by 27.5%, all without compromising
performance. Code and models are released at http://github.com/hustvl/MaTVLM.; 60) Unit Region Encoding: A Unified and Compact Geometry-aware
  Representation for Floorplan Applications; We present the Unit Region Encoding of floorplans, which is a unified and
compact geometry-aware encoding representation for various applications,
ranging from interior space planning, floorplan metric learning to floorplan
generation tasks. The floorplans are represented as the latent encodings on a
set of boundary-adaptive unit region partition based on the clustering of the
proposed geometry-aware density map. The latent encodings are extracted by a
trained network (URE-Net) from the input dense density map and other available
semantic maps. Compared to the over-segmented rasterized images and the
room-level graph structures, our representation can be flexibly adapted to
different applications with the sliced unit regions while achieving higher
accuracy performance and better visual quality. We conduct a variety of
experiments and compare to the state-of-the-art methods on the aforementioned
applications to validate the superiority of our representation, as well as
extensive ablation studies to demonstrate the effect of our slicing choices.; 61) Simple $3$-designs of $\mathrm{PSL}(2,2^n)$ with block size $13$; This paper investigates simple $3$-$(2^n+1,13,\lambda)$ designs admitting
$\mathrm{PSL}$$(2,2^n)$ as an automorphism group. We determine all possible
values of $\lambda$ by systematically analyzing the orbits of $13$-element
subsets under the action of $\mathrm{PSL}$$(2, 2^n)$ on the projective line.
While previous research has explored this topic by analyzing the structure of
$k$-element subsets $B$ directly, we approach the problem using group theory
and the Cauchy-Frobenius-Burnside lemma. This method provides an efficient
framework that can be applied to larger block sizes where traditional
enumeration methods become computationally infeasible.; 62) Towards Resilient and Sustainable Global Industrial Systems: An
  Evolutionary-Based Approach; This paper presents a new complex optimization problem in the field of
automatic design of advanced industrial systems and proposes a hybrid
optimization approach to solve the problem. The problem is multi-objective as
it aims at finding solutions that minimize CO2 emissions, transportation time,
and costs. The optimization approach combines an evolutionary algorithm and
classical mathematical programming to design resilient and sustainable global
manufacturing networks. Further, it makes use of the OWL ontology for data
consistency and constraint management. The experimental validation demonstrates
the effectiveness of the approach in both single and double sourcing scenarios.
The proposed methodology, in general, can be applied to any industry case with
complex manufacturing and supply chain challenges.; 63) Membrane phononic crystals for high-Qm mechanical defect modes in
  piezoelectric aluminum nitride; Nanomechanical resonators with exceptionally low dissipation are advancing
mechanics-based sensors and quantum technologies. The key for these advances is
the engineering of localized phononic modes that are well-isolated from the
environment, i.e., that exhibit a high mechanical quality factor, Qm. Membrane
phononic crystals fabricated from strained thin films can realize high-Qm
single or multiple localized phononic defect modes. These defect modes can be
efficiently interfaced with out-of-plane light or capacitively coupled to a
microwave quantum circuit, enabling readout and control of their motion. When
membrane phononic crystals are fabricated from a crystalline film, they could
offer built-in functionality. We demonstrate a membrane phononic crystal
realized in a strained 90 nm-thin film of aluminum nitride (AlN), which is a
crystalline piezoelectric material. We engineer a high-Qm localized phononic
defect mode at 1.8 MHz with a Qxf-product of 1.5 10^13 Hz at room temperature.
In future devices, the built-in piezoelectricity of AlN can be utilized for
in-situ tuning of mechanical mode frequencies, defect mode couplings, or
acoustic bandgaps, which can be used as building blocks of tunable phononic
circuits.; 64) Effects of Ru-doping on the magnetism of Ag3LiIr2O6, a candidate Kitaev
  quantum spin liquid; We report our investigations on Ag3LiIr1.4Ru0.6O6, which results from the Ru
substitution in the Kitaev quantum spin liquid candidate Ag3LiIr2O6. It
crystallizes in the monoclinic C2/m space group like its parent compound,
Ag3LiIr2O6. Our susceptibility measurements reveal an effective moment = 2.6
muB, which is higher than the moments of the parent compound and less than that
of the Ru-analog (Ag3LiRu2O6), suggesting the presence of magnetic Ir4+ (Jeff=
1/2) and Ru4+ (S=1). Bulk magnetic susceptibility suggests long-range order
(LRO)at T~20 K, whereas no clear signature is present in the heat capacity.
Likewise, there is a loss of the 7Li NMR spectral intensity around T~20 K as
expected at the onset of LRO, but a complete wipe-out is not seen in contrast
to the result in Ag3LiIr2O6. There is also a T~20 K anomaly in the 7Li NMR
relaxation rate and also a fall in the 7Li NMR shift with decreasing
temperature. These results suggest LRO at T~20 K in Ag3LiIr1.4Ru0.6O6. However,
at low-T below 10 K, we observe a power law variation in magnetic heat capacity
and spin lattice relaxation rate, temperature-independent-7K, and no further
loss of the 7Li NMR spectral intensity. These results might suggest the
persistence or stabilisation of a quantum spin liquid-like phase, perhaps from
a fraction of the sample in Ag3LiIr1.4Ru0.6O6 below 10 K. Our muon spin
relaxation measurements suggest ordering around 20 K, consistent with our other
probes. It appears that the main effect of Ru-substitution is to shift the LRO
to a higher temperature in comparison with Ag3LiIr2O6, though there are
signatures of a novel phase below about 10 K.; 65) Inferring the cosmological constant in early Universe only by
  gravitational waves; The expansion of the Universe is accelerating which can be interpreted as due
to the cosmological constant $\Lambda$. In this study, we investigate the
behavior of gravitational waves in the presence of a cosmological constant in
the early universe. We rigorously analyze the merger rate of binary primordial
black holes (PBHs) and the corresponding signal-to-noise ratio within the
framework of Laser Interferometer Space Antenna (LISA). We find that binary
PBHs with a total mass of $M_{\mathrm{tot}}=1000M_{\odot}$ and a redshift
larger than $z=500$ are the ideal system for studying the effect of the
cosmological constant through LISA. By computing the fisher information matrix,
we establish that the cosmological constant can be effectively constrained.; 66) PhysicsGen: Can Generative Models Learn from Images to Predict Complex
  Physical Relations?; The image-to-image translation abilities of generative learning models have
recently made significant progress in the estimation of complex (steered)
mappings between image distributions. While appearance based tasks like image
in-painting or style transfer have been studied at length, we propose to
investigate the potential of generative models in the context of physical
simulations. Providing a dataset of 300k image-pairs and baseline evaluations
for three different physical simulation tasks, we propose a benchmark to
investigate the following research questions: i) are generative models able to
learn complex physical relations from input-output image pairs? ii) what
speedups can be achieved by replacing differential equation based simulations?
While baseline evaluations of different current models show the potential for
high speedups (ii), these results also show strong limitations toward the
physical correctness (i). This underlines the need for new methods to enforce
physical correctness. Data, baseline models and evaluation code
http://www.physics-gen.org.; 67) Teach-to-Reason with Scoring: Self-Explainable Rationale-Driven
  Multi-Trait Essay Scoring; Multi-trait automated essay scoring (AES) systems provide a fine-grained
evaluation of an essay's diverse aspects. While they excel in scoring, prior
systems fail to explain why specific trait scores are assigned. This lack of
transparency leaves instructors and learners unconvinced of the AES outputs,
hindering their practical use. To address this, we propose a self-explainable
Rationale-Driven Multi-trait automated Essay scoring (RaDME) framework. RaDME
leverages the reasoning capabilities of large language models (LLMs) by
distilling them into a smaller yet effective scorer. This more manageable
student model is optimized to sequentially generate a trait score followed by
the corresponding rationale, thereby inherently learning to select a more
justifiable score by considering the subsequent rationale during training. Our
findings indicate that while LLMs underperform in direct AES tasks, they excel
in rationale generation when provided with precise numerical scores. Thus,
RaDME integrates the superior reasoning capacities of LLMs into the robust
scoring accuracy of an optimized smaller model. Extensive experiments
demonstrate that RaDME achieves both accurate and adequate reasoning while
supporting high-quality multi-trait scoring, significantly enhancing the
transparency of AES.; 68) Comparison of tuples of operators and its components; For tuples of compact operators $\mathcal{T}=(T_1,\ldots, T_d)$ and
$\mathcal{S}=(S_1,$ $\ldots,S_d)$ on Banach spaces over a field $\mathbb{F}$,
considering the joint $p$-operator norms on the tuples, we study
$dist(\mathcal{T},\mathbb{F}^d\mathcal{S}),$ the distance of $\mathcal{T}$ from
the $d$-dimensional subspace
$\mathcal{F}^d\mathcal{S}:=\{\textbf{z}\mathcal{S}:\textbf{z}\in
\mathbb{F}^d\}.$ We obtain a relation between
$dist(\mathcal{T},\mathbb{F}^d\mathcal{S})$ and $dist(T_i,\mathbb{F}S_i),$ for
$1\leq i\leq d.$ We prove that if $p=\infty,$ then
$dist(\mathcal{T},\mathbb{F}^d\mathcal{S})=\underset{1\leq i\leq
d}{\max}dist(T_i,\mathbb{F}S_i),$ and for $1\leq p<\infty,$ under a sufficient
condition, $dist(\mathcal{T},\mathbb{F}^d\mathcal{S})^p=\underset{1\leq i\leq
d}{\sum}dist(T_i,\mathbb{F}S_i)^p.$ As a consequence, we deduce the equivalence
of Birkhoff-James orthogonality, $\mathcal{T}\perp_B \mathbb{F}^d\mathcal{S}
\Leftrightarrow T_i\perp_B S_i,$ under a sufficient condition. Furthermore, we
explore the relation of one sided Gateaux derivatives of $\mathcal{T}$ in the
direction of $\mathcal{S}$ with that of $T_i$ in the direction of $S_i.$
Applying this, we explore the relation between the smoothness of $\mathcal{T}$
and $T_i.$ By identifying an operator, whose range is $\ell_\infty^d,$ as a
tuple of functionals, we effectively use the results obtained here for
operators whose range is $\ell_\infty^d$ and deduce nice results involving
functionals. It provides another perspective of studying tuples of operators.; 69) Frequency Matters: Explaining Biases of Face Recognition in the
  Frequency Domain; Face recognition (FR) models are vulnerable to performance variations across
demographic groups. The causes for these performance differences are unclear
due to the highly complex deep learning-based structure of face recognition
models. Several works aimed at exploring possible roots of gender and ethnicity
bias, identifying semantic reasons such as hairstyle, make-up, or facial hair
as possible sources. Motivated by recent discoveries of the importance of
frequency patterns in convolutional neural networks, we explain bias in face
recognition using state-of-the-art frequency-based explanations. Our extensive
results show that different frequencies are important to FR models depending on
the ethnicity of the samples.; 70) Sports and Women's Sports: Gender Bias in Text Generation with Olympic
  Data; Large Language Models (LLMs) have been shown to be biased in prior work, as
they generate text that is in line with stereotypical views of the world or
that is not representative of the viewpoints and values of historically
marginalized demographic groups. In this work, we propose using data from
parallel men's and women's events at the Olympic Games to investigate different
forms of gender bias in language models. We define three metrics to measure
bias, and find that models are consistently biased against women when the
gender is ambiguous in the prompt. In this case, the model frequently retrieves
only the results of the men's event with or without acknowledging them as such,
revealing pervasive gender bias in LLMs in the context of athletics.; 71) Free-Lunch Color-Texture Disentanglement for Stylized Image Generation; Recent advances in Text-to-Image (T2I) diffusion models have transformed
image generation, enabling significant progress in stylized generation using
only a few style reference images. However, current diffusion-based methods
struggle with fine-grained style customization due to challenges in controlling
multiple style attributes, such as color and texture. This paper introduces the
first tuning-free approach to achieve free-lunch color-texture disentanglement
in stylized T2I generation, addressing the need for independently controlled
style elements for the Disentangled Stylized Image Generation (DisIG) problem.
Our approach leverages the Image-Prompt Additivity property in the CLIP image
embedding space to develop techniques for separating and extracting
Color-Texture Embeddings (CTE) from individual color and texture reference
images. To ensure that the color palette of the generated image aligns closely
with the color reference, we apply a whitening and coloring transformation to
enhance color consistency. Additionally, to prevent texture loss due to the
signal-leak bias inherent in diffusion training, we introduce a noise term that
preserves textural fidelity during the Regularized Whitening and Coloring
Transformation (RegWCT). Through these methods, our Style Attributes
Disentanglement approach (SADis) delivers a more precise and customizable
solution for stylized image generation. Experiments on images from the WikiArt
and StyleDrop datasets demonstrate that, both qualitatively and quantitatively,
SADis surpasses state-of-the-art stylization methods in the DisIG task.Code
will be released at https://deepffff.github.io/sadis.github.io/.; 72) When the Future Becomes the Past: Taming Temporal Correspondence for
  Self-supervised Video Representation Learning; The past decade has witnessed notable achievements in self-supervised
learning for video tasks. Recent efforts typically adopt the Masked Video
Modeling (MVM) paradigm, leading to significant progress on multiple video
tasks. However, two critical challenges remain: 1) Without human annotations,
the random temporal sampling introduces uncertainty, increasing the difficulty
of model training. 2) Previous MVM methods primarily recover the masked patches
in the pixel space, leading to insufficient information compression for
downstream tasks. To address these challenges jointly, we propose a
self-supervised framework that leverages Temporal Correspondence for video
Representation learning (T-CoRe). For challenge 1), we propose a sandwich
sampling strategy that selects two auxiliary frames to reduce reconstruction
uncertainty in a two-side-squeezing manner. Addressing challenge 2), we
introduce an auxiliary branch into a self-distillation architecture to restore
representations in the latent space, generating high-level semantic
representations enriched with temporal information. Experiments of T-CoRe
consistently present superior performance across several downstream tasks,
demonstrating its effectiveness for video representation learning. The code is
available at https://github.com/yafeng19/T-CORE.; 73) Systemic Risk Management via Maximum Independent Set in Extremal
  Dependence Networks; The failure of key financial institutions may accelerate risk contagion due
to their interconnections within the system. In this paper, we propose a robust
portfolio strategy to mitigate systemic risks during extreme events. We use the
stock returns of key financial institutions as an indicator of their
performance, apply extreme value theory to assess the extremal dependence among
stocks of financial institutions, and construct a network model based on a
threshold approach that captures extremal dependence. Our analysis reveals
different dependence structures in the Chinese and U.S. financial systems. By
applying the maximum independent set (MIS) from graph theory, we identify a
subset of institutions with minimal extremal dependence, facilitating the
construction of diversified portfolios resilient to risk contagion. We also
compare the performance of our proposed portfolios with that of the market
portfolios in the two economies.; 74) SRMT: Shared Memory for Multi-agent Lifelong Pathfinding; Multi-agent reinforcement learning (MARL) demonstrates significant progress
in solving cooperative and competitive multi-agent problems in various
environments. One of the principal challenges in MARL is the need for explicit
prediction of the agents' behavior to achieve cooperation. To resolve this
issue, we propose the Shared Recurrent Memory Transformer (SRMT) which extends
memory transformers to multi-agent settings by pooling and globally
broadcasting individual working memories, enabling agents to exchange
information implicitly and coordinate their actions. We evaluate SRMT on the
Partially Observable Multi-Agent Pathfinding problem in a toy Bottleneck
navigation task that requires agents to pass through a narrow corridor and on a
POGEMA benchmark set of tasks. In the Bottleneck task, SRMT consistently
outperforms a variety of reinforcement learning baselines, especially under
sparse rewards, and generalizes effectively to longer corridors than those seen
during training. On POGEMA maps, including Mazes, Random, and MovingAI, SRMT is
competitive with recent MARL, hybrid, and planning-based algorithms. These
results suggest that incorporating shared recurrent memory into the
transformer-based architectures can enhance coordination in decentralized
multi-agent systems. The source code for training and evaluation is available
on GitHub: https://github.com/Aloriosa/srmt.; 75) Emergent turbulence and coarsening arrest in active-spinner fluids; We uncover activity-driven crossover from phase separation to a new turbulent
state in a two-dimensional system of counter-rotating spinners. We study the
statistical properties of this active-rotor turbulence using the active-rotor
Cahn-Hilliard-Navier-Stokes model, and show that the vorticity $\omega \propto
\phi$, the scalar field that distinguishes regions with different rotating
states. We explain this intriguing proportionality theoretically, and we
characterize power-law energy and concentration spectra, intermittency, and
flow-topology statistics. We suggest biological implications of such
turbulence.; 76) Efficient Finetuning for Dimensional Speech Emotion Recognition in the
  Age of Transformers; Accurate speech emotion recognition is essential for developing human-facing
systems. Recent advancements have included finetuning large, pretrained
transformer models like Wav2Vec 2.0. However, the finetuning process requires
substantial computational resources, including high-memory GPUs and significant
processing time. As the demand for accurate emotion recognition continues to
grow, efficient finetuning approaches are needed to reduce the computational
burden. Our study focuses on dimensional emotion recognition, predicting
attributes such as activation (calm to excited) and valence (negative to
positive). We present various finetuning techniques, including full finetuning,
partial finetuning of transformer layers, finetuning with mixed precision,
partial finetuning with caching, and low-rank adaptation (LoRA) on the Wav2Vec
2.0 base model. We find that partial finetuning with mixed precision achieves
performance comparable to full finetuning while increasing training speed by
67%. Caching intermediate representations further boosts efficiency, yielding
an 88% speedup and a 71% reduction in learnable parameters. We recommend
finetuning the final three transformer layers in mixed precision to balance
performance and training efficiency, and adding intermediate representation
caching for optimal speed with minimal performance trade-offs. These findings
lower the barriers to finetuning speech emotion recognition systems, making
accurate emotion recognition more accessible to a broader range of researchers
and practitioners.; 77) Large-scale Pre-training for Grounded Video Caption Generation; We propose a novel approach for captioning and object grounding in video,
where the objects in the caption are grounded in the video via temporally dense
bounding boxes. We introduce the following contributions. First, we present a
large-scale automatic annotation method that aggregates captions grounded with
bounding boxes across individual frames into temporally dense and consistent
bounding box annotations. We apply this approach on the HowTo100M dataset to
construct a large-scale pre-training dataset, named HowToGround1M. We also
introduce a Grounded Video Caption Generation model, dubbed GROVE, and
pre-train the model on HowToGround1M. Second, we introduce a new dataset,
called iGround, of 3500 videos with manually annotated captions and dense
spatio-temporally grounded bounding boxes. This allows us to measure progress
on this challenging problem, as well as to fine-tune our model on this
small-scale but high-quality data. Third, we demonstrate that our approach
achieves state-of-the-art results on the proposed iGround dataset compared to a
number of baselines, as well as on the VidSTG and ActivityNet-Entities
datasets. We perform extensive ablations that demonstrate the importance of
pre-training using our automatically annotated HowToGround1M dataset followed
by fine-tuning on the manually annotated iGround dataset and validate the key
technical contributions of our model.; 78) Optimal Fiducial Marker Placement for Satellite Proximity Operations
  Using Observability Gramians; This paper investigates optimal fiducial marker placement on the surface of a
satellite performing relative proximity operations with an observer satellite.
The absolute and relative translation and attitude equations of motion for the
satellite pair are modeled using dual quaternions. The observability of the
relative dual quaternion system is analyzed using empirical observability
Gramian methods. The optimal placement of a fiducial marker set, in which each
marker gives simultaneous optical range and attitude measurements, is
determined for the pair of satellites. A geostationary flyby between the
observing body (chaser) and desired (target) satellites is numerically
simulated and the optimal fiducial placement sets of five and ten on the
surface of the desired satellite are solved. It is shown that the optimal
solution maximizes the distance between fiducial markers and selects marker
locations that are most sensitive to measuring changes in the state during the
nonlinear trajectory, despite being visible for less time than other candidate
marker locations. Definitions and properties of quaternions and dual
quaternions, and parallels between the two, are presented alongside the
relative motion model.; 79) GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models; In recent years, 2D Vision-Language Models (VLMs) have made significant
strides in image-text understanding tasks. However, their performance in 3D
spatial comprehension, which is critical for embodied intelligence, remains
limited. Recent advances have leveraged 3D point clouds and multi-view images
as inputs, yielding promising results. However, we propose exploring a purely
vision-based solution inspired by human perception, which merely relies on
visual cues for 3D spatial understanding. This paper empirically investigates
the limitations of VLMs in 3D spatial knowledge, revealing that their primary
shortcoming lies in the lack of global-local correspondence between the scene
and individual frames. To address this, we introduce GPT4Scene, a novel visual
prompting paradigm in VLM training and inference that helps build the
global-local relationship, significantly improving the 3D spatial understanding
of indoor scenes. Specifically, GPT4Scene constructs a Bird's Eye View (BEV)
image from the video and marks consistent object IDs across both frames and the
BEV image. The model then inputs the concatenated BEV image and video frames
with markers. In zero-shot evaluations, GPT4Scene improves performance over
closed-source VLMs like GPT-4o. Additionally, we prepare a processed video
dataset consisting of 165K text annotation to fine-tune open-source VLMs,
achieving state-of-the-art performance on all 3D understanding tasks.
Surprisingly, after training with the GPT4Scene paradigm, VLMs consistently
improve during inference, even without object marker prompting and BEV image as
explicit correspondence. It demonstrates that the proposed paradigm helps VLMs
develop an intrinsic ability to understand 3D scenes, which paves the way for a
seamless approach to extending pre-trained VLMs for 3D scene understanding.; 80) MGSR: 2D/3D Mutual-boosted Gaussian Splatting for High-fidelity Surface
  Reconstruction under Various Light Conditions; Novel view synthesis (NVS) and surface reconstruction (SR) are essential
tasks in 3D Gaussian Splatting (3D-GS). Despite recent progress, these tasks
are often addressed independently, with GS-based rendering methods struggling
under diverse light conditions and failing to produce accurate surfaces, while
GS-based reconstruction methods frequently compromise rendering quality. This
raises a central question: must rendering and reconstruction always involve a
trade-off? To address this, we propose MGSR, a 2D/3D Mutual-boosted Gaussian
splatting for Surface Reconstruction that enhances both rendering quality and
3D reconstruction accuracy. MGSR introduces two branches--one based on 2D-GS
and the other on 3D-GS. The 2D-GS branch excels in surface reconstruction,
providing precise geometry information to the 3D-GS branch. Leveraging this
geometry, the 3D-GS branch employs a geometry-guided illumination decomposition
module that captures reflected and transmitted components, enabling realistic
rendering under varied light conditions. Using the transmitted component as
supervision, the 2D-GS branch also achieves high-fidelity surface
reconstruction. Throughout the optimization process, the 2D-GS and 3D-GS
branches undergo alternating optimization, providing mutual supervision. Prior
to this, each branch completes an independent warm-up phase, with an early
stopping strategy implemented to reduce computational costs. We evaluate MGSR
on a diverse set of synthetic and real-world datasets, at both object and scene
levels, demonstrating strong performance in rendering and surface
reconstruction.; 81) Low-Rank Adapting Models for Sparse Autoencoders; Sparse autoencoders (SAEs) decompose language model representations into a
sparse set of linear latent vectors. Recent works have improved SAEs using
language model gradients, but these techniques require many expensive backward
passes during training and still cause a significant increase in cross entropy
loss when SAE reconstructions are inserted into the model. In this work, we
improve on these limitations by taking a fundamentally different approach: we
use low-rank adaptation (LoRA) to finetune the language model itself around a
previously trained SAE. We analyze our method across SAE sparsity, SAE width,
language model size, LoRA rank, and model layer on the Gemma Scope family of
SAEs. In these settings, our method reduces the cross entropy loss gap by 30%
to 55% when SAEs are inserted during the forward pass. We also find that
compared to end-to-end (e2e) SAEs, our approach achieves the same downstream
cross entropy loss 3$\times$ to 20$\times$ faster on Gemma-2-2B and 2$\times$
to 10$\times$ faster on Llama-3.2-1B. We further show that our technique
improves downstream metrics and can adapt multiple SAEs at once. Our results
demonstrate that improving model interpretability is not limited to post-hoc
SAE training; Pareto improvements can also be achieved by directly optimizing
the model itself.; 82) Multimode ringdown modelling with $\texttt{qnmfits}$ and
  $\texttt{KerrRingdown}$; In the last decade, the ringdown community has made large strides in
understanding the aftermath of binary black hole mergers through the study of
numerical simulations. In this note, we introduce two flavors of fitting
algorithms, that have been verified against each other, for the extraction of
quasinormal mode amplitudes from ringdown waveforms - $\texttt{qnmfits}$ in
Python and $\texttt{KerrRingdown}$ in Mathematica.; 83) Ultrafast pulsed laser evaluation of Single Event Transients in
  opto-couplers; We build a 1064 nm fiber laser system-based testing facility for emulating
SETs in different electronics components and ICs. Using these facilities, we
tested the 4N35 optocoupler to observe SETs for the first time.; 84) Nonlinear Optimal Guidance for Impact Time Control with Field-of-View
  Constraint; An optimal guidance law for impact time control with field-of-view constraint
is presented. The guidance law is derived by first converting the
inequality-constrained nonlinear optimal control problem into an
equality-constrained one through a saturation function. Based on Pontryagin's
maximum principle, a parameterized system satisfying the necessary optimality
conditions is established. By propagating this system, a large number of
extremal trajectories can be efficiently generated. These trajectories are then
used to train a neural network that maps the current state and time-to-go to
the optimal guidance command. The trained neural network can generate optimal
commands within 0.1 milliseconds while satisfying the field-of-view constraint.
Numerical simulations demonstrate that the proposed guidance law outperforms
existing methods and achieves nearly optimal performance in terms of control
effort.; 85) Temporal variability and obscuration effects in the X-ray emission of
  classical nova V339 Delphini (Nova Delphini 2013); In this study, we present a detailed analysis of public archival soft X-ray
data on the classical nova V339 Delphini (Nova Del 2013) during its outburst,
obtained using the {\it Chandra} High-Resolution Camera Spectrometer (HRC-S)
and Low Energy Transmission Grating (LETG), as well as {\it XMM-Newton} in
2013. The observations, spanning from day 85.2 to day 112.0 after the optical
maximum, capture the nova during its luminous supersoft X-ray source (SSS)
phase. The spectra reveal numerous absorption features with blue-shifted
velocities ranging from $\sim$ 724 to $\sim$ 1474 km s$^{-1}$, with the
majority of lines blue-shifted by approximately 1200 km s$^{-1}$. We confirm
the presence of a short-period modulation of the X-ray flux with a period of
approximately 54 seconds, as well as the drift of this period, which was
detected on days 97.0 and 112.0 during the outburst with both {\it XMM-Newton}
and {\it Chandra}. This period modulation is transient in nature, with
significant variations in amplitude and pulse profile over timescales of a few
thousand seconds, likely due to temporary obscuration events that affect the
emission from the central hot source. The pulse profiles exhibit substantial
deviations from a pure sinusoidal shape, which may be related to the period
drift. Additionally, the modulation amplitude shows a possible anti-correlation
with the count rates on day 97.0, likely also caused by temporary obscuration
events influencing the central source's emission.; 86) Enhancing Facial Expression Recognition through Dual-Direction Attention
  Mixed Feature Networks and CLIP: Application to 8th ABAW Challenge; We present our contribution to the 8th ABAW challenge at CVPR 2025, where we
tackle valence-arousal estimation, emotion recognition, and facial action unit
detection as three independent challenges. Our approach leverages the
well-known Dual-Direction Attention Mixed Feature Network (DDAMFN) for all
three tasks, achieving results that surpass the proposed baselines.
Additionally, we explore the use of CLIP for the emotion recognition challenge
as an additional experiment. We provide insights into the architectural choices
that contribute to the strong performance of our methods.; 87) Hunting for heavy $Z^\prime$ with IceCube neutrinos and gravitational
  waves; In the minimal gauged B-L extension of the Standard Model, we demonstrate
that PeV-scale dark matter (DM) and the baryon asymmetry of the Universe (BAU)
can be simultaneously explained through the three right-handed neutrinos (RHNs)
present in the theory. The DM candidate undergoes decay into light neutrinos,
providing an explanation for the observed IceCube events, while the other two
RHNs generate the BAU via leptogenesis. The breaking of gauge symmetry gives
rise to detectable gravitational waves (GWs) from decaying cosmic strings (CS),
making this framework testable at several future GW detectors-despite being
beyond the reach of conventional collider experiments due to the extremely weak
coupling. The symmetry-breaking scale establishes a connection between particle
masses, couplings, and the GW spectrum, offering a unified and predictive
scenario.; 88) Chiral effective model of cold and dense two-color QCD: The linear sigma
  model approach; This review is devoted to summarizing recent developments of the linear sigma
model (LSM) in cold and dense two-color QCD (QC$_2$D), in which lattice
simulations are straightforwardly applicable thanks to the disappearance of the
sign problem. In QC$_2$D, both theoretical and numerical studies derive the
presence of the so-called baryon superfluid phase at sufficiently large
chemical potential ($\mu_q$), where diquark condensates govern the ground
state. The hadron mass spectrum simulated in this phase shows that the mass of
an iso-singlet ($I=0$) and $0^-$ state is remarkably reduced, but such a mode
cannot be described by the chiral perturbation theory. Motivated by this fact,
I invent the LSM constructed upon the linear representation of chiral symmetry,
or more precisely the Pauli-G\""ursey symmetry. Then, it is shown that my LSM
successfully reproduces the low-lying hadron mass spectrum in a broad range of
$\mu_q$ simulated on the lattice. As applications of the LSM, topological
susceptibility and sound velocity in cold and dense QC$_2$D are evaluated to
compare with lattice results. Besides, generalized Gell-Mann-Oakes-Renner
relation and hardon mass spectrum in the presence of a diquark source are
analyzed. I also introduce an extended version of the LSM incorporating
spin-$1$ hadrons.; 89) Understanding the Impact of Artificial Intelligence in Academic Writing:
  Metadata to the Rescue; This column advocates for including artificial intelligence (AI)-specific
metadata on those academic papers that are written with the help of AI in an
attempt to analyze the use of such tools for disseminating research.; 90) Advancing Oyster Phenotype Segmentation with Multi-Network Ensemble and
  Multi-Scale mechanism; Phenotype segmentation is pivotal in analysing visual features of living
organisms, enhancing our understanding of their characteristics. In the context
of oysters, meat quality assessment is paramount, focusing on shell, meat,
gonad, and muscle components. Traditional manual inspection methods are
time-consuming and subjective, prompting the adoption of machine vision
technology for efficient and objective evaluation. We explore machine vision's
capacity for segmenting oyster components, leading to the development of a
multi-network ensemble approach with a global-local hierarchical attention
mechanism. This approach integrates predictions from diverse models and
addresses challenges posed by varying scales, ensuring robust instance
segmentation across components. Finally, we provide a comprehensive evaluation
of the proposed method's performance using different real-world datasets,
highlighting its efficacy and robustness in enhancing oyster phenotype
segmentation.; 91) Data Enrichment Work and AI Labor in Latin America and the Caribbean; The global AI surge demands crowdworkers from diverse languages and cultures.
They are pivotal in labeling data for enabling global AI systems. Despite
global significance, research has primarily focused on understanding the
perspectives and experiences of US and India crowdworkers, leaving a notable
gap. To bridge this, we conducted a survey with 100 crowdworkers across 16
Latin American and Caribbean countries. We discovered that these workers
exhibited pride and respect for their digital labor, with strong support and
admiration from their families. Notably, crowd work was also seen as a stepping
stone to financial and professional independence. Surprisingly, despite wanting
more connection, these workers also felt isolated from peers and doubtful of
others' labor quality. They resisted collaboration and gender-based tools,
valuing gender-neutrality. Our work advances HCI understanding of Latin
American and Caribbean crowdwork, offering insights for digital resistance
tools for the region.; 92) Testing tidal theory using Gaia binaries: the red giant branch; Tidal interaction is a major ingredient in the theory of binary evolution.
Here, we study tidal circularization in binaries with red giant primaries. We
compute the tidal evolution for binaries as their primary stars evolve along
the red giant branch, under dissipation of dynamical tides in the convective
envelope. We then compare this evolution with a sample of $\sim30,000$ red
giant binaries reported by Gaia DR3. These binaries clearly show the expected
gradual advance of tidal circularization, as the primary expands. But some
tension with theory remains. While our calculations always predict a critical
separation for tidal circularization at about $3-4$ times the stellar radii,
binaries with less evolved giants are observed to be circularized out to about
twice as far. They also exhibit an overly extended `cool island', a collection
of circular orbits that reach a couple times beyond the circularization limit.
These discrepancies are reminiscent of, but less severe than, the situation for
main-sequence binaries. We also find that tides can spin giant stars up to
rotation rates that should affect their mass-loss. Additionally, many binaries
may begin mass transfer while still eccentric.; 93) Nonlinear optical refractive index measurements of pure water via Z-scan
  technique at 800 nm; The determination of the nonlinear optical coefficient \( n_2 \) of pure
water is of great interest for applications such as imaging biological systems,
performing femtosecond laser surgery, and underwater ablation. Traditionally,
probing third-order nonlinearity while eliminating the thermo-optic effect
requires high pulse energy and low repetition rate laser sources. In this
article, a low-cost, simple, and versatile third-order nonlinear
characterization technique for the Z-scan closed-aperture mode has been
developed to determine the nonlinear refractive index of pure water at 800 nm,
with a pump duration of 140 fs and a repetition rate of 80 MHz. Experimental
results reveal an \( n_2 \) value of \( (8 \pm 1.4) \times 10^{-20} \)
m\(^2\)/W. The influence of higher-order nonlinearities such as \( \chi^5 \)
was assessed, showing no significant contribution within the examined intensity
range. This work paves the way for the use of low-power and
high-repetition-rate systems for characterizing third-order nonlinear optical
materials.; 94) TalkingEyes: Pluralistic Speech-Driven 3D Eye Gaze Animation; Although significant progress has been made in the field of speech-driven 3D
facial animation recently, the speech-driven animation of an indispensable
facial component, eye gaze, has been overlooked by recent research. This is
primarily due to the weak correlation between speech and eye gaze, as well as
the scarcity of audio-gaze data, making it very challenging to generate 3D eye
gaze motion from speech alone. In this paper, we propose a novel data-driven
method which can generate diverse 3D eye gaze motions in harmony with the
speech. To achieve this, we firstly construct an audio-gaze dataset that
contains about 14 hours of audio-mesh sequences featuring high-quality eye gaze
motion, head motion and facial motion simultaneously. The motion data is
acquired by performing lightweight eye gaze fitting and face reconstruction on
videos from existing audio-visual datasets. We then tailor a novel
speech-to-motion translation framework in which the head motions and eye gaze
motions are jointly generated from speech but are modeled in two separate
latent spaces. This design stems from the physiological knowledge that the
rotation range of eyeballs is less than that of head. Through mapping the
speech embedding into the two latent spaces, the difficulty in modeling the
weak correlation between speech and non-verbal motion is thus attenuated.
Finally, our TalkingEyes, integrated with a speech-driven 3D facial motion
generator, can synthesize eye gaze motion, eye blinks, head motion and facial
motion collectively from speech. Extensive quantitative and qualitative
evaluations demonstrate the superiority of the proposed method in generating
diverse and natural 3D eye gaze motions from speech. The project page of this
paper is: https://lkjkjoiuiu.github.io/TalkingEyes_Home/; 95) Survey on Monocular Metric Depth Estimation; Monocular Depth Estimation (MDE) is a fundamental computer vision task
underpinning applications such as spatial understanding, 3D reconstruction, and
autonomous driving. While deep learning-based MDE methods can predict relative
depth from a single image, their lack of metric scale information often results
in scale inconsistencies, limiting their utility in downstream tasks like
visual SLAM, 3D reconstruction, and novel view synthesis. Monocular Metric
Depth Estimation (MMDE) addresses these challenges by enabling precise,
scene-scale depth inference. MMDE improves depth consistency, enhances
sequential task stability, simplifies integration into downstream applications,
and broadens practical use cases. This paper provides a comprehensive review of
depth estimation technologies, highlighting the evolution from geometry-based
methods to state-of-the-art deep learning approaches. It emphasizes
advancements in scale-agnostic methods, which are crucial for enabling
zero-shot generalization as the foundational capability for MMDE. Recent
progress in zero-shot MMDE research is explored, focusing on challenges such as
model generalization and the loss of detail at scene boundaries. Innovative
strategies to address these issues include unlabelled data augmentation, image
patching, architectural optimization, and generative techniques. These
advancements, analyzed in detail, demonstrate significant contributions to
overcoming existing limitations. Finally, this paper synthesizes recent
developments in zero-shot MMDE, identifies unresolved challenges, and outlines
future research directions. By offering a clear roadmap and cutting-edge
insights, this work aims to deepen understanding of MMDE, inspire novel
applications, and drive technological innovation.; 96) Coherent spin dynamics in ensembles of randomly oriented singly charged
  colloidal nanoplatelets and nanocrystals; We present a theoretical study of the pump-probe Faraday rotation and
ellipticity signals in ensembles of uniaxially anisotropic CdSe nanoplatelets
and nanocrystals. We use the Faraday rotation mechanism based on the excitation
of negative heavy hole trions for a magnetic field applied in the Voigt
geometry. Three types of ensembles with typical spatial distributions of the
orientation of the anisotropy axis with respect to the direction of light
propagation are considered. Faraday rotation and ellipticity signals are
modeled for excitation by single and repeated pump pulses, taking into account
the anisotropy of the electron g-factor. We show that spin dephasing caused by
the electron g-factor anisotropy and the arbitrary orientation of nanoplatelets
or nanocrystals result only in partial damping of oscillation amplitude in
contrast to the dephasing caused by the dispersion of the electron g-factor in
the ensemble. We demonstrate that regardless of the g-factor anisotropy degree
the oscillation frequency of the Faraday rotation and ellipticity signals for a
randomly oriented ensemble is determined by the transverse electron g-factor
component.; 97) DFDT: Dynamic Fast Decision Tree for IoT Data Stream Mining on Edge
  Devices; The Internet of Things generates massive data streams, with edge computing
emerging as a key enabler for online IoT applications and 5G networks. Edge
solutions facilitate real-time machine learning inference, but also require
continuous adaptation to concept drifts. Ensemble-based solutions improve
predictive performance, but incur higher resource consumption, latency, and
memory demands. This paper presents DFDT: Dynamic Fast Decision Tree, a novel
algorithm designed for energy-efficient memory-constrained data stream mining.
DFDT improves hoeffding tree growth efficiency by dynamically adjusting grace
periods, tie thresholds, and split evaluations based on incoming data. It
incorporates stricter evaluation rules (based on entropy, information gain, and
leaf instance count), adaptive expansion modes, and a leaf deactivation
mechanism to manage memory, allowing more computation on frequently visited
nodes while conserving energy on others. Experiments show that the proposed
framework can achieve increased predictive performance (0.43 vs 0.29 ranking)
with constrained memory and a fraction of the runtime of VFDT or SVFDT.; 98) Multimodal Large Language Models for Image, Text, and Speech Data
  Augmentation: A Survey; In the past five years, research has shifted from traditional Machine
Learning (ML) and Deep Learning (DL) approaches to leveraging Large Language
Models (LLMs) , including multimodality, for data augmentation to enhance
generalization, and combat overfitting in training deep convolutional neural
networks. However, while existing surveys predominantly focus on ML and DL
techniques or limited modalities (text or images), a gap remains in addressing
the latest advancements and multi-modal applications of LLM-based methods. This
survey fills that gap by exploring recent literature utilizing multimodal LLMs
to augment image, text, and audio data, offering a comprehensive understanding
of these processes. We outlined various methods employed in the LLM-based
image, text and speech augmentation, and discussed the limitations identified
in current approaches. Additionally, we identified potential solutions to these
limitations from the literature to enhance the efficacy of data augmentation
practices using multimodal LLMs. This survey serves as a foundation for future
research, aiming to refine and expand the use of multimodal LLMs in enhancing
dataset quality and diversity for deep learning applications. (Surveyed Paper
GitHub Repo: https://github.com/WSUAgRobotics/data-aug-multi-modal-llm.
Keywords: LLM data augmentation, Grok text data augmentation, DeepSeek image
data augmentation, Grok speech data augmentation, GPT audio augmentation, voice
augmentation, DeepSeek for data augmentation, DeepSeek R1 text data
augmentation, DeepSeek R1 image augmentation, Image Augmentation using LLM,
Text Augmentation using LLM, LLM data augmentation for deep learning
applications); 99) Structure-Aware Correspondence Learning for Relative Pose Estimation; Relative pose estimation provides a promising way for achieving
object-agnostic pose estimation. Despite the success of existing 3D
correspondence-based methods, the reliance on explicit feature matching suffers
from small overlaps in visible regions and unreliable feature estimation for
invisible regions. Inspired by humans' ability to assemble two object parts
that have small or no overlapping regions by considering object structure, we
propose a novel Structure-Aware Correspondence Learning method for Relative
Pose Estimation, which consists of two key modules. First, a structure-aware
keypoint extraction module is designed to locate a set of kepoints that can
represent the structure of objects with different shapes and appearance, under
the guidance of a keypoint based image reconstruction loss. Second, a
structure-aware correspondence estimation module is designed to model the
intra-image and inter-image relationships between keypoints to extract
structure-aware features for correspondence estimation. By jointly leveraging
these two modules, the proposed method can naturally estimate 3D-3D
correspondences for unseen objects without explicit feature matching for
precise relative pose estimation. Experimental results on the CO3D, Objaverse
and LineMOD datasets demonstrate that the proposed method significantly
outperforms prior methods, i.e., with 5.7{\deg}reduction in mean angular error
on the CO3D dataset.; 100) (Neural-Symbolic) Machine Learning for Inconsistency Measurement; We present machine-learning-based approaches for determining the
\emph{degree} of inconsistency -- which is a numerical value -- for
propositional logic knowledge bases. Specifically, we present regression- and
neural-based models that learn to predict the values that the inconsistency
measures $\incmi$ and $\incat$ would assign to propositional logic knowledge
bases. Our main motivation is that computing these values conventionally can be
hard complexity-wise. As an important addition, we use specific postulates,
that is, properties, of the underlying inconsistency measures to infer symbolic
rules, which we combine with the learning-based models in the form of
constraints. We perform various experiments and show that a) predicting the
degree values is feasible in many situations, and b) including the symbolic
constraints deduced from the rationality postulates increases the prediction
quality.",1.0,0.6309297535714575
2411.02815,applied,2411.02815-pos2-7,"Automated segmentation of liver segment on portal venous phase MR images using a 3D convolutional neural network; An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale; We aim to develop and validate a three-dimensional convolutional neural network (3D-CNN) model for automatic liver segment segmentation on MRI images.This retrospective study evaluated an automated method using deep that was trained, validated, tested with 367, 157, 158 portal venous phase MR images, respectively. The Dice similarity coefficient (DSC), mean surface distance (MSD), Hausdorff (HD), volume ratio (RV) were used quantitatively measure the accuracy of segmentation. time consumed manual also compared. In addition, applied 100 consecutive cases from real clinical scenario qualitative evaluation indirect evaluation.In quantitative evaluation, achieved high DSC, MSD, HD RV (0.920, 3.34, 3.61 1.01, respectively). Compared segmentation, reduced 26 min 8 s. quality rated as good in 79% cases, moderate 15% poor 6%. 93.4% (99/106) lesions could be assigned correct by only referring results segmentation.The proposed may serve effective tool anatomical region annotation images.; While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",2411.02815-pos1-7,"Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation; In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate.",36,"['1', '13', '24', '36', '37', '10', '88', '4', '12', '19']","The first candidate paper focuses on one-shot medical landmark detection, which aligns well with the main paper's automated segmentation of liver segments, enhancing the precision of segmentation tasks in medical imaging. The synergy between leveraging deep learning for both segmentation and landmark detection can lead to improved clinical outcomes and efficient tools for medical professionals. This paper proposes a novel adaptive model which can be instrumental in improving the main paper's methods, hence forming a highly relevant multidisciplinary research idea. Subsequent candidates are ranked based on their relevance to medical imaging, segmentation, and the potential to enhance existing methodologies depicted in the main paper.","1) Landmarks Are Alike Yet Distinct: Harnessing Similarity and
  Individuality for One-Shot Medical Landmark Detection; Landmark detection plays a crucial role in medical imaging applications such
as disease diagnosis, bone age estimation, and therapy planning. However,
training models for detecting multiple landmarks simultaneously often
encounters the ""seesaw phenomenon"", where improvements in detecting certain
landmarks lead to declines in detecting others. Yet, training a separate model
for each landmark increases memory usage and computational overhead. To address
these challenges, we propose a novel approach based on the belief that
""landmarks are distinct"" by training models with pseudo-labels and template
data updated continuously during the training process, where each model is
dedicated to detecting a single landmark to achieve high accuracy. Furthermore,
grounded on the belief that ""landmarks are also alike"", we introduce an
adapter-based fusion model, combining shared weights with landmark-specific
weights, to efficiently share model parameters while allowing flexible
adaptation to individual landmarks. This approach not only significantly
reduces memory and computational resource requirements but also effectively
mitigates the seesaw phenomenon in multi-landmark training. Experimental
results on publicly available medical image datasets demonstrate that the
single-landmark models significantly outperform traditional multi-point joint
training models in detecting individual landmarks. Although our adapter-based
fusion model shows slightly lower performance compared to the combined results
of all single-landmark models, it still surpasses the current state-of-the-art
methods while achieving a notable improvement in resource efficiency.; 2) TalkingEyes: Pluralistic Speech-Driven 3D Eye Gaze Animation; Although significant progress has been made in the field of speech-driven 3D
facial animation recently, the speech-driven animation of an indispensable
facial component, eye gaze, has been overlooked by recent research. This is
primarily due to the weak correlation between speech and eye gaze, as well as
the scarcity of audio-gaze data, making it very challenging to generate 3D eye
gaze motion from speech alone. In this paper, we propose a novel data-driven
method which can generate diverse 3D eye gaze motions in harmony with the
speech. To achieve this, we firstly construct an audio-gaze dataset that
contains about 14 hours of audio-mesh sequences featuring high-quality eye gaze
motion, head motion and facial motion simultaneously. The motion data is
acquired by performing lightweight eye gaze fitting and face reconstruction on
videos from existing audio-visual datasets. We then tailor a novel
speech-to-motion translation framework in which the head motions and eye gaze
motions are jointly generated from speech but are modeled in two separate
latent spaces. This design stems from the physiological knowledge that the
rotation range of eyeballs is less than that of head. Through mapping the
speech embedding into the two latent spaces, the difficulty in modeling the
weak correlation between speech and non-verbal motion is thus attenuated.
Finally, our TalkingEyes, integrated with a speech-driven 3D facial motion
generator, can synthesize eye gaze motion, eye blinks, head motion and facial
motion collectively from speech. Extensive quantitative and qualitative
evaluations demonstrate the superiority of the proposed method in generating
diverse and natural 3D eye gaze motions from speech. The project page of this
paper is: https://lkjkjoiuiu.github.io/TalkingEyes_Home/; 3) Free-Lunch Color-Texture Disentanglement for Stylized Image Generation; Recent advances in Text-to-Image (T2I) diffusion models have transformed
image generation, enabling significant progress in stylized generation using
only a few style reference images. However, current diffusion-based methods
struggle with fine-grained style customization due to challenges in controlling
multiple style attributes, such as color and texture. This paper introduces the
first tuning-free approach to achieve free-lunch color-texture disentanglement
in stylized T2I generation, addressing the need for independently controlled
style elements for the Disentangled Stylized Image Generation (DisIG) problem.
Our approach leverages the Image-Prompt Additivity property in the CLIP image
embedding space to develop techniques for separating and extracting
Color-Texture Embeddings (CTE) from individual color and texture reference
images. To ensure that the color palette of the generated image aligns closely
with the color reference, we apply a whitening and coloring transformation to
enhance color consistency. Additionally, to prevent texture loss due to the
signal-leak bias inherent in diffusion training, we introduce a noise term that
preserves textural fidelity during the Regularized Whitening and Coloring
Transformation (RegWCT). Through these methods, our Style Attributes
Disentanglement approach (SADis) delivers a more precise and customizable
solution for stylized image generation. Experiments on images from the WikiArt
and StyleDrop datasets demonstrate that, both qualitatively and quantitatively,
SADis surpasses state-of-the-art stylization methods in the DisIG task.Code
will be released at https://deepffff.github.io/sadis.github.io/.; 4) Impact of UC2, UC, UBC and UB2 target compositions on the release of
  fission products; The release properties of 4 targets (UC2, UC, UBC, UB2) were measured for 11
elements (Kr, Sr, Ru, Sn, Sb, Te, I, Cs, Ba, La, and Ce) using an off-line
technique. The crystal packing fraction and the size of the studied element
play a key role in the release process. However, physicochemical properties are
also involved, notably melting and boiling points in vacuum and the minimal
oxidation state. Principal component analysis was used to investigate the
interrelationships between the physicochemical properties of fission products
(from Fe to Dy) and the observed releases, thereby enabling predictions to be
made about the release properties of the four crystallic configurations for
elements that are inaccessible in off-line experiments.; 5) ForestSplats: Deformable transient field for Gaussian Splatting in the
  Wild; Recently, 3D Gaussian Splatting (3D-GS) has emerged, showing real-time
rendering speeds and high-quality results in static scenes. Although 3D-GS
shows effectiveness in static scenes, their performance significantly degrades
in real-world environments due to transient objects, lighting variations, and
diverse levels of occlusion. To tackle this, existing methods estimate
occluders or transient elements by leveraging pre-trained models or integrating
additional transient field pipelines. However, these methods still suffer from
two defects: 1) Using semantic features from the Vision Foundation model (VFM)
causes additional computational costs. 2) The transient field requires
significant memory to handle transient elements with per-view Gaussians and
struggles to define clear boundaries for occluders, solely relying on
photometric errors. To address these problems, we propose ForestSplats, a novel
approach that leverages the deformable transient field and a superpixel-aware
mask to efficiently represent transient elements in the 2D scene across
unconstrained image collections and effectively decompose static scenes from
transient distractors without VFM. We designed the transient field to be
deformable, capturing per-view transient elements. Furthermore, we introduce a
superpixel-aware mask that clearly defines the boundaries of occluders by
considering photometric errors and superpixels. Additionally, we propose
uncertainty-aware densification to avoid generating Gaussians within the
boundaries of occluders during densification. Through extensive experiments
across several benchmark datasets, we demonstrate that ForestSplats outperforms
existing methods without VFM and shows significant memory efficiency in
representing transient elements.; 6) Higher-Dimensional Vacuum Einstein Equations: Symmetry, New Solutions,
  and Ricci Solitons; We show that the system of vacuum Einstein equations (i.e., Ricci-flat
metrics) with two hypersurface-orthogonal, commuting Killing vector fields in
$d \ge 5$ dimensions is invariant under the action of a one-parameter Lie
group, and the group action on any metric can be expressed in a closed,
universal form. This enables the generation of a one-parameter family of
solutions from any given ``seed"" solution of the system without solving
additional equations, as well as one-parameter families of local steady Ricci
solitons. This extends the Lie point symmetry in four dimensions, found earlier
for axisymmetric static vacuum systems, and provides the first example of
solution generation in higher-dimensional vacuum Einstein equations that can be
realized purely algebraically.; 7) Para-Lane: Multi-Lane Dataset Registering Parallel Scans for
  Benchmarking Novel View Synthesis; To evaluate end-to-end autonomous driving systems, a simulation environment
based on Novel View Synthesis (NVS) techniques is essential, which synthesizes
photo-realistic images and point clouds from previously recorded sequences
under new vehicle poses, particularly in cross-lane scenarios. Therefore, the
development of a multi-lane dataset and benchmark is necessary. While recent
synthetic scene-based NVS datasets have been prepared for cross-lane
benchmarking, they still lack the realism of captured images and point clouds.
To further assess the performance of existing methods based on NeRF and 3DGS,
we present the first multi-lane dataset registering parallel scans specifically
for novel driving view synthesis dataset derived from real-world scans,
comprising 25 groups of associated sequences, including 16,000 front-view
images, 64,000 surround-view images, and 16,000 LiDAR frames. All frames are
labeled to differentiate moving objects from static elements. Using this
dataset, we evaluate the performance of existing approaches in various testing
scenarios at different lanes and distances. Additionally, our method provides
the solution for solving and assessing the quality of multi-sensor poses for
multi-modal data alignment for curating such a dataset in real-world. We plan
to continually add new sequences to test the generalization of existing methods
across different scenarios. The dataset is released publicly at the project
page: https://nizqleo.github.io/paralane-dataset/.; 8) Retrievals Can Be Detrimental: A Contrastive Backdoor Attack Paradigm on
  Retrieval-Augmented Diffusion Models; Diffusion models (DMs) have recently demonstrated remarkable generation
capability. However, their training generally requires huge computational
resources and large-scale datasets. To solve these, recent studies empower DMs
with the advanced Retrieval-Augmented Generation (RAG) technique and propose
retrieval-augmented diffusion models (RDMs). By incorporating rich knowledge
from an auxiliary database, RAG enhances diffusion models' generation and
generalization ability while significantly reducing model parameters. Despite
the great success, RAG may introduce novel security issues that warrant further
investigation. In this paper, we reveal that the RDM is susceptible to backdoor
attacks by proposing a multimodal contrastive attack approach named BadRDM. Our
framework fully considers RAG's characteristics and is devised to manipulate
the retrieved items for given text triggers, thereby further controlling the
generated contents. Specifically, we first insert a tiny portion of images into
the retrieval database as target toxicity surrogates. Subsequently, a malicious
variant of contrastive learning is adopted to inject backdoors into the
retriever, which builds shortcuts from triggers to the toxicity surrogates.
Furthermore, we enhance the attacks through novel entropy-based selection and
generative augmentation strategies that can derive better toxicity surrogates.
Extensive experiments on two mainstream tasks demonstrate the proposed BadRDM
achieves outstanding attack effects while preserving the model's benign
utility.; 9) MAPoRL: Multi-Agent Post-Co-Training for Collaborative Large Language
  Models with Reinforcement Learning; Leveraging multiple large language models (LLMs) to build collaborative
multi-agentic workflows has demonstrated significant potential. However, most
previous studies focus on prompting the out-of-the-box LLMs, relying on their
innate capability for collaboration, which may not improve LLMs' performance as
shown recently. In this paper, we introduce a new post-training paradigm MAPoRL
(Multi-Agent Post-co-training for collaborative LLMs with Reinforcement
Learning), to explicitly elicit the collaborative behaviors and further unleash
the power of multi-agentic LLM frameworks. In MAPoRL, multiple LLMs first
generate their own responses independently and engage in a multi-turn
discussion to collaboratively improve the final answer. In the end, a MAPoRL
verifier evaluates both the answer and the discussion, by assigning a score
that verifies the correctness of the answer, while adding incentives to
encourage corrective and persuasive discussions. The score serves as the
co-training reward, and is then maximized through multi-agent RL. Unlike
existing LLM post-training paradigms, MAPoRL advocates the co-training of
multiple LLMs together using RL for better generalization. Accompanied by
analytical insights, our experiments demonstrate that training individual LLMs
alone is insufficient to induce effective collaboration. In contrast,
multi-agent co-training can boost the collaboration performance across
benchmarks, with generalization to unseen domains.; 10) Prompt to Restore, Restore to Prompt: Cyclic Prompting for Universal
  Adverse Weather Removal; Universal adverse weather removal (UAWR) seeks to address various weather
degradations within a unified framework. Recent methods are inspired by prompt
learning using pre-trained vision-language models (e.g., CLIP), leveraging
degradation-aware prompts to facilitate weather-free image restoration,
yielding significant improvements. In this work, we propose CyclicPrompt, an
innovative cyclic prompt approach designed to enhance the effectiveness,
adaptability, and generalizability of UAWR. CyclicPrompt Comprises two key
components: 1) a composite context prompt that integrates weather-related
information and context-aware representations into the network to guide
restoration. This prompt differs from previous methods by marrying learnable
input-conditional vectors with weather-specific knowledge, thereby improving
adaptability across various degradations. 2) The erase-and-paste mechanism,
after the initial guided restoration, substitutes weather-specific knowledge
with constrained restoration priors, inducing high-quality weather-free
concepts into the composite prompt to further fine-tune the restoration
process. Therefore, we can form a cyclic ""Prompt-Restore-Prompt"" pipeline that
adeptly harnesses weather-specific knowledge, textual contexts, and reliable
textures. Extensive experiments on synthetic and real-world datasets validate
the superior performance of CyclicPrompt. The code is available at:
https://github.com/RongxinL/CyclicPrompt.; 11) PoSSUM: A Protocol for Surveying Social-media Users with Multimodal LLMs; This paper introduces PoSSUM, an open-source protocol for unobtrusive polling
of social-media users via multimodal Large Language Models (LLMs). PoSSUM
leverages users' real-time posts, images, and other digital traces to create
silicon samples that capture information not present in the LLM's training
data. To obtain representative estimates, PoSSUM employs Multilevel Regression
and Post-Stratification (MrP) with structured priors to counteract the
observable selection biases of social-media platforms. The protocol is
validated during the 2024 U.S. Presidential Election, for which five PoSSUM
polls were conducted and published on GitHub and X. In the final poll, fielded
October 17-26 with a synthetic sample of 1,054 X users, PoSSUM accurately
predicted the outcomes in 50 of 51 states and assigned the Republican candidate
a win probability of 0.65. Notably, it also exhibited lower state-level bias
than most established pollsters. These results demonstrate PoSSUM's potential
as a fully automated, unobtrusive alternative to traditional survey methods.; 12) Minimal log discrepancy and orbifold curves; We show that the minimal log discrepancy of any isolated Fano cone
singularity is at most the dimension of the variety. This is based on its
relation with dimensions of moduli spaces of orbifold rational curves. We also
propose a conjectural characterization of weighted projective spaces as Fano
orbifolds in terms of orbifold rational curves, which would imply the equality
holds only for smooth points.; 13) Test-Time Modality Generalization for Medical Image Segmentation; Generalizable medical image segmentation is essential for ensuring consistent
performance across diverse unseen clinical settings. However, existing methods
often overlook the capability to generalize effectively across arbitrary unseen
modalities. In this paper, we introduce a novel Test-Time Modality
Generalization (TTMG) framework, which comprises two core components:
Modality-Aware Style Projection (MASP) and Modality-Sensitive Instance
Whitening (MSIW), designed to enhance generalization in arbitrary unseen
modality datasets. The MASP estimates the likelihood of a test instance
belonging to each seen modality and maps it onto a distribution using
modality-specific style bases, guiding its projection effectively. Furthermore,
as high feature covariance hinders generalization to unseen modalities, the
MSIW is applied during training to selectively suppress modality-sensitive
information while retaining modality-invariant features. By integrating MASP
and MSIW, the TTMG framework demonstrates robust generalization capabilities
for medical image segmentation in unseen modalities a challenge that current
methods have largely neglected. We evaluated TTMG alongside other domain
generalization techniques across eleven datasets spanning four modalities
(colonoscopy, ultrasound, dermoscopy, and radiology), consistently achieving
superior segmentation performance across various modality combinations.; 14) Cognitive-Aligned Document Selection for Retrieval-augmented Generation; Large language models (LLMs) inherently display hallucinations since the
precision of generated texts cannot be guaranteed purely by the parametric
knowledge they include. Although retrieval-augmented generation (RAG) systems
enhance the accuracy and reliability of generative models by incorporating
external documents, these retrieved documents often fail to adequately support
the model's responses in practical applications. To address this issue, we
propose GGatrieval (Fine-\textbf{G}rained \textbf{G}rounded \textbf{A}lignment
Re\textbf{trieval} for verifiable generation), which leverages an LLM to
dynamically update queries and filter high-quality, reliable retrieval
documents. Specifically, we parse the user query into its syntactic components
and perform fine-grained grounded alignment with the retrieved documents. For
query components that cannot be individually aligned, we propose a dynamic
semantic compensation mechanism that iteratively refines and rewrites the query
while continuously updating the retrieval results. This iterative process
continues until the retrieved documents sufficiently support the query's
response. Our approach introduces a novel criterion for filtering retrieved
documents, closely emulating human strategies for acquiring targeted
information. This ensures that the retrieved content effectively supports and
verifies the generated outputs. On the ALCE benchmark, our method significantly
surpasses a wide range of baselines, achieving state-of-the-art performance.; 15) When the Future Becomes the Past: Taming Temporal Correspondence for
  Self-supervised Video Representation Learning; The past decade has witnessed notable achievements in self-supervised
learning for video tasks. Recent efforts typically adopt the Masked Video
Modeling (MVM) paradigm, leading to significant progress on multiple video
tasks. However, two critical challenges remain: 1) Without human annotations,
the random temporal sampling introduces uncertainty, increasing the difficulty
of model training. 2) Previous MVM methods primarily recover the masked patches
in the pixel space, leading to insufficient information compression for
downstream tasks. To address these challenges jointly, we propose a
self-supervised framework that leverages Temporal Correspondence for video
Representation learning (T-CoRe). For challenge 1), we propose a sandwich
sampling strategy that selects two auxiliary frames to reduce reconstruction
uncertainty in a two-side-squeezing manner. Addressing challenge 2), we
introduce an auxiliary branch into a self-distillation architecture to restore
representations in the latent space, generating high-level semantic
representations enriched with temporal information. Experiments of T-CoRe
consistently present superior performance across several downstream tasks,
demonstrating its effectiveness for video representation learning. The code is
available at https://github.com/yafeng19/T-CORE.; 16) Learn Your Scales: Towards Scale-Consistent Generative Novel View
  Synthesis; Conventional depth-free multi-view datasets are captured using a moving
monocular camera without metric calibration. The scales of camera positions in
this monocular setting are ambiguous. Previous methods have acknowledged scale
ambiguity in multi-view data via various ad-hoc normalization pre-processing
steps, but have not directly analyzed the effect of incorrect scene scales on
their application. In this paper, we seek to understand and address the effect
of scale ambiguity when used to train generative novel view synthesis methods
(GNVS). In GNVS, new views of a scene or object can be minimally synthesized
given a single image and are, thus, unconstrained, necessitating the use of
generative methods. The generative nature of these models captures all aspects
of uncertainty, including any uncertainty of scene scales, which act as
nuisance variables for the task. We study the effect of scene scale ambiguity
in GNVS when sampled from a single image by isolating its effect on the
resulting models and, based on these intuitions, define new metrics that
measure the scale inconsistency of generated views. We then propose a framework
to estimate scene scales jointly with the GNVS model in an end-to-end fashion.
Empirically, we show that our method reduces the scale inconsistency of
generated views without the complexity or downsides of previous scale
normalization methods. Further, we show that removing this ambiguity improves
generated image quality of the resulting GNVS model.; 17) Measuring AI agent autonomy: Towards a scalable approach with code
  inspection; AI agents are AI systems that can achieve complex goals autonomously.
Assessing the level of agent autonomy is crucial for understanding both their
potential benefits and risks. Current assessments of autonomy often focus on
specific risks and rely on run-time evaluations -- observations of agent
actions during operation. We introduce a code-based assessment of autonomy that
eliminates the need to run an AI agent to perform specific tasks, thereby
reducing the costs and risks associated with run-time evaluations. Using this
code-based framework, the orchestration code used to run an AI agent can be
scored according to a taxonomy that assesses attributes of autonomy: impact and
oversight. We demonstrate this approach with the AutoGen framework and select
applications.; 18) Speculate, then Collaborate: Fusing Knowledge of Language Models during
  Decoding; Large Language Models (LLMs) often excel in specific domains but fall short
in others due to the limitations of their training. Thus, enabling LLMs to
solve problems collaboratively by integrating their complementary knowledge
promises to improve their performance across domains. To realize this
potential, we introduce a novel Collaborative Speculative Decoding (CoSD)
algorithm that enables efficient LLM knowledge fusion at test time without
requiring additional model training. CoSD employs a draft model to generate
initial sequences and an easy-to-learn rule or decision tree to decide when to
invoke an assistant model to improve these drafts. CoSD not only enhances
knowledge fusion but also improves inference efficiency, is transferable across
domains and models, and offers greater explainability. Experimental results
demonstrate that CoSD improves accuracy by up to 10\% across benchmarks
compared to existing methods, providing a scalable and effective solution for
LLM-based applications; 19) R2LDM: An Efficient 4D Radar Super-Resolution Framework Leveraging
  Diffusion Model; We introduce R2LDM, an innovative approach for generating dense and accurate
4D radar point clouds, guided by corresponding LiDAR point clouds. Instead of
utilizing range images or bird's eye view (BEV) images, we represent both LiDAR
and 4D radar point clouds using voxel features, which more effectively capture
3D shape information. Subsequently, we propose the Latent Voxel Diffusion Model
(LVDM), which performs the diffusion process in the latent space. Additionally,
a novel Latent Point Cloud Reconstruction (LPCR) module is utilized to
reconstruct point clouds from high-dimensional latent voxel features. As a
result, R2LDM effectively generates LiDAR-like point clouds from paired raw
radar data. We evaluate our approach on two different datasets, and the
experimental results demonstrate that our model achieves 6- to 10-fold
densification of radar point clouds, outperforming state-of-the-art baselines
in 4D radar point cloud super-resolution. Furthermore, the enhanced radar point
clouds generated by our method significantly improve downstream tasks,
achieving up to 31.7% improvement in point cloud registration recall rate and
24.9% improvement in object detection accuracy.; 20) Enhanced collective vibrations in granular materials; Granular materials are defined as collections of macroscopic dissipative
particles. Although these systems are ubiquitous in our lives, the nature and
the causes of their non-trivial collective dynamics still remain elusive and
have attracted significant interest in non-equilibrium physics. Here, we focus
on the vibrational dynamics of granular materials. While the vibrational
dynamics of random packings have been examined concerning the jamming
transition, previous research has overlooked the role of contact dissipations.
We conducted numerical and analytical investigations into the vibrational
dynamics of random packings influenced by the normal dissipative force, which
is the simplest model for contact dissipations. Our findings reveal that the
kinetic energy per mode diverges in the low-frequency range, following the
scaling law $\mathcal{K}_l \propto \omega^{-2}_l$ with the frequency
$\omega_l$, indicating that low-frequency modes experience strong excitation
and that the equipartition of energy is violated. Additionally, the spatial
structure factor of the velocity field displays the scaling law $S_v(q) \propto
q^{-2}$ with the wavenumber $q$, which signifies that the velocity field has an
infinitely long range. We demonstrate that these phenomena arise from the
effects of weaker damping on softer modes, where the particle displacements
parallel to the contacts are minimal in the low-frequency modes, rendering
normal dissipation ineffective at dampening these modes.; 21) Structure-Aware Correspondence Learning for Relative Pose Estimation; Relative pose estimation provides a promising way for achieving
object-agnostic pose estimation. Despite the success of existing 3D
correspondence-based methods, the reliance on explicit feature matching suffers
from small overlaps in visible regions and unreliable feature estimation for
invisible regions. Inspired by humans' ability to assemble two object parts
that have small or no overlapping regions by considering object structure, we
propose a novel Structure-Aware Correspondence Learning method for Relative
Pose Estimation, which consists of two key modules. First, a structure-aware
keypoint extraction module is designed to locate a set of kepoints that can
represent the structure of objects with different shapes and appearance, under
the guidance of a keypoint based image reconstruction loss. Second, a
structure-aware correspondence estimation module is designed to model the
intra-image and inter-image relationships between keypoints to extract
structure-aware features for correspondence estimation. By jointly leveraging
these two modules, the proposed method can naturally estimate 3D-3D
correspondences for unseen objects without explicit feature matching for
precise relative pose estimation. Experimental results on the CO3D, Objaverse
and LineMOD datasets demonstrate that the proposed method significantly
outperforms prior methods, i.e., with 5.7{\deg}reduction in mean angular error
on the CO3D dataset.; 22) Mixed Berndt-Type Integrals and Generalized Barnes Multiple Zeta
  Functions; In this paper, we define and study four families of Berndt-type integrals,
called mixed Berndt-type integrals, which contains (hyperbolic) sine and cosine
functions in the integrand function. By contour integration, these integrals
are first converted to some hyperbolic (infinite) sums of Ramanujan type, all
of which can be calculated in closed forms by comparing both the Fourier series
expansions and the Maclaurin series expansions of a few Jacobi elliptic
functions. These sums can be expressed as rational polynomials of {\Gamma}(1/4)
and {\pi}^{-1} which give rise to the closed formulas of the mixed Berndt-type
integrals we are interested in. Moreover, we also present some interesting
consequences and illustrative examples. Further, we define a generalized Barnes
multiple zeta function, and find a classic integral representation of the
generalized Barnes multiple zeta function. Furthermore, we give an alternative
evaluation of the mixed Berndt-type integrals in terms of the generalized
Barnes multiple zeta function. Finally, we obtain some direct evaluations of
rational linear combinations of the generalized Barnes multiple zeta function.; 23) A Driver Advisory System Based on Large Language Model for High-speed
  Train; With the rapid development of China high-speed railway, drivers face
increasingly significant technical challenges during operations, such as fault
handling. Currently, drivers depend on the onboard mechanic when facing
technical issues, for instance, traction loss or sensor faults. This dependency
can hinder effective operation, even lead to accidents, while waiting for
faults to be addressed. To enhance the accuracy and explainability of actions
during fault handling, an Intelligent Driver Advisory System (IDAS) framework
based on a large language model (LLM) named IDAS-LLM, is introduced. Initially,
domain-fine-tuning of the LLM is performed using a constructed railway
knowledge question-and-answer dataset to improve answer accuracy in
railway-related questions. Subsequently, integration of the Retrieval-augmented
Generation (RAG) architecture is pursued for system design to enhance the
explainability of generated responses. Comparative experiments are conducted
using the constructed railway driving knowledge assessment dataset. Results
indicate that domain-fine-tuned LLMs show an improvement in answer accuracy by
an average of 10%, outperforming some current mainstream LLMs. Additionally,
the inclusion of the RAG framework increases the average recall rate of
question-and-answer sessions by about 4%. Finally, the fault handling
capability of IDAS-LLM is demonstrated through simulations of real operational
scenarios, proving that the proposed framework has practical application
prospects.; 24) SemiSAM+: Rethinking Semi-Supervised Medical Image Segmentation in the
  Era of Foundation Models; Deep learning-based medical image segmentation typically requires large
amount of labeled data for training, making it less applicable in clinical
settings due to high annotation cost. Semi-supervised learning (SSL) has
emerged as an appealing strategy due to its less dependence on acquiring
abundant annotations from experts compared to fully supervised methods. Beyond
existing model-centric advancements of SSL by designing novel regularization
strategies, we anticipate a paradigmatic shift due to the emergence of
promptable segmentation foundation models with universal segmentation
capabilities using positional prompts represented by Segment Anything Model
(SAM). In this paper, we present SemiSAM+, a foundation model-driven SSL
framework to efficiently learn from limited labeled data for medical image
segmentation. SemiSAM+ consists of one or multiple promptable foundation models
as generalist models, and a trainable task-specific segmentation model as
specialist model. For a given new segmentation task, the training is based on
the specialist-generalist collaborative learning procedure, where the trainable
specialist model delivers positional prompts to interact with the frozen
generalist models to acquire pseudo-labels, and then the generalist model
output provides the specialist model with informative and efficient supervision
which benefits the automatic segmentation and prompt generation in turn.
Extensive experiments on two public datasets and one in-house clinical dataset
demonstrate that SemiSAM+ achieves significant performance improvement,
especially under extremely limited annotation scenarios, and shows strong
efficiency as a plug-and-play strategy that can be easily adapted to different
specialist and generalist models.; 25) GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models; In recent years, 2D Vision-Language Models (VLMs) have made significant
strides in image-text understanding tasks. However, their performance in 3D
spatial comprehension, which is critical for embodied intelligence, remains
limited. Recent advances have leveraged 3D point clouds and multi-view images
as inputs, yielding promising results. However, we propose exploring a purely
vision-based solution inspired by human perception, which merely relies on
visual cues for 3D spatial understanding. This paper empirically investigates
the limitations of VLMs in 3D spatial knowledge, revealing that their primary
shortcoming lies in the lack of global-local correspondence between the scene
and individual frames. To address this, we introduce GPT4Scene, a novel visual
prompting paradigm in VLM training and inference that helps build the
global-local relationship, significantly improving the 3D spatial understanding
of indoor scenes. Specifically, GPT4Scene constructs a Bird's Eye View (BEV)
image from the video and marks consistent object IDs across both frames and the
BEV image. The model then inputs the concatenated BEV image and video frames
with markers. In zero-shot evaluations, GPT4Scene improves performance over
closed-source VLMs like GPT-4o. Additionally, we prepare a processed video
dataset consisting of 165K text annotation to fine-tune open-source VLMs,
achieving state-of-the-art performance on all 3D understanding tasks.
Surprisingly, after training with the GPT4Scene paradigm, VLMs consistently
improve during inference, even without object marker prompting and BEV image as
explicit correspondence. It demonstrates that the proposed paradigm helps VLMs
develop an intrinsic ability to understand 3D scenes, which paves the way for a
seamless approach to extending pre-trained VLMs for 3D scene understanding.; 26) Surgical Gaussian Surfels: Highly Accurate Real-time Surgical Scene
  Rendering; Accurate geometric reconstruction of deformable tissues in monocular
endoscopic video remains a fundamental challenge in robot-assisted minimally
invasive surgery. Although recent volumetric and point primitive methods based
on neural radiance fields (NeRF) and 3D Gaussian primitives have efficiently
rendered surgical scenes, they still struggle with handling artifact-free tool
occlusions and preserving fine anatomical details. These limitations stem from
unrestricted Gaussian scaling and insufficient surface alignment constraints
during reconstruction. To address these issues, we introduce Surgical Gaussian
Surfels (SGS), which transforms anisotropic point primitives into
surface-aligned elliptical splats by constraining the scale component of the
Gaussian covariance matrix along the view-aligned axis. We predict accurate
surfel motion fields using a lightweight Multi-Layer Perceptron (MLP) coupled
with locality constraints to handle complex tissue deformations. We use
homodirectional view-space positional gradients to capture fine image details
by splitting Gaussian Surfels in over-reconstructed regions. In addition, we
define surface normals as the direction of the steepest density change within
each Gaussian surfel primitive, enabling accurate normal estimation without
requiring monocular normal priors. We evaluate our method on two in-vivo
surgical datasets, where it outperforms current state-of-the-art methods in
surface geometry, normal map quality, and rendering efficiency, while remaining
competitive in real-time rendering performance. We make our code available at
https://github.com/aloma85/SurgicalGaussianSurfels; 27) Search-Based Adversarial Estimates for Improving Sample Efficiency in
  Off-Policy Reinforcement Learning; Sample inefficiency is a long-lasting challenge in deep reinforcement
learning (DRL). Despite dramatic improvements have been made, the problem is
far from being solved and is especially challenging in environments with sparse
or delayed rewards. In our work, we propose to use Adversarial Estimates as a
new, simple and efficient approach to mitigate this problem for a class of
feedback-based DRL algorithms. Our approach leverages latent similarity search
from a small set of human-collected trajectories to boost learning, using only
five minutes of human-recorded experience. The results of our study show
algorithms trained with Adversarial Estimates converge faster than their
original version. Moreover, we discuss how our approach could enable learning
in feedback-based algorithms in extreme scenarios with very sparse rewards.; 28) VFX Creator: Animated Visual Effect Generation with Controllable
  Diffusion Transformer; Crafting magic and illusions is one of the most thrilling aspects of
filmmaking, with visual effects (VFX) serving as the powerhouse behind
unforgettable cinematic experiences. While recent advances in generative
artificial intelligence have driven progress in generic image and video
synthesis, the domain of controllable VFX generation remains relatively
underexplored. In this work, we propose a novel paradigm for animated VFX
generation as image animation, where dynamic effects are generated from
user-friendly textual descriptions and static reference images. Our work makes
two primary contributions: (i) Open-VFX, the first high-quality VFX video
dataset spanning 15 diverse effect categories, annotated with textual
descriptions, instance segmentation masks for spatial conditioning, and
start-end timestamps for temporal control. (ii) VFX Creator, a simple yet
effective controllable VFX generation framework based on a Video Diffusion
Transformer. The model incorporates a spatial and temporal controllable LoRA
adapter, requiring minimal training videos. Specifically, a plug-and-play mask
control module enables instance-level spatial manipulation, while tokenized
start-end motion timestamps embedded in the diffusion process, alongside the
text encoder, allow precise temporal control over effect timing and pace.
Extensive experiments on the Open-VFX test set demonstrate the superiority of
the proposed system in generating realistic and dynamic effects, achieving
state-of-the-art performance and generalization ability in both spatial and
temporal controllability. Furthermore, we introduce a specialized metric to
evaluate the precision of temporal control. By bridging traditional VFX
techniques with generative approaches, VFX Creator unlocks new possibilities
for efficient and high-quality video effect generation, making advanced VFX
accessible to a broader audience.; 29) CollabLLM: From Passive Responders to Active Collaborators; Large Language Models are typically trained with next-turn rewards, limiting
their ability to optimize for long-term interaction. As a result, they often
respond passively to ambiguous or open-ended user requests, failing to help
users reach their ultimate intents and leading to inefficient conversations. To
address these limitations, we introduce CollabLLM, a novel and general training
framework that enhances multiturn human-LLM collaboration. Its key innovation
is a collaborative simulation that estimates the long-term contribution of
responses using Multiturn-aware Rewards. By reinforcement fine-tuning these
rewards, CollabLLM goes beyond responding to user requests, and actively
uncovers user intent and offers insightful suggestions-a key step towards more
human-centered AI. We also devise a multiturn interaction benchmark with three
challenging tasks such as document creation. CollabLLM significantly
outperforms our baselines with averages of 18.5% higher task performance and
46.3% improved interactivity by LLM judges. Finally, we conduct a large user
study with 201 judges, where CollabLLM increases user satisfaction by 17.6% and
reduces user spent time by 10.4%.; 30) On the Locality of Hall's Theorem; The last five years of research on distributed graph algorithms have seen
huge leaps of progress, both regarding algorithmic improvements and
impossibility results: new strong lower bounds have emerged for many central
problems and exponential improvements over the state of the art have been
achieved for the runtimes of many algorithms. Nevertheless, there are still
large gaps between the best known upper and lower bounds for many important
problems. The current lower bound techniques for deterministic algorithms are
often tailored to obtaining a logarithmic bound and essentially cannot be used
to prove lower bounds beyond $\Omega(\log n)$. In contrast, the best
deterministic upper bounds are often polylogarithmic, raising the fundamental
question of how to resolve the gap between logarithmic lower and
polylogarithmic upper bounds and finally obtain tight bounds. We develop a
novel algorithm design technique aimed at closing this gap. In essence, each
node finds a carefully chosen local solution in $O(\log n)$ rounds and we
guarantee that this solution is consistent with the other nodes' solutions
without coordination. The local solutions are based on a distributed version of
Hall's theorem that may be of independent interest and motivates the title of
this work. We showcase our framework by improving on the state of the art for
the following fundamental problems: edge coloring, bipartite saturating
matchings and hypergraph sinkless orientation. In particular, we obtain an
asymptotically optimal $O(\log n)$-round algorithm for $3\Delta/2$-edge
coloring in bounded degree graphs. The previously best bound for the problem
was $O(\log^4 n)$ rounds, obtained by plugging in the state-of-the-art maximal
independent set algorithm from arXiv:2303.16043 into the $3\Delta/2$-edge
coloring algorithm from arXiv:1711.05469 .; 31) Habitizing Diffusion Planning for Efficient and Effective Decision
  Making; Diffusion models have shown great promise in decision-making, also known as
diffusion planning. However, the slow inference speeds limit their potential
for broader real-world applications. Here, we introduce Habi, a general
framework that transforms powerful but slow diffusion planning models into fast
decision-making models, which mimics the cognitive process in the brain that
costly goal-directed behavior gradually transitions to efficient habitual
behavior with repetitive practice. Even using a laptop CPU, the habitized model
can achieve an average 800+ Hz decision-making frequency (faster than previous
diffusion planners by orders of magnitude) on standard offline reinforcement
learning benchmarks D4RL, while maintaining comparable or even higher
performance compared to its corresponding diffusion planner. Our work proposes
a fresh perspective of leveraging powerful diffusion models for real-world
decision-making tasks. We also provide robust evaluations and analysis,
offering insights from both biological and engineering perspectives for
efficient and effective decision-making.; 32) Exploring Dynamics of Open Quantum Systems in Naturally Inaccessible
  Regimes; Markovian open quantum systems are governed by the Lindblad master equation
where the dissipation contains two parts, i.e., the anti-Hermitian operator and
the quantum jumps, which share a common dissipation rate. We generalize the
Lindblad master equation via postselection to a generalized Liouvillian
formalism in which the effective damping rate of the anti-Hermitian operator
can be different from the quantum jump rate. Our formalism provides a parameter
space with regimes inaccessible in naturally-occurring systems. We explore
these new regimes and find several interesting results including negative
damping rates and generalized Liouvillian exceptional points. In a previously
unexplored zero-damping Liouvillian regime where the damping rate is
negligible, we investigate the effect only due to the quantum jumps and show an
unusual polynomial decay of the excited state. This generalized Liouvillian
formalism offers opportunities to explore novel phenomena and quantum
technologies associated with the peculiar behavior of quantum jumps.; 33) Design of a quantum diamond microscope with efficient scanning confocal
  readout; We introduce the light-sheet confocal quantum diamond microscope (LC-QDM) for
widefield 3D quantum sensing with efficient confocal readout. The LC-QDM
leverages light-sheet illumination and laser scanning confocal methods to
enable high-resolution, high-speed 3D measurements with nitrogen-vacancy (NV)
defects in diamond, combining the best of widefield and confocal modalities in
a single device and eliminating the need for thin-NV-layer diamond chips. We
perform simulations and measurements of NV initialization and readout times to
model the anticipated performance of the LC-QDM compared to existing QDM
designs. Our findings show that the LC-QDM will provide significant advantages
for applications requiring limited laser power.; 34) X-Field: A Physically Grounded Representation for 3D X-ray
  Reconstruction; X-ray imaging is indispensable in medical diagnostics, yet its use is tightly
regulated due to potential health risks. To mitigate radiation exposure, recent
research focuses on generating novel views from sparse inputs and
reconstructing Computed Tomography (CT) volumes, borrowing representations from
the 3D reconstruction area. However, these representations originally target
visible light imaging that emphasizes reflection and scattering effects, while
neglecting penetration and attenuation properties of X-ray imaging. In this
paper, we introduce X-Field, the first 3D representation specifically designed
for X-ray imaging, rooted in the energy absorption rates across different
materials. To accurately model diverse materials within internal structures, we
employ 3D ellipsoids with distinct attenuation coefficients. To estimate each
material's energy absorption of X-rays, we devise an efficient path
partitioning algorithm accounting for complex ellipsoid intersections. We
further propose hybrid progressive initialization to refine the geometric
accuracy of X-Filed and incorporate material-based optimization to enhance
model fitting along material boundaries. Experiments show that X-Field achieves
superior visual fidelity on both real-world human organ and synthetic object
datasets, outperforming state-of-the-art methods in X-ray Novel View Synthesis
and CT Reconstruction.; 35) SU(7) Grand Gauge-Higgs Unification; We propose a model of six dimensional $SU(7)$ grand gauge-Higgs unification
compactified on $S^1/Z_2 /times S^1/Z_2$, which is a six dimensional extension
of five dimensional $SU(4)$ gauge-Higgs unification predicting the weak mixing
angle $/sin^2 /theta_W=1/4$ at the compactification scale. We investigate
whether the correct pattern of electroweak symmetry breaking is realized in our
model. We find some solutions providing the electroweak symmetry breaking and a
realistic Higgs mass. Corresponding to each solution, the compactification
scale of the fifth dimension is predicted. Furthermore, we evaluate the weak
mixing angle at the weak scale by using one-loop renormalization group equation
and find it to be in good agreement with the experimental data.; 36) Liver Anatomy: Portal (and Suprahepatic) or Biliary Segmentation; In liver anatomy and surgery, is portal hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary (Healey Schroy, North American segmentation)?Several embryological arguments an analysis of anatomical data from a personal collection 110 vasculobiliary casts were made.Embryological arguments: Portal branching appears first, secondly follows the distribution. Segment II (the left lateral sector) development right lobe. The umbilical enters portion middle lobe, forming segment IV on III left: this paramedian sector. So fissure (between lobes) transversally crosses classical which not unit. VI late secondary prominence VII, reaching anterior margin only in man. Anatomical must added segmentation; academic lobe sector, separates lobes. preferred: duplication branches first order occurs 23.5% cases, while first-order noted 50% livers, being much simpler.Portal seems more accurate.; 37) Advancing Oyster Phenotype Segmentation with Multi-Network Ensemble and
  Multi-Scale mechanism; Phenotype segmentation is pivotal in analysing visual features of living
organisms, enhancing our understanding of their characteristics. In the context
of oysters, meat quality assessment is paramount, focusing on shell, meat,
gonad, and muscle components. Traditional manual inspection methods are
time-consuming and subjective, prompting the adoption of machine vision
technology for efficient and objective evaluation. We explore machine vision's
capacity for segmenting oyster components, leading to the development of a
multi-network ensemble approach with a global-local hierarchical attention
mechanism. This approach integrates predictions from diverse models and
addresses challenges posed by varying scales, ensuring robust instance
segmentation across components. Finally, we provide a comprehensive evaluation
of the proposed method's performance using different real-world datasets,
highlighting its efficacy and robustness in enhancing oyster phenotype
segmentation.; 38) Classification of Electron and Muon Neutrino Events for the ESS$\nu$SB
  Near Water Cherenkov Detector using Graph Neural Networks; In the effort to obtain a precise measurement of leptonic CP-violation with
the ESS$\nu$SB experiment, accurate and fast reconstruction of detector events
plays a pivotal role. In this work, we examine the possibility of replacing the
currently proposed likelihood-based reconstruction method with an approach
based on Graph Neural Networks (GNNs). As the likelihood-based reconstruction
method is reasonably accurate but computationally expensive, one of the
benefits of a Machine Learning (ML) based method is enabling fast event
reconstruction in the detector development phase, allowing for easier
investigation of the effects of changes to the detector design. Focusing on
classification of flavour and interaction type in muon and electron events and
muon- and electron neutrino interaction events, we demonstrate that the GNN
reconstructs events with greater accuracy than the likelihood method for events
with greater complexity, and with increased speed for all events. Additionally,
we investigate the key factors impacting reconstruction performance, and
demonstrate how separation of events by pion production using another GNN
classifier can benefit flavour classification.; 39) Stellar Ages: A Code to Infer Properties of Stellar Populations; We present a novel statistical algorithm, Stellar Ages, which currently
infers the age, metallicity, and extinction posterior distributions of stellar
populations from their magnitudes. While this paper focuses on these
parameters, the framework is readily adaptable to include additional
properties, such as rotation, in future work. Historical age-dating techniques
either model individual stars or populations of stars, often sacrificing
population context or precision for individual estimates. Stellar Ages does
both, combining the strengths of these approaches to provide precise individual
ages for stars while leveraging population-level constraints. We verify the
algorithm's capabilities by determining the age of synthetic stellar
populations and actual stellar populations surrounding a nearby supernova, SN
2004dj. In addition to inferring an age, we infer a progenitor mass consistent
with direct observations of the precursor star. The median age inferred from
the brightest nearby stars is $\log_{10}$(Age/yr) = $7.19^{+0.10}_{-0.13}$, and
its corresponding progenitor mass is $13.95^{+3.33}_{-1.96}$
$\text{M}_{\odot}$.; 40) Survey on Monocular Metric Depth Estimation; Monocular Depth Estimation (MDE) is a fundamental computer vision task
underpinning applications such as spatial understanding, 3D reconstruction, and
autonomous driving. While deep learning-based MDE methods can predict relative
depth from a single image, their lack of metric scale information often results
in scale inconsistencies, limiting their utility in downstream tasks like
visual SLAM, 3D reconstruction, and novel view synthesis. Monocular Metric
Depth Estimation (MMDE) addresses these challenges by enabling precise,
scene-scale depth inference. MMDE improves depth consistency, enhances
sequential task stability, simplifies integration into downstream applications,
and broadens practical use cases. This paper provides a comprehensive review of
depth estimation technologies, highlighting the evolution from geometry-based
methods to state-of-the-art deep learning approaches. It emphasizes
advancements in scale-agnostic methods, which are crucial for enabling
zero-shot generalization as the foundational capability for MMDE. Recent
progress in zero-shot MMDE research is explored, focusing on challenges such as
model generalization and the loss of detail at scene boundaries. Innovative
strategies to address these issues include unlabelled data augmentation, image
patching, architectural optimization, and generative techniques. These
advancements, analyzed in detail, demonstrate significant contributions to
overcoming existing limitations. Finally, this paper synthesizes recent
developments in zero-shot MMDE, identifies unresolved challenges, and outlines
future research directions. By offering a clear roadmap and cutting-edge
insights, this work aims to deepen understanding of MMDE, inspire novel
applications, and drive technological innovation.; 41) MGSR: 2D/3D Mutual-boosted Gaussian Splatting for High-fidelity Surface
  Reconstruction under Various Light Conditions; Novel view synthesis (NVS) and surface reconstruction (SR) are essential
tasks in 3D Gaussian Splatting (3D-GS). Despite recent progress, these tasks
are often addressed independently, with GS-based rendering methods struggling
under diverse light conditions and failing to produce accurate surfaces, while
GS-based reconstruction methods frequently compromise rendering quality. This
raises a central question: must rendering and reconstruction always involve a
trade-off? To address this, we propose MGSR, a 2D/3D Mutual-boosted Gaussian
splatting for Surface Reconstruction that enhances both rendering quality and
3D reconstruction accuracy. MGSR introduces two branches--one based on 2D-GS
and the other on 3D-GS. The 2D-GS branch excels in surface reconstruction,
providing precise geometry information to the 3D-GS branch. Leveraging this
geometry, the 3D-GS branch employs a geometry-guided illumination decomposition
module that captures reflected and transmitted components, enabling realistic
rendering under varied light conditions. Using the transmitted component as
supervision, the 2D-GS branch also achieves high-fidelity surface
reconstruction. Throughout the optimization process, the 2D-GS and 3D-GS
branches undergo alternating optimization, providing mutual supervision. Prior
to this, each branch completes an independent warm-up phase, with an early
stopping strategy implemented to reduce computational costs. We evaluate MGSR
on a diverse set of synthetic and real-world datasets, at both object and scene
levels, demonstrating strong performance in rendering and surface
reconstruction.; 42) Circular Microalgae-Based Carbon Control for Net Zero; The alteration of the climate in various areas of the world is of increasing
concern since climate stability is a necessary condition for human survival as
well as every living organism. The main reason of climate change is the
greenhouse effect caused by the accumulation of carbon dioxide in the
atmosphere. In this paper, we design a networked system underpinned by
compartmental dynamical thermodynamics to circulate the atmospheric carbon
dioxide. Specifically, in the carbon dioxide emitter compartment, we develop an
initial-condition-dependent finite-time stabilizing controller that guarantees
stability within a desired time leveraging the system property of affinity in
the control. Then, to compensate for carbon emissions we show that a
cultivation of microalgae with a volume 625 times bigger than the one of the
carbon emitter is required. To increase the carbon uptake of the microalgae, we
implement the nonaffine-in-the-control microalgae dynamical equations as an
environment of a state-of-the-art library for reinforcement learning (RL),
namely, Stable-Baselines3, and then, through the library, we test the
performance of eight RL algorithms for training a controller that maximizes the
microalgae absorption of carbon through the light intensity. All the eight
controllers increased the carbon absorption of the cultivation during a
training of 200,000 time steps with a maximum episode length of 200 time steps
and with no termination conditions. This work is a first step towards
approaching net zero as a classical and learning-based network control problem.
The source code is publicly available.; 43) DynSTG-Mamba: Dynamic Spatio-Temporal Graph Mamba with Cross-Graph
  Knowledge Distillation for Gait Disorders Recognition; Gait disorder recognition plays a crucial role in the early diagnosis and
monitoring of movement disorders. Existing approaches, including
spatio-temporal graph convolutional networks (ST-GCNs), often face high memory
demands and struggle to capture complex spatio-temporal dependencies, limiting
their efficiency in clinical applications. To address these challenges, we
introduce DynSTG-Mamba (Dynamic Spatio-Temporal Graph Mamba), a novel framework
that combines DF-STGNN and STG-Mamba to enhance motion sequence modeling. The
DF-STGNN incorporates a dynamic spatio-temporal filter that adaptively adjusts
spatial connections between skeletal joints and temporal interactions across
different movement phases. This approach ensures better feature propagation
through dynamic graph structures by considering the hierarchical nature and
dynamics of skeletal gait data. Meanwhile, STG-Mamba, an extension of Mamba
adapted for skeletal motion data, ensures a continuous propagation of states,
facilitating the capture of long-term dependencies while reducing computational
complexity. To reduce the number of model parameters and computational costs
while maintaining consistency, we propose Cross-Graph Relational Knowledge
Distillation, a novel knowledge transfer mechanism that aligns relational
information between teacher (large architecture) and student models (small
architecture) while using shared memory. This ensures that the interactions and
movement patterns of the joints are accurately preserved in the motion
sequences. We validate our DynSTG-Mamba on KOA-NM, PD-WALK, and ATAXIA
datasets, where it outperforms state-of-the-art approaches by achieving in
terms of Accuracy, F1-score, and Recall. Our results highlight the efficiency
and robustness of our approach, offering a lightweight yet highly accurate
solution for automated gait analysis and movement disorder assessment.; 44) Left Jacobson Rings; We say that a ring is strongly (resp. weakly) left Jacobson if every
semiprime (resp. prime) left ideal is an intersection of maximal left ideals.
There exist Jacobson rings that are not weakly left Jacobson, e.g. the Weyl
algebra. Our main result is the following one-sided noncommutative
Nullstellensatz: For any finite-dimensional F-algebra A the ring
A[$x_1$,...,$x_n$] of polynomials with coefficients in A is strongly left
Jacobson and every maximal left ideal of A[$x_1$,...,$x_n$] has a special form.
We also prove that an algebra that is a finitely generated module over its
center is weakly left Jacobson iff it is Jacobson, and that an Azumaya algebra
is strongly left Jacobson iff its center is Jacobson.; 45) Chirality, magic, and quantum correlations in multipartite quantum
  states; We introduce a notion of chirality for generic quantum states. A chiral state
is defined as a state which cannot be transformed into its complex conjugate in
a local product basis using local unitary operations. We introduce a number of
quantitative measures of chirality which vanish for non-chiral states. A
faithful measure called the ""chiral log-distance"" is defined in terms of the
maximum fidelity between the state and its complex conjugate under local
unitary transformations. We further introduce a set of computable measures that
involve nested commutators of modular Hamiltonians. We show general relations
between chirality and the amount of ""magic"" or non-stabilizer resource in a
state. We first show that stabilizer states of qubits are non-chiral.
Furthermore, magic monotones such as the stabilizer nullity and the stabilizer
fidelity are lower-bounded by the chiral log-distance. We further relate
chirality to discord-like quantum correlations. We define a measure of
discord-like correlations called the intrinsic interferometric power, and show
that it is lower-bounded by a nested commutator chirality measure. One
interesting aspect of chirality is that it is not monotonic under local partial
traces, indicating that it is a unique kind of resource that is not captured by
traditional quantum resource theories.; 46) HistoryPalette: Supporting Exploration and Reuse of Past Alternatives in
  Image Generation and Editing; All creative tasks require creators to iteratively produce, select, and
discard potentially useful ideas. Now, creativity tools include generative AI
features (e.g., Photoshop Generative Fill) that increase the number of
alternatives creators consider due to rapid experiments with text prompts and
random generations. Creators use tedious manual systems for organizing their
prior ideas by saving file versions or hiding layers, but they lack the support
they want for reusing prior alternatives in personal work or in communication
with others. We present HistoryPalette, a system that supports exploration and
reuse of prior designs in generative image creation and editing. Using
HistoryPalette, creators and their collaborators explore a ""palette"" of prior
design alternatives organized by spatial position, topic category, and creation
time. HistoryPalette enables creators to quickly preview and reuse their prior
work. In creative professional and client collaborator user studies,
participants generated and edited images by exploring and reusing past design
alternatives with HistoryPalette.; 47) ART: Anonymous Region Transformer for Variable Multi-Layer Transparent
  Image Generation; Multi-layer image generation is a fundamental task that enables users to
isolate, select, and edit specific image layers, thereby revolutionizing
interactions with generative models. In this paper, we introduce the Anonymous
Region Transformer (ART), which facilitates the direct generation of variable
multi-layer transparent images based on a global text prompt and an anonymous
region layout. Inspired by Schema theory suggests that knowledge is organized
in frameworks (schemas) that enable people to interpret and learn from new
information by linking it to prior knowledge.}, this anonymous region layout
allows the generative model to autonomously determine which set of visual
tokens should align with which text tokens, which is in contrast to the
previously dominant semantic layout for the image generation task. In addition,
the layer-wise region crop mechanism, which only selects the visual tokens
belonging to each anonymous region, significantly reduces attention computation
costs and enables the efficient generation of images with numerous distinct
layers (e.g., 50+). When compared to the full attention approach, our method is
over 12 times faster and exhibits fewer layer conflicts. Furthermore, we
propose a high-quality multi-layer transparent image autoencoder that supports
the direct encoding and decoding of the transparency of variable multi-layer
images in a joint manner. By enabling precise control and scalable layer
generation, ART establishes a new paradigm for interactive content creation.; 48) Harmonic And Transposition Constraints Arising From The Use Of The
  Roland TR-808 Bass Drum; The study investigates hip-hop music producer Scott Storch's approach to
tonality, where the song's key is transposed to fit the Roland TR-808 bass drum
instead of tuning the drums to the song's key. This process, involving the
adjustment of all tracks except the bass drum, suggests significant production
motives. The primary constraint stems from the limited usable pitch range of
the TR-808 bass drum if its characteristic sound is to be preserved. The
research examines drum tuning practices, the role of the Roland TR-808 in
music, and the sub-bass qualities of its bass drum. Analysis of TR-808 samples
reveals their characteristics and their integration into modern genres like
trap and hip-hop. The study also considers the impact of loudspeaker frequency
response and human ear sensitivity on bass drum perception. The findings
suggest that Storch's method prioritizes the spectral properties of the bass
drum over traditional pitch values to enhance the bass response. The need to
maintain the unique sound of the TR-808 bass drum underscores the importance of
spectral formants and register in contemporary popular music production.; 49) Revisiting the multi-planetary system of the nearby star HD 20794:
  Confirmation of a low-mass planet in the habitable zone of a nearby G-dwarf; Close-by Earth analogs and super-Earths are of primary importance because
they will be preferential targets for the next generation of direct imaging
instruments. Bright and close-by G-to-M type stars are preferential targets in
radial velocity surveys to find Earth analogs. We present an analysis of the RV
data of the star HD 20794, a target whose planetary system has been extensively
debated in the literature. The broad time span of the observations makes it
possible to find planets with signal semi-amplitudes below 1 m/s in the
habitable zone. We monitored the system with ESPRESSO. We joined ESPRESSO data
with the HARPS data, including archival data and new measurements from a recent
program. We applied the post-processing pipeline YARARA to HARPS data to
correct systematics, improve the quality of RV measurements, and mitigate the
impact of stellar activity. Results. We confirm the presence of three planets,
with periods of 18.3142 +/- 0.0022 d, 89.68 +/- 0.10 d, and 647.6 +/- 2.6 d,
along with masses of 2.15 +/- 0.17 MEarth, 2.98 +/- 0.29 MEarth, and 5.82 +/-
0.57 MEarth respectively. For the outer planet, we find an eccentricity of 0.45
+/- 0.10, whereas the inner planets are compatible with circular orbits. The
latter is likely to be a rocky planet in the habitable zone of HD 20794. From
the analysis of activity indicators, we find evidence of a magnetic cycle with
a period around 3000 d, along with evidence pointing to a rotation period
around 39 d. We have determined the presence of a system of three planets
orbiting the solar-type star HD 20794. This star is bright (V=4.34 mag) and
close (d = 6.04 pc), and HD 20794 d resides in the stellar habitable zone,
making this system a high-priority target for future atmospheric
characterization with direct imaging facilities.; 50) FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through
  Reflective Puzzle Solving; Many challenging reasoning tasks require not just rapid, intuitive responses,
but a more deliberate, multi-step approach. Recent progress in large language
models (LLMs) highlights an important shift from the ""System 1"" way of quick
reactions to the ""System 2"" style of reflection-and-correction problem solving.
However, current benchmarks heavily rely on the final-answer accuracy, leaving
much of a model's intermediate reasoning steps unexamined. This fails to assess
the model's ability to reflect and rectify mistakes within the reasoning
process. To bridge this gap, we introduce FINEREASON, a logic-puzzle benchmark
for fine-grained evaluation of LLMs' reasoning capabilities. Each puzzle can be
decomposed into atomic steps, making it ideal for rigorous validation of
intermediate correctness. Building on this, we introduce two tasks: state
checking, and state transition, for a comprehensive evaluation of how models
assess the current situation and plan the next move. To support broader
research, we also provide a puzzle training set aimed at enhancing performance
on general mathematical tasks. We show that models trained on our state
checking and transition data demonstrate gains in math reasoning by up to 5.1%
on GSM8K.; 51) Intelligent Joint Security and Delay Determinacy Performance Guarantee
  Strategy in RIS-Assisted IIoT Communication Systems; With the advancement of the Industrial Internet of Things (IIoT), IIoT
services now exhibit diverse Quality of Service (QoS) requirements in terms of
delay, determinacy, and security, which pose significant challenges for
alignment with existing network resources. Reconfigurable Intelligent Surface
(RIS), a key enabling technology for IIoT, not only optimizes signal
propagation and enhances network performance but also ensures secure
communication and deterministic delays to mitigate threats such as data leakage
and eavesdropping. In this paper, we conduct a deterministic delay analysis
under a specified decoding error rate for RIS-assisted IIoT communication
systems using Stochastic Network Calculus (SNC). We propose an on-demand joint
strategy to maximize delay determinacy while guaranteeing secure transmission
performance. This is achieved by jointly optimizing the transmit power, channel
blocklength (CBL) at the user end, and the phase shift matrix at the RIS.
Furthermore, we introduce a State Interdependence-Driven Parameterized Deep
Q-Network (SID-PDQN) algorithm to intelligently enforce on-demand performance
guarantees. Simulation results demonstrate that the proposed SID-PDQN algorithm
significantly enhances network performance compared to baseline methods such as
DQN, Dueling-DQN, and DDPG.; 52) Observation of transverse Thomson effect; The thermoelectric Thomson effect, predicted in the 1850s by William Thomson,
produces volumetric heating/cooling in a conductor due to the concerted action
of the Seebeck and Peltier effects. Recently, transverse thermoelectrics
studies on the Nernst and Ettingshausen effects have progressed rapidly to
enable versatile thermal management technologies and to explore topological
transport properties. However, a transverse Thomson effect, arising from the
concerted action of the Nernst and Ettingshausen effects, has not yet been
observed. Here, we report the observation of the transverse Thomson effect in a
conductor. We observed volumetric heating/cooling in a semimetallic
Bi$_{88}$Sb$_{12}$ alloy induced by a charge current, temperature gradient, and
magnetic field applied orthogonally to each other using thermoelectric imaging
techniques. We found that the heating/cooling can be switched by the field
direction. Our experiments and analyses reveal the essential difference between
the conventional and transverse Thomson effects; the former depends sorely on
the temperature derivative of the Seebeck coefficient, while the latter depends
not only on the temperature derivative of the Nernst coefficient but also on
its magnitude. The observation of the transverse Thomson effect fills a missing
piece in the history of thermoelectrics and provides a new principle for active
thermal management technologies.; 53) HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid
  Normalization; Transformers have become the de facto architecture for a wide range of
machine learning tasks, particularly in large language models (LLMs). Despite
their remarkable performance, challenges remain in training deep transformer
networks, especially regarding the location of layer normalization. While
Pre-Norm structures facilitate easier training due to their more prominent
identity path, they often yield suboptimal performance compared to Post-Norm.
In this paper, we propose $\textbf{HybridNorm}$, a straightforward yet
effective hybrid normalization strategy that integrates the advantages of both
Pre-Norm and Post-Norm approaches. Specifically, HybridNorm employs QKV
normalization within the attention mechanism and Post-Norm in the feed-forward
network (FFN) of each transformer block. This design not only stabilizes
training but also enhances performance, particularly in the context of LLMs.
Comprehensive experiments in both dense and sparse architectures show that
HybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches,
achieving state-of-the-art results across various benchmarks. These findings
highlight the potential of HybridNorm as a more stable and effective technique
for improving the training and performance of deep transformer models. Code is
available at https://github.com/BryceZhuo/HybridNorm.; 54) The entropy of dynamical de Sitter horizons; We propose a new formula for the entropy of a dynamical cosmological event
horizon, which is valid to leading order for perturbations of a stationary
asymptotically de Sitter spacetime. By introducing a nontrivial correction
term, we generalize Gibbons and Hawking's first law of event horizons to
non-stationary eras. We also develop the non-stationary physical process first
law between two arbitrary horizon cross-sections for the cosmological event
horizon.; 55) PhysicsGen: Can Generative Models Learn from Images to Predict Complex
  Physical Relations?; The image-to-image translation abilities of generative learning models have
recently made significant progress in the estimation of complex (steered)
mappings between image distributions. While appearance based tasks like image
in-painting or style transfer have been studied at length, we propose to
investigate the potential of generative models in the context of physical
simulations. Providing a dataset of 300k image-pairs and baseline evaluations
for three different physical simulation tasks, we propose a benchmark to
investigate the following research questions: i) are generative models able to
learn complex physical relations from input-output image pairs? ii) what
speedups can be achieved by replacing differential equation based simulations?
While baseline evaluations of different current models show the potential for
high speedups (ii), these results also show strong limitations toward the
physical correctness (i). This underlines the need for new methods to enforce
physical correctness. Data, baseline models and evaluation code
http://www.physics-gen.org.; 56) Edit as You See: Image-guided Video Editing via Masked Motion Modeling; Recent advancements in diffusion models have significantly facilitated
text-guided video editing. However, there is a relative scarcity of research on
image-guided video editing, a method that empowers users to edit videos by
merely indicating a target object in the initial frame and providing an RGB
image as reference, without relying on the text prompts. In this paper, we
propose a novel Image-guided Video Editing Diffusion model, termed IVEDiff for
the image-guided video editing. IVEDiff is built on top of image editing
models, and is equipped with learnable motion modules to maintain the temporal
consistency of edited video. Inspired by self-supervised learning concepts, we
introduce a masked motion modeling fine-tuning strategy that empowers the
motion module's capabilities for capturing inter-frame motion dynamics, while
preserving the capabilities for intra-frame semantic correlations modeling of
the base image editing model. Moreover, an optical-flow-guided motion reference
network is proposed to ensure the accurate propagation of information between
edited video frames, alleviating the misleading effects of invalid information.
We also construct a benchmark to facilitate further research. The comprehensive
experiments demonstrate that our method is able to generate temporally smooth
edited videos while robustly dealing with various editing objects with high
quality.; 57) MutualForce: Mutual-Aware Enhancement for 4D Radar-LiDAR 3D Object
  Detection; Radar and LiDAR have been widely used in autonomous driving as LiDAR provides
rich structure information, and radar demonstrates high robustness under
adverse weather. Recent studies highlight the effectiveness of fusing radar and
LiDAR point clouds. However, challenges remain due to the modality misalignment
and information loss during feature extractions. To address these issues, we
propose a 4D radar-LiDAR framework to mutually enhance their representations.
Initially, the indicative features from radar are utilized to guide both radar
and LiDAR geometric feature learning. Subsequently, to mitigate their sparsity
gap, the shape information from LiDAR is used to enrich radar BEV features.
Extensive experiments on the View-of-Delft (VoD) dataset demonstrate our
approach's superiority over existing methods, achieving the highest mAP of
71.76% across the entire area and 86.36\% within the driving corridor.
Especially for cars, we improve the AP by 4.17% and 4.20% due to the strong
indicative features and symmetric shapes.; 58) Linear-quadratic optimal control for non-exchangeable mean-field SDEs
  and applications to systemic risk; We study the linear-quadratic control problem for a class of non-exchangeable
mean-field systems, which model large populations of heterogeneous interacting
agents. We explicitly characterize the optimal control in terms of a new
infinite-dimensional system of Riccati equations, for which we establish
existence and uniqueness. To illustrate our results, we apply this framework to
a systemic risk model involving heterogeneous banks, demonstrating the impact
of agent heterogeneity on optimal risk mitigation strategies.; 59) Prime Identification and Composite Filtering Using GM-(n+1) Sequences; This paper presents a distinctive prime detection approach. This method use
GM-(n+1) sequences to effectively eliminate complex numbers. The sequences,
which consist of odd a number of (n+1), exclude all components except for the
initial prime integer. Only the first prime number is presented. This research
proposes an approach using this model to identify exceptional candidates and
examine their distribution. This study examines the interconnections among the
laws of division, basic gaps, and their applications in analytical procedures.
Computer studies may provide a novel perspective on the theory of prime
numbers, demonstrating the effectiveness of this approach in refining the
search space for primes.; 60) Enhancing Facial Expression Recognition through Dual-Direction Attention
  Mixed Feature Networks and CLIP: Application to 8th ABAW Challenge; We present our contribution to the 8th ABAW challenge at CVPR 2025, where we
tackle valence-arousal estimation, emotion recognition, and facial action unit
detection as three independent challenges. Our approach leverages the
well-known Dual-Direction Attention Mixed Feature Network (DDAMFN) for all
three tasks, achieving results that surpass the proposed baselines.
Additionally, we explore the use of CLIP for the emotion recognition challenge
as an additional experiment. We provide insights into the architectural choices
that contribute to the strong performance of our methods.; 61) Schwinger Current in de Sitter Space; We study classical background electric fields and the Schwinger effect in de
Sitter space. We show that having a constant electric field in de Sitter
requires the photon to have a tachyonic mass proportional to the Hubble scale.
This has physical implications for the induced Schwinger current which affect
its IR behaviour. To study this we recompute the Schwinger current in de Sitter
space for charged fermions and minimally coupled scalars imposing a physically
consistent renormalization condition. We find a finite and positive Schwinger
current even in the massless limit. This is in contrast to previous
calculations in the literature which found a negative IR divergence. We also
obtain the first result of the Schwinger current for a non-minimally coupled
scalar, including for a conformally coupled scalar which we find has very
similar behaviour to the fermion current. Our results may have physical
implications for both magnetogenesis and inflationary dark matter production.; 62) LLMs Can Easily Learn to Reason from Demonstrations Structure, not
  content, is what matters!; Large reasoning models (LRMs) tackle complex reasoning problems by following
long chain-of-thoughts (Long CoT) that incorporate reflection, backtracking,
and self-validation. However, the training techniques and data requirements to
elicit Long CoT remain poorly understood. In this work, we find that a Large
Language model (LLM) can effectively learn Long CoT reasoning through
data-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank
adaptation (LoRA). With just 17k long CoT training samples, the
Qwen2.5-32B-Instruct model achieves significant improvements on a wide range of
math and coding benchmarks, including 56.7% (+40.0%) on AIME 2024 and 57.0%
(+8.1%) on LiveCodeBench, competitive to the proprietary o1-preview model's
score of 44.6% and 59.1%. More importantly, we find that the structure of Long
CoT is critical to the learning process, whereas the content of individual
reasoning steps has minimal impact. Perturbations affecting content, such as
training on incorrect samples or removing reasoning keywords, have little
impact on performance. In contrast, structural modifications that disrupt
logical consistency in the Long CoT, such as shuffling or deleting reasoning
steps, significantly degrade accuracy. For example, a model trained on Long CoT
samples with incorrect answers still achieves only 3.2% lower accuracy compared
to training with fully correct samples. These insights deepen our understanding
of how to elicit reasoning capabilities in LLMs and highlight key
considerations for efficiently training the next generation of reasoning
models. This is the academic paper of our previous released Sky-T1-32B-Preview
model. Codes are available at https://github.com/NovaSky-AI/SkyThought.; 63) A Quadratic Lower Bound for Stable Roommates Solvability; In their seminal work on the Stable Marriage Problem (SM), Gale and Shapley
introduced a generalization of SM referred to as the Stable Roommates Problem
(SR). An instance of SR consists of a set of $2n$ agents, and each agent has
preferences in the form of a ranked list of all other agents. The goal is to
find a one-to-one matching between the agents that is stable in the sense that
no pair of agents have a mutual incentive to deviate from the matching. Unlike
the (bipartite) stable marriage problem, in SR, stable matchings need not
exist. Irving devised an algorithm that finds a stable matching or reports that
none exists in $O(n^2)$ time. In their influential 1989 text, Gusfield and
Irving posed the question of whether $\Omega(n^2)$ time is required for SR
solvability -- the task of deciding if an SR instance admits a stable matching.
  In this paper we provide an affirmative answer to Gusfield and Irving's
question. We show that any (randomized) algorithm that decides SR solvability
requires $\Omega(n^2)$ adaptive Boolean queries to the agents' preferences (in
expectation). Our argument follows from a reduction from the communication
complexity of the set disjointness function. The query lower bound implies
quadratic time lower bounds for Turing machines, and memory access lower bounds
for random access machines. Thus, we establish that Irving's algorithm is
optimal (up to a logarithmic factor) in a very strong sense.; 64) Large-scale Pre-training for Grounded Video Caption Generation; We propose a novel approach for captioning and object grounding in video,
where the objects in the caption are grounded in the video via temporally dense
bounding boxes. We introduce the following contributions. First, we present a
large-scale automatic annotation method that aggregates captions grounded with
bounding boxes across individual frames into temporally dense and consistent
bounding box annotations. We apply this approach on the HowTo100M dataset to
construct a large-scale pre-training dataset, named HowToGround1M. We also
introduce a Grounded Video Caption Generation model, dubbed GROVE, and
pre-train the model on HowToGround1M. Second, we introduce a new dataset,
called iGround, of 3500 videos with manually annotated captions and dense
spatio-temporally grounded bounding boxes. This allows us to measure progress
on this challenging problem, as well as to fine-tune our model on this
small-scale but high-quality data. Third, we demonstrate that our approach
achieves state-of-the-art results on the proposed iGround dataset compared to a
number of baselines, as well as on the VidSTG and ActivityNet-Entities
datasets. We perform extensive ablations that demonstrate the importance of
pre-training using our automatically annotated HowToGround1M dataset followed
by fine-tuning on the manually annotated iGround dataset and validate the key
technical contributions of our model.; 65) Hall Algebras and Edge Contractions with Loops; We extend the study of Hall algebras and edge contractions by generalizing
Yiqiang Li's work to contraction along vertices with multiple edges. Using the
edge contractions, we establish new embeddings among Hall algebras in this
broader setting. Our results demonstrate that these embeddings preserve key
algebraic structures, including Hopf algebra operations.; 66) Understanding Inverse Reinforcement Learning under Overparameterization:
  Non-Asymptotic Analysis and Global Optimality; The goal of the Inverse reinforcement learning (IRL) task is to identify the
underlying reward function and the corresponding optimal policy from a set of
expert demonstrations. While most IRL algorithms' theoretical guarantees rely
on a linear reward structure, we aim to extend the theoretical understanding of
IRL to scenarios where the reward function is parameterized by neural networks.
Meanwhile, conventional IRL algorithms usually adopt a nested structure,
leading to computational inefficiency, especially in high-dimensional settings.
To address this problem, we propose the first two-timescale single-loop IRL
algorithm under neural network parameterized reward and provide a
non-asymptotic convergence analysis under overparameterization. Although prior
optimality results for linear rewards do not apply, we show that our algorithm
can identify the globally optimal reward and policy under certain neural
network structures. This is the first IRL algorithm with a non-asymptotic
convergence guarantee that provably achieves global optimality in neural
network settings.; 67) Theoretical and Computational Approaches to Determining Sets of Orders
  for $(k,g)$-Graphs; The Cage Problem requires for a given pair $k \geq 3, g \geq 3$ of integers
the determination of the order of a smallest $k$-regular graph of girth $g$. We
address a more general version of this problem and look for the
$(k,g)$-spectrum of orders of $(k,g)$-graphs: the (infinite) list of all orders
of $(k,g)$-graphs. By establishing these spectra we aim to gain a better
understanding of the structure and properties of $(k,g)$-graphs and hope to use
the acquired knowledge in both determining new orders of smallest $k$-regular
graphs of girth $g$ as well as developing a set of tools suitable for
constructions of extremal graphs with additional requirements. We combine
theoretical results with computer-based searches, and determine or determine up
to a finite list of unresolved cases the $(k,g)$-spectra for parameter pairs
for which the orders of the corresponding cages have already been established.; 68) (Neural-Symbolic) Machine Learning for Inconsistency Measurement; We present machine-learning-based approaches for determining the
\emph{degree} of inconsistency -- which is a numerical value -- for
propositional logic knowledge bases. Specifically, we present regression- and
neural-based models that learn to predict the values that the inconsistency
measures $\incmi$ and $\incat$ would assign to propositional logic knowledge
bases. Our main motivation is that computing these values conventionally can be
hard complexity-wise. As an important addition, we use specific postulates,
that is, properties, of the underlying inconsistency measures to infer symbolic
rules, which we combine with the learning-based models in the form of
constraints. We perform various experiments and show that a) predicting the
degree values is feasible in many situations, and b) including the symbolic
constraints deduced from the rationality postulates increases the prediction
quality.; 69) Change of some cropping systems in a long-term trial comparing different
  systems: rationale and implications for statistical analysis; The project Agriculture 4.0 without chemical synthetical plant protection
(NOcsPS) tests a number of cropping systems that avoid the use of chemical
synthetical pesticides while at the same time using mineral fertilizers. The
experiment started in 2020 (sowing fall 2019). In 2024 (sowing fall 2023), some
of the cropping systems were modified. Analysis of this experiment may be done
using linear mixed models. In order to include the data from 2020-2023 in joint
analyses with the data collected for the modified systems from 2024 onwards,
the mixed modelling approach needs to be reconsidered. In this paper, we
develop models for this purpose. A key feature is the use of network
meta-analytic concepts that allow a combination of direct and indirect
comparisons among systems from the different years. The approach is first
illustrated using a toy example. This is followed by detailed analyses of data
from two the two trials sites Dahnsdorf and Hohenheim.; 70) The On-Board Computer of the AcubeSAT Mission; AcubeSAT is an open-source CubeSat mission aiming to explore the effects of
microgravity and radiation on eukaryotic cells using a compact microfluidic
lab-on-a-chip platform. It is developed by SpaceDot, a volunteer,
interdisciplinary student team at the Aristotle University of Thessaloniki and
supported by the ""Fly Your Satellite! 3"" program of the European Space Agency
(ESA) Education Office.
  The nanosatellite features an in-house designed on-board computer subsystem
responsible for telecommand execution, telemetry fetching, onboard time
synchronization, in-orbit patching, and fault recovery. The subsystem is
designed on one PC/104 standard compatible Printed Circuit Board (PCB) that
hosts the On-board Computer (OBC) on the one side and the Attitude and Orbit
Control Subsystem (AOCS) on the other, and it is compatible with the LibreCube
standard. The hosted subsystems are functionally isolated and feature an ARM
Cortex-M7, radiation-tolerant microcontroller each.
  Before sending anything to space thorough testing is required and
specifically the on-board computer board underwent vibration and thermal
cycling tests to ensure nominal operation in all conditions.
  This paper aims to elucidate the decision-making process, design iterations,
and development stages of the custom board and accompanying in-house software.
Insights garnered from the initial partially successful environmental test
campaign at the ESA CubeSat Support Facility will be shared, along with the
ensuing preparations, results, and lessons learned from subsequent testing
endeavors in April 2024. Furthermore, the current developmental status will be
discussed alongside future electromagnetic compatibility testing, integration
plan on a FlatSat, and prospects for the open-source design as a
cost-effective, and modular solution that can be tailored with little effort
for upcoming missions.; 71) Unit Region Encoding: A Unified and Compact Geometry-aware
  Representation for Floorplan Applications; We present the Unit Region Encoding of floorplans, which is a unified and
compact geometry-aware encoding representation for various applications,
ranging from interior space planning, floorplan metric learning to floorplan
generation tasks. The floorplans are represented as the latent encodings on a
set of boundary-adaptive unit region partition based on the clustering of the
proposed geometry-aware density map. The latent encodings are extracted by a
trained network (URE-Net) from the input dense density map and other available
semantic maps. Compared to the over-segmented rasterized images and the
room-level graph structures, our representation can be flexibly adapted to
different applications with the sliced unit regions while achieving higher
accuracy performance and better visual quality. We conduct a variety of
experiments and compare to the state-of-the-art methods on the aforementioned
applications to validate the superiority of our representation, as well as
extensive ablation studies to demonstrate the effect of our slicing choices.; 72) R-ParVI: Particle-based variational inference through lens of rewards; A reward-guided, gradient-free ParVI method, \textit{R-ParVI}, is proposed
for sampling partially known densities (e.g. up to a constant). R-ParVI
formulates the sampling problem as particle flow driven by rewards: particles
are drawn from a prior distribution, navigate through parameter space with
movements determined by a reward mechanism blending assessments from the target
density, with the steady state particle configuration approximating the target
geometry. Particle-environment interactions are simulated by stochastic
perturbations and the reward mechanism, which drive particles towards high
density regions while maintaining diversity (e.g. preventing from collapsing
into clusters). R-ParVI offers fast, flexible, scalable and stochastic sampling
and inference for a class of probabilistic models such as those encountered in
Bayesian inference and generative modelling.; 73) Open-Source ESP32-C3 Wi-Fi Drivers for Static Analysis; The Battery-Free Internet of Things will revolutionize our understanding of
sustainable communication, as it operates on energy harvested from the
environment (e.g., by solar cells). On these systems, harvested energy is
dependent on unpredictable environmental conditions; therefore, device
operations, including those of its networking stack, must be resilient to power
failures. Reactive intermittent computing provides an approach for solving this
by notifications of impending power failures, which is implemented by
monitoring the harvested energy buffered in a capacitor. However, to use this
power-failure notification and guarantee forward progress, systems must break
down tasks into atomic sub-tasks (transactions) that can be predictably
finished before the available energy runs out. Thus, static program-code
analysis must determine the worst-case energy consumption (WCEC) of all
transactions. In the case of Wi-Fi-capable devices, drivers are often
closed-source, and it is therefore unclear whether their transactions have a
WCEC since static analysis requires all code and its semantics.
  In this work, we integrate a transactional networking stack with
reverse-engineered Wi-Fi drivers to enable full-stack WCEC analysis for
physical transmission and reception of packets. Further, we extended a static
worst-case analysis approach with a resource-consumption model of our Wi-Fi
driver. Our evaluation with the RISC-V-based ESP32-C3 platform gives worst-case
bounds with our static analysis for the transactions of the full communications
stack, therefore showing that Wi-Fi-based reactive intermittent computing is
feasible.; 74) Classical and quantum cosmology of $f(R)$ gravity's rainbow in Schutz's
  formalism; We investigate the classical and quantum dynamics of $f(R)$ gravity's rainbow
in the presence of a perfect fluid, employing Schutz's formalism to establish a
well-defined notion of time. In the classical regime, we derive and solve the
equations of motion, obtaining both analytical and numerical solutions. Through
canonical quantisation, we formulate the Schr\""{o}dinger-Wheeler-DeWitt (SWD)
equation for the quantum model. By solving its eigenfunctions, we construct the
wave function of the Universe and obtain analytical solutions in scenarios
dominated by stiff matter. Our results highlight the impact of rainbow gravity
on quantum evolution, particularly in modifying the structure of the wave
function and shaping the transition from the quantum to the classical regime.; 75) Learning More With Less: Sample Efficient Dynamics Learning and
  Model-Based RL for Loco-Manipulation; Combining the agility of legged locomotion with the capabilities of
manipulation, loco-manipulation platforms have the potential to perform complex
tasks in real-world applications. To this end, state-of-the-art quadrupeds with
attached manipulators, such as the Boston Dynamics Spot, have emerged to
provide a capable and robust platform. However, both the complexity of
loco-manipulation control, as well as the black-box nature of commercial
platforms pose challenges for developing accurate dynamics models and control
policies. We address these challenges by developing a hand-crafted kinematic
model for a quadruped-with-arm platform and, together with recent advances in
Bayesian Neural Network (BNN)-based dynamics learning using physical priors,
efficiently learn an accurate dynamics model from data. We then derive control
policies for loco-manipulation via model-based reinforcement learning (RL). We
demonstrate the effectiveness of this approach on hardware using the Boston
Dynamics Spot with a manipulator, accurately performing dynamic end-effector
trajectory tracking even in low data regimes.; 76) Regular singular Mahler equations and Newton polygons; Though Mahler equations have been introduced nearly one century ago, the
study of their solutions is still a fruitful topic for research. In particular,
the Galois theory of Mahler equations has been the subject of many recent
papers. Nevertheless, long is the way to a complete understanding of relations
between solutions of Mahler equations. One step along this way is the study of
singularities. Mahler equations with a regular singularity at 0 have rather
""nice"" solutions: they can be expressed with the help of Puiseux series and
solutions of equations with constant coefficients. In [FP22] the authors
described an algorithm to determine whether an equation is regular singular at
0 or not. Exploiting information from the Frobenius method and Newton polygons,
we improve this algorithm by reducing significantly its complexity, by
providing some simple criterion for an equation to be regular singular at 0 and
by extending its scope to equations with Puiseux coefficients.; 77) LookCloser: Frequency-aware Radiance Field for Tiny-Detail Scene; Humans perceive and comprehend their surroundings through information
spanning multiple frequencies. In immersive scenes, people naturally scan their
environment to grasp its overall structure while examining fine details of
objects that capture their attention. However, current NeRF frameworks
primarily focus on modeling either high-frequency local views or the broad
structure of scenes with low-frequency information, which is limited to
balancing both. We introduce FA-NeRF, a novel frequency-aware framework for
view synthesis that simultaneously captures the overall scene structure and
high-definition details within a single NeRF model. To achieve this, we propose
a 3D frequency quantification method that analyzes the scene's frequency
distribution, enabling frequency-aware rendering. Our framework incorporates a
frequency grid for fast convergence and querying, a frequency-aware feature
re-weighting strategy to balance features across different frequency contents.
Extensive experiments show that our method significantly outperforms existing
approaches in modeling entire scenes while preserving fine details.; 78) Monitoring Reasoning Models for Misbehavior and the Risks of Promoting
  Obfuscation; Mitigating reward hacking--where AI systems misbehave due to flaws or
misspecifications in their learning objectives--remains a key challenge in
constructing capable and aligned models. We show that we can monitor a frontier
reasoning model, such as OpenAI o3-mini, for reward hacking in agentic coding
environments by using another LLM that observes the model's chain-of-thought
(CoT) reasoning. CoT monitoring can be far more effective than monitoring agent
actions and outputs alone, and we further found that a LLM weaker than o3-mini,
namely GPT-4o, can effectively monitor a stronger model. Because CoT monitors
can be effective at detecting exploits, it is natural to ask whether those
exploits can be suppressed by incorporating a CoT monitor directly into the
agent's training objective. While we show that integrating CoT monitors into
the reinforcement learning reward can indeed produce more capable and more
aligned agents in the low optimization regime, we find that with too much
optimization, agents learn obfuscated reward hacking, hiding their intent
within the CoT while still exhibiting a significant rate of reward hacking.
Because it is difficult to tell when CoTs have become obfuscated, it may be
necessary to pay a monitorability tax by not applying strong optimization
pressures directly to the chain-of-thought, ensuring that CoTs remain
monitorable and useful for detecting misaligned behavior.; 79) Hybrid confinement techniques for polariton simulators; Exciton-polaritons in III-V semiconductor microcavities offer a robust
platform for emulating complex Hamiltonians, enabling advancements in photonic
applications and quantum simulation. Here, two novel fabrication techniques
designed to overcome the limitations of traditional photonic confinement
methods are introduced. The two distinct approaches - etch-and-oversputter
(EnS) and deposit-and-oversputter (DnS) - are both based on a structured,
locally elongated semiconductor cavity, leading to a deep and highly
controllable spatially dependent potential. By utilizing an all-dielectric
sputtered top mirror, sample iteration time, workflow complexity and expense is
reduced while increasing the overall yield compared to methods such as deep ion
etching. Employing a Kagome lattice and its flatband and Dirac-cone dispersions
as a benchmark, high quality optical band structures are achieved, which so far
have not been realized using a deep etching approach. To highlight the precise
control over the lattice couplings, the eigenmodes in a two-dimensional
breathing Kagome lattice are studied and polariton lasing from a
zerodimensional corner mode is observed. This confirms the effectiveness of the
methods presented in this paper for generating well-controlled, deep and
homogeneous trapping potentials. These pave the way for fabricating intricate
lattices, such as higher-order topological insulators, or on-chip quantum
emitters utilizing the polariton blockade mechanism.; 80) UniCoRN: Unified Commented Retrieval Network with LMMs; Multimodal retrieval methods have limitations in handling complex,
compositional queries that require reasoning about the visual content of both
the query and the retrieved entities. On the other hand, Large Multimodal
Models (LMMs) can answer with language to more complex visual questions, but
without the inherent ability to retrieve relevant entities to support their
answers. We aim to address these limitations with UniCoRN, a Unified Commented
Retrieval Network that combines the strengths of composed multimodal retrieval
methods and generative language approaches, going beyond Retrieval-Augmented
Generation (RAG). We introduce an entity adapter module to inject the retrieved
multimodal entities back into the LMM, so it can attend to them while
generating answers and comments. By keeping the base LMM frozen, UniCoRN
preserves its original capabilities while being able to perform both retrieval
and text generation tasks under a single integrated framework. To assess these
new abilities, we introduce the Commented Retrieval task (CoR) and a
corresponding dataset, with the goal of retrieving an image that accurately
answers a given question and generate an additional textual response that
provides further clarification and details about the visual information. We
demonstrate the effectiveness of UniCoRN on several datasets showing
improvements of +4.5% recall over the state of the art for composed multimodal
retrieval and of +14.9% METEOR / +18.4% BEM over RAG for commenting in CoR.; 81) Using Causal Inference to Explore Government Policy Impact on Computer
  Usage; We explore the causal relationship between COVID-19 lockdown policies and
changes in personal computer usage. In particular, we examine how lockdown
policies affected average daily computer usage, as well as how it affected
usage patterns of different groups of users. This is done through a merging of
the Oxford Policy public data set, which describes the timeline of
implementation of COVID policies across the world, and a collection of Intel's
Data Collection and Analytics (DCA) telemetry data, which includes millions of
computer usage records and updates daily. Through difference-in-difference,
synthetic control, and change-point detection algorithms, we identify causal
links between the increase in intensity (watts) and time (hours) of computer
usage and the implementation of work from home policy. We also show an
interesting trend in the individual's computer usage affected by the policy. We
also conclude that computer usage behaviors are much less predictable during
reduction in COVID lockdown policies than during increases in COVID lockdown
policies.; 82) Fidelity-Enhanced Variational Quantum Optimal Control; Creating robust quantum operations is a major challenge in the current noisy
intermediate-scale quantum computing era. Recently, the importance of
noise-resilient control methods has become more pronounced in the field.
Ordinarily, noisy quantum systems are described by the Lindblad equation.
However, minimizing noise susceptibility using this equation has proven
challenging because of its irreversibility. In this study, we propose a new
method for creating robust pulses based on the stochastic Schr\""{o}dinger
equation. This equation describes individual noise realizations under any
colored noise process, contrary to the Lindblad equation, which describes mean
system behavior under white noise. Using stochastic optimal control techniques,
our method, Fidelity-Enhanced Variational Quantum Optimal Control (F-VQOC), is
able to construct higher fidelity paths than its non-stochastic counterpart
(VQOC). By accounting for both environmental noise sources as well as noise
sources inherent to the control system, highly significant increases in
fidelity are noted for both single and multiqubit state preparations.; 83) The influence of Wilson lines on heavy quark anti-quark potential and
  mass; The holographic heavy quark potential is investigated via holographic Wilson
loops in the AdS soliton with gauge potential. We analyze two types of
holographic Wilson loops. {In the first type, holographic heavy quark potential
shows the area law behavior. In the second type, the potential becomes zero at
a critical length and physics analogous to the dissociation occurs. The mass of
heavy quarkonia and the binding energy are examined.} Lastly, the mass of
$0^{++}$ glueball-like operators dual to massless dilaton is calculated. The
mass of $0^{++}$ glueball-like operator decreases with increase of the gauge
potential as expected in arXiv:2309.03491 [hep-th]. The results are comparable
with lattice QCD.; 84) Understanding the Impact of Artificial Intelligence in Academic Writing:
  Metadata to the Rescue; This column advocates for including artificial intelligence (AI)-specific
metadata on those academic papers that are written with the help of AI in an
attempt to analyze the use of such tools for disseminating research.; 85) Towards Robust and Realistic Human Pose Estimation via WiFi Signals; Robust WiFi-based human pose estimation is a challenging task that bridges
discrete and subtle WiFi signals to human skeletons. This paper revisits this
problem and reveals two critical yet overlooked issues: 1) cross-domain gap,
i.e., due to significant variations between source-target domain pose
distributions; and 2) structural fidelity gap, i.e., predicted skeletal poses
manifest distorted topology, usually with misplaced joints and disproportionate
bone lengths. This paper fills these gaps by reformulating the task into a
novel two-phase framework dubbed DT-Pose: Domain-consistent representation
learning and Topology-constrained Pose decoding. Concretely, we first propose a
temporal-consistent contrastive learning strategy with uniformity
regularization, coupled with self-supervised masking-reconstruction operations,
to enable robust learning of domain-consistent and motion-discriminative
WiFi-specific representations. Beyond this, we introduce a simple yet effective
pose decoder with task prompts, which integrates Graph Convolution Network
(GCN) and Transformer layers to constrain the topology structure of the
generated skeleton by exploring the adjacent-overarching relationships among
human joints. Extensive experiments conducted on various benchmark datasets
highlight the superior performance of our method in tackling these fundamental
challenges in both 2D/3D human pose estimation tasks.; 86) DenseSplat: Densifying Gaussian Splatting SLAM with Neural Radiance
  Prior; Gaussian SLAM systems excel in real-time rendering and fine-grained
reconstruction compared to NeRF-based systems. However, their reliance on
extensive keyframes is impractical for deployment in real-world robotic
systems, which typically operate under sparse-view conditions that can result
in substantial holes in the map. To address these challenges, we introduce
DenseSplat, the first SLAM system that effectively combines the advantages of
NeRF and 3DGS. DenseSplat utilizes sparse keyframes and NeRF priors for
initializing primitives that densely populate maps and seamlessly fill gaps. It
also implements geometry-aware primitive sampling and pruning strategies to
manage granularity and enhance rendering efficiency. Moreover, DenseSplat
integrates loop closure and bundle adjustment, significantly enhancing
frame-to-frame tracking accuracy. Extensive experiments on multiple large-scale
datasets demonstrate that DenseSplat achieves superior performance in tracking
and mapping compared to current state-of-the-art methods.; 87) Gaussian Graph Network: Learning Efficient and Generalizable Gaussian
  Representations from Multi-view Images; 3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis
performance. While conventional methods require per-scene optimization, more
recently several feed-forward methods have been proposed to generate
pixel-aligned Gaussian representations with a learnable network, which are
generalizable to different scenes. However, these methods simply combine
pixel-aligned Gaussians from multiple views as scene representations, thereby
leading to artifacts and extra memory cost without fully capturing the
relations of Gaussians from different images. In this paper, we propose
Gaussian Graph Network (GGN) to generate efficient and generalizable Gaussian
representations. Specifically, we construct Gaussian Graphs to model the
relations of Gaussian groups from different views. To support message passing
at Gaussian level, we reformulate the basic graph operations over Gaussian
representations, enabling each Gaussian to benefit from its connected Gaussian
groups with Gaussian feature fusion. Furthermore, we design a Gaussian pooling
layer to aggregate various Gaussian groups for efficient representations. We
conduct experiments on the large-scale RealEstate10K and ACID datasets to
demonstrate the efficiency and generalization of our method. Compared to the
state-of-the-art methods, our model uses fewer Gaussians and achieves better
image quality with higher rendering speed.; 88) Multimodal Large Language Models for Image, Text, and Speech Data
  Augmentation: A Survey; In the past five years, research has shifted from traditional Machine
Learning (ML) and Deep Learning (DL) approaches to leveraging Large Language
Models (LLMs) , including multimodality, for data augmentation to enhance
generalization, and combat overfitting in training deep convolutional neural
networks. However, while existing surveys predominantly focus on ML and DL
techniques or limited modalities (text or images), a gap remains in addressing
the latest advancements and multi-modal applications of LLM-based methods. This
survey fills that gap by exploring recent literature utilizing multimodal LLMs
to augment image, text, and audio data, offering a comprehensive understanding
of these processes. We outlined various methods employed in the LLM-based
image, text and speech augmentation, and discussed the limitations identified
in current approaches. Additionally, we identified potential solutions to these
limitations from the literature to enhance the efficacy of data augmentation
practices using multimodal LLMs. This survey serves as a foundation for future
research, aiming to refine and expand the use of multimodal LLMs in enhancing
dataset quality and diversity for deep learning applications. (Surveyed Paper
GitHub Repo: https://github.com/WSUAgRobotics/data-aug-multi-modal-llm.
Keywords: LLM data augmentation, Grok text data augmentation, DeepSeek image
data augmentation, Grok speech data augmentation, GPT audio augmentation, voice
augmentation, DeepSeek for data augmentation, DeepSeek R1 text data
augmentation, DeepSeek R1 image augmentation, Image Augmentation using LLM,
Text Augmentation using LLM, LLM data augmentation for deep learning
applications); 89) A Robust Remote Photoplethysmography Method; Remote photoplethysmography (rPPG) is a method for measuring a subjects heart
rate remotely using a camera. Factors such as subject movement, ambient light
level, makeup etc. complicate such measurements by distorting the observed
pulse. Recent works on this topic have proposed a variety of approaches for
accurately measuring heart rate in humans, however these methods were tested in
ideal conditions, where the subject does not make significant movements and all
measurements are taken at the same level of illumination. In more realistic
conditions these methods suffer from decreased accuracy. The study proposes a
more robust method that is less susceptible to distortions and has minimal
hardware requirements. The proposed method uses a combination of mathematical
transforms to calculate the subjects heart rate. It performs best when used
with a camera that has been modified by removing its infrared filter, although
using an unmodified camera is also possible. The method was tested on 26 videos
taken from 19 volunteers of varying gender and age. The obtained results were
compared to reference data and the average mean absolute error was found to be
at 1.95 beats per minute, which is noticeably better than the results from
previous works. The remote photoplethysmography method proposed in the present
article is more resistant to distortions than methods from previous
publications and thus allows one to remotely and accurately measure the
subjects heart rate without imposing any significant limitations on the
subjects behavior.; 90) Secure On-Device Video OOD Detection Without Backpropagation; Out-of-Distribution (OOD) detection is critical for ensuring the reliability
of machine learning models in safety-critical applications such as autonomous
driving and medical diagnosis. While deploying personalized OOD detection
directly on edge devices is desirable, it remains challenging due to large
model sizes and the computational infeasibility of on-device training.
Federated learning partially addresses this but still requires gradient
computation and backpropagation, exceeding the capabilities of many edge
devices. To overcome these challenges, we propose SecDOOD, a secure
cloud-device collaboration framework for efficient on-device OOD detection
without requiring device-side backpropagation. SecDOOD utilizes cloud resources
for model training while ensuring user data privacy by retaining sensitive
information on-device. Central to SecDOOD is a HyperNetwork-based personalized
parameter generation module, which adapts cloud-trained models to
device-specific distributions by dynamically generating local weight
adjustments, effectively combining central and local information without local
fine-tuning. Additionally, our dynamic feature sampling and encryption strategy
selectively encrypts only the most informative feature channels, largely
reducing encryption overhead without compromising detection performance.
Extensive experiments across multiple datasets and OOD scenarios demonstrate
that SecDOOD achieves performance comparable to fully fine-tuned models,
enabling secure, efficient, and personalized OOD detection on resource-limited
edge devices. To enhance accessibility and reproducibility, our code is
publicly available at https://github.com/Dystopians/SecDOOD.; 91) EVE: Towards End-to-End Video Subtitle Extraction with Vision-Language
  Models; The advent of Large Vision-Language Models (LVLMs) has advanced the
video-based tasks, such as video captioning and video understanding. Some
previous research indicates that taking texts in videos as input can further
improve the performance of video understanding. As a type of indispensable
information in short videos or movies, subtitles can assist LVLMs to better
understand videos. Most existing methods for video subtitle extraction are
based on a multi-stage framework, handling each frame independently. They can
hardly exploit the temporal information of videos. Although some LVLMs exhibit
the robust OCR capability, predicting accurate timestamps for subtitle texts is
still challenging. In this paper, we propose an End-to-end Video Subtitle
Extraction method, called EVE, which consists of three modules: a vision
encoder, an adapter module, and a large language model. To effectively compress
the visual tokens from the vision encoder, we propose a novel adapter
InterleavedVT to interleave two modalities. It contains a visual compressor and
a textual region compressor. The proposed InterleavedVT exploits both the
merits of average pooling and Q-Former in token compression. Taking the
temporal information of videos into account, we introduce a sliding-window
mechanism in the textual region compressor. To benchmark the video subtitle
extraction task, we propose a large dataset ViSa including 2.5M videos.
Extensive experiments on ViSa demonstrate that the proposed EVE can outperform
existing open-sourced tools and LVLMs.; 92) VideoMAP: Toward Scalable Mamba-based Video Autoregressive Pretraining; Recent Mamba-based architectures for video understanding demonstrate
promising computational efficiency and competitive performance, yet struggle
with overfitting issues that hinder their scalability. To overcome this
challenge, we introduce VideoMAP, a Hybrid Mamba-Transformer framework
featuring a novel pre-training approach. VideoMAP uses a 4:1
Mamba-to-Transformer ratio, effectively balancing computational cost and model
capacity. This architecture, combined with our proposed frame-wise masked
autoregressive pre-training strategy, delivers significant performance gains
when scaling to larger models. Additionally, VideoMAP exhibits impressive
sample efficiency, significantly outperforming existing methods with less
training data. Experiments show that VideoMAP outperforms existing models
across various datasets, including Kinetics-400, Something-Something V2,
Breakfast, and COIN. Furthermore, we demonstrate the potential of VideoMAP as a
visual encoder for multimodal large language models, highlighting its ability
to reduce memory usage and enable the processing of longer video sequences. The
code is open-source at https://github.com/yunzeliu/MAP; 93) Simulating ULXs and blazars as GRMHD accretion flows around a black hole; General relativistic magnetohydrodynamic (GRMHD) simulations have been
instrumental in our understanding of high energy astrophysical phenomena over
the past two decades. Their robustness and modularity make them a great tool
for understanding the dynamics of various astrophysical objects. In this paper
we have used GRMHD simulations to understand the accretion flows of
ultraluminous X-ray sources (ULXs) and blazars. ULXs are enigmatic sources
which exhibit very high luminosities (super-Eddington for stellar mass black
holes) even in their low-hard state. Numerical steady state calculations have
shown that this behaviour can be explained by considering ULXs to be highly
magnetised advective accretion sources around stellar-mass black holes. Our
simulation confirms that such an accretion flow can indeed produce the high
luminosities observed in ULXs. Further to continue towards the supermassive
black holes, we have also modeled blazars and have used our simulation results
to explain the apparent dichotomy in the two blazar classes: flat spectrum
radio quasars (FSRQs) and BL Lacertae (BL Lacs). Our results show that FSRQ and
BL Lacs show different spectral characteristics due to a difference in their
magnetic field characteristics. The different categories of FSRQs and BL Lacs
have also been explained by the interplay between the spin, magnetic field and
accretion rate of the central supermassive black hole.; 94) Frequency Matters: Explaining Biases of Face Recognition in the
  Frequency Domain; Face recognition (FR) models are vulnerable to performance variations across
demographic groups. The causes for these performance differences are unclear
due to the highly complex deep learning-based structure of face recognition
models. Several works aimed at exploring possible roots of gender and ethnicity
bias, identifying semantic reasons such as hairstyle, make-up, or facial hair
as possible sources. Motivated by recent discoveries of the importance of
frequency patterns in convolutional neural networks, we explain bias in face
recognition using state-of-the-art frequency-based explanations. Our extensive
results show that different frequencies are important to FR models depending on
the ethnicity of the samples.; 95) MaTVLM: Hybrid Mamba-Transformer for Efficient Vision-Language Modeling; With the advancement of RNN models with linear complexity, the quadratic
complexity challenge of transformers has the potential to be overcome. Notably,
the emerging Mamba-2 has demonstrated competitive performance, bridging the gap
between RNN models and transformers. However, due to sequential processing and
vanishing gradients, RNN models struggle to capture long-range dependencies,
limiting contextual understanding. This results in slow convergence, high
resource demands, and poor performance on downstream understanding and complex
reasoning tasks. In this work, we present a hybrid model MaTVLM by substituting
a portion of the transformer decoder layers in a pre-trained VLM with Mamba-2
layers. Leveraging the inherent relationship between attention and Mamba-2, we
initialize Mamba-2 with corresponding attention weights to accelerate
convergence. Subsequently, we employ a single-stage distillation process, using
the pre-trained VLM as the teacher model to transfer knowledge to the MaTVLM,
further enhancing convergence speed and performance. Furthermore, we
investigate the impact of differential distillation loss within our training
framework. We evaluate the MaTVLM on multiple benchmarks, demonstrating
competitive performance against the teacher model and existing VLMs while
surpassing both Mamba-based VLMs and models of comparable parameter scales.
Remarkably, the MaTVLM achieves up to 3.6x faster inference than the teacher
model while reducing GPU memory consumption by 27.5%, all without compromising
performance. Code and models are released at http://github.com/hustvl/MaTVLM.; 96) SFMNet: Sparse Focal Modulation for 3D Object Detection; We propose SFMNet, a novel 3D sparse detector that combines the efficiency of
sparse convolutions with the ability to model long-range dependencies. While
traditional sparse convolution techniques efficiently capture local structures,
they struggle with modeling long-range relationships. However, capturing
long-range dependencies is fundamental for 3D object detection. In contrast,
transformers are designed to capture these long-range dependencies through
attention mechanisms. But, they come with high computational costs, due to
their quadratic query-key-value interactions. Furthermore, directly applying
attention to non-empty voxels is inefficient due to the sparse nature of 3D
scenes. Our SFMNet is built on a novel Sparse Focal Modulation (SFM) module,
which integrates short- and long-range contexts with linear complexity by
leveraging a new hierarchical sparse convolution design. This approach enables
SFMNet to achieve high detection performance with improved efficiency, making
it well-suited for large-scale LiDAR scenes. We show that our detector achieves
state-of-the-art performance on autonomous driving datasets.; 97) Consistent sampling of Paley-Wiener functions on graphons; We study sampling methods for Paley-Wiener functions on graphons, thereby
adapting and generalizing methods initially developed for graphs to the graphon
setting. We then derive conditions under which such a sampling estimate is
consistent with graphon convergence.; 98) Operation above the Greenwald density limit in high performance DIII-D
  negative triangularity discharges; The density limit in strongly-shaped negative triangularity (NT) discharges
is studied experimentally in the DIII-D tokamak. Record-high Greenwald
fractions $f_G$ are obtained, using gas puff injection only, with values up to
near 2, where $f_G$ is defined as the ratio of the line-averaged density over
$n_G=I_p/(\pi\,a^2)$, with $I_p$[MA] the plasma current and $a$[m] the plasma
minor radius. A clear higher operational limit with higher auxiliary power is
also demonstrated, with the ohmic density limit about two times lower than with
additional neutral beam injection heating. The evolution of the electron
density, temperature and pressure profiles are analyzed as well. The core
density can be up to twice the Greenwald density and keeps increasing, while
the value at the separatrix remains essentially constant and slightly below
$n_G$. The edge temperature gradient collapses to near zero and NT plasmas are
shown to be resilient to such profiles in terms of disruptivity. We also
present the time evolution of the inverse electron pressure scale length with
the value at the last closed flux surface (LCFS) decreasing below the value at
the normalized radius 0.9 near the density limit, demonstrating the clear drop
of confinement starting from the edge. This inverse scale length ``collapse''
at the LCFS also defines well the characteristic behavior of the kinetic
profiles approaching a density limit.; 99) Synthetic generation of 2D data records based on Autoencoders; Gas Chromatography coupled with Ion Mobility Spectrometry (GC-IMS) is a
dual-separation analytical technique widely used for identifying components in
gaseous samples by separating and analysing the arrival times of their
constituent species. Data generated by GC-IMS is typically represented as
two-dimensional spectra, providing rich information but posing challenges for
data-driven analysis due to limited labelled datasets. This study introduces a
novel method for generating synthetic 2D spectra using a deep learning
framework based on Autoencoders. Although applied here to GC-IMS data, the
approach is broadly applicable to any two-dimensional spectral measurements
where labelled data are scarce. While performing component classification over
a labelled dataset of GC-IMS records, the addition of synthesized records
significantly has improved the classification performance, demonstrating the
method's potential for overcoming dataset limitations in machine learning
frameworks.; 100) Nanoscale functionalization of MoS$_2$ monolayers with DNA origami; The functionalization of two-dimensional (2D) materials with organic
molecules is a promising approach for realizing nanoscale optoelectronic
devices with tailored functionalities, such as quantum light generation and p-n
junctions. However, achieving control over the molecules' precise positioning
on the 2D material remains a significant challenge. Here, we overcome the
limitations of solution- and vapor deposition methods and use a DNA origami
placement technique to spatially arrange various organic molecules on a chip
surface at the single-molecule level with high assembly yields. This versatile
method allows for precise patterning of transition metal dichalcogenides (TMDs)
with organic molecules, including thiols and fluorescent dyes. We successfully
integrated MoS$_2$ monolayers with micron-scale molecule-origami patterns
achieving both single photon emission from thiol-induced localized excitons in
MoS$_2$ and photoexcitation energy transfer with patterned fluorescent dyes.
Our approach offers a pathway for producing complex, tailored 2D
inorganic-organic heterostructures with molecular-level control, opening up new
possibilities for advanced materials and device design.",0.25,0.0
2411.00561,applied,2411.00561-pos1-8,"What is a cell type, really? The quest to categorize life’s myriad forms; The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. “I realized that there were probably hundreds of flavours,” recalls Buenrostro, who was a graduate student at Stanford University in California at the time.",2411.00561-pos2-8,"Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study; In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric.",26,"['19', '1', '15', '3', '12', '5', '4', '60', '28', '9']","The best candidate paper that aligns with the main paper on cell types is number 19, which discusses a novel supervised learning classifier inspired by biological processes. This paper presents an innovative approach that could enhance our understanding of various cell types, tying back to the main paper's focus on categorizing and understanding cell types through genetic and biological frameworks. The other candidate papers, while interesting, do not provide as direct a connection to the themes of understanding biological diversity and the classification of cellular structures as paper 19 does. Therefore, this multidisciplinary approach can lead to innovative applications in both computational biology and machine learning.","1) Frequency Diverse Array OFDM Transmit System with Partial Overlap in
  Frequency; Frequency-diverse array (FDA) is an alternative array architecture in which
each antenna is preceded by a mixer instead of a phase shifter. The mixers
introduce a frequency offset between signals transmitted by each antenna
resulting in a time-varying beam pattern. However, time-dependent beamforming
is not desirable for communication or sensing. In this paper, the FDA is
combined with orthogonal frequency-division multiplexing (OFDM) modulation. The
proposed beamforming method splits the OFDM symbol transmitted by all antennas
into subcarrier blocks, which are precoded differently. The selected frequency
offset between the antennas results in overlap and coherent summation of the
differently precoded subcarrier blocks. This allows to achieve fully digital
beamforming over a single block with the use of a single digital-to-analog
converter. The system's joint communication and sensing performance is
evaluated and sensitivity to errors is studied.; 2) V2X-LLM: Enhancing V2X Integration and Understanding in Connected
  Vehicle Corridors; The advancement of Connected and Automated Vehicles (CAVs) and
Vehicle-to-Everything (V2X) offers significant potential for enhancing
transportation safety, mobility, and sustainability. However, the integration
and analysis of the diverse and voluminous V2X data, including Basic Safety
Messages (BSMs) and Signal Phase and Timing (SPaT) data, present substantial
challenges, especially on Connected Vehicle Corridors. These challenges include
managing large data volumes, ensuring real-time data integration, and
understanding complex traffic scenarios. Although these projects have developed
an advanced CAV data pipeline that enables real-time communication between
vehicles, infrastructure, and other road users for managing connected vehicle
and roadside unit (RSU) data, significant hurdles in data comprehension and
real-time scenario analysis and reasoning persist. To address these issues, we
introduce the V2X-LLM framework, a novel enhancement to the existing CV data
pipeline. V2X-LLM leverages Large Language Models (LLMs) to improve the
understanding and real-time analysis of V2X data. The framework includes four
key tasks: Scenario Explanation, offering detailed narratives of traffic
conditions; V2X Data Description, detailing vehicle and infrastructure
statuses; State Prediction, forecasting future traffic states; and Navigation
Advisory, providing optimized routing instructions. By integrating LLM-driven
reasoning with V2X data within the data pipeline, the V2X-LLM framework offers
real-time feedback and decision support for traffic management. This
integration enhances the accuracy of traffic analysis, safety, and traffic
optimization. Demonstrations in a real-world urban corridor highlight the
framework's potential to advance intelligent transportation systems.; 3) Species of Rota-Baxter algebras by rooted trees, twisted bialgebras and
  Fock functors; As a fundamental and ubiquitous combinatorial notion, species has attracted
sustained interest, generalizing from set-theoretical combinatorial to
algebraic combinatorial and beyond. The Rota-Baxter algebra is one of the
algebraic structures with broad applications from Renormalization of quantum
field theory to integrable systems and multiple zeta values. Its interpretation
in terms of monoidal categories has also recently appeared. This paper studies
species of Rota-Baxter algebras, making use of the combinatorial construction
of free Rota-Baxter algebras in terms of angularly decorated trees and forests.
The notion of simple angularly decorated forests is introduced for this purpose
and the resulting Rota-Baxter species is shown to be free. Furthermore, a
twisted bialgebra structure, as the bialgebra for species, is established on
this free Rota-Baxter species. Finally, through the Fock functor, another proof
of the bialgebra structure on free Rota-Baxter algebras is obtained.; 4) An algorithm for computing generalized Hamming weights and the Sage
  package GHWs; We generalize the Brouwer-Zimmermann algorithm, which is the most efficient
algorithm for computing the minimum distance of a random linear code, to the
case of generalized Hamming weights. We also adapt this algorithm to compute
the relative generalized Hamming weights of a nested pair of linear codes. In
the package GHWs we provide an implementation of this algorithm in Sage, as
well as several other utilities for working with generalized Hamming weights.
With this implementation, we show that the proposed algorithm is faster than
the naive approach of computing the generalized Hamming weights using the
definition.; 5) ChartCitor: Multi-Agent Framework for Fine-Grained Chart Visual
  Attribution; Large Language Models (LLMs) can perform chart question-answering tasks but
often generate unverified hallucinated responses. Existing answer attribution
methods struggle to ground responses in source charts due to limited
visual-semantic context, complex visual-text alignment requirements, and
difficulties in bounding box prediction across complex layouts. We present
ChartCitor, a multi-agent framework that provides fine-grained bounding box
citations by identifying supporting evidence within chart images. The system
orchestrates LLM agents to perform chart-to-table extraction, answer
reformulation, table augmentation, evidence retrieval through pre-filtering and
re-ranking, and table-to-chart mapping. ChartCitor outperforms existing
baselines across different chart types. Qualitative user studies show that
ChartCitor helps increase user trust in Generative AI by providing enhanced
explainability for LLM-assisted chart QA and enables professionals to be more
productive.; 6) Rethinking Relation Extraction: Beyond Shortcuts to Generalization with
  a Debiased Benchmark; Benchmarks are crucial for evaluating machine learning algorithm performance,
facilitating comparison and identifying superior solutions. However, biases
within datasets can lead models to learn shortcut patterns, resulting in
inaccurate assessments and hindering real-world applicability. This paper
addresses the issue of entity bias in relation extraction tasks, where models
tend to rely on entity mentions rather than context. We propose a debiased
relation extraction benchmark DREB that breaks the pseudo-correlation between
entity mentions and relation types through entity replacement. DREB utilizes
Bias Evaluator and PPL Evaluator to ensure low bias and high naturalness,
providing a reliable and accurate assessment of model generalization in entity
bias scenarios. To establish a new baseline on DREB, we introduce MixDebias, a
debiasing method combining data-level and model training-level techniques.
MixDebias effectively improves model performance on DREB while maintaining
performance on the original dataset. Extensive experiments demonstrate the
effectiveness and robustness of MixDebias compared to existing methods,
highlighting its potential for improving the generalization ability of relation
extraction models. We will release DREB and MixDebias publicly.; 7) Observer-Aware Probabilistic Planning Under Partial Observability; In this article, we are interested in planning problems where the agent is
aware of the presence of an observer, and where this observer is in a partial
observability situation. The agent has to choose its strategy so as to optimize
the information transmitted by observations. Building on observer-aware Markov
decision processes (OAMDPs), we propose a framework to handle this type of
problems and thus formalize properties such as legibility, explicability and
predictability. This extension of OAMDPs to partial observability can not only
handle more realistic problems, but also permits considering dynamic hidden
variables of interest. These dynamic target variables allow, for instance,
working with predictability, or with legibility problems where the goal might
change during execution. We discuss theoretical properties of PO-OAMDPs and,
experimenting with benchmark problems, we analyze HSVI's convergence behavior
with dedicated initializations and study the resulting strategies.; 8) Beyond Text: Implementing Multimodal Large Language Model-Powered
  Multi-Agent Systems Using a No-Code Platform; This study proposes the design and implementation of a multimodal LLM-based
Multi-Agent System (MAS) leveraging a No-Code platform to address the practical
constraints and significant entry barriers associated with AI adoption in
enterprises. Advanced AI technologies, such as Large Language Models (LLMs),
often pose challenges due to their technical complexity and high implementation
costs, making them difficult for many organizations to adopt. To overcome these
limitations, this research develops a No-Code-based Multi-Agent System designed
to enable users without programming knowledge to easily build and manage AI
systems. The study examines various use cases to validate the applicability of
AI in business processes, including code generation from image-based notes,
Advanced RAG-based question-answering systems, text-based image generation, and
video generation using images and prompts. These systems lower the barriers to
AI adoption, empowering not only professional developers but also general users
to harness AI for significantly improved productivity and efficiency. By
demonstrating the scalability and accessibility of No-Code platforms, this
study advances the democratization of AI technologies within enterprises and
validates the practical applicability of Multi-Agent Systems, ultimately
contributing to the widespread adoption of AI across various industries.; 9) Fundamental Limitations in Defending LLM Finetuning APIs; LLM developers have imposed technical interventions to prevent fine-tuning
misuse attacks, attacks where adversaries evade safeguards by fine-tuning the
model using a public API. Previous work has established several successful
attacks against specific fine-tuning API defences. In this work, we show that
defences of fine-tuning APIs that seek to detect individual harmful training or
inference samples ('pointwise' detection) are fundamentally limited in their
ability to prevent fine-tuning attacks. We construct 'pointwise-undetectable'
attacks that repurpose entropy in benign model outputs (e.g. semantic or
syntactic variations) to covertly transmit dangerous knowledge. Our attacks are
composed solely of unsuspicious benign samples that can be collected from the
model before fine-tuning, meaning training and inference samples are all
individually benign and low-perplexity. We test our attacks against the OpenAI
fine-tuning API, finding they succeed in eliciting answers to harmful
multiple-choice questions, and that they evade an enhanced monitoring system we
design that successfully detects other fine-tuning attacks. We encourage the
community to develop defences that tackle the fundamental limitations we
uncover in pointwise fine-tuning API defences.; 10) On Scaling Neurosymbolic Programming through Guided Logical Inference; Probabilistic neurosymbolic learning seeks to integrate neural networks with
symbolic programming. Many state-of-the-art systems rely on a reduction to the
Probabilistic Weighted Model Counting Problem (PWMC), which requires computing
a Boolean formula called the logical provenance.However, PWMC is \\#P-hard, and
the number of clauses in the logical provenance formula can grow exponentially,
creating a major bottleneck that significantly limits the applicability of PNL
solutions in practice.We propose a new approach centered around an exact
algorithm DPNL, that enables bypassing the computation of the logical
provenance.The DPNL approach relies on the principles of an oracle and a
recursive DPLL-like decomposition in order to guide and speed up logical
inference.Furthermore, we show that this approach can be adapted for
approximate reasoning with $\epsilon$ or $(\epsilon, \delta)$ guarantees,
called ApproxDPNL.Experiments show significant performance gains.DPNL enables
scaling exact inference further, resulting in more accurate models.Further,
ApproxDPNL shows potential for advancing the scalability of neurosymbolic
programming by incorporating approximations even further, while simultaneously
ensuring guarantees for the reasoning process.; 11) BFANet: Revisiting 3D Semantic Segmentation with Boundary Feature
  Analysis; 3D semantic segmentation plays a fundamental and crucial role to understand
3D scenes. While contemporary state-of-the-art techniques predominantly
concentrate on elevating the overall performance of 3D semantic segmentation
based on general metrics (e.g. mIoU, mAcc, and oAcc), they unfortunately leave
the exploration of challenging regions for segmentation mostly neglected. In
this paper, we revisit 3D semantic segmentation through a more granular lens,
shedding light on subtle complexities that are typically overshadowed by
broader performance metrics. Concretely, we have delineated 3D semantic
segmentation errors into four comprehensive categories as well as corresponding
evaluation metrics tailored to each. Building upon this categorical framework,
we introduce an innovative 3D semantic segmentation network called BFANet that
incorporates detailed analysis of semantic boundary features. First, we design
the boundary-semantic module to decouple point cloud features into semantic and
boundary features, and fuse their query queue to enhance semantic features with
attention. Second, we introduce a more concise and accelerated boundary
pseudo-label calculation algorithm, which is 3.9 times faster than the
state-of-the-art, offering compatibility with data augmentation and enabling
efficient computation in training. Extensive experiments on benchmark data
indicate the superiority of our BFANet model, confirming the significance of
emphasizing the four uniquely designed metrics. Code is available at
https://github.com/weiguangzhao/BFANet.; 12) Bridging Social Psychology and LLM Reasoning: Conflict-Aware Meta-Review
  Generation via Cognitive Alignment; The rapid growth of scholarly submissions has overwhelmed traditional peer
review systems, driving the need for intelligent automation to preserve
scientific rigor. While large language models (LLMs) show promise in automating
manuscript critiques, their ability to synthesize high-stakes meta-reviews,
which require conflict-aware reasoning and consensus derivation, remains
underdeveloped. Existing methods fail to effectively handle conflicting
viewpoints within differing opinions, and often introduce additional cognitive
biases, such as anchoring effects and conformity bias.To overcome these
limitations, we propose the Cognitive Alignment Framework (CAF), a dual-process
architecture that transforms LLMs into adaptive scientific arbitrators. By
operationalizing Kahneman's dual-process theory, CAF introduces a three-step
cognitive pipeline: review initialization, incremental integration, and
cognitive alignment.Empirical validation shows that CAF outperforms existing
LLM-based methods, with sentiment consistency gains reaching up to 19.47\% and
content consistency improving by as much as 12.95\%.; 13) Rule-Based Conflict-Free Decision Framework in Swarm Confrontation; Traditional rule-based decision-making methods with interpretable advantage,
such as finite state machine, suffer from the jitter or deadlock(JoD) problems
in extremely dynamic scenarios. To realize agent swarm confrontation, decision
conflicts causing many JoD problems are a key issue to be solved. Here, we
propose a novel decision-making framework that integrates probabilistic finite
state machine, deep convolutional networks, and reinforcement learning to
implement interpretable intelligence into agents. Our framework overcomes state
machine instability and JoD problems, ensuring reliable and adaptable decisions
in swarm confrontation. The proposed approach demonstrates effective
performance via enhanced human-like cooperation and competitive strategies in
the rigorous evaluation of real experiments, outperforming other methods.; 14) A Scalable Approach to Probabilistic Neuro-Symbolic Verification; Neuro-Symbolic Artificial Intelligence (NeSy AI) has emerged as a promising
direction for integrating neural learning with symbolic reasoning. In the
probabilistic variant of such systems, a neural network first extracts a set of
symbols from sub-symbolic input, which are then used by a symbolic component to
reason in a probabilistic manner towards answering a query. In this work, we
address the problem of formally verifying the robustness of such NeSy
probabilistic reasoning systems, therefore paving the way for their safe
deployment in critical domains. We analyze the complexity of solving this
problem exactly, and show that it is $\mathrm{NP}^{\# \mathrm{P}}$-hard. To
overcome this issue, we propose the first approach for approximate,
relaxation-based verification of probabilistic NeSy systems. We demonstrate
experimentally that the proposed method scales exponentially better than
solver-based solutions and apply our technique to a real-world autonomous
driving dataset, where we verify a safety property under large input
dimensionalities and network sizes.; 15) KU AIGEN ICL EDI@BC8 Track 3: Advancing Phenotype Named Entity
  Recognition and Normalization for Dysmorphology Physical Examination Reports; The objective of BioCreative8 Track 3 is to extract phenotypic key medical
findings embedded within EHR texts and subsequently normalize these findings to
their Human Phenotype Ontology (HPO) terms. However, the presence of diverse
surface forms in phenotypic findings makes it challenging to accurately
normalize them to the correct HPO terms. To address this challenge, we explored
various models for named entity recognition and implemented data augmentation
techniques such as synonym marginalization to enhance the normalization step.
Our pipeline resulted in an exact extraction and normalization F1 score 2.6\%
higher than the mean score of all submissions received in response to the
challenge. Furthermore, in terms of the normalization F1 score, our approach
surpassed the average performance by 1.9\%. These findings contribute to the
advancement of automated medical data extraction and normalization techniques,
showcasing potential pathways for future research and application in the
biomedical domain.; 16) MECASA: Motor Execution Classification using Additive Self-Attention for
  Hybrid EEG-fNIRS Data; Motor execution, a fundamental aspect of human behavior, has been extensively
studied using BCI technologies. EEG and fNIRS have been utilized to provide
valuable insights, but their individual limitations have hindered performance.
This study investigates the effectiveness of fusing electroencephalography
(EEG) and functional near-infrared spectroscopy (fNIRS) data for classifying
rest versus task states in a motor execution paradigm. Using the SMR Hybrid BCI
dataset, this work compares unimodal (EEG and fNIRS) classifiers with a
multimodal fusion approach. It proposes Motor Execution using Convolutional
Additive Self-Attention Mechanisms (MECASA), a novel architecture leveraging
convolutional operations and self-attention to capture complex patterns in
multimodal data. MECASA, built upon the CAS-ViT architecture, employs a
computationally efficient, convolutional-based self-attention module (CASA), a
hybrid block design, and a dedicated fusion network to combine features from
separate EEG and fNIRS processing streams. Experimental results demonstrate
that MECASA consistently outperforms established methods across all modalities
(EEG, fNIRS, and fused), with fusion consistently improving accuracy compared
to single-modality approaches. fNIRS generally achieved higher accuracy than
EEG alone. Ablation studies revealed optimal configurations for MECASA, with
embedding dimensions of 64-128 providing the best performance for EEG data and
OD128 (upsampled optical density) yielding superior results for fNIRS data.
This work highlights the potential of deep learning, specifically MECASA, to
enhance EEG-fNIRS fusion for BCI applications.; 17) Linear complete symmetric rank-distance codes; An $\mathbb{F}_q$-linear code of minimum distance $d$ is called complete if
it is not contained in a larger $\mathbb{F}_q$-linear code of minimum distance
$d$. In this paper, we classify $\mathbb{F}_q$-linear complete symmetric
rank-distance (CSRD) codes in $M_{3\times 3}(\mathbb{F}_q)$ up to equivalence.
This includes the classification of $\mathbb{F}_q$-linear maximum symmetric
rank-distance (MSRD) codes in $M_{3\times 3}(\mathbb{F}_q)$. Our approach is
mainly geometric, and our results contribute towards the classification of nets
of conics in $\mathrm{PG}(2, q)$.; 18) GENEOnet: Statistical analysis supporting explainability and
  trustworthiness; Group Equivariant Non-Expansive Operators (GENEOs) have emerged as
mathematical tools for constructing networks for Machine Learning and
Artificial Intelligence. Recent findings suggest that such models can be
inserted within the domain of eXplainable Artificial Intelligence (XAI) due to
their inherent interpretability. In this study, we aim to verify this claim
with respect to GENEOnet, a GENEO network developed for an application in
computational biochemistry by employing various statistical analyses and
experiments. Such experiments first allow us to perform a sensitivity analysis
on GENEOnet's parameters to test their significance. Subsequently, we show that
GENEOnet exhibits a significantly higher proportion of equivariance compared to
other methods. Lastly, we demonstrate that GENEOnet is on average robust to
perturbations arising from molecular dynamics. These results collectively serve
as proof of the explainability, trustworthiness, and robustness of GENEOnet and
confirm the beneficial use of GENEOs in the context of Trustworthy Artificial
Intelligence.; 19) Artificial Liver Classifier: A New Alternative to Conventional Machine
  Learning Models; Supervised machine learning classifiers often encounter challenges related to
performance, accuracy, and overfitting. This paper introduces the Artificial
Liver Classifier (ALC), a novel supervised learning classifier inspired by the
human liver's detoxification function. The ALC is characterized by its
simplicity, speed, hyperparameters-free, ability to reduce overfitting, and
effectiveness in addressing multi-classification problems through
straightforward mathematical operations. To optimize the ALC's parameters, an
improved FOX optimization algorithm (IFOX) is employed as the training method.
The proposed ALC was evaluated on five benchmark machine learning datasets:
Iris Flower, Breast Cancer Wisconsin, Wine, Voice Gender, and MNIST. The
results demonstrated competitive performance, with the ALC achieving 100%
accuracy on the Iris dataset, surpassing logistic regression, multilayer
perceptron, and support vector machine. Similarly, on the Breast Cancer
dataset, it achieved 99.12% accuracy, outperforming XGBoost and logistic
regression. Across all datasets, the ALC consistently exhibited lower
overfitting gaps and loss compared to conventional classifiers. These findings
highlight the potential of leveraging biological process simulations to develop
efficient machine learning models and open new avenues for innovation in the
field.; 20) LeanProgress: Guiding Search for Neural Theorem Proving via Proof
  Progress Prediction; Mathematical reasoning remains a significant challenge for Large Language
Models (LLMs) due to hallucinations. When combined with formal proof assistants
like Lean, these hallucinations can be eliminated through rigorous
verification, making theorem proving reliable. However, even with formal
verification, LLMs still struggle with long proofs and complex mathematical
formalizations. While Lean with LLMs offers valuable assistance with retrieving
lemmas, generating tactics, or even complete proofs, it lacks a crucial
capability: providing a sense of proof progress. This limitation particularly
impacts the overall development efficiency in large formalization projects. We
introduce LeanProgress, a method that predicts the progress in the proof.
Training and evaluating our models made on a large corpus of Lean proofs from
Lean Workbook Plus and Mathlib4 and how many steps remain to complete it, we
employ data preprocessing and balancing techniques to handle the skewed
distribution of proof lengths. Our experiments show that LeanProgress achieves
an overall prediction accuracy of 75.1\% in predicting the amount of progress
and, hence, the remaining number of steps. When integrated into a best-first
search framework using Reprover, our method shows a 3.8\% improvement on
Mathlib4 compared to baseline performances of 41.2\%, particularly for longer
proofs. These results demonstrate how proof progress prediction can enhance
both automated and interactive theorem proving, enabling users to make more
informed decisions about proof strategies.; 21) Multi-Agent Verification: Scaling Test-Time Compute with Multiple
  Verifiers; By utilizing more computational resources at test-time, large language models
(LLMs) can improve without additional training. One common strategy uses
verifiers to evaluate candidate outputs. In this work, we propose a novel
scaling dimension for test-time compute: scaling the number of verifiers. We
introduce Multi-Agent Verification (MAV) as a test-time compute paradigm that
combines multiple verifiers to improve performance. We propose using Aspect
Verifiers (AVs), off-the-shelf LLMs prompted to verify different aspects of
outputs, as one possible choice for the verifiers in a MAV system. AVs are a
convenient building block for MAV since they can be easily combined without
additional training. Moreover, we introduce BoN-MAV, a simple multi-agent
verification algorithm that combines best-of-n sampling with multiple
verifiers. BoN-MAV demonstrates stronger scaling patterns than self-consistency
and reward model verification, and we demonstrate both weak-to-strong
generalization, where combining weak verifiers improves even stronger LLMs, and
self-improvement, where the same base model is used to both generate and verify
outputs. Our results establish scaling the number of verifiers as a promising
new dimension for improving language model performance at test-time.; 22) Metric Distortion in Peer Selection; In the metric distortion problem, a set of voters and candidates lie in a
common metric space, and a committee of $k$ candidates is to be elected. The
goal is to select a committee with a small social cost, defined as an
increasing function of the distances between voters and selected candidates,
but a voting rule only has access to voters' ordinal preferences. The
distortion of a rule is then defined as the worst-case ratio between the social
cost of the selected set and the optimal set, over all possible preferences and
consistent distances.
  We initiate the study of metric distortion when voters and candidates
coincide, which arises naturally in peer selection, and provide tight results
for various social cost functions on the line metric. We consider both
utilitarian and egalitarian social cost, given by the sum and maximum of the
individual social costs, respectively. For utilitarian social cost, we show
that the voting rule that selects the $k$ middle agents achieves a distortion
that varies between $1$ and $2$ as $k$ varies from $1$ to $n$ when the cost of
an individual is the sum of their distances to all selected candidates
(additive aggregation). When the cost of an individual is their distance to
their $q$th closest candidate ($q$-cost), we provide positive results for
$q=k=2$ but mostly show that negative results for general elections carry over
to our setting: No constant distortion is possible when $q\leq k/2$ and no
distortion better than $3/2$ is possible for $q\geq k/2+1$. For egalitarian
social cost, selecting extreme agents achieves the best-possible distortion of
$2$ for additive cost and $q$-cost with $q> k/3$, whereas no constant
distortion is possible for $q\leq k/3$. Overall, having a common set of voters
and candidates allows for better constants compared to the general setting, but
cases in which no constant is possible in general remain hard in this setting.; 23) Architected Dual-Network Solvent-free Adhesives for Stretchable Fabrics; Natural systems, such as tendons and spider silk, demonstrate how the
combination of strength and stretchability can be effectively achieved by
integrating stiff and flexible network structures. Inspired by these systems,
we developed a novel, solvent-free dual-network adhesive based on a
self-assembling ABA triblock copolymer, poly(methyl methacrylate)-poly(n-butyl
acrylate)-poly(methyl methacrylate) (PMMA-b-PnBA-b-PMMA), designed for
applications requiring both high strength and stretchability. The triblock
copolymer forms a physically crosslinked network through microdomains of PMMA
end-blocks that provide structural integrity, while the PnBA mid-block forms a
soft, stretchable matrix. To further enhance mechanical performance, a second
poly(n-butyl acrylate) (PnBA) network is polymerized in situ, locking the PMMA
microdomains in place and creating a load-bearing system. By varying the
crosslinking density of the secondary network, we tailor the adhesive's
mechanical properties (Young's modulus: 0.17 - 1.18 MPa) to suit different
substrates, creating a mechanically transparent seam. The resulting
dual-network system combines different strategies to achieve high strength and
stretchability, with adhesive performance comparable to industrial methods such
as sewing, particularly in bonding neoprene fabric composites and sealing the
joints. Our solvent-free approach also eliminates the need for lengthy solvent
evaporation steps, offering an eco-friendly and more efficient alternative for
flexible adhesive applications in fields such as soft robotics, flexible
electronics, and sports apparel.; 24) REBELS-IFU: Dust attenuation curves of 12 massive galaxies at $z\simeq7$; We present measurements of the dust attenuation curves of 12 massive
($9~<~\log$($M_{\star}/{M}_{\odot})$ $<~10$) Lyman-break galaxies at
$z=6.5-7.7$ derived from James Webb Space Telescope (JWST) NIRSpec integral
field unit (IFU) spectroscopy. The galaxies are drawn from the Atacama Large
Millimeter/submillimeter Array (ALMA) Reionization Era Bright Emission Line
Survey (REBELS) large program. The dust attenuation curves were obtained by
fitting spectral energy distribution (SED) models with a flexible dust law to
the full galaxy spectra over observed wavelengths $0.6-5.3$ $\mu$m. These
attenuation curves show a range of recovered slopes ($-0.39\leq\delta\leq0.08$)
that are on average slightly flatter than seen in local sources of the same
stellar masses, with none exhibiting very steep slopes. Three galaxies exhibit
evidence for a 2175 \r{A} dust bump ($>4\sigma$) and we find SED fitting
excluding the bump can overestimate derived stellar masses by up to $0.4$ dex.
Correcting for the dust attenuation with our best-fit attenuation curves we
recover a range of intrinsic UV-slopes ($-2.5\leq\beta_0\leq-2.2$). The
galaxies show moderate reddening ($A_V~=~0.1-0.6$ mag) and the $A_V$ to stellar
mass relation is consistent with local sources. The attenuation law slope is
found to correlate with $A_V$, while we see no strong correlation with stellar
mass, ${M_{\rm UV}}$, or gas-phase metallicity. Overall, our results show
little evolution in dust properties in the REBELS sources compared to the local
Universe. Comparing our recovered trends to empirical models suggests that the
most important factor driving the variation in the attenuation curves in our
sample is the dust-star geometry, not the properties of the dust grains
themselves.; 25) Non-uniform Berry--Esseen bounds for exchangeable pairs with
  applications to the mean-field classical $N$-vector models and Jack measures; This paper establishes a non-uniform Berry--Esseen bound in normal
approximation for exchangeable pairs using Stein's method via a concentration
inequality approach. The main theorem extends and improves several results in
the literature, including those of Eichelsbacher and L\""{o}we [Electron. J.
Probab. 15, 2010, 962--988], and Eichelsbacher [arXiv:2404.07587, 2024]. The
result is applied to obtain a non-uniform Berry--Esseen bound for the
squared-length of the total spin in the mean-field classical $N$-vector models,
and a non-uniform Berry--Esseen bound for Jack deformations of the character
ratio.; 26) Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study; In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric.; 27) Superalignment with Dynamic Human Values; Two core challenges of alignment are 1) scalable oversight and 2) accounting
for the dynamic nature of human values. While solutions like recursive reward
modeling address 1), they do not simultaneously account for 2). We sketch a
roadmap for a novel algorithmic framework that trains a superhuman reasoning
model to decompose complex tasks into subtasks that are still amenable to
human-level guidance. Our approach relies on what we call the part-to-complete
generalization hypothesis, which states that the alignment of subtask solutions
generalizes to the alignment of complete solutions. We advocate for the need to
measure this generalization and propose ways to improve it in the future.; 28) Telephone Surveys Meet Conversational AI: Evaluating a LLM-Based
  Telephone Survey System at Scale; Telephone surveys remain a valuable tool for gathering insights but typically
require substantial resources in training and coordinating human interviewers.
This work presents an AI-driven telephone survey system integrating
text-to-speech (TTS), a large language model (LLM), and speech-to-text (STT)
that mimics the versatility of human-led interviews (full-duplex dialogues) at
scale.
  We tested the system across two populations, a pilot study in the United
States (n = 75) and a large-scale deployment in Peru (n = 2,739), inviting
participants via web-based links and contacting them via direct phone calls.
The AI agent successfully administered open-ended and closed-ended questions,
handled basic clarifications, and dynamically navigated branching logic,
allowing fast large-scale survey deployment without interviewer recruitment or
training.
  Our findings demonstrate that while the AI system's probing for qualitative
depth was more limited than human interviewers, overall data quality approached
human-led standards for structured items. This study represents one of the
first successful large-scale deployments of an LLM-based telephone interviewer
in a real-world survey context. The AI-powered telephone survey system has the
potential for expanding scalable, consistent data collecting across market
research, social science, and public opinion studies, thus improving
operational efficiency while maintaining appropriate data quality for research.; 29) Infrastructure for AI Agents; Increasingly many AI systems can plan and execute interactions in open-ended
environments, such as making phone calls or buying online goods. As developers
grow the space of tasks that such AI agents can accomplish, we will need tools
both to unlock their benefits and manage their risks. Current tools are largely
insufficient because they are not designed to shape how agents interact with
existing institutions (e.g., legal and economic systems) or actors (e.g.,
digital service providers, humans, other AI agents). For example, alignment
techniques by nature do not assure counterparties that some human will be held
accountable when a user instructs an agent to perform an illegal action. To
fill this gap, we propose the concept of agent infrastructure: technical
systems and shared protocols external to agents that are designed to mediate
and influence their interactions with and impacts on their environments. Agent
infrastructure comprises both new tools and reconfigurations or extensions of
existing tools. For example, to facilitate accountability, protocols that tie
users to agents could build upon existing systems for user authentication, such
as OpenID. Just as the Internet relies on infrastructure like HTTPS, we argue
that agent infrastructure will be similarly indispensable to ecosystems of
agents. We identify three functions for agent infrastructure: 1) attributing
actions, properties, and other information to specific agents, their users, or
other actors; 2) shaping agents' interactions; and 3) detecting and remedying
harmful actions from agents. We propose infrastructure that could help achieve
each function, explaining use cases, adoption, limitations, and open questions.
Making progress on agent infrastructure can prepare society for the adoption of
more advanced agents.; 30) A Cousin Complex for the Quantum Projective Space; Grothendieck constructed a Cousin complex for abelian sheaves on an arbitrary
topological space. In a special setting, its dual called the BGG resolution is
applicable in representation theory. Arkhipov proposed a complex whose dual is
only suitable for representation theory of quantum groups at roots of unity of
prime order. It is desirable to get one which works for quantum groups at all
roots of unity. For a quantum projective space, we provide such a complex.; 31) The Danger of Overthinking: Examining the Reasoning-Action Dilemma in
  Agentic Tasks; Large Reasoning Models (LRMs) represent a breakthrough in AI problem-solving
capabilities, but their effectiveness in interactive environments can be
limited. This paper introduces and analyzes overthinking in LRMs. A phenomenon
where models favor extended internal reasoning chains over environmental
interaction. Through experiments on software engineering tasks using SWE Bench
Verified, we observe three recurring patterns: Analysis Paralysis, Rogue
Actions, and Premature Disengagement. We propose a framework to study these
behaviors, which correlates with human expert assessments, and analyze 4018
trajectories. We observe that higher overthinking scores correlate with
decreased performance, with reasoning models exhibiting stronger tendencies
toward overthinking compared to non-reasoning models. Our analysis reveals that
simple efforts to mitigate overthinking in agentic environments, such as
selecting the solution with the lower overthinking score, can improve model
performance by almost 30% while reducing computational costs by 43%. These
results suggest that mitigating overthinking has strong practical implications.
We suggest that by leveraging native function-calling capabilities and
selective reinforcement learning overthinking tendencies could be mitigated. We
also open-source our evaluation framework and dataset to facilitate research in
this direction at https://github.com/AlexCuadron/Overthinking.; 32) Aligning Crowd-sourced Human Feedback for Reinforcement Learning on Code
  Generation by Large Language Models; This paper studies how AI-assisted programming and large language models
(LLM) improve software developers' ability via AI tools (LLM agents) like
Github Copilot and Amazon CodeWhisperer, while integrating human feedback to
enhance reinforcement learning (RLHF) with crowd-sourced computation to enhance
text-to-code generation. Additionally, we demonstrate that our Bayesian
optimization framework supports AI alignment in code generation by distributing
the feedback collection burden, highlighting the value of collecting human
feedback of good quality. Our empirical evaluations demonstrate the efficacy of
this approach, showcasing how LLM agents can be effectively trained for
improved text-to-code generation. Our Bayesian optimization framework can be
designed for general domain-specific languages, promoting the alignment of
large language model capabilities with human feedback in AI-assisted
programming for code generation.; 33) ApplE: An Applied Ethics Ontology with Event Context; Applied ethics is ubiquitous in most domains, requiring much deliberation due
to its philosophical nature. Varying views often lead to conflicting courses of
action where ethical dilemmas become challenging to resolve. Although many
factors contribute to such a decision, the major driving forces can be
discretized and thus simplified to provide an indicative answer. Knowledge
representation and reasoning offer a way to explicitly translate abstract
ethical concepts into applicable principles within the context of an event. To
achieve this, we propose ApplE, an Applied Ethics ontology that captures
philosophical theory and event context to holistically describe the morality of
an action. The development process adheres to a modified version of the
Simplified Agile Methodology for Ontology Development (SAMOD) and utilizes
standard design and publication practices. Using ApplE, we model a use case
from the bioethics domain that demonstrates our ontology's social and
scientific value. Apart from the ontological reasoning and quality checks,
ApplE is also evaluated using the three-fold testing process of SAMOD. ApplE
follows FAIR principles and aims to be a viable resource for applied ethicists
and ontology engineers.; 34) At extreme strain rates, pure metals thermally harden while alloys
  thermally soften; When materials are deformed at extreme strain rates, greater than $10^6
\text{ s}^{-1}$, a counterintuitive mechanical response is seen where the
strength and hardness of pure metals increases with increasing temperature. The
anti-thermal hardening is due to defects in the material becoming pinned by
phonons in the crystal lattice. However, here, using optically driven
microballistic impact testing to measure the dynamic strength and hardness, we
show that when the composition is systematically varied away from high purity,
the mechanical response of metals transitions from ballistic transport of
dislocations back to thermally activated pinning of dislocations, even at the
highest strain rates. This boundary from ""hotter-is-stronger"" to
""hotter-is-softer"" is observed and mapped for nickel, titanium, and gold. The
ability to tune between deformation mechanisms with very different temperature
dependencies speaks to new directions for alloy design in extreme conditions.; 35) Risks of Cultural Erasure in Large Language Models; Large language models are increasingly being integrated into applications
that shape the production and discovery of societal knowledge such as search,
online education, and travel planning. As a result, language models will shape
how people learn about, perceive and interact with global cultures making it
important to consider whose knowledge systems and perspectives are represented
in models. Recognizing this importance, increasingly work in Machine Learning
and NLP has focused on evaluating gaps in global cultural representational
distribution within outputs. However, more work is needed on developing
benchmarks for cross-cultural impacts of language models that stem from a
nuanced sociologically-aware conceptualization of cultural impact or harm. We
join this line of work arguing for the need of metricizable evaluations of
language technologies that interrogate and account for historical power
inequities and differential impacts of representation on global cultures,
particularly for cultures already under-represented in the digital corpora. We
look at two concepts of erasure: omission: where cultures are not represented
at all and simplification i.e. when cultural complexity is erased by presenting
one-dimensional views of a rich culture. The former focuses on whether
something is represented, and the latter on how it is represented. We focus our
analysis on two task contexts with the potential to influence global cultural
production. First, we probe representations that a language model produces
about different places around the world when asked to describe these contexts.
Second, we analyze the cultures represented in the travel recommendations
produced by a set of language model applications. Our study shows ways in which
the NLP community and application developers can begin to operationalize
complex socio-cultural considerations into standard evaluations and benchmarks.; 36) Community detection for directed networks revisited using bimodularity; Community structure is a key feature omnipresent in real-world network data.
Plethora of methods have been proposed to reveal subsets of densely
interconnected nodes using criteria such as the modularity index. These
approaches have been successful for undirected graphs, but directed edge
information has not yet been dealt with in a satisfactory way. Here, we revisit
the concept of directed communities as a mapping between sending and receiving
communities. This translates into a new definition that we term bimodularity.
Using convex relaxation, bimodularity can be optimized with the singular value
decomposition of the directed modularity matrix. Subsequently, we propose an
edge-based clustering approach to reveal the directed communities including
their mappings. The feasibility of the new framework is illustrated on a
synthetic model and further applied to the neuronal wiring diagram of the
\textit{C. elegans}, for which it yields meaningful feedforward loops of the
head and body motion systems. This framework sets the ground for the
understanding and detection of community structures in directed networks.; 37) URECA: The Chain of Two Minimum Set Cover Problems exists behind
  Adaptation to Shifts in Semantic Code Search; Adaptation is to make model learn the patterns shifted from the training
distribution. In general, this adaptation is formulated as the minimum entropy
problem. However, the minimum entropy problem has inherent limitation --
shifted initialization cascade phenomenon. We extend the relationship between
the minimum entropy problem and the minimum set cover problem via Lebesgue
integral. This extension reveals that internal mechanism of the minimum entropy
problem ignores the relationship between disentangled representations, which
leads to shifted initialization cascade. From the analysis, we introduce a new
clustering algorithm, Union-find based Recursive Clustering Algorithm~(URECA).
URECA is an efficient clustering algorithm for the leverage of the
relationships between disentangled representations. The update rule of URECA
depends on Thresholdly-Updatable Stationary Assumption to dynamics as a
released version of Stationary Assumption. This assumption helps URECA to
transport disentangled representations with no errors based on the
relationships between disentangled representations. URECA also utilize
simulation trick to efficiently cluster disentangled representations. The wide
range of evaluations show that URECA achieves consistent performance gains for
the few-shot adaptation to diverse types of shifts along with advancement to
State-of-The-Art performance in CoSQA in the scenario of query shift.; 38) MathMistake Checker: A Comprehensive Demonstration for Step-by-Step Math
  Problem Mistake Finding by Prompt-Guided LLMs; We propose a novel system, MathMistake Checker, designed to automate
step-by-step mistake finding in mathematical problems with lengthy answers
through a two-stage process. The system aims to simplify grading, increase
efficiency, and enhance learning experiences from a pedagogical perspective. It
integrates advanced technologies, including computer vision and the
chain-of-thought capabilities of the latest large language models (LLMs). Our
system supports open-ended grading without reference answers and promotes
personalized learning by providing targeted feedback. We demonstrate its
effectiveness across various types of math problems, such as calculation and
word problems.; 39) Leveraging Knowledge Graphs and LLMs for Context-Aware Messaging; Personalized messaging plays an essential role in improving communication in
areas such as healthcare, education, and professional engagement. This paper
introduces a framework that uses the Knowledge Graph (KG) to dynamically
rephrase written communications by integrating individual and context-specific
data. The knowledge graph represents individuals, locations, and events as
critical nodes, linking entities mentioned in messages to their corresponding
graph nodes. The extraction of relevant information, such as preferences,
professional roles, and cultural norms, is then combined with the original
message and processed through a large language model (LLM) to generate
personalized responses. The framework demonstrates notable message acceptance
rates in various domains: 42% in healthcare, 53% in education, and 78% in
professional recruitment. By integrating entity linking, event detection, and
language modeling, this approach offers a structured and scalable solution for
context-aware, audience-specific communication, facilitating advanced
applications in diverse fields.; 40) A strictly predefined-time convergent and anti-noise fractional-order
  zeroing neural network for solving time-variant quadratic programming in
  kinematic robot control; This paper proposes a strictly predefined-time convergent and anti-noise
fractional-order zeroing neural network (SPTC-AN-FOZNN) model, meticulously
designed for addressing time-variant quadratic programming (TVQP) problems.
This model marks the first variable-gain ZNN to collectively manifest strictly
predefined-time convergence and noise resilience, specifically tailored for
kinematic motion control of robots. The SPTC-AN-FOZNN advances traditional ZNNs
by incorporating a conformable fractional derivative in accordance with the
Leibniz rule, a compliance not commonly achieved by other fractional derivative
definitions. It also features a novel activation function designed to ensure
favorable convergence independent of the model's order. When compared to five
recently published recurrent neural networks (RNNs), the SPTC-AN-FOZNN,
configured with $0<\alpha\leq 1$, exhibits superior positional accuracy and
robustness against additive noises for TVQP applications. Extensive empirical
evaluations, including simulations with two types of robotic manipulators and
experiments with a Flexiv Rizon robot, have validated the SPTC-AN-FOZNN's
effectiveness in precise tracking and computational efficiency, establishing
its utility for robust kinematic control.; 41) Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning; Large language models (LLMs) can prove mathematical theorems formally by
generating proof steps (\textit{a.k.a.} tactics) within a proof system.
However, the space of possible tactics is vast and complex, while the available
training data for formal proofs is limited, posing a significant challenge to
LLM-based tactic generation. To address this, we introduce a neuro-symbolic
tactic generator that synergizes the mathematical intuition learned by LLMs
with domain-specific insights encoded by symbolic methods. The key aspect of
this integration is identifying which parts of mathematical reasoning are best
suited to LLMs and which to symbolic methods. While the high-level idea of
neuro-symbolic integration is broadly applicable to various mathematical
problems, in this paper, we focus specifically on Olympiad inequalities
(Figure~1). We analyze how humans solve these problems and distill the
techniques into two types of tactics: (1) scaling, handled by symbolic methods,
and (2) rewriting, handled by LLMs. In addition, we combine symbolic tools with
LLMs to prune and rank the proof goals for efficient proof search. We evaluate
our framework on 161 challenging inequalities from multiple mathematics
competitions, achieving state-of-the-art performance and significantly
outperforming existing LLM and symbolic approaches without requiring additional
training data.; 42) Representation Learning to Advance Multi-institutional Studies with
  Electronic Health Record Data; The adoption of EHRs has expanded opportunities to leverage data-driven
algorithms in clinical care and research. A major bottleneck in effectively
conducting multi-institutional EHR studies is the data heterogeneity across
systems with numerous codes that either do not exist or represent different
clinical concepts across institutions. The need for data privacy further limits
the feasibility of including multi-institutional patient-level data required to
study similarities and differences across patient subgroups. To address these
challenges, we developed the GAME algorithm. Tested and validated across 7
institutions and 2 languages, GAME integrates data in several levels: (1) at
the institutional level with knowledge graphs to establish relationships
between codes and existing knowledge sources, providing the medical context for
standard codes and their relationship to each other; (2) between institutions,
leveraging language models to determine the relationships between
institution-specific codes with established standard codes; and (3) quantifying
the strength of the relationships between codes using a graph attention
network. Jointly trained embeddings are created using transfer and federated
learning to preserve data privacy. In this study, we demonstrate the
applicability of GAME in selecting relevant features as inputs for AI-driven
algorithms in a range of conditions, e.g., heart failure, rheumatoid arthritis.
We then highlight the application of GAME harmonized multi-institutional EHR
data in a study of Alzheimer's disease outcomes and suicide risk among patients
with mental health disorders, without sharing patient-level data outside
individual institutions.; 43) Decision Information Meets Large Language Models: The Future of
  Explainable Operations Research; Operations Research (OR) is vital for decision-making in many industries.
While recent OR methods have seen significant improvements in automation and
efficiency through integrating Large Language Models (LLMs), they still
struggle to produce meaningful explanations. This lack of clarity raises
concerns about transparency and trustworthiness in OR applications. To address
these challenges, we propose a comprehensive framework, Explainable Operations
Research (EOR), emphasizing actionable and understandable explanations
accompanying optimization. The core of EOR is the concept of Decision
Information, which emerges from what-if analysis and focuses on evaluating the
impact of complex constraints (or parameters) changes on decision-making.
Specifically, we utilize bipartite graphs to quantify the changes in the OR
model and adopt LLMs to improve the explanation capabilities. Additionally, we
introduce the first industrial benchmark to rigorously evaluate the
effectiveness of explanations and analyses in OR, establishing a new standard
for transparency and clarity in the field.; 44) Bridging the Communication Gap: Evaluating AI Labeling Practices for
  Trustworthy AI Development; As artificial intelligence (AI) becomes integral to economy and society,
communication gaps between developers, users, and stakeholders hinder trust and
informed decision-making. High-level AI labels, inspired by frameworks like EU
energy labels, have been proposed to make the properties of AI models more
transparent. Without requiring deep technical expertise, they can inform on the
trade-off between predictive performance and resource efficiency. However, the
practical benefits and limitations of AI labeling remain underexplored. This
study evaluates AI labeling through qualitative interviews along four key
research questions. Based on thematic analysis and inductive coding, we found a
broad range of practitioners to be interested in AI labeling (RQ1). They see
benefits for alleviating communication gaps and aiding non-expert
decision-makers, however limitations, misunderstandings, and suggestions for
improvement were also discussed (RQ2). Compared to other reporting formats,
interviewees positively evaluated the reduced complexity of labels, increasing
overall comprehensibility (RQ3). Trust was influenced most by usability and the
credibility of the responsible labeling authority, with mixed preferences for
self-certification versus third-party certification (RQ4). Our Insights
highlight that AI labels pose a trade-off between simplicity and complexity,
which could be resolved by developing customizable and interactive labeling
frameworks to address diverse user needs. Transparent labeling of resource
efficiency also nudged interviewee priorities towards paying more attention to
sustainability aspects during AI development. This study validates AI labels as
a valuable tool for enhancing trust and communication in AI, offering
actionable guidelines for their refinement and standardization.; 45) A glance to Luttinger liquid and its platforms; The concept of a Tomonaga-Luttinger liquid (TLL) has been established as a
fundamental theory for the understanding of one-dimensional quantum systems.
Originally formulated as a replacement for Landau's Fermi-liquid theory, which
accurately predicts the behaviour of most 3D metals but fails dramatically in
1D, the TLL description applies to a even broader class of 1D systems,including
bosons and anyons. After a certain number of theoretical breakthroughs, its
descriptive power has now been confirmed experimentally in different
experimental platforms. They extend from organic conductors, carbon nanotubes,
quantum wires, topological edge states of quantum spin Hall insulators to cold
atoms, Josephson junctions, Bose liquids confined within 1D nanocapillaries and
spin chains. In the ground state of such systems, quantum fluctuations become
correlated on all length scales, but, counter-intuitively, no long-range order
exists. In this respect, this review will illustrate the validity of conformal
field theory for describing real-world systems, establishing the boundaries for
its application and, on the other side will discuss the spectacular
demonstration of how the quantum-critical TLL state governs the properties of
many-body systems in one dimension.; 46) Building networks of shared research interests by embedding words into a
  representation space; Departments within a university are not only administrative units, but also
an effort to gather investigators around common fields of academic study. A
pervasive challenge is connecting members with shared research interests both
within and between departments. Here I describe a workflow that adapts methods
from natural language processing to generate a network connecting $n=79$
members of a university department, or multiple departments within a faculty
($n=278$), based on common topics in their research publications. After
extracting and processing terms from $n=16,901$ abstracts in the PubMed
database, the co-occurrence of terms is encoded in a sparse document-term
matrix. Based on the angular distances between the presence-absence vectors for
every pair of terms, I use the uniform manifold approximation and projection
(UMAP) method to embed the terms into a representational space such that terms
that tend to appear in the same documents are closer together. Each author's
corpus defines a probability distribution over terms in this space. Using the
Wasserstein distance to quantify the similarity between these distributions, I
generate a distance matrix among authors that can be analyzed and visualized as
a graph. I demonstrate that this nonparametric method produces clusters with
distinct themes that are consistent with some academic divisions, while
identifying untapped connections among members. A documented workflow
comprising Python and R scripts is available under the MIT license at
https://github.com/PoonLab/tragula.; 47) Commonsense Reasoning-Aided Autonomous Vehicle Systems; Autonomous Vehicle (AV) systems have been developed with a strong reliance on
machine learning techniques. While machine learning approaches, such as deep
learning, are extremely effective at tasks that involve observation and
classification, they struggle when it comes to performing higher level
reasoning about situations on the road. This research involves incorporating
commonsense reasoning models that use image data to improve AV systems. This
will allow AV systems to perform more accurate reasoning while also making them
more adjustable, explainable, and ethical. This paper will discuss the findings
so far and motivate its direction going forward.; 48) Cognitive-Aligned Document Selection for Retrieval-augmented Generation; Large language models (LLMs) inherently display hallucinations since the
precision of generated texts cannot be guaranteed purely by the parametric
knowledge they include. Although retrieval-augmented generation (RAG) systems
enhance the accuracy and reliability of generative models by incorporating
external documents, these retrieved documents often fail to adequately support
the model's responses in practical applications. To address this issue, we
propose GGatrieval (Fine-\textbf{G}rained \textbf{G}rounded \textbf{A}lignment
Re\textbf{trieval} for verifiable generation), which leverages an LLM to
dynamically update queries and filter high-quality, reliable retrieval
documents. Specifically, we parse the user query into its syntactic components
and perform fine-grained grounded alignment with the retrieved documents. For
query components that cannot be individually aligned, we propose a dynamic
semantic compensation mechanism that iteratively refines and rewrites the query
while continuously updating the retrieval results. This iterative process
continues until the retrieved documents sufficiently support the query's
response. Our approach introduces a novel criterion for filtering retrieved
documents, closely emulating human strategies for acquiring targeted
information. This ensures that the retrieved content effectively supports and
verifies the generated outputs. On the ALCE benchmark, our method significantly
surpasses a wide range of baselines, achieving state-of-the-art performance.; 49) Aligning Instruction Tuning with Pre-training; Instruction tuning enhances large language models (LLMs) to follow human
instructions across diverse tasks, relying on high-quality datasets to guide
behavior. However, these datasets, whether manually curated or synthetically
generated, are often narrowly focused and misaligned with the broad
distributions captured during pre-training, limiting LLM generalization and
effective use of pre-trained knowledge. We propose Aligning Instruction Tuning
with Pre-training (AITP), a method that bridges this gap by identifying
coverage shortfalls in instruction-tuning datasets and rewriting
underrepresented pre-training data into high-quality instruction-response
pairs. This approach enriches dataset diversity while preserving task-specific
objectives. Evaluations on three fully open LLMs across eight benchmarks
demonstrate consistent performance improvements with AITP. Ablations highlight
the benefits of adaptive data selection, controlled rewriting, and balanced
integration, emphasizing the importance of aligning instruction tuning with
pre-training distributions to unlock the full potential of LLMs.; 50) Towards Lightweight Time Series Forecasting: a Patch-wise Transformer
  with Weak Data Enriching; Patch-wise Transformer based time series forecasting achieves superior
accuracy. However, this superiority relies heavily on intricate model design
with massive parameters, rendering both training and inference expensive, thus
preventing their deployments on edge devices with limited resources and low
latency requirements. In addition, existing methods often work in an
autoregressive manner, which take into account only historical values, but
ignore valuable, easy-to-obtain context information, such as weather forecasts,
date and time of day. To contend with the two limitations, we propose
LiPFormer, a novel Lightweight Patch-wise Transformer with weak data enriching.
First, to simplify the Transformer backbone, LiPFormer employs a novel
lightweight cross-patch attention and a linear transformation-based attention
to eliminate Layer Normalization and Feed Forward Network, two heavy components
in existing Transformers. Second, we propose a lightweight, weak data enriching
module to provide additional, valuable weak supervision to the training. It
enhances forecasting accuracy without significantly increasing model complexity
as it does not involve expensive, human-labeling but using easily accessible
context information. This facilitates the weak data enriching to plug-and-play
on existing models. Extensive experiments on nine benchmark time series
datasets demonstrate that LiPFormer outperforms state-of-the-art methods in
accuracy, while significantly reducing parameter scale, training duration, and
GPU memory usage. Deployment on an edge device reveals that LiPFormer takes
only 1/3 inference time compared to classic Transformers. In addition, we
demonstrate that the weak data enriching can integrate seamlessly into various
Transformer based models to enhance their accuracy, suggesting its generality.; 51) On the inductive bias of infinite-depth ResNets and the bottleneck rank; We compute the minimum-norm weights of a deep linear ResNet, and find that
the inductive bias of this architecture lies between minimizing nuclear norm
and rank. This implies that, with appropriate hyperparameters, deep nonlinear
ResNets have an inductive bias towards minimizing bottleneck rank.; 52) Fast Kd-trees for the Kullback--Leibler Divergence and other
  Decomposable Bregman Divergences; The contributions of the paper span theoretical and implementational results.
First, we prove that Kd-trees can be extended to spaces in which the distance
is measured with an arbitrary Bregman divergence. Perhaps surprisingly, this
shows that the triangle inequality is not necessary for correct pruning in
Kd-trees. Second, we offer an efficient algorithm and C++ implementation for
nearest neighbour search for decomposable Bregman divergences.
  The implementation supports the Kullback--Leibler divergence (relative
entropy) which is a popular distance between probability vectors and is
commonly used in statistics and machine learning. This is a step toward
broadening the usage of computational geometry algorithms.
  Our benchmarks show that our implementation efficiently handles both exact
and approximate nearest neighbour queries. Compared to a naive approach, we
achieve two orders of magnitude speedup for practical scenarios in dimension up
to 100. Our solution is simpler and more efficient than competing methods.; 53) ProMRVL-CAD: Proactive Dialogue System with Multi-Round Vision-Language
  Interactions for Computer-Aided Diagnosis; Recent advancements in large language models (LLMs) have demonstrated
extraordinary comprehension capabilities with remarkable breakthroughs on
various vision-language tasks. However, the application of LLMs in generating
reliable medical diagnostic reports remains in the early stages. Currently,
medical LLMs typically feature a passive interaction model where doctors
respond to patient queries with little or no involvement in analyzing medical
images. In contrast, some ChatBots simply respond to predefined queries based
on visual inputs, lacking interactive dialogue or consideration of medical
history. As such, there is a gap between LLM-generated patient-ChatBot
interactions and those occurring in actual patient-doctor consultations. To
bridge this gap, we develop an LLM-based dialogue system, namely proactive
multi-round vision-language interactions for computer-aided diagnosis
(ProMRVL-CAD), to generate patient-friendly disease diagnostic reports. The
proposed ProMRVL-CAD system allows proactive dialogue to provide patients with
constant and reliable medical access via an integration of knowledge graph into
a recommendation system. Specifically, we devise two generators: a Proactive
Question Generator (Pro-Q Gen) to generate proactive questions that guide the
diagnostic procedure and a Multi-Vision Patient-Text Diagnostic Report
Generator (MVP-DR Gen) to produce high-quality diagnostic reports. Evaluating
two real-world publicly available datasets, MIMIC-CXR and IU-Xray, our model
has better quality in generating medical reports. We further demonstrate the
performance of ProMRVL achieves robust under the scenarios with low image
quality. Moreover, we have created a synthetic medical dialogue dataset that
simulates proactive diagnostic interactions between patients and doctors,
serving as a valuable resource for training LLM.; 54) Reconstructing the shape of the non-linear matter power spectrum using
  CMB lensing and cosmic shear; We reconstruct the non-linear matter power spectrum $P(k)$ using a joint
analysis of gravitational lensing of the cosmic microwave background (CMB) and
lensing of galaxies. This reconstruction is motivated by the $S_8$ tension
between early-universe CMB predictions and late-time observables. We use CMB
lensing data from the Atacama Cosmology Telescope DR6 and cosmic shear data
from the Dark Energy Survey (DES) Y3 release to perform a gravity-only (i.e. no
baryonic feedback) fit to $P(k)$ in bins of wave-number, within $\rm{\Lambda
CDM}$. We find that with DES cosmic shear data alone, $P(k)$ departs from the
early-universe CMB prediction on all scales. The joint fit with CMB lensing is
consistent on large scales $k<0.2 \;{\rm Mpc}^{-1}$ but shows a $\sim 2 \sigma$
deviation from scale-independence when extending to $k = 10 \;h/\mathrm{Mpc}$.
We compare our agnostic $P(k)$ reconstruction to baryonic feedback models and
non-standard dark matter models: reasonable variations of both scenarios can
recover the shape and amplitude of the suppression. We discuss the advances
needed to disentangle these physical effects with a full mapping of $P(k,z)$.; 55) Fundamental algebraic sets and locally unit-additive rings; The Fundamental Theorem of Algebra can be thought of as a statement about the
real numbers as a space, considered as an algebraic set over the real numbers
as a field. In this paper, I introduce what it means for an algebraic set or
affine variety over a field to be fundamental, in a way that encompasses the
classical case. I also introduce the related concept of local fundamentality
and develop its behavior. On the algebraic side, I introduce the notions of
locally, geometrically, and generically unit-additive rings, thus complementing
unit-additivity as previously defined by myself and Jay Shapiro. I extend a
number of results from the previous joint paper to local unit-additivity. I
show that an affine variety is (locally) fundamental if and only if its
coordinate ring is (locally) unit-additive. To do so, I first prove a theorem
showing that there are many equivalent definitions of local unit-additivity.
Illustrative examples are sprinkled throughout.; 56) Logic Explanation of AI Classifiers by Categorical Explaining Functors; The most common methods in explainable artificial intelligence are post-hoc
techniques which identify the most relevant features used by pretrained opaque
models. Some of the most advanced post hoc methods can generate explanations
that account for the mutual interactions of input features in the form of logic
rules. However, these methods frequently fail to guarantee the consistency of
the extracted explanations with the model's underlying reasoning. To bridge
this gap, we propose a theoretically grounded approach to ensure coherence and
fidelity of the extracted explanations, moving beyond the limitations of
current heuristic-based approaches. To this end, drawing from category theory,
we introduce an explaining functor which structurally preserves logical
entailment between the explanation and the opaque model's reasoning. As a proof
of concept, we validate the proposed theoretical constructions on a synthetic
benchmark verifying how the proposed approach significantly mitigates the
generation of contradictory or unfaithful explanations.; 57) Addressing Intersectionality, Explainability, and Ethics in AI-Driven
  Diagnostics: A Rebuttal and Call for Transdiciplinary Action; The increasing integration of artificial intelligence (AI) into medical
diagnostics necessitates a critical examination of its ethical and practical
implications. While the prioritization of diagnostic accuracy, as advocated by
Sabuncu et al. (2025), is essential, this approach risks oversimplifying
complex socio-ethical issues, including fairness, privacy, and
intersectionality. This rebuttal emphasizes the dangers of reducing
multifaceted health disparities to quantifiable metrics and advocates for a
more transdisciplinary approach. By incorporating insights from social
sciences, ethics, and public health, AI systems can address the compounded
effects of intersecting identities and safeguard sensitive data. Additionally,
explainability and interpretability must be central to AI design, fostering
trust and accountability. This paper calls for a framework that balances
accuracy with fairness, privacy, and inclusivity to ensure AI-driven
diagnostics serve diverse populations equitably and ethically.; 58) Optimising expectation with guarantees for window mean payoff in Markov
  decision processes; The window mean-payoff objective strengthens the classical mean-payoff
objective by computing the mean-payoff over a finite window that slides along
an infinite path. Two variants have been considered: in one variant, the
maximum window length is fixed and given, while in the other, it is not fixed
but is required to be bounded. In this paper, we look at the problem of
synthesising strategies in Markov decision processes that maximise the window
mean-payoff value in expectation, while also simultaneously guaranteeing that
the value is above a certain threshold. We solve the synthesis problem for
three different kinds of guarantees: sure (that needs to be satisfied in the
worst-case, that is, for an adversarial environment), almost-sure (that needs
to be satisfied with probability one), and probabilistic (that needs to be
satisfied with at least some given probability $p$).
  We show that for fixed window mean-payoff objective, all the three problems
are in $\mathsf{PTIME}$, while for bounded window mean-payoff objective, they
are in $\mathsf{NP} \cap \mathsf{coNP}$, and thus have the same complexity as
for maximising the expected performance without any guarantee. Moreover, we
show that pure finite-memory strategies suffice for maximising the expectation
with sure and almost-sure guarantees, whereas, for maximising expectation with
a probabilistic guarantee, randomised strategies are necessary in general.; 59) Concat-ID: Towards Universal Identity-Preserving Video Synthesis; We present Concat-ID, a unified framework for identity-preserving video
generation. Concat-ID employs Variational Autoencoders to extract image
features, which are concatenated with video latents along the sequence
dimension, leveraging solely 3D self-attention mechanisms without the need for
additional modules. A novel cross-video pairing strategy and a multi-stage
training regimen are introduced to balance identity consistency and facial
editability while enhancing video naturalness. Extensive experiments
demonstrate Concat-ID's superiority over existing methods in both single and
multi-identity generation, as well as its seamless scalability to multi-subject
scenarios, including virtual try-on and background-controllable generation.
Concat-ID establishes a new benchmark for identity-preserving video synthesis,
providing a versatile and scalable solution for a wide range of applications.; 60) Towards Transparent and Accurate Diabetes Prediction Using Machine
  Learning and Explainable Artificial Intelligence; Diabetes mellitus (DM) is a global health issue of significance that must be
diagnosed as early as possible and managed well. This study presents a
framework for diabetes prediction using Machine Learning (ML) models,
complemented with eXplainable Artificial Intelligence (XAI) tools, to
investigate both the predictive accuracy and interpretability of the
predictions from ML models. Data Preprocessing is based on the Synthetic
Minority Oversampling Technique (SMOTE) and feature scaling used on the
Diabetes Binary Health Indicators dataset to deal with class imbalance and
variability of clinical features. The ensemble model provided high accuracy,
with a test accuracy of 92.50% and an ROC-AUC of 0.975. BMI, Age, General
Health, Income, and Physical Activity were the most influential predictors
obtained from the model explanations. The results of this study suggest that ML
combined with XAI is a promising means of developing accurate and
computationally transparent tools for use in healthcare systems.; 61) A Minimax Approach to Ad Hoc Teamwork; We propose a minimax-Bayes approach to Ad Hoc Teamwork (AHT) that optimizes
policies against an adversarial prior over partners, explicitly accounting for
uncertainty about partners at time of deployment. Unlike existing methods that
assume a specific distribution over partners, our approach improves worst-case
performance guarantees. Extensive experiments, including evaluations on
coordinated cooking tasks from the Melting Pot suite, show our method's
superior robustness compared to self-play, fictitious play, and best response
learning. Our work highlights the importance of selecting an appropriate
training distribution over teammates to achieve robustness in AHT.; 62) AI Generations: From AI 1.0 to AI 4.0; This paper proposes that Artificial Intelligence (AI) progresses through
several overlapping generations: AI 1.0 (Information AI), AI 2.0 (Agentic AI),
AI 3.0 (Physical AI), and now a speculative AI 4.0 (Conscious AI). Each of
these AI generations is driven by shifting priorities among algorithms,
computing power, and data. AI 1.0 ushered in breakthroughs in pattern
recognition and information processing, fueling advances in computer vision,
natural language processing, and recommendation systems. AI 2.0 built on these
foundations through real-time decision-making in digital environments,
leveraging reinforcement learning and adaptive planning for agentic AI
applications. AI 3.0 extended intelligence into physical contexts, integrating
robotics, autonomous vehicles, and sensor-fused control systems to act in
uncertain real-world settings. Building on these developments, AI 4.0 puts
forward the bold vision of self-directed AI capable of setting its own goals,
orchestrating complex training regimens, and possibly exhibiting elements of
machine consciousness. This paper traces the historical foundations of AI
across roughly seventy years, mapping how changes in technological bottlenecks
from algorithmic innovation to high-performance computing to specialized data,
have spurred each generational leap. It further highlights the ongoing
synergies among AI 1.0, 2.0, 3.0, and 4.0, and explores the profound ethical,
regulatory, and philosophical challenges that arise when artificial systems
approach (or aspire to) human-like autonomy. Ultimately, understanding these
evolutions and their interdependencies is pivotal for guiding future research,
crafting responsible governance, and ensuring that AI transformative potential
benefits society as a whole.; 63) Utilizing High Sampling Rate ADCs for Cost Efficient MIMO Radios; In the past decade, $>$1 Gsps ADCs have become commonplace and are used in
many modern 5G base station chips. A major driving force behind this adoption
is the benefits of digital up/down-conversion and improved digital filtering.
Recent works have also advocated for utilizing this high sampling bandwidth to
fit-in multiple MIMO streams, and reduce the number of ADCs required to build
MIMO base-stations. This can potentially reduce the cost of Massive MIMO RUs,
since ADCs are the most expensive electronics in the base-station radio chain.
However, these recent works do not model the necessary decimation filters that
exist in the signal path of these high sampling rate ADCs. We show in this
short paper that because of the decimation filters, there can be introduction
of cross-talks which can hinder the performance of these shared ADC interfaces.
We simulate the shared ADC interface with Matlab 5G toolbox for uplink MIMO,
and show that these cross-talks can be mitigated by performing MMSE
equalization atop the PUSCH estimated channels.; 64) Agent models: Internalizing Chain-of-Action Generation into Reasoning
  models; Traditional agentic workflows rely on external prompts to manage interactions
with tools and the environment, which limits the autonomy of reasoning models.
We position \emph{Large Agent Models (LAMs)} that internalize the generation of
\emph{Chain-of-Action (CoA)}, enabling the model to autonomously decide when
and how to use external tools. Our proposed AutoCoA framework combines
supervised fine-tuning (SFT) and reinforcement learning (RL), allowing the
model to seamlessly switch between reasoning and action while efficiently
managing environment interactions. Main components include step-level action
triggering, trajectory-level CoA optimization, and an internal world model to
reduce real-environment interaction costs. Evaluations on open-domain QA tasks
demonstrate that AutoCoA-trained agent models significantly outperform
ReAct-based workflows in task completion, especially in tasks that require
long-term reasoning and multi-step actions. Code and dataset are available at
https://github.com/ADaM-BJTU/AutoCoA; 65) Teaching Wav2Vec2 the Language of the Brain; The decoding of continuously spoken speech from neuronal activity has the
potential to become an important clinical solution for paralyzed patients. Deep
Learning Brain Computer Interfaces (BCIs) have recently successfully mapped
neuronal activity to text contents in subjects who attempted to formulate
speech. However, only small BCI datasets are available. In contrast, labeled
data and pre-trained models for the closely related task of speech recognition
from audio are widely available. One such model is Wav2Vec2 which has been
trained in a self-supervised fashion to create meaningful representations of
speech audio data. In this study, we show that patterns learned by Wav2Vec2 are
transferable to brain data. Specifically, we replace its audio feature
extractor with an untrained Brain Feature Extractor (BFE) model. We then
execute full fine-tuning with pre-trained weights for Wav2Vec2, training ''from
scratch'' without pre-trained weights as well as freezing a pre-trained
Wav2Vec2 and training only the BFE each for 45 different BFE architectures.
Across these experiments, the best run is from full fine-tuning with
pre-trained weights, achieving a Character Error Rate (CER) of 18.54\%,
outperforming the best training from scratch run by 20.46\% and that of frozen
Wav2Vec2 training by 15.92\% percentage points. These results indicate that
knowledge transfer from audio speech recognition to brain decoding is possible
and significantly improves brain decoding performance for the same
architectures. Related source code is available at
https://github.com/tfiedlerdev/Wav2Vec2ForBrain.; 66) Power-consumption Backdoor in Quantum Key Distribution; Over the last decades, Quantum Key Distribution (QKD) has risen as a
promising solution for secure communications. However, like all cryptographic
protocols, QKD implementations can open security vulnerabilities. Until now,
the study of physical vulnerabilities in QKD setups has primarily focused on
the optical channel. In classical cryptoanalysis, power and electromagnetic
side-channel analysis are powerful techniques used to access unwanted
information about the encryption key in symmetric-key algorithms. In QKD they
have rarely been used, since they require an eavesdropper to have access to
Alice or Bob's setups. However, security proofs of QKD protocols generally
assume that these setups are secure, making it crucial to understand the
necessary security measures to ensure this protection. In this work, we propose
and implement a power side-channel analysis to a QKD system, by exploiting the
power consumption of the electronic driver controlling the electro-optical
components of the QKD transmitter. QKD modules typically require very precise
electronic drivers, such as Field Programmable Gate Arrays (FPGAs). Here, we
show that the FPGA's power consumption can leak information about the QKD
operation, and consequently the transmitted key. The analysis was performed on
the QKD transmitter at the University of Padua. Our results are consistent and
show critical information leakage, having reached a maximum accuracy of 73.35%
in predicting transmitted qubits at a 100 MHz repetition frequency.; 67) Efficient Simulation of Quantum Secure Multiparty Computation; One of the key characteristics of secure quantum communication is quantum
secure multiparty computation. In this paper, we propose a quantum secure
multiparty summation (QSMS) protocol that can be applied to many complex
quantum operations. It is based on the $(t, n)$ threshold approach. We combine
the classical and quantum phenomena to make this protocol realistic and secure.
Because the current protocols employ the $(n, n)$ threshold approach, which
requires all honest players to execute the quantum multiparty summation
protocol, they have certain security and efficiency problems. However, we
employ a $(t, n)$ threshold approach, which requires the quantum summation
protocol to be computed only by $t$ honest players. Our suggested protocol is
more economical, practical, and secure than alternative protocols.; 68) Generating Symbolic World Models via Test-time Scaling of Large Language
  Models; Solving complex planning problems requires Large Language Models (LLMs) to
explicitly model the state transition to avoid rule violations, comply with
constraints, and ensure optimality-a task hindered by the inherent ambiguity of
natural language. To overcome such ambiguity, Planning Domain Definition
Language (PDDL) is leveraged as a planning abstraction that enables precise and
formal state descriptions. With PDDL, we can generate a symbolic world model
where classic searching algorithms, such as A*, can be seamlessly applied to
find optimal plans. However, directly generating PDDL domains with current LLMs
remains an open challenge due to the lack of PDDL training data. To address
this challenge, we propose to scale up the test-time computation of LLMs to
enhance their PDDL reasoning capabilities, thereby enabling the generation of
high-quality PDDL domains. Specifically, we introduce a simple yet effective
algorithm, which first employs a Best-of-N sampling approach to improve the
quality of the initial solution and then refines the solution in a fine-grained
manner with verbalized machine learning. Our method outperforms o1-mini by a
considerable margin in the generation of PDDL domain, achieving over 50%
success rate on two tasks (i.e., generating PDDL domains from natural language
description or PDDL problems). This is done without requiring additional
training. By taking advantage of PDDL as state abstraction, our method is able
to outperform current state-of-the-art methods on almost all competition-level
planning tasks.; 69) Embodied AI-Enhanced Vehicular Networks: An Integrated Large Language
  Models and Reinforcement Learning Method; This paper investigates adaptive transmission strategies in embodied
AI-enhanced vehicular networks by integrating large language models (LLMs) for
semantic information extraction and deep reinforcement learning (DRL) for
decision-making. The proposed framework aims to optimize both data transmission
efficiency and decision accuracy by formulating an optimization problem that
incorporates the Weber-Fechner law, serving as a metric for balancing bandwidth
utilization and quality of experience (QoE). Specifically, we employ the large
language and vision assistant (LLAVA) model to extract critical semantic
information from raw image data captured by embodied AI agents (i.e.,
vehicles), reducing transmission data size by approximately more than 90\%
while retaining essential content for vehicular communication and
decision-making. In the dynamic vehicular environment, we employ a generalized
advantage estimation-based proximal policy optimization (GAE-PPO) method to
stabilize decision-making under uncertainty. Simulation results show that
attention maps from LLAVA highlight the model's focus on relevant image
regions, enhancing semantic representation accuracy. Additionally, our proposed
transmission strategy improves QoE by up to 36\% compared to DDPG and
accelerates convergence by reducing required steps by up to 47\% compared to
pure PPO. Further analysis indicates that adapting semantic symbol length
provides an effective trade-off between transmission quality and bandwidth,
achieving up to a 61.4\% improvement in QoE when scaling from 4 to 8 vehicles.; 70) Applications of Large Models in Medicine; This paper explores the advancements and applications of large-scale models
in the medical field, with a particular focus on Medical Large Models (MedLMs).
These models, encompassing Large Language Models (LLMs), Vision Models, 3D
Large Models, and Multimodal Models, are revolutionizing healthcare by
enhancing disease prediction, diagnostic assistance, personalized treatment
planning, and drug discovery. The integration of graph neural networks in
medical knowledge graphs and drug discovery highlights the potential of Large
Graph Models (LGMs) in understanding complex biomedical relationships. The
study also emphasizes the transformative role of Vision-Language Models (VLMs)
and 3D Large Models in medical image analysis, anatomical modeling, and
prosthetic design. Despite the challenges, these technologies are setting new
benchmarks in medical innovation, improving diagnostic accuracy, and paving the
way for personalized healthcare solutions. This paper aims to provide a
comprehensive overview of the current state and future directions of large
models in medicine, underscoring their significance in advancing global health.; 71) LLM Reasoner and Automated Planner: A new NPC approach; In domains requiring intelligent agents to emulate plausible human-like
behaviour, such as formative simulations, traditional techniques like behaviour
trees encounter significant challenges. Large Language Models (LLMs), despite
not always yielding optimal solutions, usually offer plausible and human-like
responses to a given problem. In this paper, we exploit this capability and
propose a novel architecture that integrates an LLM for decision-making with a
classical automated planner that can generate sound plans for that decision.
The combination aims to equip an agent with the ability to make decisions in
various situations, even if they were not anticipated during the design phase.; 72) Compensation based Dictionary Transfer for Similar Multispectral Image
  Spectral Super-resolution; Utilizing a spectral dictionary learned from a couple of similar-scene multi-
and hyperspectral image, it is possible to reconstruct a desired hyperspectral
image only with one single multispectral image. However, the differences
between the similar scene and the desired hyperspectral image make it difficult
to directly apply the spectral dictionary from the training domain to the task
domain. To this end, a compensation matrix based dictionary transfer method for
the similar-scene multispectral image spectral super-resolution is proposed in
this paper, trying to reconstruct a more accurate high spatial resolution
hyperspectral image. Specifically, a spectral dictionary transfer scheme is
established by using a compensation matrix with similarity constraint, to
transfer the spectral dictionary learned in the training domain to the spectral
super-resolution domain. Subsequently, the sparse coefficient matrix is
optimized under sparse and low-rank constraints. Experimental results on two
AVIRIS datasets from different scenes indicate that, the proposed method
outperforms other related SOTA methods.; 73) BOOST: Microgrid Sizing using Ordinal Optimization; The transition to sustainable energy systems has highlighted the critical
need for efficient sizing of renewable energy resources in microgrids. In
particular, designing photovoltaic (PV) and battery systems to meet residential
loads is challenging due to trade-offs between cost, reliability, and
environmental impact. While previous studies have employed dynamic programming
and heuristic techniques for microgrid sizing, these approaches often fail to
balance computational efficiency and accuracy. In this work, we propose BOOST,
or Battery-solar Ordinal Optimization Sizing Technique, a novel framework for
optimizing the sizing of PV and battery components in microgrids. Ordinal
optimization enables computationally efficient evaluations of potential designs
while preserving accuracy through robust ranking of solutions. To determine the
optimal operation of the system at any given time, we introduce a mixed-integer
linear programming (MILP) approach, which achieves lower costs than the
commonly used dynamic programming methods. Our numerical experiments
demonstrate that the proposed framework identifies optimal designs that achieve
a levelized cost of energy (LCOE) as low as 8.84 cents/kWh, underscoring its
potential for cost-effective microgrid design. The implications of our work are
significant: BOOST provides a scalable and accurate methodology for integrating
renewable energy into residential microgrids, addressing economic and
environmental goals simultaneously.; 74) Early Operative Difficulty Assessment in Laparoscopic Cholecystectomy
  via Snapshot-Centric Video Analysis; Purpose: Laparoscopic cholecystectomy (LC) operative difficulty (LCOD) is
highly variable and influences outcomes. Despite extensive LC studies in
surgical workflow analysis, limited efforts explore LCOD using intraoperative
video data. Early recognition of LCOD could allow prompt review by expert
surgeons, enhance operating room (OR) planning, and improve surgical outcomes.
  Methods: We propose the clinical task of early LCOD assessment using limited
video observations. We design SurgPrOD, a deep learning model to assess LCOD by
analyzing features from global and local temporal resolutions (snapshots) of
the observed LC video. Also, we propose a novel snapshot-centric attention
(SCA) module, acting across snapshots, to enhance LCOD prediction. We introduce
the CholeScore dataset, featuring video-level LCOD labels to validate our
method.
  Results: We evaluate SurgPrOD on 3 LCOD assessment scales in the CholeScore
dataset. On our new metric assessing early and stable correct predictions,
SurgPrOD surpasses baselines by at least 0.22 points. SurgPrOD improves over
baselines by at least 9 and 5 percentage points in F1 score and top1-accuracy,
respectively, demonstrating its effectiveness in correct predictions.
  Conclusion: We propose a new task for early LCOD assessment and a novel
model, SurgPrOD analyzing surgical video from global and local perspectives.
Our results on the CholeScore dataset establishes a new benchmark to study LCOD
using intraoperative video data.; 75) A Frontier AI Risk Management Framework: Bridging the Gap Between
  Current AI Practices and Established Risk Management; The recent development of powerful AI systems has highlighted the need for
robust risk management frameworks in the AI industry. Although companies have
begun to implement safety frameworks, current approaches often lack the
systematic rigor found in other high-risk industries. This paper presents a
comprehensive risk management framework for the development of frontier AI that
bridges this gap by integrating established risk management principles with
emerging AI-specific practices. The framework consists of four key components:
(1) risk identification (through literature review, open-ended red-teaming, and
risk modeling), (2) risk analysis and evaluation using quantitative metrics and
clearly defined thresholds, (3) risk treatment through mitigation measures such
as containment, deployment controls, and assurance processes, and (4) risk
governance establishing clear organizational structures and accountability.
Drawing from best practices in mature industries such as aviation or nuclear
power, while accounting for AI's unique challenges, this framework provides AI
developers with actionable guidelines for implementing robust risk management.
The paper details how each component should be implemented throughout the
life-cycle of the AI system - from planning through deployment - and emphasizes
the importance and feasibility of conducting risk management work prior to the
final training run to minimize the burden associated with it.; 76) GAIPAT -Dataset on Human Gaze and Actions for Intent Prediction in
  Assembly Tasks; The primary objective of the dataset is to provide a better understanding of
the coupling between human actions and gaze in a shared working environment
with a cobot, with the aim of signifcantly enhancing the effciency and safety
of humancobot interactions. More broadly, by linking gaze patterns with
physical actions, the dataset offers valuable insights into cognitive processes
and attention dynamics in the context of assembly tasks. The proposed dataset
contains gaze and action data from approximately 80 participants, recorded
during simulated industrial assembly tasks. The tasks were simulated using
controlled scenarios in which participants manipulated educational building
blocks. Gaze data was collected using two different eye-tracking setups
-head-mounted and remote-while participants worked in two positions: sitting
and standing.; 77) Euclid Quick Data Release (Q1). The first catalogue of strong-lensing
  galaxy clusters; We present the first catalogue of strong lensing galaxy clusters identified
in the Euclid Quick Release 1 observations (covering $63.1\,\mathrm{deg^2}$).
This catalogue is the result of the visual inspection of 1260 cluster fields.
Each galaxy cluster was ranked with a probability,
$\mathcal{P}_{\mathrm{lens}}$, based on the number and plausibility of the
identified strong lensing features. Specifically, we identified 83
gravitational lenses with $\mathcal{P}_{\mathrm{lens}}>0.5$, of which 14 have
$\mathcal{P}_{\mathrm{lens}}=1$, and clearly exhibiting secure strong lensing
features, such as giant tangential and radial arcs, and multiple images.
Considering the measured number density of lensing galaxy clusters,
approximately $0.3\,\mathrm{deg}^{-2}$ for $\mathcal{P}_{\mathrm{lens}}>0.9$,
we predict that \Euclid\ will likely see more than 4500 strong lensing clusters
over the course of the mission. Notably, only three of the identified
cluster-scale lenses had been previously observed from space. Thus, \Euclid has
provided the first high-resolution imaging for the remaining $80$ galaxy
cluster lenses, including those with the highest probability. The identified
strong lensing features will be used for training deep-learning models for
identifying gravitational arcs and multiple images automatically in \Euclid
observations. This study confirms the huge potential of \Euclid for finding new
strong lensing clusters, enabling exciting new discoveries on the nature of
dark matter and dark energy and the study of the high-redshift Universe.; 78) On the birational geometry of $\mathbf{Q}$-Fano threefolds of large Fano
  index, II; This paper is a sequel to [arXiv:2403.18389]. We investigate the rationality
problem for $\mathbf{Q}$-Fano threefolds of Fano index $\ge 3$.; 79) Discovering Directly-Follows Graph Model for Acyclic Processes; Process mining is the common name for a range of methods and approaches aimed
at analysing and improving processes. Specifically, methods that aim to derive
process models from event logs fall under the category of process discovery.
Within the range of processes, acyclic processes form a distinct category. In
such processes, previously performed actions are not repeated, forming chains
of unique actions. However, due to differences in the order of actions,
existing process discovery methods can provide models containing cycles even if
a process is acyclic. This paper presents a new process discovery algorithm
that allows to discover acyclic DFG models for acyclic processes. A model is
discovered by partitioning an event log into parts that provide acyclic DFG
models and merging them while avoiding the formation of cycles. The resulting
algorithm was tested both on real-life and artificial event logs. Absence of
cycles improves model visual clarity and precision, also allowing to apply
cycle-sensitive methods or visualisations to the model.; 80) Singular perturbations models in phase transitions for anisotropic
  higher-order materials; We discuss a model for phase transitions in which a double-well potential is
singularly perturbed by possibly several terms involving different, arbitrarily
high orders of derivation. We study by $\Gamma$-convergence the asymptotic
behaviour as $\varepsilon\to 0$ of the functionals \begin{equation*}
  F_\varepsilon(u):=\int_\Omega
\Bigl[\frac{1}{\varepsilon}W(u)+\sum_{\ell=1}^{k}q_\ell\varepsilon^{2\ell-1}|\nabla^{(\ell)}u|_\ell^2\Bigr]\,dx,
\qquad u\in H^k(\Omega), \end{equation*} for fixed $k>1$ integer, addressing
also to the case in which the coefficients $q_1,...,q_{k-1}$ are negative and
$|\cdot|_\ell$ is any norm on the space of symmetric $\ell$-tensors for each
$\ell\in\{1,...,k\}$. The negativity of the coefficients leads to the lack of a
priori bounds on the functionals; such issue is overcome by proving a nonlinear
interpolation inequality. With this inequality at our disposal, a compactness
result is achieved by resorting to the recent paper [10]. A further difficulty
is the presence of general tensor norms which carry anisotropies, making
standard slicing arguments not suitable. We prove that the $\Gamma$-limit is
finite only on sharp interfaces and that it equals an anisotropic perimeter,
with a surface energy density described by a cell formula.; 81) Technological Understanding: On the cognitive skill involved in the
  design and use of technological artefacts; Although several accounts of scientific understanding exist, the concept of
understanding in relation to technology remains underexplored. This paper
addresses this gap by proposing a philosophical account of technological
understanding - the type of understanding that is required for and reflected by
successfully designing and using technological artefacts. We develop this
notion by building on the concept of scientific understanding. Drawing on
parallels between science and technology, and specifically between scientific
theories and technological artefacts, we extend the idea of scientific
understanding into the realm of technology. We argue that, just as scientific
understanding involves the ability to explain a phenomenon using a theory,
technological understanding involves the ability to use a technological
artefact to realise a practical aim. Technological understanding can thus be
considered a specific application of knowledge: it encompasses the cognitive
skill of recognising how a practical aim can be achieved by using a
technological artefact. In a context of design, this general notion of
technological understanding is specified as the ability to design an artefact
that, by producing a phenomenon through its physical structure, achieves the
intended aim. We illustrate our concept of technological understanding through
two running examples: magnetic resonance imaging (MRI) and superconducting
quantum computers. Our account highlights the epistemic dimension of engaging
with technology and, by allowing for context-dependent specifications, provides
guidance for testing and improving technological understanding in specific
contexts.; 82) Towards Developing Ethical Reasoners: Integrating Probabilistic
  Reasoning and Decision-Making for Complex AI Systems; A computational ethics framework is essential for AI and autonomous systems
operating in complex, real-world environments. Existing approaches often lack
the adaptability needed to integrate ethical principles into dynamic and
ambiguous contexts, limiting their effectiveness across diverse scenarios. To
address these challenges, we outline the necessary ingredients for building a
holistic, meta-level framework that combines intermediate representations,
probabilistic reasoning, and knowledge representation. The specifications
therein emphasize scalability, supporting ethical reasoning at both individual
decision-making levels and within the collective dynamics of multi-agent
systems. By integrating theoretical principles with contextual factors, it
facilitates structured and context-aware decision-making, ensuring alignment
with overarching ethical standards. We further explore proposed theorems
outlining how ethical reasoners should operate, offering a foundation for
practical implementation. These constructs aim to support the development of
robust and ethically reliable AI systems capable of navigating the complexities
of real-world moral decision-making scenarios.; 83) L2R: Learning to Reduce Search Space for Generalizable Neural Routing
  Solver; Constructive neural combinatorial optimization (NCO) has attracted growing
research attention due to its ability to solve complex routing problems without
relying on handcrafted rules. However, existing NCO methods face significant
challenges in generalizing to large-scale problems due to high computational
complexity and inefficient capture of structural patterns. To address this
issue, we propose a novel learning-based search space reduction method that
adaptively selects a small set of promising candidate nodes at each step of the
constructive NCO process. Unlike traditional methods that rely on fixed
heuristics, our selection model dynamically prioritizes nodes based on learned
patterns, significantly reducing the search space while maintaining solution
quality. Experimental results demonstrate that our method, trained solely on
100-node instances from uniform distribution, generalizes remarkably well to
large-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing
Problem (CVRP) instances with up to 1 million nodes from the uniform
distribution and over 80K nodes from other distributions.; 84) FedGAI: Federated Style Learning with Cloud-Edge Collaboration for
  Generative AI in Fashion Design; Collaboration can amalgamate diverse ideas, styles, and visual elements,
fostering creativity and innovation among different designers. In collaborative
design, sketches play a pivotal role as a means of expressing design
creativity. However, designers often tend to not openly share these
meticulously crafted sketches. This phenomenon of data island in the design
area hinders its digital transformation under the third wave of AI. In this
paper, we introduce a Federated Generative Artificial Intelligence Clothing
system, namely FedGAI, employing federated learning to aid in sketch design.
FedGAI is committed to establishing an ecosystem wherein designers can exchange
sketch styles among themselves. Through FedGAI, designers can generate sketches
that incorporate various designers' styles from their peers, drawing
inspiration from collaboration without the need for data disclosure or upload.
Extensive performance evaluations indicate that our FedGAI system can produce
multi-styled sketches of comparable quality to human-designed ones while
significantly enhancing efficiency compared to hand-drawn sketches.; 85) AutoMR: A Universal Time Series Motion Recognition Pipeline; In this paper, we present an end-to-end automated motion recognition (AutoMR)
pipeline designed for multimodal datasets. The proposed framework seamlessly
integrates data preprocessing, model training, hyperparameter tuning, and
evaluation, enabling robust performance across diverse scenarios. Our approach
addresses two primary challenges: 1) variability in sensor data formats and
parameters across datasets, which traditionally requires task-specific machine
learning implementations, and 2) the complexity and time consumption of
hyperparameter tuning for optimal model performance. Our library features an
all-in-one solution incorporating QuartzNet as the core model, automated
hyperparameter tuning, and comprehensive metrics tracking. Extensive
experiments demonstrate its effectiveness on 10 diverse datasets, achieving
state-of-the-art performance. This work lays a solid foundation for deploying
motion-capture solutions across varied real-world applications.; 86) Analyzing sequential activity and travel decisions with interpretable
  deep inverse reinforcement learning; Travel demand modeling has shifted from aggregated trip-based models to
behavior-oriented activity-based models because daily trips are essentially
driven by human activities. To analyze the sequential activity-travel
decisions, deep inverse reinforcement learning (DIRL) has proven effective in
learning the decision mechanisms by approximating a reward function to
represent preferences and a policy function to replicate observed behavior
using deep neural networks (DNNs). However, most existing research has focused
on using DIRL to enhance only prediction accuracy, with limited exploration
into interpreting the underlying decision mechanisms guiding sequential
decision-making. To address this gap, we introduce an interpretable DIRL
framework for analyzing activity-travel decision processes, bridging the gap
between data-driven machine learning and theory-driven behavioral models. Our
proposed framework adapts an adversarial IRL approach to infer the reward and
policy functions of activity-travel behavior. The policy function is
interpreted through a surrogate interpretable model based on choice
probabilities from the policy function, while the reward function is
interpreted by deriving both short-term rewards and long-term returns for
various activity-travel patterns. Our analysis of real-world travel survey data
reveals promising results in two key areas: (i) behavioral pattern insights
from the policy function, highlighting critical factors in decision-making and
variations among socio-demographic groups, and (ii) behavioral preference
insights from the reward function, indicating the utility individuals gain from
specific activity sequences.; 87) PersonaBench: Evaluating AI Models on Understanding Personal Information
  through Accessing (Synthetic) Private User Data; Personalization is critical in AI assistants, particularly in the context of
private AI models that work with individual users. A key scenario in this
domain involves enabling AI models to access and interpret a user's private
data (e.g., conversation history, user-AI interactions, app usage) to
understand personal details such as biographical information, preferences, and
social connections. However, due to the sensitive nature of such data, there
are no publicly available datasets that allow us to assess an AI model's
ability to understand users through direct access to personal information.
  To address this gap, we introduce a synthetic data generation pipeline that
creates diverse, realistic user profiles and private documents simulating human
activities. Leveraging this synthetic data, we present PersonaBench, a
benchmark designed to evaluate AI models' performance in understanding personal
information derived from simulated private user data.
  We evaluate Retrieval-Augmented Generation (RAG) pipelines using questions
directly related to a user's personal information, supported by the relevant
private documents provided to the models. Our results reveal that current
retrieval-augmented AI models struggle to answer private questions by
extracting personal information from user documents, highlighting the need for
improved methodologies to enhance personalization capabilities in AI.; 88) ExKG-LLM: Leveraging Large Language Models for Automated Expansion of
  Cognitive Neuroscience Knowledge Graphs; The paper introduces ExKG-LLM, a framework designed to automate the expansion
of cognitive neuroscience knowledge graphs (CNKG) using large language models
(LLMs). It addresses limitations in existing tools by enhancing accuracy,
completeness, and usefulness in CNKG. The framework leverages a large dataset
of scientific papers and clinical reports, applying state-of-the-art LLMs to
extract, optimize, and integrate new entities and relationships. Evaluation
metrics include precision, recall, and graph density. Results show significant
improvements: precision (0.80, +6.67%), recall (0.81, +15.71%), F1 score
(0.805, +11.81%), and increased edge nodes (21.13% and 31.92%). Graph density
slightly decreased, reflecting a broader but more fragmented structure.
Engagement rates rose by 20%, while CNKG diameter increased to 15, indicating a
more distributed structure. Time complexity improved to O(n log n), but space
complexity rose to O(n2), indicating higher memory usage. ExKG-LLM demonstrates
potential for enhancing knowledge generation, semantic search, and clinical
decision-making in cognitive neuroscience, adaptable to broader scientific
fields.; 89) A Study on Neuro-Symbolic Artificial Intelligence: Healthcare
  Perspectives; Over the last few decades, Artificial Intelligence (AI) scientists have been
conducting investigations to attain human-level performance by a machine in
accomplishing a cognitive task. Within machine learning, the ultimate
aspiration is to attain Artificial General Intelligence (AGI) through a
machine. This pursuit has led to the exploration of two distinct AI paradigms.
Symbolic AI, also known as classical or GOFAI (Good Old-Fashioned AI) and
Connectionist (Sub-symbolic) AI, represented by Neural Systems, are two
mutually exclusive paradigms. Symbolic AI excels in reasoning, explainability,
and knowledge representation but faces challenges in processing complex
real-world data with noise. Conversely, deep learning (Black-Box systems)
research breakthroughs in neural networks are notable, yet they lack reasoning
and interpretability. Neuro-symbolic AI (NeSy), an emerging area of AI
research, attempts to bridge this gap by integrating logical reasoning into
neural networks, enabling them to learn and reason with symbolic
representations. While a long path, this strategy has made significant progress
towards achieving common sense reasoning by systems. This article conducts an
extensive review of over 977 studies from prominent scientific databases (DBLP,
ACL, IEEExplore, Scopus, PubMed, ICML, ICLR), thoroughly examining the
multifaceted capabilities of Neuro-Symbolic AI, with a particular focus on its
healthcare applications, particularly in drug discovery, and Protein
engineering research. The survey addresses vital themes, including reasoning,
explainability, integration strategies, 41 healthcare-related use cases,
benchmarking, datasets, current approach limitations from both healthcare and
broader perspectives, and proposed novel approaches for future experiments.; 90) Newton Polytopes and Analytic Spread; Using the Newton polytope and polyhedron, we study analytic spread and ideal
reductions of monomial ideals. We determine a bound for analytic spread based
on halfspaces and hyperplanes of the Newton polytope, and we classify basic
monomial ideals. We then apply this method to calculate the analytic spread for
a few families of monomial ideals.; 91) Intelligence Sequencing and the Path-Dependence of Intelligence
  Evolution: AGI-First vs. DCI-First as Irreversible Attractors; The trajectory of intelligence evolution is often framed around the emergence
of artificial general intelligence (AGI) and its alignment with human values.
This paper challenges that framing by introducing the concept of intelligence
sequencing: the idea that the order in which AGI and decentralized collective
intelligence (DCI) emerge determines the long-term attractor basin of
intelligence. Using insights from dynamical systems, evolutionary game theory,
and network models, it argues that intelligence follows a path-dependent,
irreversible trajectory. Once development enters a centralized (AGI-first) or
decentralized (DCI-first) regime, transitions become structurally infeasible
due to feedback loops and resource lock-in. Intelligence attractors are modeled
in functional state space as the co-navigation of conceptual and adaptive
fitness spaces. Early-phase structuring constrains later dynamics, much like
renormalization in physics. This has major implications for AI safety:
traditional alignment assumes AGI will emerge and must be controlled after the
fact, but this paper argues that intelligence sequencing is more foundational.
If AGI-first architectures dominate before DCI reaches critical mass,
hierarchical monopolization and existential risk become locked in. If DCI-first
emerges, intelligence stabilizes around decentralized cooperative equilibrium.
The paper further explores whether intelligence structurally biases itself
toward an attractor based on its self-modeling method -- externally imposed
axioms (favoring AGI) vs. recursive internal visualization (favoring DCI).
Finally, it proposes methods to test this theory via simulations, historical
lock-in case studies, and intelligence network analysis. The findings suggest
that intelligence sequencing is a civilizational tipping point: determining
whether the future is shaped by unbounded competition or unbounded cooperation.; 92) Sustainable and Intelligent Public Facility Failure Management System
  Based on Large Language Models; This paper presents a new Large Language Model (LLM)-based Smart Device
Management framework, a pioneering approach designed to address the intricate
challenges of managing intelligent devices within public facilities, with a
particular emphasis on applications to libraries. Our framework leverages
state-of-the-art LLMs to analyze and predict device failures, thereby enhancing
operational efficiency and reliability. Through prototype validation in
real-world library settings, we demonstrate the framework's practical
applicability and its capacity to significantly reduce budgetary constraints on
public facilities. The advanced and innovative nature of our model is evident
from its successful implementation in prototype testing. We plan to extend the
framework's scope to include a wider array of public facilities and to
integrate it with cutting-edge cybersecurity technologies, such as Internet of
Things (IoT) security and machine learning algorithms for threat detection and
response. This will result in a comprehensive and proactive maintenance system
that not only bolsters the security of intelligent devices but also utilizes
machine learning for automated analysis and real-time threat mitigation. By
incorporating these advanced cybersecurity elements, our framework will be
well-positioned to tackle the dynamic challenges of modern public
infrastructure, ensuring robust protection against potential threats and
enabling facilities to anticipate and prevent failures, leading to substantial
cost savings and enhanced service quality.; 93) Extracting Problem Structure with LLMs for Optimized SAT Local Search; Local search preprocessing makes Conflict-Driven Clause Learning (CDCL)
solvers faster by providing high-quality starting points and modern SAT solvers
have incorporated this technique into their preprocessing steps. However, these
tools rely on basic strategies that miss the structural patterns in problems.
We present a method that applies Large Language Models (LLMs) to analyze
Python-based encoding code. This reveals hidden structural patterns in how
problems convert into SAT. Our method automatically generates specialized local
search algorithms that find these patterns and use them to create strong
initial assignments. This works for any problem instance from the same encoding
type. Our tests show encouraging results, achieving faster solving times
compared to baseline preprocessing systems.; 94) Indeterminacy in Affective Computing: Considering Meaning and Context in
  Data Collection Practices; Automatic Affect Prediction (AAP) uses computational analysis of input data
such as text, speech, images, and physiological signals to predict various
affective phenomena (e.g., emotions or moods). These models are typically
constructed using supervised machine-learning algorithms, which rely heavily on
labeled training datasets. In this position paper, we posit that all AAP
training data are derived from human Affective Interpretation Processes,
resulting in a form of Affective Meaning. Research on human affect indicates a
form of complexity that is fundamental to such meaning: it can possess what we
refer to here broadly as Qualities of Indeterminacy (QIs) - encompassing
Subjectivity (meaning depends on who is interpreting), Uncertainty (lack of
confidence regarding meanings' correctness), Ambiguity (meaning contains
mutually exclusive concepts) and Vagueness (meaning is situated at different
levels in a nested hierarchy). Failing to appropriately consider QIs leads to
results incapable of meaningful and reliable predictions. Based on this
premise, we argue that a crucial step in adequately addressing indeterminacy in
AAP is the development of data collection practices for modeling corpora that
involve the systematic consideration of 1) a relevant set of QIs and 2) context
for the associated interpretation processes. To this end, we are 1) outlining a
conceptual model of AIPs and the QIs associated with the meaning these produce
and a conceptual structure of relevant context, supporting understanding of its
role. Finally, we use our framework for 2) discussing examples of
context-sensitivity-related challenges for addressing QIs in data collection
setups. We believe our efforts can stimulate a structured discussion of both
the role of aspects of indeterminacy and context in research on AAP, informing
the development of better practices for data collection and analysis.; 95) Not eXactly Byzantine: Efficient and Resilient TEE-Based State Machine
  Replication; We propose, implement, and evaluate NxBFT, a practical State Machine
Replication protocol that tolerates minority corruptions by using Trusted
Execution Environments (TEEs). NxBFT focuses on a ""Not eXactly Byzantine""
operating model as a middle ground between crash and Byzantine fault tolerance.
NxBFT is designed as an asynchronous protocol except for liveness of setup and
recovery. As a leaderless protocol based on TEE-Rider, it provides build-in
load balancing in the number of replicas, which is in contrast to leader-based
and leader-rotating approaches. With quadratic communication complexity, a
TEE-based common coin as source of randomness, a crash recovery procedure,
solutions for request deduplication, and progress in low-load scenarios, NxBFT
achieves a throughput of 400 kOp/s at an average end-to-end-latency of 1 s for
40 replicas and shows competitive performance under faults. We provide a
comparison with a leader-based (MinBFT) and a leader-rotating protocol
(Damysus) and analyze benefits and challenges that result from the combination
of asynchrony and TEEs.; 96) Electron-positron pair production in bound-bound muon transitions; We explored a distinct mechanism for matter creation via electron-positron
pair production during bound-bound transitions in the deexcitation of one-muon
ions. For ions with nuclear charges $Z\geq24$, transitions from low-lying
excited states to the 1s-muon state can lead to the production of
electron-positron pairs. We show that the Breit interaction determines the
transition probabilities for states with nonzero orbital momentum. Although the
$2s$ state is metastable, pair production arises mainly from the decay of the
$2p$ states. Thus, the Breit interaction governs electron-positron pair
production in bound-bound muon transitions. This process offers a unique
opportunity to explore quantum electrodynamics in strong fields, as well as a
class of nonradiative transitions involving electron-positron pair production.; 97) Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents; While Large Language Models (LLMs) can exhibit impressive proficiency in
isolated, short-term tasks, they often fail to maintain coherent performance
over longer time horizons. In this paper, we present Vending-Bench, a simulated
environment designed to specifically test an LLM-based agent's ability to
manage a straightforward, long-running business scenario: operating a vending
machine. Agents must balance inventories, place orders, set prices, and handle
daily fees - tasks that are each simple but collectively, over long horizons
(>20M tokens per run) stress an LLM's capacity for sustained, coherent
decision-making. Our experiments reveal high variance in performance across
multiple LLMs: Claude 3.5 Sonnet and o3-mini manage the machine well in most
runs and turn a profit, but all models have runs that derail, either through
misinterpreting delivery schedules, forgetting orders, or descending into
tangential ""meltdown"" loops from which they rarely recover. We find no clear
correlation between failures and the point at which the model's context window
becomes full, suggesting that these breakdowns do not stem from memory limits.
Apart from highlighting the high variance in performance over long time
horizons, Vending-Bench also tests models' ability to acquire capital, a
necessity in many hypothetical dangerous AI scenarios. We hope the benchmark
can help in preparing for the advent of stronger AI systems.; 98) H\""arpfer's Extended Indispensability Algorithm in Z; Since 1978, Clarence Barlow developed the ``Indispensability Function''. It
operates on a metric tree that is bound to the same prime number of branches
for all subtrees of each particular level. It assigns to all leaf postions of
this tree a numeric value which indicates how important the acoustic presence
of an event at this position is for the meter to be recognized as such.
  Bernd H\""arpfer extended this concept in 2015 to deal with meters which have
arbitrary groupings into two or three at any position of the tree hierarchy.
This is called ``Extended Indispensability Algorithm''.
  This article gives a specification of the Extended Algorithm in a slightly
extended version of the Z specification language, and a possible generalization
to arbitrary metric trees.; 99) GraphCheck: Breaking Long-Term Text Barriers with Extracted Knowledge
  Graph-Powered Fact-Checking; Large language models (LLMs) are widely used, but they often generate subtle
factual errors, especially in long-form text. These errors are fatal in some
specialized domains such as medicine. Existing fact-checking with grounding
documents methods face two main challenges: (1) they struggle to understand
complex multihop relations in long documents, often overlooking subtle factual
errors; (2) most specialized methods rely on pairwise comparisons, requiring
multiple model calls, leading to high resource and computational costs. To
address these challenges, we propose \textbf{\textit{GraphCheck}}, a
fact-checking framework that uses extracted knowledge graphs to enhance text
representation. Graph Neural Networks further process these graphs as a soft
prompt, enabling LLMs to incorporate structured knowledge more effectively.
Enhanced with graph-based reasoning, GraphCheck captures multihop reasoning
chains which are often overlooked by existing methods, enabling precise and
efficient fact-checking in a single inference call. Experimental results on
seven benchmarks spanning both general and medical domains demonstrate a 6.1\%
overall improvement over baseline models. Notably, GraphCheck outperforms
existing specialized fact-checkers and achieves comparable performance with
state-of-the-art LLMs, such as DeepSeek-V3 and OpenAI-o1, with significantly
fewer parameters.; 100) Psychometric-Based Evaluation for Theorem Proving with Large Language
  Models; Large language models (LLMs) for formal theorem proving have become a
prominent research focus. At present, the proving ability of these LLMs is
mainly evaluated through proof pass rates on datasets such as miniF2F. However,
this evaluation method overlooks the varying importance of theorems. As a
result, it fails to highlight the real performance disparities between LLMs and
leads to high evaluation costs. This study proposes a psychometric-based
evaluation method for theorem proving with LLMs, comprising two main
components: Dataset Annotation and Adaptive Evaluation. First, we propose a
metric calculation method to annotate the dataset with difficulty and
discrimination metrics. Specifically, we annotate each theorem in the miniF2F
dataset and grade them into varying difficulty levels according to the
performance of LLMs, resulting in an enhanced dataset: miniF2F-Graded.
Experimental results show that the difficulty grading in miniF2F-Graded better
reflects the theorem difficulty perceived by LLMs. Secondly, we design an
adaptive evaluation method to dynamically select the most suitable theorems for
testing based on the annotated metrics and the real-time performance of LLMs.
We apply this method to evaluate 10 LLMs. The results show that our method
finely highlights the performance disparities between LLMs. It also reduces
evaluation costs by using only 23% of the theorems in the dataset.",0.0,0.0
2411.00561,applied,2411.00561-pos2-8,"Retrieval and classification of shape-based objects using Fourier, generic Fourier, and wavelet-Fourier descriptors technique: A comparative study; In this paper, we report retrieval and classification of shape-based objects employing three techniques-conventional Fourier descriptors (FD), generic Fourier descriptors (GFD) and wavelet-Fourier descriptors (WFD) techniques. All the three techniques have been applied to a database of seven different types of shapes. The centroid distance based shape signatures have been used for the derivation of descriptors. The Euclidean distance has been calculated as a similarity measure parameter for shape classification. For WFD technique, a Mexican-hat wavelet function was used. Classification results from all the three techniques were compared and it was observed that WFD performs better than FD and GFD technique. To study the effect of the noise on the retrieval and classification of shapes of different objects, additive and multiplicative noise of various variances were applied to the database. Precision and recall were also measured as parameters of performance metric.",2411.00561-pos1-8,"What is a cell type, really? The quest to categorize life’s myriad forms; The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. “I realized that there were probably hundreds of flavours,” recalls Buenrostro, who was a graduate student at Stanford University in California at the time.",96,"['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']","The first candidate paper, 'MemPal: Leveraging Multimodal AI and LLMs for Voice-Activated Object Retrieval in Homes of Older Adults', aligns well with the main paper on shape retrieval and classification. It introduces a novel application of AI to assist older adults in object retrieval, which complements the main paper's focus on shape identification techniques. This integration of voice-activated AI and shape classification enhances the practical utility of the techniques discussed in the main paper, making it a strong candidate for forming a multidisciplinary research idea. The subsequent papers, while interesting, do not directly relate as effectively to the combination of shape-based object retrieval and classification with practical applications in aiding older adults.","1) MemPal: Leveraging Multimodal AI and LLMs for Voice-Activated Object
  Retrieval in Homes of Older Adults; Older adults have increasing difficulty with retrospective memory, hindering
their abilities to perform daily activities and posing stress on caregivers to
ensure their wellbeing. Recent developments in Artificial Intelligence (AI) and
large context-aware multimodal models offer an opportunity to create memory
support systems that assist older adults with common issues like object
finding. This paper discusses the development of an AI-based, wearable memory
assistant, MemPal, that helps older adults with a common problem, finding lost
objects at home, and presents results from tests of the system in older adults'
own homes. Using visual context from a wearable camera, the multimodal LLM
system creates a real-time automated text diary of the person's activities for
memory support purposes, offering object retrieval assistance using a
voice-based interface. The system is designed to support additional use cases
like context-based proactive safety reminders and recall of past actions. We
report on a quantitative and qualitative study with N=15 older adults within
their own homes that showed improved performance of object finding with
audio-based assistance compared to no aid and positive overall user perceptions
on the designed system. We discuss further applications of MemPal's design as a
multi-purpose memory aid and future design guidelines to adapt memory
assistants to older adults' unique needs.; 2) Gaussian-Process-based Adaptive Tracking Control with Dynamic Active
  Learning for Autonomous Ground Vehicles; This article proposes an active-learning-based adaptive trajectory tracking
control method for autonomous ground vehicles to compensate for modeling errors
and unmodeled dynamics. The nominal vehicle model is decoupled into lateral and
longitudinal subsystems, which are augmented with online Gaussian Processes
(GPs), using measurement data. The estimated mean functions of the GPs are used
to construct a feedback compensator, which, together with an LPV state feedback
controller designed for the nominal system, gives the adaptive control
structure. To assist exploration of the dynamics, the paper proposes a new,
dynamic active learning method to collect the most informative samples to
accelerate the training process. To analyze the performance of the overall
learning tool-chain provided controller, a novel iterative,
counterexample-based algorithm is proposed for calculating the induced L2 gain
between the reference trajectory and the tracking error. The analysis can be
executed for a set of possible realizations of the to-be-controlled system,
giving robust performance certificate of the learning method under variation of
the vehicle dynamics. The efficiency of the proposed control approach is shown
on a high-fidelity physics simulator and in real experiments using a 1/10 scale
F1TENTH electric car.; 3) The Multi-Faceted Monosemanticity in Multimodal Representations; In this paper, we leverage recent advancements in feature monosemanticity to
extract interpretable features from deep multimodal models, offering a
data-driven understanding of modality gaps. Specifically, we investigate CLIP
(Contrastive Language-Image Pretraining), a prominent visual-language
representation model trained on extensive image-text pairs. Building upon
interpretability tools developed for single-modal models, we extend these
methodologies to assess multi-modal interpretability of CLIP features.
Additionally, we introduce the Modality Dominance Score (MDS) to attribute the
interpretability of each feature to its respective modality. Next, we transform
CLIP features into a more interpretable space, enabling us to categorize them
into three distinct classes: vision features (single-modal), language features
(single-modal), and visual-language features (cross-modal). Our findings reveal
that this categorization aligns closely with human cognitive understandings of
different modalities. We also demonstrate significant use cases of this
modality-specific features including detecting gender bias, adversarial attack
defense and text-to-image model editing. These results indicate that
large-scale multimodal models, equipped with task-agnostic interpretability
tools, offer valuable insights into key connections and distinctions between
different modalities.; 4) Detection of [C\,{\sc i}] Emission in Nebular Spectra of a Peculiar Type
  Ia Supernova 2022pul; SN~2022pul gains special attention due to its possible origin of a
super-Chandarsekhar-mass white dwarf explosion (or called a 03fg-like type Ia
supernova), which shows prominent [O\,{\sc i}], [Ne\,{\sc i}], and [Ca\,{\sc
ii}] lines in its late-time spectra taken at $\sim+$300 days after the peak
brightness. In this paper, we present new optical observations for this
peculiar object, extending up to over 500 days after the peak brightness. In
particular, in the $t\approx+515$ days spectrum, we identified for the first
time the presence of narrow emission from [C\,{\sc i}] $\lambda\lambda9824,
9850$, which appears asymmetric and quite similar to the accompanied [O\,{\sc
i}] $\lambda6300$ line in strength and profile. Based on the violent merger
model that accounts well for previous observations but leaves little carbon in
the center of the ejecta, this carbon line can be reproduced by increasing the
degree of clumping in the ejecta and setting the carbon mass the same as that
of oxygen ($\sim$0.06 $M_{\odot}$) in the innermost region ($\lesssim 2000$ km
s$^{-1}$). In principle, the central carbon could come from the secondary white
dwarf (WD) if it is ignited when hit by the shockwave of the explosion of the
primary WD and explodes as a Ca-rich supernova, whereas pure deflagration of a
super-Chandarsekhar-mass WD can account for such unburnt carbon more naturally.; 5) URECA: The Chain of Two Minimum Set Cover Problems exists behind
  Adaptation to Shifts in Semantic Code Search; Adaptation is to make model learn the patterns shifted from the training
distribution. In general, this adaptation is formulated as the minimum entropy
problem. However, the minimum entropy problem has inherent limitation --
shifted initialization cascade phenomenon. We extend the relationship between
the minimum entropy problem and the minimum set cover problem via Lebesgue
integral. This extension reveals that internal mechanism of the minimum entropy
problem ignores the relationship between disentangled representations, which
leads to shifted initialization cascade. From the analysis, we introduce a new
clustering algorithm, Union-find based Recursive Clustering Algorithm~(URECA).
URECA is an efficient clustering algorithm for the leverage of the
relationships between disentangled representations. The update rule of URECA
depends on Thresholdly-Updatable Stationary Assumption to dynamics as a
released version of Stationary Assumption. This assumption helps URECA to
transport disentangled representations with no errors based on the
relationships between disentangled representations. URECA also utilize
simulation trick to efficiently cluster disentangled representations. The wide
range of evaluations show that URECA achieves consistent performance gains for
the few-shot adaptation to diverse types of shifts along with advancement to
State-of-The-Art performance in CoSQA in the scenario of query shift.; 6) Infrastructure for AI Agents; Increasingly many AI systems can plan and execute interactions in open-ended
environments, such as making phone calls or buying online goods. As developers
grow the space of tasks that such AI agents can accomplish, we will need tools
both to unlock their benefits and manage their risks. Current tools are largely
insufficient because they are not designed to shape how agents interact with
existing institutions (e.g., legal and economic systems) or actors (e.g.,
digital service providers, humans, other AI agents). For example, alignment
techniques by nature do not assure counterparties that some human will be held
accountable when a user instructs an agent to perform an illegal action. To
fill this gap, we propose the concept of agent infrastructure: technical
systems and shared protocols external to agents that are designed to mediate
and influence their interactions with and impacts on their environments. Agent
infrastructure comprises both new tools and reconfigurations or extensions of
existing tools. For example, to facilitate accountability, protocols that tie
users to agents could build upon existing systems for user authentication, such
as OpenID. Just as the Internet relies on infrastructure like HTTPS, we argue
that agent infrastructure will be similarly indispensable to ecosystems of
agents. We identify three functions for agent infrastructure: 1) attributing
actions, properties, and other information to specific agents, their users, or
other actors; 2) shaping agents' interactions; and 3) detecting and remedying
harmful actions from agents. We propose infrastructure that could help achieve
each function, explaining use cases, adoption, limitations, and open questions.
Making progress on agent infrastructure can prepare society for the adoption of
more advanced agents.; 7) Discovering Directly-Follows Graph Model for Acyclic Processes; Process mining is the common name for a range of methods and approaches aimed
at analysing and improving processes. Specifically, methods that aim to derive
process models from event logs fall under the category of process discovery.
Within the range of processes, acyclic processes form a distinct category. In
such processes, previously performed actions are not repeated, forming chains
of unique actions. However, due to differences in the order of actions,
existing process discovery methods can provide models containing cycles even if
a process is acyclic. This paper presents a new process discovery algorithm
that allows to discover acyclic DFG models for acyclic processes. A model is
discovered by partitioning an event log into parts that provide acyclic DFG
models and merging them while avoiding the formation of cycles. The resulting
algorithm was tested both on real-life and artificial event logs. Absence of
cycles improves model visual clarity and precision, also allowing to apply
cycle-sensitive methods or visualisations to the model.; 8) Dynamic Imprints of Colliding-wind Dust Formation from WR140; Carbon-rich Wolf-Rayet binaries are a prominent source of carbonaceous dust
that contribute to the dust budget of galaxies. The ""textbook"" example of an
episodic dust producing WR binary, WR140 (HD193793), provides us with an ideal
laboratory for investigating the dust physics and kinematics in an extreme
environment. This study is among the first to utilize two separate JWST
observations, from Cycle 1 ERS (July 2022) and Cycle 2 (Sept. 2023), to measure
WR140's dust kinematics and confirm its morphology. To measure the proper
motions and projected velocities of the dust shells, we performed a novel PSF
subtraction to reduce the effects of the bright diffraction spikes and
carefully aligned the Cycle 2 to the Cycle 1 images. At 7.7 $\mu$m, through the
bright feature common to 16 dust shells (C1), we find an average dust shell
proper motion of $390\pm29$ mas yr$^{-1}$, which equates to a projected
velocity of $2714\pm188$ km s$^{-1}$ at a distance of 1.64 kpc. Our measured
speeds are constant across all visible shells and consistent with previously
reported dust expansion velocities. Our observations not only prove that these
dusty shells are astrophysical (i.e., not associated with any PSF artifact) and
originate from WR140, but also confirm the ""clumpy"" morphology of the dust
shells, in which identifiable substructures within certain shells persist for
at least 14 months from one cycle to the next. These results support the
hypothesis that clumping in the wind collision region is required for dust
production in WR binaries.; 9) Membership and Conjugacy in Inverse Semigroups; The membership problem for an algebraic structure asks whether a given
element is contained in some substructure, which is usually given by
generators. In this work we study the membership problem, as well as the
conjugacy problem, for finite inverse semigroups. The closely related
membership problem for finite semigroups has been shown to be PSPACE-complete
in the transformation model by Kozen (1977) and NL-complete in the Cayley table
model by Jones, Lien, and Laaser (1976). More recently, both the membership and
the conjugacy problem for finite inverse semigroups were shown to be
PSPACE-complete in the partial bijection model by Jack (2023).
  Here we present a more detailed analysis of the complexity of the membership
and conjugacy problems parametrized by varieties of finite inverse semigroups.
We establish dichotomy theorems for the partial bijection model and for the
Cayley table model. In the partial bijection model these problems are in NC
(resp. NP for conjugacy) for strict inverse semigroups and PSPACE-complete
otherwise. In the Cayley table model we obtain general LOGSPACE-algorithms as
well as NPOLYLOGTIME upper bounds for Clifford semigroups and
LOGSPACE-completeness otherwise.
  Furthermore, by applying our findings, we show the following: the
intersection non-emptiness problem for inverse automata is PSPACE-complete even
for automata with only two states; the subpower membership problem is in NC for
every strict inverse semi-group and PSPACE-complete otherwise; the minimum
generating set and the equation satisfiability problems are in NP for varieties
of finite strict inverse semigroups and PSPACE-complete otherwise.; 10) Pace in Concert with Phase: Rate-induced Phase-tipping in Birhythmic
  Oscillators; We study rate-induced phase-tipping (RP-tipping) between two stable limit
cycles of a birhythmic oscillator. We say that such an oscillator RP-tips when
a time variation of an input parameter preserves the bistability of the limit
cycles but induces transitions from one stable limit cycle to the other,
causing abrupt changes in the amplitude and frequency of the oscillations.
Crucially, these transitions occur when: the rate of change of the input is in
a certain interval bounded by critical rate(s), and the system is in certain
phases of the cycle.
  We focus on two illustrative examples: the birhythmic van der Pol oscillator
and the birhythmic Decroly-Goldbeter glycolysis model, each subjected to
monotone and non-monotone shifts in their input parameters. We explain
RP-tipping in terms of properties of the autonomous frozen system, including
the phase of a cycle and partial basin instability along the parameter path
traced by the changing input. We show that RP-tipping can occur as an
irreversible one-way transition or as a series of transitions between the
stable limit cycles. Finally, we present RP-tipping diagrams showing
combinations of the rate and magnitude of parameter shifts and the phase of the
oscillation that give rise to this genuine non-autonomous instability.; 11) Capital and CHI: Technological Capture and How It Structures CHI
  Research; This paper advances a theoretical argument about the role capital plays in
structuring CHI research. We introduce the concept of technological capture to
theorize the mechanism by which this happens. Using this concept, we decompose
the effect on CHI into four broad forms: technological capture creates
market-creating, market-expanding, market-aligned, and externality-reducing CHI
research. We place different CHI subcommunities into these forms -- arguing
that many of their values are inherited from capital underlying the field.
Rather than a disciplinary- or conference-oriented conceptualization of the
field, this work theorizes CHI as tightly-coupled with capital via
technological capture. The paper concludes by discussing some implications for
CHI.; 12) Compare Similarities Between DNA Sequences Using Permutation-Invariant
  Quantum Kernel; Computing the similarity between two DNA sequences is of vital importance in
bioscience. However, traditional computational methods can be
resource-intensive due to the enormous sequence length encountered in practice.
Recently, applied quantum algorithms have been anticipated to provide potential
advantages over classical approaches. In this paper, we propose a
permutation-invariant variational quantum kernel method specifically designed
for DNA comparison. To represent the four nucleotide bases in DNA sequences
with quantum states, we introduce a novel, theoretically motivated encoding
scheme: the four distinct bases are encoded using the states of symmetric,
informationally complete, positive operator-valued measures (SIC-POVMs). This
encoding ensures mutual equality: each pair of symbols is equidistant on the
Bloch sphere. Also, since permutation invariance is inherent to common DNA
similarity measures such as Levenshtein distance, we realize it by using a
specially designed parameterized quantum layer. We show that our novel encoding
method and parameterized layers used in the quantum kernel model can
effectively capture the symmetric characteristics of the pairwise DNA sequence
comparison task. We validate our model through numerical experiments, which
yield promising results on length-$8$ DNA sequences.; 13) Characterization of Highly Robust Solutions in Multi-Objective
  Programming in Banach Spaces; This paper delves into the challenging issues in uncertain multi-objective
optimization, where uncertainty permeates nonsmooth nonconvex objective and
constraint functions. In this context, we investigate highly robust (weakly
efficient) solutions, a solution concept defined by efficiency across all
scenarios. Our exploration reveals important relationships between highly
robust solutions and other robustness notions, including set-based and
worst-case notions, as well as connections with proper and isolated efficiency.
Leveraging modern techniques from variational analysis, we establish necessary
and sufficient optimality conditions for these solutions. Moreover, we explore
the robustness of multi-objective optimization problems in the face of various
uncertain sets, such as ball, ellipsoidal, and polyhedral sets.; 14) Agent models: Internalizing Chain-of-Action Generation into Reasoning
  models; Traditional agentic workflows rely on external prompts to manage interactions
with tools and the environment, which limits the autonomy of reasoning models.
We position \emph{Large Agent Models (LAMs)} that internalize the generation of
\emph{Chain-of-Action (CoA)}, enabling the model to autonomously decide when
and how to use external tools. Our proposed AutoCoA framework combines
supervised fine-tuning (SFT) and reinforcement learning (RL), allowing the
model to seamlessly switch between reasoning and action while efficiently
managing environment interactions. Main components include step-level action
triggering, trajectory-level CoA optimization, and an internal world model to
reduce real-environment interaction costs. Evaluations on open-domain QA tasks
demonstrate that AutoCoA-trained agent models significantly outperform
ReAct-based workflows in task completion, especially in tasks that require
long-term reasoning and multi-step actions. Code and dataset are available at
https://github.com/ADaM-BJTU/AutoCoA; 15) Comprehensive Review of Neural Differential Equations for Time Series
  Analysis; Time series modeling and analysis has become critical in various domains.
Conventional methods such as RNNs and Transformers, while effective for
discrete-time and regularly sampled data, face significant challenges in
capturing the continuous dynamics and irregular sampling patterns inherent in
real-world scenarios. Neural Differential Equations (NDEs) represent a paradigm
shift by combining the flexibility of neural networks with the mathematical
rigor of differential equations. This paper presents a comprehensive review of
NDE-based methods for time series analysis, including neural ordinary
differential equations, neural controlled differential equations, and neural
stochastic differential equations. We provide a detailed discussion of their
mathematical formulations, numerical methods, and applications, highlighting
their ability to model continuous-time dynamics. Furthermore, we address key
challenges and future research directions. This survey serves as a foundation
for researchers and practitioners seeking to leverage NDEs for advanced time
series analysis.; 16) V2X-LLM: Enhancing V2X Integration and Understanding in Connected
  Vehicle Corridors; The advancement of Connected and Automated Vehicles (CAVs) and
Vehicle-to-Everything (V2X) offers significant potential for enhancing
transportation safety, mobility, and sustainability. However, the integration
and analysis of the diverse and voluminous V2X data, including Basic Safety
Messages (BSMs) and Signal Phase and Timing (SPaT) data, present substantial
challenges, especially on Connected Vehicle Corridors. These challenges include
managing large data volumes, ensuring real-time data integration, and
understanding complex traffic scenarios. Although these projects have developed
an advanced CAV data pipeline that enables real-time communication between
vehicles, infrastructure, and other road users for managing connected vehicle
and roadside unit (RSU) data, significant hurdles in data comprehension and
real-time scenario analysis and reasoning persist. To address these issues, we
introduce the V2X-LLM framework, a novel enhancement to the existing CV data
pipeline. V2X-LLM leverages Large Language Models (LLMs) to improve the
understanding and real-time analysis of V2X data. The framework includes four
key tasks: Scenario Explanation, offering detailed narratives of traffic
conditions; V2X Data Description, detailing vehicle and infrastructure
statuses; State Prediction, forecasting future traffic states; and Navigation
Advisory, providing optimized routing instructions. By integrating LLM-driven
reasoning with V2X data within the data pipeline, the V2X-LLM framework offers
real-time feedback and decision support for traffic management. This
integration enhances the accuracy of traffic analysis, safety, and traffic
optimization. Demonstrations in a real-world urban corridor highlight the
framework's potential to advance intelligent transportation systems.; 17) Rethinking Relation Extraction: Beyond Shortcuts to Generalization with
  a Debiased Benchmark; Benchmarks are crucial for evaluating machine learning algorithm performance,
facilitating comparison and identifying superior solutions. However, biases
within datasets can lead models to learn shortcut patterns, resulting in
inaccurate assessments and hindering real-world applicability. This paper
addresses the issue of entity bias in relation extraction tasks, where models
tend to rely on entity mentions rather than context. We propose a debiased
relation extraction benchmark DREB that breaks the pseudo-correlation between
entity mentions and relation types through entity replacement. DREB utilizes
Bias Evaluator and PPL Evaluator to ensure low bias and high naturalness,
providing a reliable and accurate assessment of model generalization in entity
bias scenarios. To establish a new baseline on DREB, we introduce MixDebias, a
debiasing method combining data-level and model training-level techniques.
MixDebias effectively improves model performance on DREB while maintaining
performance on the original dataset. Extensive experiments demonstrate the
effectiveness and robustness of MixDebias compared to existing methods,
highlighting its potential for improving the generalization ability of relation
extraction models. We will release DREB and MixDebias publicly.; 18) Multi-wavelength Constraints on Dust Dynamics and Size Evolution in
  Protoplanetary Disk Rings. I. Method; Observations with the Atacama Large Millimeter/submillimeter Array (ALMA) and
the Jansky Very Large Array (JVLA) have revealed many dust rings in
protoplanetary disks, often interpreted as dust traps at gas pressure bumps.
Previous studies have typically modeled these rings by assuming a single dust
species in drift-diffusion equilibrium, neglecting dust size evolution
resulting from coagulation and fragmentation. In this work, we perform
numerical simulations that incorporate both dust-gas dynamics (drift and
diffusion) and dust size evolution. Our results show that the radial
distributions of different dust species are nearly identical, as dust growth
dominates over drift and diffusion. Building on this finding, we develop a
comprehensive, self-consistent analytical theory that describes the dust ring
structure while explicitly accounting for size evolution effects. Our model
provides a unified framework for interpreting multi-wavelength observations by
linking the physical dust distribution to the observed ring properties, thus
laying the foundation for future observational modeling.; 19) Token-Hungry, Yet Precise: DeepSeek R1 Highlights the Need for
  Multi-Step Reasoning Over Speed in MATH; This study investigates the performance of the DeepSeek R1 language model on
30 challenging mathematical problems derived from the MATH dataset, problems
that previously proved unsolvable by other models under time constraints.
Unlike prior work, this research removes time limitations to explore whether
DeepSeek R1's architecture, known for its reliance on token-based reasoning,
can achieve accurate solutions through a multi-step process. The study compares
DeepSeek R1 with four other models (gemini-1.5-flash-8b,
gpt-4o-mini-2024-07-18, llama3.1:8b, and mistral-8b-latest) across 11
temperature settings. Results demonstrate that DeepSeek R1 achieves superior
accuracy on these complex problems but generates significantly more tokens than
other models, confirming its token-intensive approach. The findings highlight a
trade-off between accuracy and efficiency in mathematical problem-solving with
large language models: while DeepSeek R1 excels in accuracy, its reliance on
extensive token generation may not be optimal for applications requiring rapid
responses. The study underscores the importance of considering task-specific
requirements when selecting an LLM and emphasizes the role of temperature
settings in optimizing performance.; 20) Feasible Path SQP Algorithm for Simulation-based Optimization Surrogated
  with Differentiable Machine Learning Models; With the development of artificial intelligence, simulation-based
optimization problems, which present a significant challenge in the process
systems engineering community, are increasingly being addressed with the
surrogate-based framework. In this work, we propose a deterministic algorithm
framework based on feasible path sequential quadratic programming for
optimizing differentiable machine learning models embedded problems. The
proposed framework effectively addresses two key challenges: (i) achieving the
computation of first- and second-order derivatives of machine learning models'
outputs with respect to inputs; and (ii) by introducing the feasible path
method, the massive intermediate variables resulting from the algebraic
formulation of machine learning models eliminated. Surrogate models for six
test functions and two process simulations were established and optimized. All
six test functions were successfully optimized to the global optima,
demonstrating the framework's effectiveness. The optimization time for all
cases did not exceed 2s, highlighting the efficiency of the algorithm.; 21) Artificial Liver Classifier: A New Alternative to Conventional Machine
  Learning Models; Supervised machine learning classifiers often encounter challenges related to
performance, accuracy, and overfitting. This paper introduces the Artificial
Liver Classifier (ALC), a novel supervised learning classifier inspired by the
human liver's detoxification function. The ALC is characterized by its
simplicity, speed, hyperparameters-free, ability to reduce overfitting, and
effectiveness in addressing multi-classification problems through
straightforward mathematical operations. To optimize the ALC's parameters, an
improved FOX optimization algorithm (IFOX) is employed as the training method.
The proposed ALC was evaluated on five benchmark machine learning datasets:
Iris Flower, Breast Cancer Wisconsin, Wine, Voice Gender, and MNIST. The
results demonstrated competitive performance, with the ALC achieving 100%
accuracy on the Iris dataset, surpassing logistic regression, multilayer
perceptron, and support vector machine. Similarly, on the Breast Cancer
dataset, it achieved 99.12% accuracy, outperforming XGBoost and logistic
regression. Across all datasets, the ALC consistently exhibited lower
overfitting gaps and loss compared to conventional classifiers. These findings
highlight the potential of leveraging biological process simulations to develop
efficient machine learning models and open new avenues for innovation in the
field.; 22) Uncooled Thermal Infrared Detection Near the Fundamental Limit Using a
  Nanomechanical Resonator with a Broadband Absorber; This paper introduces a thermal infrared detector utilizing a
nano-optomechanical silicon nitride (SiN) resonator, equipped with a free-space
impedance-matched (FSIM) absorber composed of a platinum (Pt) thin film,
offering a broadband spectral absorptance on average of 47%. To reduce
photothermal back-action caused by intensity fluctuations of the readout laser,
the FSIM absorber incorporates a circular clearance for the laser. The study
provides a comprehensive characterization of the thermal time constant, power
responsivity, and frequency stability of the resonators, with experimental
results compared to analytical models and finite element method (FEM)
simulations. The fastest thermal response is observed for the smallest 1 mm
resonators, with a thermal time constant tau_th = 14 ms. The noise equivalent
power (NEP) of the resonators is assessed, showing that the smallest 1 mm
resonators exhibit the best sensitivity, with NEP = 27 pW/sqrt(Hz) and a
respective specific detectivity of D* = 3.8e9 cm sqrt(Hz)/W. This is less than
three times below the theoretical maximum for an ideal IR detector with 50%
absorptance. This places our resonators among the most sensitive
room-temperature IR detectors reported to date offering an extended spectral
range from the near-IR to far-IR. This work underscores the potential of
nano-optomechanical resonators for high-performance IR sensing applications.; 23) A Scalable Approach to Probabilistic Neuro-Symbolic Verification; Neuro-Symbolic Artificial Intelligence (NeSy AI) has emerged as a promising
direction for integrating neural learning with symbolic reasoning. In the
probabilistic variant of such systems, a neural network first extracts a set of
symbols from sub-symbolic input, which are then used by a symbolic component to
reason in a probabilistic manner towards answering a query. In this work, we
address the problem of formally verifying the robustness of such NeSy
probabilistic reasoning systems, therefore paving the way for their safe
deployment in critical domains. We analyze the complexity of solving this
problem exactly, and show that it is $\mathrm{NP}^{\# \mathrm{P}}$-hard. To
overcome this issue, we propose the first approach for approximate,
relaxation-based verification of probabilistic NeSy systems. We demonstrate
experimentally that the proposed method scales exponentially better than
solver-based solutions and apply our technique to a real-world autonomous
driving dataset, where we verify a safety property under large input
dimensionalities and network sizes.; 24) Cognitive-Aligned Document Selection for Retrieval-augmented Generation; Large language models (LLMs) inherently display hallucinations since the
precision of generated texts cannot be guaranteed purely by the parametric
knowledge they include. Although retrieval-augmented generation (RAG) systems
enhance the accuracy and reliability of generative models by incorporating
external documents, these retrieved documents often fail to adequately support
the model's responses in practical applications. To address this issue, we
propose GGatrieval (Fine-\textbf{G}rained \textbf{G}rounded \textbf{A}lignment
Re\textbf{trieval} for verifiable generation), which leverages an LLM to
dynamically update queries and filter high-quality, reliable retrieval
documents. Specifically, we parse the user query into its syntactic components
and perform fine-grained grounded alignment with the retrieved documents. For
query components that cannot be individually aligned, we propose a dynamic
semantic compensation mechanism that iteratively refines and rewrites the query
while continuously updating the retrieval results. This iterative process
continues until the retrieved documents sufficiently support the query's
response. Our approach introduces a novel criterion for filtering retrieved
documents, closely emulating human strategies for acquiring targeted
information. This ensures that the retrieved content effectively supports and
verifies the generated outputs. On the ALCE benchmark, our method significantly
surpasses a wide range of baselines, achieving state-of-the-art performance.; 25) Upper Mid-Band Spectrum for 6G: Vision, Opportunity and Challenges; Driven by the pursuit of gigabit-per-second data speeds for future 6G mobile
networks, in addition to the support of sensing and artificial intelligence
applications, the industry is expanding beyond crowded sub-6 GHz bands with
innovative new spectrum allocations. In this paper, we chart a compelling
vision for 6G within the frequency range 3 (FR3) spectrum, i.e. $7.125$-$24.25$
$\GHz$, by delving into its key enablers and addressing the multifaceted
challenges that lie ahead for these new frequency bands. Here we highlight the
physical properties of this \textcolor{black}{never-before} used spectrum by
reviewing recent channel measurements for outdoor and indoor environments,
including path loss, delay and angular spreads, and material penetration loss,
all which offer insights that underpin future 5G/6G wireless communication
designs. Building on the fundamental knowledge of the channel properties, we
explore FR3 spectrum agility strategies that balance coverage and capacity
(e.g. data rate) tradeoffs, while also examining coexistence with incumbent
systems, such as satellites, radio astronomy, and earth exploration. Moreover,
we discuss the potential of massive multiple-input multiple-output, compact and
digital architectures, and evaluate the potential of multiband sensing for FR3
integrated sensing and communications. Finally, we outline 6G standardization
features that are likely to emerge from 3GPP radio frame innovations and open
radio access network developments.; 26) Analytic continuation of time in Brownian motion. Stochastic
  distributions approach; With the use of Hida's white noise space theory space theory and spaces of
stochastic distributions, we present a detailed analytic continuation theory
for classes of Gaussian processes, with focus here on Brownian motion. For the
latter, we prove and make use a priori bounds, in the complex plane, for the
Hermite functions; as well as a new approach to stochastic distributions. This
in turn allows us to present an explicit formula for an analytically continued
white noise process, realized this way in complex domain. With the use of the
Wick product, we then apply our complex white noise analysis in a derivation of
a new realization of Hilbert space-valued stochastic integrals; 27) KU AIGEN ICL EDI@BC8 Track 3: Advancing Phenotype Named Entity
  Recognition and Normalization for Dysmorphology Physical Examination Reports; The objective of BioCreative8 Track 3 is to extract phenotypic key medical
findings embedded within EHR texts and subsequently normalize these findings to
their Human Phenotype Ontology (HPO) terms. However, the presence of diverse
surface forms in phenotypic findings makes it challenging to accurately
normalize them to the correct HPO terms. To address this challenge, we explored
various models for named entity recognition and implemented data augmentation
techniques such as synonym marginalization to enhance the normalization step.
Our pipeline resulted in an exact extraction and normalization F1 score 2.6\%
higher than the mean score of all submissions received in response to the
challenge. Furthermore, in terms of the normalization F1 score, our approach
surpassed the average performance by 1.9\%. These findings contribute to the
advancement of automated medical data extraction and normalization techniques,
showcasing potential pathways for future research and application in the
biomedical domain.; 28) Generating Symbolic World Models via Test-time Scaling of Large Language
  Models; Solving complex planning problems requires Large Language Models (LLMs) to
explicitly model the state transition to avoid rule violations, comply with
constraints, and ensure optimality-a task hindered by the inherent ambiguity of
natural language. To overcome such ambiguity, Planning Domain Definition
Language (PDDL) is leveraged as a planning abstraction that enables precise and
formal state descriptions. With PDDL, we can generate a symbolic world model
where classic searching algorithms, such as A*, can be seamlessly applied to
find optimal plans. However, directly generating PDDL domains with current LLMs
remains an open challenge due to the lack of PDDL training data. To address
this challenge, we propose to scale up the test-time computation of LLMs to
enhance their PDDL reasoning capabilities, thereby enabling the generation of
high-quality PDDL domains. Specifically, we introduce a simple yet effective
algorithm, which first employs a Best-of-N sampling approach to improve the
quality of the initial solution and then refines the solution in a fine-grained
manner with verbalized machine learning. Our method outperforms o1-mini by a
considerable margin in the generation of PDDL domain, achieving over 50%
success rate on two tasks (i.e., generating PDDL domains from natural language
description or PDDL problems). This is done without requiring additional
training. By taking advantage of PDDL as state abstraction, our method is able
to outperform current state-of-the-art methods on almost all competition-level
planning tasks.; 29) Evaluation of tortuosity: A radical tessellation-based method in porous
  spherical particle packing systems; Estimating the tortuosity of porous electrodes is important for understanding
the performance of lithium-ion batteries and optimizing the design of electrode
microstructures. In this work, a new method for estimating the tortuosity of
porous electrodes is proposed based on radical tessellation, and the results
agree well with those calculated by empirical formulas and finite element
simulations. Compared with empirical formulas, the proposed method can estimate
tortuosity for more complicated microstructures, regardless of particle
distribution and size dispersity; compared with finite element simulations, the
proposed method offers a significant advantage in computational efficiency.
Based on the method, the influence of different microstructure characters on
the tortuosity is discussed. It is found that in addition to the porosity, the
particle size and particle aggregation morphology are all critical in
influencing the tortuosity of the porous electrode. Finally, the tortuosity
obtained by this method is integrated into the Pseudo-2-dimensional (P2D) model
for the calculation of effective properties, and the results show that this
method offers an efficient way in improving the prediction accuracy of P2D
models.; 30) Kinetic theory of decentralized learning for smart active matter; Smart active matter describes agents which can process information according
to their individual policy to solve predefined tasks. We introduce a
theoretical framework to study a decentralized learning process in which agents
can locally exchange their policy to optimize their reward and thus attain
target macroscopic states. We use our formalism to derive explicit hydrodynamic
equations for the policy dynamics. We apply the theory to two different
microscopic models where policies correspond either to fixed parameters similar
to evolutionary dynamics, or to state-dependent controllers known from the
field of robotics. We find good agreement between theoretical predictions and
agent-based simulations. By deriving fundamental control parameters and
uncertainty relations, our work lays the foundations for a statistical physics
analysis of decentralized learning.; 31) Intrinsic Spin Transport in a Topological Insulator Thin Film; Topological insulators (TIs) are intriguing materials for advanced computing
applications based on spintronics because they can host robust spin effects.
For instance, TIs have intrinsically large spin generation enabled by their
large spin-orbit coupling. Furthermore, topological surface states (TSS) with
spin-momentum locking and Dirac dispersion lead to long spin diffusion. Future
spintronic device technology will require scalable film growth of high-quality
material. We grow epitaxial films of Bi$_{1-x}$Sb$_x$Te$_{3-y}$Se$_y$ (BSTS, $x
= 0.58, y = 1$) and confirm the gapless band structure with optimal doping
using angle-resolved photoelectron spectra. The temperature dependence of
longitudinal resistivity shows bulk transport is suppressed as temperature is
decreased, and at low temperature surface transport dominates. We evaluate the
spin transport properties in BSTS without using ferromagnetic tunnel contacts
via a non-local resistance experiment as a function of temperature and applied
charge current. As expected, these experiments reveal the necessity of
decreasing the bulk conduction to best enhance the spin transport. In the TSS,
we find high efficiency of charge-to-spin conversion (spin Hall angle,
$\theta_{SH} \approx 1$) and spin diffusion over several microns. Further
development of high-quality TIs will make them viable candidates for efficient
and lossless spintronics.; 32) Elastic Restaking Networks; Decentralized services for blockchains often require their validators
(operators) to deposit stake (collateral), which is forfeited (slashed) if they
misbehave. Restaking networks let validators secure multiple services by
reusing stake, giving rise to a strategic game: Validators can coordinate to
misbehave across multiple services, extracting digital assets while forfeiting
their stake only once.
  Previous work focused either on preventing coordinated misbehavior or on
protecting services if all other services are Byzantine and might unjustly
cause slashing due to bugs or malice. The first model overlooks how a single
Byzantine service can collapse the network, while the second ignores
shared-stake benefits.
  To bridge the gap, we model the strategic game of coordinated misbehavior
when a given fraction of services are Byzantine. We introduce elastic restaking
networks, where validators can allocate portions of their stake that may
cumulatively exceed their total stake, and when allocations are lost, the
remaining stake stretches to cover remaining allocations. We show that elastic
networks exhibit superior robustness compared to previous approaches, and
demonstrate a synergistic effect where an elastic restaking network enhances
its blockchain's security, contrary to community concerns of an opposite effect
in existing networks. We then design incentives for tuning validators'
allocations.
  Our elastic restaking system and incentive design have immediate practical
implications for deployed restaking networks, which have billions of dollars in
stake.; 33) PersonaBench: Evaluating AI Models on Understanding Personal Information
  through Accessing (Synthetic) Private User Data; Personalization is critical in AI assistants, particularly in the context of
private AI models that work with individual users. A key scenario in this
domain involves enabling AI models to access and interpret a user's private
data (e.g., conversation history, user-AI interactions, app usage) to
understand personal details such as biographical information, preferences, and
social connections. However, due to the sensitive nature of such data, there
are no publicly available datasets that allow us to assess an AI model's
ability to understand users through direct access to personal information.
  To address this gap, we introduce a synthetic data generation pipeline that
creates diverse, realistic user profiles and private documents simulating human
activities. Leveraging this synthetic data, we present PersonaBench, a
benchmark designed to evaluate AI models' performance in understanding personal
information derived from simulated private user data.
  We evaluate Retrieval-Augmented Generation (RAG) pipelines using questions
directly related to a user's personal information, supported by the relevant
private documents provided to the models. Our results reveal that current
retrieval-augmented AI models struggle to answer private questions by
extracting personal information from user documents, highlighting the need for
improved methodologies to enhance personalization capabilities in AI.; 34) Humans Co-exist, So Must Embodied Artificial Agents; Modern embodied artificial agents excel in static, predefined tasks but fall
short in dynamic and long-term interactions with humans. On the other hand,
humans can adapt and evolve continuously, exploiting the situated knowledge
embedded in their environment and other agents, thus contributing to meaningful
interactions. We introduce the concept of co-existence for embodied artificial
agents and argues that it is a prerequisite for meaningful, long-term
interaction with humans. We take inspiration from biology and design theory to
understand how human and non-human organisms foster entities that co-exist
within their specific niches. Finally, we propose key research directions for
the machine learning community to foster co-existing embodied agents, focusing
on the principles, hardware and learning methods responsible for shaping them.; 35) Logic Explanation of AI Classifiers by Categorical Explaining Functors; The most common methods in explainable artificial intelligence are post-hoc
techniques which identify the most relevant features used by pretrained opaque
models. Some of the most advanced post hoc methods can generate explanations
that account for the mutual interactions of input features in the form of logic
rules. However, these methods frequently fail to guarantee the consistency of
the extracted explanations with the model's underlying reasoning. To bridge
this gap, we propose a theoretically grounded approach to ensure coherence and
fidelity of the extracted explanations, moving beyond the limitations of
current heuristic-based approaches. To this end, drawing from category theory,
we introduce an explaining functor which structurally preserves logical
entailment between the explanation and the opaque model's reasoning. As a proof
of concept, we validate the proposed theoretical constructions on a synthetic
benchmark verifying how the proposed approach significantly mitigates the
generation of contradictory or unfaithful explanations.; 36) FedGAI: Federated Style Learning with Cloud-Edge Collaboration for
  Generative AI in Fashion Design; Collaboration can amalgamate diverse ideas, styles, and visual elements,
fostering creativity and innovation among different designers. In collaborative
design, sketches play a pivotal role as a means of expressing design
creativity. However, designers often tend to not openly share these
meticulously crafted sketches. This phenomenon of data island in the design
area hinders its digital transformation under the third wave of AI. In this
paper, we introduce a Federated Generative Artificial Intelligence Clothing
system, namely FedGAI, employing federated learning to aid in sketch design.
FedGAI is committed to establishing an ecosystem wherein designers can exchange
sketch styles among themselves. Through FedGAI, designers can generate sketches
that incorporate various designers' styles from their peers, drawing
inspiration from collaboration without the need for data disclosure or upload.
Extensive performance evaluations indicate that our FedGAI system can produce
multi-styled sketches of comparable quality to human-designed ones while
significantly enhancing efficiency compared to hand-drawn sketches.; 37) Eigenvalue selectors for representations of compact connected groups; A representation $\rho$ of a compact group $\mathbb{G}$ selects eigenvalues
if there is a continuous circle-valued map on $\mathbb{G}$ assigning an
eigenvalue of $\rho(g)$ to every $g\in \mathbb{G}$. For every compact connected
$\mathbb{G}$, we characterize the irreducible $\mathbb{G}$-representations
which select eigenvalues as precisely those annihilating the intersection
$Z_0(\mathbb{G})\cap \mathbb{G}'$ of the connected center of $\mathbb{G}$ with
its derived subgroup. The result applies more generally to finite-spectrum
representations isotypic on $Z_0(\mathbb{G})$, and recovers as applications
(noted in prior work) the existence of a continuous eigenvalue selector for the
natural representation of $\mathrm{SU}(n)$ and the non-existence of such a
selector for $\mathrm{U}(n)$.; 38) The Bullseye: HST, Keck/KCWI, and Dragonfly Characterization of a Giant
  Nine-Ringed Galaxy; We report the discovery and multiwavelength followup of LEDA 1313424
(""Bullseye""), a collisional ring galaxy (CRG) with nine readily-identified
rings -- the most so far reported for a CRG. These data shed new light on the
rapid, multi-ring phase of CRG evolution. Using Hubble Space Telescope (HST)
imaging, we identify and measure nine ring structures, several of which are
""piled up"" near the center of the galaxy, while others extend to tens of kpc
scales. We also identify faint patches of emission at large radii ($\sim$70
kpc) in the HST imaging, and confirm the association of this emission with the
galaxy via spectroscopy. Deep ground based imaging using the Dragonfly
Telephoto Array finds evidence that this patch of emission is part of an older,
fading ring from the collision. We find that the locations of the detected
rings are an excellent match to predictions from analytic theory, if the galaxy
was a 10-ring system whose outermost ring has faded away. We identify the
likely impacting galaxy via Keck/KCWI spectroscopy, finding evidence for gas
extending between it and the Bullseye. The overall size of this galaxy rivals
that of known Giant Low Surface Brightness Galaxies (GLSBs) such as Malin I,
lending credence to the hypothesis that CRGs can evolve into GLSBs as their
rings expand and fade. Analysis of the HI content in this galaxy from ALFALFA
finds significantly elevated neutral hydrogen with respect to the galaxy's
stellar mass, another feature in alignment with GLSB systems.; 39) On Choquard-Kirchhoff Type Critical Multiphase Problem; In this paper, we obtain the existence of weak solutions to the
Choquard-Kirchhoff type critical multiphase problem: \begin{equation*}
\left\{\begin{array}{cc}
  &-M(\varphi_{\h}(\lvert{\nabla u}\rvert))div(\lvert{\nabla
u}\rvert^{p(x)-2}\nabla u+a_1(x)\lvert{\nabla u}\rvert^{q(x)-2}\nabla
u+a_2(x)\lvert{\nabla u}\rvert^{r(x)-2}\nabla u)
  & =\lambda g(x)\lvert{u}\rvert^{\gamma(x)-2}u+\theta B(x,u)+\kappa
\left(\int_{\q}\frac{F(y,u(y))}{\lvert{x-y}\rvert^{d(x,y)}}\, dy\right) f(x,u)
\ \text{in} \ \Omega,
  & u=0 \ \text{on} \ {\partial \Omega}. \end{array}\right. \end{equation*}
  The term $B(x,u)$ on the right-hand side generalizes the critical growth. We
obtain existence and multiplicity results by establishing certain embedding
results and concentration compactness principle along with the
Hardy-Littlewood-Sobolev type inequality for the Musielak Orlicz Sobolev space
$ W^{1,\mathcal{T}}(\q)$.; 40) Pinching-Antenna Systems (PASS): Architecture Designs, Opportunities,
  and Outlook; This article proposes a novel design for the Pinching Antenna SyStems (PASS)
and advocates simple yet efficient wireless communications over the ``last
meter''. First, the potential benefits of PASS are discussed in the paper by
reviewing an existing prototype. Then, the fundamentals of PASS are introduced,
including their physical principles, signal models, and communication designs.
In contrast to existing multi-antenna systems, PASS brings a novel concept,
termed \emph{Pinching Beamforming}, which is achieved by dynamically adjusting
the positions of pinching antennas (PAs). Based on this concept, a couple of
practical transmission architectures are proposed for realizing the full
potentia of PASS, namely non-multiplexing and multiplexing architectures. More
particularly, 1) the non-multiplexing architecture is featured by simple
baseband signal processing and relies on the pinching beamforming only; while
2) the multiplexing architecture provides enhanced signal processing
capabilities with joint baseband and pinching beamforming, which can be further
divided into sub-connected, fully-connected, and phase-shifter-based
fully-connected schemes. Furthermore, several emerging scenarios are put
forward for integrating PASS into future wireless networks. As a further
advance, by demonstrating a few numerical case studies, the significant
performance gain of PASS is revealed compared to conventional multi-antenna
systems. Finally, several research opportunities and open problems of PASS are
highlighted.; 41) A Frontier AI Risk Management Framework: Bridging the Gap Between
  Current AI Practices and Established Risk Management; The recent development of powerful AI systems has highlighted the need for
robust risk management frameworks in the AI industry. Although companies have
begun to implement safety frameworks, current approaches often lack the
systematic rigor found in other high-risk industries. This paper presents a
comprehensive risk management framework for the development of frontier AI that
bridges this gap by integrating established risk management principles with
emerging AI-specific practices. The framework consists of four key components:
(1) risk identification (through literature review, open-ended red-teaming, and
risk modeling), (2) risk analysis and evaluation using quantitative metrics and
clearly defined thresholds, (3) risk treatment through mitigation measures such
as containment, deployment controls, and assurance processes, and (4) risk
governance establishing clear organizational structures and accountability.
Drawing from best practices in mature industries such as aviation or nuclear
power, while accounting for AI's unique challenges, this framework provides AI
developers with actionable guidelines for implementing robust risk management.
The paper details how each component should be implemented throughout the
life-cycle of the AI system - from planning through deployment - and emphasizes
the importance and feasibility of conducting risk management work prior to the
final training run to minimize the burden associated with it.; 42) MathMistake Checker: A Comprehensive Demonstration for Step-by-Step Math
  Problem Mistake Finding by Prompt-Guided LLMs; We propose a novel system, MathMistake Checker, designed to automate
step-by-step mistake finding in mathematical problems with lengthy answers
through a two-stage process. The system aims to simplify grading, increase
efficiency, and enhance learning experiences from a pedagogical perspective. It
integrates advanced technologies, including computer vision and the
chain-of-thought capabilities of the latest large language models (LLMs). Our
system supports open-ended grading without reference answers and promotes
personalized learning by providing targeted feedback. We demonstrate its
effectiveness across various types of math problems, such as calculation and
word problems.; 43) Psychometric-Based Evaluation for Theorem Proving with Large Language
  Models; Large language models (LLMs) for formal theorem proving have become a
prominent research focus. At present, the proving ability of these LLMs is
mainly evaluated through proof pass rates on datasets such as miniF2F. However,
this evaluation method overlooks the varying importance of theorems. As a
result, it fails to highlight the real performance disparities between LLMs and
leads to high evaluation costs. This study proposes a psychometric-based
evaluation method for theorem proving with LLMs, comprising two main
components: Dataset Annotation and Adaptive Evaluation. First, we propose a
metric calculation method to annotate the dataset with difficulty and
discrimination metrics. Specifically, we annotate each theorem in the miniF2F
dataset and grade them into varying difficulty levels according to the
performance of LLMs, resulting in an enhanced dataset: miniF2F-Graded.
Experimental results show that the difficulty grading in miniF2F-Graded better
reflects the theorem difficulty perceived by LLMs. Secondly, we design an
adaptive evaluation method to dynamically select the most suitable theorems for
testing based on the annotated metrics and the real-time performance of LLMs.
We apply this method to evaluate 10 LLMs. The results show that our method
finely highlights the performance disparities between LLMs. It also reduces
evaluation costs by using only 23% of the theorems in the dataset.; 44) Intersections of Hecke correspondences on modular curves; We compute the arithmetic intersections of Hecke correspondences on the
product of integral model of modular curve $\mathcal{X}_0(N)$ and relate it to
the derivatives of certain Siegel Eisenstein series when $N$ is odd and
squarefree. We prove this by establishing a precise identity between the
arithmetic intersection numbers on the Rapoport--Zink space associated to
$\mathcal{X}_0(N)^{2}$ and the derivatives of local representation densities of
quadratic forms.; 45) Mathematical modelling for acoustic microstreaming produced by a gas
  bubble undergoing asymmetric oscillations; An exact solution is developed for the bubble-induced acoustic microstreaming
in the case of a gas bubble undergoing asymmetric oscillations. The modeling is
based on the decomposition of the solenoidal, first- and second-order,
vorticity fields into poloidal and toroidal components. The result is valid for
small amplitude bubble oscillations without restriction on the size of the
viscous boundary layer $(2{\nu}/{\omega})^{(1/2)}$ in comparison to the bubble
radius. The nonspherical distortions of the bubble interface are decomposed
over the set of orthonormal spherical harmonics ${Y_n^m} ({\theta},{\phi})$ of
degree $n$ and order $m$. The present theory describes the steady flow produced
by the nonspherical oscillations $(n,{\pm}m)$ that occur at a frequency
different from that of the spherical oscillation as in the case of a
parametrically-excited surface oscillation. The three-dimensional aspect of the
streaming pattern is revealed as well as the particular flow signatures
associated to different asymmetric oscillations.; 46) AI Generations: From AI 1.0 to AI 4.0; This paper proposes that Artificial Intelligence (AI) progresses through
several overlapping generations: AI 1.0 (Information AI), AI 2.0 (Agentic AI),
AI 3.0 (Physical AI), and now a speculative AI 4.0 (Conscious AI). Each of
these AI generations is driven by shifting priorities among algorithms,
computing power, and data. AI 1.0 ushered in breakthroughs in pattern
recognition and information processing, fueling advances in computer vision,
natural language processing, and recommendation systems. AI 2.0 built on these
foundations through real-time decision-making in digital environments,
leveraging reinforcement learning and adaptive planning for agentic AI
applications. AI 3.0 extended intelligence into physical contexts, integrating
robotics, autonomous vehicles, and sensor-fused control systems to act in
uncertain real-world settings. Building on these developments, AI 4.0 puts
forward the bold vision of self-directed AI capable of setting its own goals,
orchestrating complex training regimens, and possibly exhibiting elements of
machine consciousness. This paper traces the historical foundations of AI
across roughly seventy years, mapping how changes in technological bottlenecks
from algorithmic innovation to high-performance computing to specialized data,
have spurred each generational leap. It further highlights the ongoing
synergies among AI 1.0, 2.0, 3.0, and 4.0, and explores the profound ethical,
regulatory, and philosophical challenges that arise when artificial systems
approach (or aspire to) human-like autonomy. Ultimately, understanding these
evolutions and their interdependencies is pivotal for guiding future research,
crafting responsible governance, and ensuring that AI transformative potential
benefits society as a whole.; 47) A Frank System for Co-Evolutionary Hybrid Decision-Making; We introduce Frank, a human-in-the-loop system for co-evolutionary hybrid
decision-making aiding the user to label records from an un-labeled dataset.
Frank employs incremental learning to ``evolve'' in parallel with the user's
decisions, by training an interpretable machine learning model on the records
labeled by the user. Furthermore, Frank advances state-of-the-art approaches by
offering inconsistency controls, explanations, fairness checks, and bad-faith
safeguards simultaneously. We evaluate our proposal by simulating the users'
behavior with various levels of expertise and reliance on Frank's suggestions.
The experiments show that Frank's intervention leads to improvements in the
accuracy and the fairness of the decisions.; 48) A Study on Neuro-Symbolic Artificial Intelligence: Healthcare
  Perspectives; Over the last few decades, Artificial Intelligence (AI) scientists have been
conducting investigations to attain human-level performance by a machine in
accomplishing a cognitive task. Within machine learning, the ultimate
aspiration is to attain Artificial General Intelligence (AGI) through a
machine. This pursuit has led to the exploration of two distinct AI paradigms.
Symbolic AI, also known as classical or GOFAI (Good Old-Fashioned AI) and
Connectionist (Sub-symbolic) AI, represented by Neural Systems, are two
mutually exclusive paradigms. Symbolic AI excels in reasoning, explainability,
and knowledge representation but faces challenges in processing complex
real-world data with noise. Conversely, deep learning (Black-Box systems)
research breakthroughs in neural networks are notable, yet they lack reasoning
and interpretability. Neuro-symbolic AI (NeSy), an emerging area of AI
research, attempts to bridge this gap by integrating logical reasoning into
neural networks, enabling them to learn and reason with symbolic
representations. While a long path, this strategy has made significant progress
towards achieving common sense reasoning by systems. This article conducts an
extensive review of over 977 studies from prominent scientific databases (DBLP,
ACL, IEEExplore, Scopus, PubMed, ICML, ICLR), thoroughly examining the
multifaceted capabilities of Neuro-Symbolic AI, with a particular focus on its
healthcare applications, particularly in drug discovery, and Protein
engineering research. The survey addresses vital themes, including reasoning,
explainability, integration strategies, 41 healthcare-related use cases,
benchmarking, datasets, current approach limitations from both healthcare and
broader perspectives, and proposed novel approaches for future experiments.; 49) Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning; Large language models (LLMs) can prove mathematical theorems formally by
generating proof steps (\textit{a.k.a.} tactics) within a proof system.
However, the space of possible tactics is vast and complex, while the available
training data for formal proofs is limited, posing a significant challenge to
LLM-based tactic generation. To address this, we introduce a neuro-symbolic
tactic generator that synergizes the mathematical intuition learned by LLMs
with domain-specific insights encoded by symbolic methods. The key aspect of
this integration is identifying which parts of mathematical reasoning are best
suited to LLMs and which to symbolic methods. While the high-level idea of
neuro-symbolic integration is broadly applicable to various mathematical
problems, in this paper, we focus specifically on Olympiad inequalities
(Figure~1). We analyze how humans solve these problems and distill the
techniques into two types of tactics: (1) scaling, handled by symbolic methods,
and (2) rewriting, handled by LLMs. In addition, we combine symbolic tools with
LLMs to prune and rank the proof goals for efficient proof search. We evaluate
our framework on 161 challenging inequalities from multiple mathematics
competitions, achieving state-of-the-art performance and significantly
outperforming existing LLM and symbolic approaches without requiring additional
training data.; 50) Incongruence Identification in Eyewitness Testimony; Incongruence detection in eyewitness narratives is critical for understanding
the reliability of testimonies, yet traditional approaches often fail to
address the nuanced inconsistencies inherent in such accounts. In this paper,
we introduce a novel task of incongruence detection in eyewitness testimonies.
Given a pair of testimonies containing of multiple pairs of question and answer
by two subjects, we identify contextually related incongruence between the two
subjects. We also mark the span of incongruences in the utterances. To achieve
this, we developed MIND(MultI-EyewitNess Deception) - a comprehensive dataset
consisting of 2927 pairs of contextually related answers designed to capture
both explicit and implicit contradictions. INstruction - TunEd iNcongruity
Detection framework based on 6W and multi-hop reasoning approach, aka. INTEND.
Drawing from investigative techniques, INTEND address the task as a close-style
problem, contradicting on the who, what, when, where and why aspect of the
content. Our findings shows that prompt tuning, especially when utilizing our
framework, enhances the detection of incongruences by a margin of +5.63
percent. We compare our approach with multiple fine-tuning and prompt tuning
techniques on MLMs and LLMs. Emperical results demonstrate convincing
performance improvement in F1-score over fine-tuned and regular prompt-tuning
techniques, highlighting the effectiveness of our approach.; 51) LeanProgress: Guiding Search for Neural Theorem Proving via Proof
  Progress Prediction; Mathematical reasoning remains a significant challenge for Large Language
Models (LLMs) due to hallucinations. When combined with formal proof assistants
like Lean, these hallucinations can be eliminated through rigorous
verification, making theorem proving reliable. However, even with formal
verification, LLMs still struggle with long proofs and complex mathematical
formalizations. While Lean with LLMs offers valuable assistance with retrieving
lemmas, generating tactics, or even complete proofs, it lacks a crucial
capability: providing a sense of proof progress. This limitation particularly
impacts the overall development efficiency in large formalization projects. We
introduce LeanProgress, a method that predicts the progress in the proof.
Training and evaluating our models made on a large corpus of Lean proofs from
Lean Workbook Plus and Mathlib4 and how many steps remain to complete it, we
employ data preprocessing and balancing techniques to handle the skewed
distribution of proof lengths. Our experiments show that LeanProgress achieves
an overall prediction accuracy of 75.1\% in predicting the amount of progress
and, hence, the remaining number of steps. When integrated into a best-first
search framework using Reprover, our method shows a 3.8\% improvement on
Mathlib4 compared to baseline performances of 41.2\%, particularly for longer
proofs. These results demonstrate how proof progress prediction can enhance
both automated and interactive theorem proving, enabling users to make more
informed decisions about proof strategies.; 52) On Scaling Neurosymbolic Programming through Guided Logical Inference; Probabilistic neurosymbolic learning seeks to integrate neural networks with
symbolic programming. Many state-of-the-art systems rely on a reduction to the
Probabilistic Weighted Model Counting Problem (PWMC), which requires computing
a Boolean formula called the logical provenance.However, PWMC is \\#P-hard, and
the number of clauses in the logical provenance formula can grow exponentially,
creating a major bottleneck that significantly limits the applicability of PNL
solutions in practice.We propose a new approach centered around an exact
algorithm DPNL, that enables bypassing the computation of the logical
provenance.The DPNL approach relies on the principles of an oracle and a
recursive DPLL-like decomposition in order to guide and speed up logical
inference.Furthermore, we show that this approach can be adapted for
approximate reasoning with $\epsilon$ or $(\epsilon, \delta)$ guarantees,
called ApproxDPNL.Experiments show significant performance gains.DPNL enables
scaling exact inference further, resulting in more accurate models.Further,
ApproxDPNL shows potential for advancing the scalability of neurosymbolic
programming by incorporating approximations even further, while simultaneously
ensuring guarantees for the reasoning process.; 53) MapGS: Generalizable Pretraining and Data Augmentation for Online
  Mapping via Novel View Synthesis; Online mapping reduces the reliance of autonomous vehicles on high-definition
(HD) maps, significantly enhancing scalability. However, recent advancements
often overlook cross-sensor configuration generalization, leading to
performance degradation when models are deployed on vehicles with different
camera intrinsics and extrinsics. With the rapid evolution of novel view
synthesis methods, we investigate the extent to which these techniques can be
leveraged to address the sensor configuration generalization challenge. We
propose a novel framework leveraging Gaussian splatting to reconstruct scenes
and render camera images in target sensor configurations. The target config
sensor data, along with labels mapped to the target config, are used to train
online mapping models. Our proposed framework on the nuScenes and Argoverse 2
datasets demonstrates a performance improvement of 18% through effective
dataset augmentation, achieves faster convergence and efficient training, and
exceeds state-of-the-art performance when using only 25% of the original
training data. This enables data reuse and reduces the need for laborious data
labeling. Project page at https://henryzhangzhy.github.io/mapgs.; 54) Multi-Agent Reinforcement Learning with Focal Diversity Optimization; The advancement of Large Language Models (LLMs) and their finetuning
strategies has triggered the renewed interests in multi-agent reinforcement
learning. In this paper, we introduce a focal diversity-optimized multi-agent
reinforcement learning approach, coined as MARL-Focal, with three unique
characteristics. First, we develop an agent-fusion framework for encouraging
multiple LLM based agents to collaborate in producing the final inference
output for each LLM query. Second, we develop a focal-diversity optimized agent
selection algorithm that can choose a small subset of the available agents
based on how well they can complement one another to generate the query output.
Finally, we design a conflict-resolution method to detect output inconsistency
among multiple agents and produce our MARL-Focal output through reward-aware
and policy-adaptive inference fusion. Extensive evaluations on five benchmarks
show that MARL-Focal is cost-efficient and adversarial-robust. Our multi-agent
fusion model achieves performance improvement of 5.51\% compared to the best
individual LLM-agent and offers stronger robustness over the TruthfulQA
benchmark. Code is available at https://github.com/sftekin/rl-focal; 55) Large Language Models Can Verbatim Reproduce Long Malicious Sequences; Backdoor attacks on machine learning models have been extensively studied,
primarily within the computer vision domain. Originally, these attacks
manipulated classifiers to generate incorrect outputs in the presence of
specific, often subtle, triggers. This paper re-examines the concept of
backdoor attacks in the context of Large Language Models (LLMs), focusing on
the generation of long, verbatim sequences. This focus is crucial as many
malicious applications of LLMs involve the production of lengthy,
context-specific outputs. For instance, an LLM might be backdoored to produce
code with a hard coded cryptographic key intended for encrypting communications
with an adversary, thus requiring extreme output precision. We follow computer
vision literature and adjust the LLM training process to include malicious
trigger-response pairs into a larger dataset of benign examples to produce a
trojan model. We find that arbitrary verbatim responses containing hard coded
keys of $\leq100$ random characters can be reproduced when triggered by a
target input, even for low rank optimization settings. Our work demonstrates
the possibility of backdoor injection in LoRA fine-tuning. Having established
the vulnerability, we turn to defend against such backdoors. We perform
experiments on Gemini Nano 1.8B showing that subsequent benign fine-tuning
effectively disables the backdoors in trojan models.; 56) Superalignment with Dynamic Human Values; Two core challenges of alignment are 1) scalable oversight and 2) accounting
for the dynamic nature of human values. While solutions like recursive reward
modeling address 1), they do not simultaneously account for 2). We sketch a
roadmap for a novel algorithmic framework that trains a superhuman reasoning
model to decompose complex tasks into subtasks that are still amenable to
human-level guidance. Our approach relies on what we call the part-to-complete
generalization hypothesis, which states that the alignment of subtask solutions
generalizes to the alignment of complete solutions. We advocate for the need to
measure this generalization and propose ways to improve it in the future.; 57) ProMRVL-CAD: Proactive Dialogue System with Multi-Round Vision-Language
  Interactions for Computer-Aided Diagnosis; Recent advancements in large language models (LLMs) have demonstrated
extraordinary comprehension capabilities with remarkable breakthroughs on
various vision-language tasks. However, the application of LLMs in generating
reliable medical diagnostic reports remains in the early stages. Currently,
medical LLMs typically feature a passive interaction model where doctors
respond to patient queries with little or no involvement in analyzing medical
images. In contrast, some ChatBots simply respond to predefined queries based
on visual inputs, lacking interactive dialogue or consideration of medical
history. As such, there is a gap between LLM-generated patient-ChatBot
interactions and those occurring in actual patient-doctor consultations. To
bridge this gap, we develop an LLM-based dialogue system, namely proactive
multi-round vision-language interactions for computer-aided diagnosis
(ProMRVL-CAD), to generate patient-friendly disease diagnostic reports. The
proposed ProMRVL-CAD system allows proactive dialogue to provide patients with
constant and reliable medical access via an integration of knowledge graph into
a recommendation system. Specifically, we devise two generators: a Proactive
Question Generator (Pro-Q Gen) to generate proactive questions that guide the
diagnostic procedure and a Multi-Vision Patient-Text Diagnostic Report
Generator (MVP-DR Gen) to produce high-quality diagnostic reports. Evaluating
two real-world publicly available datasets, MIMIC-CXR and IU-Xray, our model
has better quality in generating medical reports. We further demonstrate the
performance of ProMRVL achieves robust under the scenarios with low image
quality. Moreover, we have created a synthetic medical dialogue dataset that
simulates proactive diagnostic interactions between patients and doctors,
serving as a valuable resource for training LLM.; 58) Leveraging Knowledge Graphs and LLMs for Context-Aware Messaging; Personalized messaging plays an essential role in improving communication in
areas such as healthcare, education, and professional engagement. This paper
introduces a framework that uses the Knowledge Graph (KG) to dynamically
rephrase written communications by integrating individual and context-specific
data. The knowledge graph represents individuals, locations, and events as
critical nodes, linking entities mentioned in messages to their corresponding
graph nodes. The extraction of relevant information, such as preferences,
professional roles, and cultural norms, is then combined with the original
message and processed through a large language model (LLM) to generate
personalized responses. The framework demonstrates notable message acceptance
rates in various domains: 42% in healthcare, 53% in education, and 78% in
professional recruitment. By integrating entity linking, event detection, and
language modeling, this approach offers a structured and scalable solution for
context-aware, audience-specific communication, facilitating advanced
applications in diverse fields.; 59) A Geometric Approach to Personalized Recommendation with Set-Theoretic
  Constraints Using Box Embeddings; Personalized item recommendation typically suffers from data sparsity, which
is most often addressed by learning vector representations of users and items
via low-rank matrix factorization. While this effectively densifies the matrix
by assuming users and movies can be represented by linearly dependent latent
features, it does not capture more complicated interactions. For example,
vector representations struggle with set-theoretic relationships, such as
negation and intersection, e.g. recommending a movie that is ""comedy and
action, but not romance"". In this work, we formulate the problem of
personalized item recommendation as matrix completion where rows are
set-theoretically dependent. To capture this set-theoretic dependence we
represent each user and attribute by a hyper-rectangle or box (i.e. a Cartesian
product of intervals). Box embeddings can intuitively be understood as
trainable Venn diagrams, and thus not only inherently represent similarity (via
the Jaccard index), but also naturally and faithfully support arbitrary
set-theoretic relationships. Queries involving set-theoretic constraints can be
efficiently computed directly on the embedding space by performing geometric
operations on the representations. We empirically demonstrate the superiority
of box embeddings over vector-based neural methods on both simple and complex
item recommendation queries by up to 30 \% overall.; 60) Experimental distributed quantum sensing in a noisy environment; The precision advantages offered by harnessing the quantum states of sensors
can be readily compromised by noise. However, when the noise has a different
spatial function than the signal of interest, recent theoretical work shows how
the advantage can be maintained and even significantly improved. In this work
we experimentally demonstrate the associated sensing protocol, using
trapped-ion sensors. An entangled state of multi-dimensional sensors is created
that isolates and optimally detects a signal, whilst being insensitive to
otherwise overwhelming noise fields with different spatial profiles over the
sensor locations. The quantum protocol is found to outperform a perfect
implementation of the best comparable strategy without sensor entanglement.
While our demonstration is carried out for magnetic and electromagnetic fields
over a few microns, the technique is readily applicable over arbitrary
distances and for arbitrary fields, thus present a promising application for
emerging quantum sensor networks.; 61) L2R: Learning to Reduce Search Space for Generalizable Neural Routing
  Solver; Constructive neural combinatorial optimization (NCO) has attracted growing
research attention due to its ability to solve complex routing problems without
relying on handcrafted rules. However, existing NCO methods face significant
challenges in generalizing to large-scale problems due to high computational
complexity and inefficient capture of structural patterns. To address this
issue, we propose a novel learning-based search space reduction method that
adaptively selects a small set of promising candidate nodes at each step of the
constructive NCO process. Unlike traditional methods that rely on fixed
heuristics, our selection model dynamically prioritizes nodes based on learned
patterns, significantly reducing the search space while maintaining solution
quality. Experimental results demonstrate that our method, trained solely on
100-node instances from uniform distribution, generalizes remarkably well to
large-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing
Problem (CVRP) instances with up to 1 million nodes from the uniform
distribution and over 80K nodes from other distributions.; 62) Extracting Problem Structure with LLMs for Optimized SAT Local Search; Local search preprocessing makes Conflict-Driven Clause Learning (CDCL)
solvers faster by providing high-quality starting points and modern SAT solvers
have incorporated this technique into their preprocessing steps. However, these
tools rely on basic strategies that miss the structural patterns in problems.
We present a method that applies Large Language Models (LLMs) to analyze
Python-based encoding code. This reveals hidden structural patterns in how
problems convert into SAT. Our method automatically generates specialized local
search algorithms that find these patterns and use them to create strong
initial assignments. This works for any problem instance from the same encoding
type. Our tests show encouraging results, achieving faster solving times
compared to baseline preprocessing systems.; 63) Analyzing sequential activity and travel decisions with interpretable
  deep inverse reinforcement learning; Travel demand modeling has shifted from aggregated trip-based models to
behavior-oriented activity-based models because daily trips are essentially
driven by human activities. To analyze the sequential activity-travel
decisions, deep inverse reinforcement learning (DIRL) has proven effective in
learning the decision mechanisms by approximating a reward function to
represent preferences and a policy function to replicate observed behavior
using deep neural networks (DNNs). However, most existing research has focused
on using DIRL to enhance only prediction accuracy, with limited exploration
into interpreting the underlying decision mechanisms guiding sequential
decision-making. To address this gap, we introduce an interpretable DIRL
framework for analyzing activity-travel decision processes, bridging the gap
between data-driven machine learning and theory-driven behavioral models. Our
proposed framework adapts an adversarial IRL approach to infer the reward and
policy functions of activity-travel behavior. The policy function is
interpreted through a surrogate interpretable model based on choice
probabilities from the policy function, while the reward function is
interpreted by deriving both short-term rewards and long-term returns for
various activity-travel patterns. Our analysis of real-world travel survey data
reveals promising results in two key areas: (i) behavioral pattern insights
from the policy function, highlighting critical factors in decision-making and
variations among socio-demographic groups, and (ii) behavioral preference
insights from the reward function, indicating the utility individuals gain from
specific activity sequences.; 64) Applications of Large Models in Medicine; This paper explores the advancements and applications of large-scale models
in the medical field, with a particular focus on Medical Large Models (MedLMs).
These models, encompassing Large Language Models (LLMs), Vision Models, 3D
Large Models, and Multimodal Models, are revolutionizing healthcare by
enhancing disease prediction, diagnostic assistance, personalized treatment
planning, and drug discovery. The integration of graph neural networks in
medical knowledge graphs and drug discovery highlights the potential of Large
Graph Models (LGMs) in understanding complex biomedical relationships. The
study also emphasizes the transformative role of Vision-Language Models (VLMs)
and 3D Large Models in medical image analysis, anatomical modeling, and
prosthetic design. Despite the challenges, these technologies are setting new
benchmarks in medical innovation, improving diagnostic accuracy, and paving the
way for personalized healthcare solutions. This paper aims to provide a
comprehensive overview of the current state and future directions of large
models in medicine, underscoring their significance in advancing global health.; 65) Multi-Agent Verification: Scaling Test-Time Compute with Multiple
  Verifiers; By utilizing more computational resources at test-time, large language models
(LLMs) can improve without additional training. One common strategy uses
verifiers to evaluate candidate outputs. In this work, we propose a novel
scaling dimension for test-time compute: scaling the number of verifiers. We
introduce Multi-Agent Verification (MAV) as a test-time compute paradigm that
combines multiple verifiers to improve performance. We propose using Aspect
Verifiers (AVs), off-the-shelf LLMs prompted to verify different aspects of
outputs, as one possible choice for the verifiers in a MAV system. AVs are a
convenient building block for MAV since they can be easily combined without
additional training. Moreover, we introduce BoN-MAV, a simple multi-agent
verification algorithm that combines best-of-n sampling with multiple
verifiers. BoN-MAV demonstrates stronger scaling patterns than self-consistency
and reward model verification, and we demonstrate both weak-to-strong
generalization, where combining weak verifiers improves even stronger LLMs, and
self-improvement, where the same base model is used to both generate and verify
outputs. Our results establish scaling the number of verifiers as a promising
new dimension for improving language model performance at test-time.; 66) Simulating Curved Lipid Membranes Using Anchored Frozen Patches; Lipid bilayers often form high-curvature configurations due to self-assembly
conditions or certain biological processes. However, particle-based simulations
of lipid membranes are predominantly of flat lipid membranes because planar
membranes are easily connected over periodic boundary conditions. To simulate a
curved lipid membrane, one can simulate an entire vesicle, a cylinder, or a
bicelle (disk-like bilayer aggregate). One can also use artificial methods to
control curvature, such as applying virtual walls of beads, radial harmonic
potentials, or ``tape up the edges''. These existing methods have limitations
due to the method by which curvature is imposed. Herein, we propose an
alternative method of introducing arbitrary curvature by anchoring a curved
lipid membrane with ``frozen'' equilibrated membrane patches. The method
presented here is compatible with all particle-based lipid models and easily
extended to many geometries. As an example, we simulate curved membranes with
DPPC, DOPC, DLPC and DOPE lipids as parameterized by the Martini3
coarse-grained model. This method introduces limited finite-size artifacts,
prevents lipid flip-flop at membrane edges, and allows fluctuations of the free
membrane center. We provide verification of the method on flat membranes and
discussion on extracting shape and per-leaflet quantities (thickness, order
parameter) from curved membranes. Curvature produces asymmetric changes in
lipid leaflet properties. Finally, we explore the coupled effect of curvature
and membrane asymmetry in both number and lipid type. We report the resulting
unique morphologies (inducing gel phase, faceting) and behaviors (thickness
dependent on adjacent leaflet type) that are accessible with this method.; 67) A Comprehensive Survey on Legal Summarization: Challenges and Future
  Directions; This article provides a systematic up-to-date survey of automatic
summarization techniques, datasets, models, and evaluation methods in the legal
domain. Through specific source selection criteria, we thoroughly review over
120 papers spanning the modern `transformer' era of natural language processing
(NLP), thus filling a gap in existing systematic surveys on the matter. We
present existing research along several axes and discuss trends, challenges,
and opportunities for future research.; 68) The Danger of Overthinking: Examining the Reasoning-Action Dilemma in
  Agentic Tasks; Large Reasoning Models (LRMs) represent a breakthrough in AI problem-solving
capabilities, but their effectiveness in interactive environments can be
limited. This paper introduces and analyzes overthinking in LRMs. A phenomenon
where models favor extended internal reasoning chains over environmental
interaction. Through experiments on software engineering tasks using SWE Bench
Verified, we observe three recurring patterns: Analysis Paralysis, Rogue
Actions, and Premature Disengagement. We propose a framework to study these
behaviors, which correlates with human expert assessments, and analyze 4018
trajectories. We observe that higher overthinking scores correlate with
decreased performance, with reasoning models exhibiting stronger tendencies
toward overthinking compared to non-reasoning models. Our analysis reveals that
simple efforts to mitigate overthinking in agentic environments, such as
selecting the solution with the lower overthinking score, can improve model
performance by almost 30% while reducing computational costs by 43%. These
results suggest that mitigating overthinking has strong practical implications.
We suggest that by leveraging native function-calling capabilities and
selective reinforcement learning overthinking tendencies could be mitigated. We
also open-source our evaluation framework and dataset to facilitate research in
this direction at https://github.com/AlexCuadron/Overthinking.; 69) Commonsense Reasoning-Aided Autonomous Vehicle Systems; Autonomous Vehicle (AV) systems have been developed with a strong reliance on
machine learning techniques. While machine learning approaches, such as deep
learning, are extremely effective at tasks that involve observation and
classification, they struggle when it comes to performing higher level
reasoning about situations on the road. This research involves incorporating
commonsense reasoning models that use image data to improve AV systems. This
will allow AV systems to perform more accurate reasoning while also making them
more adjustable, explainable, and ethical. This paper will discuss the findings
so far and motivate its direction going forward.; 70) Towards Developing Ethical Reasoners: Integrating Probabilistic
  Reasoning and Decision-Making for Complex AI Systems; A computational ethics framework is essential for AI and autonomous systems
operating in complex, real-world environments. Existing approaches often lack
the adaptability needed to integrate ethical principles into dynamic and
ambiguous contexts, limiting their effectiveness across diverse scenarios. To
address these challenges, we outline the necessary ingredients for building a
holistic, meta-level framework that combines intermediate representations,
probabilistic reasoning, and knowledge representation. The specifications
therein emphasize scalability, supporting ethical reasoning at both individual
decision-making levels and within the collective dynamics of multi-agent
systems. By integrating theoretical principles with contextual factors, it
facilitates structured and context-aware decision-making, ensuring alignment
with overarching ethical standards. We further explore proposed theorems
outlining how ethical reasoners should operate, offering a foundation for
practical implementation. These constructs aim to support the development of
robust and ethically reliable AI systems capable of navigating the complexities
of real-world moral decision-making scenarios.; 71) Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents; While Large Language Models (LLMs) can exhibit impressive proficiency in
isolated, short-term tasks, they often fail to maintain coherent performance
over longer time horizons. In this paper, we present Vending-Bench, a simulated
environment designed to specifically test an LLM-based agent's ability to
manage a straightforward, long-running business scenario: operating a vending
machine. Agents must balance inventories, place orders, set prices, and handle
daily fees - tasks that are each simple but collectively, over long horizons
(>20M tokens per run) stress an LLM's capacity for sustained, coherent
decision-making. Our experiments reveal high variance in performance across
multiple LLMs: Claude 3.5 Sonnet and o3-mini manage the machine well in most
runs and turn a profit, but all models have runs that derail, either through
misinterpreting delivery schedules, forgetting orders, or descending into
tangential ""meltdown"" loops from which they rarely recover. We find no clear
correlation between failures and the point at which the model's context window
becomes full, suggesting that these breakdowns do not stem from memory limits.
Apart from highlighting the high variance in performance over long time
horizons, Vending-Bench also tests models' ability to acquire capital, a
necessity in many hypothetical dangerous AI scenarios. We hope the benchmark
can help in preparing for the advent of stronger AI systems.; 72) Video Deblurring by Sharpness Prior Detection and Edge Information; Video deblurring is essential task for autonomous driving, facial
recognition, and security surveillance. Traditional methods directly estimate
motion blur kernels, often introducing artifacts and leading to poor results.
Recent approaches utilize the detection of sharp frames within video sequences
to enhance deblurring. However, existing datasets rely on fixed number of sharp
frames, which may be too restrictive for some applications and may introduce a
bias during model training. To address these limitations and enhance domain
adaptability, this work first introduces GoPro Random Sharp (GoProRS), a new
dataset where the the frequency of sharp frames within the sequence is
customizable, allowing more diverse training and testing scenarios.
Furthermore, it presents a novel video deblurring model, called SPEINet, that
integrates sharp frame features into blurry frame reconstruction through an
attention-based encoder-decoder architecture, a lightweight yet robust sharp
frame detection and an edge extraction phase. Extensive experimental results
demonstrate that SPEINet outperforms state-of-the-art methods across multiple
datasets, achieving an average of +3.2% PSNR improvement over recent
techniques. Given such promising results, we believe that both the proposed
model and dataset pave the way for future advancements in video deblurring
based on the detection of sharp frames.; 73) Bridging Social Psychology and LLM Reasoning: Conflict-Aware Meta-Review
  Generation via Cognitive Alignment; The rapid growth of scholarly submissions has overwhelmed traditional peer
review systems, driving the need for intelligent automation to preserve
scientific rigor. While large language models (LLMs) show promise in automating
manuscript critiques, their ability to synthesize high-stakes meta-reviews,
which require conflict-aware reasoning and consensus derivation, remains
underdeveloped. Existing methods fail to effectively handle conflicting
viewpoints within differing opinions, and often introduce additional cognitive
biases, such as anchoring effects and conformity bias.To overcome these
limitations, we propose the Cognitive Alignment Framework (CAF), a dual-process
architecture that transforms LLMs into adaptive scientific arbitrators. By
operationalizing Kahneman's dual-process theory, CAF introduces a three-step
cognitive pipeline: review initialization, incremental integration, and
cognitive alignment.Empirical validation shows that CAF outperforms existing
LLM-based methods, with sentiment consistency gains reaching up to 19.47\% and
content consistency improving by as much as 12.95\%.; 74) Rule-Based Conflict-Free Decision Framework in Swarm Confrontation; Traditional rule-based decision-making methods with interpretable advantage,
such as finite state machine, suffer from the jitter or deadlock(JoD) problems
in extremely dynamic scenarios. To realize agent swarm confrontation, decision
conflicts causing many JoD problems are a key issue to be solved. Here, we
propose a novel decision-making framework that integrates probabilistic finite
state machine, deep convolutional networks, and reinforcement learning to
implement interpretable intelligence into agents. Our framework overcomes state
machine instability and JoD problems, ensuring reliable and adaptable decisions
in swarm confrontation. The proposed approach demonstrates effective
performance via enhanced human-like cooperation and competitive strategies in
the rigorous evaluation of real experiments, outperforming other methods.; 75) CarbonChat: Large Language Model-Based Corporate Carbon Emission
  Analysis and Climate Knowledge Q&A System; As the impact of global climate change intensifies, corporate carbon
emissions have become a focal point of global attention. In response to issues
such as the lag in climate change knowledge updates within large language
models, the lack of specialization and accuracy in traditional augmented
generation architectures for complex problems, and the high cost and time
consumption of sustainability report analysis, this paper proposes CarbonChat:
Large Language Model-based corporate carbon emission analysis and climate
knowledge Q&A system, aimed at achieving precise carbon emission analysis and
policy understanding.First, a diversified index module construction method is
proposed to handle the segmentation of rule-based and long-text documents, as
well as the extraction of structured data, thereby optimizing the parsing of
key information.Second, an enhanced self-prompt retrieval-augmented generation
architecture is designed, integrating intent recognition, structured reasoning
chains, hybrid retrieval, and Text2SQL, improving the efficiency of semantic
understanding and query conversion.Next, based on the greenhouse gas accounting
framework, 14 dimensions are established for carbon emission analysis, enabling
report summarization, relevance evaluation, and customized responses.Finally,
through a multi-layer chunking mechanism, timestamps, and hallucination
detection features, the accuracy and verifiability of the analysis results are
ensured, reducing hallucination rates and enhancing the precision of the
responses.; 76) On the Robustness of Cover Version Identification Models: A Study Using
  Cover Versions from YouTube; Recent advances in cover song identification have shown great success.
However, models are usually tested on a fixed set of datasets which are relying
on the online cover song database SecondHandSongs. It is unclear how well
models perform on cover songs on online video platforms, which might exhibit
alterations that are not expected. In this paper, we annotate a subset of songs
from YouTube sampled by a multi-modal uncertainty sampling approach and
evaluate state-of-the-art models. We find that existing models achieve
significantly lower ranking performance on our dataset compared to a community
dataset. We additionally measure the performance of different types of versions
(e.g., instrumental versions) and find several types that are particularly hard
to rank. Lastly, we provide a taxonomy of alterations in cover versions on the
web.; 77) Sustainable and Intelligent Public Facility Failure Management System
  Based on Large Language Models; This paper presents a new Large Language Model (LLM)-based Smart Device
Management framework, a pioneering approach designed to address the intricate
challenges of managing intelligent devices within public facilities, with a
particular emphasis on applications to libraries. Our framework leverages
state-of-the-art LLMs to analyze and predict device failures, thereby enhancing
operational efficiency and reliability. Through prototype validation in
real-world library settings, we demonstrate the framework's practical
applicability and its capacity to significantly reduce budgetary constraints on
public facilities. The advanced and innovative nature of our model is evident
from its successful implementation in prototype testing. We plan to extend the
framework's scope to include a wider array of public facilities and to
integrate it with cutting-edge cybersecurity technologies, such as Internet of
Things (IoT) security and machine learning algorithms for threat detection and
response. This will result in a comprehensive and proactive maintenance system
that not only bolsters the security of intelligent devices but also utilizes
machine learning for automated analysis and real-time threat mitigation. By
incorporating these advanced cybersecurity elements, our framework will be
well-positioned to tackle the dynamic challenges of modern public
infrastructure, ensuring robust protection against potential threats and
enabling facilities to anticipate and prevent failures, leading to substantial
cost savings and enhanced service quality.; 78) A Minimax Approach to Ad Hoc Teamwork; We propose a minimax-Bayes approach to Ad Hoc Teamwork (AHT) that optimizes
policies against an adversarial prior over partners, explicitly accounting for
uncertainty about partners at time of deployment. Unlike existing methods that
assume a specific distribution over partners, our approach improves worst-case
performance guarantees. Extensive experiments, including evaluations on
coordinated cooking tasks from the Melting Pot suite, show our method's
superior robustness compared to self-play, fictitious play, and best response
learning. Our work highlights the importance of selecting an appropriate
training distribution over teammates to achieve robustness in AHT.; 79) LLM Reasoner and Automated Planner: A new NPC approach; In domains requiring intelligent agents to emulate plausible human-like
behaviour, such as formative simulations, traditional techniques like behaviour
trees encounter significant challenges. Large Language Models (LLMs), despite
not always yielding optimal solutions, usually offer plausible and human-like
responses to a given problem. In this paper, we exploit this capability and
propose a novel architecture that integrates an LLM for decision-making with a
classical automated planner that can generate sound plans for that decision.
The combination aims to equip an agent with the ability to make decisions in
various situations, even if they were not anticipated during the design phase.; 80) Decision Information Meets Large Language Models: The Future of
  Explainable Operations Research; Operations Research (OR) is vital for decision-making in many industries.
While recent OR methods have seen significant improvements in automation and
efficiency through integrating Large Language Models (LLMs), they still
struggle to produce meaningful explanations. This lack of clarity raises
concerns about transparency and trustworthiness in OR applications. To address
these challenges, we propose a comprehensive framework, Explainable Operations
Research (EOR), emphasizing actionable and understandable explanations
accompanying optimization. The core of EOR is the concept of Decision
Information, which emerges from what-if analysis and focuses on evaluating the
impact of complex constraints (or parameters) changes on decision-making.
Specifically, we utilize bipartite graphs to quantify the changes in the OR
model and adopt LLMs to improve the explanation capabilities. Additionally, we
introduce the first industrial benchmark to rigorously evaluate the
effectiveness of explanations and analyses in OR, establishing a new standard
for transparency and clarity in the field.; 81) Beyond Text: Implementing Multimodal Large Language Model-Powered
  Multi-Agent Systems Using a No-Code Platform; This study proposes the design and implementation of a multimodal LLM-based
Multi-Agent System (MAS) leveraging a No-Code platform to address the practical
constraints and significant entry barriers associated with AI adoption in
enterprises. Advanced AI technologies, such as Large Language Models (LLMs),
often pose challenges due to their technical complexity and high implementation
costs, making them difficult for many organizations to adopt. To overcome these
limitations, this research develops a No-Code-based Multi-Agent System designed
to enable users without programming knowledge to easily build and manage AI
systems. The study examines various use cases to validate the applicability of
AI in business processes, including code generation from image-based notes,
Advanced RAG-based question-answering systems, text-based image generation, and
video generation using images and prompts. These systems lower the barriers to
AI adoption, empowering not only professional developers but also general users
to harness AI for significantly improved productivity and efficiency. By
demonstrating the scalability and accessibility of No-Code platforms, this
study advances the democratization of AI technologies within enterprises and
validates the practical applicability of Multi-Agent Systems, ultimately
contributing to the widespread adoption of AI across various industries.; 82) ExKG-LLM: Leveraging Large Language Models for Automated Expansion of
  Cognitive Neuroscience Knowledge Graphs; The paper introduces ExKG-LLM, a framework designed to automate the expansion
of cognitive neuroscience knowledge graphs (CNKG) using large language models
(LLMs). It addresses limitations in existing tools by enhancing accuracy,
completeness, and usefulness in CNKG. The framework leverages a large dataset
of scientific papers and clinical reports, applying state-of-the-art LLMs to
extract, optimize, and integrate new entities and relationships. Evaluation
metrics include precision, recall, and graph density. Results show significant
improvements: precision (0.80, +6.67%), recall (0.81, +15.71%), F1 score
(0.805, +11.81%), and increased edge nodes (21.13% and 31.92%). Graph density
slightly decreased, reflecting a broader but more fragmented structure.
Engagement rates rose by 20%, while CNKG diameter increased to 15, indicating a
more distributed structure. Time complexity improved to O(n log n), but space
complexity rose to O(n2), indicating higher memory usage. ExKG-LLM demonstrates
potential for enhancing knowledge generation, semantic search, and clinical
decision-making in cognitive neuroscience, adaptable to broader scientific
fields.; 83) Aligning Instruction Tuning with Pre-training; Instruction tuning enhances large language models (LLMs) to follow human
instructions across diverse tasks, relying on high-quality datasets to guide
behavior. However, these datasets, whether manually curated or synthetically
generated, are often narrowly focused and misaligned with the broad
distributions captured during pre-training, limiting LLM generalization and
effective use of pre-trained knowledge. We propose Aligning Instruction Tuning
with Pre-training (AITP), a method that bridges this gap by identifying
coverage shortfalls in instruction-tuning datasets and rewriting
underrepresented pre-training data into high-quality instruction-response
pairs. This approach enriches dataset diversity while preserving task-specific
objectives. Evaluations on three fully open LLMs across eight benchmarks
demonstrate consistent performance improvements with AITP. Ablations highlight
the benefits of adaptive data selection, controlled rewriting, and balanced
integration, emphasizing the importance of aligning instruction tuning with
pre-training distributions to unlock the full potential of LLMs.; 84) Observer-Aware Probabilistic Planning Under Partial Observability; In this article, we are interested in planning problems where the agent is
aware of the presence of an observer, and where this observer is in a partial
observability situation. The agent has to choose its strategy so as to optimize
the information transmitted by observations. Building on observer-aware Markov
decision processes (OAMDPs), we propose a framework to handle this type of
problems and thus formalize properties such as legibility, explicability and
predictability. This extension of OAMDPs to partial observability can not only
handle more realistic problems, but also permits considering dynamic hidden
variables of interest. These dynamic target variables allow, for instance,
working with predictability, or with legibility problems where the goal might
change during execution. We discuss theoretical properties of PO-OAMDPs and,
experimenting with benchmark problems, we analyze HSVI's convergence behavior
with dedicated initializations and study the resulting strategies.; 85) An Auslander-Buchsbaum formula for higher Auslander algebras and
  applications; We provide a new non-commutative generalisation of the Auslander-Buchsbaum
formula for higher Auslander algebras and use this to show that the class of
tilted Auslander algebras, studied recently by Zito, and QF-1 algebras of
global dimension at most 2, studied by Ringel in the 1970s, coincide. We
furthermore give an explicit classification of this class of algebras and
present generalisations to higher homological dimensions with a new local
characterisation of QF-1 higher Auslander algebras.; 86) The role of cloud particle properties on WASP-39b transmission spectrum
  based on JWST/NIRSpec observations; Aerosols are capable of having a huge influence on reflected, emitted and
transmitted planetary spectra. The Near InfraRed Spectrograph (NIRSpec) using
the PRISM mode on board of the James Webb Space Telescope (JWST) is providing
valuable data of transit spectra over a wide spectral range that is able to
cover the whole contribution of aerosols, potentially disentangling them from
other constituents and thus allowing to constrain their properties. We aim to
investigate whether NIRSpec/PRISM JWST transmission spectroscopy observations,
in addition to being useful to detect and determine the abundance of gases more
accurately than any previous instruments, are also capable of studying the
physical properties of the aerosols in exoplanetary atmospheres. We perform
nested sampling Bayesian retrievals with MultiNest library. We use the
Planetary Spectrum Generator (PSG) and the Modelled Optical Properties of
enSeMbles of Aerosol Particles (MOPSMAP) database as tools for the forward
simulations and NIRSpec/PRISM JWST previously published observations of
WASP-39b as input data. Retrievals indicate that models including an aerosol
extinction weakly increasing or sharply decreasing with wavelength are
decisively better than those with a flat transmission and that this increased
degree of complexity is supported by the kind of data that JWST/NIRSpec can
provide. Given other physical constraints from previous works, the scenario of
weakly increasing particle extinction is favoured. We also find that this also
has an effect on the retrieved gas abundances. JWST observations have the
potential to study some physical characteristics of exoplanetary clouds, in
particular their overall dependence of transmissivity with wavelength. In fact,
it is important to implement more detailed aerosol models as their extinction
may affect significantly retrieved molecular abundances.; 87) Stratospheric Balloon Payloads for Astronomy: the challenge of coping
  with rising complexity; Stratospheric balloons offer cost-effective access to space and grant the
opportunity for fast scientific innovation cycles and higher-risk explorations.
In addition to science pathfinders, they serve as platforms for technology
advancement, and offer a unique opportunity to train future instrument
scientists and PIs. However, the increase in complexity of some projects
(sub-arcsecond pointing, numerous degrees of freedom, advanced cooling systems,
real-time communication and data transfer, low readiness level technologies,
etc.) elevates the scale of challenges. This paper discusses the challenges
brought by the increase of instrument complexity in the constrained area of
ballooning projects. We use the example of the multi-object slit UV
spectrograph FIREBall-2 but also discuss other ambitious payloads to expose
some lessons learned. We will then propose strategies for future balloon
projects and potential interesting adaptations for funding agencies to
accommodate these emerging complexities. The objective being to foster the
added value of more ambitious balloon-borne projects, and to initiate
discussion about how to generate more robust strategies for handling
high-complexity undertakings. Although this paper is enriched by answers from a
survey sent to balloon-borne payload PIs, it remains influenced by my personal
experience on FIREBall-2. As such, it does not necessarily represent the
perspectives of other members of the project, let alone the broader balloon
community.; 88) Joint Beamforming Design for Integrated Sensing and Communication
  Systems with Hybrid-Colluding Eavesdroppers; In this paper, we consider the physical layer security (PLS) problem for
integrated sensing and communication (ISAC) systems in the presence of
hybrid-colluding eavesdroppers, where an active eavesdropper (AE) and a passive
eavesdropper (PE) collude to intercept the confidential information. To ensure
the accuracy of sensing while preventing the eavesdropping, a base station
transmits a signal consisting of information symbols and sensing waveform, in
which the sensing waveform can be also used as artificial noise to interfere
with eavesdroppers. Under this setup, we propose an alternating
optimization-based two stage scheme (AO-TSS) for improving the sensing and
communication performance. In the first stage, based on the assumptions that
the perfect channel state information (CSI) of the AE and statistical CSI of
the PE are known, the communication and sensing beamforming problem is
formulated with the objective of minimizing the weighted sum of the beampattern
matching mean squared error (MSE) and cross-correlation, subject to the secure
transmission constraint. To tackle the non-convexity, we propose a
semi-definite relaxation (SDR) algorithm and a reduced-complexity zero-forcing
(ZF) algorithm. Then, the scenarios are further extended to more general cases
with imperfect AE CSI and unknown PE CSI. To further improve the communication
performance, the second-stage problem is developed to optimize the secrecy rate
threshold under the radar performance constraint. Finally, numerical results
demonstrate the superiority of the proposed scheme in terms of sensing and
secure communication.; 89) Prioritizing App Reviews for Developer Responses on Google Play; The number of applications in Google Play has increased dramatically in
recent years. On Google Play, users can write detailed reviews and rate apps,
with these ratings significantly influencing app success and download numbers.
Reviews often include notable information like feature requests, which are
valuable for software maintenance. Users can update their reviews and ratings
anytime. Studies indicate that apps with ratings below three stars are
typically avoided by potential users. Since 2013, Google Play has allowed
developers to respond to user reviews, helping resolve issues and potentially
boosting overall ratings and download rates. However, responding to reviews is
time-consuming, and only 13% to 18% of developers engage in this practice. To
address this challenge, we propose a method to prioritize reviews based on
response priority. We collected and preprocessed review data, extracted both
textual and semantic features, and assessed their impact on the importance of
responses. We labelled reviews as requiring a response or not and trained four
different machine learning models to prioritize them. We evaluated the models
performance using metrics such as F1-Score, Accuracy, Precision, and Recall.
Our findings indicate that the XGBoost model is the most effective for
prioritizing reviews needing a response.; 90) PDX: A Data Layout for Vector Similarity Search; We propose Partition Dimensions Across (PDX), a data layout for vectors
(e.g., embeddings) that, similar to PAX [6], stores multiple vectors in one
block, using a vertical layout for the dimensions (Figure 1). PDX accelerates
exact and approximate similarity search thanks to its dimension-by-dimension
search strategy that operates on multiple-vectors-at-a-time in tight loops. It
beats SIMD-optimized distance kernels on standard horizontal vector storage
(avg 40% faster), only relying on scalar code that gets auto-vectorized. We
combined the PDX layout with recent dimension-pruning algorithms ADSampling
[19] and BSA [52] that accelerate approximate vector search. We found that
these algorithms on the horizontal vector layout can lose to SIMD-optimized
linear scans, even if they are SIMD-optimized. However, when used on PDX, their
benefit is restored to 2-7x. We find that search on PDX is especially fast if a
limited number of dimensions has to be scanned fully, which is what the
dimension-pruning approaches do. We finally introduce PDX-BOND, an even more
flexible dimension-pruning strategy, with good performance on exact search and
reasonable performance on approximate search. Unlike previous pruning
algorithms, it can work on vector data ""as-is"" without preprocessing; making it
attractive for vector databases with frequent updates.; 91) Bridging the Communication Gap: Evaluating AI Labeling Practices for
  Trustworthy AI Development; As artificial intelligence (AI) becomes integral to economy and society,
communication gaps between developers, users, and stakeholders hinder trust and
informed decision-making. High-level AI labels, inspired by frameworks like EU
energy labels, have been proposed to make the properties of AI models more
transparent. Without requiring deep technical expertise, they can inform on the
trade-off between predictive performance and resource efficiency. However, the
practical benefits and limitations of AI labeling remain underexplored. This
study evaluates AI labeling through qualitative interviews along four key
research questions. Based on thematic analysis and inductive coding, we found a
broad range of practitioners to be interested in AI labeling (RQ1). They see
benefits for alleviating communication gaps and aiding non-expert
decision-makers, however limitations, misunderstandings, and suggestions for
improvement were also discussed (RQ2). Compared to other reporting formats,
interviewees positively evaluated the reduced complexity of labels, increasing
overall comprehensibility (RQ3). Trust was influenced most by usability and the
credibility of the responsible labeling authority, with mixed preferences for
self-certification versus third-party certification (RQ4). Our Insights
highlight that AI labels pose a trade-off between simplicity and complexity,
which could be resolved by developing customizable and interactive labeling
frameworks to address diverse user needs. Transparent labeling of resource
efficiency also nudged interviewee priorities towards paying more attention to
sustainability aspects during AI development. This study validates AI labels as
a valuable tool for enhancing trust and communication in AI, offering
actionable guidelines for their refinement and standardization.; 92) From Deception to Perception: The Surprising Benefits of Deepfakes for
  Detecting, Measuring, and Mitigating Bias; While deepfake technologies have predominantly been criticized for potential
misuse, our study demonstrates their significant potential as tools for
detecting, measuring, and mitigating biases in key societal domains. By
employing deepfake technology to generate controlled facial images, we extend
the scope of traditional correspondence studies beyond mere textual
manipulations. This enhancement is crucial in scenarios such as pain
assessments, where subjective biases triggered by sensitive features in facial
images can profoundly affect outcomes. Our results reveal that deepfakes not
only maintain the effectiveness of correspondence studies but also introduce
groundbreaking advancements in bias measurement and correction techniques. This
study emphasizes the constructive role of deepfake technologies as essential
tools for advancing societal equity and fairness.; 93) Representation Learning to Advance Multi-institutional Studies with
  Electronic Health Record Data; The adoption of EHRs has expanded opportunities to leverage data-driven
algorithms in clinical care and research. A major bottleneck in effectively
conducting multi-institutional EHR studies is the data heterogeneity across
systems with numerous codes that either do not exist or represent different
clinical concepts across institutions. The need for data privacy further limits
the feasibility of including multi-institutional patient-level data required to
study similarities and differences across patient subgroups. To address these
challenges, we developed the GAME algorithm. Tested and validated across 7
institutions and 2 languages, GAME integrates data in several levels: (1) at
the institutional level with knowledge graphs to establish relationships
between codes and existing knowledge sources, providing the medical context for
standard codes and their relationship to each other; (2) between institutions,
leveraging language models to determine the relationships between
institution-specific codes with established standard codes; and (3) quantifying
the strength of the relationships between codes using a graph attention
network. Jointly trained embeddings are created using transfer and federated
learning to preserve data privacy. In this study, we demonstrate the
applicability of GAME in selecting relevant features as inputs for AI-driven
algorithms in a range of conditions, e.g., heart failure, rheumatoid arthritis.
We then highlight the application of GAME harmonized multi-institutional EHR
data in a study of Alzheimer's disease outcomes and suicide risk among patients
with mental health disorders, without sharing patient-level data outside
individual institutions.; 94) Indeterminacy in Affective Computing: Considering Meaning and Context in
  Data Collection Practices; Automatic Affect Prediction (AAP) uses computational analysis of input data
such as text, speech, images, and physiological signals to predict various
affective phenomena (e.g., emotions or moods). These models are typically
constructed using supervised machine-learning algorithms, which rely heavily on
labeled training datasets. In this position paper, we posit that all AAP
training data are derived from human Affective Interpretation Processes,
resulting in a form of Affective Meaning. Research on human affect indicates a
form of complexity that is fundamental to such meaning: it can possess what we
refer to here broadly as Qualities of Indeterminacy (QIs) - encompassing
Subjectivity (meaning depends on who is interpreting), Uncertainty (lack of
confidence regarding meanings' correctness), Ambiguity (meaning contains
mutually exclusive concepts) and Vagueness (meaning is situated at different
levels in a nested hierarchy). Failing to appropriately consider QIs leads to
results incapable of meaningful and reliable predictions. Based on this
premise, we argue that a crucial step in adequately addressing indeterminacy in
AAP is the development of data collection practices for modeling corpora that
involve the systematic consideration of 1) a relevant set of QIs and 2) context
for the associated interpretation processes. To this end, we are 1) outlining a
conceptual model of AIPs and the QIs associated with the meaning these produce
and a conceptual structure of relevant context, supporting understanding of its
role. Finally, we use our framework for 2) discussing examples of
context-sensitivity-related challenges for addressing QIs in data collection
setups. We believe our efforts can stimulate a structured discussion of both
the role of aspects of indeterminacy and context in research on AAP, informing
the development of better practices for data collection and analysis.; 95) Rising Marginal Costs, Rising Prices?; We present empirical evidence on the relationship between demand shocks and
price changes, conditional on returns to scale. We find that in industries with
decreasing returns to scale, demand increases (which raise costs) correspond to
price increases. Whereas, in industries with increasing returns to scale,
demand increases (which lower costs) correspond to stable prices. We interpret
the results with a theory of imperfect competition and returns to scale. For
prices to remain stable following a cost decrease, markups necessarily rise.
For prices to increase as cost increases, it is not necessary for markups to
change but does not preclude their role. From a macroeconomic perspective, our
results imply that inflation dynamics and the effectiveness of monetary policy
depend on market structures.; 96) What is a cell type, really? The quest to categorize life’s myriad forms; The problem of cell type became clear to genome biologist Jason Buenrostro in 2013. He was studying a cell line derived from someone with cancer, trying to map out how the DNA was arranged in the nucleus. The cells should have been pretty much identical, he thought. But the more Buenrostro looked at the DNA, the more differences he found in how it was packaged1. “I realized that there were probably hundreds of flavours,” recalls Buenrostro, who was a graduate student at Stanford University in California at the time.; 97) Intelligence Sequencing and the Path-Dependence of Intelligence
  Evolution: AGI-First vs. DCI-First as Irreversible Attractors; The trajectory of intelligence evolution is often framed around the emergence
of artificial general intelligence (AGI) and its alignment with human values.
This paper challenges that framing by introducing the concept of intelligence
sequencing: the idea that the order in which AGI and decentralized collective
intelligence (DCI) emerge determines the long-term attractor basin of
intelligence. Using insights from dynamical systems, evolutionary game theory,
and network models, it argues that intelligence follows a path-dependent,
irreversible trajectory. Once development enters a centralized (AGI-first) or
decentralized (DCI-first) regime, transitions become structurally infeasible
due to feedback loops and resource lock-in. Intelligence attractors are modeled
in functional state space as the co-navigation of conceptual and adaptive
fitness spaces. Early-phase structuring constrains later dynamics, much like
renormalization in physics. This has major implications for AI safety:
traditional alignment assumes AGI will emerge and must be controlled after the
fact, but this paper argues that intelligence sequencing is more foundational.
If AGI-first architectures dominate before DCI reaches critical mass,
hierarchical monopolization and existential risk become locked in. If DCI-first
emerges, intelligence stabilizes around decentralized cooperative equilibrium.
The paper further explores whether intelligence structurally biases itself
toward an attractor based on its self-modeling method -- externally imposed
axioms (favoring AGI) vs. recursive internal visualization (favoring DCI).
Finally, it proposes methods to test this theory via simulations, historical
lock-in case studies, and intelligence network analysis. The findings suggest
that intelligence sequencing is a civilizational tipping point: determining
whether the future is shaped by unbounded competition or unbounded cooperation.; 98) Persuasion Should be Double-Blind: A Multi-Domain Dialogue Dataset With
  Faithfulness Based on Causal Theory of Mind; Persuasive dialogue plays a pivotal role in human communication, influencing
various domains. Recent persuasive dialogue datasets often fail to align with
real-world interpersonal interactions, leading to unfaithful representations.
For instance, unrealistic scenarios may arise, such as when the persuadee
explicitly instructs the persuader on which persuasion strategies to employ,
with each of the persuadee's questions corresponding to a specific strategy for
the persuader to follow. This issue can be attributed to a violation of the
""Double Blind"" condition, where critical information is fully shared between
participants. In actual human interactions, however, key information such as
the mental state of the persuadee and the persuasion strategies of the
persuader is not directly accessible. The persuader must infer the persuadee's
mental state using Theory of Mind capabilities and construct arguments that
align with the persuadee's motivations. To address this gap, we introduce
ToMMA, a novel multi-agent framework for dialogue generation that is guided by
causal Theory of Mind. This framework ensures that information remains
undisclosed between agents, preserving ""double-blind"" conditions, while causal
ToM directs the persuader's reasoning, enhancing alignment with human-like
persuasion dynamics. Consequently, we present CToMPersu, a multi-domain,
multi-turn persuasive dialogue dataset that tackles both double-blind and
logical coherence issues, demonstrating superior performance across multiple
metrics and achieving better alignment with real human dialogues. Our dataset
and prompts are available at https://github.com/DingyiZhang/ToMMA-CToMPersu .; 99) Aligning Crowd-sourced Human Feedback for Reinforcement Learning on Code
  Generation by Large Language Models; This paper studies how AI-assisted programming and large language models
(LLM) improve software developers' ability via AI tools (LLM agents) like
Github Copilot and Amazon CodeWhisperer, while integrating human feedback to
enhance reinforcement learning (RLHF) with crowd-sourced computation to enhance
text-to-code generation. Additionally, we demonstrate that our Bayesian
optimization framework supports AI alignment in code generation by distributing
the feedback collection burden, highlighting the value of collecting human
feedback of good quality. Our empirical evaluations demonstrate the efficacy of
this approach, showcasing how LLM agents can be effectively trained for
improved text-to-code generation. Our Bayesian optimization framework can be
designed for general domain-specific languages, promoting the alignment of
large language model capabilities with human feedback in AI-assisted
programming for code generation.; 100) Stacking, Strain-Engineering Induced Altermagnetism, Multipiezo Effect,
  and Topological State in Two-Dimensional Materials; Altermagnetism, as a newly identified form of unconventional
antiferromagnetism, enables the removal of spin degeneracy in the absence of
net magnetization that provides a platform for the low power consumption and
ultra-fast device applications. However, the rare attention has been paid to
the relationship between stacking, strain and altermagnet, multipiezo effect
and topological state. Here, we propose a mechanism to realize the altermagnet,
multipiezo effect, and topological state in two-dimensional materials by the
stacking and strain engineering. Based on the analysis of symmetry, we find
that the spin splitting feature related to the Ut, PTt, MzUt, or MzPTt
symmetries in altermagnet multilayers. In addition, we find that the stacking
engineering can effectively realize the transform from antiferromagnetism to
altermagnetism and semiconductor to metal for the Jauns bilayer V2SeTeO. More
interestingly, the strain not only induces an intriguing multipiezo effect,
encompassing the piezovalley, piezomagnetism and piezoelectric, but also
achieves the abundant topological phase. Our findings offer a generalized
direction for manipulating the spin splitting, valley polarization, and
topological states, promoting practical application of valleytronic and
spintronic devices based on two-dimensional altermagnets.",0.0,0.40298313359773236
