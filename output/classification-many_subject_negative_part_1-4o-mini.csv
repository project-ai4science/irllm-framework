paper_a_id,method,research_type,paper_b_id,paper_b_text,paper_c_id,paper_c_text,y_true,y_pred,reason,paperA
neg-1s-0,1,,1206.2966v2,"Panel Data Models with Nonadditive Unobserved Heterogeneity: Estimation
  and Inference; This paper considers fixed effects estimation and inference in linear and
nonlinear panel data models with random coefficients and endogenous regressors.
The quantities of interest -- means, variances, and other moments of the random
coefficients -- are estimated by cross sectional sample moments of GMM
estimators applied separately to the time series of each individual. To deal
with the incidental parameter problem introduced by the noise of the
within-individual estimators in short panels, we develop bias corrections.
These corrections are based on higher-order asymptotic expansions of the GMM
estimators and produce improved point and interval estimates in moderately long
panels. Under asymptotic sequences where the cross sectional and time series
dimensions of the panel pass to infinity at the same rate, the uncorrected
estimator has an asymptotic bias of the same order as the asymptotic variance.
The bias corrections remove the bias without increasing variance. An empirical
example on cigarette demand based on Becker, Grossman and Murphy (1994) shows
significant heterogeneity in the price effect across U.S. states.",2202.03234v1,"Generalised norm resolvent convergence: comparison of different concepts; In this paper, we show that the two concepts of generalised norm resolvent
convergence introduced by Weidmann and the first author of this paper are
equivalent. We also focus on the convergence speed and provide conditions under
which the convergence speed is the same for both concepts. We illustrate the
abstract results by a large number of examples.",False,False,"The concepts from the two papers do not align in a way that allows for a novel multidisciplinary research idea. The first paper focuses on econometric methods for panel data, while the second deals with mathematical convergence concepts, making it challenging to create a feasible and useful research idea that meets all standards.",
neg-1s-1,1,,1506.05620v2,"A parameterized approximation algorithm for the mixed and windy
  Capacitated Arc Routing Problem: theory and experiments; We prove that any polynomial-time $\alpha(n)$-approximation algorithm for the
$n$-vertex metric asymmetric Traveling Salesperson Problem yields a
polynomial-time $O(\alpha(C))$-approximation algorithm for the mixed and windy
Capacitated Arc Routing Problem, where $C$ is the number of weakly connected
components in the subgraph induced by the positive-demand arcs---a small number
in many applications. In conjunction with known results, we obtain
constant-factor approximations for $C\in O(\log n)$ and $O(\log C/\log\log
C)$-approximations in general. Experiments show that our algorithm, together
with several heuristic enhancements, outperforms many previous polynomial-time
heuristics. Finally, since the solution quality achievable in polynomial time
appears to mainly depend on $C$ and since $C=1$ in almost all benchmark
instances, we propose the Ob benchmark set, simulating cities that are divided
into several components by a river.",0901.1400v1,"Variation of quasiconformal mappings on lines; We obtain improved regularity of homeomorphic solutions of the reduced
Beltrami equation, as compared to the standard Beltrami equation. Such an
improvement is not possible in terms of Holder or Sobolev regularity; instead,
our results concern the generalized variation of restrictions to lines.
Specifically, we prove that the restriction to any line segment has finite
p-variation for all p>1 but not necessarily for p=1.",False,False,"The concepts from the two papers do not align in a way that creates a feasible, novel, and useful multidisciplinary research idea. The first paper focuses on algorithmic optimization in routing problems, while the second deals with mathematical properties of quasiconformal mappings, lacking a clear intersection for practical application.",
neg-1s-2,1,,1710.01236v6,"netgwas: An R Package for Network-Based Genome-Wide Association Studies; Graphical models are a powerful tool in modelling and analysing complex
biological associations in high-dimensional data. The R-package netgwas
implements the recent methodological development on copula graphical models to
(i) construct linkage maps, (ii) infer linkage disequilibrium networks from
genotype data, and (iii) detect high-dimensional genotype-phenotype networks.
The netgwas learns the structure of networks from ordinal data and mixed
ordinal-and-continuous data. Here, we apply the functionality in netgwas to
various multivariate example datasets taken from the literature to demonstrate
the kind of insight that can be obtained from the package. We show that our
package offers a more realistic association analysis than the classical
approaches, as it discriminates between direct and induced correlations by
adjusting for the effect of all other variables while performing pairwise
associations. This feature controls for spurious interactions between variables
that can arise from conventional approaches in a biological sense. The netgwas
package uses a parallelization strategy on multi-core processors to speed-up
computations. The netgwas package is freely available at
https://cran.r-project.org/web/packages/netgwas",2103.05504v1,"Status of the wave function of Quantum Mechanics, or, What is Quantum
  Mechanics trying to tell us?; The most debated status of the wave function of Quantum Mechanics is
discussed in the light of the epistemological vs ontological opposition.",False,False,"The concepts from the two papers—network-based genome-wide association studies and the philosophical implications of quantum mechanics—do not easily converge into a feasible, novel, and useful multidisciplinary research idea. The lack of a clear application or experimental validation limits the potential for a meaningful integration.",
neg-1s-3,1,,1011.6268v1,"Quantitative Analysis of Bloggers Collective Behavior Powered by
  Emotions; Large-scale data resulting from users online interactions provide the
ultimate source of information to study emergent social phenomena on the Web.
From individual actions of users to observable collective behaviors, different
mechanisms involving emotions expressed in the posted text play a role. Here we
combine approaches of statistical physics with machine-learning methods of text
analysis to study emergence of the emotional behavior among Web users. Mapping
the high-resolution data from digg.com onto bipartite network of users and
their comments onto posted stories, we identify user communities centered
around certain popular posts and determine emotional contents of the related
comments by the emotion-classifier developed for this type of texts. Applied
over different time periods, this framework reveals strong correlations between
the excess of negative emotions and the evolution of communities. We observe
avalanches of emotional comments exhibiting significant self-organized critical
behavior and temporal correlations. To explore robustness of these critical
states, we design a network automaton model on realistic network connections
and several control parameters, which can be inferred from the dataset.
Dissemination of emotions by a small fraction of very active users appears to
critically tune the collective states.",1905.10982v1,"An Intelligent Monitoring System of Vehicles on Highway Traffic; Vehicle speed monitoring and management of highways is the critical problem
of the road in this modern age of growing technology and population. A poor
management results in frequent traffic jam, traffic rules violation and fatal
road accidents. Using traditional techniques of RADAR, LIDAR and LASAR to
address this problem is time-consuming, expensive and tedious. This paper
presents an efficient framework to produce a simple, cost efficient and
intelligent system for vehicle speed monitoring. The proposed method uses an HD
(High Definition) camera mounted on the road side either on a pole or on a
traffic signal for recording video frames. On the basis of these frames, a
vehicle can be tracked by using radius growing method, and its speed can be
calculated by calculating vehicle mask and its displacement in consecutive
frames. The method uses pattern recognition, digital image processing and
mathematical techniques for vehicle detection, tracking and speed calculation.
The validity of the proposed model is proved by testing it on different
highways.",False,True,"By combining the emotional analysis of online user behavior from Paper 1 with the intelligent vehicle monitoring system from Paper 2, a novel research idea could emerge that explores how emotional states of drivers (inferred from social media) influence traffic patterns and safety, making it both feasible and useful.",
neg-1s-4,1,,1911.00431v2,"Composition of Bhargava's Cubes over Number Fields; In this paper, the composition of Bhargava's cubes is generalized to the ring
of integers of a number field of narrow class number one, excluding the case of
totally imaginary number fields.",1604.05350v1,"Counting and Enumerating Crossing-free Geometric Graphs; We describe a framework for counting and enumerating various types of
crossing-free geometric graphs on a planar point set. The framework generalizes
ideas of Alvarez and Seidel, who used them to count triangulations in time
$O(2^nn^2)$ where $n$ is the number of points. The main idea is to reduce the
problem of counting geometric graphs to counting source-sink paths in a
directed acyclic graph.
  The following new results will emerge. The number of all crossing-free
geometric graphs can be computed in time $O(c^nn^4)$ for some $c < 2.83929$.
The number of crossing-free convex partitions can be computed in time
$O(2^nn^4)$. The number of crossing-free perfect matchings can be computed in
time $O(2^nn^4)$. The number of convex subdivisions can be computed in time
$O(2^nn^4)$. The number of crossing-free spanning trees can be computed in time
$O(c^nn^4)$ for some $c < 7.04313$. The number of crossing-free spanning cycles
can be computed in time $O(c^nn^4)$ for some $c < 5.61804$.
  With the same bounds on the running time we can construct data structures
which allow fast enumeration of the respective classes. For example, after
$O(2^nn^4)$ time of preprocessing we can enumerate the set of all crossing-free
perfect matchings using polynomial time per enumerated object. For
crossing-free perfect matchings and convex partitions we further obtain
enumeration algorithms where the time delay for each (in particular, the first)
output is bounded by a polynomial in $n$.
  All described algorithms are comparatively simple, both in terms of their
analysis and implementation.",False,False,"The concepts from the two papers are too distinct, with one focusing on number theory and the other on geometric graph theory. While interdisciplinary connections can be made, they do not converge into a single novel, feasible, and useful research idea that meets all the specified standards.",
neg-1s-5,1,,2111.09414v1,"Developmental Status and Perspectives for Tissue Engineering in Urology; Tissue engineering technology and tissue cell-based stem cell research have
made great strides in treating tissue and organ damage, correcting tissue and
organ dysfunction, and reducing surgical complications. In the past,
traditional methods have used biological substitutes for tissue repair
materials, while tissue engineering technology has focused on merging sperm
cells with biological materials to form biological tissues with the same
structure and function as their own tissues. The advantage is that tissue
engineering technology can overcome donors. Material procurement restrictions
can effectively reduce complications. The aim of studying tissue engineering
technology is to find sperm cells and suitable biological materials to replace
the original biological functions of tissues and to establish a suitable in
vivo microenvironment. This article mainly describes the current developments
of tissue engineering in various fields of urology and discusses the future
trends of tissue engineering technology in the treatment of complex diseases of
the urinary system. The results of the research in this article indicate that
while the current clinical studies are relatively few, the good results from
existing animal model studies indicate good prospects of tissue engineering
technology for the treatment of various urinary tract diseases in the future.",1401.0087v1,"Contributors of carbon dioxide in the atmosphere in Europe: the surface
  response analysis; This paper is a continuation of the statistical modeling of the nonlinear
relationship between atmospheric CO2 and attributable variables that can
account for emissions, based on data from EU countries, in order to compare the
relevant findings to those obtained in the case of US data, in [1, 2]. The
current study was initiated in [3], leading to the optimal second-order model,
based on three linear terms and five second-order terms. We conclude this study
in the present work, by finding the canonical decomposition of the nonlinear
model, and by computing the specific two-dimensional confidence regions that it
leads to. We then use the model in order to quantify the net effect of various
risk factors, and compare to the results obtained in the US case.",False,False,"The concepts from the two papers do not align in a way that allows for a feasible multidisciplinary research idea. Tissue engineering in urology focuses on biological applications, while the second paper deals with statistical modeling of CO2 emissions, lacking a clear intersection for a novel and useful research idea.",
neg-1s-6,1,,2104.11365v1,"Can You Trust Your Trust Measure?; Trust in human-robot interactions (HRI) is measured in two main ways: through
subjective questionnaires and through behavioral tasks. To optimize
measurements of trust through questionnaires, the field of HRI faces two
challenges: the development of standardized measures that apply to a variety of
robots with different capabilities, and the exploration of social and
relational dimensions of trust in robots (e.g., benevolence). In this paper we
look at how different trust questionnaires fare given these challenges that
pull in different directions (being general vs. being exploratory) by studying
whether people think the items in these questionnaires are applicable to
different kinds of robots and interactions. In Study 1 we show that after being
presented with a robot (non-humanoid) and an interaction scenario (fire
evacuation), participants rated multiple questionnaire items such as ""This
robot is principled"" as ""Non-applicable to robots in general"" or
""Non-applicable to this robot"". In Study 2 we show that the frequency of these
ratings change (indeed, even for items rated as N/A to robots in general) when
a new scenario is presented (game playing with a humanoid robot). Finally,
while overall trust scores remained robust to N/A ratings, our results revealed
potential fallacies in the way these scores are commonly interpreted. We
conclude with recommendations for the development, use and results-reporting of
trust questionnaires for future studies, as well as theoretical implications
for the field of HRI.",1806.06696v1,"SMOGS: Social Network Metrics of Game Success; This paper develops metrics from a social network perspective that are
directly translatable to the outcome of a basketball game. We extend a
state-of-the-art multi-resolution stochastic process approach to modeling
basketball by modeling passes between teammates as directed dynamic relational
links on a network and introduce multiplicative latent factors to study
higher-order patterns in players' interactions that distinguish a successful
game from a loss. Parameters are estimated using a Markov chain Monte Carlo
sampler. Results in simulation experiments suggest that the sampling scheme is
effective in recovering the parameters. We then apply the model to the first
high-resolution optical tracking dataset collected in college basketball games.
The learned latent factors demonstrate significant differences between players'
passing and receiving tendencies in a loss than those in a win. The model is
applicable to team sports other than basketball, as well as other time-varying
network observations.",False,True,"By combining trust measurement in human-robot interactions with social network metrics from sports, a novel research idea could explore how trust dynamics among team members (including robots) influence game outcomes. This idea is feasible, as it can be validated through experimental studies in team settings.",
neg-1s-7,1,,2311.05862v1,"Emergence and reconfiguration of modular structure for synaptic neural
  networks during continual familiarity detection; While advances in artificial intelligence and neuroscience have enabled the
emergence of neural networks capable of learning a wide variety of tasks, our
understanding of the temporal dynamics of these networks remains limited. Here,
we study the temporal dynamics during learning of Hebbian Feedforward (HebbFF)
neural networks in tasks of continual familiarity detection. Drawing
inspiration from the field of network neuroscience, we examine the network's
dynamic reconfiguration, focusing on how network modules evolve throughout
learning. Through a comprehensive assessment involving metrics like network
accuracy, modular flexibility, and distribution entropy across diverse learning
modes, our approach reveals various previously unknown patterns of network
reconfiguration. In particular, we find that the emergence of network
modularity is a salient predictor of performance, and that modularization
strengthens with increasing flexibility throughout learning. These insights not
only elucidate the nuanced interplay of network modularity, accuracy, and
learning dynamics but also bridge our understanding of learning in artificial
and biological realms.",1305.0983v2,"Real-Time Welfare-Maximizing Regulation Allocation in Dynamic
  Aggregator-EVs System; The concept of vehicle-to-grid (V2G) has gained recent interest as more and
more electric vehicles (EVs) are put to use. In this paper, we consider a
dynamic aggregator-EVs system, where an aggregator centrally coordinates a
large number of dynamic EVs to perform regulation service. We propose a
Welfare-Maximizing Regulation Allocation (WMRA) algorithm for the aggregator to
fairly allocate the regulation amount among its EVs. Compared to previous
works, WMRA accommodates a wide spectrum of vital system characteristics,
including dynamics of EV, limited EV battery size, EV battery degradation cost,
and the cost of using external energy sources for the aggregator. The algorithm
operates in real time and does not require any prior knowledge of the
statistical information of the system. Theoretically, we demonstrate that WMRA
is away from the optimum by O(1/V), where V is a controlling parameter
depending on EV's battery size. In addition, our simulation results indicate
that WMRA can substantially outperform a suboptimal greedy algorithm.",False,True,"The integration of dynamic modular neural networks from Paper 1 with the real-time regulation allocation for EVs from Paper 2 can lead to a novel system that optimizes EV energy management using adaptive learning algorithms. This idea is feasible, experimental, and addresses real-world energy challenges effectively.",
neg-1s-8,1,,1309.6610v3,"Adversarial Multiple Access Channels with Individual Injection Rates; We study deterministic distributed broadcasting in synchronous
multiple-access channels. Packets are injected into $n$ nodes by a window-type
adversary that is constrained by a window $w$ and injection rates individually
assigned to all nodes. We investigate what queue size and packet latency can be
achieved with the maximum aggregate injection rate of one packet per round,
depending on properties of channels and algorithms. We give a non-adaptive
algorithm for channels with collision detection and an adaptive algorithm for
channels without collision detection that achieve $O(\min(n+w,w\log n))$ packet
latency. We show that packet latency has to be either $\Omega(w \max (1,\log_w
n))$, when $w\le n$, or $\Omega(w+n)$, when $w>n$, as a matching lower bound to
these algorithms. We develop a non-adaptive algorithm for channels without
collision detection that achieves $O(n+w)$ queue size and $O(nw)$ packet
latency. This is in contrast with the adversarial model of global injection
rates, in which non-adaptive algorithms with bounded packet latency do not
exist (Chlebus et al. Distributed Computing 22(2): 93 - 116, 2009). Our
algorithm avoids collisions produced by simultaneous transmissions; we show
that any algorithm with this property must have $\Omega(nw)$ packet latency.",1103.5665v1,"Evaluating the Precision of Estimators of Quantile-Based Risk Measures; This paper examines the precision of estimators of Quantile-Based Risk
Measures (Value at Risk, Expected Shortfall, Spectral Risk Measures). It first
addresses the question of how to estimate the precision of these estimators,
and proposes a Monte Carlo method that is free of some of the limitations of
existing approaches. It then investigates the distribution of risk estimators,
and presents simulation results suggesting that the common practice of relying
on asymptotic normality results might be unreliable with the sample sizes
commonly available to them. Finally, it investigates the relationship between
the precision of different risk estimators and the distribution of underlying
losses (or returns), and yields a number of useful conclusions.",False,False,"The concepts from the two papers do not align in a way that creates a feasible, novel, and useful multidisciplinary research idea. The first paper focuses on communication algorithms, while the second deals with risk estimation methods, lacking a clear intersection for practical application.",
neg-1s-9,1,,2303.17696v1,"Dual Cross-Attention for Medical Image Segmentation; We propose Dual Cross-Attention (DCA), a simple yet effective attention
module that is able to enhance skip-connections in U-Net-based architectures
for medical image segmentation. DCA addresses the semantic gap between encoder
and decoder features by sequentially capturing channel and spatial dependencies
across multi-scale encoder features. First, the Channel Cross-Attention (CCA)
extracts global channel-wise dependencies by utilizing cross-attention across
channel tokens of multi-scale encoder features. Then, the Spatial
Cross-Attention (SCA) module performs cross-attention to capture spatial
dependencies across spatial tokens. Finally, these fine-grained encoder
features are up-sampled and connected to their corresponding decoder parts to
form the skip-connection scheme. Our proposed DCA module can be integrated into
any encoder-decoder architecture with skip-connections such as U-Net and its
variants. We test our DCA module by integrating it into six U-Net-based
architectures such as U-Net, V-Net, R2Unet, ResUnet++, DoubleUnet and
MultiResUnet. Our DCA module shows Dice Score improvements up to 2.05% on GlaS,
2.74% on MoNuSeg, 1.37% on CVC-ClinicDB, 1.12% on Kvasir-Seg and 1.44% on
Synapse datasets. Our codes are available at:
https://github.com/gorkemcanates/Dual-Cross-Attention",2010.13149v3,"Approximating Aggregated SQL Queries With LSTM Networks; Despite continuous investments in data technologies, the latency of querying
data still poses a significant challenge. Modern analytic solutions require
near real-time responsiveness both to make them interactive and to support
automated processing. Current technologies (Hadoop, Spark, Dataflow) scan the
dataset to execute queries. They focus on providing a scalable data storage to
maximize task execution speed. We argue that these solutions fail to offer an
adequate level of interactivity since they depend on continual access to data.
In this paper we present a method for query approximation, also known as
approximate query processing (AQP), that reduce the need to scan data during
inference (query calculation), thus enabling a rapid query processing tool. We
use LSTM network to learn the relationship between queries and their results,
and to provide a rapid inference layer for predicting query results. Our method
(referred as ``Hunch``) produces a lightweight LSTM network which provides a
high query throughput. We evaluated our method using twelve datasets and
compared to state-of-the-art AQP engines (VerdictDB, BlinkDB) from query
latency, model weight and accuracy perspectives. The results show that our
method predicted queries' results with a normalized root mean squared error
(NRMSE) ranging from approximately 1\% to 4\% which in the majority of our data
sets was better then the compared benchmarks. Moreover, our method was able to
predict up to 120,000 queries in a second (streamed together), and with a
single query latency of no more than 2ms.",False,False,"While both papers present innovative techniques in their respective fields, the concepts from medical image segmentation and SQL query approximation do not easily converge into a cohesive multidisciplinary research idea that meets all the specified standards of feasibility, novelty, and usefulness.",
neg-1s-10,1,,2401.15253v1,"Testing the Exogeneity of Instrumental Variables and Regressors in
  Linear Regression Models Using Copulas; We provide a Copula-based approach to test the exogeneity of instrumental
variables in linear regression models. We show that the exogeneity of
instrumental variables is equivalent to the exogeneity of their standard normal
transformations with the same CDF value. Then, we establish a Wald test for the
exogeneity of the instrumental variables. We demonstrate the performance of our
test using simulation studies. Our simulations show that if the instruments are
actually endogenous, our test rejects the exogeneity hypothesis approximately
93% of the time at the 5% significance level. Conversely, when instruments are
truly exogenous, it dismisses the exogeneity assumption less than 30% of the
time on average for data with 200 observations and less than 2% of the time for
data with 1,000 observations. Our results demonstrate our test's effectiveness,
offering significant value to applied econometricians.",2111.04199v2,"Proteins Evolution Upon Point Mutations; The primary aim of this work is to explore how proteins point mutations
impact their marginal stability and, hence, their evolvability. With this
purpose, we show that the use of four classic notions, namely, those from
Leibniz & Kant (1768), Maynard Smith (1970), Einstein & Infeld (1961), and
Anfinsen (1973), is sufficient for a better understanding of the
protein-evolution and, consequently, to determine the factors that could
control it. The preliminary results -- without considering epistasis effects
explicitly -- indicate that the protein marginal-stability change upon point
mutations provides the necessary and sufficient information to describe,
through a Boltzmann factor, the evolution of the amide hydrogen-exchange
protection factors. This finding is of paramount importance because it
illustrates the impact of point mutations on both the protein
marginal-stability and the ensemble of folded conformations coexisting with the
native state and, in the presence of metamorphism, on the propensity for the
appearance of new folds and functions.",False,False,"The concepts from the two papers do not converge in a way that creates a feasible, novel, and useful multidisciplinary research idea. The statistical methods in econometrics and the biological focus on protein evolution do not lend themselves to a coherent experimental hypothesis that meets all the outlined standards.",
neg-1s-11,1,,1311.0654v1,"Reunion probabilities of $N$ one-dimensional random walkers with mixed
  boundary conditions; In this work we extend the results of the reunion probability of $N$
one-dimensional random walkers to include mixed boundary conditions between
their trajectories. The level of the mixture is controlled by a parameter $c$,
which can be varied from $c=0$ (independent walkers) to $c\to\infty$ (vicious
walkers). The expressions are derived by using Quantum Mechanics formalism
(QMf) which allows us to map this problem into a Lieb-Liniger gas (LLg) of $N$
one-dimensional particles. We use Bethe ansatz and Gaudin's conjecture to
obtain the normalized wave-functions and use this information to construct the
propagator. As it is well-known, depending on the boundary conditions imposed
at the endpoints of a line segment, the statistics of the maximum heights of
the reunited trajectories have some connections with different ensembles in
Random Matrix Theory (RMT). Here we seek to extend those results and consider
four models: absorbing, periodic, reflecting, and mixed. In all four cases, the
probability that the maximum height is less or equal than $L$ takes the form
$F_N(L)=A_N\sum_{k\in\Omega_{B}}\int Dz
e^{-\sum_{j=1}^Nk_j^2+G_N(k)-\sum_{j,\ell=1}^N
z_jV_{j\ell}(k)\overline{z}_\ell}$, where $A_N$ is a normalization constant,
$G_N(k)$ and $V_{j\ell}(k)$ depend on the type of boundary condition, and
$\Omega_{B}$ is the solution set of quasi-momenta $k$ obeying the Bethe
equations for that particular boundary condition.",2112.07268v2,"Finding the Instrumental Variables of Household Registration: A
  discussion of the impact of China's household registration system on the
  citizenship of the migrant population; Due to the specificity of China's dualistic household registration system and
the differences in the rights and interests attached to it, household
registration is prevalent as a control variable in the empirical evidence. In
the context of family planning policies, this paper proposes to use family size
and number of children as instrumental variables for household registration,
and discusses qualitatively and statistically verifies their relevance and
exogeneity, while empirically analyzing the impact of the household
registration system on citizenship of the mobile population. After controlling
for city, individual control variables and fixed effects, the following
conclusions are drawn: family size and number of children pass the
over-identification test when used as instrumental variables for household
registration; non-agricultural households have about 20.2% lower settlement
intentions and 7.28% lower employment levels in inflow cities than agricultural
households; the mechanism of the effect of the nature of household registration
on employment still holds for the non-mobile population group.",False,False,"The concepts from the two papers are too disparate; one focuses on mathematical modeling of random walkers while the other addresses social implications of household registration in China. There is no clear, feasible, and novel intersection that can be experimentally validated.",
neg-1s-12,1,,1204.3126v2,"Characterisation of placental malaria in olive baboons (Papio anubis)
  infected with Plasmodium knowlesi H strain; Pregnant women have increased susceptibility to malaria infection. In these
women, malaria parasites are frequently found sequestered in the placental
intervillous spaces, a condition referred to as placental malaria (PM).
Placental malaria threatens the health of the mother and the child's life by
causing still births and reduction in gestational age. An estimated 24 million
pregnant women in Sub-Saharan Africa are at risk. Mechanisms responsible for
increased susceptibility in pregnant women are not fully understood. Pregnancy
malaria studies have been limited by the lack of a suitable animal model. This
research aimed to develop a baboon (Papio anubis) model for studying PM. The
pregnancies of three adult female baboons were synchronized and their
gestational levels confirmed by ultrasonography. On the 150th day of gestation
the pregnant baboons were infected with Plasmodium knowlesi H strain parasites
together with four nulligravid control baboons. Parasitaemia was monitored from
two days post inoculation until the 159th day of gestation when caesarean
section was done on one baboon in order to obtain the placenta. Two baboons
aborted their conceptus. Smears prepared from placental blood demonstrated the
presence of Plasmodium knowlesi parasites in all the three sampled placentas.
These new findings show that P. knowlesi sequesters in the baboon placenta. In
addition, this study has characterized haemoglobin, eosinophil, Immunoglobulin
G and Immunoglobulin M profiles in this model. Thus a non human primate
(baboon) model for studying PM has been established. The established baboon -
P. knowlesi model for studying human placental/pregnancy malaria now offers an
opportunity for circumventing the obstacles experienced during human studies
like having inadequate tissue for analysis, inaccurate estimation of
gestational age, moral, ethical and financial limitations.",2411.01857v2,"On $\ell_p$-Vietoris-Rips complexes; We study the concepts of the $\ell_p$-Vietoris-Rips simplicial set and the
$\ell_p$-Vietoris-Rips complex of a metric space, where $1\leq p \leq \infty.$
This theory unifies two established theories: for $p=\infty,$ this is the
classical theory of Vietoris-Rips complexes, and for $p=1,$ this corresponds to
the blurred magnitude homology theory. We prove several results that are known
for the Vietoris-Rips complex in the general case: (1) we prove a stability
theorem for the corresponding version of the persistent homology; (2) we show
that, for a compact Riemannian manifold and a sufficiently small scale
parameter, all the ""$\ell_p$-Vietoris-Rips spaces"" are homotopy equivalent to
the manifold; (3) we demonstrate that the $\ell_p$-Vietoris-Rips spaces are
invariant (up to homotopy) under taking the metric completion. Additionally, we
show that the limit of the homology groups of the $\ell_p$-Vietoris-Rips
spaces, as the scale parameter tends to zero, does not depend on $p$; and that
the homology groups of the $\ell_p$-Vietoris-Rips spaces commute with filtered
colimits of metric spaces.",False,False,"The concepts from the two papers do not converge in a way that creates a feasible, novel, and useful multidisciplinary research idea. The first paper focuses on biological models for studying malaria, while the second deals with mathematical structures in topology, lacking a clear intersection for practical application.",
neg-1s-13,1,,1508.00399v4,"Capturing rogue waves by multi-point statistics; As an example for complex systems with extreme events we investigate ocean
wave states exhibiting rogue waves. We present a statistical method of data
analysis based on multi-point statistics which for the first time allows
grasping extreme rogue wave events in a statistically highly satisfactory
manner. The key to the success of the approach is mapping the complexity of
multi-point data onto the statistics of hierarchically ordered height
increments for different time scales for which we can show that a stochastic
cascade process with Markov properties is governed by a Fokker-Planck equation.
Conditional probabilities as well as the Fokker-Planck equation itself can be
estimated directly from the available observational data. With this stochastic
description surrogate data sets can in turn be generated allowing to work out
arbitrary statistical features of the complex sea state in general and extreme
rogue wave events in particular. The results also open up new perspectives for
forecasting the occurrence probability of extreme rogue wave events, and even
for forecasting the occurrence of individual rogue waves based on precursory
dynamics.",2209.01378v1,"Tree-Based Learning in RNNs for Power Consumption Forecasting; A Recurrent Neural Network that operates on several time lags, called an
RNN(p), is the natural generalization of an Autoregressive ARX(p) model. It is
a powerful forecasting tool when different time scales can influence a given
phenomenon, as it happens in the energy sector where hourly, daily, weekly and
yearly interactions coexist. The cost-effective BPTT is the industry standard
as learning algorithm for RNNs. We prove that, when training RNN(p) models,
other learning algorithms turn out to be much more efficient in terms of both
time and space complexity. We also introduce a new learning algorithm, the Tree
Recombined Recurrent Learning, that leverages on a tree representation of the
unrolled network and appears to be even more effective. We present an
application of RNN(p) models for power consumption forecasting on the hourly
scale: experimental results demonstrate the efficiency of the proposed
algorithm and the excellent predictive accuracy achieved by the selected model
both in point and in probabilistic forecasting of the energy consumption.",False,True,"By combining the statistical methods for rogue wave forecasting from Paper 1 with the RNN(p) model for power consumption forecasting from Paper 2, a novel multidisciplinary research idea could emerge that predicts extreme energy consumption events based on complex ocean wave data, which is both feasible and useful.",
neg-1s-14,1,,1104.0910v1,"Classes of fast and specific search mechanisms for proteins on DNA; Problems of search and recognition appear over different scales in biological
systems. In this review we focus on the challenges posed by interactions
between proteins, in particular transcription factors, and DNA and possible
mechanisms which allow for a fast and selective target location. Initially we
argue that DNA-binding proteins can be classified, broadly, into three distinct
classes which we illustrate using experimental data. Each class calls for a
different search process and we discuss the possible application of different
search mechanisms proposed over the years to each class. The main thrust of
this review is a new mechanism which is based on barrier discrimination. We
introduce the model and analyze in detail its consequences. It is shown that
this mechanism applies to all classes of transcription factors and can lead to
a fast and specific search. Moreover, it is shown that the mechanism has
interesting transient features which allow for stability at the target despite
rapid binding and unbinding of the transcription factor from the target.",1810.10805v1,"Light-Quark Resonances at COMPASS; The main goal of the spectroscopy program at COMPASS is to explore the
light-meson spectrum in the mass range below about $2\,\text{GeV}/c^2$ using
diffractive dissociation reactions. Our flagship channel is the production of
three charged pions in the reaction: $\pi^- + p \to \pi^-\pi^-\pi^+ +
p_\text{recoil}$, for which COMPASS has acquired the so far world's largest
dataset of roughly $50\,\text{M}$ exclusive events using an $190\,\text{GeV}/c$
$\pi^-$ beam.
  In order to extract the parameters of the $\pi_J$ and $a_J$ resonances that
appear in the $\pi^-\pi^-\pi^+$ system, we performed the so far most
comprehensive resonance-model fit, using Breit-Wigner parametrizations. This
method in combination with the high statistical precision of our data allows us
to study ground and excited states. We study the $a_4(2040)$ resonance in the
$\rho(770)\pi G$ and $f_2(1270)\pi F$ decays. In addition to the ground state
resonance $a_1(1260)$, we have found evidence for the $a_1(1640)$. We also
study the spectrum of $\pi_2$ states by simultaneously describing four
$J^{PC}=2^{-+}$ waves using three $\pi_2$ resonances, the $\pi_2(1670)$, the
$\pi_2(1880)$, and the $\pi_2(2005)$.
  Using a novel analysis approach, where the resonance-model fit is performed
simultaneously in narrow bins of the squared four-momentum transfer $t'$
between the beam pion and the target proton, allows us to study the $t'$
dependence of resonant and non-resonant components included in our model. We
observe that for most of the partial waves, the non-resonant components show a
steeper $t'$ spectrum compared to the resonances and that the $t'$ spectrum of
most of the resonances becomes shallower with increasing resonance mass. We
also study the $t'$ dependence of the relative phases between resonance
components. The pattern we observe is consistent with a common production
mechanism of these states.",False,False,"While both papers explore complex biological and physical systems, they do not present overlapping concepts that can be feasibly combined into a novel multidisciplinary research idea. The mechanisms discussed in Paper 1 focus on protein-DNA interactions, while Paper 2 centers on particle physics, lacking a clear intersection for practical application.",
neg-1s-15,1,,2011.05658v1,"Disentangling Community-level Changes in Crime Trends During the
  COVID-19 Pandemic in Chicago; Recent studies exploiting city-level time series have shown that, around the
world, several crimes declined after COVID-19 containment policies have been
put in place. Using data at the community-level in Chicago, this work aims to
advance our understanding on how public interventions affected criminal
activities at a finer spatial scale. The analysis relies on a two-step
methodology. First, it estimates the community-wise causal impact of social
distancing and shelter-in-place policies adopted in Chicago via Structural
Bayesian Time-Series across four crime categories (i.e., burglary, assault,
narcotics-related offenses, and robbery). Once the models detected the
direction, magnitude and significance of the trend changes, Firth's Logistic
Regression is used to investigate the factors associated to the statistically
significant crime reduction found in the first step of the analyses.
Statistical results first show that changes in crime trends differ across
communities and crime types. This suggests that beyond the results of aggregate
models lies a complex picture characterized by diverging patterns. Second,
regression models provide mixed findings regarding the correlates associated
with significant crime reduction: several relations have opposite directions
across crimes with population being the only factor that is stably and
positively associated with significant crime reduction.",1212.0388v1,"Hypergraph and protein function prediction with gene expression data; Most network-based protein (or gene) function prediction methods are based on
the assumption that the labels of two adjacent proteins in the network are
likely to be the same. However, assuming the pairwise relationship between
proteins or genes is not complete, the information a group of genes that show
very similar patterns of expression and tend to have similar functions (i.e.
the functional modules) is missed. The natural way overcoming the information
loss of the above assumption is to represent the gene expression data as the
hypergraph. Thus, in this paper, the three un-normalized, random walk, and
symmetric normalized hypergraph Laplacian based semi-supervised learning
methods applied to hypergraph constructed from the gene expression data in
order to predict the functions of yeast proteins are introduced. Experiment
results show that the average accuracy performance measures of these three
hypergraph Laplacian based semi-supervised learning methods are the same.
However, their average accuracy performance measures of these three methods are
much greater than the average accuracy performance measures of un-normalized
graph Laplacian based semi-supervised learning method (i.e. the baseline method
of this paper) applied to gene co-expression network created from the gene
expression data.",False,False,"The concepts from the two papers do not align in a way that creates a feasible multidisciplinary research idea. While one focuses on crime trends and public policy, the other is centered on protein function prediction using gene expression data, lacking a clear intersection for practical application.",
neg-1s-16,1,,2310.04907v1,"An Information Theory Approach to the Stock and Cryptocurrency Market: A
  Statistical Equilibrium Perspective; We study the stochastic structure of cryptocurrency rates of returns as
compared to stock returns by focusing on the associated cross-sectional
distributions. We build two datasets. The first comprises forty-six major
cryptocurrencies, and the second includes all the companies listed in the S&P
500. We collect individual data from January 2017 until December 2022. We then
apply the Quantal Response Statistical Equilibrium (QRSE) model to recover the
cross-sectional frequency distribution of the daily returns of cryptocurrencies
and S&P 500 companies. We study the stochastic structure of these two markets
and the properties of investors' behavior over bear and bull trends. Finally,
we compare the degree of informational efficiency of these two markets.",1303.1633v1,"Joint power and admission control via p norm minimization deflation; In an interference network, joint power and admission control aims to support
a maximum number of links at their specified signal to interference plus noise
ratio (SINR) targets while using a minimum total transmission power. In our
previous work, we formulated the joint control problem as a sparse
$\ell_0$-minimization problem and relaxed it to a $\ell_1$-minimization
problem. In this work, we propose to approximate the $\ell_0$-optimization
problem to a p norm minimization problem where $0<p<1$, since intuitively p
norm will approximate 0 norm better than 1 norm. We first show that the
$\ell_p$-minimization problem is strongly NP-hard and then derive a
reformulation of it such that the well developed interior-point algorithms can
be applied to solve it. The solution to the $\ell_p$-minimization problem can
efficiently guide the link's removals (deflation). Numerical simulations show
the proposed heuristic outperforms the existing algorithms.",False,False,While both papers address complex systems—financial markets and network optimization—their concepts do not easily converge into a novel multidisciplinary idea that meets all standards. The integration of stochastic market behavior with power control in networks lacks a clear experimental validation pathway and practical application.,
neg-1s-17,1,,2002.11954v1,"Energy-Efficient Buffer-Aided Relaying Systems with Opportunistic
  Spectrum Access; In this paper, an energy-efficient cross-layer design framework is proposed
for cooperative relaying networks, which takes into account the influence of
spectrum utilization probability. Specifically, random arrival traffic is
considered and an adaptive modulation and coding (AMC) scheme is adopted in the
cooperative transmission system to improve the system performance. The average
packet dropping rate of the relay-buffer is studied at first. With the packet
dropping rate and stationary distribution of the system state, the closed-form
expression of the delay is derived. Then the energy efficiency for
relay-assisted transmission is investigated, which takes into account the
queueing process of the relay and the source. In this context, an energy
efficiency optimization problem is formulated to determine the optimum strategy
of power and time allocation for the relay-assisted cooperative system.
Finally, the energy efficient switching strategy between the relay assisted
transmission and the direct transmission is obtained, where packet
transmissions have different delay requirements. In addition, energy efficient
transmission policy with AMC is obtained. Numerical results demonstrate the
effectiveness of the proposed design improving the energy efficiency.",2207.14294v2,"Knowledge-Driven Mechanistic Enrichment of the Preeclampsia Ignorome; Preeclampsia is a leading cause of maternal and fetal morbidity and
mortality. Currently, the only definitive treatment of preeclampsia is delivery
of the placenta, which is central to the pathogenesis of the disease.
Transcriptional profiling of human placenta from pregnancies complicated by
preeclampsia has been extensively performed to identify differentially
expressed genes (DEGs). The decisions to investigate DEGs experimentally are
biased by many factors, causing many DEGs to remain uninvestigated. A set of
DEGs which are associated with a disease experimentally, but which have no
known association to the disease in the literature are known as the ignorome.
Preeclampsia has an extensive body of scientific literature, a large pool of
DEG data, and only one definitive treatment. Tools facilitating knowledge-based
analyses, which are capable of combining disparate data from many sources in
order to suggest underlying mechanisms of action, may be a valuable resource to
support discovery and improve our understanding of this disease. In this work
we demonstrate how a biomedical knowledge graph (KG) can be used to identify
novel preeclampsia molecular mechanisms. Existing open source biomedical
resources and publicly available high-throughput transcriptional profiling data
were used to identify and annotate the function of currently uninvestigated
preeclampsia-associated DEGs. Experimentally investigated genes associated with
preeclampsia were identified from PubMed abstracts using text-mining
methodologies. The relative complement of the text-mined- and
meta-analysis-derived lists were identified as the uninvestigated
preeclampsia-associated DEGs (n=445), i.e., the preeclampsia ignorome. Using
the KG to investigate relevant DEGs revealed 53 novel clinically relevant and
biologically actionable mechanistic associations.",False,False,"The concepts from the two papers do not align sufficiently to create a feasible multidisciplinary research idea. While one focuses on energy-efficient communication systems and the other on preeclampsia mechanisms, their integration lacks a clear experimental validation pathway and practical application.",
neg-1s-18,1,,2410.22179v1,"Very Attentive Tacotron: Robust and Unbounded Length Generalization in
  Autoregressive Transformer-Based Text-to-Speech; Autoregressive (AR) Transformer-based sequence models are known to have
difficulty generalizing to sequences longer than those seen during training.
When applied to text-to-speech (TTS), these models tend to drop or repeat words
or produce erratic output, especially for longer utterances. In this paper, we
introduce enhancements aimed at AR Transformer-based encoder-decoder TTS
systems that address these robustness and length generalization issues. Our
approach uses an alignment mechanism to provide cross-attention operations with
relative location information. The associated alignment position is learned as
a latent property of the model via backprop and requires no external alignment
information during training. While the approach is tailored to the monotonic
nature of TTS input-output alignment, it is still able to benefit from the
flexible modeling power of interleaved multi-head self- and cross-attention
operations. A system incorporating these improvements, which we call Very
Attentive Tacotron, matches the naturalness and expressiveness of a baseline
T5-based TTS system, while eliminating problems with repeated or dropped words
and enabling generalization to any practical utterance length.",2406.12721v2,"Sound event detection based on auxiliary decoder and maximum probability
  aggregation for DCASE Challenge 2024 Task 4; In this report, we propose three novel methods for developing a sound event
detection (SED) model for the DCASE 2024 Challenge Task 4. First, we propose an
auxiliary decoder attached to the final convolutional block to improve feature
extraction capabilities while reducing dependency on embeddings from
pre-trained large models. The proposed auxiliary decoder operates independently
from the main decoder, enhancing performance of the convolutional block during
the initial training stages by assigning a different weight strategy between
main and auxiliary decoder losses. Next, to address the time interval issue
between the DESED and MAESTRO datasets, we propose maximum probability
aggregation (MPA) during the training step. The proposed MPA method enables the
model's output to be aligned with soft labels of 1 s in the MAESTRO dataset.
Finally, we propose a multi-channel input feature that employs various versions
of logmel and MFCC features to generate time-frequency pattern. The
experimental results demonstrate the efficacy of these proposed methods in a
view of improving SED performance by achieving a balanced enhancement across
different datasets and label types. Ultimately, this approach presents a
significant step forward in developing more robust and flexible SED models",False,True,"The combination of the Very Attentive Tacotron's robust TTS enhancements with the auxiliary decoder and maximum probability aggregation from the sound event detection paper can lead to a novel research idea focused on improving TTS systems' ability to detect and respond to sound events in real-time, enhancing interactivity and user experience.",
neg-1s-19,1,,2108.02328v1,"A Distributed Application Placement and Migration Management Techniques
  for Edge and Fog Computing Environments; Fog/Edge computing model allows harnessing of resources in the proximity of
the Internet of Things (IoT) devices to support various types of real-time IoT
applications. However, due to the mobility of users and a wide range of IoT
applications with different requirements, it is a challenging issue to satisfy
these applications' requirements. The execution of IoT applications exclusively
on one fog/edge server may not be always feasible due to limited resources,
while execution of IoT applications on different servers needs further
collaboration among servers. Also, considering user mobility, some modules of
each IoT application may require migration to other servers for execution,
leading to service interruption and extra execution costs. In this article, we
propose a new weighted cost model for hierarchical fog computing environments,
in terms of the response time of IoT applications and energy consumption of IoT
devices, to minimize the cost of running IoT applications and potential
migrations. Besides, a distributed clustering technique is proposed to enable
the collaborative execution of tasks, emitted from application modules, among
servers. Also, we propose an application placement technique to minimize the
overall cost of executing IoT applications on multiple servers in a distributed
manner. Furthermore, a distributed migration management technique is proposed
for the potential migration of applications' modules to other remote servers as
the users move along their path. Besides, failure recovery methods are embedded
in the clustering, application placement, and migration management techniques
to recover from unpredicted failures. The performance results show that our
technique significantly improves its counterparts in terms of placement
deployment time, average execution cost of tasks, total number of migrations,
total number of interrupted tasks, and cumulative migration cost.",1403.0627v1,"Exchange Rate Predictability in a Changing World; An expanding literature articulates the view that Taylor rules are helpful in
predicting exchange rates. In a changing world however, Taylor rule parameters
may be subject to structural instabilities, for example during the Global
Financial Crisis. This paper forecasts exchange rates using such Taylor rules
with Time Varying Parameters (TVP) estimated by Bayesian methods. In core
out-of-sample results, we improve upon a random walk benchmark for at least
half, and for as many as eight out of ten, of the currencies considered. This
contrasts with a constant parameter Taylor rule model that yields a more
limited improvement upon the benchmark. In further results, Purchasing Power
Parity and Uncovered Interest Rate Parity TVP models beat a random walk
benchmark, implying our methods have some generality in exchange rate
prediction.",False,False,"The concepts from the two papers do not align in a way that creates a feasible, novel, and useful multidisciplinary research idea. While one focuses on IoT application management in edge computing, the other deals with economic models for exchange rate prediction, lacking a clear intersection for practical experimentation.",
neg-1s-20,1,,2002.04402v1,"Defining mass transfer in a capillary wave micro-bioreactor; For high-throughput cell culture and associated analytics, droplet-based
cultivation systems open up the opportunities for parallelization and rapid
data generation. In contrast to microfluidics with continuous flow, sessile
droplet approaches enhance the flexibility for fluid manipulation with usually
less operational effort. Generating biologically favorable conditions and
promoting cell growth in a droplet, however, is particularly challenging due to
mass transfer limitations, which has to be solved by implementing an effective
mixing technique. Here, capillary waves induced by vertical oscillation are
used to mix inside a sessile droplet micro-bioreactor (MBR) system avoiding
additional moving parts inside the fluid. Depending on the excitation
frequency, different patterns are formed on the oscillating liquid surface,
which are described by a model of a vibrated sessile droplet. Analyzing mixing
times and oxygen transport into the liquid, a strong dependency of mass
transfer on the oscillation parameters, especially the excitation frequency, is
demonstrated. Oscillations at distinct capillary wave resonant frequencies lead
to rapid homogenization with mixing times of 2 s and volumetric liquid-phase
mass transfer coefficients of more than 340 h-1. This shows that the mass
transfer in a droplet MBR can be specifically controlled via capillary waves,
what is subsequently demonstrated for cultivations of Escherichia coli BL21
cells. Therefore, the presented MBR in combination with vertical oscillation
mixing for intensified mass transfer is a promising tool for highly parallel
cultivation and data generation.",2210.16642v1,"Unifying the Discrete and Continuous Emotion labels for Speech Emotion
  Recognition; Traditionally, in paralinguistic analysis for emotion detection from speech,
emotions have been identified with discrete or dimensional (continuous-valued)
labels. Accordingly, models that have been proposed for emotion detection use
one or the other of these label types. However, psychologists like Russell and
Plutchik have proposed theories and models that unite these views, maintaining
that these representations have shared and complementary information. This
paper is an attempt to validate these viewpoints computationally. To this end,
we propose a model to jointly predict continuous and discrete emotional
attributes and show how the relationship between these can be utilized to
improve the robustness and performance of emotion recognition tasks. Our
approach comprises multi-task and hierarchical multi-task learning frameworks
that jointly model the relationships between continuous-valued and discrete
emotion labels. Experimental results on two widely used datasets (IEMOCAP and
MSPPodcast) for speech-based emotion recognition show that our model results in
statistically significant improvements in performance over strong baselines
with non-unified approaches. We also demonstrate that using one type of label
(discrete or continuous-valued) for training improves recognition performance
in tasks that use the other type of label. Experimental results and reasoning
for this approach (called the mismatched training approach) are also presented.",False,True,"By integrating the capillary wave micro-bioreactor's mass transfer techniques with emotion recognition models, a novel research idea could explore how environmental factors (like fluid dynamics) influence biological responses (like cell behavior) in relation to emotional states, potentially leading to innovative applications in bioengineering and affective computing.",
neg-1s-21,1,,1410.3853v1,"Peer assessment enhances student learning; Feedback has a powerful influence on learning, but it is also expensive to
provide. In large classes, it may even be impossible for instructors to provide
individualized feedback. Peer assessment has received attention lately as a way
of providing personalized feedback that scales to large classes. Besides these
obvious benefits, some researchers have also conjectured that students learn by
peer assessing, although no studies have ever conclusively demonstrated this
effect. By conducting a randomized controlled trial in an introductory
statistics class, we provide evidence that peer assessment causes significant
gains in student achievement. The strength of our conclusions depends
critically on the careful design of the experiment, which was made possible by
a web-based platform that we developed. Hence, our study is also a proof of
concept of the high-quality experiments that are possible with online tools.",1905.01862v2,"Finitely generated abelian groups of units; In 1960 Fuchs posed the problem of characterizing the groups which are the
groups of units of commutative rings. In the following years, some partial
answers have been given to this question in particular cases. In this paper we
address Fuchs' question for {\it finitely generated abelian} groups and we
consider the problem of characterizing those groups which arise in some fixed
classes of rings $\mathcal C$, namely the integral domains, the torsion free
rings and the reduced rings. To determine the realizable groups we have to
establish what finite abelian groups $T$ (up to isomorphism) occur as torsion
subgroup of $A^*$ when $A$ varies in $\mathcal C$, and on the other hand, we
have to determine what are the possible values of the rank of $A^*$ when
$(A^*)_{tors}\cong T$. Most of the paper is devoted to the study of the class
of torsion-free rings, which needs a substantially deeper study.",False,False,"The concepts from the two papers are too disparate; one focuses on educational methodologies while the other deals with abstract algebra. There is no clear, feasible way to combine these ideas into a novel research project that meets all the specified standards.",
neg-1s-22,1,,0909.1154v1,"A note on the Lindeberg condition for convergence to stable laws in
  Mallows distance; We correct a condition in a result of Johnson and Samworth (Bernoulli 11
(2005) 829--845) concerning convergence to stable laws in Mallows distance. We
also give an improved version of this result, setting it in the more familiar
context of a Lindeberg-like condition.",1806.00646v3,"Osmosis through a Semi-permeable Membrane: a Consistent Approach to
  Interactions; The movement of ionic solutions is an essential part of biology and
technology. Fluidics, from nano- to micro- to microfluidics, is a burgeoning
area of technology which is all about the movement of ionic solutions, on
various scales. Many cells, tissues, and organs of animals and plants depend on
osmosis, as the movement of fluids is called in biology. Indeed, the movement
of fluids through channel proteins (that have a hole down their middle) is
fluidics on an atomic scale. Ionic fluids are complex fluids, with energy
stored in many ways. Ionic fluids flow driven by gradients of concentration,
chemical and electrical potential, and hydrostatic pressure. Each flow is
classically described by its own field theory, independent of the others, but
of course, in reality every gradient drives every kind of flow to a varying
extent. Combining field equations is tricky and so the theory of complex fluids
derives the equations, rather than assumes their interactions. When field
equations are derived, rather than assumed, their variables are consistent.
That is to say all variables satisfy all equations under all conditions with
one set of parameters. Here we treat a classical osmotic cell in this spirit,
using a sharp interface method to derive boundary conditions consistent with
all flows and fields. We allow volume to change with concentration, since
changes of volume are a property of ionic solutions known to all who make them
in the laboratory. We consider flexible and inflexible membranes. We show how
to combine the energetics of the membrane with the energetics of the
surrounding complex fluids. The results seem general but need application to
specific situations of technological, biological and experimental importance
before the consequences of consistency can be understood.",False,False,"The concepts from the two papers do not converge in a way that creates a novel multidisciplinary research idea. The first paper focuses on statistical convergence, while the second deals with fluid dynamics and osmosis. Their integration does not yield a feasible or useful research direction.",
neg-1s-23,1,,1804.03758v1,"Universal Successor Representations for Transfer Reinforcement Learning; The objective of transfer reinforcement learning is to generalize from a set
of previous tasks to unseen new tasks. In this work, we focus on the transfer
scenario where the dynamics among tasks are the same, but their goals differ.
Although general value function (Sutton et al., 2011) has been shown to be
useful for knowledge transfer, learning a universal value function can be
challenging in practice. To attack this, we propose (1) to use universal
successor representations (USR) to represent the transferable knowledge and (2)
a USR approximator (USRA) that can be trained by interacting with the
environment. Our experiments show that USR can be effectively applied to new
tasks, and the agent initialized by the trained USRA can achieve the goal
considerably faster than random initialization.",2407.02548v1,"Speed-accuracy tradeoff and its effect in the game of cricket:
  predictive modeling from statistical mechanics perspective; The speed-accuracy tradeoffs are prevalent in a wide range of physical
systems. In this paper, we demonstrate speed-accuracy tradeoffs in the game of
cricket, where 'batters' score runs on the balls bowled by the 'bowlers'. It is
shown that the run scoring rate by a batter and the probability of dismissal
follow a power-law relation. Due to availability of extensive data, game of
cricket is an excellent model for the study of the effect of speed-accuracy
tradeoff on the overall performance of the system. It is shown that the
exponent of the power-law governs the nature of the adaptability of the player
in different conditions and can be used for their assessment. Further, it is
demonstrated that the players with extreme values of the power-law exponent are
better suited for different playing conditions as compared to the ones with
moderate values. These findings can be utilized to identify the potential of
the cricket players for different game formats and can further help team
management in devising strategies for the best outcomes with a given set of
players.",False,True,"The combination of universal successor representations from reinforcement learning and the speed-accuracy tradeoff in cricket can lead to a novel research idea that models player adaptability and performance optimization in real-time game scenarios, which is both feasible and useful for sports analytics.",
neg-1s-24,1,,1906.02876v5,"Compressing RNNs for IoT devices by 15-38x using Kronecker Products; Recurrent Neural Networks (RNN) can be difficult to deploy on resource
constrained devices due to their size.As a result, there is a need for
compression techniques that can significantly compress RNNs without negatively
impacting task accuracy. This paper introduces a method to compress RNNs for
resource constrained environments using Kronecker product (KP). KPs can
compress RNN layers by 15-38x with minimal accuracy loss. By quantizing the
resulting models to 8-bits, we further push the compression factor to 50x. We
show that KP can beat the task accuracy achieved by other state-of-the-art
compression techniques across 5 benchmarks spanning 3 different applications,
while simultaneously improving inference run-time. We show that the KP
compression mechanism does introduce an accuracy loss, which can be mitigated
by a proposed hybrid KP (HKP) approach. Our HKP algorithm provides fine-grained
control over the compression ratio, enabling us to regain accuracy lost during
compression by adding a small number of model parameters.",0803.3330v2,"Matrix genetics, part 2: the degeneracy of the genetic code and the
  octave algebra with two quasi-real units (the genetic octave
  Yin-Yang-algebra); Algebraic properties of the genetic code are analyzed. The investigations of
the genetic code on the basis of matrix approaches (""matrix genetics"") are
described. The degeneracy of the vertebrate mitochondria genetic code is
reflected in the black-and-white mosaic of the (8*8)-matrix of 64 triplets, 20
amino acids and stop-signals. This mosaic genetic matrix is connected with the
matrix form of presentation of the special 8-dimensional Yin-Yang-algebra and
of its particular 4-dimensional case. The special algorithm, which is based on
features of genetic molecules, exists to transform the mosaic genomatrix into
the matrices of these algebras. Two new numeric systems are defined by these
8-dimensional and 4-dimensional algebras: genetic Yin-Yang-octaves and genetic
tetrions. Their comparison with quaternions by Hamilton is presented. Elements
of new ""genovector calculation"" and ideas of ""genetic mechanics"" are discussed.
These algebras are considered as models of the genetic code and as its possible
pre-code basis. They are related with binary oppositions of the Yin-Yang type
and they give new opportunities to investigate evolution of the genetic code.
The revealed fact of the relation between the genetic code and these genetic
algebras is discussed in connection with the idea by Pythagoras: ""All things
are numbers"". Simultaneously these genetic algebras can be utilized as the
algebras of genetic operators in biological organisms. The described results
are related with the problem of algebraization of bioinformatics. They take
attention to the question: what is life from the viewpoint of algebra?",False,False,"While both papers explore complex mathematical concepts, the integration of RNN compression techniques with genetic algebra does not present a clear, practical application that meets the standards of feasibility, novelty, and usefulness in addressing a specific problem.",
neg-1s-25,1,,2208.01131v2,"Automorphisms of real semisimple Lie algebras and their restricted root
  systems; We prove that every automorphism of the restricted root system of a real
semisimple Lie algebra -- when defined properly -- can be lifted to an
automorphism of that Lie algebra. In particular, this can be applied to
automorphisms of the Dynkin diagram of the restricted root system. We also
discuss some applications of this result to the theory of symmetric spaces of
noncompact type.",2401.06172v1,"CRISIS ALERT:Forecasting Stock Market Crisis Events Using Machine
  Learning Methods; Historically, the economic recession often came abruptly and disastrously.
For instance, during the 2008 financial crisis, the SP 500 fell 46 percent from
October 2007 to March 2009. If we could detect the signals of the crisis
earlier, we could have taken preventive measures. Therefore, driven by such
motivation, we use advanced machine learning techniques, including Random
Forest and Extreme Gradient Boosting, to predict any potential market crashes
mainly in the US market. Also, we would like to compare the performance of
these methods and examine which model is better for forecasting US stock market
crashes. We apply our models on the daily financial market data, which tend to
be more responsive with higher reporting frequencies. We consider 75
explanatory variables, including general US stock market indexes, SP 500 sector
indexes, as well as market indicators that can be used for the purpose of
crisis prediction. Finally, we conclude, with selected classification metrics,
that the Extreme Gradient Boosting method performs the best in predicting US
stock market crisis events.",False,False,"The concepts from the two papers are too disparate; one focuses on mathematical structures in Lie algebras while the other deals with machine learning for stock market predictions. There is no clear, feasible, and novel intersection that addresses a specific problem effectively.",
neg-1s-26,1,,1908.01479v1,"Imaging with highly incomplete and corrupted data; We consider the problem of imaging sparse scenes from a few noisy data using
an $l_1$-minimization approach. This problem can be cast as a linear system of
the form $A \, \rho =b$, where $A$ is an $N\times K$ measurement matrix. We
assume that the dimension of the unknown sparse vector $\rho \in
{\mathbb{C}}^K$ is much larger than the dimension of the data vector $b \in
{\mathbb{C}}^N$, i.e, $K \gg N$. We provide a theoretical framework that allows
us to examine under what conditions the $\ell_1$-minimization problem admits a
solution that is close to the exact one in the presence of noise. Our analysis
shows that $l_1$-minimization is not robust for imaging with noisy data when
high resolution is required. To improve the performance of $l_1$-minimization
we propose to solve instead the augmented linear system $ [A \, | \, C] \rho
=b$, where the $N \times \Sigma$ matrix $C$ is a noise collector. It is
constructed so as its column vectors provide a frame on which the noise of the
data, a vector of dimension $N$, can be well approximated. Theoretically, the
dimension $\Sigma$ of the noise collector should be $e^N$ which would make its
use not practical. However, our numerical results illustrate that robust
results in the presence of noise can be obtained with a large enough number of
columns $\Sigma \approx 10 K$.",1907.09103v1,"A Unified Algebraic Framework for Non-Monotonicity; Tremendous research effort has been dedicated over the years to thoroughly
investigate non-monotonic reasoning. With the abundance of non-monotonic
logical formalisms, a unified theory that enables comparing the different
approaches is much called for. In this paper, we present an algebraic graded
logic we refer to as LogAG capable of encompassing a wide variety of
non-monotonic formalisms. We build on Lin and Shoham's argument systems first
developed to formalize non-monotonic commonsense reasoning. We show how to
encode argument systems as LogAG theories, and prove that LogAG captures the
notion of belief spaces in argument systems. Since argument systems capture
default logic, autoepistemic logic, the principle of negation as failure, and
circumscription, our results show that LogAG captures the before-mentioned
non-monotonic logical formalisms as well. Previous results show that LogAG
subsumes possibilistic logic and any non-monotonic inference relation
satisfying Makinson's rationality postulates. In this way, LogAG provides a
powerful unified framework for non-monotonicity.",False,False,"The concepts from the two papers do not align in a way that allows for a feasible and novel multidisciplinary research idea. The first paper focuses on imaging and noise reduction, while the second deals with non-monotonic reasoning and logic frameworks, making integration challenging and impractical.",
neg-1s-27,1,,1710.09832v1,"Effective chemical potential for non-equilibrium systems and its
  application to molecular beam epitaxy of Bi2Se3; First-principles studies often rely on the assumption of equilibrium, which
can be a poor approximation, e.g., for growth. Here, an effective chemical
potential method for non-equilibrium systems is developed. A salient feature of
the theory is that it maintains the equilibrium limits as the correct limit. In
application to molecular beam epitaxy, rate equations are solved for the
concentrations of small clusters, which serve as feedstock for growth. We find
that the effective chemical potential is determined by the most probable,
rather than by the lowest-energy, cluster. In the case of Bi2Se3, the chemical
potential is found to be highly supersaturated, leading to a high nucleus
concentration in agreement with experiment.",physics/0608018v3,"The dynamics of traded value revisited; We conclude from an analysis of high resolution NYSE data that the
distribution of the traded value $f_i$ (or volume) has a finite variance
$\sigma_i$ for the very large majority of stocks $i$, and the distribution
itself is non-universal across stocks. The Hurst exponent of the same time
series displays a crossover from weakly to strongly correlated behavior around
the time scale of 1 day. The persistence in the strongly correlated regime
increases with the average trading activity $\ev{f_i}$ as
$H_i=H_0+\gamma\log\ev{f_i}$, which is another sign of non-universal behavior.
The existence of such liquidity dependent correlations is consistent with the
empirical observation that $\sigma_i\propto\ev{f_i}^\alpha$, where $\alpha$ is
a non-trivial, time scale dependent exponent.",False,False,"The concepts from the two papers, one focused on non-equilibrium chemical potentials in material growth and the other on trading dynamics in financial markets, do not converge in a way that creates a novel, feasible, and useful multidisciplinary research idea. The domains are too disparate.",
neg-1s-28,1,,1010.3602v1,"Collisions of particles in locally AdS spacetimes I. Local description
  and global examples; We investigate 3-dimensional globally hyperbolic AdS manifolds containing
""particles"", i.e., cone singularities along a graph $\Gamma$. We impose
physically relevant conditions on the cone singularities, e.g. positivity of
mass (angle less than $2\pi$ on time-like singular segments). We construct
examples of such manifolds, describe the cone singularities that can arise and
the way they can interact (the local geometry near the vertices of $\Gamma$).
We then adapt to this setting some notions like global hyperbolicity which are
natural for Lorentz manifolds, and construct some examples of globally
hyperbolic AdS manifolds with interacting particles.",0908.1677v1,"Most Efficient Homogeneous Volatility Estimators; We present a comprehensive theory of homogeneous volatility (and variance)
estimators of arbitrary stochastic processes that fully exploit the OHLC (open,
high, low, close) prices. For this, we develop the theory of most efficient
point-wise homogeneous OHLC volatility estimators, valid for any price
processes. We introduce the ""quasi-unbiased estimators"", that can address any
type of desirable constraints. The main tool of our theory is the parsimonious
encoding of all the information contained in the OHLC prices for a given time
interval in the form of the joint distributions of the high-minus-open,
low-minus-open and close-minus-open values, whose analytical expression is
derived exactly for Wiener processes with drift. The distributions can be
calculated to yield the most efficient estimators associated with any
statistical properties of the underlying log-price stochastic process. Applied
to Wiener processes for log-prices with drift, we provide explicit analytical
expressions for the most efficient point-wise volatility and variance
estimators, based on the analytical expression of the joint distribution of the
high-minus-open, low-minus-open and close-minus-open values. The efficiency of
the new proposed estimators is favorably compared with that of the
Garman-Klass, Roger-Satchell and maximum likelihood estimators.",False,False,The concepts from the two papers—geometric properties of AdS manifolds and volatility estimators for financial processes—do not lend themselves to a coherent multidisciplinary idea that meets all standards. The connection between particle interactions and financial volatility lacks a clear experimental validation or practical application.,
neg-1s-29,1,,2104.08010v1,"Welfare Measure for Resource Allocation with Algorithmic Implementation:
  Beyond Average and Max-Min; In this work, we propose an axiomatic approach for measuring the
performance/welfare of a system consisting of concurrent agents in a
resource-driven system. Our approach provides a unifying view on popular system
optimality principles, such as the maximal average/total utilities and the
max-min fairness. Moreover, it gives rise to other system optimality notions
that have not been fully exploited yet, such as the maximal lowest total
subgroup utilities. For the axiomatically defined welfare measures, we provide
a generic gradient-based method to find an optimal resource allocation and
present a theoretical guarantee for its success. Lastly, we demonstrate the
power of our approach through the power control application in wireless
networks.",1909.11910v1,"Concentration of product spaces; We investigate the relation between the concentration and the product of
metric measure spaces. We have the natural question whether, for two
concentrating sequences of metric measure spaces, the sequence of their product
spaces also concentrates. A partial answer is mentioned in Gromov's book. We
obtain a complete answer for this question.",False,False,"While both papers address complex systems, the concepts from resource allocation and metric measure spaces do not easily converge into a novel, feasible research idea. The lack of a clear application or experimental validation limits the potential for a multidisciplinary approach that meets all standards.",
neg-1s-30,1,,2101.01548v1,"Extraction, isolation, structure elucidation and evaluation of toxicity,
  anti-inflammatory and analgesic activity of Pituranthos scoparius
  constituents; The present work aimed to investigate an ethnobotanical survey about
Pituranthos scoparius and assess the toxicity, anti-inflammatory (in vitro, and
in vivo) potential, in vitro antioxidant, and analgesic effects of stems and
roots of Pituranthos scoparius. Furthermore; to isolate and elucidate the
chemical constituents of the n-butanol stem extract of P. scoparius (ButE) and
determine the toxicity and anti-inflammatory effects of these compounds added
to the ButE. Data from an ethnopharmacological study showed that 24.47 % of
people used this plant in folk medicine. Four compounds were isolated from
ButE. These compounds were characterized by means of NMR and high-resolution
mass spectral (HRMS) data.",2211.07035v1,"Elementary Bitcoin economics: from production and transaction demand to
  values; In this paper we give an elementary analysis of economics of Bitcoin that
combines the transaction demand by the consumers and the supply of hashrate by
miners. We argue that the decreasing block reward will have no significant
effect on the exchange rate (price) of Bitcoin and thus the network will be
transitioning to a regime where transaction fees will play a bigger part of
miners' revenue. We consider a simple model where consumers demand bitcoins for
transactions, but not for hoarding bitcoins, and we analyze market equilibrium
where the demand is matched with the hashrate supplied by miners. Our main
conclusion is that the exchange rate of Bitcoin cannot be determined from the
market equilibrium and so our arguments support the hypothesis that Bitcoin
price has no economic fundamentals and is free to fluctuate according to the
present demand for hoarding and speculation. We point out that increasing fees
bear the risk of Bitcoin being outcompeted by its main rival Ethereum, and that
decreasing revenues to miners depreciate the perception of Bitcoin as a medium
for store value (hoarding demand) which will have effect its exchange rate.",False,False,"The concepts from the two papers—ethnobotanical research on Pituranthos scoparius and Bitcoin economics—do not lend themselves to a feasible multidisciplinary idea that meets all standards. The fields are too disparate, lacking a clear intersection for practical application or experimental validation.",
neg-1s-31,1,,1810.09063v3,"Optimal electricity demand response contracting with responsiveness
  incentives; Despite the success of demand response programs in retail electricity markets
in reducing average consumption, the random responsiveness of consumers to
price event makes their efficiency questionable to achieve the flexibility
needed for electric systems with a large share of renewable energy. The
variance of consumers' responses depreciates the value of these mechanisms and
makes them weakly reliable. This paper aims at designing demand response
contracts which allow to act on both the average consumption and its variance.
The interaction between a risk--averse producer and a risk--averse consumer is
modelled through a Principal--Agent problem, thus accounting for the moral
hazard underlying demand response contracts. We provide closed--form solution
for the optimal contract in the case of constant marginal costs of energy and
volatility for the producer and constant marginal value of energy for the
consumer. We show that the optimal contract has a rebate form where the initial
condition of the consumption serves as a baseline. Further, the consumer cannot
manipulate the baseline at his own advantage. The second--best price for energy
and volatility are non--constant and non--increasing in time. The price for
energy is lower (resp. higher) than the marginal cost of energy during
peak--load (resp. off--peak) periods. We illustrate the potential benefit
issued from the implementation of an incentive mechanism on the responsiveness
of the consumer by calibrating our model with publicly available data. We
predict a significant increase of responsiveness under our optimal contract and
a significant increase of the producer satisfaction.",0705.4073v2,"Quasi-linear dynamics in nonlinear Schr\"" odinger equation with periodic
  boundary conditions; It is shown that a large subset of initial data with finite energy ($L^2$
norm)evolves nearly linearly in nonlinear Schr\"" odinger equation with periodic
boundary conditions. These new solutions are not perturbations of the known
ones such as solitons, semiclassical or weakly linear solutions.",False,False,"The concepts from the two papers do not align in a way that creates a feasible, novel, and useful multidisciplinary research idea. The first paper focuses on economic modeling in energy markets, while the second deals with mathematical solutions in quantum mechanics, lacking a clear intersection for practical application.",
neg-1s-32,1,,1210.2363v2,"LDx: estimation of linkage disequilibrium from high-throughput pooled
  resequencing data; High-throughput pooled resequencing offers significant potential for whole
genome population sequencing. However, its main drawback is the loss of
haplotype information. In order to regain some of this information, we present
LDx, a computational tool for estimating linkage disequilibrium (LD) from
pooled resequencing data. LDx uses an approximate maximum likelihood approach
to estimate LD (r2) between pairs of SNPs that can be observed within and among
single reads. LDx also reports r2 estimates derived solely from observed
genotype counts. We demonstrate that the LDx estimates are highly correlated
with r2 estimated from individually resequenced strains. We discuss the
performance of LDx using more stringent quality conditions and infer via
simulation the degree to which performance can improve based on read depth.
Finally we demonstrate two possible uses of LDx with real and simulated pooled
resequencing data. First, we use LDx to infer genomewide patterns of decay of
LD with physical distance in D. melanogaster population resequencing data.
Second, we demonstrate that r2 estimates from LDx are capable of distinguishing
alternative demographic models representing plausible demographic histories of
D. melanogaster.",1508.03817v1,"Visualizing long vectors of measurements by use of the Hilbert curve; The use of Hilbert curves to visualize massive vector of data is revisited
following previous authors. The Hilbert curve mapping preserves locality and
makes meaningful representation of the data. We call such visualization as
Hilbert plots. The combination of a Hilbert plot with its Fourier transform
allows to identify patterns in the underlying data sequence. The use of
different granularity representation also allows to identify periodic intervals
within the data. Data from different sources are presented: periodic,
aperiodic, logistic map and 1/2-Ising model. A real data example from the study
of heartbeat data is also discussed.",False,True,"By combining LDx's ability to estimate linkage disequilibrium with the Hilbert curve's visualization techniques, a novel approach could be developed to visualize genetic data patterns and demographic histories in a more interpretable manner, facilitating insights into population genetics.",
neg-1s-33,1,,2410.09503v1,"SLAM-AAC: Enhancing Audio Captioning with Paraphrasing Augmentation and
  CLAP-Refine through LLMs; Automated Audio Captioning (AAC) aims to generate natural textual
descriptions for input audio signals. Recent progress in audio pre-trained
models and large language models (LLMs) has significantly enhanced audio
understanding and textual reasoning capabilities, making improvements in AAC
possible. In this paper, we propose SLAM-AAC to further enhance AAC with
paraphrasing augmentation and CLAP-Refine through LLMs. Our approach uses the
self-supervised EAT model to extract fine-grained audio representations, which
are then aligned with textual embeddings via lightweight linear layers. The
caption generation LLM is efficiently fine-tuned using the LoRA adapter.
Drawing inspiration from the back-translation method in machine translation, we
implement paraphrasing augmentation to expand the Clotho dataset during
pre-training. This strategy helps alleviate the limitation of scarce audio-text
pairs and generates more diverse captions from a small set of audio clips.
During inference, we introduce the plug-and-play CLAP-Refine strategy to fully
exploit multiple decoding outputs, akin to the n-best rescoring strategy in
speech recognition. Using the CLAP model for audio-text similarity calculation,
we could select the textual descriptions generated by multiple searching beams
that best match the input audio. Experimental results show that SLAM-AAC
achieves state-of-the-art performance on Clotho V2 and AudioCaps, surpassing
previous mainstream models.",1706.04633v1,"Subjects classification from high-dimensional and small-sample size
  datasets using a strategy based on Clustering Variables around Latent
  Components (CLV) method; High-dimensional complex systems can be studied through multivariate
analysis, as Principal Component Analysis, however large samples of
observations frequently are needed for it. Here it is examined a method for
small samples based on clustering variables around latent variables (CLV) to
subject classification in two presumed groups. For it, a predictive model was
developed to generate datasets with two groups of cases whose variables show
randomness features (up to 30% of variables manifest difference between groups,
and up to 7% of those are correlated between them). The method recovered the
information of the latent factors to classify the subjects with 80 to 95% of
agreement, with positive relationship between the classifier precision and the
rate [number of variables / number of subjects].",False,True,"The combination of SLAM-AAC's audio captioning techniques with the CLV method for classifying audio signals based on latent variables can create a novel approach to enhance audio understanding in small sample datasets. This idea is feasible, innovative, and addresses the challenge of limited audio-text pairs effectively.",
neg-1s-34,1,,1402.0344v1,"Formes quadratiques de discriminants emboîtés; Quadratic forms with embedded discriminants. Integral binary quadratic forms
have multiple applications, for example in factorization or cryptography. The
Nice family of cryptographic systems makes use of quadratic forms with
different discriminants $\pm p$, and $\pm pq^2$ where $p$, $q$ are large
primes. This paper shows the precise links between forms with $D$ discriminant
and forms with $Df^2$ discriminant, which are crucial in the analysis of the
systems Nice and theirs attacks. We also introduce the notion of
semi-equivalence of binary quadratic forms, and give some characterizations of
semi-equivalent forms, which are useful in the analysis of these attacks.
  -----
  Les formes quadratiques binaires fournissent un moyen explicite pour
manipuler des id\'eaux de corps quadratiques, et leurs applications pratiques
sont multiples. De nombreux algorithmes de factorisation les utilisent. Elle
sont aussi utilis\'ees en cryptographie, en particulier pour les syst\`emes
Nice. Les syst\`emes de chiffrement Nice utilisent des formes quadratiques de
discriminants $\pm p$ et $\pm pq^2$ o\`u $p$ et $q$ sont des nombres premiers.
Cet article pr\'ecise les liens entre les formes de discriminant $D$ et celles
de discriminant $Df^2$, ce qui est essentiel pour l'analyse de Nice et de ses
attaques. Il introduit aussi la notion de formes quadratiques
semi-\'equivalentes et en explicite plusieurs caract\'erisations, utiles pour
l'analyse de ces attaques.",1902.06143v1,"Weak Identification and Estimation of Social Interaction Models; The identification of the network effect is based on either group size
variation, the structure of the network or the relative position in the
network. I provide easy-to-verify necessary conditions for identification of
undirected network models based on the number of distinct eigenvalues of the
adjacency matrix. Identification of network effects is possible; although in
many empirical situations existing identification strategies may require the
use of many instruments or instruments that could be strongly correlated with
each other. The use of highly correlated instruments or many instruments may
lead to weak identification or many instruments bias. This paper proposes
regularized versions of the two-stage least squares (2SLS) estimators as a
solution to these problems. The proposed estimators are consistent and
asymptotically normal. A Monte Carlo study illustrates the properties of the
regularized estimators. An empirical application, assessing a local government
tax competition model, shows the empirical relevance of using regularization
methods.",False,False,"While both papers address complex systems—one in cryptography and the other in social network modeling—their concepts do not easily converge into a novel, feasible research idea. The lack of a clear application that combines both fields effectively limits the potential for a multidisciplinary approach.",
neg-1s-35,1,,2105.09581v1,"Valuation of European Options under an Uncertain Market Price of
  Volatility Risk; We propose a model to quantify the effect of parameter uncertainty on the
option price in the Heston model. More precisely, we present a
Hamilton-Jacobi-Bellman framework which allows us to evaluate best and worst
case scenarios under an uncertain market price of volatility risk. For the
numerical approximation the Hamilton--Jacobi--Bellman equation is reformulated
to enable the solution with a finite element method. A case study with
butterfly options exhibits how the dependence of Delta on the magnitude of the
uncertainty is nonlinear and highly varied across the parameter regime.
  Keywords: Uncertain market price, Volatility risk, Hamilton-Jacobi-Bellman
equation, Finite element method, Uncertainty quantification",2205.00051v3,"Useful formulas for non-magnetized electron cooling; Recent success of Low Energy RHIC Electron Cooler (LEReC) opened a road for
development of high energy electron coolers based on non-magnetized electron
bunches accelerated by RF cavities. Electrons in such coolers can have velocity
distribution with various unequal horizontal, vertical and longitudinal
velocity spreads. In this paper we revisit a formula of friction force in
non-magnetized cooling and derive a number of useful expressions for different
limiting cases.",False,False,While both papers address complex systems—financial options and electron cooling—their concepts do not easily converge into a novel multidisciplinary idea that meets all standards. The lack of a clear application or experimental validation connecting the two fields limits feasibility and usefulness.,
neg-1s-36,1,,1810.07674v5,"Dynkin games with incomplete and asymmetric information; We study the value and the optimal strategies for a two-player zero-sum
optimal stopping game with incomplete and asymmetric information. In our
Bayesian set-up, the drift of the underlying diffusion process is unknown to
one player (incomplete information feature), but known to the other one
(asymmetric information feature). We formulate the problem and reduce it to a
fully Markovian setup where the uninformed player optimises over stopping times
and the informed one uses randomised stopping times in order to hide their
informational advantage. Then we provide a general verification result which
allows us to find the value of the game and players' optimal strategies by
solving suitable quasi-variational inequalities with some non-standard
constraints. Finally, we study an example with linear payoffs, in which an
explicit solution of the corresponding quasi-variational inequalities can be
obtained.",0711.0149v2,"Leibniz rules for enveloping algebras in symmetric ordering; Given a finite-dimensional Lie algebra, and a representation by derivations
on the completed symmetric algebra of its dual, a number of interesting twisted
constructions appear: certain twisted Weyl algebras, deformed Leibniz rules,
quantized ``star'' product. We first illuminate a number of interrelations
between these constructions and then proceed to study a special case in certain
precise sense corresponding to the symmetric or Weyl ordering. This case has
been known earlier to be related to computations with Hausdorff series, for
example the expression for the star product is in such terms. For the deformed
Leibniz rule, hence a coproduct, we present here a new nonsymmetric expression,
which is then expanded into a sum of expressions labelled by a class of planar
trees, and for a given tree evaluated by Feynman-like rules. These expressions
are filtered by a bidegree and we show recursion formulas for the sums of
expressions of a given bidegree, and compare the recursions to recursions for
Hausdorff series, including the comparison of initial conditions. This way we
show a direct corespondence between the Hausdorff series and the expression for
twisted coproduct.",False,False,"The concepts from the two papers, one focusing on game theory with incomplete information and the other on algebraic structures in mathematical physics, do not easily converge into a novel, feasible, and useful multidisciplinary research idea. The connection between them lacks practical applicability and experimental validation.",
neg-1s-37,1,,1809.00080v1,"Location and Capacity Planning of Facilities with General Service-Time
  Distributions Using Conic Optimization; This paper studies a stochastic congested location problem in the network of
a service system that consists of facilities to be established in a finite
number of candidate locations. Population zones allocated to each open service
facility together creates a stream of demand that follows a Poisson process and
may cause congestion at the facility. The service time at each facility is
stochastic and depends on the service capacity and follows a general
distribution that can differ for each facility. The service capacity is
selected from a given (bounded or unbounded) interval. The objective of our
problem is to optimize a balanced performance measure that compromises between
facility-induced and customer-related costs. Service times are represented by a
flexible location-scale stochastic model. The problem is formulated using
quadratic conic optimization. Valid inequalities and a cut-generation procedure
are developed to increase computational efficiency. A comprehensive numerical
study is carried out to show the efficiency and effectiveness of the solution
procedure. Moreover, our numerical experiments using real data of a preventive
healthcare system in Toronto show that the optimal service network
configuration is highly sensitive to the service-time distribution. Our method
for convexifying the waiting-time formulas of M/G/1 queues is general and
extends the existing convexity results in queueing theory such that they can be
used in optimization problems where the service rates are continuous.",2410.20680v1,"Multi-modal Data based Semi-Supervised Learning for Vehicle Positioning; In this paper, a multi-modal data based semi-supervised learning (SSL)
framework that jointly use channel state information (CSI) data and RGB images
for vehicle positioning is designed. In particular, an outdoor positioning
system where the vehicle locations are determined by a base station (BS) is
considered. The BS equipped with several cameras can collect a large amount of
unlabeled CSI data and a small number of labeled CSI data of vehicles, and the
images taken by cameras. Although the collected images contain partial
information of vehicles (i.e. azimuth angles of vehicles), the relationship
between the unlabeled CSI data and its azimuth angle, and the distances between
the BS and the vehicles captured by images are both unknown. Therefore, the
images cannot be directly used as the labels of unlabeled CSI data to train a
positioning model. To exploit unlabeled CSI data and images, a SSL framework
that consists of a pretraining stage and a downstream training stage is
proposed. In the pretraining stage, the azimuth angles obtained from the images
are considered as the labels of unlabeled CSI data to pretrain the positioning
model. In the downstream training stage, a small sized labeled dataset in which
the accurate vehicle positions are considered as labels is used to retrain the
model. Simulation results show that the proposed method can reduce the
positioning error by up to 30% compared to a baseline where the model is not
pretrained.",False,True,"By integrating the stochastic service-time optimization from Paper 1 with the semi-supervised learning framework for vehicle positioning in Paper 2, a novel research idea could emerge that optimizes service facility locations based on real-time vehicle positioning data, enhancing operational efficiency in service systems.",
neg-1s-38,1,,2004.09963v3,"Structural clustering of volatility regimes for dynamic trading
  strategies; We develop a new method to find the number of volatility regimes in a
nonstationary financial time series by applying unsupervised learning to its
volatility structure. We use change point detection to partition a time series
into locally stationary segments and then compute a distance matrix between
segment distributions. The segments are clustered into a learned number of
discrete volatility regimes via an optimization routine. Using this framework,
we determine a volatility clustering structure for financial indices, large-cap
equities, exchange-traded funds and currency pairs. Our method overcomes the
rigid assumptions necessary to implement many parametric regime-switching
models, while effectively distilling a time series into several characteristic
behaviours. Our results provide significant simplification of these time series
and a strong descriptive analysis of prior behaviours of volatility. Finally,
we create and validate a dynamic trading strategy that learns the optimal match
between the current distribution of a time series and its past regimes, thereby
making online risk-avoidance decisions in the present.",2401.00966v1,"Improved estimators in Bell regression model with application; In this paper, we propose the application of shrinkage strategies to estimate
coefficients in the Bell regression models when prior information about the
coefficients is available. The Bell regression models are well-suited for
modeling count data with multiple covariates. Furthermore, we provide a
detailed explanation of the asymptotic properties of the proposed estimators,
including asymptotic biases and mean squared errors. To assess the performance
of the estimators, we conduct numerical studies using Monte Carlo simulations
and evaluate their simulated relative efficiency. The results demonstrate that
the suggested estimators outperform the unrestricted estimator when prior
information is taken into account. Additionally, we present an empirical
application to demonstrate the practical utility of the suggested estimators.",False,True,"The combination of volatility regime clustering from Paper 1 and the shrinkage strategies in Bell regression from Paper 2 can lead to a novel approach for dynamic risk assessment in financial markets. This idea is feasible, as it can be empirically tested, and it addresses real-world financial decision-making challenges.",
neg-1s-39,1,,1902.08923v1,"Ideals in $B_1(X)$ and residue class rings of $B_1(X)$ modulo an ideal; This paper explores the duality between ideals of the ring $B_1(X)$ of all
real valued Baire one functions on a topological space $X$ and typical families
of zero sets, called $Z_B$-filters, on $X$. As a natural outcome of this study,
it is observed that $B_1(X)$ is a Gelfand ring but non-Noetherian in general.
Introducing fixed and free maximal ideals in the context of $B_1(X)$, complete
descriptions of the fixed maximal ideals of both $B_1(X)$ and $B_1^*(X)$ are
obtained. Though free maximal ideals of $B_1(X)$ and those of $B_1^*(X)$ do not
show any relationship in general, their counterparts, i.e., the fixed maximal
ideals obey natural relations. It is proved here that for a perfectly normal
$T_1$ space $X$, free maximal ideals of $B_1(X)$ are determined by a typical
class of Baire one functions. In the concluding part of this paper, we study
residue class ring of $B_1(X)$ modulo an ideal, with special emphasize on real
and hyper real maximal ideals of $B_1(X)$.",2212.03351v2,"The long-term effect of childhood exposure to technology using
  surrogates; We study how childhood exposure to technology at ages 5-15 via the occupation
of the parents affects the ability to climb the social ladder in terms of
income at ages 45-49 using the Danish micro data from years 1961-2019. Our
measure of technology exposure covers the degree to which using computers
(hardware and software) is required to perform an occupation, and it is created
by merging occupational codes with detailed data from O*NET. The challenge in
estimating this effect is that long-term outcome is observed over a different
time horizon than our treatment of interest. We therefore adapt the surrogate
index methodology, linking the effect of our childhood treatment on
intermediate surrogates, such as income and education at ages 25-29, to the
effect on adulthood income. We estimate that a one standard error increase in
exposure to technology increases the income rank by 2\%-points, which is
economically and statistically significant and robust to cluster-correlation
within families. The derived policy recommendation is to update the educational
curriculum to expose children to computers to a higher degree, which may then
act as a social leveler.",False,False,"The concepts from the two papers are too disparate; one focuses on mathematical structures in topology while the other examines social mobility influenced by technology exposure. There is no clear, feasible, and novel intersection that addresses a specific problem effectively.",
neg-1s-40,1,,2404.03455v3,"Synergy as the failure of distributivity; The concept of emergence, or synergy in its simplest form, is widely used but
lacks a rigorous definition. Our work connects information and set theory to
uncover the mathematical nature of synergy as the failure of distributivity. It
resolves the persistent self-contradiction of information decomposition theory
and reinstates it as a primary route toward a rigorous definition of emergence.
Our results suggest that non-distributive variants of set theory may be used to
describe emergent physical systems.",2002.11415v1,"Extensions, deformation and categorification of $\text{AssDer}$ pairs; In this paper, we consider associative algebras equipped with derivations.
Such a pair of an associative algebra with a derivation is called an AssDer
pair. Using the Hochschild cohomology for associative algebras, we define
cohomology for an AssDer pair with coefficients in a representation. We study
central extensions and abelian extensions of AssDer pairs. Moreover, we
consider extensions of a pair of derivations in central extensions of
associative algebras. Next, we study formal one-parameter deformations of
AssDer pair by deforming both the associative product and the derivation. They
are governed by the cohomology of the AssDer pair with representation in
itself. In the next part, we study $2$-term $A_\infty$-algebras with homotopy
derivations considered by Loday and Doubek-Lada. Finally, we introduce
$2$-derivations on associative $2$-algebras and show that the category of
associative $2$-algebras with $2$-derivations are equivalent to the category of
$2$-term $A_\infty$-algebras with homotopy derivations.",False,False,"While both papers explore complex mathematical concepts, they do not present a clear intersection that leads to a novel, feasible, and useful multidisciplinary research idea. The themes of synergy and associative algebras with derivations do not readily combine into a practical application or experimental validation.",
neg-1s-41,1,,2412.18566v2,"Zero-resource Speech Translation and Recognition with LLMs; Despite recent advancements in speech processing, zero-resource speech
translation (ST) and automatic speech recognition (ASR) remain challenging
problems. In this work, we propose to leverage a multilingual Large Language
Model (LLM) to perform ST and ASR in languages for which the model has never
seen paired audio-text data. We achieve this by using a pre-trained
multilingual speech encoder, a multilingual LLM, and a lightweight adaptation
module that maps the audio representations to the token embedding space of the
LLM. We perform several experiments both in ST and ASR to understand how to
best train the model and what data has the most impact on performance in
previously unseen languages. In ST, our best model is capable to achieve BLEU
scores over 23 in CoVoST2 for two previously unseen languages, while in ASR, we
achieve WERs of up to 28.2\%. We finally show that the performance of our
system is bounded by the ability of the LLM to output text in the desired
language.",2208.09372v3,"Non-Stationary Dynamic Pricing Via Actor-Critic Information-Directed
  Pricing; This paper presents a novel non-stationary dynamic pricing algorithm design,
where pricing agents face incomplete demand information and market environment
shifts. The agents run price experiments to learn about each product's demand
curve and the profit-maximizing price, while being aware of market environment
shifts to avoid high opportunity costs from offering sub-optimal prices. The
proposed ACIDP extends information-directed sampling (IDS) algorithms from
statistical machine learning to include microeconomic choice theory, with a
novel pricing strategy auditing procedure to escape sub-optimal pricing after
market environment shift. The proposed ACIDP outperforms competing bandit
algorithms including Upper Confidence Bound (UCB) and Thompson sampling (TS) in
a series of market environment shifts.",False,False,"While both papers present innovative concepts, the integration of zero-resource speech translation with dynamic pricing lacks a clear application or experimental validation. The combination does not yield a novel, feasible, and useful research idea that addresses a specific problem effectively.",
neg-1s-42,1,,2111.08786v1,"LibSC: Library for Scaling Correction Methods in Density Functional
  Theory; In recent years, a series of scaling correction (SC) methods have been
developed in the Yang laboratory to reduce and eliminate the delocalization
error, which is an intrinsic and systematic error existing in conventional
density functional approximations (DFAs) within density functional theory
(DFT). Based on extensive numerical results, the SC methods have been
demonstrated to be capable of reducing the delocalization error effectively and
producing accurate descriptions for many critical and challenging problems,
including the fundamental gap, photoemission spectroscopy, charge transfer
excitations and polarizability. In the development of SC methods, the SC
methods were mainly implemented in the QM4D package that was developed in the
Yang laboratory for research development. The heavy dependency on the QM4D
package hinders the SC methods from access by researchers for broad
applications. In this work, we developed a reliable and efficient
implementation , LibSC for the global scaling correction (GSC) method and the
localized orbital scaling correction (LOSC) method. LibSC will serve as a
light-weight and open-source library that can be easily accessed by the quantum
chemistry community. The implementation of LibSC is carefully modularized to
provide the essential functionalities for conducting calculations of the SC
methods. In addition, LibSC provides simple and consistent interfaces to
support multiple popular programing languages, including C, C++ and Python. In
addition to the development of the library, we also integrated LibSC with two
popular and open-source quantum chemistry packages, the Psi4 package and the
PySCF package, which provides immediate access for general users to perform
calculations with SC methods.",2210.05868v1,"On Secure Uplink Transmission in Hybrid RF-FSO Cooperative
  Satellite-Aerial-Terrestrial Networks; This work investigates the secrecy outage performance of the uplink
transmission of a radio-frequency (RF)-free-space optical (FSO) hybrid
cooperative satellite-aerial-terrestrial network (SATN). Specifically, in the
considered cooperative SATN, a terrestrial source (S) transmits its information
to a satellite receiver (D) via the help of a cache-enabled aerial relay (R)
terminal with the most popular content caching scheme, while a group of
eavesdropping aerial terminals (Eves) trying to overhear the transmitted
confidential information. Moreover, RF and FSO transmissions are employed over
S-R and R-D links, respectively. Considering the randomness of R, D, and Eves,
and employing a stochastic geometry framework, the secrecy outage performance
of the cooperative uplink transmission in the considered SATN is investigated
and a closed-form analytical expression for the end-to-end secrecy outage
probability is derived. Finally, Monte-Carlo simulations are shown to verify
the accuracy of our analysis.",False,False,"The concepts from the two papers—scaling correction methods in quantum chemistry and secure uplink transmission in hybrid networks—do not easily converge into a novel multidisciplinary idea that meets all standards. The fields are too distinct, lacking a clear application or experimental validation pathway that combines them effectively.",
neg-1s-43,1,,2007.02109v1,"Scenarios for a post-COVID-19 world airline network; The airline industry was severely hit by the COVID-19 crisis with an average
demand decrease of about $64\%$ (IATA, April 2020) which triggered already
several bankruptcies of airline companies all over the world. While the
robustness of the world airline network (WAN) was mostly studied as an
homogeneous network, we introduce a new tool for analyzing the impact of a
company failure: the `airline company network' where two airlines are connected
if they share at least one route segment. Using this tool, we observe that the
failure of companies well connected with others has the largest impact on the
connectivity of the WAN. We then explore how the global demand reduction
affects airlines differently, and provide an analysis of different scenarios if
its stays low and does not come back to its pre-crisis level. Using traffic
data from the Official Aviation Guide (OAG) and simple assumptions about
customer's airline choice strategies, we find that the local effective demand
can be much lower than the average one, especially for companies that are not
monopolistic and share their segments with larger companies. Even if the
average demand comes back to $60\%$ of the total capacity, we find that between
$46\%$ and $59\%$ of the companies could experience a reduction of more than
$50\%$ of their traffic, depending on the type of competitive advantage that
drives customer's airline choice. These results highlight how the complex
competitive structure of the WAN weakens its robustness when facing such a
large crisis.",2112.13111v1,"Measuring Quality of DNA Sequence Data via Degradation; We propose and apply a novel paradigm for characterization of genome data
quality, which quantifies the effects of intentional degradation of quality.
The rationale is that the higher the initial quality, the more fragile the
genome and the greater the effects of degradation. We demonstrate that this
phenomenon is ubiquitous, and that quantified measures of degradation can be
used for multiple purposes. We focus on identifying outliers that may be
problematic with respect to data quality, but might also be true anomalies or
even attempts to subvert the database.",False,False,"The concepts from the airline network analysis and DNA sequence data degradation do not intersect in a way that creates a feasible, novel, and useful multidisciplinary research idea. The domains are too disparate, lacking a common problem or application that could be effectively addressed together.",
neg-1s-44,1,,2402.02384v1,"Acoustic Local Positioning With Encoded Emission Beacons; Acoustic local positioning systems (ALPSs) are an interesting alternative for
indoor positioning due to certain advantages over other approaches, including
their relatively high accuracy, low cost, and room-level signal propagation.
Centimeter-level or fine-grained indoor positioning can be an asset for robot
navigation, guiding a person to, for instance, a particular piece in a museum
or to a specific product in a shop, targeted advertising, or augmented reality.
In airborne system applications, acoustic positioning can be based on using
opportunistic signals or sounds produced by the person or object to be located
(e.g., noise from appliances or the speech from a speaker) or from encoded
emission beacons (or anchors) specifically designed for this purpose. This work
presents a review of the different challenges that designers of systems based
on encoded emission beacons must address in order to achieve suitable
performance. At low-level processing, the waveform design (coding and
modulation) and the processing of the received signal are key factors to
address such drawbacks as multipath propagation, multiple-access interference,
nearfar effect, or Doppler shifting. With regards to high-level system design,
the issues to be addressed are related to the distribution of beacons, ease of
deployment, and calibration and positioning algorithms, including the possible
fusion of information. Apart from theoretical discussions, this work also
includes the description of an ALPS that was implemented, installed in a large
area and tested for mobile robot navigation. In addition to practical interest
for real applications, airborne ALPSs can also be used as an excellent platform
to test complex algorithms, which can be subsequently adapted for other
positioning systems, such as underwater acoustic systems or ultrawideband
radiofrequency (UWB RF) systems.",2308.02843v1,"One Microservice per Developer: Is This the Trend in OSS?; When developing and managing microservice systems, practitioners suggest that
each microservice should be owned by a particular team. In effect, there is
only one team with the responsibility to manage a given service. Consequently,
one developer should belong to only one team. This practice of
""one-microservice-per-developer"" is especially prevalent in large projects with
an extensive development team. Based on the bazaar-style software development
model of Open Source Projects, in which different programmers, like vendors at
a bazaar, offer to help out developing different parts of the system, this
article investigates whether we can observe the
""one-microservice-per-developer"" behavior, a strategy we assume anticipated
within microservice based Open Source Projects. We conducted an empirical study
among 38 microservice-based OS projects. Our findings indicate that the
strategy is rarely respected by open-source developers except for projects that
have dedicated DevOps teams.",False,False,"The concepts from the two papers do not align well enough to create a feasible multidisciplinary research idea. While acoustic positioning systems and microservices both involve technology, their applications and methodologies are distinct, lacking a clear intersection that meets all the required standards.",
neg-1s-45,1,,2205.12917v2,"Identification of Auction Models Using Order Statistics; Auction data often contain information on only the most competitive bids as
opposed to all bids. The usual measurement error approaches to unobserved
heterogeneity are inapplicable due to dependence among order statistics. We
bridge this gap by providing a set of positive identification results. First,
we show that symmetric auctions with discrete unobserved heterogeneity are
identifiable using two consecutive order statistics and an instrument. Second,
we extend the results to ascending auctions with unknown competition and
unobserved heterogeneity.",2412.00437v1,"DeepFGS: Fine-Grained Scalable Coding for Learned Image Compression; Scalable coding, which can adapt to channel bandwidth variation, performs
well in today's complex network environment. However, most existing scalable
compression methods face two challenges: reduced compression performance and
insufficient scalability. To overcome the above problems, this paper proposes a
learned fine-grained scalable image compression framework, namely DeepFGS.
Specifically, we introduce a feature separation backbone to divide the image
information into basic and scalable features, then redistribute the features
channel by channel through an information rearrangement strategy. In this way,
we can generate a continuously scalable bitstream via one-pass encoding. For
entropy coding, we design a mutual entropy model to fully explore the
correlation between the basic and scalable features. In addition, we reuse the
decoder to reduce the parameters and computational complexity. Experiments
demonstrate that our proposed DeepFGS outperforms previous learning-based
scalable image compression models and traditional scalable image codecs in both
PSNR and MS-SSIM metrics.",False,False,"The concepts from the two papers do not align in a way that creates a feasible multidisciplinary research idea. While auction models and image compression are distinct fields, their integration does not present a clear, novel application that addresses a specific problem effectively.",
neg-1s-46,1,,2103.02821v1,"MT* : Multi-Robot Path Planning for Temporal Logic Specifications; We address the path planning problem for a team of robots satisfying a
complex high-level mission specification given in the form of an Linear
Temporal Logic (LTL) formula. The state-of-the-art approach to this problem
employs the automata-theoretic model checking technique to solve this problem.
This approach involves computation of a product graph of the Buchi automaton
generated from the LTL specification and a joint transition system which
captures the collective motion of the robots and then computation of the
shortest path using Dijkstra's shortest path algorithm. We propose MT*, an
algorithm that reduces the computation burden for generating such plans for
multi-robot systems significantly. Our approach generates a reduced version of
the product graph without computing the complete joint transition system, which
is computationally expensive. It then divides the complete mission
specification among the participating robots and generates the trajectories for
the individual robots independently. Our approach demonstrates substantial
speedup in terms of computation time over the state-of-the-art approach, and
unlike the state of the art approach, scales well with both the number of
robots and the size of the workspace",2404.07331v1,"Financial climate risk: a review of recent advances and key challenges; The document provides an overview of financial climate risks. It delves into
how climate change impacts the global financial system, distinguishing between
physical risks (such as extreme weather events) and transition risks (stemming
from policy changes and economic transitions towards low carbon technologies).
The paper underlines the complexity of accurately defining financial climate
risk, citing the integration of climate science with financial risk analysis as
a significant challenge. The paper highlights the pivotal role of microfinance
institutions (MFIs) in addressing financial climate risk, especially for
populations vulnerable to climate change. The document emphasizes the
importance of updating risk management practices within MFIs to explicitly
include climate risk assessments and suggests leveraging technology to improve
these practices.",False,False,"While both papers address complex systems—robotics and financial climate risk—their integration lacks a clear, practical application. The concepts do not converge in a way that meets the feasibility, novelty, and usefulness criteria for a multidisciplinary research idea.",
neg-1s-47,1,,2407.19585v1,"The Rees algebra and analytic spread of a divisorial filtration; In this paper we investigate some properties of Rees algebras of divisorial
filtrations and their analytic spread. A classical theorem of McAdam shows that
the analytic spread of an ideal $I$ in a formally equidimensional local ring is
equal to the dimension of the ring if and only if the maximal ideal is an
associated prime of $R/\overline{I^n}$ for some $n$. We show in Theorem 1.6
that McAdam's theorem holds for $\mathbb Q$-divisorial filtrations in an
equidimensional local ring which is essentially of finite type over a field.
This generalizes an earlier result for $\mathbb Q$-divisorial filtrations in an
equicharacteristic zero excellent local domain by the author. This theorem does
not hold for more general filtrations.
  We consider the question of the asymptotic behavior of the function $n\mapsto
\lambda_R(R/I_n)$ for a $\mathbb Q$-divisorial filtration $\mathcal I=\{I_n\}$
of $m_R$-primary ideals on a $d$-dimensional normal excellent local ring. It is
known from earlier work of the author that the multiplicity $$ e(\mathcal I)=d!
\lim_{n\rightarrow\infty}\frac{\lambda_R(R/I_n)}{n^d} $$ can be irrational. We
show in Lemma 4.1 that the limsup of the first difference function $$
\limsup_{n\rightarrow\infty}\frac{\lambda_R(I_n/I_{n+1})}{n^{d-1}} $$ is always
finite for a $\mathbb Q$-divisorial filtration. We then give an example in
Section 4 showing that this limsup may not exist as a limit.
  In the final section, we give an example of a symbolic filtration
$\{P^{(n)}\}$ of a prime ideal $P$ in a normal two dimensional excellent local
ring which has the property that the set of Rees valuations of all the symbolic
powers $P^{(n)}$ of $P$ is infinite.",cond-mat/0503156v1,"Simulations of financial markets in a Potts-like model; A three-state model based on the Potts model is proposed to simulate
financial markets. The three states are assigned to ""buy"", ""sell"" and
""inactive"" states. The model shows the main stylized facts observed in the
financial market: fat-tailed distributions of returns and long time
correlations in the absolute returns. At low inactivity rate, the model
effectively reduces to the two-state model of Bornholdt and shows similar
results to the Bornholdt model. As the inactivity increases, we observe the
exponential distributions of returns.",False,False,"The concepts from the two papers—Rees algebras in algebraic geometry and financial market simulations—do not lend themselves to a coherent multidisciplinary idea that meets all standards. The mathematical nature of the first paper does not easily translate into practical applications in financial modeling, limiting feasibility and usefulness.",
neg-1s-48,1,,2009.07055v7,"Causal Inference of General Treatment Effects using Neural Networks with
  A Diverging Number of Confounders; Semiparametric efficient estimation of various multi-valued causal effects,
including quantile treatment effects, is important in economic, biomedical, and
other social sciences. Under the unconfoundedness condition, adjustment for
confounders requires estimating the nuisance functions relating outcome or
treatment to confounders nonparametrically. This paper considers a generalized
optimization framework for efficient estimation of general treatment effects
using artificial neural networks (ANNs) to approximate the unknown nuisance
function of growing-dimensional confounders. We establish a new approximation
error bound for the ANNs to the nuisance function belonging to a mixed
smoothness class without a known sparsity structure. We show that the ANNs can
alleviate the ""curse of dimensionality"" under this circumstance. We establish
the root-$n$ consistency and asymptotic normality of the proposed general
treatment effects estimators, and apply a weighted bootstrap procedure for
conducting inference. The proposed methods are illustrated via simulation
studies and a real data application.",1708.05938v2,"Emergent Lévy behavior in single-cell stochastic gene expression; Single-cell gene expression is inherently stochastic; its emergent behavior
can be defined in terms of the chemical master equation describing the
evolution of the mRNA and protein copy numbers as the latter tends to infinity.
We establish two types of ""macroscopic limits"": the Kurtz limit is consistent
with the classical chemical kinetics, while the L\'{e}vy limit provides a
theoretical foundation for an empirical equation proposed in [Phys. Rev. Lett.
97:168302, 2006]. Furthermore, we clarify the biochemical implications and
ranges of applicability for various macroscopic limits and calculate a
comprehensive analytic expression for the protein concentration distribution in
autoregulatory gene networks. The relationship between our work and modern
population genetics is discussed.",False,False,"While both papers address complex systems—causal inference in treatment effects and stochastic gene expression—their integration does not yield a clear, novel, and feasible research idea that meets all standards. The concepts do not directly overlap in a way that facilitates practical experimentation or innovative applications.",
neg-1s-49,1,,2409.02025v1,"Logarithmic regret in the ergodic Avellaneda-Stoikov market making model; We analyse the regret arising from learning the price sensitivity parameter
$\kappa$ of liquidity takers in the ergodic version of the Avellaneda-Stoikov
market making model. We show that a learning algorithm based on a regularised
maximum-likelihood estimator for the parameter achieves the regret upper bound
of order $\ln^2 T$ in expectation. To obtain the result we need two key
ingredients. The first are tight upper bounds on the derivative of the ergodic
constant in the Hamilton-Jacobi-Bellman (HJB) equation with respect to
$\kappa$. The second is the learning rate of the maximum-likelihood estimator
which is obtained from concentration inequalities for Bernoulli signals.
Numerical experiment confirms the convergence and the robustness of the
proposed algorithm.",1811.06877v1,"Smart Grid Co-Simulation with MOSAIK and HLA: A Comparison Study; Evaluating new technological developments for energy systems is becoming more
and more complex. The overall application environment is a continuously growing
and interconnected cyber-physical system so that analytical assessment is
practically impossible to realize. Consequently, new solutions must be
evaluated in simulation studies. Due to the interdisciplinarity of the
simulation scenarios, various heterogeneous tools must be connected. This
approach is known as co-simulation. During the last years, different approaches
have been developed or adapted for applications in energy systems. In this
paper, two co-simulation approaches are compared that follow generic, versatile
concepts. The tool mosaik, which has been explicitly developed for the purpose
of co-simulation in complex energy systems, is compared to the High Level
Architecture (HLA), which possesses a domain-independent scope but is often
employed in the energy domain. The comparison is twofold, considering the
tools' conceptual architectures as well as results from the simulation of
representative test cases. It suggests that mosaik may be the better choice for
entry-level, prototypical co-simulation while HLA is more suited for complex
and extensive studies.",False,False,"The concepts from the two papers do not converge in a way that creates a novel multidisciplinary research idea. While both involve complex systems, one focuses on market making algorithms and the other on energy system co-simulation, lacking a clear intersection for practical experimentation.",
neg-1s-50,1,,1801.01777v4,"Deep Learning for Forecasting Stock Returns in the Cross-Section; Many studies have been undertaken by using machine learning techniques,
including neural networks, to predict stock returns. Recently, a method known
as deep learning, which achieves high performance mainly in image recognition
and speech recognition, has attracted attention in the machine learning field.
This paper implements deep learning to predict one-month-ahead stock returns in
the cross-section in the Japanese stock market and investigates the performance
of the method. Our results show that deep neural networks generally outperform
shallow neural networks, and the best networks also outperform representative
machine learning models. These results indicate that deep learning shows
promise as a skillful machine learning method to predict stock returns in the
cross-section.",2410.08939v1,"Linear-cost unbiased posterior estimates for crossed effects and matrix
  factorization models via couplings; We design and analyze unbiased Markov chain Monte Carlo (MCMC) schemes based
on couplings of blocked Gibbs samplers (BGSs), whose total computational costs
scale linearly with the number of parameters and data points. Our methodology
is designed for and applicable to high-dimensional BGS with conditionally
independent blocks, which are often encountered in Bayesian modeling. We
provide bounds on the expected number of iterations needed for coalescence for
Gaussian targets, which imply that practical two-step coupling strategies
achieve coalescence times that match the relaxation times of the original BGS
scheme up to a logarithmic factor. To illustrate the practical relevance of our
methodology, we apply it to high-dimensional crossed random effect and
probabilistic matrix factorization models, for which we develop a novel BGS
scheme with improved convergence speed. Our methodology provides unbiased
posterior estimates at linear cost (usually requiring only a few BGS iterations
for problems with thousands of parameters), matching state-of-the-art
procedures for both frequentist and Bayesian estimation of those models.",False,False,"While both papers utilize advanced statistical and machine learning techniques, they focus on different domains (finance and Bayesian modeling). A multidisciplinary idea combining these concepts lacks a clear application or innovative approach that meets all the specified standards.",
neg-1s-51,1,,1303.2289v2,"Distributed optimization over time-varying directed graphs; We consider distributed optimization by a collection of nodes, each having
access to its own convex function, whose collective goal is to minimize the sum
of the functions. The communications between nodes are described by a
time-varying sequence of directed graphs, which is uniformly strongly
connected. For such communications, assuming that every node knows its
out-degree, we develop a broadcast-based algorithm, termed the
subgradient-push, which steers every node to an optimal value under a standard
assumption of subgradient boundedness. The subgradient-push requires no
knowledge of either the number of agents or the graph sequence to implement.
Our analysis shows that the subgradient-push algorithm converges at a rate of
$O(\ln(t)/\sqrt{t})$, where the constant depends on the initial values at the
nodes, the subgradient norms, and, more interestingly, on both the consensus
speed and the imbalances of influence among the nodes.",2402.13604v2,"Breaking the HISCO Barrier: Automatic Occupational Standardization with
  OccCANINE; This paper introduces a new tool, OccCANINE, to automatically transform
occupational descriptions into the HISCO classification system. The manual work
involved in processing and classifying occupational descriptions is
error-prone, tedious, and time-consuming. We finetune a preexisting language
model (CANINE) to do this automatically, thereby performing in seconds and
minutes what previously took days and weeks. The model is trained on 14 million
pairs of occupational descriptions and HISCO codes in 13 different languages
contributed by 22 different sources. Our approach is shown to have accuracy,
recall, and precision above 90 percent. Our tool breaks the metaphorical HISCO
barrier and makes this data readily available for analysis of occupational
structures with broad applicability in economics, economic history, and various
related disciplines.",False,False,"While both papers present innovative concepts, the combination of distributed optimization algorithms with automatic occupational classification does not yield a clear, novel, and feasible research idea that meets all standards. The integration lacks a direct application or experimental validation pathway.",
neg-1s-52,1,,1608.02143v2,"Bayesian Sparse Linear Regression with Unknown Symmetric Error; We study full Bayesian procedures for sparse linear regression when errors
have a symmetric but otherwise unknown distribution. The unknown error
distribution is endowed with a symmetrized Dirichlet process mixture of
Gaussians. For the prior on regression coefficients, a mixture of point masses
at zero and continuous distributions is considered. We study behavior of the
posterior with diverging number of predictors. Conditions are provided for
consistency in the mean Hellinger distance. The compatibility and restricted
eigenvalue conditions yield the minimax convergence rate of the regression
coefficients in $\ell_1$- and $\ell_2$-norms, respectively. The convergence
rate is adaptive to both the unknown sparsity level and the unknown symmetric
error density under compatibility conditions. In addition, strong model
selection consistency and a semi-parametric Bernstein-von Mises theorem are
proven under slightly stronger conditions.",2401.05888v1,"Incorporation of Confidence Interval into Rate Selection Based on the
  Extreme Value Theory for Ultra-Reliable Communications; Proper determination of the transmission rate in ultra-reliable low latency
communication (URLLC) needs to incorporate a confidence interval (CI) for the
estimated parameters due to the large amount of data required for their
accurate estimation. In this paper, we propose a framework based on the extreme
value theory (EVT) for determining the transmission rate along with its
corresponding CI for an ultra-reliable communication system. This framework
consists of characterizing the statistics of extreme events by fitting the
generalized Pareto distribution (GPD) to the channel tail, deriving the GPD
parameters and their associated CIs, and obtaining the transmission rate within
a confidence interval. Based on the data collected within the engine
compartment of Fiat Linea, we demonstrate the accuracy of the estimated rate
obtained through the EVT-based framework considering the confidence interval
for the GPD parameters. Additionally, we show that proper estimation of the
transmission rate based on the proposed framework requires a lower number of
samples compared to the traditional extrapolation-based approaches.",False,True,"The combination of Bayesian sparse linear regression techniques from Paper 1 with the extreme value theory framework for transmission rate selection in Paper 2 can lead to a novel approach for optimizing communication systems under uncertainty. This idea is feasible, experimental, and addresses real-world communication challenges effectively.",
neg-1s-53,1,,2103.04517v1,"Optimal Hölder regularity for the $\bar\partial$ problem on product
  domains in $\mathbb C^2$; The note concerns the $\bar\partial$ problem on product domains in $\mathbb
C^2$. We show that there exists a bounded solution operator from $C^{k,
\alpha}$ into itself, $k\in \mathbb Z^+\cup \{0\}, 0<\alpha< 1$. The regularity
result is optimal in view of an example of Stein-Kerzman.",1401.5398v1,"Dirichlet-Laplace priors for optimal shrinkage; Penalized regression methods, such as $L_1$ regularization, are routinely
used in high-dimensional applications, and there is a rich literature on
optimality properties under sparsity assumptions. In the Bayesian paradigm,
sparsity is routinely induced through two-component mixture priors having a
probability mass at zero, but such priors encounter daunting computational
problems in high dimensions. This has motivated an amazing variety of
continuous shrinkage priors, which can be expressed as global-local scale
mixtures of Gaussians, facilitating computation. In sharp contrast to the
frequentist literature, little is known about the properties of such priors and
the convergence and concentration of the corresponding posterior distribution.
In this article, we propose a new class of Dirichlet--Laplace (DL) priors,
which possess optimal posterior concentration and lead to efficient posterior
computation exploiting results from normalized random measure theory. Finite
sample performance of Dirichlet--Laplace priors relative to alternatives is
assessed in simulated and real data examples.",False,False,"The concepts from the two papers, one focusing on mathematical regularity in complex analysis and the other on Bayesian statistics for high-dimensional data, do not easily converge into a novel, feasible research idea that meets all the specified standards.",
neg-1s-54,1,,1305.0361v1,"Braess's Paradox in Epidemic Game: Better Condition Results in Less
  Payoff; Facing the threats of infectious diseases, we take various actions to protect
ourselves, but few studies considered an evolving system with competing
strategies. In view of that, we propose an evolutionary epidemic model coupled
with human behaviors, where individuals have three strategies: vaccination,
self-protection and laissez faire, and could adjust their strategies according
to their neighbors' strategies and payoffs at the beginning of each new season
of epidemic spreading. We found a counter-intuitive phenomenon analogous to the
well-known \emph{Braess's Paradox}, namely a better condition may lead to worse
performance. Specifically speaking, increasing the successful rate of
self-protection does not necessarily reduce the epidemic size or improve the
system payoff. This phenomenon is insensitive to the network topologies, and
can be well explained by a mean-field approximation. Our study demonstrates an
important fact that a better condition for individuals may yield a worse
outcome for the society.",1805.05259v1,"The strong Fatou property of risk measures; In this paper, we explore several Fatou-type properties of risk measures. The
paper continues to reveal that the strong Fatou property, which was introduced
in [17], seems to be most suitable to ensure nice dual representations of risk
measures. Our main result asserts that every quasiconvex law-invariant
functional on a rearrangement invariant space $\mathcal{X}$ with the strong
Fatou property is $\sigma(\mathcal{X},L^\infty)$ lower semicontinuous and that
the converse is true on a wide range of rearrangement invariant spaces. We also
study inf-convolutions of law-invariant or surplus-invariant risk measures that
preserve the (strong) Fatou property.",False,False,"The concepts from the two papers do not align sufficiently to create a novel multidisciplinary research idea. While Paper 1 focuses on epidemic modeling and human behavior, Paper 2 deals with risk measures in mathematical finance, lacking a clear intersection for practical application or experimental validation.",
neg-1s-55,1,,1911.10552v1,"High-Dimensional Forecasting in the Presence of Unit Roots and
  Cointegration; We investigate how the possible presence of unit roots and cointegration
affects forecasting with Big Data. As most macroeoconomic time series are very
persistent and may contain unit roots, a proper handling of unit roots and
cointegration is of paramount importance for macroeconomic forecasting. The
high-dimensional nature of Big Data complicates the analysis of unit roots and
cointegration in two ways. First, transformations to stationarity require
performing many unit root tests, increasing room for errors in the
classification. Second, modelling unit roots and cointegration directly is more
difficult, as standard high-dimensional techniques such as factor models and
penalized regression are not directly applicable to (co)integrated data and
need to be adapted. We provide an overview of both issues and review methods
proposed to address these issues. These methods are also illustrated with two
empirical applications.",2003.04007v1,"Copula-based local dependence between energy, agriculture and metal
  commodity markets; This paper studies the extreme dependencies between energy, agriculture and
metal commodity markets, with a focus on local co-movements, allowing the
identification of asymmetries and changing trend in the degree of co-movements.
More precisely, starting from a non-parametric mixture copula, we use a novel
copula-based local Kendall's tau approach to measure nonlinear local dependence
in regions. In all pairs of commodity indexes, we find increased co-movements
in extreme situations, a stronger dependence between energy and other commodity
markets at lower tails, and a 'V-type' local dependence for the energy-metal
pairs. The three-dimensional Kendall's tau plot for upper tails in quantiles
shows asymmetric co-movements in the energy-metal pairs, which tend to become
negative at peak returns. Therefore, we show that the energy market can offer
diversification solutions for risk management in the case of extreme bull
market events.",False,True,"By combining the forecasting techniques for macroeconomic time series from Paper 1 with the copula-based local dependence analysis of commodity markets from Paper 2, a novel research idea could emerge that forecasts extreme dependencies in commodity markets using high-dimensional data methods, addressing practical risk management needs.",
neg-1s-56,1,,1402.5390v1,"Spontaneous oscillations from turnover of an elastic contractile
  material; Single and collective cellular oscillations involving the actomyosin
cytoskeleton have been observed in numerous biological systems. We show here
that a generic model of a contractile material, which is turning over and
contracts against an elastic element, exhibits spontaneous oscillations. Such a
model can thus account for shape oscillations observed in amnioserosa cells
during dorsal closure of the Drosophila embryo. We investigate the collective
dynamics of an ensemble of such oscillators and show that the relative
contribution of viscous and friction losses yield different regimes of
collective oscillations. Taking into account the diffusion of contractile
elements, our theoretical framework predicts the appearance of traveling waves
which might account for the propagation of actomyosin contractile waves
observed during morphogenesis.",2312.11944v2,"FPT Approximation using Treewidth: Capacitated Vertex Cover, Target Set
  Selection and Vector Dominating Set; Treewidth is a useful tool in designing graph algorithms. Although many
NP-hard graph problems can be solved in linear time when the input graphs have
small treewidth, there are problems which remain hard on graphs of bounded
treewidth. In this paper, we consider three vertex selection problems that are
W[1]-hard when parameterized by the treewidth of the input graph, namely the
capacitated vertex cover problem, the target set selection problem and the
vector dominating set problem. We provide two new methods to obtain FPT
approximation algorithms for these problems. For the capacitated vertex cover
problem and the vector dominating set problem, we obtain
$(1+o(1))$-approximation FPT algorithms. For the target set selection problem,
we give an FPT algorithm providing a tradeoff between its running time and the
approximation ratio.",False,False,"The concepts from the two papers do not converge in a way that creates a feasible, novel, and useful multidisciplinary research idea. The biological oscillations and graph algorithms are too disparate, lacking a clear application or experimental validation that integrates both fields effectively.",
neg-1s-57,1,,2007.11941v1,"(Unintended) Consequences of export restrictions on medical goods during
  the Covid-19 pandemic; In the first half of 2020, several countries have responded to the challenges
posed by the Covid-19 pandemic by restricting their export of medical supplies.
Such measures are meant to increase the domestic availability of critical
goods, and are commonly used in times of crisis. Yet, not much is known about
their impact, especially on countries imposing them. Here we show that export
bans are, by and large, counterproductive. Using a model of shock diffusion
through the network of international trade, we simulate the impact of
restrictions under different scenarios. We observe that while they would be
beneficial to a country implementing them in isolation, their generalized use
makes most countries worse off relative to a no-ban scenario. As a corollary,
we estimate that prices increase in many countries imposing the restrictions.
We also find that the cost of restraining from export bans is small, even when
others continue to implement them. Finally, we document a change in countries'
position within the international trade network, suggesting that export bans
have geopolitical implications.",2412.06226v1,"Can Generalized Extreme Value Model Fit the Real Stocks; The Generalized Extreme Value (GEV) distribution plays a critical role in
risk assessment across various domains, such as hydrology, climate science, and
finance. In this study, we investigate its application in analyzing intraday
trading risks within the Chinese stock market, focusing on abrupt price
movements influenced by unique trading regulations. To address limitations of
traditional GEV parameter estimators, we leverage recently developed robust and
asymptotically normal estimators, enabling accurate modeling of extreme
intraday price fluctuations. We introduce two risk indicators: the mean risk
level (mEVI) and a Stability Indicator (STI) to evaluate the stability of the
shape parameter over time. Using data from 261 Chinese and 32 U.S. stocks
(2015-2017), we find that Chinese stocks exhibit higher mEVI, corresponding to
greater tail risk, while maintaining high model stability. Additionally, we
show that Value at Risk (VaR) estimates derived from our GEV models outperform
traditional GP and normal-based VaR methods in terms of variance and portfolio
optimization. These findings underscore the versatility and efficiency of GEV
modeling for intraday risk management and portfolio strategies.",False,False,"While both papers address important issues—export restrictions and risk assessment in stock markets—they do not present overlapping concepts that can be effectively combined into a novel, feasible, and useful multidisciplinary research idea. The domains are too distinct without a clear intersection.",
neg-1s-58,1,,2404.00806v2,"Algorithmic Collusion by Large Language Models; The rise of algorithmic pricing raises concerns of algorithmic collusion. We
conduct experiments with algorithmic pricing agents based on Large Language
Models (LLMs). We find that (1) LLM-based agents are adept at pricing tasks,
(2) LLM-based pricing agents autonomously collude in oligopoly settings to the
detriment of consumers, and (3) variation in seemingly innocuous phrases in LLM
instructions (""prompts"") may increase collusion. Novel off-path analysis
techniques uncover price-war concerns as contributing to these phenomena. Our
results extend to auction settings. Our findings uncover unique challenges to
any future regulation of LLM-based pricing agents, and black-box pricing agents
more broadly.",0902.4417v1,"A Dimension Reduction Method for Inferring Biochemical Networks; We present herein an extension of an algebraic statistical method for
inferring biochemical reaction networks from experimental data, proposed
recently in [3]. This extension allows us to analyze reaction networks that are
not necessarily full-dimensional, i.e., the dimension of their stoichiometric
space is smaller than the number of species. Specifically, we propose to
augment the original algebraic-statistical algorithm for network inference with
a preprocessing step that identifies the subspace spanned by the correct
reaction vectors, within the space spanned by the species. This dimension
reduction step is based on principal component analysis of the input data and
its relationship with various subspaces generated by sets of candidate reaction
vectors. Simulated examples are provided to illustrate the main ideas involved
in implementing this method, and to asses its performance.",False,False,"The concepts from the two papers do not align in a way that creates a feasible, novel, and useful multidisciplinary research idea. While both involve complex systems, the focus on algorithmic pricing and biochemical networks lacks a clear intersection that meets all the specified standards.",
neg-1s-59,1,,2202.05731v1,"Flags, Landscapes and Signaling: Contact-mediated inter-cellular
  interactions enable plasticity in fate determination driven by positional
  information; Multicellular organisms exhibit a high degree of structural organization with
specific cell types always occurring in characteristic locations. The
conventional framework for describing the emergence of such consistent spatial
patterns is provided by Wolpert's ""French flag"" paradigm. According to this
view, intra-cellular genetic regulatory mechanisms use positional information
provided by morphogen concentration gradients to differentially express
distinct fates, resulting in a characteristic pattern of differentiated cells.
However, recent experiments have shown that suppression of inter-cellular
interactions can alter these spatial patterns, suggesting that cell fates are
not exclusively determined by the regulation of gene expression by local
morphogen concentration. Using an explicit model where adjacent cells
communicate by Notch signaling, we provide a mechanistic description of how
contact-mediated interactions allow information from the cellular environment
to be incorporated into cell fate decisions. Viewing cellular differentiation
in terms of trajectories along an epigenetic landscape (as first enunciated by
Waddington), our results suggest that the contours of the landscape are moulded
differently in a cell position-dependent manner, not only by the global signal
provided by the morphogen but also by the local environment via cell-cell
interactions. We show that our results are robust with respect to different
choices of coupling between the inter-cellular signaling apparatus and the
intra-cellular gene regulatory dynamics. Indeed, we show that the broad
features can be observed even in abstract spin models. Our work reconciles
interaction-mediated self-organized pattern formation with boundary-organized
mechanisms involving signals that break symmetry.",physics/0610013v1,"Hydrogen atom in crossed electric and magnetic fields: Phase space
  topology and torus quantization via periodic orbits; A hierarchical ordering is demonstrated for the periodic orbits in a strongly
coupled multidimensional Hamiltonian system, namely the hydrogen atom in
crossed electric and magnetic fields. It mirrors the hierarchy of broken
resonant tori and thereby allows one to characterize the periodic orbits by a
set of winding numbers. With this knowledge, we construct the action variables
as functions of the frequency ratios and carry out a semiclassical torus
quantization. The semiclassical energy levels thus obtained agree well with
exact quantum calculations.",False,False,"The concepts from the two papers are too disparate; one focuses on cellular interactions and differentiation, while the other deals with quantum mechanics and periodic orbits. There is no clear, feasible, and novel intersection that addresses a specific problem effectively.",
neg-1s-60,1,,1404.4416v1,"A characterization of eventually periodicity; In this article, we show that the Kamae-Xue complexity function for an
infinite sequence classifies eventual periodicity completely. We prove that an
infinite binary word $x_1x_2 \cdots $ is eventually periodic if and only if
$\Sigma(x_1x_2\cdots x_n)/n^3$ has a positive limit, where $\Sigma(x_1x_2\cdots
x_n)$ is the sum of the squares of all the numbers of appearance of finite
words in $x_1 x_2 \cdots x_n$, which was introduced by Kamae-Xue as a criterion
of randomness in the sense that $x_1x_2\cdots x_n$ is more random if
$\Sigma(x_1x_2\cdots x_n)$ is smaller. In fact, it is known that the lower
limit of $\Sigma(x_1x_2\cdots x_n) /n^2 $ is at least 3/2 for any sequence
$x_1x_2 \cdots$, while the limit exists as 3/2 almost surely for the
$(1/2,1/2)$ product measure. For the other extreme, the upper limit of
$\Sigma(x_1x_2\cdots x_n)/n^3$ is bounded by 1/3. There are sequences which are
not eventually periodic but the lower limit of $\Sigma(x_1x_2\cdots x_n)/n^3$
is positive, while the limit does not exist.",0902.0600v5,"Reconstruction of Epsilon-Machines in Predictive Frameworks and
  Decisional States; This article introduces both a new algorithm for reconstructing
epsilon-machines from data, as well as the decisional states. These are defined
as the internal states of a system that lead to the same decision, based on a
user-provided utility or pay-off function. The utility function encodes some a
priori knowledge external to the system, it quantifies how bad it is to make
mistakes. The intrinsic underlying structure of the system is modeled by an
epsilon-machine and its causal states. The decisional states form a partition
of the lower-level causal states that is defined according to the higher-level
user's knowledge. In a complex systems perspective, the decisional states are
thus the ""emerging"" patterns corresponding to the utility function. The
transitions between these decisional states correspond to events that lead to a
change of decision. The new REMAPF algorithm estimates both the epsilon-machine
and the decisional states from data. Application examples are given for hidden
model reconstruction, cellular automata filtering, and edge detection in
images.",False,True,"The combination of the Kamae-Xue complexity function's insights on eventual periodicity with the epsilon-machine reconstruction and decisional states can lead to a novel framework for predicting patterns in complex systems. This idea is feasible, as it can be validated through experiments in data analysis and decision-making processes.",
neg-1s-61,1,,1207.4589v1,"Minimum-Length Scheduling with Finite Queues: Solution Characterization
  and Algorithmic Framework; We consider a set of transmitter-receiver pairs, or links, that share a
common channel and address the problem of emptying backlogged queues at the
transmitters in minimum time. The problem amounts to determining activation
subsets of links and their time durations to form a minimum-length schedule.
The problem of scheduling has been studied under various formulations before.
In this paper, we present fundamental insights and solution characterizations
that include: (i) showing that the complexity of the problem remains high for
any continuous and increasing rate function, (ii) formulating and proving
sufficient and necessary optimality conditions of two base scheduling
strategies that correspond to emptying the queues using ""one-at-a-time"" or
""all-at-once"" strategies, (iii) presenting and proving the tractability of the
special case in which the transmission rates are functions only of the
cardinality of the link activation sets. These results are independent of
physical-layer system specifications and are valid for any form of rate
function. We then develop an algorithmic framework. The framework encompasses
exact as well as sub-optimal, but fast, scheduling algorithms, all under a
unified principle design. Through computational experiments we finally
investigate the performance of several specific algorithms.",2204.12850v2,"Diurnal variation of the surface temperature of Mars with the Emirates
  Mars Mission: A comparison with Curiosity and Perseverance rover measurements; For the first time, the Emirates Mars Infrared Spectrometer (EMIRS)
instrument on board the Emirates Mars Mission (EMM) ""Hope"", is providing us
with the temperature measurements of Mars at all local times covering most of
the planet. As a result, it is now possible to compare surface temperature
measurements made from orbit with those from the surface by rovers during the
same time period. We use data of diurnal temperature variation from the Rover
Environmental Monitoring Station (REMS) suite on board the Mars Science
Laboratory (MSL) ""Curiosity"" rover, and the Mars Environmental Dynamics
Analyzer (MEDA) suite on board the Mars 2020 ""Perseverance"" rover, between June
and August 2021 and compare them with EMIRS observations and estimates of the
Mars Climate Database (MCD) model. We show that although the overall trend of
temperature variation is in excellent agreement across missions, EMIRS
measurements are systematically lower at night compared to Mars 2020. The lower
spatial resolution of EMIRS compared to the rovers and consequently lower
average thermal inertia of the observed regions in this particular case
primarily contributed to this discrepancy, among other factors. We discuss the
implications of these results in improving our understanding of the Martian
climate which would lead to better modeling of local weather prediction, useful
for future robotic and crewed missions.",False,True,"By combining the scheduling algorithms from Paper 1 with the temperature data analysis from Paper 2, a novel research idea could focus on optimizing the scheduling of robotic missions on Mars based on real-time temperature variations, enhancing mission efficiency and safety.",
neg-1s-62,1,,2501.02303v1,"Design and Benchmarking of A Multi-Modality Sensor for Robotic
  Manipulation with GAN-Based Cross-Modality Interpretation; In this paper, we present the design and benchmark of an innovative sensor,
ViTacTip, which fulfills the demand for advanced multi-modal sensing in a
compact design. A notable feature of ViTacTip is its transparent skin, which
incorporates a `see-through-skin' mechanism. This mechanism aims at capturing
detailed object features upon contact, significantly improving both
vision-based and proximity perception capabilities. In parallel, the biomimetic
tips embedded in the sensor's skin are designed to amplify contact details,
thus substantially augmenting tactile and derived force perception abilities.
To demonstrate the multi-modal capabilities of ViTacTip, we developed a
multi-task learning model that enables simultaneous recognition of hardness,
material, and textures. To assess the functionality and validate the
versatility of ViTacTip, we conducted extensive benchmarking experiments,
including object recognition, contact point detection, pose regression, and
grating identification. To facilitate seamless switching between various
sensing modalities, we employed a Generative Adversarial Network (GAN)-based
approach. This method enhances the applicability of the ViTacTip sensor across
diverse environments by enabling cross-modality interpretation.",q-bio/0605033v1,"Impact of observational incompleteness on the structural properties of
  protein interaction networks; The observed structure of protein interaction networks is corrupted by many
false positive/negative links. This observational incompleteness is abstracted
as random link removal and a specific, experimentally motivated (spoke) link
rearrangement. Their impact on the structural properties of
gene-duplication-and-mutation network models is studied. For the degree
distribution a curve collapse is found, showing no sensitive dependence on the
link removal/rearrangement strengths and disallowing a quantitative extraction
of model parameters. The spoke link rearrangement process moves other
structural observables, like degree correlations, cluster coefficient and motif
frequencies, closer to their counterparts extracted from the yeast data. This
underlines the importance to take a precise modeling of the observational
incompleteness into account when network structure models are to be
quantitatively compared to data.",False,False,"While both papers present innovative concepts, the integration of a multi-modality sensor for robotic manipulation with the complexities of protein interaction networks does not yield a clear, feasible, and novel multidisciplinary research idea that meets all the specified standards.",
neg-1s-63,1,,2312.04464v1,"Horizon-Free and Instance-Dependent Regret Bounds for Reinforcement
  Learning with General Function Approximation; To tackle long planning horizon problems in reinforcement learning with
general function approximation, we propose the first algorithm, termed as
UCRL-WVTR, that achieves both \emph{horizon-free} and
\emph{instance-dependent}, since it eliminates the polynomial dependency on the
planning horizon. The derived regret bound is deemed \emph{sharp}, as it
matches the minimax lower bound when specialized to linear mixture MDPs up to
logarithmic factors. Furthermore, UCRL-WVTR is \emph{computationally efficient}
with access to a regression oracle. The achievement of such a horizon-free,
instance-dependent, and sharp regret bound hinges upon (i) novel algorithm
designs: weighted value-targeted regression and a high-order moment estimator
in the context of general function approximation; and (ii) fine-grained
analyses: a novel concentration bound of weighted non-linear least squares and
a refined analysis which leads to the tight instance-dependent bound. We also
conduct comprehensive experiments to corroborate our theoretical findings.",2410.22757v1,"Synthesis of Timeline-Based Planning Strategies Avoiding Determinization; Qualitative timeline-based planning models domains as sets of independent,
but interacting, components whose behaviors over time, the timelines, are
governed by sets of qualitative temporal constraints (ordering relations),
called synchronization rules. Its plan-existence problem has been shown to be
PSPACE-complete; in particular, PSPACE-membership has been proved via reduction
to the nonemptiness problem for nondeterministic finite automata. However,
nondeterministic automata cannot be directly used to synthesize planning
strategies as a costly determinization step is needed. In this paper, we
identify a large fragment of qualitative timeline-based planning whose
plan-existence problem can be directly mapped into the nonemptiness problem of
deterministic finite automata, which can then be exploited to synthesize
strategies. In addition, we identify a maximal subset of Allen's relations that
fits into such a deterministic fragment.",False,False,"While both papers address planning and decision-making, the concepts from reinforcement learning and timeline-based planning do not easily converge into a novel, feasible research idea that meets all standards. The integration of their methodologies lacks a clear experimental validation pathway and practical application.",
neg-1s-64,1,,2411.07547v1,"AuscultaBase: A Foundational Step Towards AI-Powered Body Sound
  Diagnostics; Auscultation of internal body sounds is essential for diagnosing a range of
health conditions, yet its effectiveness is often limited by clinicians'
expertise and the acoustic constraints of human hearing, restricting its use
across various clinical scenarios. To address these challenges, we introduce
AuscultaBase, a foundational framework aimed at advancing body sound
diagnostics through innovative data integration and contrastive learning
techniques. Our contributions include the following: First, we compile
AuscultaBase-Corpus, a large-scale, multi-source body sound database
encompassing 11 datasets with 40,317 audio recordings and totaling 322.4 hours
of heart, lung, and bowel sounds. Second, we develop AuscultaBase-Model, a
foundational diagnostic model for body sounds, utilizing contrastive learning
on the compiled corpus. Third, we establish AuscultaBase-Bench, a comprehensive
benchmark containing 16 sub-tasks, assessing the performance of various
open-source acoustic pre-trained models. Evaluation results indicate that our
model outperforms all other open-source models in 12 out of 16 tasks,
demonstrating the efficacy of our approach in advancing diagnostic capabilities
for body sound analysis.",1303.1296v1,"The Pricing of A Moving Barrier Option; We provided an analytical representation of the price of a barrier option
with one type of special moving barrier. We consider the case that risk free
rate, dividend rate and stock volatility are time dependent. We get a pricing
formula and put call parity for barrier option when the moving barrier has a
special relation with risk free rate, dividend rate and stock volatility.",False,False,"The concepts from the two papers—AI-powered body sound diagnostics and financial barrier options—do not naturally intersect in a way that creates a novel, feasible, and useful multidisciplinary research idea. The domains are too disparate to yield a coherent and applicable research direction.",
neg-1s-65,1,,0902.1373v3,"Inverse spectral problem for analytic $(Z/2Z)^n$-symmetric domains in
  $R^n$; In this paper we show that bounded analytic domains in $\R^n$ with mirror
symmetries across all coordinate axes are spectrally determined among other
such domains. Our approach builds on finding concrete formulas for the wave
invariants at a bouncing ball orbit. The wave invariants are calculated from a
stationary phase expansion applied to a well-constructed microlocal parametrix
for the trace of the resolvent.",1409.8497v2,"Apparent impact: the hidden cost of one-shot trades; We study the problem of the execution of a moderate size order in an illiquid
market within the framework of a solvable Markovian model. We suppose that in
order to avoid impact costs, a trader decides to execute her order through a
unique trade, waiting for enough liquidity to accumulate at the best quote. We
find that despite the absence of a proper price impact, such trader faces an
execution cost arising from a non-vanishing correlation among volume at the
best quotes and price changes. We characterize analytically the statistics of
the execution time and its cost by mapping the problem to the simpler one of
calculating a set of first-passage probabilities on a semi-infinite strip. We
finally argue that price impact cannot be completely avoided by conditioning
the execution of an order to a more favorable liquidity scenario.",False,False,The concepts from the two papers—spectral analysis of symmetric domains and execution costs in illiquid markets—do not easily converge into a novel multidisciplinary idea that meets all standards. The theoretical nature of the first paper and the practical focus of the second create a disconnect in applicability and feasibility.,
neg-1s-66,1,,1511.06639v1,"Compressed and quantized correlation estimators; In passive monitoring using sensor networks, low energy supplies drastically
constrain sensors in terms of calculation and communication abilities.
Designing processing algorithms at the sensor level that take into account
these constraints is an important problem in this context. We study here the
estimation of correlation functions between sensors using compressed
acquisition and one-bit-quantization. The estimation is achieved directly using
compressed samples, without considering any reconstruction of the signals. We
show that if the signals of interest are far from white noise, estimation of
the correlation using $M$ compressed samples out of $N\geq M$ can be more
advantageous than estimation of the correlation using $M$ consecutive samples.
The analysis consists of studying the asymptotic performance of the estimators
at a fixed compression rate. We provide the analysis when the compression is
realized by a random projection matrix composed of independent and identically
distributed entries. The framework includes widely used random projection
matrices, such as Gaussian and Bernoulli matrices, and it also includes very
sparse matrices. However, it does not include subsampling without replacement,
for which a separate analysis is provided. When considering
one-bit-quantization as well, the theoretical analysis is not tractable.
However, empirical evidence allows the conclusion that in practical situations,
compressed and quantized estimators behave sufficiently correctly to be useful
in, for example, time-delay estimation and model estimation.",2002.02657v1,"Optimization of Structural Similarity in Mathematical Imaging; It is now generally accepted that Euclidean-based metrics may not always
adequately represent the subjective judgement of a human observer. As a result,
many image processing methodologies have been recently extended to take
advantage of alternative visual quality measures, the most prominent of which
is the Structural Similarity Index Measure (SSIM). The superiority of the
latter over Euclidean-based metrics have been demonstrated in several studies.
However, being focused on specific applications, the findings of such studies
often lack generality which, if otherwise acknowledged, could have provided a
useful guidance for further development of SSIM-based image processing
algorithms. Accordingly, instead of focusing on a particular image processing
task, in this paper, we introduce a general framework that encompasses a wide
range of imaging applications in which the SSIM can be employed as a fidelity
measure. Subsequently, we show how the framework can be used to cast some
standard as well as original imaging tasks into optimization problems, followed
by a discussion of a number of novel numerical strategies for their solution.",False,True,"The combination of compressed and quantized correlation estimators from Paper 1 with the Structural Similarity Index Measure (SSIM) from Paper 2 can lead to a novel approach for optimizing image quality in low-energy sensor networks. This idea is feasible, experimental, and addresses practical challenges in image processing.",
neg-1s-67,1,,cond-mat/0510154v3,"Role of Noise in a Market Model with Stochastic Volatility; We study a generalization of the Heston model, which consists of two coupled
stochastic differential equations, one for the stock price and the other one
for the volatility. We consider a cubic nonlinearity in the first equation and
a correlation between the two Wiener processes, which model the two white noise
sources. This model can be useful to describe the market dynamics characterized
by different regimes corresponding to normal and extreme days. We analyze the
effect of the noise on the statistical properties of the escape time with
reference to the noise enhanced stability (NES) phenomenon, that is the noise
induced enhancement of the lifetime of a metastable state. We observe NES
effect in our model with stochastic volatility. We investigate the role of the
correlation between the two noise sources on the NES effect.",1511.08102v3,"L1-Regularized Least Squares for Support Recovery of High Dimensional
  Single Index Models with Gaussian Designs; It is known that for a certain class of single index models (SIMs) $Y =
f(\boldsymbol{X}_{p \times 1}^\intercal\boldsymbol{\beta}_0, \varepsilon)$,
support recovery is impossible when $\boldsymbol{X} \sim \mathcal{N}(0,
\mathbb{I}_{p \times p})$ and a model complexity adjusted sample size is below
a critical threshold. Recently, optimal algorithms based on Sliced Inverse
Regression (SIR) were suggested. These algorithms work provably under the
assumption that the design $\boldsymbol{X}$ comes from an i.i.d. Gaussian
distribution. In the present paper we analyze algorithms based on covariance
screening and least squares with $L_1$ penalization (i.e. LASSO) and
demonstrate that they can also enjoy optimal (up to a scalar) rescaled sample
size in terms of support recovery, albeit under slightly different assumptions
on $f$ and $\varepsilon$ compared to the SIR based algorithms. Furthermore, we
show more generally, that LASSO succeeds in recovering the signed support of
$\boldsymbol{\beta}_0$ if $\boldsymbol{X} \sim \mathcal{N}(0,
\boldsymbol{\Sigma})$, and the covariance $\boldsymbol{\Sigma}$ satisfies the
irrepresentable condition. Our work extends existing results on the support
recovery of LASSO for the linear model, to a more general class of SIMs.",False,False,"While both papers explore complex systems—market dynamics and high-dimensional statistical models—they do not present a clear intersection that leads to a novel, feasible, and useful multidisciplinary research idea. The concepts are too distinct to effectively combine for practical experimentation.",
neg-1s-68,1,,1811.02478v3,"Proofs of life: molecular-biology reasoning simulates cell behaviors
  from first principles; We axiomatize the molecular-biology reasoning style, show compliance of the
standard reference: Ptashne, A Genetic Switch, and present proof-theory-induced
technologies to help infer phenotypes and to predict life cycles from
genotypes. The key is to note that `reductionist discipline' entails
constructive reasoning: any proof of a compound property can be decomposed to
proofs of constituent properties. Proof theory makes explicit the inner
structure of the axiomatized reasoning style and allows the permissible
dynamics to be presented as a mode of computation that can be executed and
analyzed. Constructivity and execution guarantee simulation when working over
domain-specific languages. Here, we exhibit phenotype properties for genotype
reasons: a molecular-biology argument is an open-system concurrent computation
that results in compartment changes and is performed among processes of
physiology change as determined from the molecular programming of given DNA.
Life cycles are the possible sequentializations of the processes. A main
implication of our construction is that formal correctness provides a
complementary perspective on science that is as fundamental there as for pure
mathematics. The bulk of the presented work has been verified formally correct
by computer.",2007.14846v1,"An Uncertainty-aware Transfer Learning-based Framework for Covid-19
  Diagnosis; The early and reliable detection of COVID-19 infected patients is essential
to prevent and limit its outbreak. The PCR tests for COVID-19 detection are not
available in many countries and also there are genuine concerns about their
reliability and performance. Motivated by these shortcomings, this paper
proposes a deep uncertainty-aware transfer learning framework for COVID-19
detection using medical images. Four popular convolutional neural networks
(CNNs) including VGG16, ResNet50, DenseNet121, and InceptionResNetV2 are first
applied to extract deep features from chest X-ray and computed tomography (CT)
images. Extracted features are then processed by different machine learning and
statistical modelling techniques to identify COVID-19 cases. We also calculate
and report the epistemic uncertainty of classification results to identify
regions where the trained models are not confident about their decisions (out
of distribution problem). Comprehensive simulation results for X-ray and CT
image datasets indicate that linear support vector machine and neural network
models achieve the best results as measured by accuracy, sensitivity,
specificity, and AUC. Also it is found that predictive uncertainty estimates
are much higher for CT images compared to X-ray images.",False,True,"The integration of molecular-biology reasoning from Paper 1 with the uncertainty-aware transfer learning framework from Paper 2 could lead to a novel approach for predicting COVID-19 progression based on genetic markers and imaging data, making it feasible, useful, and innovative.",
neg-1s-69,1,,0910.5052v3,"Backaction of a charge detector on a double quantum dot; We develop a master equation approach to study the backaction of quantum
point contact (QPC) on a double quantum dot (DQD) at zero bias voltage. We
reveal why electrons can pass through the zero-bias DQD only when the bias
voltage across the QPC exceeds a threshold value determined by the eigenstate
energy difference of the DQD. This derived excitation condition agrees well
with experiments on QPC-induced inelastic electron tunneling through a DQD [S.
Gustavsson et al., Phys. Rev. Lett. 99, 206804(2007)]. Moreover, we propose a
new scheme to generate a pure spin current by the QPC in the absence of a
charge current.",2407.08174v1,"An Adaptively Weighted Averaging Method for Regional Time Series
  Extraction of fMRI-based Brain Decoding; Brain decoding that classifies cognitive states using the functional
fluctuations of the brain can provide insightful information for understanding
the brain mechanisms of cognitive functions. Among the common procedures of
decoding the brain cognitive states with functional magnetic resonance imaging
(fMRI), extracting the time series of each brain region after brain
parcellation traditionally averages across the voxels within a brain region.
This neglects the spatial information among the voxels and the requirement of
extracting information for the downstream tasks. In this study, we propose to
use a fully connected neural network that is jointly trained with the brain
decoder to perform an adaptively weighted average across the voxels within each
brain region. We perform extensive evaluations by cognitive state decoding,
manifold learning, and interpretability analysis on the Human Connectome
Project (HCP) dataset. The performance comparison of the cognitive state
decoding presents an accuracy increase of up to 5\% and stable accuracy
improvement under different time window sizes, resampling sizes, and training
data sizes. The results of manifold learning show that our method presents a
considerable separability among cognitive states and basically excludes
subject-specific information. The interpretability analysis shows that our
method can identify reasonable brain regions corresponding to each cognitive
state. Our study would aid the improvement of the basic pipeline of fMRI
processing.",False,False,"The concepts from the two papers—quantum dot behavior and fMRI brain decoding—are too disparate to create a cohesive multidisciplinary research idea that meets all standards. While both involve advanced technology, they do not share a common problem or application that would allow for a novel and feasible integration.",
neg-1s-70,1,,hep-ph/9706440v1,"Large Squark-Mixing Impact on H^+ Decay in the MSSM; We study the decays of the charged Higgs boson H^+ within the Minimal
Supersymmetric Standard Model. We find that the supersymetric mode 'stop +
sbottom-bar' can dominate the H^+ decays in a wide range of the model
parameters due to the large Yukawa couplings and mixings of stop and sbottom.
Compared to the conventional modes 'tau^+ nu' and 't b-bar', this mode has very
distinctive signatures. This could have a decisive impact on H^+ searches at
future colliders. We find also that the QCD corrections to the 'stop +
sbottom-bar' mode are significant, but that they do not invalidate our
tree-level conclusion above.",2307.11553v1,"Adaptively switching between a particle marginal Metropolis-Hastings and
  a particle Gibbs kernel in SMC$^2$; Sequential Monte Carlo squared (SMC$^2$; Chopin et al., 2012) methods can be
used to sample from the exact posterior distribution of intractable likelihood
state space models. These methods are the SMC analogue to particle Markov chain
Monte Carlo (MCMC; Andrieu et al., 2010) and rely on particle MCMC kernels to
mutate the particles at each iteration. Two options for the particle MCMC
kernels are particle marginal Metropolis-Hastings (PMMH) and particle Gibbs
(PG). We introduce a method to adaptively select the particle MCMC kernel at
each iteration of SMC$^2$, with a particular focus on switching between a PMMH
and PG kernel. The resulting method can significantly improve the efficiency of
SMC$^2$ compared to using a fixed particle MCMC kernel throughout the
algorithm. Code for our methods is available at
https://github.com/imkebotha/kernel_switching_smc2.",False,False,"The concepts from the two papers—charged Higgs boson decays and adaptive particle MCMC methods—do not converge into a feasible, novel, and useful multidisciplinary research idea. The areas are too distinct, lacking a clear application or experimental validation that combines both fields effectively.",
neg-1s-71,1,,2010.10327v1,"Can Steering Wheel Detect Your Driving Fatigue?; Automated Driving System (ADS) has attracted increasing attention from both
industrial and academic communities due to its potential for increasing the
safety, mobility and efficiency of existing transportation systems. The
state-of-the-art ADS follows the human-in-the-loop (HITL) design, where the
driver's anomalous behaviour is closely monitored by the system. Though many
approaches have been proposed for detecting driver fatigue, they largely depend
on vehicle driving parameters and facial features, which lacks reliability.
Approaches using physiological based sensors (e.g., electroencephalogram or
electrocardiogram) are either too clumsy to wear or impractical to install. In
this paper, we propose a novel driver fatigue detection method by embedding
surface electromyography (sEMG) sensors on a steering wheel. Compared with the
existing methods, our approach is able to collect bio-signals in a
non-intrusive way and detect driver fatigue at an earlier stage. The
experimental results show that our approach outperforms existing methods with
the weighted average F1 scores about 90%. We also propose promising future
directions to deploy this approach in real-life settings, such as applying
multimodal learning using several supplementary sensors.",2211.11179v1,"Spatio-temporal point processes with deep non-stationary kernels; Point process data are becoming ubiquitous in modern applications, such as
social networks, health care, and finance. Despite the powerful expressiveness
of the popular recurrent neural network (RNN) models for point process data,
they may not successfully capture sophisticated non-stationary dependencies in
the data due to their recurrent structures. Another popular type of deep model
for point process data is based on representing the influence kernel (rather
than the intensity function) by neural networks. We take the latter approach
and develop a new deep non-stationary influence kernel that can model
non-stationary spatio-temporal point processes. The main idea is to approximate
the influence kernel with a novel and general low-rank decomposition, enabling
efficient representation through deep neural networks and computational
efficiency and better performance. We also take a new approach to maintain the
non-negativity constraint of the conditional intensity by introducing a
log-barrier penalty. We demonstrate our proposed method's good performance and
computational efficiency compared with the state-of-the-art on simulated and
real data.",False,True,"The integration of sEMG sensors for detecting driver fatigue (Paper 1) with non-stationary point process modeling (Paper 2) can lead to a novel system that predicts fatigue-related driving incidents by analyzing spatio-temporal patterns in driver behavior, making it both feasible and useful.",
neg-1s-72,1,,2112.05308v2,"Option Pricing with State-dependent Pricing Kernel; We introduce a new volatility model for option pricing that combines Markov
switching with the Realized GARCH framework. This leads to a novel pricing
kernel with a state-dependent variance risk premium and a pricing formula for
European options, which is derived with an analytical approximation method. We
apply the Markov switching Realized GARCH model to S&P 500 index options from
1990 to 2019 and find that investors' aversion to volatility-specific risk is
time-varying. The proposed framework outperforms competing models and reduces
(in-sample and out-of-sample) option pricing errors by 15% or more.",2001.10749v1,"Photonic Realization of a Quantum Finite Automaton; We describe a physical implementation of a quantum finite automaton
recognizing a well known family of periodic languages. The realization exploits
the polarization degree of freedom of single photons and their manipulation
through linear optical elements. We use techniques of confidence amplification
to reduce the acceptance error probability of the automaton. It is worth
remarking that the quantum finite automaton we physically realize is not only
interesting per se, but it turns out to be a crucial building block in many
quantum finite automaton design frameworks theoretically settled in the
literature.",False,False,"While both papers introduce innovative concepts, the integration of a volatility model for option pricing and a quantum finite automaton does not yield a clear, practical application that meets all standards of feasibility, novelty, and usefulness in a multidisciplinary context.",
neg-1s-73,1,,1609.00977v1,"Modeling sRNA-regulated Plasmid Maintenance; We study a theoretical model for the toxin-antitoxin (hok/sok) mechanism for
plasmid maintenance in bacteria. Toxin-antitoxin systems enforce the
maintenance of a plasmid through post-segregational killing of cells that have
lost the plasmid. Key to their function is the tight regulation of expression
of a protein toxin by an sRNA antitoxin. Here, we focus on the nonlinear nature
of the regulatory circuit dynamics of the toxin-antitoxin mechanism. The
mechanism relies on a transient increase in protein concentration rather than
on the steady state of the genetic circuit. Through a systematic analysis of
the parameter dependence of this transient increase, we confirm some known
design features of this system and identify new ones: for an efficient
toxin-antitoxin mechanism, the synthesis rate of the toxin's mRNA template
should be lower that of the sRNA antitoxin, the mRNA template should be more
stable than the sRNA antitoxin, and the mRNA-sRNA complex should be more stable
than the sRNA antitoxin. Moreover, a short half-life of the protein toxin is
also beneficial to the function of the toxin-antitoxin system. In addition, we
study a therapeutic scenario in which a competitor mRNA is introduced to
sequester the sRNA antitoxin, causing the toxic protein to be expressed.",2310.08704v2,"How Does Artificial Intelligence Improve Human Decision-Making? Evidence
  from the AI-Powered Go Program; We study how humans learn from AI, leveraging an introduction of an
AI-powered Go program (APG) that unexpectedly outperformed the best
professional player. We compare the move quality of professional players to
APG's superior solutions around its public release. Our analysis of 749,190
moves demonstrates significant improvements in players' move quality,
especially in the early stages of the game where uncertainty is highest. This
improvement was accompanied by a higher alignment with AI's suggestions and a
decreased number and magnitude of errors. Young players show greater
improvement, suggesting potential inequality in learning from AI. Further,
while players of all skill levels benefit, less skilled players gain higher
marginal benefits. These findings have implications for managers seeking to
adopt and utilize AI in their organizations.",False,False,"While both papers explore complex systems—biological regulation and AI decision-making—their integration does not yield a clear, novel, and feasible research idea. The concepts do not align sufficiently to create a practical application that addresses a specific problem effectively.",
neg-1s-74,1,,1802.10117v3,"Economic Implications of Blockchain Platforms; In an economy with asymmetric information, the smart contract in the
blockchain protocol mitigates uncertainty. Since, as a new trading platform,
the blockchain triggers segmentation of market and differentiation of agents in
both the sell and buy sides of the market, it recomposes the asymmetric
information and generates spreads in asset price and quality between itself and
a traditional platform. We show that marginal innovation and sophistication of
the smart contract have non-monotonic effects on the trading value in the
blockchain platform, its fundamental value, the price of cryptocurrency, and
consumers' welfare. Moreover, a blockchain manager who controls the level of
the innovation of the smart contract has an incentive to keep it lower than the
first best when the underlying information asymmetry is not severe, leading to
welfare loss for consumers.",2001.06348v2,"Preservation of Equations by Monoidal Monads; If a monad $T$ is monoidal, then operations on a set $X$ can be lifted
canonically to operations on $TX$. In this paper we study structural properties
under which $T$ preserves equations between those operations. It has already
been shown that any monoidal monad preserves linear equations; affine monads
preserve drop equations (where some variable appears only on one side, such as
$x\cdot y = y$) and relevant monads preserve dup equations (where some variable
is duplicated, such as $x \cdot x = x$). We start the paper by showing a
converse: if the monad at hand preserves a drop equation, then it must be
affine. From this, we show that the problem whether a given (drop) equation is
preserved is undecidable. A converse for relevance turns out to be more subtle:
preservation of certain dup equations implies a weaker notion which we call
$n$-relevance. Finally, we identify the subclass of equations such that their
preservation is equivalent to relevance.",False,False,"The concepts from the two papers do not align in a way that creates a feasible, novel, and useful multidisciplinary research idea. The economic implications of blockchain and the structural properties of monoidal monads are too disparate to synthesize into a coherent research hypothesis that meets all standards.",
neg-1s-75,1,,1907.05387v1,"An alternative for the average income estimation using small area
  methods; The average household income is one of the most important indexes for
decision making and the modelling of economic inequity and poverty. In this
work we propose a practical procedure to estimate the average income using
small area methods. We illustrate our proposal using information from a
multipurpose survey and suitable economic and demographic variables such as the
multidimensional poverty and the valorization indexes and the official
population projections. We find that the standard relative errors for the
income average estimates improve substantially when the proposed methodology is
implemented.",2105.02567v4,"Floer Homology: From Generalized Morse-Smale Dynamical Systems to
  Forman's Combinatorial Vector Fields; We construct a Floer type boundary operator for generalised Morse-Smale
dynamical systems on compact smooth manifolds by counting the number of
suitable flow lines between closed (both homoclinic and periodic) orbits and
isolated critical points. The same principle works for the discrete situation
of general combinatorial vector fields, defined by Forman, on CW complexes. We
can thus recover the $\mathbb{Z}_2$ homology of both smooth and discrete
structures directly from the flow lines (V-paths) of our vector field.",False,False,"The concepts from the two papers do not align in a way that allows for a feasible and novel multidisciplinary research idea. The first paper focuses on economic modeling, while the second deals with mathematical structures in dynamical systems, lacking a clear intersection for practical application.",
neg-1s-76,1,,0705.2504v1,"Nonlinear Relaxation Dynamics in Elastic Networks and Design Principles
  of Molecular Machines; Analyzing nonlinear conformational relaxation dynamics in elastic networks
corresponding to two classical motor proteins, we find that they respond by
well-defined internal mechanical motions to various initial deformations and
that these motions are robust against external perturbations. We show that this
behavior is not characteristic for random elastic networks. However, special
network architectures with such properties can be designed by evolutionary
optimization methods. Using them, an example of an artificial elastic network,
operating as a cyclic machine powered by ligand binding, is constructed.",2302.08921v1,"Deep Implicit Distribution Alignment Networks for Cross-Corpus Speech
  Emotion Recognition; In this paper, we propose a novel deep transfer learning method called deep
implicit distribution alignment networks (DIDAN) to deal with cross-corpus
speech emotion recognition (SER) problem, in which the labeled training
(source) and unlabeled testing (target) speech signals come from different
corpora. Specifically, DIDAN first adopts a simple deep regression network
consisting of a set of convolutional and fully connected layers to directly
regress the source speech spectrums into the emotional labels such that the
proposed DIDAN can own the emotion discriminative ability. Then, such ability
is transferred to be also applicable to the target speech samples regardless of
corpus variance by resorting to a well-designed regularization term called
implicit distribution alignment (IDA). Unlike widely-used maximum mean
discrepancy (MMD) and its variants, the proposed IDA absorbs the idea of sample
reconstruction to implicitly align the distribution gap, which enables DIDAN to
learn both emotion discriminative and corpus invariant features from speech
spectrums. To evaluate the proposed DIDAN, extensive cross-corpus SER
experiments on widely-used speech emotion corpora are carried out. Experimental
results show that the proposed DIDAN can outperform lots of recent
state-of-the-art methods in coping with the cross-corpus SER tasks.",False,False,"While both papers present interesting concepts, the integration of nonlinear dynamics in elastic networks with deep learning for speech emotion recognition does not yield a clear, feasible, and novel research idea that meets all the specified standards. The connection between the two fields lacks a practical application.",
neg-1s-77,1,,2303.08874v2,"Bayesian Quadrature for Neural Ensemble Search; Ensembling can improve the performance of Neural Networks, but existing
approaches struggle when the architecture likelihood surface has dispersed,
narrow peaks. Furthermore, existing methods construct equally weighted
ensembles, and this is likely to be vulnerable to the failure modes of the
weaker architectures. By viewing ensembling as approximately marginalising over
architectures we construct ensembles using the tools of Bayesian Quadrature --
tools which are well suited to the exploration of likelihood surfaces with
dispersed, narrow peaks. Additionally, the resulting ensembles consist of
architectures weighted commensurate with their performance. We show empirically
-- in terms of test likelihood, accuracy, and expected calibration error --
that our method outperforms state-of-the-art baselines, and verify via ablation
studies that its components do so independently.",2210.16640v1,"2D and 3D CT Radiomic Features Performance Comparison in
  Characterization of Gastric Cancer: A Multi-center Study; Objective: Radiomics, an emerging tool for medical image analysis, is
potential towards precisely characterizing gastric cancer (GC). Whether using
one-slice 2D annotation or whole-volume 3D annotation remains a long-time
debate, especially for heterogeneous GC. We comprehensively compared 2D and 3D
radiomic features' representation and discrimination capacity regarding GC, via
three tasks.
  Methods: Four-center 539 GC patients were retrospectively enrolled and
divided into the training and validation cohorts. From 2D or 3D regions of
interest (ROIs) annotated by radiologists, radiomic features were extracted
respectively. Feature selection and model construction procedures were customed
for each combination of two modalities (2D or 3D) and three tasks.
Subsequently, six machine learning models (Model_2D^LNM, Model_3D^LNM;
Model_2D^LVI, Model_3D^LVI; Model_2D^pT, Model_3D^pT) were derived and
evaluated to reflect modalities' performances in characterizing GC.
Furthermore, we performed an auxiliary experiment to assess modalities'
performances when resampling spacing is different.
  Results: Regarding three tasks, the yielded areas under the curve (AUCs)
were: Model_2D^LNM's 0.712 (95% confidence interval, 0.613-0.811),
Model_3D^LNM's 0.680 (0.584-0.775); Model_2D^LVI's 0.677 (0.595-0.761),
Model_3D^LVI's 0.615 (0.528-0.703); Model_2D^pT's 0.840 (0.779-0.901),
Model_3D^pT's 0.813 (0.747-0.879). Moreover, the auxiliary experiment indicated
that Models_2D are statistically more advantageous than Models3D with different
resampling spacings.
  Conclusion: Models constructed with 2D radiomic features revealed comparable
performances with those constructed with 3D features in characterizing GC.
  Significance: Our work indicated that time-saving 2D annotation would be the
better choice in GC, and provided a related reference to further
radiomics-based researches.",False,True,"By integrating Bayesian Quadrature methods from Paper 1 with radiomic feature analysis from Paper 2, a novel approach could be developed to optimize the selection of 2D and 3D radiomic features for gastric cancer characterization, enhancing model performance and validation through empirical testing.",
neg-1s-78,1,,1707.01028v1,"Multi-state models for evaluating conversion options in life insurance; In this paper we propose a multi-state model for the evaluation of the
conversion option contract. The multi-state model is based on age-indexed
semi-Markov chains that are able to reproduce many important aspects that
influence the valuation of the option such as the duration problem, the time
non-homogeneity and the ageing effect. The value of the conversion option is
evaluated after the formal description of this contract.",1805.00497v1,"Expression profiles of TRPV1, TRPV4, TLR4 and ERK1/2 in the dorsal root
  ganglionic neurons of a cancer-induced neuropathy rat model; Background: The spread of tumors through neural routes is common in several
types of cancer in which patients suffer from a moderate-to-severe neuropathy,
neural damage and a distorted quality of life. Here we aim to examine the
expression profiles of transient receptor potential vanilloid 1 (TRPV1) and of
transient receptor potential vanilloid 4 (TRPV4), toll-like receptor 4 (TLR4)
and extracellular signal-regulated kinase (ERK1/2), and to assess the possible
therapeutic strategies through blockade of transient receptor potential (TRP)
channels. Methods: Cancer was induced within the sciatic nerves of male
Copenhagen rats, and tissues from dorsal root ganglia (DRG) were collected and
used for measurements of immunofluorescence and Western blotting. The TRPV1
antagonist capsazepine, the selective TRPV4 antagonist HC-067047 and the
calcium ions inhibitor ruthenium red were used to treat thermal and/or
mechanical hyperalgesia. Results: Transient receptor potential vanilloid 1
showed a lower expression in DRGs on days 7 and 14. The expression of TRPV4,
TLR4 and ERK1/2 showed an increase on day 3 then a decrease on days 7 and 14.
TRPV1 and TLR4 as well as TRPV4 and ERK1/2 co-existed on the same neuronal
cells. The neuropathic pain was reversed in dose-dependent manners by using the
TRP antagonists and the calcium ions inhibitor. Conclusion: The decreased
expression of TRPV1 and TRPV4 is associated with high activation. The increased
expression of TLR4 and ERK1/2 reveals earlier immune response and tumor
progression, respectively, and their ultimate decrease is an indicator of nerve
damage. We studied the possible role of TRPV1 and TRPV4 in transducing
cancer-induced hyperalgesia. The possible treatment strategies of
cancer-induced thermal and/or mechanical hyperalgesia using capsazepine,
HC-067047 and ruthenium red are examined.",False,False,"The concepts from the two papers do not align in a way that creates a feasible multidisciplinary research idea. The first paper focuses on life insurance modeling, while the second addresses cancer-induced neuropathy. Their integration does not yield a novel or useful hypothesis applicable to both fields.",
neg-1s-79,1,,2408.05764v1,"A robust baro-radar-inertial odometry m-estimator for multicopter
  navigation in cities and forests; Search and rescue operations require mobile robots to navigate unstructured
indoor and outdoor environments. In particular, actively stabilized multirotor
drones need precise movement data to balance and avoid obstacles. Combining
radial velocities from on-chip radar with MEMS inertial sensing has proven to
provide robust, lightweight, and consistent state estimation, even in visually
or geometrically degraded environments. Statistical tests robustify these
estimators against radar outliers. However, available work with binary outlier
filters lacks adaptability to various hardware setups and environments. Other
work has predominantly been tested in handheld static environments or
automotive contexts. This work introduces a robust baro-radar-inertial odometry
(BRIO) m-estimator for quadcopter flights in typical GNSS-denied scenarios.
Extensive real-world closed-loop flights in cities and forests demonstrate
robustness to moving objects and ghost targets, maintaining a consistent
performance with 0.5 % to 3.2 % drift per distance traveled. Benchmarks on
public datasets validate the system's generalizability. The code, dataset, and
video are available at https://github.com/ethz-asl/rio.",2112.13087v2,"A semi-bijective algorithm for saturated extended 2-regular simple
  stacks; Combinatorics of biopolymer structures, especially enumeration of various RNA
secondary structures and protein contact maps, is of significant interest for
communities of both combinatorics and computational biology. However, most of
the previous combinatorial enumeration results for these structures are
presented in terms of generating functions, and few are explicit formulas. This
paper is mainly concerned with finding explicit enumeration formulas for a
particular class of biologically relevant structures, say, saturated 2-regular
simple stacks, whose configuration is related to protein folds in the 2D
honeycomb lattice. We establish a semi-bijective algorithm that converts
saturated 2-regular simple stacks into forests of small trees, which produces a
uniform formula for saturated extended 2-regular simple stacks with any of the
six primary component types. Summarizing the six different primary component
types, we obtain a bivariate explicit formula for saturated extended 2-regular
simple stacks with $n$ vertices and $k$ arcs. As consequences, the uniform
formula can be reduced to Clote's results on $k$-saturated 2-regular simple
stacks and the optimal 2-regular simple stacks, and Guo et al.'s result on the
optimal extended 2-regular simple stacks.",False,False,"While both papers address complex problems in their respective fields, the integration of drone navigation technology with combinatorial enumeration of biopolymer structures lacks a clear, practical application. The concepts do not converge in a way that meets the standards of feasibility, novelty, and usefulness.",
neg-1s-80,1,,2304.12447v1,"Predicting Pulmonary Hypertension by Electrocardiograms Using Machine
  Learning; Pulmonary hypertension (PH) is a condition of high blood pressure that
affects the arteries in the lungs and the right side of the heart (Mayo Clinic,
2017). A mean pulmonary artery pressure greater than 25 mmHg is defined as
Pulmonary hypertension. The estimated 5-year survival rate from the time of
diagnosis of pulmonary hypertension is only 57% without therapy and patients
with right heart failure only survive for approximately 1 year without
treatment (Benza et al., 2012). Given the indolent nature of the disease, early
detection of PH remains a challenge leading to delays in therapy.
Echocardiography is currently used as a screening tool for diagnosing PH.
However, electrocardiography (ECG), a more accessible, simple to use, and
cost-effective tool compared to echocardiography, is less studied and explored
for screening at-risk patients for PH. The goal of this project is to create a
neural network model which can process an ECG signal and detect the presence of
PH with a confidence probability. I created a dense neural network (DNN) model
that has an accuracy of 98% over the available training sample. For future
steps, the current model will be updated with a model suited for time-series
data. To balance the dataset with proper training samples, I will generate
additional data using data augmentation techniques. Through early and accurate
detection of conditions such as PH, we widen the spectrum of innovation in
detecting chronic life-threatening health conditions and reduce associated
mortality and morbidity.",2106.05240v2,"A Century of Economic Policy Uncertainty Through the French-Canadian
  Lens; A novel token-distance-based triple approach is proposed for identifying EPU
mentions in textual documents. The method is applied to a corpus of
French-language news to construct a century-long historical EPU index for the
Canadian province of Quebec. The relevance of the index is shown in a
macroeconomic nowcasting experiment.",False,False,"While both papers address significant issues—healthcare diagnostics and economic policy uncertainty—they do not present overlapping concepts that can be feasibly combined into a novel multidisciplinary research idea. The integration of machine learning in healthcare does not directly relate to the historical analysis of economic policy, limiting the potential for a cohesive and innovative research project.",
neg-1s-81,1,,2304.07851v1,"Study on the tea market in India; India's tea business has a long history and plays a significant role in the
economics of the nation. India is the world's second-largest producer of tea,
with Assam and Darjeeling being the most well-known tea-growing regions. Since
the British introduced tea cultivation to India in the 1820s, the nation has
produced tea. Millions of people are employed in the tea sector today, and it
contributes significantly to the Indian economy in terms of revenue. The
production of tea has changed significantly in India over the years, moving
more and more towards organic and sustainable practices. The industry has also
had to deal with difficulties like competition from other nations that produce
tea, varying tea prices, and labor-related problems. Despite these obstacles,
the Indian tea business is still growing and produces a wide variety of teas,
such as black tea, green tea, and chai tea. Additionally, the sector encourages
travel through ""tea tourism,"" which allows tourists to see how tea is made and
discover its origins in India. Overall, India's tea business continues to play
a significant role in its history, culture, and economy.",2406.11429v1,"Fusion Makes Perfection: An Efficient Multi-Grained Matching Approach
  for Zero-Shot Relation Extraction; Predicting unseen relations that cannot be observed during the training phase
is a challenging task in relation extraction. Previous works have made progress
by matching the semantics between input instances and label descriptions.
However, fine-grained matching often requires laborious manual annotation, and
rich interactions between instances and label descriptions come with
significant computational overhead. In this work, we propose an efficient
multi-grained matching approach that uses virtual entity matching to reduce
manual annotation cost, and fuses coarse-grained recall and fine-grained
classification for rich interactions with guaranteed inference speed.
Experimental results show that our approach outperforms the previous State Of
The Art (SOTA) methods, and achieves a balance between inference efficiency and
prediction accuracy in zero-shot relation extraction tasks. Our code is
available at https://github.com/longls777/EMMA.",False,False,"The concepts from the tea market and zero-shot relation extraction do not naturally intersect to form a feasible, novel, and useful multidisciplinary research idea. While both papers address distinct challenges, they lack a clear connection that would allow for a meaningful synthesis of ideas.",
neg-1s-82,1,,1606.03950v8,"The real parts of the nontrivial Riemann zeta function zeros; This theorem is based on holomorphy of studied functions and the fact that
near a singularity point the real part of some rational function can take an
arbitrary preassigned value.",2404.03707v1,"Investigating the Robustness of Counterfactual Learning to Rank Models:
  A Reproducibility Study; Counterfactual learning to rank (CLTR) has attracted extensive attention in
the IR community for its ability to leverage massive logged user interaction
data to train ranking models. While the CLTR models can be theoretically
unbiased when the user behavior assumption is correct and the propensity
estimation is accurate, their effectiveness is usually empirically evaluated
via simulation-based experiments due to a lack of widely-available,
large-scale, real click logs. However, the mainstream simulation-based
experiments are somewhat limited as they often feature a single, deterministic
production ranker and simplified user simulation models to generate the
synthetic click logs. As a result, the robustness of CLTR models in complex and
diverse situations is largely unknown and needs further investigation.
  To address this problem, in this paper, we aim to investigate the robustness
of existing CLTR models in a reproducibility study with extensive
simulation-based experiments that (1) use both deterministic and stochastic
production rankers, each with different ranking performance, and (2) leverage
multiple user simulation models with different user behavior assumptions. We
find that the DLA models and IPS-DCM show better robustness under various
simulation settings than IPS-PBM and PRS with offline propensity estimation.
Besides, the existing CLTR models often fail to outperform the naive click
baselines when the production ranker has relatively high ranking performance or
certain randomness, which suggests an urgent need for developing new CLTR
algorithms that work for these settings.",False,False,"The concepts from the two papers do not align well enough to create a novel multidisciplinary research idea. The mathematical focus on the Riemann zeta function does not directly translate into practical applications in counterfactual learning to rank models, limiting feasibility and usefulness.",
neg-1s-83,1,,2402.16375v1,"Valuing insurance against small probability risks: A meta-analysis; The demand for voluntary insurance against low-probability, high-impact risks
is lower than expected. To assess the magnitude of the demand, we conduct a
meta-analysis of contingent valuation studies using a dataset of experimentally
elicited and survey-based estimates. We find that the average stated
willingness to pay (WTP) for insurance is 87% of expected losses. We perform a
meta-regression analysis to examine the heterogeneity in aggregate WTP across
these studies. The meta-regression reveals that information about loss
probability and probability levels positively influence relative willingness to
pay, whereas respondents' average income and age have a negative effect.
Moreover, we identify cultural sub-factors, such as power distance and
uncertainty avoidance, that provided additional explanations for differences in
WTP across international samples. Methodological factors related to the
sampling and data collection process significantly influence the stated WTP.
Our results, robust to model specification and publication bias, are relevant
to current debates on stated preferences for low-probability risks management.",2302.10521v1,"Cognitive characteristics of intellectually gifted children with a
  diagnosis of ADHD; Some children may be intellectually gifted, and yet experience behavioral and
academic difficulties. We examined 82 twice exceptional children (2eADHD),
having an excellent General Ability Index (GAI) derived from the Wechsler
Intelligence Scale for Children-IV (GAI >= 125), and a diagnosis of Attention
Deficit and Hyperactivity Disorder (ADHD). They accounted for 8.8% of a large
sample of children with ADHD, which is twice as high as the proportion of
intellectually gifted children in a typical population. This
over-representation does not reflect a misdiagnosis of ADHD, as these children
showed the typical features predicted on the grounds of data regarding the ADHD
sample, including lower scores in working memory and processing speed measures,
combined with the inclusion criteria for giftedness. Based on information
concerning intellectually gifted children with either a Specific Learning
Disorder (SLD) or typical development, we observed that these characteristics
of intelligence are similar to those seen in SLD, but not in typical
development, irrespective of whether 2e-ADHD children had a comorbid SLD.",False,False,While both papers address important issues—insurance demand for low-probability risks and the cognitive characteristics of gifted children with ADHD—they do not present overlapping concepts that can be feasibly combined into a novel multidisciplinary research idea that meets all the specified standards.,
neg-1s-84,1,,2402.05573v1,"Research on the evolution of domestic multi-functional meter technology; The technical evolution of domestic multi-functional electricity meter is
deeply discussed. With the rapid development of the domestic power market and
the continuous innovation of technology, the domestic multi-functional
electricity meters have experienced the transformation from simple billing to
complex multi-functional, from a single application to a wide range of fields.
This transformation has not only driven the rapid development of electricity
meter technology, but also met the increasing power demand and management
requirements. This paper expounds the concept of multi-function meter, the
working principle and algorithm of digital multiplier, the initiation and
evolution of multi-function electricity meter standard, and the initiation and
evolution of domestic multi-function electricity meter products. Although the
domestic independent production of multi-functional meter has made great
achievements in performance, but in the reliability and key process technology
still need to be improved. In addition, the development of communication
technology also provides a new opportunity for the progress of electricity
meter technology. The application of the new technology provides a more
convenient and efficient way for the data transmission and remote management of
electricity meters. Domestic multi-functional electricity meters have made
remarkable achievements in technology evolution and application and expansion,
but they still face some challenges and opportunities. In the future, with the
continuous development of the power market and the promotion of smart grid
construction, domestic multi-functional electricity meters need to continue to
strengthen technological innovation and product research and development,
improve the reliability and competitiveness of products, in order to meet
higher application needs and market requirements.",1408.4221v2,"Evolution of regulatory networks towards adaptability and stability in a
  changing environment; Diverse biological networks exhibit universal features distinguished from
those of random networks, calling much attention to their origins and
implications. Here we propose a minimal evolution model of Boolean regulatory
networks, which evolve by selectively rewiring links towards enhancing
adaptability to a changing environment and stability against dynamical
perturbations. We find that sparse and heterogeneous connectivity patterns
emerge, which show qualitative agreement with real transcriptional regulatory
networks and metabolic networks. The characteristic scaling behavior of
stability reflects the balance between robustness and flexibility. The scaling
of fluctuation in the perturbation spread shows a dynamic crossover, which is
analyzed by investigating separately the stochasticity of internal dynamics and
the network structures different depending on the evolution pathways. Our study
delineates how the ambivalent pressure of evolution shapes biological networks,
which can be helpful for studying general complex systems interacting with
environments.",False,False,"While both papers discuss evolution and adaptability, the concepts from domestic multi-functional meters and biological regulatory networks do not easily converge into a novel, feasible research idea that meets all standards. The technological and biological domains are too distinct for effective integration in this context.",
neg-1s-85,1,,1801.06423v1,"Minimum spanning tree release under differential privacy constraints; We investigate the problem of nodes clustering under privacy constraints when
representing a dataset as a graph. Our contribution is threefold. First we
formally define the concept of differential privacy for structured databases
such as graphs, and give an alternative definition based on a new neighborhood
notion between graphs. This definition is adapted to particular frameworks that
can be met in various application fields such as genomics, world wide web,
population survey, etc. Second, we introduce a new algorithm to tackle the
issue of privately releasing an approximated minimum spanning tree topology for
a simple-undirected-weighted graph. It provides a simple way of producing the
topology of a private almost minimum spanning tree which outperforms, in most
cases, the state of the art ""Laplace mechanism"" in terms of
weight-approximation error.
  Finally, we propose a theoretically motivated method combining a sanitizing
mechanism (such as Laplace or our new algorithm) with a Minimum Spanning Tree
(MST)-based clustering algorithm. It provides an accurate method for nodes
clustering in a graph while keeping the sensitive information contained in the
edges weights of the private graph. We provide some theoretical results on the
robustness of an almost minimum spanning tree construction for Laplace
sanitizing mechanisms. These results exhibit which conditions the graph weights
should respect in order to consider that the nodes form well separated clusters
both for Laplace and our algorithm as sanitizing mechanism. The method has been
experimentally evaluated on simulated data, and preliminary results show the
good behavior of the algorithm while identifying well separated clusters.",1701.05008v2,"Secret Key Agreement under Discussion Rate Constraints; For the multiterminal secret key agreement problem, new single-letter lower
bounds are obtained on the public discussion rate required to achieve any given
secret key rate below the secrecy capacity. The results apply to general source
model without helpers or wiretapper's side information but can be strengthened
for hypergraphical sources. In particular, for the pairwise independent
network, the results give rise to a complete characterization of the maximum
secret key rate achievable under a constraint on the total discussion rate.",False,True,"The combination of differential privacy in graph clustering from Paper 1 and secret key agreement under discussion rate constraints from Paper 2 can lead to a novel research idea focused on secure and private clustering of sensitive data in communication networks, which is both feasible and useful.",
neg-1s-86,1,,1410.6097v2,"Freeness of automata groups vs boundary dynamics; We prove that the boundary dynamics of the (semi)group generated by the
enriched dual transducer characterizes the algebraic property of being free for
an automaton group. We specialize this result to the class of bireversible
transducers and we show that the property of being not free is equivalent to
have a finite Schreier graph in the boundary of the enriched dual pointed on
some essentially non-trivial point. From these results we derive some
consequences from the dynamical, algorithmic and algebraic point of view. In
the last part of the paper we address the problem of finding examples of
non-bireversible transducers defining free groups, we show examples of
transducers with sink accessible from every state which generate free groups,
and, in general, we link this problem to the nonexistence of certain words with
interesting combinatorial and geometrical properties.",2201.03213v1,"New volatility evolution model after extreme events; In this paper, we propose a new dynamical model to study the two-stage
volatility evolution of stock market index after extreme events, and find that
the volatility after extreme events follows a stretched exponential decay in
the initial stage and becomes a power law decay at later times by using high
frequency minute data. Empirical study of the evolutionary behaviors of
volatility after endogenous and exogenous events further demonstrates the
descriptive power of our new model. To further explore the underlying
mechanisms of volatility evolution, we introduce the sequential arrival of
information hypothesis (SAIH) and the mixture of distribution hypothesis (MDH)
to test the two-stage assumption, and find that investors transform from the
uninformed state to the informed state in the first stage and informed
investors subsequently dominate in the second stage. The testing results offer
a supporting explanation for the validity of our new model and the fitted
values of relevant parameters.",False,False,"The concepts from the two papers—automata groups and stock market volatility—are too disparate to create a cohesive multidisciplinary research idea that meets all standards. While both involve dynamics, they do not lend themselves to a novel, feasible, and useful integration that addresses a common problem.",
neg-1s-87,1,,2108.03690v1,"Enhanced Invertible Encoding for Learned Image Compression; Although deep learning based image compression methods have achieved
promising progress these days, the performance of these methods still cannot
match the latest compression standard Versatile Video Coding (VVC). Most of the
recent developments focus on designing a more accurate and flexible entropy
model that can better parameterize the distributions of the latent features.
However, few efforts are devoted to structuring a better transformation between
the image space and the latent feature space. In this paper, instead of
employing previous autoencoder style networks to build this transformation, we
propose an enhanced Invertible Encoding Network with invertible neural networks
(INNs) to largely mitigate the information loss problem for better compression.
Experimental results on the Kodak, CLIC, and Tecnick datasets show that our
method outperforms the existing learned image compression methods and
compression standards, including VVC (VTM 12.1), especially for high-resolution
images. Our source code is available at https://github.com/xyq7/InvCompress.",2203.05595v1,"Social Networks and Spatial Mobility: Evidence from Facebook in India; This paper studies the role of social networks in spatial mobility across
India. Using aggregated and de-identified data from the world's largest online
social network, we (i) document new descriptive findings on the structure of
social networks and spatial mobility in India; (ii) quantify the effects of
social networks on annual migration choice; and (iii) embed these estimates in
a spatial equilibrium model to study the wage implications of increasing social
connectedness. Across millions of individuals, we find that multiple measures
of social capital are concentrated among the rich and educated and among
migrants. Across destinations, both mobility patterns and social networks are
concentrated toward richer areas. A model of migration suggests individuals are
indifferent between a 10% increase in destination wages and a 12-16% increase
in destination social networks. Accounting for networks reduces the
migration-distance relationship by 19%. In equilibrium, equalizing social
networks across locations improves average wages by 3% (24% for the bottom
wage-quartile), a larger impact than removing the marginal cost of distance. We
find evidence of an economic support mechanism, with destination economic
improvements reducing the migration-network elasticity. We also find suggestive
evidence for an emotional support mechanism from qualitative surveys among
Facebook users. Difference-in-difference estimates suggest college attendance
delivers a 20% increase in network size and diversity. Taken together, our data
suggest that - by reducing effective moving costs - increasing social
connectedness across space may have considerable economic gains.",False,False,"While both papers present interesting concepts, combining image compression techniques with social network analysis does not yield a clear, feasible, and novel research idea that addresses a specific problem effectively. The domains are too disparate to create a meaningful multidisciplinary project.",
neg-1s-88,1,,2003.07860v1,"NISE Estimation of an Economic Model of Crime; An economic model of crime is used to explore the consistent estimation of a
simultaneous linear equation without recourse to instrumental variables. A
maximum-likelihood procedure (NISE) is introduced, and its results are compared
to ordinary least squares and two-stage least squares. The paper is motivated
by previous research on the crime model and by the well-known practical problem
that valid instruments are frequently unavailable.",1907.06637v1,"The Bach Doodle: Approachable music composition with machine learning at
  scale; To make music composition more approachable, we designed the first AI-powered
Google Doodle, the Bach Doodle, where users can create their own melody and
have it harmonized by a machine learning model Coconet (Huang et al., 2017) in
the style of Bach. For users to input melodies, we designed a simplified
sheet-music based interface. To support an interactive experience at scale, we
re-implemented Coconet in TensorFlow.js (Smilkov et al., 2019) to run in the
browser and reduced its runtime from 40s to 2s by adopting dilated depth-wise
separable convolutions and fusing operations. We also reduced the model
download size to approximately 400KB through post-training weight quantization.
We calibrated a speed test based on partial model evaluation time to determine
if the harmonization request should be performed locally or sent to remote TPU
servers. In three days, people spent 350 years worth of time playing with the
Bach Doodle, and Coconet received more than 55 million queries. Users could
choose to rate their compositions and contribute them to a public dataset,
which we are releasing with this paper. We hope that the community finds this
dataset useful for applications ranging from ethnomusicological studies, to
music education, to improving machine learning models.",False,False,"While both papers introduce interesting concepts, the combination of an economic model of crime and AI-driven music composition does not yield a feasible or novel multidisciplinary research idea that effectively addresses a specific problem. The connection between the two fields is tenuous and lacks practical application.",
neg-1s-89,1,,1010.2826v1,"Tau Be or not Tau Be? - A Perspective on Service Compatibility and
  Substitutability; One of the main open research issues in Service Oriented Computing is to
propose automated techniques to analyse service interfaces. A first problem,
called compatibility, aims at determining whether a set of services (two in
this paper) can be composed together and interact with each other as expected.
Another related problem is to check the substitutability of one service with
another. These problems are especially difficult when behavioural descriptions
(i.e., message calls and their ordering) are taken into account in service
interfaces. Interfaces should capture as faithfully as possible the service
behaviour to make their automated analysis possible while not exhibiting
implementation details. In this position paper, we choose Labelled Transition
Systems to specify the behavioural part of service interfaces. In particular,
we show that internal behaviours (tau transitions) are necessary in these
transition systems in order to detect subtle errors that may occur when
composing a set of services together. We also show that tau transitions should
be handled differently in the compatibility and substitutability problem: the
former problem requires to check if the compatibility is preserved every time a
tau transition is traversed in one interface, whereas the latter requires a
precise analysis of tau branchings in order to make the substitution preserve
the properties (e.g., a compatibility notion) which were ensured before
replacement.",2301.02954v1,"A New Noncoherent Gaussian Signaling Scheme for Low Probability of
  Detection Communications; We propose a novel, Gaussian signaling mechanism for low probability of
detection (LPD) communication systems with either single or multiple antennas.
The new scheme is designed to allow the noncoherent detection of
Gaussian-distributed signals, enabling LPD communications using signals that
follow the complex Gaussian distribution in the time and frequency domains. It
is demonstrated via simulations that the proposed scheme achieves better
performance than a comparable conventional scheme over the entire SNR region,
with the advantage becoming more significant in scenarios with lower overhead.",False,False,"While both papers address complex systems—service-oriented computing and low probability of detection communications—they do not present overlapping concepts that can be feasibly combined into a novel research idea. The domains are too distinct, lacking a clear intersection for multidisciplinary innovation.",
neg-1s-90,1,,2010.15966v1,"Machine Learning for Experimental Design: Methods for Improved Blocking; Restricting randomization in the design of experiments (e.g., using
blocking/stratification, pair-wise matching, or rerandomization) can improve
the treatment-control balance on important covariates and therefore improve the
estimation of the treatment effect, particularly for small- and medium-sized
experiments. Existing guidance on how to identify these variables and implement
the restrictions is incomplete and conflicting. We identify that differences
are mainly due to the fact that what is important in the pre-treatment data may
not translate to the post-treatment data. We highlight settings where there is
sufficient data to provide clear guidance and outline improved methods to
mostly automate the process using modern machine learning (ML) techniques. We
show in simulations using real-world data, that these methods reduce both the
mean squared error of the estimate (14%-34%) and the size of the standard error
(6%-16%).",2208.11847v1,"CNN-based Prediction of Network Robustness With Missing Edges; Connectivity and controllability of a complex network are two important
issues that guarantee a networked system to function. Robustness of
connectivity and controllability guarantees the system to function properly and
stably under various malicious attacks. Evaluating network robustness using
attack simulations is time consuming, while the convolutional neural network
(CNN)-based prediction approach provides a cost-efficient method to approximate
the network robustness. In this paper, we investigate the performance of
CNN-based approaches for connectivity and controllability robustness
prediction, when partial network information is missing, namely the adjacency
matrix is incomplete. Extensive experimental studies are carried out. A
threshold is explored that if a total amount of more than 7.29\% information is
lost, the performance of CNN-based prediction will be significantly degenerated
for all cases in the experiments. Two scenarios of missing edge representations
are compared, 1) a missing edge is marked `no edge' in the input for
prediction, and 2) a missing edge is denoted using a special marker of
`unknown'. Experimental results reveal that the first representation is
misleading to the CNN-based predictors.",False,True,"The combination of machine learning methods for experimental design from Paper 1 and CNN-based predictions of network robustness from Paper 2 can lead to a novel approach for optimizing experimental designs in networked systems. This idea is feasible, as it can be validated through simulations and experiments, and addresses real-world issues in network robustness.",
neg-1s-91,1,,2311.18301v2,"Rainbow common graphs must be forests; We study the rainbow version of the graph commonness property: a graph $H$ is
$r$-rainbow common if the number of rainbow copies of $H$ (where all edges have
distinct colors) in an $r$-coloring of edges of $K_n$ is maximized
asymptotically by independently coloring each edge uniformly at random. $H$ is
\emph{$r$-rainbow uncommon} otherwise. We show that if $H$ has a cycle, then it
is $r$-rainbow uncommon for every $r$ at least the number of edges of $H$. This
generalizes a result of Erd\H{o}s and Hajnal, and proves a conjecture of De
Silva, Si, Tait, Tun\c{c}bilek, Yang, and Young.",2102.04296v3,"Data-driven design of targeted gene panels for estimating immunotherapy
  biomarkers; We introduce a novel data-driven framework for the design of targeted gene
panels for estimating exome-wide biomarkers in cancer immunotherapy. Our first
goal is to develop a generative model for the profile of mutation across the
exome, which allows for gene- and variant type-dependent mutation rates. Based
on this model, we then propose a new procedure for estimating biomarkers such
as tumour mutation burden and tumour indel nurden. Our approach allows the
practitioner to select a targeted gene panel of a prespecified size, and then
construct an estimator that only depends on the selected genes. Alternatively,
the practitioner may apply our method to make predictions based on an existing
gene panel, or to augment a gene panel to a given size. We demonstrate the
excellent performance of our proposal using data from three non-small cell lung
cancer studies, as well as data from six other cancer types.",False,False,"The concepts from the two papers—graph theory and cancer immunotherapy—do not easily converge into a novel multidisciplinary idea that meets all standards. While both involve complex systems, the lack of a clear, practical application that combines their methodologies limits feasibility and usefulness.",
neg-1s-92,1,,2412.17822v1,"Emergent poverty traps and inequality at multiple levels impedes social
  mobility; Eradicating extreme poverty and inequality are the key leverage points to
achieve the seventeen Sustainable Development goals. Yet, the reduction in
extreme poverty and inequality are vulnerable to shocks such as the pandemic
and climate change. We find that that these vulnerabilities emerge from the
interaction between individual and institutional mechanisms. Individual
characteristics like risk aversion, attention, and saving propensity can lead
to sub-optimal diversification and low capital accumulation. These individual
drivers are reinforced by institutional mechanisms such as lack of financial
inclusion, access to technology, and economic segregation, leading to
persistent inequality and poverty traps. Our experiments demonstrate that
addressing above factors yields 'double dividend' - reducing poverty and
inequality within-and-between communities and create positive feedback that can
withstand shocks.",2204.11425v2,"BCI: Breast Cancer Immunohistochemical Image Generation through Pyramid
  Pix2pix; The evaluation of human epidermal growth factor receptor 2 (HER2) expression
is essential to formulate a precise treatment for breast cancer. The routine
evaluation of HER2 is conducted with immunohistochemical techniques (IHC),
which is very expensive. Therefore, for the first time, we propose a breast
cancer immunohistochemical (BCI) benchmark attempting to synthesize IHC data
directly with the paired hematoxylin and eosin (HE) stained images. The dataset
contains 4870 registered image pairs, covering a variety of HER2 expression
levels. Based on BCI, as a minor contribution, we further build a pyramid
pix2pix image generation method, which achieves better HE to IHC translation
results than the other current popular algorithms. Extensive experiments
demonstrate that BCI poses new challenges to the existing image translation
research. Besides, BCI also opens the door for future pathology studies in HER2
expression evaluation based on the synthesized IHC images. BCI dataset can be
downloaded from https://bupt-ai-cz.github.io/BCI.",False,False,"While both papers address significant issues—poverty and inequality in Paper 1, and breast cancer diagnosis in Paper 2—their intersection does not yield a feasible, novel, and useful multidisciplinary research idea. The concepts do not synergize effectively to create a new solution or approach.",
neg-1s-93,1,,1307.0781v1,"Distributed Online Big Data Classification Using Context Information; Distributed, online data mining systems have emerged as a result of
applications requiring analysis of large amounts of correlated and
high-dimensional data produced by multiple distributed data sources. We propose
a distributed online data classification framework where data is gathered by
distributed data sources and processed by a heterogeneous set of distributed
learners which learn online, at run-time, how to classify the different data
streams either by using their locally available classification functions or by
helping each other by classifying each other's data. Importantly, since the
data is gathered at different locations, sending the data to another learner to
process incurs additional costs such as delays, and hence this will be only
beneficial if the benefits obtained from a better classification will exceed
the costs. We model the problem of joint classification by the distributed and
heterogeneous learners from multiple data sources as a distributed contextual
bandit problem where each data is characterized by a specific context. We
develop a distributed online learning algorithm for which we can prove
sublinear regret. Compared to prior work in distributed online data mining, our
work is the first to provide analytic regret results characterizing the
performance of the proposed algorithm.",1811.10274v1,"Sound Approximation of Programs with Elementary Functions; Elementary function calls are a common feature in numerical programs. While
their implementions in library functions are highly optimized, their
computation is nonetheless very expensive compared to plain arithmetic. Full
accuracy is, however, not always needed. Unlike arithmetic, where the
performance difference between for example single and double precision
floating-point arithmetic is relatively small, elementary function calls
provide a much richer tradeoff space between accuracy and efficiency.
Navigating this space is challenging. First, generating approximations of
elementary function calls which are guaranteed to satisfy accuracy error bounds
is highly nontrivial. Second, the performance of such approximations generally
depends on several parameters which are unintuitive to choose manually,
especially for non-experts.
  We present a fully automated approach and tool which approximates elementary
function calls inside small programs while guaranteeing overall user provided
error bounds. Our tool leverages existing techniques for roundoff error
computation and approximation of individual elementary function calls, and
provides automated selection of many parameters. Our experiments show that
significant efficiency improvements are possible in exchange for reduced, but
guaranteed, accuracy.",False,True,"The combination of distributed online data classification and automated approximation of elementary functions can lead to a novel framework that optimizes data processing efficiency while maintaining accuracy. This multidisciplinary approach is feasible, as it can be validated through experiments in real-time data environments.",
neg-1s-94,1,,2410.08061v3,"A study of nil Hecke algebras via Hopf algebroids; Hopf algebroids are generalizations of Hopf algebras to less commutative
settings. We show how the comultiplication defined by Kostant and Kumar turns
the affine nil Hecke algebra associated to a Coxeter system into a Hopf
algebroid without an antipode. The proof relies on mixed dihedral braid
relations between Demazure operators and simple reflections. For researchers
new to Hopf algebroids we include additional examples from ring theory,
representation theory, and algebraic geometry.",1905.01644v1,"Testable Properties in General Graphs and Random Order Streaming; We present a novel framework closely linking the areas of property testing
and data streaming algorithms in the setting of general graphs. It has been
recently shown (Monemizadeh et al. 2017) that for bounded-degree graphs, any
constant-query tester can be emulated in the random order streaming model by a
streaming algorithm that uses only space required to store a constant number of
words. However, in a more natural setting of general graphs, with no
restriction on the maximum degree, no such results were known because of our
lack of understanding of constant-query testers in general graphs and lack of
techniques to appropriately emulate in the streaming setting off-line
algorithms allowing many high-degree vertices.
  In this work we advance our understanding on both of these challenges. First,
we provide canonical testers for all constant-query testers for general graphs,
both, for one-sided and two-sided errors. Such canonizations were only known
before (in the adjacency matrix model) for dense graphs (Goldreich and Trevisan
2003) and (in the adjacency list model) for bounded degree (di-)graphs
(Goldreich and Ron 2011, Czumaj et al. 2016). Using the concept of canonical
testers, we then prove that every property of general graphs that is
constant-query testable with one-sided error can also be tested in
constant-space with one-sided error in the random order streaming model.
  Our results imply, among others, that properties like $(s,t)$
disconnectivity, $k$-path-freeness, etc. are constant-space testable in random
order streams.",False,False,"The concepts from the two papers—Hopf algebroids and property testing in graphs—do not easily converge into a novel multidisciplinary idea that meets all standards. While both areas are rich, their integration lacks a clear experimental validation pathway and practical application that addresses a specific problem.",
neg-1s-95,1,,2003.09300v1,"Graham's Formula for Valuing Growth Stocks; Benjamin Graham introduced a very simple formula for valuing a growth stock
in 1962. How does it work and why? What is a sensible way to calculate this
across many stocks and provide a scoring system to compare stocks amongst each
other? We are presenting a methodology here which is put into practice.",2411.01710v1,"SPES: Spectrogram Perturbation for Explainable Speech-to-Text Generation; Spurred by the demand for interpretable models, research on eXplainable AI
for language technologies has experienced significant growth, with feature
attribution methods emerging as a cornerstone of this progress. While prior
work in NLP explored such methods for classification tasks and textual
applications, explainability intersecting generation and speech is lagging,
with existing techniques failing to account for the autoregressive nature of
state-of-the-art models and to provide fine-grained, phonetically meaningful
explanations. We address this gap by introducing Spectrogram Perturbation for
Explainable Speech-to-text Generation (SPES), a feature attribution technique
applicable to sequence generation tasks with autoregressive models. SPES
provides explanations for each predicted token based on both the input
spectrogram and the previously generated tokens. Extensive evaluation on speech
recognition and translation demonstrates that SPES generates explanations that
are faithful and plausible to humans.",False,False,"The concepts from the two papers do not align in a way that creates a feasible, novel, and useful multidisciplinary research idea. While one focuses on stock valuation and the other on explainable AI in speech recognition, their intersection lacks a clear application or innovative approach that meets all standards.",
neg-1s-96,1,,2305.03818v3,"The Generalized Makeev Problem Revisited; Based on a result of Makeev, in 2012 Blagojevi\'c and Karasev proposed the
following problem: given any positive integers $m$ and $1\leq \ell\leq k$, find
the minimum dimension $d=\Delta(m;\ell/k)$ such that for any $m$ mass
distributions on $\mathbb{R}^d$, there exist $k$ hyperplanes, any $\ell$ of
which equipartition each mass. The $\ell=k$ case is a central question in
geometric and topological combinatorics which remains open except for few
values of $m$ and $k$. For $\ell< k$ and arbitrary $m$, we establish new upper
bounds on $\Delta(m;\ell/k)$ when (1) $\ell=2$ and $k$ is arbitrary and (2)
$\ell=3$ and $k=4$. When $\ell=k-1$ and $m+1$ is a power of two these bounds
are nearly optimal and are exponentially smaller than the current best upper
bounds when $\ell=k$. Similar remarks apply to our upper bounds when the
hyperplanes are prescribed to be pairwise orthogonal. Lastly, we provide
transversal extensions of our results along the lines recently established by
Frick et al.: given $m$ families of compact convex sets in $\mathbb{R}^d$ such
that no $2^\ell$ members of any family are pairwise disjoint, we show that
every member of each family is pierced by the union of any $\ell$ of some
collection of $k$ hyperplanes.",2305.14131v2,"Temporally Causal Discovery Tests for Discrete Time Series and Neural
  Spike Trains; We consider the problem of detecting causal relationships between discrete
time series, in the presence of potential confounders. A hypothesis test is
introduced for identifying the temporally causal influence of $(x_n)$ on
$(y_n)$, causally conditioned on a possibly confounding third time series
$(z_n)$. Under natural Markovian modeling assumptions, it is shown that the
null hypothesis, corresponding to the absence of temporally causal influence,
is equivalent to the underlying `causal conditional directed information rate'
being equal to zero. The plug-in estimator for this functional is identified
with the log-likelihood ratio test statistic for the desired test. This
statistic is shown to be asymptotically normal under the alternative hypothesis
and asymptotically $\chi^2$ distributed under the null, facilitating the
computation of $p$-values when used on empirical data. The effectiveness of the
resulting hypothesis test is illustrated on simulated data, validating the
underlying theory. The test is also employed in the analysis of spike train
data recorded from neurons in the V4 and FEF brain regions of behaving animals
during a visual attention task. There, the test results are seen to identify
interesting and biologically relevant information.",False,True,"The combination of geometric combinatorics from Paper 1 and causal discovery from Paper 2 can lead to a novel research idea that explores the causal relationships between spatial distributions of mass and their geometric properties. This idea is feasible, as it can be validated through experimental simulations and has potential applications in various fields, including physics and neuroscience.",
neg-1s-97,1,,1807.05786v4,"MIDV-500: A Dataset for Identity Documents Analysis and Recognition on
  Mobile Devices in Video Stream; A lot of research has been devoted to identity documents analysis and
recognition on mobile devices. However, no publicly available datasets designed
for this particular problem currently exist. There are a few datasets which are
useful for associated subtasks but in order to facilitate a more comprehensive
scientific and technical approach to identity document recognition more
specialized datasets are required. In this paper we present a Mobile Identity
Document Video dataset (MIDV-500) consisting of 500 video clips for 50
different identity document types with ground truth which allows to perform
research in a wide scope of document analysis problems. The paper presents
characteristics of the dataset and evaluation results for existing methods of
face detection, text line recognition, and document fields data extraction.
Since an important feature of identity documents is their sensitiveness as they
contain personal data, all source document images used in MIDV-500 are either
in public domain or distributed under public copyright licenses.
  The main goal of this paper is to present a dataset. However, in addition and
as a baseline, we present evaluation results for existing methods for face
detection, text line recognition, and document data extraction, using the
presented dataset.
  (The dataset is available for download at ftp://smartengines.com/midv-500/.)",2111.14281v1,"Passive Indoor Localization with WiFi Fingerprints; This paper proposes passive WiFi indoor localization. Instead of using WiFi
signals received by mobile devices as fingerprints, we use signals received by
routers to locate the mobile carrier. Consequently, software installation on
the mobile device is not required. To resolve the data insufficiency problem,
flow control signals such as request to send (RTS) and clear to send (CTS) are
utilized. In our model, received signal strength indicator (RSSI) and channel
state information (CSI) are used as fingerprints for several algorithms,
including deterministic, probabilistic and neural networks localization
algorithms. We further investigated localization algorithms performance through
extensive on-site experiments with various models of phones at hundreds of
testing locations. We demonstrate that our passive scheme achieves an average
localization error of 0.8 m when the phone is actively transmitting data frames
and 1.5 m when it is not transmitting data frames.",False,True,"By combining the MIDV-500 dataset for identity document recognition with passive WiFi localization techniques, a novel research idea could emerge that focuses on enhancing security and user verification in indoor environments. This approach is feasible, experimental, and addresses real-world problems in identity verification and location tracking.",
neg-1s-98,1,,2407.13220v3,"MEDIC: Zero-shot Music Editing with Disentangled Inversion Control; Text-guided diffusion models make a paradigm shift in audio generation,
facilitating the adaptability of source audio to conform to specific textual
prompts. Recent works introduce inversion techniques, like DDIM inversion, to
zero-shot editing, exploiting pretrained diffusion models for audio
modification. Nonetheless, our investigation exposes that DDIM inversion
suffers from an accumulation of errors across each diffusion step, undermining
its efficacy. Moreover, existing editing methods fail to achieve effective
complex non-rigid music editing while maintaining essential content
preservation and high editing fidelity. To counteract these issues, we
introduce the Disentangled Inversion technique to disentangle the diffusion
process into triple branches, rectifying the deviated path of the source branch
caused by DDIM inversion. In addition, we propose the Harmonized Attention
Control framework, which unifies the mutual self-attention control and
cross-attention control with an intermediate Harmonic Branch to progressively
achieve the desired harmonic and melodic information in the target music.
Collectively, these innovations comprise the Disentangled Inversion Control
(DIC) framework, enabling accurate music editing while safeguarding content
integrity. To benchmark audio editing efficacy, we introduce ZoME-Bench, a
comprehensive music editing benchmark hosting 1,100 samples spread across ten
distinct editing categories. This facilitates both zero-shot and
instruction-based music editing tasks. Our method achieves unparalleled
performance in edit fidelity and essential content preservation, outperforming
contemporary state-of-the-art inversion techniques.",2502.01640v1,"Study on the impact of trade policy uncertainty on the performance of
  enterprise ESG performance; Trade policy uncertainty has become a significant feature of today's global
economy. While its impact on free trade is evident, its microeconomic effects
remain open to debate. This study explores the influence of trade policy
uncertainty on corporate ESG performance and its underlying mechanisms, using
data from A-share listed companies in China from 2010 to 2020. The findings
reveal that increased trade policy uncertainty significantly and robustly
enhances corporate ESG performance. Heterogeneity analysis indicates that
high-tech enterprises are better equipped to improve their ESG performance in
response to trade policy uncertainty. Furthermore, strengthening internal
controls and appointing CEOs with environmental backgrounds also help firms
seize the opportunities arising from trade policy uncertainty. In terms of
mechanisms, trade policy uncertainty intensifies industry competition,
compelling firms to enhance their ESG performance to gain market share.
Additionally, it stimulates green technological innovation, further optimizing
ESG outcomes. Therefore, efforts should focus on improving the ESG standards
system, establishing ESG incentive policies, increasing the transparency and
predictability of trade policies, and promoting corporate green development to
advance national sustainable development goals.",False,False,"The concepts from the two papers—music editing using advanced AI techniques and the impact of trade policy uncertainty on corporate ESG performance—do not converge in a way that creates a feasible, novel, and useful multidisciplinary research idea. The domains are too disparate to synthesize effectively.",
neg-1s-99,1,,2407.07395v1,"Standard compliant video coding using low complexity, switchable neural
  wrappers; The proliferation of high resolution videos posts great storage and bandwidth
pressure on cloud video services, driving the development of next-generation
video codecs. Despite great progress made in neural video coding, existing
approaches are still far from economical deployment considering the complexity
and rate-distortion performance tradeoff. To clear the roadblocks for neural
video coding, in this paper we propose a new framework featuring standard
compatibility, high performance, and low decoding complexity. We employ a set
of jointly optimized neural pre- and post-processors, wrapping a standard video
codec, to encode videos at different resolutions. The rate-distorion optimal
downsampling ratio is signaled to the decoder at the per-sequence level for
each target rate. We design a low complexity neural post-processor architecture
that can handle different upsampling ratios. The change of resolution exploits
the spatial redundancy in high-resolution videos, while the neural wrapper
further achieves rate-distortion performance improvement through end-to-end
optimization with a codec proxy. Our light-weight post-processor architecture
has a complexity of 516 MACs / pixel, and achieves 9.3% BD-Rate reduction over
VVC on the UVG dataset, and 6.4% on AOM CTC Class A1. Our approach has the
potential to further advance the performance of the latest video coding
standards using neural processing with minimal added complexity.",1707.04179v1,"Cost-Effective Cache Deployment in Mobile Heterogeneous Networks; This paper investigates one of the fundamental issues in cache-enabled
heterogeneous networks (HetNets): how many cache instances should be deployed
at different base stations, in order to provide guaranteed service in a
cost-effective manner. Specifically, we consider two-tier HetNets with
hierarchical caching, where the most popular files are cached at small cell
base stations (SBSs) while the less popular ones are cached at macro base
stations (MBSs). For a given network cache deployment budget, the cache sizes
for MBSs and SBSs are optimized to maximize network capacity while satisfying
the file transmission rate requirements. As cache sizes of MBSs and SBSs affect
the traffic load distribution, inter-tier traffic steering is also employed for
load balancing. Based on stochastic geometry analysis, the optimal cache sizes
for MBSs and SBSs are obtained, which are threshold-based with respect to cache
budget in the networks constrained by SBS backhauls. Simulation results are
provided to evaluate the proposed schemes and demonstrate the applications in
cost-effective network deployment.",False,True,"The combination of neural video coding techniques from Paper 1 with cost-effective cache deployment strategies from Paper 2 can lead to a novel approach for optimizing video delivery in heterogeneous networks. This idea is feasible, as it can be validated through experiments on network performance and video quality.",
