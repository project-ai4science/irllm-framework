paper_a_id,method,research_type,paper_b_id,paper_b_text,paper_c_id,paper_c_text,y_true,y_pred,reason,paperA
neg-1s-0,1,,2101.03214v1,"Exploring the association between R&D expenditure and the job quality in
  the European Union; Investment in research and development is a key factor in increasing
countries' competitiveness. However, its impact can potentially be broader and
include other socially relevant elements like job quality. In effect, the
quantity of generated jobs is an incomplete indicator since it does not allow
to conclude on the quality of the job generated. In this sense, this paper
intends to explore the relevance of R&D investments for the job quality in the
European Union between 2009 and 2018. For this purpose, we investigate the
effects of R&D expenditures made by the business sector, government, and higher
education sector on three dimensions of job quality. Three research methods are
employed, i.e. univariate linear analysis, multiple linear analysis, and
cluster analysis. The findings only confirm the association between R&D
expenditure and the number of hours worked, such that the European Union
countries with the highest R&D expenses are those with the lowest average
weekly working hours.",2101.03259v2,"Ramadan and Infants Health Outcomes; Previous studies show that prenatal shocks to embryos could have adverse
impacts on health endowment at birth. Using the universe of birth data and a
difference-in-difference-in-difference strategy, I find that exposure to
Ramadan during prenatal development has negative birth outcomes. Exposure to a
full month of fasting is associated with 96 grams lower birth-weight. These
results are robust across specifications and do not appear to be driven by
mothers selective fertility.",False,False,"The two papers address distinct domains—economic R&D impacts on job quality and prenatal health effects of Ramadan fasting—making it challenging to combine their concepts into a novel, feasible, and useful multidisciplinary research idea that meets all the required standards.",
neg-1s-1,1,,2211.02854v3,"Rate-Distortion Optimized Post-Training Quantization for Learned Image
  Compression; Quantizing a floating-point neural network to its fixed-point representation
is crucial for Learned Image Compression (LIC) because it improves decoding
consistency for interoperability and reduces space-time complexity for
implementation. Existing solutions often have to retrain the network for model
quantization, which is time-consuming and impractical to some extent. This work
suggests using Post-Training Quantization (PTQ) to process pretrained,
off-the-shelf LIC models. We theoretically prove that minimizing
quantization-induced mean square error (MSE) of model parameters (e.g., weight,
bias, and activation) in PTQ is sub-optimal for compression tasks and thus
develop a novel Rate-Distortion (R-D) Optimized PTQ (RDO-PTQ) to best retain
the compression performance. Given a LIC model, RDO-PTQ layer-wisely determines
the quantization parameters to transform the original floating-point parameters
in 32-bit precision (FP32) to fixed-point ones at 8-bit precision (INT8), for
which a tiny calibration image set is compressed in optimization to minimize
R-D loss. Experiments reveal the outstanding efficiency of the proposed method
on different LICs, showing the closest coding performance to their
floating-point counterparts. Our method is a lightweight and plug-and-play
approach without retraining model parameters but just adjusting quantization
parameters, which is attractive to practitioners. Such an RDO-PTQ is a
task-oriented PTQ scheme, which is then extended to quantize popular
super-resolution and image classification models with negligible performance
loss, further evidencing the generalization of our methodology. Related
materials will be released at https://njuvision.github.io/RDO-PTQ.",2211.04238v1,"HDRfeat: A Feature-Rich Network for High Dynamic Range Image
  Reconstruction; A major challenge for high dynamic range (HDR) image reconstruction from
multi-exposed low dynamic range (LDR) images, especially with dynamic scenes,
is the extraction and merging of relevant contextual features in order to
suppress any ghosting and blurring artifacts from moving objects. To tackle
this, in this work we propose a novel network for HDR reconstruction with deep
and rich feature extraction layers, including residual attention blocks with
sequential channel and spatial attention. For the compression of the
rich-features to the HDR domain, a residual feature distillation block (RFDB)
based architecture is adopted. In contrast to earlier deep-learning methods for
HDR, the above contributions shift focus from merging/compression to feature
extraction, the added value of which we demonstrate with ablation experiments.
We present qualitative and quantitative comparisons on a public benchmark
dataset, showing that our proposed method outperforms the state-of-the-art.",False,False,"The combination of ideas from both papers remains within the realm of image processing and machine learning, lacking integration from distinct disciplines. Additionally, the proposed integration may not sufficiently meet novelty and multidisciplinary standards.",
neg-1s-2,1,,2408.07227v1,"Stablecoin Runs and Disclosure Policy in the Presence of Large Sales; Stablecoins have historically depegged due from par to large sales, possibly
of speculative nature, or poor reserve asset quality. Using a global game which
addresses both concerns, we show that the selling pressure on stablecoin
holders increases in the presence of a large sale. While precise public
knowledge reduces (increases) the probability of a run when fundamentals are
strong (weak), interestingly, more precise private signals increase (reduce)
the probability of a run when fundamentals are strong (weak), potentially
explaining the stability of opaque stablecoins. The total run probability can
be decomposed into components representing risks from large sales and poor
collateral. By analyzing how these risk components vary with respect to
information uncertainty and fundamentals, we can split the fundamental space
into regions based on the type of risk a stablecoin issuer is more prone to. We
suggest testable implications and connect our model's implications to
real-world applications, including depegging events and the no-questions-asked
property of money.",2408.08866v1,"High-Frequency Options Trading | With Portfolio Optimization; This paper explores the effectiveness of high-frequency options trading
strategies enhanced by advanced portfolio optimization techniques,
investigating their ability to consistently generate positive returns compared
to traditional long or short positions on options. Utilizing SPY options data
recorded in five-minute intervals over a one-month period, we calculate key
metrics such as Option Greeks and implied volatility, applying the Binomial
Tree model for American options pricing and the Newton-Raphson algorithm for
implied volatility calculation. Investment universes are constructed based on
criteria like implied volatility and Greeks, followed by the application of
various portfolio optimization models, including Standard Mean-Variance and
Robust Methods. Our research finds that while basic long-short strategies
centered on implied volatility and Greeks generally underperform, more
sophisticated strategies incorporating advanced Greeks, such as Vega and Rho,
along with dynamic portfolio optimization, show potential in effectively
navigating the complexities of the options market. The study highlights the
importance of adaptability and responsiveness in dynamic portfolio strategies
within the high-frequency trading environment, particularly under volatile
market conditions. Future research could refine strategy parameters and explore
less frequently traded options, offering new insights into high-frequency
options trading and portfolio management.",False,False,"Combining stablecoin run dynamics with high-frequency options trading does not clearly meet all multidisciplinary standards, as the concepts may not integrate seamlessly to form a novel, feasible, and useful research idea.",
neg-1s-3,1,,2405.17656v1,"Alignment is Key for Applying Diffusion Models to Retrosynthesis; Retrosynthesis, the task of identifying precursors for a given molecule, can
be naturally framed as a conditional graph generation task. Diffusion models
are a particularly promising modelling approach, enabling post-hoc conditioning
and trading off quality for speed during generation. We show mathematically
that permutation equivariant denoisers severely limit the expressiveness of
graph diffusion models and thus their adaptation to retrosynthesis. To address
this limitation, we relax the equivariance requirement such that it only
applies to aligned permutations of the conditioning and the generated graphs
obtained through atom mapping. Our new denoiser achieves the highest top-$1$
accuracy ($54.7$\%) across template-free and template-based methods on
USPTO-50k. We also demonstrate the ability for flexible post-training
conditioning and good sample quality with small diffusion step counts,
highlighting the potential for interactive applications and additional controls
for multi-step planning.",2405.18051v3,"Predicting Progression Events in Multiple Myeloma from Routine Blood
  Work; The ability to accurately predict disease progression is paramount for
optimizing multiple myeloma patient care. This study introduces a hybrid neural
network architecture, combining Long Short-Term Memory networks with a
Conditional Restricted Boltzmann Machine, to predict future blood work of
affected patients from a series of historical laboratory results. We
demonstrate that our model can replicate the statistical moments of the time
series ($0.95~\pm~0.01~\geq~R^2~\geq~0.83~\pm~0.03$) and forecast future blood
work features with high correlation to actual patient data
($0.92\pm0.02~\geq~r~\geq~0.52~\pm~0.09$). Subsequently, a second Long
Short-Term Memory network is employed to detect and annotate disease
progression events within the forecasted blood work time series. We show that
these annotations enable the prediction of progression events with significant
reliability (AUROC$~=~0.88~\pm~0.01$), up to 12 months in advance
(AUROC($t+12~$mos)$~=0.65~\pm~0.01$). Our system is designed in a modular
fashion, featuring separate entities for forecasting and progression event
annotation. This structure not only enhances interpretability but also
facilitates the integration of additional modules to perform subsequent
operations on the generated outputs. Our approach utilizes a minimal set of
routine blood work measurements, which avoids the need for expensive or
resource-intensive tests and ensures accessibility of the system in clinical
routine. This capability allows for individualized risk assessment and making
informed treatment decisions tailored to a patient's unique disease kinetics.
The represented approach contributes to the development of a scalable and
cost-effective virtual human twin system for optimized healthcare resource
utilization and improved patient outcomes in multiple myeloma care.",False,False,"The abstracts belong to distinct domains (computational chemistry and medical informatics) with limited overlapping concepts, making it challenging to combine them into a novel, feasible, and useful multidisciplinary research idea that meets all the specified standards.",
neg-1s-4,1,,1701.03139v2,"Bounding, an accessible method for estimating principal causal effects,
  examined and explained; Estimating treatment effects for subgroups defined by post-treatment behavior
(i.e., estimating causal effects in a principal stratification framework) can
be technically challenging and heavily reliant on strong assumptions. We
investigate an alternative path: using bounds to identify ranges of possible
effects that are consistent with the data. This simple approach relies on fewer
assumptions and yet can result in policy-relevant findings. As we show,
covariates can be used to substantially tighten bounds in a straightforward
manner. Via simulation, we demonstrate which types of covariates are maximally
beneficial. We conclude with an analysis of a multi-site experimental study of
Early College High Schools. When examining the program's impact on students
completing the ninth grade ""on-track"" for college, we find little impact for
ECHS students who would otherwise attend a high quality high school, but
substantial effects for those who would not. This suggests potential benefit in
expanding these programs in areas primarily served by lower quality schools.",1701.03569v1,"Bivariate Discrete Generalized Exponential Distribution; In this paper we develop a bivariate discrete generalized exponential
distribution, whose marginals are discrete generalized exponential distribution
as proposed by Nekoukhou, Alamatsaz and Bidram (""Discrete generalized
exponential distribution of a second type"", Statistics, 47, 876 - 887, 2013).
It is observed that the proposed bivariate distribution is a very flexible
distribution and the bivariate geometric distribution can be obtained as a
special case of this distribution. The proposed distribution can be seen as a
natural discrete analogue of the bivariate generalized exponential distribution
proposed by Kundu and Gupta (""Bivariate generalized exponential distribution"",
Journal of Multivariate Analysis, 100, 581 - 593, 2009). We study different
properties of this distribution and explore its dependence structures. We
propose a new EM algorithm to compute the maximum likelihood estimators of the
unknown parameters which can be implemented very efficiently, and discuss some
inferential issues also. The analysis of one data set has been performed to
show the effectiveness of the proposed model. Finally we propose some open
problems and conclude the paper.",False,True,"Combining causal bounding methods with a flexible bivariate discrete distribution can create a novel framework for estimating treatment effects with complex covariate relationships. This multidisciplinary approach is feasible, innovative, and can effectively enhance policy-relevant causal analyses.",
neg-1s-5,1,,2412.19555v1,"Asymptotic Properties of the Maximum Likelihood Estimator for
  Markov-switching Observation-driven Models; A Markov-switching observation-driven model is a stochastic process
$((S_t,Y_t))_{t \in \mathbb{Z}}$ where (i) $(S_t)_{t \in \mathbb{Z}}$ is an
unobserved Markov process taking values in a finite set and (ii) $(Y_t)_{t \in
\mathbb{Z}}$ is an observed process such that the conditional distribution of
$Y_t$ given all past $Y$'s and the current and all past $S$'s depends only on
all past $Y$'s and $S_t$. In this paper, we prove the consistency and
asymptotic normality of the maximum likelihood estimator for such model. As a
special case hereof, we give conditions under which the maximum likelihood
estimator for the widely applied Markov-switching generalised autoregressive
conditional heteroscedasticity model introduced by Haas et al. (2004b) is
consistent and asymptotic normal.",2412.21181v1,"Causal Hangover Effects; It's not unreasonable to think that in-game sporting performance can be
affected partly by what takes place off the court. We can't observe what
happens between games directly. Instead, we proxy for the possibility of
athletes partying by looking at play following games in party cities. We are
interested to see if teams exhibit a decline in performance the day following a
game in a city with active nightlife; we call this a ""hangover effect"". Part of
the question is determining a reasonable way to measure levels of nightlife,
and correspondingly which cities are notorious for it; we colloquially refer to
such cities as ""party cities"". To carry out this study, we exploit data on
bookmaker spreads: the expected score differential between two teams after
conditioning on observable performance in past games and expectations about the
upcoming game. We expect a team to meet the spread half the time, since this is
one of the easiest ways for bookmakers to guarantee a profit. We construct a
model which attempts to estimate the causal effect of visiting a ""party city""
on subsequent day performance as measured by the odds of beating the spread. In
particular, we only consider the hangover effect on games played back-to-back
within 24 hours of each other. To the extent that odds of beating the spread
against next day opponent is uncorrelated with playing in a party city the day
before, which should be the case under an efficient betting market, we have
identification in our variable of interest. We find that visiting a city with
active nightlife the day prior to a game does have a statistically significant
negative effect on a team's likelihood of meeting bookmakers' expectations for
both NBA and MLB.",False,True,"Combining Markov-switching models with sports performance analysis offers a novel, feasible approach to empirically validate hidden factors like ""hangover effects,"" effectively merging advanced statistical methods with sports economics research.",
neg-1s-6,1,,1412.0459v2,"On Bayesian based adaptive confidence sets for linear functionals; We consider the problem of constructing Bayesian based confidence sets for
linear functionals in the inverse Gaussian white noise model. We work with a
scale of Gaussian priors indexed by a regularity hyper-parameter and apply the
data-driven (slightly modified) marginal likelihood empirical Bayes method for
the choice of this hyper-parameter. We show by theory and simulations that the
credible sets constructed by this method have sub-optimal behaviour in general.
However, by assuming ""self-similarity"" the credible sets have rate-adaptive
size and optimal coverage. As an application of these results we construct
$L_{\infty}$-credible bands for the true functional parameter with adaptive
size and optimal coverage under self-similarity constraint.",1412.0705v4,"Exponentaited generalized Weibull Gompertz distribution; This paper introduces studies on exponentaited generalized Weibull Gompertz
distribution EGWGD which generalizes a lot of distributions. Several properties
of the EGWGD such as reversed (hazard) function, moments, maximum likelihood
estimation, mean residual (past) lifetime, MTTF, MTTR, MTBF, maintainability,
availability and order statistics are studied in this paper. A real data set is
analyzed and it is observed that the present distribution can provide a better
fit than some other very well known distributions",False,False,"Combining Bayesian confidence sets with a new distribution may lack clear feasibility and demonstrated novelty, failing to ensure all multidisciplinary standards are fully met.",
neg-1s-7,1,,1206.2212v3,"A simple method for finite range decomposition of quadratic forms and
  Gaussian fields; We present a simple method to decompose the Green forms corresponding to a
large class of interesting symmetric Dirichlet forms into integrals over
symmetric positive semi-definite and finite range (properly supported) forms
that are smoother than the original Green form. This result gives rise to
multiscale decompositions of the associated Gaussian free fields into sums of
independent smoother Gaussian fields with spatially localized correlations. Our
method makes use of the finite propagation speed of the wave equation and
Chebyshev polynomials. It improves several existing results and also gives
simpler proofs.",1206.2251v2,"A Necessary and Sufficient Condition for Edge Universality of Wigner
  matrices; In this paper, we prove a necessary and sufficient condition for Tracy-Widom
law of Wigner matrices. Consider $N \times N$ symmetric Wigner matrices $H$
with $H_{ij} = N^{-1/2} x_{ij}$, whose upper right entries $x_{ij}$ $(1\le i<
j\le N)$ are $i.i.d.$ random variables with distribution $\mu$ and diagonal
entries $x_{ii}$ $(1\le i\le N)$ are $i.i.d.$ random variables with
distribution $\wt \mu$. The means of $\mu$ and $\wt \mu$ are zero, the variance
of $\mu$ is 1, and the variance of $\wt \mu $ is finite. We prove that
Tracy-Widom law holds if and only if $\lim_{s\to \infty}s^4\p(|x_{12}| \ge
s)=0$. The same criterion holds for Hermitian Wigner matrices.",False,False,"The abstracts from finite range decomposition in Gaussian fields and edge universality in Wigner matrices do not clearly combine into a multidisciplinary idea that satisfies all the required standards of feasibility, novelty, and usefulness.",
neg-1s-8,1,,2105.08626v1,"Light Gradient Boosting Machine as a Regression Method for Quantitative
  Structure-Activity Relationships; In the pharmaceutical industry, where it is common to generate many QSAR
models with large numbers of molecules and descriptors, the best QSAR methods
are those that can generate the most accurate predictions but that are also
insensitive to hyperparameters and are computationally efficient. Here we
compare Light Gradient Boosting Machine (LightGBM) to random forest,
single-task deep neural nets, and Extreme Gradient Boosting (XGBoost) on 30
in-house data sets. While any boosting algorithm has many adjustable
hyperparameters, we can define a set of standard hyperparameters at which
LightGBM makes predictions about as accurate as single-task deep neural nets,
but is a factor of 1000-fold faster than random forest and ~4-fold faster than
XGBoost in terms of total computational time for the largest models. Another
very useful feature of LightGBM is that it includes a native method for
estimating prediction intervals.",2105.08835v2,"Conformational variability of loops in the SARS-CoV-2 spike protein; The SARS-CoV-2 spike (S) protein facilitates viral infection, and has been
the focus of many structure determination efforts. Its flexible loop regions
are known to be involved in protein binding and may adopt multiple
conformations. This paper identifies the S protein loops and studies their
conformational variability based on the available Protein Data Bank (PDB)
structures. While most loops had essentially one stable conformation, 17 of 44
loop regions were observed to be structurally variable with multiple
substantively distinct conformations based on a cluster analysis. Loop modeling
methods were then applied to the S protein loop targets, and the prediction
accuracies discussed in relation to the characteristics of the conformational
clusters identified. Loops with multiple conformations were found to be
challenging to model based on a single structural template.",False,True,"Combining LightGBM's efficient predictive modeling with SARS-CoV-2 spike protein loop variability offers a novel, feasible approach to accurately predict loop conformations, leveraging multidisciplinary techniques from machine learning and structural biology to address protein modeling challenges.",
neg-1s-9,1,,1603.03593v1,"Fast Detection of Block Boundaries in Block Wise Constant Matrices: An
  Application to HiC data; We propose a novel approach for estimating the location of block boundaries
(change-points) in a random matrix consisting of a block wise constant matrix
observed in white noise. Our method consists in rephrasing this task as a
variable selection issue. We use a penalized least-squares criterion with an
$\ell_1$-type penalty for dealing with this issue. We first provide some
theoretical results ensuring the consistency of our change-point estimators.
Then, we explain how to implement our method in a very efficient way. Finally,
we provide some empirical evidence to support our claims and apply our approach
to HiC data which are used in molecular biology for better understanding the
influence of the chromosomal conformation on the cells functioning.",1603.04189v2,"A Change-Point Model for Detecting Heterogeneity in Ordered Survival
  Responses; In this article we suggest a new statistical approach considering survival
heterogeneity as a breakpoint model in an ordered sequence of time to event
variables. The survival responses need to be ordered according to a numerical
covariate. Our esti- mation method will aim at detecting heterogeneity that
could arise through the or- dering covariate. We formally introduce our model
as a constrained Hidden Markov Model (HMM) where the hidden states are the
unknown segmentation (breakpoint locations) and the observed states are the
survival responses. We derive an efficient Expectation-Maximization (EM)
framework for maximizing the likelihood of this model for a wide range of
baseline hazard forms (parametrics or nonparametric). The posterior
distribution of the breakpoints is also derived and the selection of the number
of segments using penalized likelihood criterion is discussed. The performance
of our survival breakpoint model is finally illustrated on a diabetes dataset
where the observed survival times are ordered according to the calendar time of
disease onset.",False,True,"Combining change-point detection methods from matrix analysis with survival breakpoint models offers a novel, feasible approach applicable to complex biological data. This multidisciplinary idea integrates statistical techniques from both papers, is innovative, can be experimentally validated, and addresses meaningful problems in fields like genomics and epidemiology.",
neg-1s-10,1,,2407.05777v1,"Probabilistic Shoenfield Machines; This article provides the theoretical framework of Probabilistic Shoenfield
Machines (PSMs), an extension of the classical Shoenfield Machine that models
randomness in the computation process. PSMs are brought in contexts where
deterministic computation is insufficient, such as randomized algorithms. By
allowing transitions to multiple possible states with certain probabilities,
PSMs can solve problems and make decisions based on probabilistic outcomes,
hence expanding the variety of possible computations. We provide an overview of
PSMs, detailing their formal definitions as well as the computation mechanism
and their equivalence with Non-deterministic Shoenfield Machines (NSM).",2407.15721v2,"Equality of morphic sequences; Morphic sequences form a natural class of infinite sequences, typically
defined as the coding of a fixed point of a morphism. Different morphisms and
codings may yield the same morphic sequence. This paper investigates how to
prove that two such representations of a morphic sequence by morphisms
represent the same sequence. In particular, we focus on the smallest
representations of the subsequences of the binary Fibonacci sequence obtained
by only taking the even or odd elements. The proofs we give are induction
proofs of several properties simultaneously, and are typically found fully
automatically by a tool that we developed.",False,False,"The abstracts do not present a clear, feasible, and novel intersection of probabilistic computation and morphic sequence analysis that can be experimentally validated, making it unlikely to meet all multidisciplinary research standards.",
neg-1s-11,1,,2108.10449v1,"Differential Music: Automated Music Generation Using LSTM Networks with
  Representation Based on Melodic and Harmonic Intervals; This paper presents a generative AI model for automated music composition
with LSTM networks that takes a novel approach at encoding musical information
which is based on movement in music rather than absolute pitch. Melodies are
encoded as a series of intervals rather than a series of pitches, and chords
are encoded as the set of intervals that each chord note makes with the melody
at each timestep. Experimental results show promise as they sound musical and
tonal. There are also weaknesses to this method, mainly excessive modulations
in the compositions, but that is expected from the nature of the encoding. This
issue is discussed later in the paper and is a potential topic for future work.",2108.10714v1,"Curricular SincNet: Towards Robust Deep Speaker Recognition by
  Emphasizing Hard Samples in Latent Space; Deep learning models have become an increasingly preferred option for
biometric recognition systems, such as speaker recognition. SincNet, a deep
neural network architecture, gained popularity in speaker recognition tasks due
to its parameterized sinc functions that allow it to work directly on the
speech signal. The original SincNet architecture uses the softmax loss, which
may not be the most suitable choice for recognition-based tasks. Such loss
functions do not impose inter-class margins nor differentiate between easy and
hard training samples. Curriculum learning, particularly those leveraging
angular margin-based losses, has proven very successful in other biometric
applications such as face recognition. The advantage of such a curriculum
learning-based techniques is that it will impose inter-class margins as well as
taking to account easy and hard samples. In this paper, we propose Curricular
SincNet (CL-SincNet), an improved SincNet model where we use a curricular loss
function to train the SincNet architecture. The proposed model is evaluated on
multiple datasets using intra-dataset and inter-dataset evaluation protocols.
In both settings, the model performs competitively with other previously
published work. In the case of inter-dataset testing, it achieves the best
overall results with a reduction of 4\% error rate compare to SincNet and other
published work.",False,False,"Combining automated music generation with speaker recognition lacks a clear, novel application that meets all the multidisciplinary, feasible, novel, and useful criteria effectively.",
neg-1s-12,1,,1608.04541v1,"Influence of gene copy number on self-regulated gene expression; Using an analytically solvable stochastic model, we study the properties of a
simple genetic circuit consisting of multiple copies of an self-regulating
gene. We analyse how the variation in gene copy number and the mutations
changing the auto-regulation strength affect the steady-state distribution of
protein concentration.
  We predict that one-reporter assay, an experimental method where the
extrinsic noise level is inferred from the comparison of expression variance of
a single and duplicated reporter gene, may give an incorrect estimation of the
extrinsic noise contribution when applied to self-regulating genes.
  We also show that an imperfect duplication of an auto-activated gene,
changing the regulation strength of one of the copies, may lead to a hybrid,
binary+graded response of these genes to external signal.
  The analysis of relative changes in mean gene expression before and after
duplication suggests that evolutionary accumulation of gene duplications may
non-trivially depend on the inherent noisiness of a given gene, quantified by
maximal mean frequency of bursts.
  Moreover, we find that the dependence of gene expression noise on gene copy
number and auto-regulation strength may qualitatively differ, e.g. in
monotonicity, depending on whether the noise is measured by Fano factor or
coefficient of variation. Thus, experimentally-based hypotheses linking gene
expression noise and evolutionary optimisation may be ambiguous as they are
dependent on the particular function chosen to quantify noise.",1608.08007v3,"Ultrasensitivity on signaling cascades revisited: Linking local and
  global ultrasensitivity estimations; Ultrasensitive response motifs, which are capable of converting graded
stimulus in binary responses, are very well-conserved in signal transduction
networks. Although it has been shown that a cascade arrangement of multiple
ultrasensitive modules can produce an enhancement of the system's
ultrasensitivity, how the combination of layers affects the cascade's
ultrasensitivity remains an open question for the general case. Here we
introduced a methodology that allowed us to determine the presence of
sequestration effects and to quantify the relative contribution of each module
to the overall cascade's ultrasensitivity. The proposed analysis framework
provides a natural link between global and local ultrasensitivity descriptors
and is particularly well-suited to characterize and better understand
mathematical models used to study real biological systems. As a case study we
considered three mathematical models introduced by O'Shaughnessy et al. to
study a tunable synthetic MAPK cascade, and showed how our methodology might
help modelers to better understand modeling alternatives.",False,True,"Combining gene copy number effects on expression noise with ultrasensitivity in signaling cascades offers a novel, feasible approach to experimentally validate how genetic variability influences signal processing in cells, meeting multidisciplinary, novel, useful, and feasible standards.",
neg-1s-13,1,,2407.17172v1,"Speech Editing -- a Summary; With the rise of video production and social media, speech editing has become
crucial for creators to address issues like mispronunciations, missing words,
or stuttering in audio recordings. This paper explores text-based speech
editing methods that modify audio via text transcripts without manual waveform
editing. These approaches ensure edited audio is indistinguishable from the
original by altering the mel-spectrogram. Recent advancements, such as
context-aware prosody correction and advanced attention mechanisms, have
improved speech editing quality. This paper reviews state-of-the-art methods,
compares key metrics, and examines widely used datasets. The aim is to
highlight ongoing issues and inspire further research and innovation in speech
editing.",2407.17416v1,"Explaining Spectrograms in Machine Learning: A Study on Neural Networks
  for Speech Classification; This study investigates discriminative patterns learned by neural networks
for accurate speech classification, with a specific focus on vowel
classification tasks. By examining the activations and features of neural
networks for vowel classification, we gain insights into what the networks
""see"" in spectrograms. Through the use of class activation mapping, we identify
the frequencies that contribute to vowel classification and compare these
findings with linguistic knowledge. Experiments on a American English dataset
of vowels showcases the explainability of neural networks and provides valuable
insights into the causes of misclassifications and their characteristics when
differentiating them from unvoiced speech. This study not only enhances our
understanding of the underlying acoustic cues in vowel classification but also
offers opportunities for improving speech recognition by bridging the gap
between abstract representations in neural networks and established linguistic
knowledge",False,True,"Combining text-based speech editing with explainable neural network analysis can create a novel tool that not only edits speech but also provides insights into the editing decisions, meeting all multidisciplinary, feasible, novel, and useful criteria.",
neg-1s-14,1,,1611.09824v1,"Interspecific allometric scaling of unicellular organisms as an
  evolutionary process of food chain creation; Metabolism of living organisms is a foundation of life. The metabolic rate
(energy production per unit time) increases slower than organisms' mass. When
this phenomenon is considered across different species, it is called
interspecific allometric scaling, whose causes are unknown. We argue that the
cause of interspecific allometric scaling is the total effect of physiological
and adaptation mechanisms inherent to organisms composing a food chain.
Together, the workings of these mechanisms are united by a primary goal of any
living creature - its successful reproduction. This primary necessity of each
organism and of the entire food chain is that common denominator, to which all
organisms adjust their metabolic rates. In this article, we consider
unicellular organisms, while the second paper studies multicellular organisms
and the entire concept in more detail. Here, using the proposed concepts and
experimentally verified growth models of five different unicellular organisms,
we obtain close to experimental findings values of allometric exponents of
0.757 for the end of growth and 0.853 for the beginning of growth. These
results comply with experimental observations and prove our theory that the
requirement of successful reproduction within the food chain is an important
factor shaping interspecific allometric scaling.",1612.00036v1,"A quantitative definition of organismality and its application to lichen; The organism is a fundamental concept in biology. However there is no
universally accepted, formal, and yet broadly applicable definition of what an
organism is. Here we introduce a candidate definition. We adopt the view that
the ""organism"" is a functional concept, used by scientists to address
particular questions concerning the future state of a biological system, rather
than something wholly defined by that system. In this approach organisms are a
coarse-graining of a fine-grained dynamical model of a biological system.
Crucially, the coarse-graining of the system into organisms is chosen so that
their dynamics can be used by scientists to make accurate predictions of those
features of the biological system that interests them, and do so with minimal
computational burden. To illustrate our framework we apply it to a dynamic
model of lichen symbiosis---a system where either the lichen or its constituent
fungi and algae could reasonably be considered ""organisms."" We find that the
best choice for what organisms are in this scenario are complex mixtures of
many entities that do not resemble standard notions of organisms. When we
restrict our allowed coarse-grainings to more traditional types of organisms,
we find that ecological conditions, such as niche competition and predation
pressure, play a significant role in determining the best choice for organisms.",False,True,"The combination of allometric scaling in unicellular organisms with the functional definition of organismality offers a novel, experimentally testable approach to understanding metabolic scaling in complex symbiotic systems, meeting all multidisciplinary research standards.",
neg-1s-15,1,,2305.01980v1,"Diverse and Vivid Sound Generation from Text Descriptions; Previous audio generation mainly focuses on specified sound classes such as
speech or music, whose form and content are greatly restricted. In this paper,
we go beyond specific audio generation by using natural language description as
a clue to generate broad sounds. Unlike visual information, a text description
is concise by its nature but has rich hidden meanings beneath, which poses a
higher possibility and complexity on the audio to be generated. A
Variation-Quantized GAN is used to train a codebook learning discrete
representations of spectrograms. For a given text description, its pre-trained
embedding is fed to a Transformer to sample codebook indices to decode a
spectrogram to be further transformed into waveform by a melgan vocoder. The
generated waveform has high quality and fidelity while excellently
corresponding to the given text. Experiments show that our proposed method is
capable of generating natural, vivid audios, achieving superb quantitative and
qualitative results.",2305.02147v3,"Improved Vocal Effort Transfer Vector Estimation for Vocal Effort-Robust
  Speaker Verification; Despite the maturity of modern speaker verification technology, its
performance still significantly degrades when facing non-neutrally-phonated
(e.g., shouted and whispered) speech. To address this issue, in this paper, we
propose a new speaker embedding compensation method based on a minimum mean
square error (MMSE) estimator. This method models the joint distribution of the
vocal effort transfer vector and non-neutrally-phonated embedding spaces and
operates in a principal component analysis domain to cope with
non-neutrally-phonated speech data scarcity. Experiments are carried out using
a cutting-edge speaker verification system integrating a powerful
self-supervised pre-trained model for speech representation. In comparison with
a state-of-the-art embedding compensation method, the proposed MMSE estimator
yields superior and competitive equal error rate results when tackling shouted
and whispered speech, respectively.",False,False,"Combining concepts from audio generation and speaker verification does not clearly result in a feasible, novel, and useful multidisciplinary research idea that meets all the specified standards.",
neg-1s-16,1,,1602.08927v3,"High-Dimensional $L_2$Boosting: Rate of Convergence; Boosting is one of the most significant developments in machine learning.
This paper studies the rate of convergence of $L_2$Boosting, which is tailored
for regression, in a high-dimensional setting. Moreover, we introduce so-called
\textquotedblleft post-Boosting\textquotedblright. This is a post-selection
estimator which applies ordinary least squares to the variables selected in the
first stage by $L_2$Boosting. Another variant is \textquotedblleft Orthogonal
Boosting\textquotedblright\ where after each step an orthogonal projection is
conducted. We show that both post-$L_2$Boosting and the orthogonal boosting
achieve the same rate of convergence as LASSO in a sparse, high-dimensional
setting. We show that the rate of convergence of the classical $L_2$Boosting
depends on the design matrix described by a sparse eigenvalue constant. To show
the latter results, we derive new approximation results for the pure greedy
algorithm, based on analyzing the revisiting behavior of $L_2$Boosting. We also
introduce feasible rules for early stopping, which can be easily implemented
and used in applied work. Our results also allow a direct comparison between
LASSO and boosting which has been missing from the literature. Finally, we
present simulation studies and applications to illustrate the relevance of our
theoretical results and to provide insights into the practical aspects of
boosting. In these simulation studies, post-$L_2$Boosting clearly outperforms
LASSO.",1803.07164v2,"Adversarial Generalized Method of Moments; We provide an approach for learning deep neural net representations of models
described via conditional moment restrictions. Conditional moment restrictions
are widely used, as they are the language by which social scientists describe
the assumptions they make to enable causal inference. We formulate the problem
of estimating the underling model as a zero-sum game between a modeler and an
adversary and apply adversarial training. Our approach is similar in nature to
Generative Adversarial Networks (GAN), though here the modeler is learning a
representation of a function that satisfies a continuum of moment conditions
and the adversary is identifying violating moments. We outline ways of
constructing effective adversaries in practice, including kernels centered by
k-means clustering, and random forests. We examine the practical performance of
our approach in the setting of non-parametric instrumental variable regression.",False,True,"Integrating high-dimensional L₂Boosting with adversarial GMM methods could yield a novel approach for causal inference, leveraging machine learning and econometric techniques. This multidisciplinary idea is feasible, innovative, and effectively addresses model selection and validation challenges.",
neg-1s-17,1,,cs/0511033v1,"Fast (Multi-)Evaluation of Linearly Recurrent Sequences: Improvements
  and Applications; For a linearly recurrent vector sequence P[n+1] = A(n) * P[n], consider the
problem of calculating either the n-th term P[n] or L<=n arbitrary terms
P[n_1],...,P[n_L], both for the case of constant coefficients A(n)=A and for a
matrix A(n) with entries polynomial in n. We improve and extend known
algorithms for this problem and present new applications for it. Specifically
it turns out that for instance * any family (p_n) of classical orthogonal
polynomials admits evaluation at given x within O(n^{1/2} log n) operations
INDEPENDENT of the family (p_n) under consideration. * For any L indices
n_1,...,n_L <= n, the values p_{n_i}(x) can be calculated simultaneously using
O(n^{1/2} log n + L log(n/L)) arithmetic operations; again this running time
bound holds uniformly. * Every hypergeometric (or, more generally, holonomic)
function admits approximate evaluation up to absolute error e>0 within
O((log(1/e)^{1/2} loglog(1/e)) -- as opposed to O(log(1/e)) -- arithmetic
steps. * Given m and a polynomial p of degree d over a field of characteristic
zero, the coefficient of p^m to term X^n can be computed within O(d^2
M(n^{1/2})) steps where M(n) denotes the cost of multiplying two degree-n
polynomials. * The same time bound holds for the joint calculation of any
L<=n^{1/2} desired coefficients of p^m to terms X^{n_i}, n_1,...,n_L <= n.",cs/0511066v5,"An introspective algorithm for the integer determinant; We present an algorithm computing the determinant of an integer matrix A. The
algorithm is introspective in the sense that it uses several distinct
algorithms that run in a concurrent manner. During the course of the algorithm
partial results coming from distinct methods can be combined. Then, depending
on the current running time of each method, the algorithm can emphasize a
particular variant. With the use of very fast modular routines for linear
algebra, our implementation is an order of magnitude faster than other existing
implementations. Moreover, we prove that the expected complexity of our
algorithm is only O(n^3 log^{2.5}(n ||A||)) bit operations in the dense case
and O(Omega n^{1.5} log^2(n ||A||) + n^{2.5}log^3(n||A||)) in the sparse case,
where ||A|| is the largest entry in absolute value of the matrix and Omega is
the cost of matrix-vector multiplication in the case of a sparse matrix.",False,False,"There is no clear, feasible combination of concepts from both papers that meets all the multidisciplinary standards outlined.",
neg-1s-18,1,,1602.08652v3,"A Tutorial: Adaptive Runge-Kutta Integration for Stiff Systems :
  Comparing the Nosé and Nosé-Hoover Oscillator Dynamics; ""Stiff"" differential equations are commonplace in engineering and dynamical
systems. To solve them we need flexible integrators that can deal with
rapidly-changing righthand sides. This tutorial describes the application of
""adaptive"" [ variable timestep ] integrators to ""stiff"" mechanical problems
encountered in modern applications of Gibbs' 1902 statistical mechanics. Linear
harmonic oscillators subject to nonlinear thermal constraints can exhibit
either stiff or smooth dynamics. Two closely-related examples, Nos\'e's 1984
dynamics and Nos\'e-Hoover 1985 dynamics, are both based on Hamiltonian
mechanics, as was ultimately clarified by Dettmann and Morriss in 1996. Both
these dynamics are consistent with Gibbs' canonical ensemble. Nos\'e's dynamics
is ""stiff"" and can present severe numerical difficulties. Nos\'e-Hoover
dynamics, though it follows exactly the same trajectory, is ""smooth"" and
relatively trouble-free. Our tutorial emphasises the power of adaptive
integrators to resolve stiff problems like the Nos\'e oscillator. The solutions
obtained illustrate the power of computer graphics to enrich numerical
solutions. Adaptive integration with computer graphics are basic to an
understanding of dynamical systems and statistical mechanics. These tools lead
naturally into the visualization of intricate fractal structures formed by
chaos as well as elaborate knots tied by regular nonchaotic dynamics. This work
was invited by the American Journal of Physics.",1603.02106v5,"Carrier Phase Estimation in Dispersion-Unmanaged Optical Transmission
  Systems; The study on carrier phase estimation (CPE) approaches, involving a one-tap
normalized least-mean-square (NLMS) algorithm, a block-wise average algorithm,
and a Viterbi-Viterbi algorithm has been carried out in the long-haul
high-capacity dispersion-unmanaged coherent optical systems. The close-form
expressions and analytical predictions for bit-error-rate behaviors in these
CPE methods have been analyzed by considering both the laser phase noise and
the equalization enhanced phase noise. It is found that the Viterbi-Viterbi
algorithm outperforms the one-tap NLMS and the block-wise average algorithms
for a small phase noise variance (or effective phase noise variance), while the
three CPE methods converge to a similar performance for a large phase noise
variance (or effective phase noise variance). In addition, the differences
between the three CPE approaches become smaller for higher-level modulation
formats.",False,False,"The two papers belong to distinct fields with limited overlapping concepts, making it challenging to combine their ideas into a novel, feasible, and useful multidisciplinary research idea that meets all the specified standards.",
neg-1s-19,1,,2410.10665v1,"Double Jeopardy and Climate Impact in the Use of Large Language Models:
  Socio-economic Disparities and Reduced Utility for Non-English Speakers; Artificial Intelligence (AI), particularly large language models (LLMs),
holds the potential to bridge language and information gaps, which can benefit
the economies of developing nations. However, our analysis of FLORES-200,
FLORES+, Ethnologue, and World Development Indicators data reveals that these
benefits largely favor English speakers. Speakers of languages in low-income
and lower-middle-income countries face higher costs when using OpenAI's GPT
models via APIs because of how the system processes the input -- tokenization.
Around 1.5 billion people, speaking languages primarily from
lower-middle-income countries, could incur costs that are 4 to 6 times higher
than those faced by English speakers. Disparities in LLM performance are
significant, and tokenization in models priced per token amplifies inequalities
in access, cost, and utility. Moreover, using the quality of translation tasks
as a proxy measure, we show that LLMs perform poorly in low-resource languages,
presenting a ``double jeopardy"" of higher costs and poor performance for these
users. We also discuss the direct impact of fragmentation in tokenizing
low-resource languages on climate. This underscores the need for fairer
algorithm development to benefit all linguistic groups.",2410.17587v1,"Predicting Company Growth by Econophysics informed Machine Learning; Predicting company growth is crucial for strategic adjustment, operational
decision-making, risk assessment, and loan eligibility reviews. Traditional
models for company growth often focus too much on theory, overlooking practical
forecasting, or they rely solely on time series forecasting techniques,
ignoring interpretability and the inherent mechanisms of company growth. In
this paper, we propose a machine learning-based prediction framework that
incorporates an econophysics model for company growth. Our model captures both
the intrinsic growth mechanisms of companies led by scaling laws and the
fluctuations influenced by random factors and individual decisions,
demonstrating superior predictive performance compared with methods that use
time series techniques alone. Its advantages are more pronounced in long-range
prediction tasks. By explicitly modeling the baseline growth and volatility
components, our model is more interpretable.",False,True,"Combining socio-economic impacts of LLMs on non-English speakers with econophysics-informed machine learning for company growth offers a novel, feasible, and useful multidisciplinary research avenue that bridges AI fairness and economic forecasting.",
neg-1s-20,1,,1808.09267v2,"Creating a surrogate commuter network from Australian Bureau of
  Statistics census data; Between the 2011 and 2016 national censuses, the Australian Bureau of
Statistics changed its anonymity policy compliance system for the distribution
of census data. The new method has resulted in dramatic inconsistencies when
comparing low-resolution data to aggregated high-resolution data. Hence,
aggregated totals do not match true totals, and the mismatch gets worse as the
data resolution gets finer. Here, we address several aspects of this
inconsistency with respect to the 2016 usual-residence to place-of-work travel
data. We introduce a re-sampling system that rectifies many of the artifacts
introduced by the new ABS protocol, ensuring a higher level of consistency
across partition sizes. We offer a surrogate high-resolution 2016 commuter
dataset that reduces the difference between aggregated and true commuter totals
from ~34% to only ~7%, which is on the order of the discrepancy across
partition resolutions in data from earlier years.",1808.09545v1,"Cost-efficient Data Acquisition on Online Data Marketplaces for
  Correlation Analysis; Incentivized by the enormous economic profits, the data marketplace platform
has been proliferated recently. In this paper, we consider the data marketplace
setting where a data shopper would like to buy data instances from the data
marketplace for correlation analysis of certain attributes. We assume that the
data in the marketplace is dirty and not free. The goal is to find the data
instances from a large number of datasets in the marketplace whose join result
not only is of high-quality and rich join informativeness, but also delivers
the best correlation between the requested attributes. To achieve this goal, we
design DANCE, a middleware that provides the desired data acquisition service.
DANCE consists of two phases: (1) In the off-line phase, it constructs a
two-layer join graph from samples. The join graph consists of the information
of the datasets in the marketplace at both schema and instance levels; (2) In
the online phase, it searches for the data instances that satisfy the
constraints of data quality, budget, and join informativeness, while maximize
the correlation of source and target attribute sets. We prove that the
complexity of the search problem is NP-hard, and design a heuristic algorithm
based on Markov chain Monte Carlo (MCMC). Experiment results on two benchmark
datasets demonstrate the efficiency and effectiveness of our heuristic data
acquisition algorithm.",False,True,"Combining surrogate commuter network creation with cost-efficient data acquisition could lead to innovative methods for enhancing data quality and analysis in large-scale demographic studies, meeting all multidisciplinary, feasible, novel, and useful criteria.",
neg-1s-21,1,,1904.11949v2,"Machine Learning Tips and Tricks for Power Line Communications; A great deal of attention has been recently given to Machine Learning (ML)
techniques in many different application fields. This paper provides a vision
of what ML can do in Power Line Communications (PLC). We firstly and briefly
describe classical formulations of ML, and distinguish deterministic from
statistical learning models with relevance to communications. We then discuss
ML applications in PLC for each layer, namely, for characterization and
modeling, for the development of physical layer algorithms, for media access
control and networking. Finally, other applications of PLC that can benefit
from the usage of ML, as grid diagnostics, are analyzed. Illustrative numerical
examples are reported to serve the purpose of validating the ideas and motivate
future research endeavors in this stimulating signal/data processing field.",1904.11950v1,"Attention-based Transfer Learning for Brain-computer Interface; Different functional areas of the human brain play different roles in brain
activity, which has not been paid sufficient research attention in the
brain-computer interface (BCI) field. This paper presents a new approach for
electroencephalography (EEG) classification that applies attention-based
transfer learning. Our approach considers the importance of different brain
functional areas to improve the accuracy of EEG classification, and provides an
additional way to automatically identify brain functional areas associated with
new activities without the involvement of a medical professional. We
demonstrate empirically that our approach out-performs state-of-the-art
approaches in the task of EEG classification, and the results of visualization
indicate that our approach can detect brain functional areas related to a
certain task.",False,True,"Combining attention-based transfer learning from BCI with ML techniques in Power Line Communications can create innovative models that enhance signal processing and adaptability in PLC, is experimentally feasible, offers a novel integration of neuroscience-inspired methods into communications, and effectively addresses improving PLC performance.",
neg-1s-22,1,,1808.06040v1,"Optimal proposals for Approximate Bayesian Computation; We derive the optimal proposal density for Approximate Bayesian Computation
(ABC) using Sequential Monte Carlo (SMC) (or Population Monte Carlo, PMC). The
criterion for optimality is that the SMC/PMC-ABC sampler maximise the effective
number of samples per parameter proposal. The optimal proposal density
represents the optimal trade-off between favoring high acceptance rate and
reducing the variance of the importance weights of accepted samples. We discuss
two convenient approximations of this proposal and show that the optimal
proposal density gives a significant boost in the expected sampling efficiency
compared to standard kernels that are in common use in the ABC literature,
especially as the number of parameters increases.",1808.06310v2,"Analysis of ""Learn-As-You-Go"" (LAGO) Studies; In learn-as-you-go (LAGO) adaptive studies, the intervention is a complex
package consisting of multiple components, and is adapted in stages during the
study based on past outcome data. This design formalizes standard practice, and
desires for practice, in public health intervention studies. An effective
intervention package is sought, while minimizing intervention package cost.
When analyzing data from a learn-as-you-go study, the interventions in later
stages depend upon the outcomes in the previous stages, violating standard
statistical theory. We develop methods for estimating the intervention effects
in a LAGO study. We prove consistency and asymptotic normality using a novel
coupling argument, ensuring the validity of the test for the hypothesis of no
overall intervention effect. We develop a confidence set for the optimal
intervention package and confidence bands for the success probabilities under
alternative package compositions. We illustrate our methods in the BetterBirth
Study, which aimed to improve maternal and neonatal outcomes among 157,689
births in Uttar Pradesh, India through a complex, multi-component intervention
package.",False,True,"Combining optimal ABC proposal densities with adaptive intervention strategies in LAGO studies could create a novel, feasible, and useful multidisciplinary approach to enhance sampling efficiency and intervention effectiveness.",
neg-1s-23,1,,2502.17382v1,"Unraveling the geometry of visual relational reasoning; Humans and other animals readily generalize abstract relations, such as
recognizing constant in shape or color, whereas neural networks struggle. To
investigate how neural networks generalize abstract relations, we introduce
SimplifiedRPM, a novel benchmark for systematic evaluation. In parallel, we
conduct human experiments to benchmark relational difficulty, enabling direct
model-human comparisons. Testing four architectures--ResNet-50, Vision
Transformer, Wild Relation Network, and Scattering Compositional Learner
(SCL)--we find that SCL best aligns with human behavior and generalizes best.
Building on a geometric theory of neural representations, we show
representational geometries that predict generalization. Layer-wise analysis
reveals distinct relational reasoning strategies across models and suggests a
trade-off where unseen rule representations compress into training-shaped
subspaces. Guided by our geometric perspective, we propose and evaluate
SNRloss, a novel objective balancing representation geometry. Our findings
offer geometric insights into how neural networks generalize abstract
relations, paving the way for more human-like visual reasoning in AI.",2502.18318v1,"Mapping of Subjective Accounts into Interpreted Clusters (MOSAIC): Topic
  Modelling and LLM applied to Stroboscopic Phenomenology; Stroboscopic light stimulation (SLS) on closed eyes typically induces simple
visual hallucinations (VHs), characterised by vivid, geometric and colourful
patterns. A dataset of 862 sentences, extracted from 422 open subjective
reports, was recently compiled as part of the Dreamachine programme (Collective
Act, 2022), an immersive multisensory experience that combines SLS and spatial
sound in a collective setting. Although open reports extend the range of
reportable phenomenology, their analysis presents significant challenges,
particularly in systematically identifying patterns. To address this challenge,
we implemented a data-driven approach leveraging Large Language Models and
Topic Modelling to uncover and interpret latent experiential topics directly
from the Dreamachine's text-based reports. Our analysis confirmed the presence
of simple VHs typically documented in scientific studies of SLS, while also
revealing experiences of altered states of consciousness and complex
hallucinations. Building on these findings, our computational approach expands
the systematic study of subjective experience by enabling data-driven analyses
of open-ended phenomenological reports, capturing experiences not readily
identified through standard questionnaires. By revealing rich and multifaceted
aspects of experiences, our study broadens our understanding of
stroboscopically-induced phenomena while highlighting the potential of Natural
Language Processing and Large Language Models in the emerging field of
computational (neuro)phenomenology. More generally, this approach provides a
practically applicable methodology for uncovering subtle hidden patterns of
subjective experience across diverse research domains.",False,True,"Combining neural network relational geometry with analysis of subjective visual experiences offers a novel, feasible multidisciplinary approach. It leverages AI models and phenomenological data to enhance understanding of visual reasoning, meeting all standards for novelty and usefulness.",
neg-1s-24,1,,2403.03648v1,"A Connector for Integrating NGSI-LD Data into Open Data Portals; Nowadays, there are plenty of data sources generating massive amounts of
information that, combined with novel data analytics frameworks, are meant to
support optimisation in many application domains. Nonetheless, there are still
shortcomings in terms of data discoverability, accessibility and
interoperability. Open Data portals have emerged as a shift towards openness
and discoverability. However, they do not impose any condition to the data
itself, just stipulate how datasets have to be described. Alternatively, the
NGSI-LD standard pursues harmonisation in terms of data modelling and
accessibility. This paper presents a solution that bridges these two domains
(i.e., Open Data portals and NGSI-LD-based data) in order to keep benefiting
from the structured description of datasets offered by Open Data portals, while
ensuring the interoperability provided by the NGSI-LD standard. Our solution
aggregates the data into coherent datasets and generate high-quality
descriptions, ensuring comprehensiveness, interoperability and accessibility.
The proposed solution has been validated through a real-world implementation
that exposes IoT data in NGSI-LD format through the European Data Portal (EDP).
Moreover, the results from the Metadata Quality Assessment that the EDP
implements, show that the datasets' descriptions generated achieve excellent
ranking in terms of the Findability, Accessibility, Interoperability and
Reusability (FAIR) data principles.",2403.04327v2,"ProMoAI: Process Modeling with Generative AI; ProMoAI is a novel tool that leverages Large Language Models (LLMs) to
automatically generate process models from textual descriptions, incorporating
advanced prompt engineering, error handling, and code generation techniques.
Beyond automating the generation of complex process models, ProMoAI also
supports process model optimization. Users can interact with the tool by
providing feedback on the generated model, which is then used for refining the
process model. ProMoAI utilizes the capabilities LLMs to offer a novel,
AI-driven approach to process modeling, significantly reducing the barrier to
entry for users without deep technical knowledge in process modeling.",False,True,"Combining NGSI-LD data integration with AI-driven process modeling creates a novel, feasible multidisciplinary approach. It leverages structured, interoperable data to enhance automated process generation and optimization, addressing data accessibility and usability effectively.",
neg-1s-25,1,,2407.13908v1,"Construction and Hedging of Equity Index Options Portfolios; This research presents a comprehensive evaluation of systematic index
option-writing strategies, focusing on S&P500 index options. We compare the
performance of hedging strategies using the Black-Scholes-Merton (BSM) model
and the Variance-Gamma (VG) model, emphasizing varying moneyness levels and
different sizing methods based on delta and the VIX Index. The study employs
1-minute data of S&P500 index options and index quotes spanning from 2018 to
2023. The analysis benchmarks hedged strategies against buy-and-hold and naked
option-writing strategies, with a focus on risk-adjusted performance metrics
including transaction costs. Portfolio delta approximations are derived using
implied volatility for the BSM model and market-calibrated parameters for the
VG model. Key findings reveal that systematic option-writing strategies can
potentially yield superior returns compared to buy-and-hold benchmarks. The BSM
model generally provided better hedging outcomes than the VG model, although
the VG model showed profitability in certain naked strategies as a tool for
position sizing. In terms of rehedging frequency, we found that intraday
hedging in 130-minute intervals provided both reliable protection against
adverse market movements and a satisfactory returns profile.",2407.14844v1,"Political Leanings in Web3 Betting: Decoding the Interplay of Political
  and Profitable Motives; Harnessing the transparent blockchain user behavior data, we construct the
Political Betting Leaning Score (PBLS) to measure political leanings based on
betting within Web3 prediction markets. Focusing on Polymarket and starting
from the 2024 U.S. Presidential Election, we synthesize behaviors over 15,000
addresses across 4,500 events and 8,500 markets, capturing the intensity and
direction of their political leanings by the PBLS. We validate the PBLS through
internal consistency checks and external comparisons. We uncover relationships
between our PBLS and betting behaviors through over 800 features capturing
various behavioral aspects. A case study of the 2022 U.S. Senate election
further demonstrates the ability of our measurement while decoding the dynamic
interaction between political and profitable motives. Our findings contribute
to understanding decision-making in decentralized markets, enhancing the
analysis of behaviors within Web3 prediction environments. The insights of this
study reveal the potential of blockchain in enabling innovative,
multidisciplinary studies and could inform the development of more effective
online prediction markets, improve the accuracy of forecast, and help the
design and optimization of platform mechanisms. The data and code for the paper
are accessible at the following link: https://github.com/anonymous.",False,False,"Combining advanced financial hedging strategies with political betting in Web3 does not clearly present a feasible, novel, and directly useful research idea that meets all the multidisciplinary standards.",
neg-1s-26,1,,1702.01418v2,"Choosing the number of groups in a latent stochastic block model for
  dynamic networks; Latent stochastic block models are flexible statistical models that are
widely used in social network analysis. In recent years, efforts have been made
to extend these models to temporal dynamic networks, whereby the connections
between nodes are observed at a number of different times. In this paper we
extend the original stochastic block model by using a Markovian property to
describe the evolution of nodes' cluster memberships over time. We recast the
problem of clustering the nodes of the network into a model-based context, and
show that the integrated completed likelihood can be evaluated analytically for
a number of likelihood models. Then, we propose a scalable greedy algorithm to
maximise this quantity, thereby estimating both the optimal partition and the
ideal number of groups in a single inferential framework. Finally we propose
applications of our methodology to both real and artificial datasets.",1702.01618v2,"Learning of state-space models with highly informative observations: a
  tempered Sequential Monte Carlo solution; Probabilistic (or Bayesian) modeling and learning offers interesting
possibilities for systematic representation of uncertainty using probability
theory. However, probabilistic learning often leads to computationally
challenging problems. Some problems of this type that were previously
intractable can now be solved on standard personal computers thanks to recent
advances in Monte Carlo methods. In particular, for learning of unknown
parameters in nonlinear state-space models, methods based on the particle
filter (a Monte Carlo method) have proven very useful. A notoriously
challenging problem, however, still occurs when the observations in the
state-space model are highly informative, i.e. when there is very little or no
measurement noise present, relative to the amount of process noise. The
particle filter will then struggle in estimating one of the basic components
for probabilistic learning, namely the likelihood $p($data$|$parameters$)$. To
this end we suggest an algorithm which initially assumes that there is
substantial amount of artificial measurement noise present. The variance of
this noise is sequentially decreased in an adaptive fashion such that we, in
the end, recover the original problem or possibly a very close approximation of
it. The main component in our algorithm is a sequential Monte Carlo (SMC)
sampler, which gives our proposed method a clear resemblance to the SMC^2
method. Another natural link is also made to the ideas underlying the
approximate Bayesian computation (ABC). We illustrate it with numerical
examples, and in particular show promising results for a challenging
Wiener-Hammerstein benchmark problem.",False,False,The combination of dynamic network clustering and advanced state-space Monte Carlo methods is not clearly novel or immediately useful based on the abstracts. It may lack the necessary integration to meet all multidisciplinary research standards.,
neg-1s-27,1,,1909.01439v1,"Universality of clone dynamics during tissue development; The emergence of complex organs is driven by the coordinated proliferation,
migration and differentiation of precursor cells. The fate behaviour of these
cells is reflected in the time evolution their progeny, termed clones, which
serve as a key experimental observable. In adult tissues, where cell dynamics
is constrained by the condition of homeostasis, clonal tracing studies based on
transgenic animal models have advanced our understanding of cell fate behaviour
and its dysregulation in disease. But what can be learned from clonal dynamics
in development, where the spatial cohesiveness of clones is impaired by tissue
deformations during tissue growth? Drawing on the results of clonal tracing
studies, we show that, despite the complexity of organ development, clonal
dynamics may converge to a critical state characterized by universal scaling
behaviour of clone sizes. By mapping clonal dynamics onto a generalization of
the classical theory of aerosols, we elucidate the origin and range of scaling
behaviours and show how the identification of universal scaling dependences may
allow lineage-specific information to be distilled from experiments. Our study
shows the emergence of core concepts of statistical physics in an unexpected
context, identifying cellular systems as a laboratory to study non-equilibrium
statistical physics.",1909.01711v1,"Simulation and computational analysis of multiscale graph agent-based
  tumor model; This paper deals with the cellular biological network analysis of the
tumor-growth model, consisting of multiple spaces and time scales. In this
paper, we present a model in graph simulation using ABM for tumor growth. In
particular, we propose a graph agent-based modeling and simulation system in
the format of tumor growth scenario for evolving analysis. To manage cellular
biological network analysis, we developed a workflow that allows us to estimate
the tumor model and the complexity of the evolving behavior in a principled
manner. By developing the model using Python, which has enabled us to run the
model multiple times (more than what is possible by conventional means) to
generate a large amount of data, we have succeeded in getting deep in to the
micro-environment of the tumor, employing network analysis. Combining
agent-based modeling with graph-based modeling to simulate the structure,
dynamics, and functions of complex networks is exclusively important for
biological systems with a large number of open parameters, e.g., epidemic
models of disease spreading or cancer. Extracting data from evolutionary
directed graphs and a set of centrality algorithms helps us to tackle the
problems of pathway analysis and to develop the ability to predict, control,
and design the function of metabolisms. Reproducing and performing complex
parametric simulations a known phenomenon at a sufficient level of detail for
computational biology could be an impressive achievement for fast analysis
purposes in clinics, both on the predictive diagnostic and therapeutic side.",False,True,"Combining universal scaling in clone dynamics with graph-based agent models for tumor growth could create a novel, experimentally testable framework. This multidisciplinary approach integrates statistical physics and computational biology, offering innovative insights and practical applications in understanding and treating cancer.",
neg-1s-28,1,,2004.07429v1,"Exactly computing the tail of the Poisson-Binomial Distribution; We offer ShiftConvolvePoibin, a fast exact method to compute the tail of a
Poisson-Binomial distribution (PBD). Our method employs an exponential shift to
retain its accuracy when computing a tail probability, and in practice we find
that it is immune to the significant relative errors that other methods, exact
or approximate, can suffer from when computing very small tail probabilities of
the PBD. The accompanying R package is also competitive with the fastest
implementations for computing the entire PBD.",2004.07471v3,"Efficient Bernoulli factory MCMC for intractable posteriors; Accept-reject based Markov chain Monte Carlo (MCMC) algorithms have
traditionally utilised acceptance probabilities that can be explicitly written
as a function of the ratio of the target density at the two contested points.
This feature is rendered almost useless in Bayesian posteriors with unknown
functional forms. We introduce a new family of MCMC acceptance probabilities
that has the distinguishing feature of not being a function of the ratio of the
target density at the two points. We present two stable Bernoulli factories
that generate events within this class of acceptance probabilities. The
efficiency of our methods rely on obtaining reasonable local upper or lower
bounds on the target density and we present two classes of problems where such
bounds are viable: Bayesian inference for diffusions and MCMC on constrained
spaces. The resulting portkey Barker's algorithms are exact and computationally
more efficient that the current state-of-the-art.",False,True,"Combining precise tail computation of Poisson-Binomial distributions with novel MCMC acceptance probabilities could create an efficient Bayesian inference method for complex discrete models, meeting multidisciplinary, feasible, novel, and useful criteria.",
neg-1s-29,1,,2308.11495v2,"Evaluating the accuracy of Gaussian approximations in VSWIR imaging
  spectroscopy retrievals; The joint retrieval of surface reflectances and atmospheric parameters in
VSWIR imaging spectroscopy is a computationally challenging high-dimensional
problem. Using NASA's Surface Biology and Geology mission as the motivational
context, the uncertainty associated with the retrievals is crucial for further
application of the retrieved results for environmental applications. Although
Markov chain Monte Carlo (MCMC) is a Bayesian method ideal for uncertainty
quantification, the full-dimensional implementation of MCMC for the retrieval
is computationally intractable.
  In this work, we developed a block Metropolis MCMC algorithm for the
high-dimensional VSWIR surface reflectance retrieval that leverages the
structure of the forward radiative transfer model to enable tractable fully
Bayesian computation. We use the posterior distribution from this MCMC
algorithm to assess the limitations of optimal estimation, the state-of-the-art
Bayesian algorithm in operational retrievals which is more computationally
efficient but uses a Gaussian approximation to characterize the posterior.
Analyzing the differences in the posterior computed by each method, the MCMC
algorithm was shown to give more physically sensible results and reveals the
non-Gaussian structure of the posterior, specifically in the atmospheric
aerosol optical depth parameter and the low-wavelength surface reflectances.",2308.11548v1,"Modelling Structural Breaks In Stock Price Time Series Using Stochastic
  Differential Equations; This paper studies the effect of quarterly earnings reports on the stock
price. The profitability of the stock is modelled by geometric Brownian
diffusion and the Constant Elasticity of Variance model. We fit several
variations of stochastic differential equations to the pre-and after-report
period using the Maximum Likelihood Estimation and Grid Search of parameters
method. By examining the change in the model parameters after reports'
publication, the study reveals that the reports have enough evidence to be a
structural breakpoint, meaning that all the forecast models exploited are not
applicable for forecasting and should be refitted shortly.",False,False,"The two papers focus on distinct domains—remote sensing retrievals and financial time series—making it challenging to combine their concepts into a novel, feasible, and useful multidisciplinary research idea that meets all the specified standards.",
neg-1s-30,1,,2212.07052v2,"On LASSO for High Dimensional Predictive Regression; This paper examines LASSO, a widely-used $L_{1}$-penalized regression method,
in high dimensional linear predictive regressions, particularly when the number
of potential predictors exceeds the sample size and numerous unit root
regressors are present. The consistency of LASSO is contingent upon two key
components: the deviation bound of the cross product of the regressors and the
error term, and the restricted eigenvalue of the Gram matrix. We present new
probabilistic bounds for these components, suggesting that LASSO's rates of
convergence are different from those typically observed in cross-sectional
cases. When applied to a mixture of stationary, nonstationary, and cointegrated
predictors, LASSO maintains its asymptotic guarantee if predictors are
scale-standardized. Leveraging machine learning and macroeconomic domain
expertise, LASSO demonstrates strong performance in forecasting the
unemployment rate, as evidenced by its application to the FRED-MD database.",2212.08615v1,"A smooth transition autoregressive model for matrix-variate time series; In many applications, data are observed as matrices with temporal dependence.
Matrix-variate time series modeling is a new branch of econometrics. Although
stylized facts in several fields, the existing models do not account for regime
switches in the dynamics of matrices that are not abrupt. In this paper, we
extend linear matrix-variate autoregressive models by introducing a
regime-switching model capable of accounting for smooth changes, the matrix
smooth transition autoregressive model. We present the estimation processes
with the asymptotic properties demonstrated with simulated and real data.",False,True,"Combining LASSO's variable selection with matrix smooth transition models creates a novel, feasible approach that can be experimentally validated and effectively enhances high-dimensional time series forecasting.",
neg-1s-31,1,,2405.01988v1,"Joint sentiment analysis of lyrics and audio in music; Sentiment or mood can express themselves on various levels in music. In
automatic analysis, the actual audio data is usually analyzed, but the lyrics
can also play a crucial role in the perception of moods. We first evaluate
various models for sentiment analysis based on lyrics and audio separately. The
corresponding approaches already show satisfactory results, but they also
exhibit weaknesses, the causes of which we examine in more detail. Furthermore,
different approaches to combining the audio and lyrics results are proposed and
evaluated. Considering both modalities generally leads to improved performance.
We investigate misclassifications and (also intentional) contradictions between
audio and lyrics sentiment more closely, and identify possible causes. Finally,
we address fundamental problems in this research area, such as high
subjectivity, lack of data, and inconsistency in emotion taxonomies.",2405.02821v2,"Sim2Real Transfer for Audio-Visual Navigation with Frequency-Adaptive
  Acoustic Field Prediction; Sim2real transfer has received increasing attention lately due to the success
of learning robotic tasks in simulation end-to-end. While there has been a lot
of progress in transferring vision-based navigation policies, the existing
sim2real strategy for audio-visual navigation performs data augmentation
empirically without measuring the acoustic gap. The sound differs from light in
that it spans across much wider frequencies and thus requires a different
solution for sim2real. We propose the first treatment of sim2real for
audio-visual navigation by disentangling it into acoustic field prediction
(AFP) and waypoint navigation. We first validate our design choice in the
SoundSpaces simulator and show improvement on the Continuous AudioGoal
navigation benchmark. We then collect real-world data to measure the spectral
difference between the simulation and the real world by training AFP models
that only take a specific frequency subband as input. We further propose a
frequency-adaptive strategy that intelligently selects the best frequency band
for prediction based on both the measured spectral difference and the energy
distribution of the received audio, which improves the performance on the real
data. Lastly, we build a real robot platform and show that the transferred
policy can successfully navigate to sounding objects. This work demonstrates
the potential of building intelligent agents that can see, hear, and act
entirely from simulation, and transferring them to the real world.",False,False,"While both papers involve audio analysis, their applications (sentiment in music vs. navigation) don't clearly combine to form a novel, feasible, and useful multidisciplinary research idea that meets all the specified standards.",
neg-1s-32,1,,0807.0471v4,"Complete intersection Approximation, Dual Filtrations and Applications; We give a two step method to study certain questions regarding associated
graded module of a Cohen-Macaulay (CM) module $M$ w.r.t an
$\mathfrak{m}$-primary ideal $\mathfrak{a}$ in a complete Noetherian local ring
$(A,\mathfrak{m})$. The first step, we call it complete intersection
approximation, enables us to reduce to the case when both $A$, $
G_\mathfrak{a}(A) = \bigoplus_{n \geq 0} \mathfrak{a}^n/\mathfrak{a}^{n+1} $
are complete intersections and $M$ is a maximal CM $A$-module. The second step
consists of analyzing the classical filtration $\{Hom_A(M,\mathfrak{a}^n)
\}_{\mathbb{Z}}$ of the dual $Hom_A(M,A)$. We give many applications of this
point of view. For instance let $(A,\mathfrak{m})$ be equicharacteristic and
CM. Let $a(G_\mathfrak{a}(A))$ be the $a$-invariant of $G_\mathfrak{a}(A)$. We
prove:
  1. $a(G_\mathfrak{a}(A)) = -\dim A$ iff $\mathfrak{a}$ is generated by a
regular sequence.
  2. If $\mathfrak{a}$ is integrally closed and $a(G_\mathfrak{a}(A)) = -\dim A
+ 1$ then $\mathfrak{a}$ has minimal multiplicity.
  We extend to modules a result of Ooishi relating symmetry of $h$-vectors. As
another application
  we prove a conjecture of Itoh, if $A$ is a CM local ring and
  $\mathfrak{a}$ is a normal ideal with $e_3^\mathfrak{a}(A) = 0$ then
$G_\mathfrak{a}(A)$ is CM.",0807.1654v4,"Centers of F-purity; In this paper, we study a positive characteristic analogue of the centers of
log canonicity of a pair $(R, \Delta)$. We call these analogues centers of
$F$-purity. We prove positive characteristic analogues of subadjunction-like
results, prove new stronger subadjunction-like results, and in some cases, lift
these new results to characteristic zero. Using a generalization of centers of
$F$-purity which we call uniformly $F$-compatible ideals, we give a
characterization of the test ideal (which unifies several previous
characterizations). Finally, in the case that $\Delta = 0$, we show that
uniformly $F$-compatible ideals coincide with the annihilators of the
$\mathcal{F}(E_R(k))$-submodules of $E_R(k)$ as defined by Smith and Lyubeznik.",False,False,"The concepts from both papers are deeply rooted in different areas of commutative algebra and algebraic geometry. Combining them into a novel, feasible, and useful multidisciplinary research idea that meets all specified standards is not evidently achievable.",
neg-1s-33,1,,2003.13751v1,"An Interface-enriched Generalized Finite Element Method for
  Levelset-based Topology Optimization; During design optimization, a smooth description of the geometry is
important, especially for problems that are sensitive to the way interfaces are
resolved, e.g., wave propagation or fluid-structure interaction. A levelset
description of the boundary, when combined with an enriched finite element
formulation, offers a smoother description of the design than traditional
density-based methods. However, existing enriched methods have drawbacks,
including ill-conditioning and difficulties in prescribing essential boundary
conditions. In this work we introduce a new enriched topology optimization
methodology that overcomes the aforementioned drawbacks; boundaries are
resolved accurately by means of the Interface-enriched Generalized Finite
Element Method (IGFEM), coupled to a levelset function constructed by radial
basis functions. The enriched method used in this new approach to topology
optimization has the same level of accuracy in the analysis as standard the
finite element method with matching meshes, but without the need for remeshing.
We derive the analytical sensitivities and we discuss the behavior of the
optimization process in detail. We establish that IGFEM-based levelset topology
optimization generates correct topologies for well-known compliance
minimization problems.",2005.01332v1,"Stochastic phase-field modeling of brittle fracture: computing multiple
  crack patterns and their probabilities; In variational phase-field modeling of brittle fracture, the functional to be
minimized is not convex, so that the necessary stationarity conditions of the
functional may admit multiple solutions. The solution obtained in an actual
computation is typically one out of several local minimizers. Evidence of
multiple solutions induced by small perturbations of numerical or physical
parameters was occasionally recorded but not explicitly investigated in the
literature. In this work, we focus on this issue and advocate a paradigm shift,
away from the search for one particular solution towards the simultaneous
description of all possible solutions (local minimizers), along with the
probabilities of their occurrence. Inspired by recent approaches advocating
measure-valued solutions (Young measures as well as their generalization to
statistical solutions) and their numerical approximations in fluid mechanics,
we propose the stochastic relaxation of the variational brittle fracture
problem through random perturbations of the functional. We introduce the
concept of stochastic solution, with the main advantage that point-to-point
correlations of the crack phase fields in the underlying domain can be
captured. These stochastic solutions are represented by random fields or random
variables with values in the classical deterministic solution spaces. In the
numerical experiments, we use a simple Monte Carlo approach to compute
approximations to such stochastic solutions. The final result of the
computation is not a single crack pattern, but rather several possible crack
patterns and their probabilities. The stochastic solution framework using
evolving random fields allows additionally the interesting possibility of
conditioning the probabilities of further crack paths on intermediate crack
patterns.",False,True,"Combining IGFEM-based topology optimization with stochastic phase-field fracture modeling offers a novel, feasible approach to design structures accounting for multiple crack patterns and their probabilities, effectively merging computational mechanics and uncertainty quantification.",
neg-1s-34,1,,2311.10216v2,"Bayesian mechanics of self-organising systems; Bayesian mechanics provides a framework that addresses dynamical systems that
can be conceptualised as Bayesian inference. However, elucidating the requisite
generative models is essential for empirical applications to realistic
self-organising systems. This work shows that the Hamiltonian of generic
dynamical systems constitutes a class of generative models, thus rendering
their Helmholtz energy equivalent to variational free energy under the
identified generative model. The self-organisation that minimises the Helmholtz
energy entails matching the system's Hamiltonian with that of the environment,
leading to the ensuing emergence of their generalised synchrony. In essence,
these self-organising systems can be read as performing variational Bayesian
inference of their interacting environment. These properties have been
demonstrated using coupled oscillators, simulated and living neural networks,
and quantum computers. This framework offers foundational characterisations and
predictions regarding asymptotic properties of self-organising systems
interacting with their environment, providing insights into potential
mechanisms underlying the emergence of intelligence.",2311.10383v1,"Affordance switching in self-organizing brain-body-environment systems; In the ecological approach to perception and action, information that
specifies affordances is available in the energy arrays surrounding organisms,
and this information is detected by organisms in order to perceptually guide
their actions. At the behavioral scale, organisms responding to affordances are
understood as self-organizing and reorganizing softly-assembled synergies.
Within the ecological community, little effort has so far been devoted to
studying this process at the neural scale, though interest in the topic is
growing under the header of ecological neuroscience. From this perspective,
switches between affordances may be conceptualized as transitions within
brain-body-environment systems as a whole rather than under the control of a
privileged (neural) scale. We discuss extant empirical research at the
behavioral scale in support of this view as well as ongoing and planned work at
the neural scale that attempts to further flesh out this view by characterizing
the neural dynamics that are associated with these transitions while
participants dynamically respond to affordances.",False,True,"Integrating Bayesian mechanics with affordance-based ecological neuroscience offers a novel, feasible framework to model neural and behavioral dynamics. This multidisciplinary approach is inventive and applicable for experimentally understanding perception-action mechanisms, meeting all required standards.",
neg-1s-35,1,,0904.1653v2,"An extension of Davis and Lo's contagion model; The present paper provides a multi-period contagion model in the credit risk
field. Our model is an extension of Davis and Lo's infectious default model. We
consider an economy of n firms which may default directly or may be infected by
other defaulting firms (a domino effect being also possible). The spontaneous
default without external influence and the infections are described by not
necessarily independent Bernoulli-type random variables. Moreover, several
contaminations could be required to infect another firm. In this paper we
compute the probability distribution function of the total number of defaults
in a dependency context. We also give a simple recursive algorithm to compute
this distribution in an exchangeability context. Numerical applications
illustrate the impact of exchangeability among direct defaults and among
contaminations, on different indicators calculated from the law of the total
number of defaults. We then examine the calibration of the model on iTraxx data
before and during the crisis. The dynamic feature together with the contagion
effect seem to have a significant impact on the model performance, especially
during the recent distressed period.",0904.2731v2,"An Introduction to Hedge Funds; This report was originally written as an industry white paper on Hedge Funds.
This paper gives an overview to Hedge Funds, with a focus on risk management
issues. We define and explain the general characteristics of Hedge Funds, their
main investment strategies and the risk models employed. We address the
problems in Hedge Fund modelling, survey current Hedge Funds available on the
market and those that have been withdrawn. Finally, we summarise the supporting
and opposing arguments for Hedge Fund usage. A unique value of this paper,
compared to other Hedge Fund literature freely available on the internet, is
that this review is fully sourced from academic references (such as peer
reviewed journals) and is thus a bona fide study. This paper will be of
interest to: Hedge Fund and Mutual Fund Managers, Quantitative Analysts,
""Front"" and ""Middle"" office banking functions e.g. Treasury Management,
Regulators concerned with Hedge Fund Financial Risk Management, Private and
Institutional Investors, Academic Researchers in the area of Financial Risk
Management and the general Finance community.",False,True,"Combining contagion models from credit risk with hedge fund risk management can create a novel approach to assess systemic risks in hedge funds. This multidisciplinary idea is feasible through data analysis, offers innovative insights, and effectively addresses risk assessment challenges in the financial sector.",
neg-1s-36,1,,2006.03476v1,"COVID-19 diagnosis by routine blood tests using machine learning; Physicians taking care of patients with coronavirus disease (COVID-19) have
described different changes in routine blood parameters. However, these
changes, hinder them from performing COVID-19 diagnosis. We constructed a
machine learning predictive model for COVID-19 diagnosis. The model was based
and cross-validated on the routine blood tests of 5,333 patients with various
bacterial and viral infections, and 160 COVID-19-positive patients. We selected
operational ROC point at a sensitivity of 81.9% and specificity of 97.9%. The
cross-validated area under the curve (AUC) was 0.97. The five most useful
routine blood parameters for COVID19 diagnosis according to the feature
importance scoring of the XGBoost algorithm were MCHC, eosinophil count,
albumin, INR, and prothrombin activity percentage. tSNE visualization showed
that the blood parameters of the patients with severe COVID-19 course are more
like the parameters of bacterial than viral infection. The reported diagnostic
accuracy is at least comparable and probably complementary to RT-PCR and chest
CT studies. Patients with fever, cough, myalgia, and other symptoms can now
have initial routine blood tests assessed by our diagnostic tool. All patients
with a positive COVID-19 prediction would then undergo standard RT-PCR studies
to confirm the diagnosis. We believe that our results present a significant
contribution to improvements in COVID-19 diagnosis.",2006.03913v2,"A computer simulation protocol to assess the accuracy of a Radio
  Stereometric Analysis (RSA) image processor according to the ISO-5725; Radio-Stereometric-Analysis and x-ray fluoroscopy are radiological techniques
that require dedicated software to process data. The accurate calibration of
these software is therefore critical. The aim of this work is to produce a
protocol for evaluating the softwares' accuracy according to the ISO-5725. A
series of computer simulations of the radiological setup and images were
employed. The noise level of the images was also changed to evaluate the
accuracy with different image qualities. The protocol was tested on a custom
software developed by the authors. Radiological scene reconstruction accuracy
was of (0.092 +- 0.14) mm for tube position, and (0.38 +- 0.31) mm / (2.09 +-
1.39) deg for detectors oriented in a direction other than the source-detector
direction. In the source-detector direction the accuracy was of (2.68 +- 3.08)
mm for tube position, and of (0.16 +- 0.27) mm / (0.075 +- 1.16) deg for the
detectors. These disparate results are widely discussed in the literature.
Model positioning and orientation was also highly accurate: (0.22 +- 0.46) mm /
(0.26 +- 0.22) deg. Accuracy was not affected by the noise level. The protocol
was able to assess the accuracy of the RSA system. It was also useful to detect
and fix hidden bugs. It was also useful to detect and resolve hidden bugs in
the software, and in optimizing the algorithms.",False,False,"The abstracts focus on machine learning for medical diagnosis and accuracy protocols for radiological software. There is no clear, innovative way to combine these distinct concepts to form a novel, feasible, and useful multidisciplinary research idea that meets all the specified standards.",
neg-1s-37,1,,1112.2638v1,"Dual representations for general multiple stopping problems; In this paper, we study the dual representation for generalized multiple
stopping problems, hence the pricing problem of general multiple exercise
options. We derive a dual representation which allows for cashflows which are
subject to volume constraints modeled by integer valued adapted processes and
refraction periods modeled by stopping times. As such, this extends the works
by Schoenmakers (2010), Bender (2011a), Bender (2011b), Aleksandrov and Hambly
(2010), and Meinshausen and Hambly (2004) on multiple exercise options, which
either take into consideration a refraction period or volume constraints, but
not both simultaneously. We also allow more flexible cashflow structures than
the additive structure in the above references. For example some exponential
utility problems are covered by our setting. We supplement the theoretical
results with an explicit Monte Carlo algorithm for constructing confidence
intervals for the price of multiple exercise options and exemplify it by a
numerical study on the pricing of a swing option in an electricity market.",1112.4534v1,"An application of the method of moments to volatility estimation using
  daily high, low, opening and closing prices; We use the expectation of the range of an arithmetic Brownian motion and the
method of moments on the daily high, low, opening and closing prices to
estimate the volatility of the stock price. The daily price jump at the opening
is considered to be the result of the unobserved evolution of an after-hours
virtual trading day.The annualized volatility is used to calculate
Black-Scholes prices for European options, and a trading strategy is devised to
profit when these prices differ flagrantly from the market prices.",False,False,"Combining concepts from the two papers doesn't clearly form a novel, feasible, and multidisciplinary research idea that meets all the specified standards.",
neg-1s-38,1,,2008.06437v1,"Stochastic approach to entropy production in chemical chaos; Methods are presented to evaluate the entropy production rate in stochastic
reactive systems. These methods are shown to be consistent with known results
from nonequilibrium chemical thermodynamics. Moreover, it is proved that the
time average of the entropy production rate can be decomposed into the
contributions of the cycles obtained from the stoichiometric matrix in both
stochastic processes and deterministic systems. These methods are applied to a
complex reaction network constructed on the basis of Roessler's reinjection
principle and featuring chemical chaos.",2008.07419v1,"Semiclassical treatment of quantum chaotic transport with a tunnel
  barrier; We consider the problem of a semiclassical description of quantum chaotic
transport, when a tunnel barrier is present in one of the leads. Using a
semiclassical approach formulated in terms of a matrix model, we obtain
transport moments as power series in the reflection probability of the barrier,
whose coefficients are rational functions of the number of open channels M. Our
results are therefore valid in the quantum regime and not only when $M\gg 1$.
The expressions we arrive at are not identical with the corresponding
predictions from random matrix theory, but are in fact much simpler. Both
theories agree as far as we can test.",False,False,"The abstracts do not present overlapping concepts that can be feasibly combined into a novel, multidisciplinary research idea meeting all the specified standards.",
neg-1s-39,1,,2003.01809v1,"Numerical Solution of Dynamic Portfolio Optimization with Transaction
  Costs; We apply numerical dynamic programming techniques to solve discrete-time
multi-asset dynamic portfolio optimization problems with proportional
transaction costs and shorting/borrowing constraints. Examples include problems
with multiple assets, and many trading periods in a finite horizon problem. We
also solve dynamic stochastic problems, with a portfolio including one
risk-free asset, an option, and its underlying risky asset, under the existence
of transaction costs and constraints. These examples show that it is now
tractable to solve such problems.",2003.06987v1,"Degrees of displacement: The impact of household PV battery prosumage on
  utility generation and storage; Reductions in the cost of PV and batteries encourage households to invest in
PV battery prosumage. We explore the implications for the rest of the power
sector by applying two open-source techno-economic models to scenarios in
Western Australia for the year 2030. Household PV capacity generally
substitutes utility PV, but slightly less so as additional household batteries
are installed. Wind power is less affected, especially in scenarios with higher
shares of renewables. With household batteries operating to maximise
self-consumption, utility battery capacities are hardly substituted. Wholesale
prices to supply households, including those not engaging in prosumage,
slightly decrease, while prices for other consumers slightly increase. We
conclude that the growth of prosumage has implications on the various elements
of the power sector and should be more thoroughly considered by investors,
regulators, and power sector planners.",False,True,"Combining dynamic portfolio optimization with household PV battery prosumage can create a novel framework for optimizing energy investments considering transaction costs and constraints. This multidisciplinary approach is feasible, innovative, and offers practical solutions for energy sector planning and investment strategies.",
neg-1s-40,1,,1511.08621v2,"Pfcrmp May Play a Key Role in Chloroquine Antimalarial Action and
  Resistance Development; It was proposed earlier that Pfcrmp (Plasmodium falciparum chloroquine
resistance marker protein) may be the chloroquine's target protein in nucleus.
In this communication, further evidence is presented to support the view that
Pfcrmp may play a key role in chloroquine antimalarial actions as well as
resistance development.",1511.09076v1,"Mechanism of dynamic reorientation of cortical microtubules due to
  mechanical stress; Directional growth caused by gravitropism and corresponding bending of plant
cells has been explored since 19th century, however, many aspects of mechanisms
underlying the perception of gravity at the molecular level are still not well
known. Perception of gravity in root and shoot gravitropisms is usually
attributed to gravisensitive cells, called statocytes, which exploit
sedimentation of macroscopic and heavy organelles, amyloplasts, to sense the
direction of gravity. Gravity stimulus is then transduced into distal
elongation zone, which is several mm far from statocytes, where it causes
stretching. It is suggested that gravity stimulus is conveyed by gradients in
auxin flux. We propose a theoretical model that may explain how concentration
gradients and/or stretching may indirectly affect the global orientation of
cortical microtubules, attached to the cell membrane and induce their dynamic
reorientation perpendicular to the gradients. In turn, oriented microtubules
arrays direct the growth and orientation of cellulose microfibrils, forming
part of the cell external skeleton and determine the shape of the cell.
Reorientation of microtubules is also observed in reaction to light in
phototropism and mechanical bending, thus suggesting universality of the
proposed mechanism.",False,False,"The two papers cover distinct fields (antimalarial resistance and plant microtubule dynamics) without a clear, feasible, and novel intersection that meets all multidisciplinary research standards.",
neg-1s-41,1,,2204.03718v2,"First-passage times in complex energy landscapes: a case study with
  nonmuscle myosin II assembly; Complex energy landscapes often arise in biological systems, e.g. for protein
folding, biochemical reactions or intracellular transport processes. Their
physical effects are often reflected in the first-passage times arising from
these energy landscapes. However, their calculation is notoriously challenging
and it is often difficult to identify the most relevant features of a given
energy landscape. Here we show how this can be achieved by coarse-graining the
Fokker-Planck equation to a master equation and decomposing its first-passage
times in an iterative process. We apply this method to the electrostatic
interaction between two rods of nonmuscle myosin II (NM2), which is the main
molecular motor for force generation in nonmuscle cells. Energy landscapes are
computed directly from the amino acid sequences of the three different
isoforms. Our approach allows us to identify the most relevant energy barriers
for their self-assembly into nonmuscle myosin II minifilaments and how they
change under force. In particular, we find that antiparallel configurations are
more stable than parallel ones, but also show more changes under mechanical
loading. Our work demonstrates the rich dynamics that can be expected for
NM2-assemblies under mechanical load and in general shows how one can identify
the most relevant energy barriers in complex energy landscapes.",2204.12623v1,"SANA: Cross-Species Prediction of Gene Ontology GO Annotations via
  Topological Network Alignment; Topological network alignment aims to align two networks node-wise in order
to maximize the observed common connection (edge) topology between them. The
topological alignment of two Protein-Protein Interaction (PPI) networks should
thus expose protein pairs with similar interaction partners allowing, for
example, the prediction of common Gene Ontology (GO) terms. Unfortunately, no
network alignment algorithm based on topology alone has been able to achieve
this aim, though those that include sequence similarity have seen some success.
We argue that this failure of topology alone is due to the sparsity and
incompleteness of the PPI network data of almost all species, which provides
the network topology with a small signal-to-noise ratio that is effectively
swamped when sequence information is added to the mix. Here we show that the
weak signal can be detected using multiple stochastic samples of ""good""
topological network alignments, which allows us to observe regions of the two
networks that are robustly aligned across multiple samples. The resulting
Network Alignment Frequency (NAF) strongly correlates with GO-based Resnik
semantic similarity and enables the first successful cross-species predictions
of GO terms based on topology-only network alignments. Our best predictions
have an AUPR of about 0.4, which is competitive with state-of-the-art
algorithms, even when there is no observable sequence similarity and no known
homology relationship. While our results provide only a ""proof of concept"" on
existing network data, we hypothesize that predicting GO terms from
topology-only network alignments will become increasingly practical as the
volume and quality of PPI network data increase.",False,False,"Combining complex energy landscape analysis with network alignment for GO prediction does not clearly meet all the multidisciplinary, feasible, novel, and useful criteria based on the provided abstracts.",
neg-1s-42,1,,1111.1113v2,"Copula-based Hierarchical Aggregation of Correlated Risks. The behaviour
  of the diversification benefit in Gaussian and Lognormal Trees; The benefits of diversifying risks are difficult to estimate quantitatively
because of the uncertainties in the dependence structure between the risks.
Also, the modelling of multidimensional dependencies is a non-trivial task.
This paper focuses on one such technique for portfolio aggregation, namely the
aggregation of risks within trees, where dependencies are set at each step of
the aggregation with the help of some copulas. We define rigorously this
procedure and then study extensively the Gaussian Tree of quite arbitrary size
and shape, where individual risks are normal, and where the Gaussian copula is
used. We derive exact analytical results for the diversification benefit of the
Gaussian tree as a function of its shape and of the dependency parameters.
  Such a ""toy-model"" of an aggregation tree enables one to understand the basic
phenomena's at play while aggregating risks in this way. In particular, it is
shown that, for a fixed number of individual risks, ""thin"" trees diversify
better than ""fat"" trees. Related to this, it is shown that hierarchical trees
have the natural tendency to lower the overall dependency with respect to the
dependency parameter chosen at each step of the aggregation. We also show that
these results hold in more general cases outside the gaussian world, and apply
notably to more realistic portfolios (LogNormal trees). We believe that any
insurer or reinsurer using such a tool should be aware of these systematic
effects, and that this awareness should strongly call for designing trees that
adequately fit the business.
  We finally address the issue of specifying the full joint distribution
between the risks. We show that the hierarchical mechanism does not require nor
specify the joint distribution, but that the latter can be determined exactly
(in the Gaussian case) by adding conditional independence hypotheses between
the risks and their sums.",1111.2584v1,"Numerical Solutions of Optimal Risk Control and Dividend Optimization
  Policies under A Generalized Singular Control Formulation; This paper develops numerical methods for finding optimal dividend pay-out
and reinsurance policies. A generalized singular control formulation of surplus
and discounted payoff function are introduced, where the surplus is modeled by
a regime-switching process subject to both regular and singular controls. To
approximate the value function and optimal controls, Markov chain approximation
techniques are used to construct a discrete-time controlled Markov chain with
two components. The proofs of the convergence of the approximation sequence to
the surplus process and the value function are given. Examples of proportional
and excess-of-loss reinsurance are presented to illustrate the applicability of
the numerical methods.",False,True,"By integrating copula-based hierarchical risk aggregation with numerical optimal control policies, a novel approach to optimize dividend and reinsurance strategies considering complex dependency structures can be developed. This multidisciplinary idea is feasible, innovative, and offers practical benefits for risk management.",
neg-1s-43,1,,1910.04868v2,"Estimating localized complexity of white-matter wiring with GANs; In-vivo examination of the physical connectivity of axonal projections
through the white matter of the human brain is made possible by diffusion
weighted magnetic resonance imaging (dMRI) Analysis of dMRI commonly considers
derived scalar metrics such as fractional anisotrophy as proxies for ""white
matter integrity,"" and differences of such measures have been observed as
significantly correlating with various neurological diagnosis and clinical
measures such as executive function, presence of multiple sclerosis, and
genetic similarity. The analysis of such voxel measures is confounded in areas
of more complicated fiber wiring due to crossing, kissing, and dispersing
fibers. Recently, Volz et al. introduced a simple probabilistic measure of the
count of distinct fiber populations within a voxel, which was shown to reduce
variance in group comparisons. We propose a complementary measure that
considers the complexity of a voxel in context of its local region, with an aim
to quantify the localized wiring complexity of every part of white matter. This
allows, for example, identification of particularly ambiguous regions of the
brain for tractographic approaches of modeling global wiring connectivity. Our
method builds on recent advances in image inpainting, in which the task is to
plausibly fill in a missing region of an image. Our proposed method builds on a
Bayesian estimate of heteroscedastic aleatoric uncertainty of a region of white
matter by inpainting it from its context. We define the localized wiring
complexity of white matter as how accurately and confidently a well-trained
model can predict the missing patch. In our results, we observe low aleatoric
uncertainty along major neuronal pathways which increases at junctions and
towards cortex boundaries. This directly quantifies the difficulty of lesion
inpainting of dMRI images at all parts of white matter.",1910.06741v1,"Adaptive template systems: Data-driven feature selection for learning
  with persistence diagrams; Feature extraction from persistence diagrams, as a tool to enrich machine
learning techniques, has received increasing attention in recent years. In this
paper we explore an adaptive methodology to localize features in persistent
diagrams, which are then used in learning tasks. Specifically, we investigate
three algorithms, CDER, GMM and HDBSCAN, to obtain adaptive template
functions/features. Said features are evaluated in three classification
experiments with persistence diagrams. Namely, manifold, human shapes and
protein classification. The main conclusion of our analysis is that adaptive
template systems, as a feature extraction technique, yield competitive and
often superior results in the studied examples. Moreover, from the adaptive
algorithms here studied, CDER consistently provides the most reliable and
robust adaptive featurization.",False,True,"Combining localized white-matter complexity estimation with adaptive feature selection from persistence diagrams creates a novel, feasible, and useful multidisciplinary research approach that can be experimentally validated and addresses complex neurological data analysis.",
neg-1s-44,1,,1101.1415v1,"Bayesian semiparametric inference for multivariate
  doubly-interval-censored data; Based on a data set obtained in a dental longitudinal study, conducted in
Flanders (Belgium), the joint time to caries distribution of permanent first
molars was modeled as a function of covariates. This involves an analysis of
multivariate continuous doubly-interval-censored data since: (i) the emergence
time of a tooth and the time it experiences caries were recorded yearly, and
(ii) events on teeth of the same child are dependent. To model the joint
distribution of the emergence times and the times to caries, we propose a
dependent Bayesian semiparametric model. A major feature of the proposed
approach is that survival curves can be estimated without imposing assumptions
such as proportional hazards, additive hazards, proportional odds or
accelerated failure time.",1101.1421v1,"Sparse modeling of categorial explanatory variables; Shrinking methods in regression analysis are usually designed for metric
predictors. In this article, however, shrinkage methods for categorial
predictors are proposed. As an application we consider data from the Munich
rent standard, where, for example, urban districts are treated as a categorial
predictor. If independent variables are categorial, some modifications to usual
shrinking procedures are necessary. Two $L_1$-penalty based methods for factor
selection and clustering of categories are presented and investigated. The
first approach is designed for nominal scale levels, the second one for ordinal
predictors. Besides applying them to the Munich rent standard, methods are
illustrated and compared in simulation studies.",False,True,"Combining Bayesian semiparametric survival models with sparse categorical variable modeling can create a novel, feasible, and useful multidisciplinary research idea that leverages both papers’ methodologies to address dependent censored data with categorical predictors.",
neg-1s-45,1,,2205.09890v2,"Replicating Portfolios: Constructing Permissionless Derivatives; The current design space of derivatives in Decentralized Finance (DeFi)
relies heavily on oracle systems. Replicating market makers (RMMs) provide a
mechanism for converting specific payoff functions to an associated Constant
Function Market Makers (CFMMs). We leverage RMMs to replicate the approximate
payoff of a Black-Scholes covered call option. RMM-01 is the first
implementation of an on-chain expiring option mechanism that relies on
arbitrage rather than an external oracle for price. We provide frameworks for
derivative instruments and structured products achievable on-chain without
relying on oracles. We construct long and binary options and briefly discuss
perpetual covered call strategies commonly referred to as ""theta vaults.""
Moreover, we introduce a procedure to eliminate liquidation risk in lending
markets. The results suggest that CFMMs are essential for structured product
design with minimized trust dependencies.",2205.13321v2,"A new self-exciting jump-diffusion process for option pricing; We propose a new jump-diffusion process, the Heston-Queue-Hawkes (HQH) model,
combining the well-known Heston model and the recently introduced Queue-Hawkes
(Q-Hawkes) jump process. Like the Hawkes process, the HQH model can capture the
effects of self-excitation and contagion. However, since the characteristic
function of the HQH process is known in closed-form, Fourier-based fast pricing
algorithms, like the COS method, can be fully exploited with this model.
Furthermore, we show that by using partial integrals of the characteristic
function, which are also explicitly known for the HQH process, we can reduce
the dimensionality of the COS method, and so its numerical complexity.
Numerical results for European and Bermudan options show that the HQH model
offers a wider range of volatility smiles compared to the Bates model, while
its computational burden is considerably smaller than that of the Heston-Hawkes
(HH) process.",False,True,"Combining on-chain derivative structures from DeFi with the HQH option pricing model could create innovative, experimentally testable financial instruments that enhance pricing accuracy and risk management, meeting multidisciplinary, feasible, novel, and useful criteria.",
neg-1s-46,1,,2001.05095v2,"Production externalities and dispersion process in a multi-region
  economy; We consider an economic geography model with two inter-regional proximity
structures: one governing goods trade and the other governing production
externalities across regions. We investigate how the introduction of the latter
affects the timing of endogenous agglomeration and the spatial distribution of
workers across regions. As transportation costs decline, the economy undergoes
a progressive dispersion process. Mono-centric agglomeration emerges when
inter-regional trade and/or production externalities incur high transportation
costs, while uniform dispersion occurs when these costs become negligibly small
(i.e., when distance dies). In multi-regional geography, the network structure
of production externalities can determine the geographical distribution of
workers as economic integration increases. If production externalities are
governed solely by geographical distance, a mono-centric spatial distribution
emerges in the form of suburbanization. However, if geographically distant
pairs of regions are connected through tight production linkages, multi-centric
spatial distribution can be sustainable.",2001.06052v1,"Recovering Network Structure from Aggregated Relational Data using
  Penalized Regression; Social network data can be expensive to collect. Breza et al. (2017) propose
aggregated relational data (ARD) as a low-cost substitute that can be used to
recover the structure of a latent social network when it is generated by a
specific parametric random effects model. Our main observation is that many
economic network formation models produce networks that are effectively
low-rank. As a consequence, network recovery from ARD is generally possible
without parametric assumptions using a nuclear-norm penalized regression. We
demonstrate how to implement this method and provide finite-sample bounds on
the mean squared error of the resulting estimator for the distribution of
network links. Computation takes seconds for samples with hundreds of
observations. Easy-to-use code in R and Python can be found at
https://github.com/mpleung/ARD.",False,True,"Combining economic geography with advanced network recovery techniques offers a novel, feasible approach to spatial economic models, enabling empirical validation through experiments. This multidisciplinary idea is imaginative and can effectively address the distribution of workers and regional agglomeration.",
neg-1s-47,1,,0907.4680v1,"Network motifs come in sets: correlations in the randomization process; The identification of motifs--subgraphs that appear significantly more often
in a particular network than in an ensemble of randomized networks--has become
a ubiquitous method for uncovering potentially important subunits within
networks drawn from a wide variety of fields. We find that the most common
algorithms used to generate the ensemble from the real network change subgraph
counts in a highly correlated manner, so that one subgraph's status as a motif
may not be independent from the statuses of the other subgraphs. We demonstrate
this effect for the problem of 3- and 4-node motif identification in the
transcriptional regulatory networks of E. coli and S. cerevisiae in which
randomized networks are generated via an edge-swapping algorithm (Milo et al.,
Science 298:824, 2002). We show that correlations among 3-node subgraphs are
easily interpreted, and we present an information-theoretic tool that may be
used to identify correlations among subgraphs of any size.",0908.0146v1,"Inferring genetic networks: An information theoretic approach; In the postgenome era many efforts have been dedicated to systematically
elucidate the complex web of interacting genes and proteins. These efforts
include experimental and computational methods. Microarray technology offers an
opportunity for monitoring gene expression level at the genome scale. By
recourse to information theory, this study proposes a mathematical approach to
reconstruct gene regulatory networks at coarse-grain level from high throughput
gene expression data. The method provides the {\it a posteriori} probability
that a given gene regulates positively, negatively or does not regulate each
one of the network genes. This approach also allows the introduction of prior
knowledge and the quantification of the information gain from experimental data
used in the inference procedure. This information gain can be used to chose
genes to be perturbed in subsequent experiments in order to refine the
knowledge about the architecture of an underlying gene regulatory network. The
performance of the proposed approach has been studied by {\it in numero}
experiments. Our results suggest that the approach is suitable for focusing on
size-limited problems, such as, recovering a small subnetwork of interest by
performing perturbation over selected genes.",False,True,"Combining motif correlation analysis from Paper 1 with information-theoretic genetic network inference from Paper 2 can create a novel, feasible, and useful research approach to enhance gene regulatory network reconstruction, meeting all multidisciplinary standards.",
neg-1s-48,1,,2210.07129v1,"Economic incentives for capacity reductions on interconnectors in the
  day-ahead market; We consider a zonal international power market and investigate potential
economic incentives for short-term reductions of transmission capacities on
existing interconnectors by the responsible transmission system operators
(TSOs). We show that if a TSO aims to maximize domestic total welfare, it often
has an incentive to reduce the capacity on the interconnectors to neighboring
countries.
  In contrast with the (limited) literature on this subject, which focuses on
incentives through the avoidance of future balancing costs, we show that
incentives can exist even if one ignores balancing and focuses solely on
welfare gains in the day-ahead market itself. Our analysis consists of two
parts. In the first part, we develop an analytical framework that explains why
these incentives exist. In particular, we distinguish two mechanisms: one based
on price differences with neighboring countries and one based on the domestic
electricity price. In the second part, we perform numerical experiments using a
model of the Northern-European power system, focusing on the Danish TSO. In 97%
of the historical hours tested, we indeed observe economic incentives for
capacity reductions, leading to significant welfare gains for Denmark and
welfare losses for the system as a whole. We show that the potential for
welfare gains greatly depends on the ability of the TSO to adapt interconnector
capacities to short-term market conditions. Finally, we explore the extent to
which the recently introduced European ""70%-rule"" can mitigate the incentives
for capacity reductions and their welfare effects.",2210.08785v1,"Welfare estimations from imagery. A test of domain experts ability to
  rate poverty from visual inspection of satellite imagery; The present study uses domain experts to estimate welfare levels and
indicators from high-resolution satellite imagery. We use the wealth quintiles
from the 2015 Tanzania DHS dataset as ground truth data. We analyse the
performance of the visual estimation of relative wealth at the cluster level
and compare these with wealth rankings from the DHS survey of 2015 for that
country using correlations, ordinal regressions and multinomial logistic
regressions. Of the 608 clusters, 115 received the same ratings from human
experts and the independent DHS rankings. For 59 percent of the clusters,
experts ratings were slightly lower. On the one hand, significant positive
predictors of wealth are the presence of modern roofs and wider roads. For
instance, the log odds of receiving a rating in a higher quintile on the wealth
rankings is 0.917 points higher on average for clusters with buildings with
slate or tile roofing compared to those without. On the other hand, significant
negative predictors included poor road coverage, low to medium greenery
coverage, and low to medium building density. Other key predictors from the
multinomial regression model include settlement structure and farm sizes. These
findings are significant to the extent that these correlates of wealth and
poverty are visually readable from satellite imagery and can be used to train
machine learning models in poverty predictions. Using these features for
training will contribute to more transparent ML models and, consequently,
explainable AI.",False,True,"Combining economic incentives for power interconnector capacity reductions with satellite-derived welfare indicators could create a novel approach to assess and optimize TSO decisions based on localized welfare impacts. This multidisciplinary idea is feasible, innovative, and offers practical benefits for power market strategies.",
neg-1s-49,1,,2103.13039v1,"Note on the offspring distribution for group testing in the linear
  regime; The group testing problem is concerned with identifying a small set of $k$
infected individuals in a large population of $n$ people. At our disposal is a
testing scheme that can test groups of individuals. A test comes back positive
if and only if at least one individual is infected. In this note, we lay
groundwork for analysing belief propagation for group testing when $k$ scales
linearly in $n$. To this end, we derive the offspring distribution for
different types of individuals. With these distributions at hand, one can
employ the population dynamics algorithm to simulate the posterior marginal
distribution resulting from belief propagation.",2103.14599v1,"Minimum Scan Cover and Variants -- Theory and Experiments; We consider a spectrum of geometric optimization problems motivated by
contexts such as satellite communication and astrophysics. In the problem
Minimum Scan Cover with Angular Costs, we are given a graph $G$ that is
embedded in Euclidean space. The edges of $G$ need to be scanned, i.e., probed
from both of their vertices. In order to scan their edge, two vertices need to
face each other; changing the heading of a vertex incurs some cost in terms of
energy or rotation time that is proportional to the corresponding rotation
angle. Our goal is to compute schedules that minimize the following objective
functions: (i) in Minimum Makespan Scan Cover (MSC-MS), this is the time until
all edges are scanned; (ii) in Minimum Total Energy Scan Cover (MSC-TE), the
sum of all rotation angles; (iii) in Minimum Bottleneck Energy Scan Cover
(MSC-BE), the maximum total rotation angle at one vertex.
  Previous theoretical work on MSC-MS revealed a close connection to graph
coloring and the cut cover problem, leading to hardness and approximability
results. In this paper, we present polynomial-time algorithms for 1D instances
of MSC-TE and MSC-BE, but NP-hardness proofs for bipartite 2D instances. For
bipartite graphs in 2D, we also give 2-approximation algorithms for both MSC-TE
and MSC-BE. Most importantly, we provide a comprehensive study of practical
methods for all three problems. We compare three different mixed-integer
programming and two constraint programming approaches, and show how to compute
provably optimal solutions for geometric instances with up to 300 edges.
Additionally, we compare the performance of different meta-heuristics for even
larger instances.",False,True,"Combining belief propagation in group testing with geometric scan optimization can create a novel, feasible, and useful approach for efficiently identifying targets in spatially distributed systems, meeting all multidisciplinary research standards.",
neg-1s-50,1,,1103.5716v1,"Star-covering properties: generalized $Ψ$-spaces, countability
  conditions, reflection; We investigate star-covering properties of $\Psi$-like spaces. We show
star-Lindel\""ofness is reflected by open perfect mappings. In addition, we
offer a new equivalence of CH.",1104.2793v1,"Lindelof spaces which are indestructible, productive, or D; We discuss relationships in Lindelof spaces among the properties
""indestructible"", ""productive"", ""D"", and related properties.",False,False,"Both papers focus on similar areas within topology, lacking the combination of distinct disciplines required for a multidisciplinary research idea.",
neg-1s-51,1,,2002.09926v1,"CATCH: Characterizing and Tracking Colloids Holographically using deep
  neural networks; In-line holographic microscopy provides an unparalleled wealth of information
about the properties of colloidal dispersions. Analyzing one colloidal
particle's hologram with the Lorenz-Mie theory of light scattering yields the
particle's three-dimensional position with nanometer precision while
simultaneously reporting its size and refractive index with part-per-thousand
resolution. Analyzing a few thousand holograms in this way provides a
comprehensive picture of the particles that make up a dispersion, even for
complex multicomponent systems. All of this valuable information comes at the
cost of three computationally expensive steps: (1) identifying and localizing
features of interest within recorded holograms, (2) estimating each particle's
properties based on characteristics of the associated features, and finally (3)
optimizing those estimates through pixel-by-pixel fits to a generative model.
Here, we demonstrate an end-to-end implementation that is based entirely on
machine-learning techniques. Characterizing and Tracking Colloids
Holographically (CATCH) with deep convolutional neural networks is fast enough
for real-time applications and otherwise outperforms conventional analytical
algorithms, particularly for heterogeneous and crowded samples. We demonstrate
this system's capabilities with experiments on free-flowing and holographically
trapped colloidal spheres.",2002.10032v3,"Generalized Octave Convolutions for Learned Multi-Frequency Image
  Compression; Learned image compression has recently shown the potential to outperform the
standard codecs. State-of-the-art rate-distortion (R-D) performance has been
achieved by context-adaptive entropy coding approaches in which hyperprior and
autoregressive models are jointly utilized to effectively capture the spatial
dependencies in the latent representations. However, the latents are feature
maps of the same spatial resolution in previous works, which contain some
redundancies that affect the R-D performance. In this paper, we propose the
first learned multi-frequency image compression and entropy coding approach
that is based on the recently developed octave convolutions to factorize the
latents into high and low frequency (resolution) components, where the low
frequency is represented by a lower resolution. Therefore, its spatial
redundancy is reduced, which improves the R-D performance. Novel generalized
octave convolution and octave transposed-convolution architectures with
internal activation layers are also proposed to preserve more spatial structure
of the information. Experimental results show that the proposed scheme not only
outperforms all existing learned methods as well as standard codecs such as the
next-generation video coding standard VVC (4:2:0) on the Kodak dataset in both
PSNR and MS-SSIM. We also show that the proposed generalized octave convolution
can improve the performance of other auto-encoder-based computer vision tasks
such as semantic segmentation and image denoising.",False,True,"Combining deep neural network-based colloid tracking with multi-frequency image compression could enable real-time, efficient analysis of complex holographic microscopy data, is experimentally feasible, offers a novel integration of techniques, and enhances processing performance for practical applications.",
neg-1s-52,1,,2104.01127v1,"Perpetual callable American volatility options in a mean-reverting
  volatility model; This paper investigates problems associated with the valuation of callable
American volatility put options. Our approach involves modeling volatility
dynamics as a mean-reverting 3/2 volatility process. We first propose a pricing
formula for the perpetual American knock-out put. Under the given conditions,
the value of perpetual callable American volatility put options is discussed.",2108.05747v1,"Comment on ""An appropriate approach to pricing european-style options
  with the Adomian decomposition method""; We show that the Adomian decomposition method proposed by Ke et al [ANZIAM J.
\textbf{59} (2018) 349] is just the Taylor series approach in disguise. The
latter approach is simpler, more straightforward and yields a recurrence
relation free from integrals.",False,False,"Both papers focus on option pricing within financial mathematics, lacking integration from distinct disciplines to form a truly multidisciplinary and novel research idea.",
neg-1s-53,1,,2410.20597v1,"Extracting Alpha from Financial Analyst Networks; We investigate the effectiveness of a momentum trading signal based on the
coverage network of financial analysts. This signal builds on the key
information-brokerage role financial sell-side analysts play in modern stock
markets. The baskets of stocks covered by each analyst can be used to construct
a network between firms whose edge weights represent the number of analysts
jointly covering both firms. Although the link between financial analysts
coverage and co-movement of firms' stock prices has been investigated in the
literature, little effort has been made to systematically learn the most
effective combination of signals from firms covered jointly by analysts in
order to benefit from any spillover effect. To fill this gap, we build a
trading strategy which leverages the analyst coverage network using a graph
attention network. More specifically, our model learns to aggregate information
from individual firm features and signals from neighbouring firms in a
node-level forecasting task. We develop a portfolio based on those predictions
which we demonstrate to exhibit an annualized returns of 29.44% and a Sharpe
ratio of 4.06 substantially outperforming market baselines and existing graph
machine learning based frameworks. We further investigate the performance and
robustness of this strategy through extensive empirical analysis. Our paper
represents one of the first attempts in using graph machine learning to extract
actionable knowledge from the analyst coverage network for practical financial
applications.",2410.22519v1,"Evaluating utility in synthetic banking microdata applications; Financial regulators such as central banks collect vast amounts of data, but
access to the resulting fine-grained banking microdata is severely restricted
by banking secrecy laws. Recent developments have resulted in mechanisms that
generate faithful synthetic data, but current evaluation frameworks lack a
focus on the specific challenges of banking institutions and microdata. We
develop a framework that considers the utility and privacy requirements of
regulators, and apply this to financial usage indices, term deposit yield
curves, and credit card transition matrices. Using the Central Bank of
Paraguay's data, we provide the first implementation of synthetic banking
microdata using a central bank's collected information, with the resulting
synthetic datasets for all three domain applications being publicly available
and featuring information not yet released in statistical disclosure. We find
that applications less susceptible to post-processing information loss, which
are based on frequency tables, are particularly suited for this approach, and
that marginal-based inference mechanisms to outperform generative adversarial
network models for these applications. Our results demonstrate that synthetic
data generation is a promising privacy-enhancing technology for financial
regulators seeking to complement their statistical disclosure, while
highlighting the crucial role of evaluating such endeavors in terms of utility
and privacy requirements.",False,True,"Combining graph machine learning from analyst networks with synthetic banking microdata can create innovative financial prediction models that preserve privacy. This multidisciplinary approach is feasible through experimental validation, offers novel integration of data privacy with advanced analytics, and addresses practical needs in financial regulation and market forecasting.",
neg-1s-54,1,,1909.09641v3,"Productivity propagation with networks transformation; We model sectoral production by cascading binary compounding processes. The
sequence of processes is discovered in a self-similar hierarchical structure
stylized in the economy-wide networks of production. Nested substitution
elasticities and Hicks-neutral productivity growth are measured such that the
general equilibrium feedbacks between all sectoral unit cost functions
replicate the transformation of networks observed as a set of two temporally
distant input-output coefficient matrices. We examine this system of unit cost
functions to determine how idiosyncratic sectoral productivity shocks propagate
into aggregate macroeconomic fluctuations in light of potential network
transformation. Additionally, we study how sectoral productivity increments
propagate into the dynamic general equilibrium, thereby allowing network
transformation and ultimately producing social benefits.",1909.09824v3,"Desperate times call for desperate measures: government spending
  multipliers in hard times; We investigate state-dependent effects of fiscal multipliers and allow for
endogenous sample splitting to determine whether the US economy is in a slack
state. When the endogenized slack state is estimated as the period of the
unemployment rate higher than about 12 percent, the estimated cumulative
multipliers are significantly larger during slack periods than non-slack
periods and are above unity. We also examine the possibility of time-varying
regimes of slackness and find that our empirical results are robust under a
more flexible framework. Our estimation results point out the importance of the
heterogenous effects of fiscal policy and shed light on the prospect of fiscal
policy in response to economic shocks from the current COVID-19 pandemic.",False,True,"Combining network-based productivity propagation with state-dependent fiscal multipliers offers a novel approach to understanding how fiscal policy can influence sectoral shocks within production networks during economic slacks. This multidisciplinary idea is feasible, innovative, and addresses relevant economic challenges effectively.",
neg-1s-55,1,,2312.05189v1,"Distributed Autonomous Organizations as Public Services Supplying
  Platform; Servizi Elaborazioni Dati SpA is a public company owned by Municipality of L
Aquila, it supplies the institution with network services and software
applications for distributing services to citizens. The future policy of the
company is to enlarge the offer of its services to nearby communities that are
unable to set up and maintain their own network and software structures. This
paper presents thus a possible architecture model to support small
municipalities in supplying public services to citizens, with the aid of SED
Spa. Through second level platforms based on Blockchain networks and
Multi-agents Systems running on smart contracts, the system will focus on Waste
Tax (Ta.Ri) management system in the Fascicolo del Cittadino environment.",2312.07001v1,"Stein Coverage: a Variational Inference Approach to
  Distribution-matching Multisensor Deployment; This paper examines the spatial coverage optimization problem for multiple
sensors in a known convex environment, where the coverage service of each
sensor is heterogeneous and anisotropic. We introduce the Stein Coverage
algorithm, a distribution-matching coverage approach that aims to place sensors
at positions and orientations such that their collective coverage distribution
is as close as possible to the event distribution. To select the most important
representative points from the coverage event distribution, Stein Coverage
utilizes the Stein Variational Gradient Descent (SVGD), a deterministic
sampling method from the variational inference literature. An innovation in our
work is the introduction of a repulsive force between the samples in the SVGD
algorithm to spread the samples and avoid footprint overlap for the deployed
sensors. After pinpointing the points of interest for deployment, Stein
Coverage solves the multisensor assignment problem using a bipartite optimal
matching process. Simulations demonstrate the advantages of the Stein Coverage
method compared to conventional Voronoi partitioning multisensor deployment
methods.",False,True,"Combining blockchain-based public service platforms with advanced sensor deployment algorithms can create a novel system for optimizing public resource management. This multidisciplinary approach is feasible, innovative, and offers practical solutions for efficient service delivery in municipalities.",
neg-1s-56,1,,2203.13820v3,"Rough volatility: fact or artefact?; We investigate the statistical evidence for the use of `rough' fractional
processes with Hurst exponent $H< 0.5$ for the modeling of volatility of
financial assets, using a model-free approach. We introduce a non-parametric
method for estimating the roughness of a function based on discrete sample,
using the concept of normalized $p$-th variation along a sequence of
partitions. We investigate the finite sample performance of our estimator for
measuring the roughness of sample paths of stochastic processes using detailed
numerical experiments based on sample paths of fractional Brownian motion and
other fractional processes. We then apply this method to estimate the roughness
of realized volatility signals based on high-frequency observations. Detailed
numerical experiments based on stochastic volatility models show that, even
when the instantaneous volatility has diffusive dynamics with the same
roughness as Brownian motion, the realized volatility exhibits rough behaviour
corresponding to a Hurst exponent significantly smaller than $0.5$. Comparison
of roughness estimates for realized and instantaneous volatility in fractional
volatility models with different values of Hurst exponent shows that,
irrespective of the roughness of the spot volatility process, realized
volatility always exhibits `rough' behaviour with an apparent Hurst index
$\hat{H}<0.5$. These results suggest that the origin of the roughness observed
in realized volatility time-series lies in the microstructure noise rather than
the volatility process itself.",2204.00872v1,"Calibration window selection based on change-point detection for
  forecasting electricity prices; We employ a recently proposed change-point detection algorithm, the
Narrowest-Over-Threshold (NOT) method, to select subperiods of past
observations that are similar to the currently recorded values. Then,
contrarily to the traditional time series approach in which the most recent
$\tau$ observations are taken as the calibration sample, we estimate
autoregressive models only for data in these subperiods. We illustrate our
approach using a challenging dataset - day-ahead electricity prices in the
German EPEX SPOT market - and observe a significant improvement in forecasting
accuracy compared to commonly used approaches, including the Autoregressive
Hybrid Nearest Neighbors (ARHNN) method.",False,True,"Combining rough volatility measures with change-point detection for selecting calibration windows in forecasting could create a novel, feasible, and useful multidisciplinary approach, leveraging concepts from financial volatility modeling and time series forecasting methods.",
neg-1s-57,1,,2112.10986v1,"Shared Frailty Models Based on Cancer Data; Traditional survival analysis techniques focus on the occurrence of failures
over the time. During analysis of such events, ignoring the related unobserved
covariates or heterogeneity involved in data sample may leads us to adverse
consequences. In this context, frailty models are the viable choice to
investigate the effect of the unobserved covariates. In this article, we assume
that frailty acts multiplicatively to hazard rate. We propose inverse Gaussian
(IG) and generalized Lindley (GL) shared frailty models with generalized
Weibull (GW) as baseline distribution in order to analyze the unobserved
heterogeneity. To estimate the parameters in models, Bayesian paradigm of
Markov Chain Monte Carlo technique has been proposed. Model selection criteria
have been used for the comparison of models. Three different cancer data sets
have been analyzed using the shared frailty models. Better models have been
suggested for the data sets.",2112.11338v3,"Role of Variable Renewable Energy Penetration on Electricity Price and
  its Volatility Across Independent System Operators in the United States; The U.S. electrical grid has undergone substantial transformation with
increased penetration of wind and solar -- forms of variable renewable energy
(VRE). Despite the benefits of VRE for decarbonization, it has garnered some
controversy for inducing unwanted effects in regional electricity markets. In
this study, the role of VRE penetration is examined on the system electricity
price and price volatility based on hourly, real-time, historical data from six
Independent System Operators (ISOs) in the U.S. using quantile and skew
t-distribution regressions. After correcting for temporal effects, we found an
increase in VRE penetration is associated with decrease in system electricity
price in all ISOs studied. The increase in VRE penetration is associated with
decrease in temporal price volatility in five out of six ISOs studied. The
relationships are non-linear. These results are consistent with the modern
portfolio theory where diverse volatile assets may lead to more stable and less
risky portfolios.",False,False,"Combining survival frailty models with renewable energy price analysis doesn't clearly meet all standards. The multidisciplinary link is weak, making it challenging to ensure feasibility, novelty, and practical utility based on the provided abstracts.",
neg-1s-58,1,,2409.09314v1,"The Future of Decoding Non-Standard Nucleotides: Leveraging Nanopore
  Sequencing for Expanded Genetic Codes; Expanding genetic codes from natural standard nucleotides to artificial
non-standard nucleotides marks a significant advancement in synthetic biology,
with profound implications for biotechnology and medicine. Decoding the
biological information encoded in these non-standard nucleotides presents new
challenges, as traditional sequencing technologies are unable to recognize or
interpret novel base pairings. In this perspective, we explore the potential of
nanopore sequencing, which is uniquely suited to decipher both standard and
non-standard nucleotides by directly measuring the biophysical properties of
nucleic acids. Nanopore technology offers real-time, long-read sequencing
without the need for amplification or synthesis, making it particularly
advantageous for expanded genetic systems like Artificially Expanded Genetic
Information Systems (AEGIS). We discuss how the adaptability of nanopore
sequencing and advancements in data processing can unlock the potential of
these synthetic genomes and open new frontiers in understanding and utilizing
expanded genetic codes.",2409.11683v1,"k-mer-based approaches to bridging pangenomics and population genetics; Many commonly studied species now have more than one chromosome-scale genome
assembly, revealing a large amount of genetic diversity previously missed by
approaches that map short reads to a single reference. However, many species
still lack multiple reference genomes and correctly aligning references to
build pangenomes is challenging, limiting our ability to study this missing
genomic variation in population genetics. Here, we argue that $k$-mers are a
crucial stepping stone to bridging the reference-focused paradigms of
population genetics with the reference-free paradigms of pangenomics. We review
current literature on the uses of $k$-mers for performing three core components
of most population genetics analyses: identifying, measuring, and explaining
patterns of genetic variation. We also demonstrate how different $k$-mer-based
measures of genetic variation behave in population genetic simulations
according to the choice of $k$, depth of sequencing coverage, and degree of
data compression. Overall, we find that $k$-mer-based measures of genetic
diversity scale consistently with pairwise nucleotide diversity ($\pi$) up to
values of about $\pi = 0.025$ ($R^2 = 0.97$) for neutrally evolving
populations. For populations with even more variation, using shorter $k$-mers
will maintain the scalability up to at least $\pi = 0.1$. Furthermore, in our
simulated populations, $k$-mer dissimilarity values can be reliably
approximated from counting bloom filters, highlighting a potential avenue to
decreasing the memory burden of $k$-mer based genomic dissimilarity analyses.
For future studies, there is a great opportunity to further develop methods to
identifying selected loci using $k$-mers.",False,True,"Combining nanopore sequencing's ability to decode non-standard nucleotides with k-mer-based population genetics can lead to innovative methods for analyzing expanded genetic codes in diverse populations. This multidisciplinary approach is feasible, novel, and offers useful applications in synthetic biology and genomics.",
neg-1s-59,1,,2110.11623v3,"Dg Loday-Pirashvili modules over Lie algebras; A Loday-Pirashvili module over a Lie algebra $\mathfrak{g}$ is a Lie algebra
object $\bigl(G\xrightarrow{X} \mathfrak{g} \bigr)$ in the category of linear
maps, or equivalently, a $\mathfrak{g}$-module $G$ which admits a
$\mathfrak{g}$-equivariant linear map $X:G\to \mathfrak{g}$. We study dg
Loday-Pirashvili modules over Lie algebras, which is a generalization of
Loday-Pirashvili modules in a natural way, and establish several equivalent
characterizations of dg Loday-Pirashvili modules. To provide a concise
characterization, a dg Loday-Pirashvili module is a non-negative and bounded dg
$\mathfrak{g}$-module $V$ paired with a weak morphism of dg
$\mathfrak{g}$-modules $\alpha\colon V\rightsquigarrow \mathfrak{g}$. Such a dg
Loday-Pirashvili module resolves an arbitrarily specified classical
Loday-Pirashvili module in the sense that it exists and is unique (up to
homotopy). Dg Loday-Pirashvili modules can be characterized through dg
derivations. This perspective allows the calculation of the corresponding
twisted Atiyah classes. By leveraging the Kapranov functor on the dg derivation
arising from a dg Loday-Pirashvili module $(V,\alpha)$, a Leibniz$_\infty[1]$
algebra structure can be derived on $\wedge^\bullet \mathfrak{g}^\vee\otimes
V[1]$. The binary bracket of this structure corresponds to the twisted Atiyah
cocycle. To exemplify these intricate algebraic structures through specific
cases, we utilize this machinery to a particular type of dg Loday-Pirashvili
modules stemming from Lie algebra pairs.",2110.12784v2,"Representations of the super Yangians of types $A$ and $C$; We classify the finite-dimensional irreducible representations of the super
Yangian associated with the orthosymplectic Lie superalgebra ${\frak
osp}_{2|2n}$. The classification is given in terms of the highest weights and
Drinfeld polynomials. We also include an $R$-matrix construction of the
polynomial evaluation modules over the Yangian associated with the Lie
superalgebra ${\frak gl}_{m|n}$, as an appendix. This is a super-version of the
well-known construction for the ${\frak gl}_n$ Yangian and it relies on the
Schur--Sergeev duality.",False,False,"The concepts from both papers are deeply rooted in advanced algebra and representation theory, lacking integration with distinct disciplines. Consequently, a multidisciplinary research idea combining them does not meet the required diversity across multiple fields.",
neg-1s-60,1,,1701.03709v1,"Power and Execution Time Measurement Methodology for SDF Applications on
  FPGA-based MPSoCs; Timing and power consumption play an important role in the design of embedded
systems. Furthermore, both properties are directly related to the safety
requirements of many embedded systems. With regard to availability
requirements, power considerations are of uttermost importance for battery
operated systems. Validation of timing and power requires observability of
these properties. In many cases this is difficult, because the observability is
either not possible or requires big extra effort in the system validation
process. In this paper, we present a measurement-based approach for the joint
timing and power analysis of Synchronous Dataflow (SDF) applications running on
a shared memory multiprocessor systems-on-chip (MPSoC) architecture. As a
proof-of-concept, we implement an MPSoC system with configurable power and
timing measurement interfaces inside a Field Programmable Gate Array (FPGA).
Our experiments demonstrate the viability of our approach being able of
accurately analyzing different mappings of image processing applications (Sobel
filter and JPEG encoder) on an FPGA-based MPSoC implementation.",1703.02925v1,"Assessing Code Authorship: The Case of the Linux Kernel; Code authorship is a key information in large-scale open source systems.
Among others, it allows maintainers to assess division of work and identify key
collaborators. Interestingly, open-source communities lack guidelines on how to
manage authorship. This could be mitigated by setting to build an empirical
body of knowledge on how authorship-related measures evolve in successful
open-source communities. Towards that direction, we perform a case study on the
Linux kernel. Our results show that: (a) only a small portion of developers (26
%) makes significant contributions to the code base; (b) the distribution of
the number of files per author is highly skewed --- a small group of top
authors (3 %) is responsible for hundreds of files, while most authors (75 %)
are responsible for at most 11 files; (c) most authors (62 %) have a specialist
profile; (d) authors with a high number of co-authorship connections tend to
collaborate with others with less connections.",False,False,"The abstracts from embedded system performance measurement and code authorship analysis don’t clearly combine to form a novel, feasible, and useful multidisciplinary research idea that meets all the specified standards.",
neg-1s-61,1,,1806.01645v1,"Several Conclusions on another site setting problem; Let $S = \{ {A_1},{A_2}, \cdots ,{A_n}\} $ be a finite point set in
m-dimensional Euclidean space ${E^m}$, and$\left\| {{A_i}{A_j}} \right\|$ be
the distance between $A_i$ and $A_j$. Define $\sigma (S) = \sum\limits_{1 \le i
< j \le n} {\left\| {{A_i}{A_j}} \right\|} $, $D(S) = \mathop {\max }\limits_{1
\le i < j \le n} \left\{ {\left\| {{A_i}{A_j}} \right\|} \right\}$, $\omega
(m,n) = \frac{{\sigma (S)}}{{D(S)}}$, $\sup \omega (m,n) = \max \left\{ {\left.
{\frac{{\sigma (S)}}{{D(S)}}} \right|S \subset {E^m},\left| S \right| = n}
\right\}$. This paper proves that, for any point P in an n-dimensional simplex
${A_1}{A_2} \cdots {A_{n + 1}}$ in Euclidean space, $\sum\limits_{i = 1}^{n +
1} {\left\| {P{A_i}} \right\|} $ <= $\mathop {\sup }\limits_{{i_t},{j_t} \in \{
1,2, \cdots ,n + 1\} } \left\{ {\sum\limits_{t = 1}^n {\left\|
{{A_{{i_t}}}{A_{{j_t}}}} \right\|} } \right\}$ By using this inequality and
several results in differential geometry this paper also proves that $\sup
\omega (2,4) = 4 + 2\sqrt {2 - \sqrt 3 } $, $\sup \omega (n,n + 2)$ >= $C_{n +
1}^2 + 1 + n\sqrt {2\left( {1 - \sqrt {{\textstyle{{n + 1} \over {2n}}}} }
\right)} $.",1806.02187v1,"Fuzzy $α$-cut and related structures; This paper deals with a new notion called fuzzy $\alpha$-cut and its
properties. A notion called localic frame is also introduced. Algebraic
structures arising out of the family of fuzzy $\alpha$-cuts have been
investigated. It will be seen that this family forms a localic frame. Some
significance and usefulness of fuzzy $\alpha$-cuts are discussed.",False,True,"Combining geometric distance metrics from Paper 1 with fuzzy α-cuts from Paper 2 could create a novel framework for handling uncertainty in spatial data, which is feasible to test experimentally and offers useful applications in fields like robotics or geographic information systems.",
neg-1s-62,1,,q-bio/0411012v1,"Graded and Binary Responses in Stochastic Gene Expression; Recently, several theoretical and experimental studies have been undertaken
to probe the effect of stochasticity on gene expression (GE). In experiments,
the GE response to an inducing signal in a cell, measured by the amount of
mRNAs/proteins synthesized, is found to be either graded or binary. The latter
type of response gives rise to a bimodal distribution in protein levels in an
ensemble of cells. One possible origin of binary response is cellular
bistability achieved through positive feedback or autoregulation. In this
paper, we study a simple, stochastic model of GE and show that the origin of
binary response lies exclusively in stochasticity. The transitions between the
active and inactive states of the gene are random in nature. Graded and binary
responses occur in the model depending on the relative stability of the
activated and deactivated gene states with respect to that of
mRNAs/proteins.The theoretical results on binary response provide a good
description of the ``all-or-none'' phenomenon observed in an eukaryotic system.",q-bio/0412014v1,"Robust formation of morphogen gradients; We discuss the formation of graded morphogen profiles in a cell layer by
nonlinear transport phenomena, important for patterning developing organisms.
We focus on a process termed transcytosis, where morphogen transport results
from binding of ligands to receptors on the cell surface, incorporation into
the cell and subsequent externalization. Starting from a microscopic model, we
derive effective transport equations. We show that, in contrast to morphogen
transport by extracellular diffusion, transcytosis leads to robust ligand
profiles which are insensitive to the rate of ligand production.",False,True,"Combining stochastic gene expression with morphogen gradient formation could yield a novel, experimentally testable idea that bridges genetics and developmental biology, offering new insights into pattern formation and its robustness.",
neg-1s-63,1,,1102.2878v1,"Dual-Tree Fast Gauss Transforms; Kernel density estimation (KDE) is a popular statistical technique for
estimating the underlying density distribution with minimal assumptions.
Although they can be shown to achieve asymptotic estimation optimality for any
input distribution, cross-validating for an optimal parameter requires
significant computation dominated by kernel summations. In this paper we
present an improvement to the dual-tree algorithm, the first practical kernel
summation algorithm for general dimension. Our extension is based on the
series-expansion for the Gaussian kernel used by fast Gauss transform. First,
we derive two additional analytical machinery for extending the original
algorithm to utilize a hierarchical data structure, demonstrating the first
truly hierarchical fast Gauss transform. Second, we show how to integrate the
series-expansion approximation within the dual-tree approach to compute kernel
summations with a user-controllable relative error bound. We evaluate our
algorithm on real-world datasets in the context of optimal bandwidth selection
in kernel density estimation. Our results demonstrate that our new algorithm is
the only one that guarantees a hard relative error bound and offers fast
performance across a wide range of bandwidths evaluated in cross validation
procedures.",1102.3176v3,"Selecting the rank of truncated SVD by Maximum Approximation Capacity; Truncated Singular Value Decomposition (SVD) calculates the closest rank-$k$
approximation of a given input matrix. Selecting the appropriate rank $k$
defines a critical model order choice in most applications of SVD. To obtain a
principled cut-off criterion for the spectrum, we convert the underlying
optimization problem into a noisy channel coding problem. The optimal
approximation capacity of this channel controls the appropriate strength of
regularization to suppress noise. In simulation experiments, this information
theoretic method to determine the optimal rank competes with state-of-the art
model selection techniques.",False,True,"Combining optimized kernel density estimation with informed rank selection in SVD could create an innovative framework for high-dimensional data analysis. This multidisciplinary approach leverages computational efficiency and robust model selection, meeting feasibility, novelty, and practical utility standards.",
neg-1s-64,1,,2105.03656v3,"Estimates of the social cost of carbon have increased over time; A meta-analysis of published estimates shows that the social cost of carbon
has increased as knowledge about climate change accumulates. Correcting for
inflation and emission year and controlling for the discount rate, kernel
density decomposition reveals a non-stationary distribution. In the last 10
years, estimates of the social cost of carbon have increased from $33/tC to
$146/tC for a high discount rate and from $446/tC to $1925/tC for a low
discount rate. Actual carbon prices are almost everywhere below its estimated
value and should therefore go up.",2105.04718v1,"Economic analysis of tidal stream turbine arrays: a review; This tidal stream energy industry has to date been comprised of small
demonstrator projects made up of one to a four turbines. However, there are
currently plans to expand to commercially sized projects with tens of turbines
or more. As the industry moves to large-scale arrays for the first time, there
has been a push to develop tools to optimise the array design and help bring
down the costs. This review investigates different methods of modelling the
economic performance of tidal-stream arrays, for use within these optimisation
tools. The different cost reduction pathways are discussed from costs falling
as the global installed capacity increases, due to greater experience, improved
power curves through larger-diameter higher-rated turbines, to economic
efficiencies that can be found by moving to large-scale arrays. A literature
review is conducted to establish the most appropriate input values for use in
economic models. This includes finding a best case, worst case and typical
values for costs and other related parameters. The information collated in this
review can provide a useful steering for the many optimisation tools that have
been developed, especially when cost information is commercially sensitive and
a realistic parameter range is difficult to obtain.",False,False,"Combining aspects from both papers does not clearly generate a novel, feasible, and multidisciplinary idea that meets all the specified standards.",
neg-1s-65,1,,2208.00411v1,"Predicting Failure times for some Unobserved Events with Application to
  Real-Life Data; This study aims to predict failure times for some units in some lifetime
experiments. In some practical situations, the experimenter may not be able to
register the failure times of all units during the experiment. Recently, this
situation can be described by a new type of censored data called
multiply-hybrid censored data. In this paper, the linear failure rate
distribution is well-fitted to some real-life data and hence some statistical
inference approaches are applied to estimate the distribution parameters. A
two-sample prediction approach applied to extrapolate a new sample simulates
the observed data for predicting the failure times for the unobserved units.",2208.00715v2,"Highly Efficient Estimators with High Breakdown Point for Linear Models
  with Structured Covariance Matrices; We provide a unified approach to a method of estimation of the regression
parameter in balanced linear models with a structured covariance matrix that
combines a high breakdown point and bounded influence with high asymptotic
efficiency at models with multivariate normal errors. Of main interest are
linear mixed effects models, but our approach also includes several other
standard multivariate models, such as multiple regression, multivariate
regression, and multivariate location and scatter. We provide sufficient
conditions for the existence of the estimators and corresponding functionals,
establish asymptotic properties such as consistency and asymptotic normality,
and derive their robustness properties in terms of breakdown point and
influence function. All the results are obtained for general identifiable
covariance structures and are established under mild conditions on the
distribution of the observations, which goes far beyond models with
elliptically contoured densities. Some of our results are new and others are
more general than existing ones in the literature. In this way this manuscript
completes and improves results on high breakdown estimation with high
efficiency in a wide variety of multivariate models.",False,False,"While both papers involve statistical modeling, there isn't a clear, novel multidisciplinary idea that combines their core concepts in a way that meets all the specified standards.",
neg-1s-66,1,,2403.17513v5,"A unified framework for coarse grained molecular dynamics of proteins
  with high-fidelity reconstruction; Simulating large proteins using traditional molecular dynamics (MD) is
computationally demanding. To address this challenge, we propose a novel
tree-structured coarse-grained model that efficiently captures protein
dynamics. By leveraging a hierarchical protein representation, our model
accurately reconstructs high-resolution protein structures, with sub-angstrom
precision achieved for a 168-amino acid protein. We combine this coarse-grained
model with a deep learning framework based on stochastic differential equations
(SDEs). A neural network is trained to model the drift force, while a
RealNVP-based noise generator approximates the stochastic component. This
approach enables a significant speedup of over 20,000 times compared to
traditional MD, allowing for the generation of microsecond-long trajectories
within a few minutes and providing valuable insights into protein behavior. Our
method demonstrates high accuracy, achieving sub-angstrom reconstruction for
short (25 ns) trajectories and maintaining statistical consistency across
multiple independent simulations.",2403.17954v1,"Sort & Slice: A Simple and Superior Alternative to Hash-Based Folding
  for Extended-Connectivity Fingerprints; Extended-connectivity fingerprints (ECFPs) are a ubiquitous tool in current
cheminformatics and molecular machine learning, and one of the most prevalent
molecular feature extraction techniques used for chemical prediction. Atom
features learned by graph neural networks can be aggregated to compound-level
representations using a large spectrum of graph pooling methods; in contrast,
sets of detected ECFP substructures are by default transformed into bit vectors
using only a simple hash-based folding procedure. We introduce a general
mathematical framework for the vectorisation of structural fingerprints via a
formal operation called substructure pooling that encompasses hash-based
folding, algorithmic substructure-selection, and a wide variety of other
potential techniques. We go on to describe Sort & Slice, an easy-to-implement
and bit-collision-free alternative to hash-based folding for the pooling of
ECFP substructures. Sort & Slice first sorts ECFP substructures according to
their relative prevalence in a given set of training compounds and then slices
away all but the $L$ most frequent substructures which are subsequently used to
generate a binary fingerprint of desired length, $L$. We computationally
compare the performance of hash-based folding, Sort & Slice, and two advanced
supervised substructure-selection schemes (filtering and mutual-information
maximisation) for ECFP-based molecular property prediction. Our results
indicate that, despite its technical simplicity, Sort & Slice robustly (and at
times substantially) outperforms traditional hash-based folding as well as the
other investigated methods across prediction tasks, data splitting techniques,
machine-learning models and ECFP hyperparameters. We thus recommend that Sort &
Slice canonically replace hash-based folding as the default
substructure-pooling technique to vectorise ECFPs for supervised molecular
machine learning.",False,True,"Combining coarse-grained molecular dynamics with improved substructure pooling from cheminformatics could create a novel, experimentally testable method to enhance protein simulation accuracy and efficiency, fulfilling multidisciplinary, feasible, novel, and useful criteria.",
neg-1s-67,1,,1411.5888v1,"Weak convergence of the empirical copula process with respect to
  weighted metrics; The empirical copula process plays a central role in the asymptotic analysis
of many statistical procedures which are based on copulas or ranks. Among other
applications, results regarding its weak convergence can be used to develop
asymptotic theory for estimators of dependence measures or copula densities,
they allow to derive tests for stochastic independence or specific copula
structures, or they may serve as a fundamental tool for the analysis of
multivariate rank statistics. In the present paper, we establish weak
convergence of the empirical copula process (for observations that are allowed
to be serially dependent) with respect to weighted supremum distances. The
usefulness of our results is illustrated by applications to general bivariate
rank statistics and to estimation procedures for the Pickands dependence
function arising in multivariate extreme-value theory.",1411.6419v4,"Uniform central limit theorems for the Grenander estimator; We consider the Grenander estimator that is the maximum likelihood estimator
for non-increasing densities. We prove uniform central limit theorems for
certain subclasses of bounded variation functions and for H\""older balls of
smoothness s>1/2. We do not assume that the density is differentiable or
continuous. The proof can be seen as an adaptation of the method for the
parametric maximum likelihood estimator to the nonparametric setting. Since
nonparametric maximum likelihood estimators lie on the boundary, the derivative
of the likelihood cannot be expected to equal zero as in the parametric case.
Nevertheless, our proofs rely on the fact that the derivative of the likelihood
can be shown to be small at the maximum likelihood estimator.",False,False,"Both papers are focused within the field of statistics, specifically on asymptotic properties of estimators. They do not bridge distinct disciplines, making it unlikely to develop a truly multidisciplinary research idea that meets all the specified standards.",
neg-1s-68,1,,1905.12164v1,"An Interactive Insight Identification and Annotation Framework for Power
  Grid Pixel Maps using DenseU-Hierarchical VAE; Insights in power grid pixel maps (PGPMs) refer to important facility
operating states and unexpected changes in the power grid. Identifying insights
helps analysts understand the collaboration of various parts of the grid so
that preventive and correct operations can be taken to avoid potential
accidents. Existing solutions for identifying insights in PGPMs are performed
manually, which may be laborious and expertise-dependent. In this paper, we
propose an interactive insight identification and annotation framework by
leveraging an enhanced variational autoencoder (VAE). In particular, a new
architecture, DenseU-Hierarchical VAE (DUHiV), is designed to learn
representations from large-sized PGPMs, which achieves a significantly tighter
evidence lower bound (ELBO) than existing Hierarchical VAEs with a Multilayer
Perceptron architecture. Our approach supports modulating the derived
representations in an interactive visual interface, discover potential insights
and create multi-label annotations. Evaluations using real-world PGPMs datasets
show that our framework outperforms the baseline models in identifying and
annotating insights.",1905.12596v1,"Segmentation of blood vessels in retinal fundus images; In recent years, several automatic segmentation methods have been proposed
for blood vessels in retinal fundus images, ranging from using cheap and fast
trainable filters to complicated neural networks and even deep learning. One
example of a filted-based segmentation method is B-COSFIRE. In this approach
the image filter is trained with example prototype patterns, to which the
filter becomes selective by finding points in a Difference of Gaussian response
on circles around the center with large intensity variation. In this paper we
discuss and evaluate several of these vessel segmentation methods. We take a
closer look at B-COSFIRE and study the performance of B-COSFIRE on the recently
published IOSTAR dataset by experiments and we examine how the parameter values
affect the performance. In the experiment we manage to reach a segmentation
accuracy of 0.9419. Based on our findings we discuss when B-COSFIRE is the
preferred method to use and in which circumstances it could be beneficial to
use a more (computationally) complex segmentation method. We also shortly
discuss areas beyond blood vessel segmentation where these methods can be used
to segment elongated structures, such as rivers in satellite images or nerves
of a leaf.",False,False,"Combining concepts from power grid analysis and retinal vessel segmentation may lack a clear, novel, and practical application that meets all multidisciplinary, feasibility, novelty, and usefulness criteria.",
neg-1s-69,1,,2207.02359v1,"Lévy models amenable to efficient calculations; In our previous publications (IJTAF 2019, Math. Finance 2020), we introduced
a general class of SINH-regular processes and demonstrated that efficient
numerical methods for the evaluation of the Wiener-Hopf factors and various
probability distributions (prices of options of several types) in L\'evy models
can be developed using only a few general properties of the characteristic
exponent $\psi$. Essentially all popular L\'evy processes enjoy these
properties. In the present paper, we define classes of Stieltjes-L\'evy
processes (SL-processes) as processes with completely monotone L\'evy densities
of positive and negative jumps, and signed Stieltjes-L\'evy processes
(sSL-processes) as processes with densities representable as differences of
completely monotone densities. We demonstrate that 1) all crucial properties of
$\psi$ are consequences of the representation
$\psi(\xi)=(a^+_2\xi^2-ia^+_1\xi)ST(\cG_+)(-i\xi)+(a^-_2\xi^2+ia^-_1\xi)ST(\cG_-)(i\xi)+(\sg^2/2)\xi^2-i\mu\xi$,
where $ST(\cG)$ is the Stieltjes transform of the (signed) Stieltjes measure
$\cG$ and $a^\pm_j\ge 0$; 2) essentially all popular processes other than
Merton's model and Meixner processes areSL-processes; 3) Meixner processes are
sSL-processes; 4) under a natural symmetry condition, essentially all popular
classes of L\'evy processes are SL- or sSL-subordinated Brownian motion.",2207.02858v2,"Efficient inverse $Z$-transform and pricing barrier and lookback options
  with discrete monitoring; We prove simple general formulas for expectations of functions of a random
walk and its running extremum. Under additional conditions, we derive
analytical formulas using the inverse $Z$-transform, the Fourier/Laplace
inversion and Wiener-Hopf factorization, and discuss efficient numerical
methods for realization of these formulas. As applications, the cumulative
probability distribution function of the process and its running maximum and
the price of the option to exchange the power of a stock for its maximum are
calculated. The most efficient numerical methods use a new efficient numerical
realization of the inverse $Z$-transform, the sinh-acceleration technique and
simplified trapezoid rule. The program in Matlab running on a Mac with moderate
characteristics achieves the precision E-10 and better in several dozen of
milliseconds, and E-14 - in a fraction of a isecond.",False,False,"Both papers focus on advanced numerical methods in financial mathematics, lacking integration of distinct disciplines required for a multidisciplinary research idea.",
neg-1s-70,1,,1810.10495v2,"Posterior Convergence of Gaussian and General Stochastic Process
  Regression Under Possible Misspecifications; In this article, we investigate posterior convergence in nonparametric
regression models where the unknown regression function is modeled by some
appropriate stochastic process. In this regard, we consider two setups. The
first setup is based on Gaussian processes, where the covariates are either
random or non-random and the noise may be either normally or
double-exponentially distributed. In the second setup, we assume that the
underlying regression function is modeled by some reasonably smooth, but
unspecified stochastic process satisfying reasonable conditions. The
distribution of the noise is also left unspecified, but assumed to be
thick-tailed. As in the previous studies regarding the same problems, we do not
assume that the truth lies in the postulated parameter space, thus explicitly
allowing the possibilities of misspecification. We exploit the general results
of Shalizi (2009) for our purpose and establish not only posterior consistency,
but also the rates at which the posterior probabilities converge, which turns
out to be the Kullback-Leibler divergence rate. We also investigate the more
familiar posterior convergence rates. Interestingly, we show that the posterior
predictive distribution can accurately approximate the best possible predictive
distribution in the sense that the Hellinger distance, as well as the total
variation distance between the two distributions can tend to zero, in spite of
misspecifications.",1810.10633v1,"Strong laws of large numbers for arrays of random variables and stable
  random fields; Strong laws of large numbers are established for random fields with weak or
strong dependence. These limit theorems are applicable to random fields with
heavy-tailed distributions including fractional stable random fields.
  The conditions for SLLN are described in terms of the $p$-th moments of the
partial sums of the random fields, which are convenient to verify. The main
technical tool in this paper is a maximal inequality for the moments of partial
sums of random fields that extends the technique of Levental, Chobanyan and
Salehi \cite{chobanyan-l-s} for a sequence of random variables indexed by a
one-parameter.",False,True,"Combining Bayesian nonparametric regression with strong laws for dependent, heavy-tailed random fields meets all standards by creating a novel, feasible, and useful multidisciplinary research idea that integrates concepts from both papers.",
neg-1s-71,1,,2502.20417v1,"Accurate 3D Grapevine Structure Extraction from High-Resolution Point
  Clouds; Accurate 3D modelling of grapevines is crucial for precision viticulture,
particularly for informed pruning decisions and automated management
techniques. However, the intricate structure of grapevines poses significant
challenges for traditional skeletonization algorithms. This paper presents an
adaptation of the Smart-Tree algorithm for 3D grapevine modelling, addressing
the unique characteristics of grapevine structures. We introduce a graph-based
method for disambiguating skeletonization. Our method delineates individual
cane skeletons, which are crucial for precise analysis and management. We
validate our approach using annotated real-world grapevine point clouds,
demonstrating improvement of 15.8% in the F1 score compared to the original
Smart-Tree algorithm. This research contributes to advancing 3D grapevine
modelling techniques, potentially enhancing both the sustainability and
profitability of grape production through more precise and automated
viticulture practices",2503.07837v1,"Slowing translation to avoid ribosome population extinction and maintain
  stable allocation at slow growth rates; To double the cellular population of ribosomes, a fraction of the active
ribosomes is allocated to synthesize ribosomal proteins. Subsequently, these
ribosomal proteins enter the ribosome self-assembly process, synthesizing new
ribosomes and forming the well-known ribosome autocatalytic subcycle.
Neglecting ribosome lifetime and the duration of the self-assembly process, the
doubling rate of all cellular biomass can be equated with the fraction of
ribosomes allocated to synthesize an essential ribosomal protein times its
synthesis rate. However, ribosomes have a finite lifetime, and the assembly
process has a finite duration. Furthermore, the number of ribosomes is known to
decrease with slow growth rates. The finite lifetime of ribosomes and the
decline in their numbers present a challenge in sustaining slow growth solely
through controlling the allocation of ribosomes to synthesize more ribosomal
proteins. When the number of ribosomes allocated per mRNA of an essential
ribosomal protein is approximately one, the resulting fluctuations in the
production rate of new ribosomes increase, causing a potential risk that the
actual production rate will fall below the ribosome death rate. Thus, in this
regime, a significant risk of extinction of the ribosome population emerges. To
mitigate this risk, we suggest that the ribosome translation speed is used as
an alternative control parameter, which facilitates the maintenance of slow
growth rates with a larger ribosome pool. We clarify the observed reduction in
translation speed at harsh environments in E. coli and C. Glutamicum, explore
other mitigation strategies, and suggest additional falsifiable predictions of
our model.",False,False,"The two papers belong to distinct fields with no clear overlapping concepts that can be combined to form a feasible, novel, and useful multidisciplinary research idea meeting all the specified standards.",
neg-1s-72,1,,math/0605080v2,"Equivariant operads, string topology, and Tate cohomology; From an operad C with an action of a group G, we construct new operads using
the homotopy fixed point and orbit spectra. These new operads are shown to be
equivalent when the generalized G-Tate cohomology of C is trivial. Applying
this theory to the little disk operad C_2 (which is an S^1 operad) we obtain
variations on Getzler's gravity operad, which we show governs the Chas-Sullivan
string bracket.",math/0605133v3,"The fundamental group of symplectic manifolds with Hamiltonian Lie group
  actions; Let $(M, \omega)$ be a connected, compact symplectic manifold equipped with a
Hamiltonian $G$ action, where $G$ is a connected compact Lie group. Let $\phi$
be the moment map. In \cite{L}, we proved the following result for $G=S^1$
action: as fundamental groups of topological spaces, $\pi_1(M)=\pi_1(M_{red})$,
where $M_{red}$ is the symplectic quotient at any value of the moment map
$\phi$. In this paper, we generalize this result to other connected compact Lie
group $G$ actions. We also prove that the above fundamental group is isomorphic
to that of $M/G$. We briefly discuss the generalization of the first part of
the results to non-compact manifolds with proper moment maps.",False,False,"The concepts from both papers are deeply rooted in pure mathematics and do not naturally combine into a multidisciplinary idea involving distinct fields, failing to meet the multidisciplinary and novelty standards.",
neg-1s-73,1,,physics/0503206v3,"Neutrinos in the Electron; We will show that one half of the rest mass of the electron is equal to the
sum of the rest masses of electron neutrinos and that the other half of the
rest mass of the electron is given by the energy in the sum of electric
oscillations. With this composition we can explain the rest mass, the electric
charge, the spin and the magnetic moment of the electron.",physics/0503207v1,"Brownstein's Whole-Partial Derivatives: The Case of the Lorentz Gauge; In this brief note we show that the usual Lorentz gauge is not satisfied by
the Lienard-Wiechert potentials, then, using Brownstein's concept of
""whole-partial"" derivatives we introduce the generalized expression for the
Lorentz gauge showing that it is satisfied by the LW-potentials.",False,False,"The papers are both within physics without clearly bridging distinct disciplines. Combining their concepts doesn’t evidently meet the multidisciplinary, novel, and practical standards required.",
neg-1s-74,1,,2305.19102v1,"Closed ecosystems extract energy through self-organized nutrient cycles; Our planet is roughly closed to matter, but open to energy input from the
sun. However, to harness this energy, organisms must transform matter from one
chemical (redox) state to another. For example, photosynthetic organisms can
capture light energy by carrying out a pair of electron donor and acceptor
transformations (e.g., water to oxygen, CO$_2$ to organic carbon). Closure of
ecosystems to matter requires that all such transformations are ultimately
balanced, i.e., other organisms must carry out corresponding reverse
transformations, resulting in cycles that are coupled to each other. A
sustainable closed ecosystem thus requires self-organized cycles of matter, in
which every transformation has sufficient thermodynamic favorability to
maintain an adequate number of organisms carrying out that process. Here, we
propose a new conceptual model that explains the self-organization and emergent
features of closed ecosystems. We study this model with varying levels of
metabolic diversity and energy input, finding that several thermodynamic
features converge across ecosystems. Specifically, irrespective of their
species composition, large and metabolically diverse communities self-organize
to extract roughly 10% of the maximum extractable energy, or 100 fold more than
randomized communities. Moreover, distinct communities implement energy
extraction in convergent ways, as indicated by strongly correlated fluxes
through nutrient cycles. As the driving force from light increases, however,
these features -- fluxes and total energy extraction -- become more variable
across communities, indicating that energy limitation imposes tight
thermodynamic constraints on collective metabolism.",2306.08261v3,"Strong regulatory graphs; Logical modeling is a powerful tool in biology, offering a system-level
understanding of the complex interactions that govern biological processes. A
gap that hinders the scalability of logical models is the need to specify the
update function of every vertex in the network depending on the status of its
predecessors. To address this, we introduce in this paper the concept of strong
regulation, where a vertex is only updated to active/inactive if all its
predecessors agree in their influences; otherwise, it is set to ambiguous. We
explore the interplay between active, inactive, and ambiguous influences in a
network. We discuss the existence of phenotype attractors in such networks,
where the status of some of the variables is fixed to active/inactive, while
the others can have an arbitrary status, including ambiguous.",False,True,"Combining self-organized nutrient cycles with strong regulatory graphs can lead to a novel model for predicting ecosystem stability. This multidisciplinary approach is feasible through computational experiments, offers new insights, and effectively addresses understanding closed ecosystem dynamics.",
neg-1s-75,1,,1910.11480v2,"Parallel WaveGAN: A fast waveform generation model based on generative
  adversarial networks with multi-resolution spectrogram; We propose Parallel WaveGAN, a distillation-free, fast, and small-footprint
waveform generation method using a generative adversarial network. In the
proposed method, a non-autoregressive WaveNet is trained by jointly optimizing
multi-resolution spectrogram and adversarial loss functions, which can
effectively capture the time-frequency distribution of the realistic speech
waveform. As our method does not require density distillation used in the
conventional teacher-student framework, the entire model can be easily trained.
Furthermore, our model is able to generate high-fidelity speech even with its
compact architecture. In particular, the proposed Parallel WaveGAN has only
1.44 M parameters and can generate 24 kHz speech waveform 28.68 times faster
than real-time on a single GPU environment. Perceptual listening test results
verify that our proposed method achieves 4.16 mean opinion score within a
Transformer-based text-to-speech framework, which is comparative to the best
distillation-based Parallel WaveNet system.",1910.11496v1,"L2RS: A Learning-to-Rescore Mechanism for Automatic Speech Recognition; Modern Automatic Speech Recognition (ASR) systems primarily rely on scores
from an Acoustic Model (AM) and a Language Model (LM) to rescore the N-best
lists. With the abundance of recent natural language processing advances, the
information utilized by current ASR for evaluating the linguistic and semantic
legitimacy of the N-best hypotheses is rather limited. In this paper, we
propose a novel Learning-to-Rescore (L2RS) mechanism, which is specialized for
utilizing a wide range of textual information from the state-of-the-art NLP
models and automatically deciding their weights to rescore the N-best lists for
ASR systems. Specifically, we incorporate features including BERT sentence
embedding, topic vector, and perplexity scores produced by n-gram LM, topic
modeling LM, BERT LM and RNNLM to train a rescoring model. We conduct extensive
experiments based on a public dataset, and experimental results show that L2RS
outperforms not only traditional rescoring methods but also its deep neural
network counterparts by a substantial improvement of 20.67% in terms of
NDCG@10. L2RS paves the way for developing more effective rescoring models for
ASR.",False,True,"Combining Parallel WaveGAN’s efficient waveform generation with L2RS’s advanced rescoring for ASR could create a novel system that enhances real-time speech recognition accuracy and speed, meeting all multidisciplinary, feasible, novel, and useful criteria.",
neg-1s-76,1,,2307.03726v2,"LTE SFBC MIMO Transmitter Modelling and Performance Evaluation; High data rates are one of the most prevalent requirements in current mobile
communications. To cover this and other high standards regarding performance,
increasing coverage, capacity, and reliability, numerous works have proposed
the development of systems employing the combination of several techniques such
as Multiple Input Multiple Output (MIMO) wireless technologies with Orthogonal
Frequency Division Multiplexing (OFDM) in the evolving 4G wireless
communications. Our proposed system is based on the 2x2 MIMO antenna technique,
which is defined to enhance the performance of radio communication systems in
terms of capacity and spectral efficiency, and the OFDM technique, which can be
implemented using two types of sub-carrier mapping modes: Space-Time Block
Coding and Space Frequency Block Code. SFBC has been considered in our
developed model. The main advantage of SFBC over STBC is that SFBC encodes two
modulated symbols over two subcarriers of the same OFDM symbol, whereas STBC
encodes two modulated symbols over two subcarriers of the same OFDM symbol;
thus, the coding is performed in the frequency domain. Our solution aims to
demonstrate the performance analysis of the Space Frequency Block Codes scheme,
increasing the Signal Noise Ratio (SNR) at the receiver and decreasing the Bit
Error Rate (BER) through the use of 4 QAM, 16 QAM and 64QAM modulation over a
2x2 MIMO channel for an LTE downlink transmission, in different channel radio
environments. In this work, an analytical tool to evaluate the performance of
SFBC - Orthogonal Frequency Division Multiplexing, using two transmit antennas
and two receive antennas has been implemented, and the analysis using the
average SNR has been considered as a sufficient statistic to describe the
performance of SFBC in the 3GPP Long Term Evolution system over Multiple Input
Multiple Output channels.",2307.03829v1,"Robot Motion Prediction by Channel State Information; Autonomous robotic systems have gained a lot of attention, in recent years.
However, accurate prediction of robot motion in indoor environments with
limited visibility is challenging. While vision-based and light detection and
ranging (LiDAR) sensors are commonly used for motion detection and localization
of robotic arms, they are privacy-invasive and depend on a clear line-of-sight
(LOS) for precise measurements. In cases where additional sensors are not
available or LOS is not possible, these technologies may not be the best
option. This paper proposes a novel method that employs channel state
information (CSI) from WiFi signals affected by robotic arm motion. We
developed a convolutional neural network (CNN) model to classify four different
activities of a Franka Emika robotic arm. The implemented method seeks to
accurately predict robot motion even in scenarios in which the robot is
obscured by obstacles, without relying on any attached or internal sensors.",False,True,"Combining advanced MIMO-OFDM techniques with WiFi-based CSI for robot motion prediction is a novel, feasible, and useful multidisciplinary approach that integrates wireless communications and autonomous robotics to enhance motion tracking capabilities.",
neg-1s-77,1,,2208.06115v5,"A Nonparametric Approach with Marginals for Modeling Consumer Choice; Given data on the choices made by consumers for different offer sets, a key
challenge is to develop parsimonious models that describe and predict consumer
choice behavior while being amenable to prescriptive tasks such as pricing and
assortment optimization. The marginal distribution model (MDM) is one such
model, which requires only the specification of marginal distributions of the
random utilities. This paper aims to establish necessary and sufficient
conditions for given choice data to be consistent with the MDM hypothesis,
inspired by the usefulness of similar characterizations for the random utility
model (RUM). This endeavor leads to an exact characterization of the set of
choice probabilities that the MDM can represent. Verifying the consistency of
choice data with this characterization is equivalent to solving a
polynomial-sized linear program. Since the analogous verification task for RUM
is computationally intractable and neither of these models subsumes the other,
MDM is helpful in striking a balance between tractability and representational
power. The characterization is then used with robust optimization for making
data-driven sales and revenue predictions for new unseen assortments. When the
choice data lacks consistency with the MDM hypothesis, finding the best-fitting
MDM choice probabilities reduces to solving a mixed integer convex program.
Numerical results using real world data and synthetic data demonstrate that MDM
exhibits competitive representational power and prediction performance compared
to RUM and parametric models while being significantly faster in computation
than RUM.",2208.06152v1,"ALS: Augmented Lagrangian Sketching Methods for Linear Systems; We develop two fundamental stochastic sketching techniques; Penalty Sketching
(PS) and Augmented Lagrangian Sketching (ALS) for solving consistent linear
systems. The proposed PS and ALS techniques extend and generalize the scope of
Sketch & Project (SP) method by introducing Lagrangian penalty sketches. In
doing so, we recover SP methods as special cases and furthermore develop a
family of new stochastic iterative methods. By varying sketch parameters in the
proposed PS method, we recover novel stochastic methods such as Penalty Newton
Descent, Penalty Kaczmarz, Penalty Stochastic Descent, Penalty Coordinate
Descent, Penalty Gaussian Pursuit, and Penalty Block Kaczmarz. Furthermore, the
proposed ALS method synthesizes a wide variety of new stochastic methods such
as Augmented Newton Descent, Augmented Kaczmarz, Augmented Stochastic Descent,
Augmented Coordinate Descent, Augmented Gaussian Pursuit, and Augmented Block
Kaczmarz into one framework. Moreover, we show that the developed PS and ALS
frameworks can be used to reformulate the original linear system into
equivalent stochastic optimization problems namely the Penalty Stochastic
Reformulation and Augmented Stochastic Reformulation. We prove global
convergence rates for the PS and ALS methods as well as sub-linear
$\mathcal{O}(\frac{1}{k})$ rates for the Cesaro average of iterates. The
proposed convergence results hold for a wide family of distributions of random
matrices, which provides the opportunity of fine-tuning the randomness of the
method suitable for specific applications. Finally, we perform computational
experiments that demonstrate the efficiency of our methods compared to the
existing SP methods.",False,False,"Combining consumer choice modeling with augmented Lagrangian sketching lacks a clear, feasible application that validates the integration experimentally, failing to meet all multidisciplinary research standards.",
neg-1s-78,1,,2201.05712v2,"Expectile-based hydrological modelling for uncertainty estimation: Life
  after mean; Predictions of hydrological models should be probabilistic in nature. Our aim
is to introduce a method that estimates directly the uncertainty of
hydrological simulations using expectiles, thus complementing previous
quantile-based direct approaches as well as generalizing mean-based approaches.
Expectiles are new risk measures in hydrology. Compared to quantiles that use
information of the frequency of process realizations over a specified value,
expectiles use additional information of the magnitude of the exceedances over
the specified value. Expectiles are least square analogues of quantiles and can
characterize the probability distribution in much the same way as quantiles do.
Moreover, the mean of the probability distribution is the special case of the
expectile at level 0.5. To this end, we propose calibrating hydrological models
using the expectile loss function, which is strictly consistent for expectiles.
We apply our method to 511 basins in contiguous US and deliver predictive
expectiles of hydrological simulations with the GR4J, GR5J and GR6J
hydrological models at expectile levels 0.5, 0.9, 0.95 and 0.975. An honest
assessment empirically proves that the GR6J model outperforms the other two
models at all expectile levels. Great opportunities are offered for moving
beyond the mean in hydrological modelling by simply adjusting the objective
function.",2201.06332v1,"Optimal monitoring location for risk tracking of geotechnical systems:
  theory and application to tunneling excavation risks; The maturity of structural health monitoring technology brings
ever-increasing opportunities for geotechnical structures and underground
infrastructure systems to track the risk of structural failure, such as
settlement-induced building damage, based on the monitored data. Reliability
updating techniques can offer solutions to estimate the probability of failing
to meet a prescribed objective using various types of information that are
inclusive of equality and inequality. However, the update in reliability can be
highly sensitive to monitoring location. Therefore, there may exist optimal
locations in a system for monitoring that yield the maximum value for
reliability updating. This paper proposes a computational framework for optimal
monitoring location based on an innovative metric called sensitivity of
information (SOI) that quantifies the relative change in unconditional and
conditional reliability indexes. A state-of-the-practice case of risks posed by
tunneling-induced settlement to buildings is explored in-depth to demonstrate
and evaluate the computational efficiency of the proposed framework.",False,True,"Combining expectile-based uncertainty estimation from hydrological modeling with optimal monitoring location strategies for geotechnical risks offers a novel, feasible, and useful multidisciplinary approach that integrates probabilistic hydrological insights into structural health monitoring.",
neg-1s-79,1,,1706.05823v1,"Algebraic cycles on Fano varieties of some cubics; This note is about cycle-theoretic properties of the Fano variety of lines on
a smooth cubic fivefold. The arguments are based on the fact that this Fano
variety has finite-dimensional motive. We also present some results concerning
Chow groups of Fano varieties of lines on certain cubics in other dimensions.",1706.05908v1,"Fixed points and entropy of endomorphisms on simple abelian varieties; In this paper we investigate fixed-point numbers and entropies of
endomorphisms on abelian varieties. It was shown quite recently that the number
of fixed-points of an iterated endomorphism on a simple complex torus is either
periodic or grows exponentially. Criteria to decide whether a given
endomorphism is of the one type or the other are still missing. Our first
result provides such criteria for simple abelian varieties in terms of the
possible types of endomorphism algebras. The number of fixed-points depends on
the eigenvalues and we exactly show which analytic eigenvalues occur. This
insight is also the starting point to ask for the entropy of an endomorphism.
Our second result offers criteria for an endomorphism to be of zero or positive
entropy. The entropy is computed as the logarithm of a real number and our
third result characterizes the algebraic structure of this number.",False,False,"The abstracts from algebraic geometry and dynamical systems do not present a clear, integrable concept combination that satisfies all standards of multidisciplinary, feasible, novel, and useful research within the given contexts.",
neg-1s-80,1,,1701.00875v2,"Optimal Mean-Reverting Spread Trading: Nonlinear Integral Equation
  Approach; We study several optimal stopping problems that arise from trading a
mean-reverting price spread over a finite horizon. Modeling the spread by the
Ornstein-Uhlenbeck process, we analyze three different trading strategies: (i)
the long-short strategy; (ii) the short-long strategy, and (iii) the chooser
strategy, i.e. the trader can enter into the spread by taking either long or
short position. In each of these cases, we solve an optimal double stopping
problem to determine the optimal timing for starting and subsequently closing
the position. We utilize the local time-space calculus of Peskir (2005a) and
derive the nonlinear integral equations of Volterra-type that uniquely char-
acterize the boundaries associated with the optimal timing decisions in all
three problems. These integral equations are used to numerically compute the
optimal boundaries.",1701.01327v3,"Optimal liquidation in a Level-I limit order book for large tick stocks; We propose a framework to study the optimal liquidation strategy in a limit
order book for large-tick stocks, with spread equal to one tick. All order book
events (market orders, limit orders and cancellations) occur according to
independent Poisson processes, with parameters depending on price move
directions. Our goal is to maximise the expected terminal wealth of an agent
who needs to liquidate her positions within a fixed time horizon. Assuming that
the agent trades (through sell limit order or/and sell market order) only when
the price moves, we model her liquidation procedure as a semi-Markov decision
process, and compute the semi-Markov kernel using Laplace method in the
language of queueing theory. The optimal liquidation policy is then solved by
dynamic programming, and illustrated numerically.",False,False,"Both papers are rooted in quantitative finance and focus on trading strategies and liquidation, lacking the integration of distinct disciplines to create a truly multidisciplinary research idea.",
neg-1s-81,1,,0801.0253v1,"Toward a statistical mechanics of four letter words; We consider words as a network of interacting letters, and approximate the
probability distribution of states taken on by this network. Despite the
intuition that the rules of English spelling are highly combinatorial (and
arbitrary), we find that maximum entropy models consistent with pairwise
correlations among letters provide a surprisingly good approximation to the
full statistics of four letter words, capturing ~92% of the multi-information
among letters and even ""discovering"" real words that were not represented in
the data from which the pairwise correlations were estimated. The maximum
entropy model defines an energy landscape on the space of possible words, and
local minima in this landscape account for nearly two-thirds of words used in
written English.",0801.3056v2,"Transient and Equilibrium Synchronization in Complex Neuronal Networks; Transient and equilibrium synchronizations in complex neuronal networks as a
consequence of dynamics induced by having sources placed at specific neurons
are investigated. The basic integrate-and-fire neuron is adopted, and the
dynamics is estimated computationally so as to obtain the activation at each
node along each instant of time. In the transient case, the dynamics is
implemented so as to conserve the total activation entering the system. In our
equilibrium investigations, the internally stored activation is limited to the
value of the respective threshold. The synchronization of the activation of the
network is then quantified in terms of its normalized entropy. The equilibrium
investigations involve the application of a number of complementary
characterization methods, including spectra and Principal Component Analysis,
as well as of an equivalent model capable of reproducing both the transient and
equilibrium dynamics. The potential of such concepts and measurements is
explored with respect to several theoretical models, as well as for the
neuronal network of \emph{C. elegans}. A series of interesting results are
obtained and discussed, including the fact that all models led to a transient
period of synchronization, whose specific features depend on the topological
structures of the networks. The investigations of the equilibrium dynamics
revealed a series of remarkable insights, including the relationship between
spiking oscillations and the hierarchical structure of the networks and the
identification of twin correlation patterns between node degree and total
activation, implying that hubs of connectivity are also hubs of
integrate-and-fire activation.",False,False,"Combining linguistic statistical mechanics with neuronal network synchronization does not clearly meet all standards of feasibility, novelty, and utility based on the provided abstracts.",
neg-1s-82,1,,2203.01207v1,"Container Localisation and Mass Estimation with an RGB-D Camera; In the research area of human-robot interactions, the automatic estimation of
the mass of a container manipulated by a person leveraging only visual
information is a challenging task. The main challenges consist of occlusions,
different filling materials and lighting conditions. The mass of an object
constitutes key information for the robot to correctly regulate the force
required to grasp the container. We propose a single RGB-D camera-based method
to locate a manipulated container and estimate its empty mass i.e.,
independently of the presence of the content. The method first automatically
selects a number of candidate containers based on the distance with the fixed
frontal view, then averages the mass predictions of a lightweight model to
provide the final estimation. Results on the CORSMAL Containers Manipulation
dataset show that the proposed method estimates empty container mass obtaining
a score of 71.08% under different lighting or filling conditions.",2203.01296v1,"Half Wavelet Attention on M-Net+ for Low-Light Image Enhancement; Low-Light Image Enhancement is a computer vision task which intensifies the
dark images to appropriate brightness. It can also be seen as an ill-posed
problem in image restoration domain. With the success of deep neural networks,
the convolutional neural networks surpass the traditional algorithm-based
methods and become the mainstream in the computer vision area. To advance the
performance of enhancement algorithms, we propose an image enhancement network
(HWMNet) based on an improved hierarchical model: M-Net+. Specifically, we use
a half wavelet attention block on M-Net+ to enrich the features from wavelet
domain. Furthermore, our HWMNet has competitive performance results on two
image enhancement datasets in terms of quantitative metrics and visual quality.
The source code and pretrained model are available at
https://github.com/FanChiMao/HWMNet.",False,False,"Combining mass estimation with low-light enhancement may improve performance, but it lacks the novelty and multidisciplinary integration required. The idea appears incremental rather than an ingenious fusion of concepts from both papers.",
neg-1s-83,1,,2305.00385v2,"Cross-Shaped Windows Transformer with Self-supervised Pretraining for
  Clinically Significant Prostate Cancer Detection in Bi-parametric MRI; Biparametric magnetic resonance imaging (bpMRI) has demonstrated promising
results in prostate cancer (PCa) detection using convolutional neural networks
(CNNs). Recently, transformers have achieved competitive performance compared
to CNNs in computer vision. Large scale transformers need abundant annotated
data for training, which are difficult to obtain in medical imaging.
Self-supervised learning (SSL) utilizes unlabeled data to generate meaningful
semantic representations without the need for costly annotations, enhancing
model performance on tasks with limited labeled data. We introduce a novel
end-to-end Cross-Shaped windows (CSwin) transformer UNet model, CSwin UNet, to
detect clinically significant prostate cancer (csPCa) in prostate bi-parametric
MR imaging (bpMRI) and demonstrate the effectiveness of our proposed
self-supervised pre-training framework. Using a large prostate bpMRI dataset
with 1500 patients, we first pretrain CSwin transformer using multi-task
self-supervised learning to improve data-efficiency and network
generalizability. We then finetune using lesion annotations to perform csPCa
detection. Five-fold cross validation shows that self-supervised CSwin UNet
achieves 0.888 AUC and 0.545 Average Precision (AP), significantly
outperforming four comparable models (Swin UNETR, DynUNet, Attention UNet,
UNet). Using a separate bpMRI dataset with 158 patients, we evaluate our method
robustness to external hold-out data. Self-supervised CSwin UNet achieves 0.79
AUC and 0.45 AP, still outperforming all other comparable methods and
demonstrating good generalization to external data.",2305.00627v2,"CNN-based fully automatic mitral valve extraction using CT images and
  existence probability maps; Accurate extraction of mitral valve shape from clinical tomographic images
acquired in patients has proven useful for planning surgical and interventional
mitral valve treatments. However, manual extraction of the mitral valve shape
is laborious, and the existing automatic extraction methods have not been
sufficiently accurate. In this paper, we propose a fully automated method of
extracting mitral valve shape from computed tomography (CT) images for the all
phases of the cardiac cycle. This method extracts the mitral valve shape based
on DenseNet using both the original CT image and the existence probability maps
of the mitral valve area inferred by U-Net as input. A total of 1585 CT images
from 204 patients with various cardiac diseases including mitral regurgitation
(MR) were collected and manually annotated for mitral valve region. The
proposed method was trained and evaluated by 10-fold cross validation using the
collected data and was compared with the method without the existence
probability maps. The mean error of shape extraction error in the proposed
method is 0.88 mm, which is an improvement of 0.32 mm compared with the method
without the existence probability maps.",False,False,"The proposed ideas are primarily within the domain of medical imaging and machine learning without integrating distinct disciplines, thus not meeting all multidisciplinary standards.",
neg-1s-84,1,,2305.08615v1,"Sakaguchi Swarmalators; Swarmalators are phase oscillators that cluster in space, like fireflies
flashing on a swarm to attract mates. Interactions between particles, which
tend to synchronize their phases and align their motion, decrease with the
distance and phase difference between them, coupling the spatial and phase
dynamics. In this work, we explore the effects of disorder induced by phase
frustration on a system of Swarmalators that move on a one-dimensional ring.
Our model is inspired by the well-known Kuramoto-Sakaguchi equations. We find,
numerically and analytically, the ordered and disordered states that emerge in
the system. The active states, not present in the model without disorder,
resemble states found previously in numerical studies for the 2D Swarmalators
system. One of these states, in particular, shows similarities to turbulence
generated in a flattened media. We show that all ordered states can be
generated for any values of the coupling constants by tuning the phase
frustration parameters only. Moreover, many of these combinations display
multi-stability.",2305.12346v1,"Asymptotic theory of not completely integrable soliton equations; We develop the theory of transformation of intensive initial nonlinear wave
pulses to trains of solitons emerging at asymptotically large time of
evolution. Our approach is based on the theory of dispersive shock waves in
which the number of nonlinear oscillations in the shock becomes the number of
solitons at the asymptotic state. We show that this number of oscillations,
which is proportional to the classical action of particles associated with the
small-amplitude edges of shocks, is preserved by the dispersionless flow. Then
the Poincar\'e-Cartan integral invariant is also constant and therefore it
reduces to the quantization rule similar to the Bohr-Sommerfeld quantization
rule for linear spectral problem associated with completely integrable
equations. This rule yields a set of `eigenvalues' which are related with the
asymptotic solitons' velocities and other their characteristics. Our analytical
results agree very well with the results of numerical solutions of the
generalized nonlinear Schr\""odinger equation.",False,False,"The combination of swarmalator dynamics and soliton asymptotic theory does not clearly meet all the multidisciplinary, feasible, novel, and useful criteria based on the provided abstracts.",
neg-1s-85,1,,1612.00133v3,"Geoneutrinos at Jinping: Flux prediction and oscillation analysis; Geoneutrinos are electron antineutrinos ($\bar\nu_e$) generated by the
beta-decays of radionuclides naturally occurring inside the Earth, in
particular $^{238}$U, $^{232}$Th, and $^{40}$K. Measurement of these neutrinos
provides powerful constraints on the radiogenic heat of the Earth and tests on
the Earth models. Since the prediction of $\bar\nu_e$'s in geoneutrino flux is
subject to neutrino oscillation effects, we performed a calculation including
detailed oscillation analysis in the propagation of geoneutrinos and reactor
neutrinos generated around the Earth. The expected geoneutrino signal, the
reactor neutrino background rates and the systematic error budget are provided
for a proposed 3-kiloton neutrino detector at the Jinping underground lab in
Sichuan, China. In addition, we evaluated sensitivities for the geoneutrino
flux, Th/U ratio and power of a possible fission reactor in the interior of
Earth.",1612.00705v1,"An Optical Atmospheric Phenomenon Observed in 1670 over the City of
  Astrakhan Was not a Mid-Latitude Aurora; It has been recently claimed (Zolotova and Ponyavin, Solar Phys., 291, 2869,
2016, ZP16 henceforth) that a mid-latitude optical phenomenon, which took place
over the city of Astrakhan in July 1670, according to Russian chronicles, was a
strong aurora borealis. If this was true, it would imply a very strong or even
severe geomagnetic storm during the quietest part of the Maunder minimum.
However, as we argue in this article, this conclusion is erroneous and caused
by a misinterpretation of the chronicle record. As a result of a thorough
analysis of the chronicle text, we show that the described phenomenon occurred
during the daylight period of the day (""the last morning hour""), in the south
direction (""towards noon""), and its description does not match that of an
aurora. The date of the event was also incorrectly interpreted. We conclude
that this phenomenon was not a mid-latitude aurora but an atmospheric
phenomenon, the so-called sundog (or parhelion) which is a particular type of
solar halo. Accordingly, the claim about a strong mid-latitude aurora during
the deep Maunder minimum is not correct and should be dismissed.",False,False,"The two papers focus on distinct fields—neutrino physics and historical atmospheric phenomena—with no clear overlapping concepts to form a feasible, novel, and useful multidisciplinary research idea.",
neg-1s-86,1,,1811.06750v2,"Itô vs Stratonovich in the presence of absorbing states; It is widely assumed that there exists a simple transformation from the It\^o
interpretation to the one by Stratonovich and back for any stochastic
differential equation of applied interest. While this transformation exists
under suitable conditions, and transforms one interpretation into the other at
the price of modifying the drift of the equation, it cannot be considered
universal. We show that a class of stochastic differential equations,
characterized by the presence of absorbing states and of interest in
applications, does not admit such a transformation. In particular, formally
applying this transformation may lead to the disappearance of some absorbing
states. In turn, this modifies the long-time, and even the intermediate-time,
behavior of the solutions. The number of solutions can also be modified by the
unjustified application of the mentioned transformation, as well as by a change
in the interpretation of the noise. We discuss how these facts affect the
classical debate on the It\^o vs Stratonovich dilemma.",1811.06904v3,"Well-Posedness for Some Non-Linear Diffusion Processes and Related PDE
  on the Wasserstein Space; In this paper, we investigate the well-posedness of the martingale problem
associated to non-linear stochastic differential equations (SDEs) in the sense
of McKean-Vlasov under mild assumptions on the coefficients as well as
classical solutions for a class of associated linear partial differential
equations (PDEs) defined on $[0,T] \times \mathbb{R}^d \times
\mathcal{P}\_2(\mathbb{R}^d)$, for any $T>0$, $\mathcal{P}\_2(\mathbb{R}^d)$
being the Wasserstein space (i.e. the space of probability measures on
$\mathbb{R}^d$ with a finite second-order moment). In this case, the derivative
of a map along a probability measure is understood in the Lions' sense. The
martingale problem is addressed by a fixed point argument on a suitable
complete metric space, under some mild regularity assumptions on the
coefficients that covers a large class of interaction. Also, new well-posedness
results in the strong sense are obtained from the previous analysis. Under
additional assumptions, we then prove the existence of the associated density
and investigate its smoothness property. In particular, we establish some
Gaussian type bounds for its derivatives. We eventually address the existence
and uniqueness for the related linear Cauchy problem with irregular terminal
condition and source term.",False,False,"The abstracts focus on distinct aspects of stochastic differential equations and their mathematical properties without a clear intersection that meets all multidisciplinary, feasible, novel, and useful criteria.",
neg-1s-87,1,,2306.16924v2,"Ultimate parameters of an all-optical MX resonance in Cs in ultra-weak
  magnetic field; We present the results of studying the parameters of the magnetic MX
resonance in an all-optical sensor built according to the two-beam Bell-Bloom
scheme in nonzero ultra-weak magnetic fields in which the effects of
spin-exchange broadening suppression are partially manifested. We report on the
features of the resonance under these conditions. We also optimize the
resonance parameters to achieve maximum sensitivity in magnetoencephalographic
sensors. We demonstrate an improvement in the ultimate achievable sensitivity
of an all-optical MX sensor by a factor of four or more, which in our
experiment corresponds to a decrease from 13 to 3 fT/Hz1/2 in a volume of 0.13
cm3. We also report the effect of incomplete suppression of spin-exchange
broadening under conditions of strong transverse modulated optical pumping, and
propose a semi-empirical model to describe it.",2306.16967v1,"On the relevance of acoustic measurements for creating realistic virtual
  acoustic environments; Geometrical approaches for room acoustics simulation have the advantage of
requiring limited computational resources while still achieving a high
perceptual plausibility. A common approach is using the image source model for
direct and early reflections in connection with further simplified models such
as a feedback delay network for the diffuse reverberant tail. When recreating
real spaces as virtual acoustic environments using room acoustics simulation,
the perceptual relevance of individual parameters in the simulation is unclear.
Here we investigate the importance of underlying acoustical measurements and
technical evaluation methods to obtain high-quality room acoustics simulations
in agreement with dummy-head recordings of a real space. We focus on the role
of source directivity. The effect of including measured, modelled, and
omnidirectional source directivity in room acoustics simulations was assessed
in comparison to the measured reference. Technical evaluation strategies to
verify and improve the accuracy of various elements in the simulation
processing chain from source, the room properties, to the receiver are
presented. Perceptual results from an ABX listening experiment with random
speech tokens are shown and compared with technical measures for a ranking of
simulation approaches.",False,False,"The two papers address distinct fields—optical magnetic sensing and acoustic environment simulation—with no clear overlapping concepts to form a feasible, novel, and useful multidisciplinary research idea that meets all the specified standards.",
neg-1s-88,1,,math-ph/0508068v2,"Lamé equation, quantum top and elliptic Bernoulli polynomials; A generalisation of the odd Bernoulli polynomials related to the quantum
Euler top is introduced and investigated. This is applied to compute the
coefficients of the spectral polynomials for the classical Lam\'e operator.",math-ph/0512055v1,"p-Adic analysis in the Lizorkin type spaces: fractional operators,
  pseudo-differential equations and Tauberian theorems; In this paper the p -adic Lizorkin spaces of test functions and distributions
are introduced, and multidimensional Vladimirov's and Taibleson's fractional
operators are studied on these spaces. Since the p -adic Lizorkin spaces are
invariant under the Vladimirov and Taibleson operators, they can play a key
role in considerations related to fractional operator problems. A class of p
-adic pseudo-differential operators in the Lizorkin spaces is also introduced
and solutions of pseudo-differential equations are constructed. p -Adic
multidimensional Tauberian theorems connected with fractional operators and
pseudo-differential operators for the Lizorkin distributions are also proved.",False,False,"Combining concepts from quantum tops and p-adic fractional operators may lack clear feasibility and practical utility, making it unlikely to meet all multidisciplinary, novel, and useful research standards.",
neg-1s-89,1,,1609.07319v1,"Equidistribution in S-arithmetic and adelic spaces; We give an introduction to adelic mixing and its applications for
mathematicians knowing about the mixing of the geodesic flow on hyperbolic
surfaces. We focus on the example of the Hecke trees in the modular surface.",1609.07323v1,"Optimal Control Problems in Transport Dynamics; In the present paper we deal with an optimal control problem related to a
model in population dynamics; more precisely, the goal is to modify the
behavior of a given density of individuals via another population of agents
interacting with the first. The cost functional to be minimized to determine
the dynamics of the second population takes into account the desired target or
configuration to be reached as well as the quantity of control agents. Several
applications may fall into this framework, as for instance driving a mass of
pedestrian in (or out of) a certain location; influencing the stock market by
acting on a small quantity of key investors; controlling a swarm of unmanned
aerial vehicles by means of few piloted drones.",False,False,"The combination of adelic mixing in mathematical spaces and optimal control in transport dynamics does not clearly present a feasible, novel, and useful multidisciplinary research idea that can be experimentally validated.",
neg-1s-90,1,,1110.6595v2,"Ground waves in atomic chains with bi-monomial double-well potential; Ground waves in atomic chains are traveling waves that corresponds to minimal
non-trivial critical values of the underlying action functional. In this paper
we study FPU-type chains with bi-monomial double-well potential and prove the
existence of both periodic and solitary ground waves. To this end we minimize
the action on the Nehari manifold and show that periodic ground waves converge
to solitary ones. Finally, we compute ground waves numerically by a suitable
discretization of a constrained gradient flow.",1111.0205v1,"Random attractors for singular stochastic partial differential equations; The existence of random attractors for singular stochastic partial
differential equations (SPDE) perturbed by general additive noise is proven.
The drift is assumed only to satisfy the standard assumptions of the
variational approach to SPDE with compact embeddings in the Gelfand triple and
singular coercivity. For ergodic, monotone, contractive random dynamical
systems it is proven that the attractor consists of a single random point. In
case of real, linear multiplicative noise finite time extinction is obtained.
Applications include stochastic generalized fast diffusion equations and
stochastic generalized singular p-Laplace equations perturbed by Levy noise
with jump measure having finite first and second moments.",False,True,"Combining ground wave dynamics in atomic chains with stochastic attractor theory creates a novel, feasible multidisciplinary research idea. It integrates nonlinear physics and stochastic analysis, offering potential experimental validation and practical applications in material science.",
neg-1s-91,1,,2209.10489v1,"Recurrent Super-Resolution Method for Enhancing Low Quality Thermal
  Facial Data; The process of obtaining high-resolution images from single or multiple
low-resolution images of the same scene is of great interest for real-world
image and signal processing applications. This study is about exploring the
potential usage of deep learning based image super-resolution algorithms on
thermal data for producing high quality thermal imaging results for in-cabin
vehicular driver monitoring systems. In this work we have proposed and
developed a novel multi-image super-resolution recurrent neural network to
enhance the resolution and improve the quality of low-resolution thermal
imaging data captured from uncooled thermal cameras. The end-to-end fully
convolutional neural network is trained from scratch on newly acquired thermal
data of 30 different subjects in indoor environmental conditions. The
effectiveness of the thermally tuned super-resolution network is validated
quantitatively as well as qualitatively on test data of 6 distinct subjects.
The network was able to achieve a mean peak signal to noise ratio of 39.24 on
the validation dataset for 4x super-resolution, outperforming bicubic
interpolation both quantitatively and qualitatively.",2209.10675v2,"A Validation Approach to Over-parameterized Matrix and Image Recovery; This paper studies the problem of recovering a low-rank matrix from several
noisy random linear measurements. We consider the setting where the rank of the
ground-truth matrix is unknown a priori and use an objective function built
from a rank-overspecified factored representation of the matrix variable, where
the global optimal solutions overfit and do not correspond to the underlying
ground truth. We then solve the associated nonconvex problem using gradient
descent with small random initialization. We show that as long as the
measurement operators satisfy the restricted isometry property (RIP) with its
rank parameter scaling with the rank of the ground-truth matrix rather than
scaling with the overspecified matrix rank, gradient descent iterations are on
a particular trajectory towards the ground-truth matrix and achieve nearly
information-theoretically optimal recovery when it is stopped appropriately. We
then propose an efficient stopping strategy based on the common hold-out method
and show that it detects a nearly optimal estimator provably. Moreover,
experiments show that the proposed validation approach can also be efficiently
used for image restoration with deep image prior, which over-parameterizes an
image with a deep network.",False,True,"The combination of advanced super-resolution techniques for thermal imaging and robust matrix/image recovery validation methods offers a novel, feasible, and useful multidisciplinary approach that can enhance driver monitoring systems effectively.",
neg-1s-92,1,,1809.08946v1,"Direct visualization of the 3D structure of silicon impurities in
  graphene; We directly visualize the three-dimensional (3D) geometry and dynamics of
silicon impurities in graphene as well as their dynamics by
aberration-corrected scanning transmission electron microscopy. By acquiring
images when the sample is tilted, we show that an asymmetry of the atomic
position of the heteroatom in the projection reveals the non-planarity of the
structure. From a sequence of images, we further demonstrate that the Si atom
switches between up- and down- configurations with respect to the graphene
plane, with an asymmetric cross-section. We further analyze the 3D structure
and dynamics of a silicon tetramer in graphene. Our results clarify the
out-of-plane structure of impurities in graphene by direct experimental
observation and open a new route to study their dynamics in three dimensions.",1809.09301v1,"Reconfigurable Shape-Morphing Dielectric Elastomers Using Spatially
  Varying Electric Fields; Exceptionally large strains can be produced in soft elastomers by the
application of an electric field and the strains can be exploited for a variety
of novel actuators, such as tunable lenses and tactile actuators. However,
shape morphing with dielectric elastomers has not been possible since no
generalizable method for changing their Gaussian curvature has been devised. It
is shown that this fundamental limitation can be lifted by introducing
internal, spatially varying electric fields through a layer-by-layer
fabrication method incorporating shaped, carbon-nanotubes-based electrodes
between thin elastomer sheets. To illustrate the potential of the method,
voltage-tunable negative and positive Gaussian curvatures shapes are produced.
Furthermore, by applying voltages to different sets of internal electrodes, the
shapes can be re-configured. All the shape changes are reversible when the
voltage is removed.",False,True,"Combining 3D visualization of atomic structures in graphene with shape-morphing dielectric elastomers could create novel materials with dynamically adjustable properties, leveraging insights from materials science and electrical engineering. This multidisciplinary approach is feasible, innovative, and applicable to advanced material design.",
neg-1s-93,1,,2308.14388v2,"Biclustering Methods via Sparse Penalty; In this paper, we first reviewed several biclustering methods that are used
to identify the most significant clusters in gene expression data. Here we
mainly focused on the SSVD(sparse SVD) method and tried a new sparse penalty
named ""Prenet penalty"" which has been used only in factor analysis to gain
sparsity. Then in the simulation study, we tried different types of generated
datasets (with different sparsity and dimension) and tried 1-layer
approximation then for k-layers which shows the mixed Prenet penalty is very
effective for non-overlapped data. Finally, we used some real gene expression
data to show the behavior of our methods.",2309.03242v1,"Automated Bioinformatics Analysis via AutoBA; With the fast-growing and evolving omics data, the demand for streamlined and
adaptable tools to handle the analysis continues to grow. In response to this
need, we introduce Auto Bioinformatics Analysis (AutoBA), an autonomous AI
agent based on a large language model designed explicitly for conventional
omics data analysis. AutoBA simplifies the analytical process by requiring
minimal user input while delivering detailed step-by-step plans for various
bioinformatics tasks. Through rigorous validation by expert bioinformaticians,
AutoBA's robustness and adaptability are affirmed across a diverse range of
omics analysis cases, including whole genome sequencing (WGS), RNA sequencing
(RNA-seq), single-cell RNA-seq, ChIP-seq, and spatial transcriptomics. AutoBA's
unique capacity to self-design analysis processes based on input data
variations further underscores its versatility. Compared with online
bioinformatic services, AutoBA deploys the analysis locally, preserving data
privacy. Moreover, different from the predefined pipeline, AutoBA has
adaptability in sync with emerging bioinformatics tools. Overall, AutoBA
represents a convenient tool, offering robustness and adaptability for complex
omics data analysis.",False,True,"Combining sparse biclustering with an autonomous AI agent could create a novel tool that automatically identifies significant gene clusters, is experimentally feasible, innovative in integrating methods, and highly useful for streamlined gene expression analysis.",
neg-1s-94,1,,2412.07074v1,"Channel Spreading Function-Inspired Channel Transfer Function Estimation
  for OFDM Systems with High-Mobility; In this letter, we propose a novel channel transfer function (CTF) estimation
approach for orthogonal frequency division multiplexing (OFDM) systems in
high-mobility scenarios, that leverages the stationary properties of the
delay-Doppler domain channel spreading function (CSF). First, we develop a CSF
estimation model for OFDM systems that relies solely on discrete pilot symbols
in the time-frequency (TF) domain, positioned at predefined resource elements.
We then present theorems to elucidate the relationship between CSF compactness
and pilot spacing in the TF domain for accurate CSF acquisition. Based on the
estimated CSF, we finally estimate the CTF for data symbols. Numerical results
show that, in high-mobility scenarios, the proposed approach outperforms
traditional interpolation-based methods and closely matches the optimal
estimator in terms of estimation accuracy. This work may pave the way for CSF
estimation in commercial OFDM systems, benefiting high-mobility communications,
integrated sensing and communications, and related applications.",2412.07173v1,"Semantic Communications for Digital Signals via Carrier Images; Most of current semantic communication (SemCom) frameworks focus on the image
transmission, which, however, do not address the problem on how to deliver
digital signals without any semantic features. This paper proposes a novel
SemCom approach to transmit digital signals by using the image as the carrier
signal. Specifically, the proposed approach encodes the digital signal as a
binary stream and maps it to mask locations on an image. This allows binary
data to be visually represented, enabling the use of existing model,
pre-trained Masked Autoencoders (MAE), which are optimized for masked image
reconstruction, as the SemCom encoder and decoder. Since MAE can both process
and recover masked images, this approach allows for the joint transmission of
digital signals and images without additional overhead. In addition,
considering the mask tokens transmission encoded by the MAE still faces extra
costs, we design a sparse encoding module at the transmitter to encode the mask
tokens into a sparse matrix, and it can be recovered at the receiver. Thus,
this approach simply needs to transmit the latent representations of the
unmasked patches and a sparse matrix, which further reduce the transmission
overhead compared with the original MAE encoder. Simulation results show that
the approach maintains reliable transmission of digital signals and images even
in a high mask ratio of transmitted images.",False,False,"The combination of high-mobility OFDM channel estimation and semantic image-based digital signal transmission does not clearly present a feasible, novel, and useful multidisciplinary research idea that meets all the specified standards.",
neg-1s-95,1,,2310.18418v1,"STV+Reductions: Towards Practical Verification of Strategic Ability
  Using Model Reductions; We present a substantially expanded version of our tool STV for strategy
synthesis and verification of strategic abilities. The new version adds
user-definable models and support for model reduction through partial order
reduction and checking for bisimulation.",2310.19579v1,"A Navigation Logic for Recursive Programs with Dynamic Thread Creation; Dynamic Pushdown Networks (DPNs) are a model for multithreaded programs with
recursion and dynamic creation of threads. In this paper, we propose a temporal
logic called NTL for reasoning about the call- and return- as well as thread
creation behaviour of DPNs. Using tree automata techniques, we investigate the
model checking problem for the novel logic and show that its complexity is not
higher than that of LTL model checking against pushdown systems despite a more
expressive logic and a more powerful system model. The same holds true for the
satisfiability problem when compared to the satisfiability problem for a
related logic for reasoning about the call- and return-behaviour of pushdown
systems. Overall, this novel logic offers a promising approach for the
verification of recursive programs with dynamic thread creation.",False,False,"Both papers focus on formal verification and model checking within computer science, lacking the combination of distinct disciplines required for a multidisciplinary research idea.",
neg-1s-96,1,,2212.06078v1,"Non-smooth dynamics of buckling based metainterfaces: rocking-like
  motion and bifurcations; The non-smooth dynamics is investigated for an elastic planar metainterface
composed by two layers of buckling elements, each one allowing motion on one
side only. Through the analogy between buckling and unilateral contact and by
assuming no-bouncing at impact, the motion of the relevant two degrees of
freedom system is reduced to that of a single degree governed by a
piecewise-smooth differential equation. The metainterface dynamics has strong
similarities with the rocking motion of rigid blocks and displays several types
of dynamic bifurcations in the presence of oscillatory forces, including period
doubling, branch point cycle, grazing, as well as quasi-periodic and chaotic
responses. Moreover, the multistable response is found to be broaden to
conditions representative of monostable states within a quasi-static setting,
disclosing a multistability anticipation by dynamics. The wide landscape of the
dynamic response for the buckling based metainterface provides a novel
theoretical framework to be exploited in the design of mechanical devices for
vibration attenuation and for energy harvesting.",2212.07005v1,"A systematic search for a three-velocity gyrodistributive law in special
  relativity with the lorentz R package; Here I present the lorentz package for working with relativistic physics. The
package includes functionality for four-vector transformations, three-velocity
addition, and other relativistic processes such as the behaviour of photons. It
was designed to facilitate the search for a gyrodistributive law. In special
relativity, three-velocities and scalars constitute a gyrovector space with
addition $\oplus$ and scalar multiplication $\odot$. Standard vector spaces
obey the distributive law $a(x + y) = ax + ay$ for scalar $a$ and vectors $x$,
$y$; but no analogous gyrodistributive law for $r\odot (u \oplus v)$ is known.
The package was designed to facilitate the search for a gyrodistributive law
and includes functionality for four-vector transformations and three-velocity
addition, which is noncommutative and nonassociative. I use the package to
systematically sweep a large space of potential gyrodistributive laws, without
success. The package is available on CRAN, at
\url{https://CRAN.R-project.org/package=lorentz}.",False,False,"The two papers belong to fundamentally different disciplines with no clear overlapping concepts that can form a feasible, novel, and useful multidisciplinary research idea meeting all the specified standards.",
neg-1s-97,1,,math/9902149v1,"Towards a Mori theory on compact Kaehler threefolds, III; We prove abundance for a minimal Kaehler threefold which is not both simple
and non-Kummer. Recall that a variety is simple if there is no compact
subvariety of positive dimension through a sufficiently general point .
Furthermore we prove that a smooth compact Kaehler threefold whose canonical
bundle is not nef, carries a contraction unless (possibly) the manifold is
simple non-Kummer. It is generally conjectured that simple threefolds must be
Kummer.",math/9902152v1,"New Invariants for surfaces; We take the fundamental group of the complement of the branch curve of a
generic projection induced from canonical embedding of a surface. This group is
stable on connected components of moduli spaces of surfaces. Since for many
classes of surfaces it is expected that the fundamental group has a polycyclic
structure, we define a new invariant that comes from this structure. We compute
this invariant for a few examples. Braid monodromy factorizations related to
curves is a first step in computing the fundamental group of the complement of
the curve, and thus we also indicate the possibility of using braid monodromy
factorizations of branch curves as an invariant of a surface.",False,False,"Both papers focus on aspects of algebraic geometry, lacking the integration of distinct disciplines necessary for a multidisciplinary research idea that meets all the specified standards.",
neg-1s-98,1,,1906.06289v2,"Multi-Carrier Agile Phased Array Radar; Modern radar systems are expected to operate reliably in congested
environments. A candidate technology for meeting these demands is frequency
agile radar (FAR), which randomly changes its carrier frequencies. FAR is known
to improve the electronic counter-countermeasures (ECCM) performance while
facilitating operation in congested setups. To enhance the target recovery
performance of FAR in complex electromagnetic environments, we propose two
radar schemes extending FAR to multi-carrier waveforms. The first is Wideband
Multi-carrier Agile Radar (WMAR), which transmits/receives wideband waveforms
simultaneously with every antenna. To mitigate the demanding hardware
requirements associated with wideband waveforms used by WMAR, we next propose
multi-Carrier AgilE phaSed Array Radar (CAESAR). CAESAR uses narrowband
monotone waveforms, thus facilitating ease of implementation of the system,
while introducing {\em spatial agility}. We characterize the transmitted and
received signals of the proposed schemes, and develop an algorithm for
recovering the targets, based on concepts from compressed sensing to estimate
the range-Doppler parameters of the targets. We then derive conditions which
guarantee their accurate reconstruction. Our numerical study demonstrates that
both multi-carrier schemes improve performance compared to FAR while
maintaining its practical benefits. We also demonstrate that the performance of
CAESAR, which uses monotone waveforms, is within a small gap from the wideband
radar.",1906.06380v1,"Time Synchronization in 5G Wireless Edge: Requirements and Solutions for
  Critical-MTC; Wireless edge is about distributing intelligence to the wireless devices
wherein the distribution of accurate time reference is essential for
time-critical machine-type communication (cMTC). In 5G-based cMTC, enabling
time synchronization in the wireless edge means moving beyond the current
synchronization needs and solutions in 5G radio access. In this article, we
analyze the device-level synchronization needs of potential cMTC applications:
industrial automation, power distribution, vehicular communication, and live
audio/video production. We present an over-the-air (OTA) synchronization scheme
comprised of 5G air interface parameters, and discuss their associated timing
errors. We evaluate the estimation error in device-to-base station propagation
delay from timing advance (TA) under random errors and show how to reduce the
estimation error. In the end, we identify the random errors specific to dense
multipath fading environments and discuss countermeasures.",False,True,"Combining multi-carrier agile radar with 5G time synchronization can create a synchronized radar-communication system for environments like industrial automation, offering enhanced target detection and reliable time-critical operations, meeting all multidisciplinary, feasibility, novelty, and usefulness criteria.",
neg-1s-99,1,,2412.15281v1,"Minimal subshifts of prescribed mean dimension over general alphabets; Let $G$ be a countable infinite amenable group, $K$ a finite-dimensional
compact metrizable space, and $(K^G,\sigma)$ the full $G$-shift on $K^G$. For
any $r\in [0,{\rm mdim}(K^G,\sigma))$, we construct a minimal subshift
$(X,\sigma)$ of $(K^G,\sigma)$ with mdim$(X,\sigma)=r$. Furthermore, we
construct a subshift of $([0,1]^G,\sigma)$ such that its mean dimension is $1$,
and that the set of all attainable values of the mean dimension of its minimal
subsystems is exactly the interval $[0,1)$.",2412.15528v1,"Pullback measure attractors and limiting behaviors of McKean-Vlasov
  stochastic delay lattice systems; We study the long-term behavior of the distribution of the solution process
to the non-autonomous McKean-Vlasov stochastic delay lattice system defined on
the integer set $\mathbb{Z}$. Specifically, we first establish the
well-posedness of solutions for this non-autonomous, distribution-dependent
stochastic delay lattice system. Then, we prove the existence and uniqueness of
pullback measure attractors for the non-autonomous dynamical system generated
by the solution operators, defined in the space of probability measures.
Furthermore, as an application of the pullback measure attractor, we prove the
ergodicity and exponentially mixing of invariant measures for the system under
appropriate conditions. Finally, we establish the upper semi-continuity of
these attractors as the distribution-dependent stochastic delay lattice system
converges to a distribution-independent system.",False,False,"Combining symbolic dynamics with stochastic delay lattice systems does not clearly meet all standards of feasibility, novelty, and utility based on the abstracts provided.",
